{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run 1: Load AbstractNet Dataset: 70K unlabeled (and 2K labeled) Abstracts into DB\n",
    "\n",
    "This notebook loads the dataset and create labeled *candidates* through labeling function. Feel extra free to document/bring up any upcoming confusion throughout the test run, e.g. are the following two consistent, the **(segment, label) pair** that we want to have, and the **candidates** that we instruct snorkel to extract? \n",
    "\n",
    "Before everything, please ensure that you have followed project-level ``README.md`` and installed all python dependencies, e.g. ``tika``.  \n",
    "\n",
    "We filtered out null abstracts from `ClydeDB.csv` ([AbstractSegmentationCrowdNLP Git repo](https://github.com/zhoujieli/AbstractSegmentationCrowdNLP.git)), resulting in 48,914 valid ones out of 56,851 total abstracts. The 48,914 abstracts are saved to `data/70kpaper.tsv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we preprocess documents by parsing them into *contexts*. *Candidates* are extracted out of *contexts*, which are *instances* (one of the *background*, *mechanism*, *method*, and *findings*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# # Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 100 #  change the number 1000 to 60,000 for real dataset \n",
    "\n",
    "from snorkel.parser import TSVDocPreprocessor\n",
    "\n",
    "doc_preprocessor = TSVDocPreprocessor('data/70kpaper_061418_cleaned.tsv', encoding=\"utf-8\",max_docs=n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get statistics on the number of documents and sentences, as below. This could take 5-8 minutes to load ~60K papers (see progress bar, also might have exception). The following code parses docs into sentences by period, averaging 4.49 sentences per documents. Earlier I spent a few hours debugging some hidden formatting error that confuses Spacey. Need to ensure that we format raw data from .csv into .tsv *without* preceeding and appending quotes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 3.91 s, sys: 347 ms, total: 4.26 s\n",
      "Wall time: 2.92 s\n",
      "Documents: 100\n",
      "Sentences: 495\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs)\n",
    "\n",
    "\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract `candidates` by defining the specific `CandidateExtractor` for abstract segmentation. We take *Background* as an example and come back with other segmentation parts, i.e., *mechanism*, *method*, *findings*, later. \n",
    "\n",
    "Some more explanation based on my understanding: \n",
    "    \n",
    "1. `Candidates` are defined as a class that contains 1+ `Span` objects within one `Sentence` context.  \n",
    "    \n",
    "2. `Span(s)` correspond to conceptual categories in text like people or disease names. \n",
    "    \n",
    "In the intro tutorial example, their `candidate` represents the possible `Spouse` mention `(Barrack Obama, Michelle Obama)`. As readers, we know this mention is true due to external knowledge and the keyword of `wedding` occuring later in the sentence. (Reference: (1) section `Writing a basic CandidateExtractor` in [Intro_tutorial_1](../intro/Intro_Tutorial_1.ipynb); (2) section `Candidate Member Functions and Variables` in [Workshop_1_Snorkel_API](../workshop/Workshop_1_Snorkel_API.ipynb)) \n",
    "\n",
    "\n",
    "+ Background: \n",
    "  - \"Recent research ... \", \n",
    "  - \"... have/has been widely ...\", \n",
    "  - \"How ... ?\" (and as the first sentence), \n",
    "  - \"Previous work...\", \n",
    "  - \"Motivated by...\", \n",
    "  - \"The success of ...\", etc.\n",
    "+ Mechanism:\n",
    "  - something\n",
    "  - some other pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `CandidateExtractor` as a wrapper of `CandidateSpace` (e.g. `Ngrams` is one type of `CandidateSpace`) and `Matcher` (e.g. `DictionaryMatcher`, `PersonMatcher`). Please make sure that `longest_match_only=True`, since this gets us longest span that contains dictionary words. (Reference: source code [candidates.py](../../snorkel/candidates.py) and [matchers.py](../../snorkel/matchers.py)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher,DictionaryMatch\n",
    "\n",
    "Background = candidate_subclass('Background', ['background_cue'])\n",
    "\n",
    "ngrams = Ngrams(n_max=30)\n",
    "# Start simple: any ngram that matches the dictionary are *background* candidates! \n",
    "dict_matcher=DictionaryMatch(d=['previous','motivated','recent','widely'],longest_match_only=True) \n",
    "cand_extractor=CandidateExtractor(Background, [ngrams], [dict_matcher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the defined `CandidateExtractor` to the all `Sentences` in the collection (splitted 90/10/10 for train/dev/test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1.05 s, sys: 118 ms, total: 1.17 s\n",
      "Wall time: 1.13 s\n",
      "Number of candidates extracted: 16 \n",
      "\n",
      "\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 208 ms, sys: 49.7 ms, total: 257 ms\n",
      "Wall time: 243 ms\n",
      "Number of candidates extracted: 0 \n",
      "\n",
      "\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 188 ms, sys: 45 ms, total: 233 ms\n",
      "Wall time: 218 ms\n",
      "Number of candidates extracted: 1 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document\n",
    "from util import number_of_people\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if number_of_people(s) <= 5:\n",
    "            if i % 10 == 8:\n",
    "                \n",
    "                dev_sents.add(s)\n",
    "            elif i % 10 == 9:\n",
    "                test_sents.add(s)\n",
    "            else:\n",
    "                train_sents.add(s)\n",
    "                \n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    %time cand_extractor.apply(sents, split=i)\n",
    "    print(\"Number of candidates extracted:\", session.query(Background).filter(Background.split == i).count(),\"\\n\\n\")\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few of those extracted `Candidates`! Obviously, since we used `DictionaryMatcher`, all `Candidates` will contain at least one word from the set \\['previous','motivated','recent','widely'\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Candidate/Span 0/16:\tSpan(\"b'Abstract: Switched LANs are become more widely used because they can provide a higher bandwidth than LANs based on shared media.'\", sentence=311, chars=[0,127], words=[0,22])\n",
      "This Candidate/Span's parent Sentence's text:\tAbstract: Switched LANs are become more widely used because they can provide a higher bandwidth than LANs based on shared media.\n",
      "\n",
      "The Candidate/Span 1/16:\tSpan(\"b'As sketched in the previous lecture, this an important piece in the general technique of verifying properties of logic programs by reasoning about proof terms.\\n'\", sentence=177, chars=[0,159], words=[0,27])\n",
      "This Candidate/Span's parent Sentence's text:\tAs sketched in the previous lecture, this an important piece in the general technique of verifying properties of logic programs by reasoning about proof terms.\n",
      "\n",
      "\n",
      "The Candidate/Span 2/16:\tSpan(\"b'contrast, in our recent research we have created software that can solve arbitrary automated mechanism design instances using a mixed integer/linear program solver (CPLEX 8.0)'\", sentence=490, chars=[3,177], words=[1,30])\n",
      "This Candidate/Span's parent Sentence's text:\tIn contrast, in our recent research we have created software that can solve arbitrary automated mechanism design instances using a mixed integer/linear program solver (CPLEX 8.0).\n",
      "\n",
      "The Candidate/Span 3/16:\tSpan(\"b', in our recent research we have created software that can solve arbitrary automated mechanism design instances using a mixed integer/linear program solver (CPLEX 8.0).'\", sentence=490, chars=[11,178], words=[2,31])\n",
      "This Candidate/Span's parent Sentence's text:\tIn contrast, in our recent research we have created software that can solve arbitrary automated mechanism design instances using a mixed integer/linear program solver (CPLEX 8.0).\n",
      "\n",
      "The Candidate/Span 4/16:\tSpan(\"b'In contrast, in our recent research we have created software that can solve arbitrary automated mechanism design instances using a mixed integer/linear program solver (CPLEX 8.0'\", sentence=490, chars=[0,176], words=[0,29])\n",
      "This Candidate/Span's parent Sentence's text:\tIn contrast, in our recent research we have created software that can solve arbitrary automated mechanism design instances using a mixed integer/linear program solver (CPLEX 8.0).\n",
      "\n",
      "The Candidate/Span 5/16:\tSpan(\"b'discuss, analyze, and experiment with a setup motivated by the behavior of real-world distributed computation networks, where the machines are differently slow at different time'\", sentence=366, chars=[3,179], words=[1,30])\n",
      "This Candidate/Span's parent Sentence's text:\tWe discuss, analyze, and experiment with a setup motivated by the behavior of real-world distributed computation networks, where the machines are differently slow at different time.\n",
      "\n",
      "The Candidate/Span 6/16:\tSpan(\"b'We discuss, analyze, and experiment with a setup motivated by the behavior of real-world distributed computation networks, where the machines are differently slow at different'\", sentence=366, chars=[0,174], words=[0,29])\n",
      "This Candidate/Span's parent Sentence's text:\tWe discuss, analyze, and experiment with a setup motivated by the behavior of real-world distributed computation networks, where the machines are differently slow at different time.\n",
      "\n",
      "The Candidate/Span 7/16:\tSpan(\"b', analyze, and experiment with a setup motivated by the behavior of real-world distributed computation networks, where the machines are differently slow at different time.'\", sentence=366, chars=[10,180], words=[2,31])\n",
      "This Candidate/Span's parent Sentence's text:\tWe discuss, analyze, and experiment with a setup motivated by the behavior of real-world distributed computation networks, where the machines are differently slow at different time.\n",
      "\n",
      "The Candidate/Span 8/16:\tSpan(\"b'Session types are widely accepted as an expressive discipline for structuring communications in concurrent and distributed systems.'\", sentence=504, chars=[0,130], words=[0,17])\n",
      "This Candidate/Span's parent Sentence's text:\tSession types are widely accepted as an expressive discipline for structuring communications in concurrent and distributed systems.\n",
      "\n",
      "The Candidate/Span 9/16:\tSpan(\"b'In the previous lectures we have considered a programming language C0 with pointers and memory and array allocation.'\", sentence=295, chars=[0,115], words=[0,18])\n",
      "This Candidate/Span's parent Sentence's text:\tIn the previous lectures we have considered a programming language C0 with pointers and memory and array allocation.\n",
      "\n",
      "The Candidate/Span 10/16:\tSpan(\"b'Previous Work: Highway Control'\", sentence=329, chars=[0,29], words=[0,4])\n",
      "This Candidate/Span's parent Sentence's text:\tPrevious Work: Highway Control\n",
      "\n",
      "The Candidate/Span 11/16:\tSpan(\"b'modality [\\xce\\xb1] and a diamond modality\\xe2\\x8c\\xa9 \\xce\\xb1\\xe2\\x8c\\xaa. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently'\", sentence=263, chars=[35,180], words=[10,39])\n",
      "This Candidate/Span's parent Sentence's text:\tFor each program α, there is a box-modality [α] and a diamond modality〈 α〉. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently [GW09].\n",
      "\n",
      "The Candidate/Span 12/16:\tSpan(\"b'[\\xce\\xb1] and a diamond modality\\xe2\\x8c\\xa9 \\xce\\xb1\\xe2\\x8c\\xaa. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently ['\", sentence=263, chars=[44,182], words=[11,40])\n",
      "This Candidate/Span's parent Sentence's text:\tFor each program α, there is a box-modality [α] and a diamond modality〈 α〉. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently [GW09].\n",
      "\n",
      "The Candidate/Span 13/16:\tSpan(\"b'\\xce\\xb1] and a diamond modality\\xe2\\x8c\\xa9 \\xce\\xb1\\xe2\\x8c\\xaa. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently [GW09'\", sentence=263, chars=[45,186], words=[12,41])\n",
      "This Candidate/Span's parent Sentence's text:\tFor each program α, there is a box-modality [α] and a diamond modality〈 α〉. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently [GW09].\n",
      "\n",
      "The Candidate/Span 14/16:\tSpan(\"b'] and a diamond modality\\xe2\\x8c\\xa9 \\xce\\xb1\\xe2\\x8c\\xaa. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently [GW09]'\", sentence=263, chars=[46,187], words=[13,42])\n",
      "This Candidate/Span's parent Sentence's text:\tFor each program α, there is a box-modality [α] and a diamond modality〈 α〉. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently [GW09].\n",
      "\n",
      "The Candidate/Span 15/16:\tSpan(\"b'and a diamond modality\\xe2\\x8c\\xa9 \\xce\\xb1\\xe2\\x8c\\xaa. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently [GW09].'\", sentence=263, chars=[48,188], words=[14,43])\n",
      "This Candidate/Span's parent Sentence's text:\tFor each program α, there is a box-modality [α] and a diamond modality〈 α〉. PDL was developed from first-order dynamic logic by Fischer-Ladner [FL79] and has become popular recently [GW09].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cands = session.query(Background).filter(Background.split == 0).all()\n",
    "\n",
    "for i in range(len(cands)):\n",
    "    print(\"The Candidate/Span \"+str(i)+\"/\"+str(len(cands))+\":\\t\"+str(cands[i].background_cue))\n",
    "    print(\"This Candidate/Span's parent Sentence's text:\\t\"+str(cands[i].get_parent().text))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Solved) Question 1:** the `CandidateExtractor` extracts only spans with length 1. But Each sentence is only getting matched once with one span (try search \"sentence=3993\", for example). The sentence is good, but we still would want longer span, e.g. half part or the whole of a sentence. \n",
    "\n",
    "`Span(\"b'previous'\", sentence=4705, chars=[20,27], words=[4,4])`\n",
    "\n",
    "**Answer 1:** overwrite `DictionaryMatch._f()`.\n",
    "\n",
    "=====================================================================================\n",
    "\n",
    "**Question 2:** Several `Span` corresponds to the same sentence, e,g, three `Span` corresponds to `sentence=490`. They wee all included even after we have set `longest_match_only=True`, as these spans all have the longest length.  \n",
    "\n",
    "**Answer 2:** Perhaps try either (1) filtering by first appearance of document ID; (2) set max_length as the length of the longest sentence length.\n",
    "\n",
    "\n",
    "### Stopped here as of Wed 06/19 9:39 pm. More to come ... ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Get info of sentence\n",
    "# for sent in (docs[0].sentences):\n",
    "#     print(sent.__dict__.keys())\n",
    "#     print(sent.text)\n",
    "#     print(sent.pos_tags)\n",
    "#     print(sent.ner_tags)\n",
    "    \n",
    "\n",
    "# sents=session.query(Sentence).order_by(Sentence.name).all()\n",
    "# print(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DictionaryMatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f918b54ee9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mngrams\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mNgrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdict_matcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionaryMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcand_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCandidateExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpouse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdict_matcher\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mDictionaryMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DictionaryMatch' is not defined"
     ]
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Background', ['background'])\n",
    "\n",
    "\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher\n",
    "\n",
    "ngrams         = Ngrams(n_max=30)\n",
    "dict_matcher = DictionaryMatch()\n",
    "cand_extractor = CandidateExtractor(Spouse, [ngrams], [dict_matcher])\n",
    "DictionaryMatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! \n",
    "\n",
    "Some debugging note at the very end (could ignore). \n",
    "\n",
    "```\n",
    "python -m spacy download en\n",
    "```\n",
    "\n",
    "Current issue: parser does not parse *by periods*. Sentence count is significantly fewer than expected! \n",
    "Potential fix: https://github.com/explosion/spaCy/issues/93\n",
    "\n",
    "======= Some more debugging log here (not necessary, could skip reading) ======\n",
    "~~~~\n",
    "Xins-MacBook-Pro:~ xin$ source activate snorkel\n",
    "(snorkel) Xins-MacBook-Pro:~ xin$ python\n",
    "Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 12:04:33) \n",
    "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import spacey\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "ModuleNotFoundError: No module named 'spacey'\n",
    ">>> import spacy\n",
    ">>> spacy.load('en')\n",
    "<spacy.en.English object at 0x1080e1da0>\n",
    ">>> model=spacy.load('en')\n",
    ">>> docs=model.tokenizer('Hello, world. Here are two sentences.')\n",
    ">>> for sent in docs.sents:\n",
    "...     pritn(sent.text)\n",
    "... \n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "  File \"spacy/tokens/doc.pyx\", line 439, in __get__ (spacy/tokens/doc.cpp:9808)\n",
    "ValueError: Sentence boundary detection requires the dependency parse, which requires data to be installed. For more info, see the documentation: \n",
    "https://spacy.io/docs/usage\n",
    "\n",
    ">>> for sent in docs.sents:\n",
    "...     print(sent.text)\n",
    "... \n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "  File \"spacy/tokens/doc.pyx\", line 439, in __get__ (spacy/tokens/doc.cpp:9808)\n",
    "ValueError: Sentence boundary detection requires the dependency parse, which requires data to be installed. For more info, see the documentation: \n",
    "https://spacy.io/docs/usage\n",
    "\n",
    ">>> from spacy.en import English\n",
    ">>> nlp = English()\n",
    ">>> doc = nlp(raw_text)\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "NameError: name 'raw_text' is not defined\n",
    ">>> raw_text='Hello, world. Here are two sentences.'\n",
    ">>> doc = nlp(raw_text)\n",
    ">>> sentences = [sent.string.strip() for sent in doc.sents]\n",
    ">>> sentences\n",
    "['Hello, world.', 'Here are two sentences.']\n",
    ">>> model(raw_text)\n",
    "Hello, world. Here are two sentences.\n",
    ">>> docs=model(raw_text)\n",
    ">>> docs.sents\n",
    "<generator object at 0x14ad31948>\n",
    ">>> docs=model(raw_text)\n",
    ">>> sentences = [sent.string.strip() for sent in docs.sents]\n",
    ">>> sentences\n",
    "['Hello, world.', 'Here are two sentences.']\n",
    ">>> \n",
    "~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
