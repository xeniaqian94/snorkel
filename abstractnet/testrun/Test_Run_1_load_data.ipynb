{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run 1: Load AbstractNet Dataset: 70K unlabeled (and 2K labeled) Abstracts into DB\n",
    "\n",
    "This notebook loads the dataset and create labeled *candidates* through labeling function. Feel extra free to document/bring up any upcoming confusion throughout the test run, e.g. are the following two consistent, the **(segment, label) pair** that we want to have, and the **candidates** that we instruct snorkel to extract? \n",
    "\n",
    "Before everything, please ensure that you have followed project-level ``README.md`` and installed all python dependencies, e.g. ``tika``.  \n",
    "\n",
    "We filtered out null abstracts from `ClydeDB.csv` ([AbstractSegmentationCrowdNLP Git repo](https://github.com/zhoujieli/AbstractSegmentationCrowdNLP.git)), resulting in 48,914 valid ones out of 56,851 total abstracts. The 48,914 abstracts are saved to `data/70kpaper.tsv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we preprocess documents by parsing them into *contexts*. *Candidates* are extracted out of *contexts*, which are *instances* (one of the *background*, *mechanism*, *method*, and *findings*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# # Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 1000 #  60,000 for real dataset \n",
    "\n",
    "from snorkel.parser import TSVDocPreprocessor\n",
    "\n",
    "doc_preprocessor = TSVDocPreprocessor('data/70kpaper_061418_cleaned.tsv', encoding=\"utf-8\",max_docs=n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get statistics on the number of documents and sentences, as below. This could take 5-8 minutes to load ~60K papers (see progress bar, also might have exception). The following code parses docs into sentences by period, averaging 4.49 sentences per documents. Earlier I spent a few hours debugging some hidden formatting error that confuses Spacey. Need to ensure that we format raw data from .csv into .tsv *without* preceeding and appending quotes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 34.3 s, sys: 1.4 s, total: 35.7 s\n",
      "Wall time: 24.2 s\n",
      "Documents: 1000\n",
      "Sentences: 4487\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs)\n",
    "\n",
    "\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract `candidates` by defining the specific `CandidateExtractor` for abstract segmentation. We take *Background* as an example and come back with other segmentation parts, i.e., *mechanism*, *method*, *findings*, later. \n",
    "\n",
    "Some more explanation based on my understanding: \n",
    "    \n",
    "1. `Candidates` are defined as a class that contains 1+ `Span` objects within one `Sentence` context.  \n",
    "    \n",
    "2. `Span(s)` correspond to conceptual categories in text like people or disease names. \n",
    "    \n",
    "In the intro tutorial example, their `candidate` represents the possible `Spouse` mention `(Barrack Obama, Michelle Obama)`. As readers, we know this mention is true due to external knowledge and the keyword of `wedding` occuring later in the sentence. (Reference: (1) section `Writing a basic CandidateExtractor` in [Intro_tutorial_1](../intro/Intro_Tutorial_1.ipynb); (2) section `Candidate Member Functions and Variables` in [Workshop_1_Snorkel_API](../workshop/Workshop_1_Snorkel_API.ipynb)) \n",
    "\n",
    "\n",
    "+ Background: \n",
    "  - \"Recent research ... \", \n",
    "  - \"... have/has been widely ...\", \n",
    "  - \"How ... ?\" (and as the first sentence), \n",
    "  - \"Previous work...\", \n",
    "  - \"Motivated by...\", \n",
    "  - \"The success of ...\", etc.\n",
    "+ Mechanism:\n",
    "  - something\n",
    "  - some other pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `CandidateExtractor` as a wrapper of `CandidateSpace` (e.g. `Ngrams` is one type of `CandidateSpace`) and `Matcher` (e.g. `DictionaryMatcher`, `PersonMatcher`). Please make sure that `longest_match_only=True`, since this gets us longest span that contains dictionary words. (Reference: source code [candidates.py](../../snorkel/candidates.py) and [matchers.py](../../snorkel/matchers.py)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher,DictionaryMatch\n",
    "\n",
    "Background = candidate_subclass('Background', ['background_cue'])\n",
    "\n",
    "ngrams = Ngrams(n_max=30)\n",
    "# Start simple: any ngram that matches the dictionary are *background* candidates! \n",
    "dict_matcher=DictionaryMatch(d=['previous','motivated','recent','widely'],longest_match_only=True) \n",
    "cand_extractor=CandidateExtractor(Background, [ngrams], [dict_matcher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the defined `CandidateExtractor` to the all `Sentences` in the collection (splitted 90/10/10 for train/dev/test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[=                                       ] 0%[review]frank pfenning - 1989 - journal of symbolic logic 54 (1):288-289.   \n",
      "[\n",
      "False\n",
      "r\n",
      "False\n",
      "e\n",
      "False\n",
      "v\n",
      "False\n",
      "i\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document\n",
    "from util import number_of_people\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if number_of_people(s) <= 5:\n",
    "            if i % 10 == 8:\n",
    "                dev_sents.add(s)\n",
    "            elif i % 10 == 9:\n",
    "                test_sents.add(s)\n",
    "            else:\n",
    "                train_sents.add(s)\n",
    "                \n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    %time cand_extractor.apply(sents, split=i)\n",
    "    print(\"Number of candidates extracted:\", session.query(Background).filter(Background.split == i).count(),\"\\n\\n\")\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few of those extracted `Candidates`! Obviously, since we used `DictionaryMatcher`, all `Candidates` will contain at least one word from the set \\['previous','motivated','recent','widely'\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Background' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b9e8fcce421a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBackground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBackground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Candidate \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackground_cue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Background' is not defined"
     ]
    }
   ],
   "source": [
    "cands = session.query(Background).filter(Background.split == 0).all()\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Candidate \"+str(i)+\"/\"+str(len(cands))+\": \"+str(cands[i].background_cue))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info of sentence\n",
    "# for sent in (docs[0].sentences):\n",
    "#     print(sent.__dict__.keys())\n",
    "#     print(sent.text)\n",
    "#     print(sent.pos_tags)\n",
    "#     print(sent.ner_tags)\n",
    "    \n",
    "\n",
    "# sents=session.query(Sentence).order_by(Sentence.name).all()\n",
    "# print(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DictionaryMatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f918b54ee9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mngrams\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mNgrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdict_matcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionaryMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcand_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCandidateExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpouse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdict_matcher\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mDictionaryMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DictionaryMatch' is not defined"
     ]
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Background', ['background'])\n",
    "\n",
    "\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher\n",
    "\n",
    "ngrams         = Ngrams(n_max=30)\n",
    "dict_matcher = DictionaryMatch()\n",
    "cand_extractor = CandidateExtractor(Spouse, [ngrams], [dict_matcher])\n",
    "DictionaryMatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! \n",
    "\n",
    "Some debugging note at the very end (could ignore). \n",
    "\n",
    "```\n",
    "python -m spacy download en\n",
    "```\n",
    "\n",
    "Current issue: parser does not parse *by periods*. Sentence count is significantly fewer than expected! \n",
    "Potential fix: https://github.com/explosion/spaCy/issues/93\n",
    "\n",
    "======= Some more debugging log here (not necessary, could skip reading) ======\n",
    "~~~~\n",
    "Xins-MacBook-Pro:~ xin$ source activate snorkel\n",
    "(snorkel) Xins-MacBook-Pro:~ xin$ python\n",
    "Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 12:04:33) \n",
    "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import spacey\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "ModuleNotFoundError: No module named 'spacey'\n",
    ">>> import spacy\n",
    ">>> spacy.load('en')\n",
    "<spacy.en.English object at 0x1080e1da0>\n",
    ">>> model=spacy.load('en')\n",
    ">>> docs=model.tokenizer('Hello, world. Here are two sentences.')\n",
    ">>> for sent in docs.sents:\n",
    "...     pritn(sent.text)\n",
    "... \n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "  File \"spacy/tokens/doc.pyx\", line 439, in __get__ (spacy/tokens/doc.cpp:9808)\n",
    "ValueError: Sentence boundary detection requires the dependency parse, which requires data to be installed. For more info, see the documentation: \n",
    "https://spacy.io/docs/usage\n",
    "\n",
    ">>> for sent in docs.sents:\n",
    "...     print(sent.text)\n",
    "... \n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "  File \"spacy/tokens/doc.pyx\", line 439, in __get__ (spacy/tokens/doc.cpp:9808)\n",
    "ValueError: Sentence boundary detection requires the dependency parse, which requires data to be installed. For more info, see the documentation: \n",
    "https://spacy.io/docs/usage\n",
    "\n",
    ">>> from spacy.en import English\n",
    ">>> nlp = English()\n",
    ">>> doc = nlp(raw_text)\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "NameError: name 'raw_text' is not defined\n",
    ">>> raw_text='Hello, world. Here are two sentences.'\n",
    ">>> doc = nlp(raw_text)\n",
    ">>> sentences = [sent.string.strip() for sent in doc.sents]\n",
    ">>> sentences\n",
    "['Hello, world.', 'Here are two sentences.']\n",
    ">>> model(raw_text)\n",
    "Hello, world. Here are two sentences.\n",
    ">>> docs=model(raw_text)\n",
    ">>> docs.sents\n",
    "<generator object at 0x14ad31948>\n",
    ">>> docs=model(raw_text)\n",
    ">>> sentences = [sent.string.strip() for sent in docs.sents]\n",
    ">>> sentences\n",
    "['Hello, world.', 'Here are two sentences.']\n",
    ">>> \n",
    "~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
