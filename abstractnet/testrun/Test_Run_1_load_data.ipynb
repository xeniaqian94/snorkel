{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run 1: Load AbstractNet Dataset: 70K unlabeled (and 2K labeled) Abstracts into DB\n",
    "\n",
    "This notebook loads the dataset and create labeled *candidates* through labeling function. Before everything, please ensure that you have followed project-level ``README.md`` and installed all python dependencies, e.g. ``tika``.  \n",
    "\n",
    "We filtered out null abstracts from `ClydeDB.csv` ([AbstractSegmentationCrowdNLP Git repo](https://github.com/zhoujieli/AbstractSegmentationCrowdNLP.git)), resulting in 48,914 valid ones out of 56,851 total abstracts. The 48,914 abstracts are saved to `data/70kpaper.tsv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we preprocess documents by parsing them into *contexts*. *Candidates* are extracted out of *contexts*, which are *instances* (one of the *background*, *mechanism*, *method*, and *findings*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# # Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 1000 #  60,000 for real dataset \n",
    "\n",
    "from snorkel.parser import TSVDocPreprocessor\n",
    "\n",
    "doc_preprocessor = TSVDocPreprocessor('data/70kpaper_061418_cleaned.tsv', encoding=\"utf-8\",max_docs=n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get statistics on the number of documents and sentences, as below. This could take 5-8 minutes to load ~60K papers (see progress bar, also might have exception). The following code parses docs into sentences by period, averaging 4.49 sentences per documents. Earlier I spent a few hours debugging some hidden formatting error that confuses Spacey. Need to ensure that we format raw data from .csv into .tsv *without* preceeding and appending quotes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 32.8 s, sys: 1.27 s, total: 34 s\n",
      "Wall time: 22.9 s\n",
      "Documents: 1000\n",
      "Sentences: 4487\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs)\n",
    "\n",
    "\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract *candidates* by following a few patterns on abstract segmentation. We take *Background* as an example and come back with other segmentation parts later. \n",
    "+ Background: \n",
    "  - \"Recent research ... \", \n",
    "  - \"... have/has been widely ...\", \n",
    "  - \"How ... ?\" (and as the first sentence), \n",
    "  - \"Previous work...\", \n",
    "  - \"Motivated by...\", \n",
    "  - \"The success of ...\", etc.\n",
    "+ Mechanism:\n",
    "  - something\n",
    "  - some other pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_sa_instance_state', 'char_offsets', 'text', 'document_id', 'dep_labels', 'entity_types', 'stable_id', 'dep_parents', 'pos_tags', 'abs_char_offsets', 'words', 'position', 'id', 'entity_cids', 'type', 'ner_tags', 'lemmas', 'document'])\n",
      "\u0000\"\u0000A\u0000n\u0000 \u0000o\u0000b\u0000s\u0000e\u0000r\u0000v\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000 \u0000f\u0000o\u0000r\u0000 \u0000v\u0000i\u0000e\u0000w\u0000i\u0000n\u0000g\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000-\u0000s\u0000e\u0000n\u0000s\u0000i\u0000t\u0000i\u0000v\u0000e\u0000 \u0000t\u0000i\u0000s\u0000s\u0000u\u0000e\u0000 \u0000i\u0000n\u0000c\u0000l\u0000u\u0000d\u0000e\u0000s\u0000 \u0000a\u0000n\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000 \u0000c\u0000o\u0000n\u0000f\u0000i\u0000g\u0000u\u0000r\u0000e\u0000d\u0000 \u0000t\u0000o\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000e\u0000 \u0000t\u0000h\u0000e\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000-\u0000s\u0000e\u0000n\u0000s\u0000i\u0000t\u0000i\u0000v\u0000e\u0000 \u0000t\u0000i\u0000s\u0000s\u0000u\u0000e\u0000,\u0000 \u0000a\u0000n\u0000 \u0000i\u0000m\u0000a\u0000g\u0000i\u0000n\u0000g\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000 \u0000c\u0000o\u0000n\u0000f\u0000i\u0000g\u0000u\u0000r\u0000e\u0000d\u0000 \u0000t\u0000o\u0000 \u0000i\u0000m\u0000a\u0000g\u0000e\u0000 \u0000a\u0000t\u0000 \u0000l\u0000e\u0000a\u0000s\u0000t\u0000 \u0000a\u0000 \u0000p\u0000o\u0000r\u0000t\u0000i\u0000o\u0000n\u0000 \u0000o\u0000f\u0000 \u0000t\u0000h\u0000e\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000-\u0000s\u0000e\u0000n\u0000s\u0000i\u0000t\u0000i\u0000v\u0000e\u0000 \u0000t\u0000i\u0000s\u0000s\u0000u\u0000e\u0000 \u0000u\u0000p\u0000o\u0000n\u0000 \u0000b\u0000e\u0000i\u0000n\u0000g\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000e\u0000d\u0000 \u0000b\u0000y\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000,\u0000 \u0000a\u0000n\u0000d\u0000 \u0000a\u0000n\u0000 \u0000i\u0000m\u0000a\u0000g\u0000e\u0000 \u0000d\u0000i\u0000s\u0000p\u0000l\u0000a\u0000y\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000 \u0000i\u0000n\u0000 \u0000c\u0000o\u0000m\u0000m\u0000u\u0000n\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000m\u0000a\u0000g\u0000i\u0000n\u0000g\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000 \u0000t\u0000o\u0000 \u0000d\u0000i\u0000s\u0000p\u0000l\u0000a\u0000y\u0000 \u0000a\u0000n\u0000 \u0000i\u0000m\u0000a\u0000g\u0000e\u0000 \u0000o\u0000f\u0000 \u0000t\u0000h\u0000e\u0000 \u0000p\u0000o\u0000r\u0000t\u0000i\u0000o\u0000n\u0000 \u0000o\u0000f\u0000 \u0000t\u0000h\u0000e\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000-\u0000s\u0000e\u0000n\u0000s\u0000i\u0000t\u0000i\u0000v\u0000e\u0000 \u0000t\u0000i\u0000s\u0000s\u0000u\u0000e\u0000.\u0000 \u0000T\u0000h\u0000e\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000 \u0000i\u0000s\u0000 \u0000c\u0000o\u0000n\u0000f\u0000i\u0000g\u0000u\u0000r\u0000e\u0000d\u0000 \u0000t\u0000o\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000e\u0000 \u0000t\u0000h\u0000e\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000-\u0000s\u0000e\u0000n\u0000s\u0000i\u0000t\u0000i\u0000v\u0000e\u0000 \u0000t\u0000i\u0000s\u0000s\u0000u\u0000e\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000a\u0000 \u0000r\u0000e\u0000d\u0000u\u0000c\u0000e\u0000d\u0000\n",
      "['NN', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "dict_keys(['_sa_instance_state', 'char_offsets', 'text', 'document_id', 'dep_labels', 'entity_types', 'stable_id', 'dep_parents', 'pos_tags', 'abs_char_offsets', 'words', 'position', 'id', 'entity_cids', 'type', 'ner_tags', 'lemmas', 'document'])\n",
      "\u0000a\u0000m\u0000o\u0000u\u0000n\u0000t\u0000 \u0000o\u0000f\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000 \u0000w\u0000i\u0000t\u0000h\u0000i\u0000n\u0000 \u0000a\u0000 \u0000p\u0000r\u0000e\u0000s\u0000e\u0000l\u0000e\u0000c\u0000t\u0000e\u0000d\u0000 \u0000w\u0000a\u0000v\u0000e\u0000l\u0000e\u0000n\u0000g\u0000t\u0000h\u0000 \u0000\"\u0000\r\n",
      "['XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'SP']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document\n",
    "from util import number_of_people\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "for sent in (docs[0].sentences):\n",
    "    print(sent.__dict__.keys())\n",
    "    print(sent.text)\n",
    "    print(sent.pos_tags)\n",
    "    print(sent.ner_tags)\n",
    "    \n",
    "\n",
    "# sents=session.query(Sentence).order_by(Sentence.name).all()\n",
    "# print(sents[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Background', ['background'])\n",
    "\n",
    "\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import PersonMatcher\n",
    "\n",
    "ngrams         = Ngrams(n_max=30)\n",
    "dict_matcher = DictionaryMatch()\n",
    "cand_extractor = CandidateExtractor(Spouse, [ngrams], [dict_matcher])\n",
    "DictionaryMatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! \n",
    "\n",
    "Some debugging note at the very end (could ignore). \n",
    "\n",
    "```\n",
    "python -m spacy download en\n",
    "```\n",
    "\n",
    "Current issue: parser does not parse *by periods*. Sentence count is significantly fewer than expected! \n",
    "Potential fix: https://github.com/explosion/spaCy/issues/93\n",
    "\n",
    "======= Some more debugging log here (not necessary, could skip reading) ======\n",
    "~~~~\n",
    "Xins-MacBook-Pro:~ xin$ source activate snorkel\n",
    "(snorkel) Xins-MacBook-Pro:~ xin$ python\n",
    "Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 12:04:33) \n",
    "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import spacey\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "ModuleNotFoundError: No module named 'spacey'\n",
    ">>> import spacy\n",
    ">>> spacy.load('en')\n",
    "<spacy.en.English object at 0x1080e1da0>\n",
    ">>> model=spacy.load('en')\n",
    ">>> docs=model.tokenizer('Hello, world. Here are two sentences.')\n",
    ">>> for sent in docs.sents:\n",
    "...     pritn(sent.text)\n",
    "... \n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "  File \"spacy/tokens/doc.pyx\", line 439, in __get__ (spacy/tokens/doc.cpp:9808)\n",
    "ValueError: Sentence boundary detection requires the dependency parse, which requires data to be installed. For more info, see the documentation: \n",
    "https://spacy.io/docs/usage\n",
    "\n",
    ">>> for sent in docs.sents:\n",
    "...     print(sent.text)\n",
    "... \n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "  File \"spacy/tokens/doc.pyx\", line 439, in __get__ (spacy/tokens/doc.cpp:9808)\n",
    "ValueError: Sentence boundary detection requires the dependency parse, which requires data to be installed. For more info, see the documentation: \n",
    "https://spacy.io/docs/usage\n",
    "\n",
    ">>> from spacy.en import English\n",
    ">>> nlp = English()\n",
    ">>> doc = nlp(raw_text)\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "NameError: name 'raw_text' is not defined\n",
    ">>> raw_text='Hello, world. Here are two sentences.'\n",
    ">>> doc = nlp(raw_text)\n",
    ">>> sentences = [sent.string.strip() for sent in doc.sents]\n",
    ">>> sentences\n",
    "['Hello, world.', 'Here are two sentences.']\n",
    ">>> model(raw_text)\n",
    "Hello, world. Here are two sentences.\n",
    ">>> docs=model(raw_text)\n",
    ">>> docs.sents\n",
    "<generator object at 0x14ad31948>\n",
    ">>> docs=model(raw_text)\n",
    ">>> sentences = [sent.string.strip() for sent in docs.sents]\n",
    ">>> sentences\n",
    "['Hello, world.', 'Here are two sentences.']\n",
    ">>> \n",
    "~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
