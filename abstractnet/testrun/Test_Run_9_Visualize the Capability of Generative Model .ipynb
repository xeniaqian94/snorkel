{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run 9: Visualize the Capability of the Generative Model\n",
    "\n",
    "This notebook continues from test run 8, and\n",
    "\n",
    "1. visualize the current capability of the generative model for Purpose and Mechanism by their label distribution. \n",
    "\n",
    "Sometimes accessing the elements within a csr_LabelMatrix will throw exception. We don't know why yet. But we suggest to re-run from `L_train = labeler.apply(split=0)`, if that happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "debug_mode=1 # if not in debug mode, please set debug_mode=0\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Loading documents and train/dev/test segments. \n",
    "\n",
    "#### The following two celsl should load in 8014 papers if they haven't been loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TSVDocPreprocessor\n",
    "session = SnorkelSession()\n",
    "\n",
    "# # Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 9000 # this is the upper limit of number of docs\n",
    "doc_preprocessor = TSVDocPreprocessor('data/70kpaper_061418_cleaned_noBookLecture_10cscw_2k_order_preserving.tsv', encoding=\"utf-8\",max_docs=n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 8014\n",
      "Sentences: 24281\n",
      "The longest sentence has 563 tokens.\n",
      "dev_doc_set length 383\n",
      "dev_sents length 835 dev_doc_set size 383 avg n_sent per doc 2.180\n",
      "test_doc_set length 2071\n",
      "test_sents length 4566 test_doc_set size 2071 avg n_sent per doc 2.205\n"
     ]
    }
   ],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "session = SnorkelSession()\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "\n",
    "docs = session.query(Document).all()\n",
    "sents = session.query(Sentence).all()  # get all sentences from snorkel.db\n",
    "n_max_corpus=0\n",
    "for sent in sents:\n",
    "    n_max_corpus=max(n_max_corpus,len(sent.words))\n",
    "print(\"The longest sentence has \"+str(n_max_corpus)+\" tokens.\")\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "dev_doc_set = set()\n",
    "test_doc_set = set()\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name[:7]==\"2K_dev_\":\n",
    "            dev_sents.add(s)\n",
    "            dev_doc_set.add(doc.name)\n",
    "        elif doc.name[:8]==\"2K_test_\":\n",
    "            test_sents.add(s)\n",
    "            test_doc_set.add(doc.name)\n",
    "        else:\n",
    "            train_sents.add(s)\n",
    "print(\"dev_doc_set length\",len(dev_doc_set))\n",
    "print(\"dev_sents length\", len(dev_sents),\"dev_doc_set size\", len(dev_doc_set), \"avg n_sent per doc\",\"%.3f\"%(float(len(dev_sents))/float(len(dev_doc_set))))\n",
    "print(\"test_doc_set length\",len(test_doc_set))\n",
    "print(\"test_sents length\", len(test_sents),\"test_doc_set size\", len(test_doc_set), \"avg n_sent per doc\",\"%.3f\"%(float(len(test_sents))/float(len(test_doc_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18911\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import *\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "\n",
    "def extract_and_display(matcher,candidate_class,candidate_class_name,train_breakdown_map=None,dev_doc_breakdown_map=None,selected_split=0,is_print=True):  # split over train/dev/test but returns only train set\n",
    "#     input(candidate_class)\n",
    "    for (i, sents) in ([(0,train_sents), (1,dev_sents), (2,test_sents)] if selected_split==0 else ([(2,test_sents)] if selected_split==2 else [(1,dev_sents)])):\n",
    "        %time matcher.apply(sents, split=i)\n",
    "        printmd(\"**Split \"+str(i)+\" - number of candidates extracted: \"+str(session.query(candidate_class).filter(candidate_class.split == i).count())+\"**\\n\\n\")\n",
    "    train_cands = session.query(candidate_class).filter(candidate_class.split == selected_split).all()\n",
    "    if is_print:\n",
    "        for i in range(min(4,len(train_cands))): # to print at most 4 cands \n",
    "            printmd(\"**\"+str(i)+\"/\"+str(len(train_cands))+\" Candidate/Span:**\\t`\"+str(train_cands[i])+\"`\")\n",
    "            printmd(\"**Its parent Sentence's text:**\\t\"+str(train_cands[i].get_parent().text))\n",
    "            printmd(\"**Its parent Document's text:**\\t\"+str(train_cands[i].get_parent().get_parent().__dict__))\n",
    "            print() \n",
    "        \n",
    "    for cand in train_cands:\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in train_breakdown_map:\n",
    "            train_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in train_breakdown_map[doc_name]:\n",
    "            train_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        train_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "        \n",
    "    dev_cands = session.query(candidate_class).filter(candidate_class.split == 1).all()\n",
    "    for cand in dev_cands:\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in dev_doc_breakdown_map:\n",
    "            dev_doc_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in dev_doc_breakdown_map[doc_name]:\n",
    "            dev_doc_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        dev_doc_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "    test_cands=session.query(candidate_class).filter(candidate_class.split==2).all()\n",
    "    \n",
    "    return train_cands,dev_cands,test_cands\n",
    "\n",
    "Segment = candidate_subclass('Segment', ['segment_cue'])\n",
    "ngrams = Ngrams(n_max=n_max_corpus) \n",
    "non_tilde_matcher=DictionaryMatch(d=['~'],longest_match_only=True,reverse=True)  \n",
    "non_tilde_segment_extractor=CandidateExtractor(Segment, [ngrams], [non_tilde_matcher])\n",
    "train_doc_breakdown_map=dict()\n",
    "dev_doc_breakdown_map=dict()\n",
    "\n",
    "# train_segments,dev_segments,test_segments=extract_and_display(non_tilde_segment_extractor,Segment,\"Segment\",train_doc_breakdown_map,dev_doc_breakdown_map,is_print=False)\n",
    "train_segments=session.query(Segment).filter(Segment.split == 0).all()\n",
    "print(len(train_segments))\n",
    "dev_segments=session.query(Segment).filter(Segment.split == 1).all()\n",
    "test_segments=session.query(Segment).filter(Segment.split == 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 Evaluate with a small dev set of groundtruth\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Load gold labels in matrix format, from this file\n",
    "2. Evaluate LFs with this gold label matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels newly created: 0\n",
      "AnnotatorLabels newly created: 0\n",
      "CPU times: user 31.3 s, sys: 967 ms, total: 32.3 s\n",
      "Wall time: 34.2 s\n",
      "(3834, 1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# current_level=1\n",
    "# current_docid_prefix=\"2K_dev\"\n",
    "\n",
    "from util import load_groundtruth_as_external_dict\n",
    "from util import write_segment_name \n",
    "from util import load_external_labels\n",
    "from snorkel.annotations import load_gold_labels\n",
    "import re\n",
    "from snorkel.lf_helpers import *\n",
    "\n",
    "# groundtruth_dict=load_groundtruth_as_external_dict(\"data/annotations_label-level_all-to-date-2018-4-25-WithTitle.labelled.originalSegments_level_\"+str(current_level)+\".csv\",delimiter=\",\")\n",
    "# printmd(\"**Take a random example doc, what segments does it have<br /><br />** \"+str(groundtruth_dict[list(groundtruth_dict.keys())[10]]))\n",
    "\n",
    "purpose_gold_path=\"data/purpose_gold_dev_testrun_8.tsv\"                       \n",
    "# write_segment_name(dev_segments,purpose_gold_path,groundtruth_dict,segment_name=\"Purpose\")   \n",
    "%time external_purpose = load_external_labels(session, Segment, annotator_name='purpose_dev_gold',isPrint=False,file_path=purpose_gold_path)\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='purpose_dev_gold', split=1)\n",
    "print((L_gold_dev).shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate and tuning more LFs based on dev set \n",
    "\n",
    "Example:\n",
    "\n",
    "    def LF_toy(c):\n",
    "        return -1 if len(str(c))>10 else 0   # this heuristic has no actual meaning  \n",
    "        tp, fp, tn, fn = test_LF(session, LF_toy, split=1, annotator_name='purpose_dev',test_labels=L_gold_dev)\n",
    "        \n",
    "Output example: \n",
    "\n",
    "    test_candidates # 6006\n",
    "    test_marginals [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
    "    ========================================\n",
    "    Scores (Un-adjusted)\n",
    "    ========================================\n",
    "    Pos. class recall: 0.0\n",
    "    Neg. class recall: 1.0\n",
    "    Precision            0.0\n",
    "    Recall               0.0\n",
    "    F1                   0.0\n",
    "    ----------------------------------------\n",
    "    TP: 0 | FP: 0 | TN: 88 | FN: 5\n",
    "    ========================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading more realistic LFs in aggregation, train a generative model, evaluate the model on dev set\n",
    "\n",
    "1. `L_purpose_train.lf_stats(session)` reports coverage, overlaps, and conflicts, which are self-evaluation. \n",
    "\n",
    "2. Note the difference between `learned accuracy` and `empirical accuracy`.\n",
    "\n",
    "    <b>Our understanding based on code:</b>\n",
    "\n",
    "    1. Learned accuracy is evaluating each LF <b>w.r.t. trained generative model output as groundtruth</b>\n",
    "\n",
    "    2. Empirical accuracy is evaluating each LF <b>w.r.t. gold label</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_LFs 8\n",
      "purpose_LFs 74\n",
      "neg_for_purpose_LFs 112\n"
     ]
    }
   ],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "from LF.util_purpose_default import common_LFs,purpose_LFs,neg_for_purpose_LFs   # neg_LFs are reverse LFs for Background, Mechanism, Method, and Finding\n",
    "\n",
    "print(\"common_LFs\",len(common_LFs))\n",
    "print(\"purpose_LFs\",len(purpose_LFs))\n",
    "print(\"neg_for_purpose_LFs\",len(neg_for_purpose_LFs)) # 39 mechanism, 35 background, 24 findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also report <b>Total Coverage</b>: Coverage that corrected from Overlaps and Conflicts, essentially # candidates that are labelled by at least one LF / # total candidates. We also did a quick sanity check to ensure this number is calculated correctly, see [`data/calculate_total_coverage_toy_test_for_test_run_8.xlsx`](data/calculate_total_coverage_toy_test_for_test_run_8.xlsx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cids_count 3834\n",
      "key_group 0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 20s, sys: 702 ms, total: 1min 21s\n",
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Total non_overlapping_coverage (percentage of labelled over all)**  1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive segments in gold labels 860 out of 3834\n",
      "sum over rows shape (3834, 1)\n",
      "TP over all segments 761\n",
      "P among all segments 860\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Total empirical recall (percentage of at least once True Positives over Positives)**  0.8848837209302326"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py:214: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n",
      "  \", try using != instead.\", SparseEfficiencyWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Empirical Recall.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_common_s_however</th>\n",
       "      <td>0</td>\n",
       "      <td>0.014867</td>\n",
       "      <td>0.014867</td>\n",
       "      <td>0.012259</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>815</td>\n",
       "      <td>2957</td>\n",
       "      <td>0.784017</td>\n",
       "      <td>0.052326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_nevertheless</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>860</td>\n",
       "      <td>2966</td>\n",
       "      <td>0.774615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_despite</th>\n",
       "      <td>2</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>857</td>\n",
       "      <td>2959</td>\n",
       "      <td>0.773570</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_spite</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>859</td>\n",
       "      <td>2967</td>\n",
       "      <td>0.775137</td>\n",
       "      <td>0.001163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_yet</th>\n",
       "      <td>4</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>855</td>\n",
       "      <td>2955</td>\n",
       "      <td>0.773048</td>\n",
       "      <td>0.005814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_but</th>\n",
       "      <td>5</td>\n",
       "      <td>0.020866</td>\n",
       "      <td>0.020866</td>\n",
       "      <td>0.019823</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>850</td>\n",
       "      <td>2899</td>\n",
       "      <td>0.759728</td>\n",
       "      <td>0.011628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_but_sometimes</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_but_also</th>\n",
       "      <td>7</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_more_a_z_than</th>\n",
       "      <td>8</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>859</td>\n",
       "      <td>2959</td>\n",
       "      <td>0.773048</td>\n",
       "      <td>0.001163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_er_than</th>\n",
       "      <td>9</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>857</td>\n",
       "      <td>2923</td>\n",
       "      <td>0.764168</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_in_order_to</th>\n",
       "      <td>10</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>852</td>\n",
       "      <td>2960</td>\n",
       "      <td>0.775137</td>\n",
       "      <td>0.009302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_implication</th>\n",
       "      <td>11</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>860</td>\n",
       "      <td>2960</td>\n",
       "      <td>0.773048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_solve</th>\n",
       "      <td>12</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>855</td>\n",
       "      <td>2962</td>\n",
       "      <td>0.774876</td>\n",
       "      <td>0.005814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_hypothesis</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>860</td>\n",
       "      <td>2967</td>\n",
       "      <td>0.774876</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_enable</th>\n",
       "      <td>14</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>857</td>\n",
       "      <td>2967</td>\n",
       "      <td>0.775659</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_aid</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_produce</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>860</td>\n",
       "      <td>2967</td>\n",
       "      <td>0.774876</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_investigat</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>859</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775659</td>\n",
       "      <td>0.001163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_give</th>\n",
       "      <td>18</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>29</td>\n",
       "      <td>69</td>\n",
       "      <td>831</td>\n",
       "      <td>2900</td>\n",
       "      <td>0.764952</td>\n",
       "      <td>0.033721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_that_can</th>\n",
       "      <td>19</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>856</td>\n",
       "      <td>2954</td>\n",
       "      <td>0.772525</td>\n",
       "      <td>0.004651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_examine</th>\n",
       "      <td>20</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>857</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.774092</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_extend</th>\n",
       "      <td>21</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>857</td>\n",
       "      <td>2953</td>\n",
       "      <td>0.772003</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_offer</th>\n",
       "      <td>22</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>853</td>\n",
       "      <td>2954</td>\n",
       "      <td>0.773309</td>\n",
       "      <td>0.008140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_we_consider</th>\n",
       "      <td>23</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>850</td>\n",
       "      <td>2959</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.011628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_how_a_z_can</th>\n",
       "      <td>24</td>\n",
       "      <td>0.026604</td>\n",
       "      <td>0.026604</td>\n",
       "      <td>0.026343</td>\n",
       "      <td>26</td>\n",
       "      <td>76</td>\n",
       "      <td>834</td>\n",
       "      <td>2893</td>\n",
       "      <td>0.762340</td>\n",
       "      <td>0.030233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_we_study</th>\n",
       "      <td>25</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>843</td>\n",
       "      <td>2955</td>\n",
       "      <td>0.776182</td>\n",
       "      <td>0.019767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_we_are_interested_in</th>\n",
       "      <td>26</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>858</td>\n",
       "      <td>2966</td>\n",
       "      <td>0.775137</td>\n",
       "      <td>0.002326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_way_to</th>\n",
       "      <td>27</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>856</td>\n",
       "      <td>2962</td>\n",
       "      <td>0.774615</td>\n",
       "      <td>0.004651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_we_seek</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>858</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775921</td>\n",
       "      <td>0.002326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_initiate</th>\n",
       "      <td>29</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>858</td>\n",
       "      <td>2964</td>\n",
       "      <td>0.774615</td>\n",
       "      <td>0.002326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_method_evaluation</th>\n",
       "      <td>163</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_method_network</th>\n",
       "      <td>164</td>\n",
       "      <td>0.044340</td>\n",
       "      <td>0.044340</td>\n",
       "      <td>0.036776</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_we_a_z_show</th>\n",
       "      <td>165</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>0.026865</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_results</th>\n",
       "      <td>166</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_we_a_z_prove</th>\n",
       "      <td>167</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_this_uncovers</th>\n",
       "      <td>168</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_observe_s_that</th>\n",
       "      <td>169</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_we_a_z_report</th>\n",
       "      <td>170</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_our_findings</th>\n",
       "      <td>171</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_spots</th>\n",
       "      <td>172</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_our_results</th>\n",
       "      <td>173</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_result_s_on</th>\n",
       "      <td>174</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_we_a_z_demonstrate</th>\n",
       "      <td>175</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_we_a_z_find</th>\n",
       "      <td>176</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.018779</td>\n",
       "      <td>0.014345</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_finally</th>\n",
       "      <td>177</td>\n",
       "      <td>0.012259</td>\n",
       "      <td>0.012259</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_our_experiments</th>\n",
       "      <td>178</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_show_s_that</th>\n",
       "      <td>179</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>0.026604</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_our_a_z_result</th>\n",
       "      <td>180</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_indicate_s_that</th>\n",
       "      <td>181</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_yield</th>\n",
       "      <td>182</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_showcase</th>\n",
       "      <td>183</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_reveal_s_that</th>\n",
       "      <td>184</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_produce_s</th>\n",
       "      <td>185</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_we_a_z_provide</th>\n",
       "      <td>186</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_can_also</th>\n",
       "      <td>187</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_finding_we_a_z_discover</th>\n",
       "      <td>188</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_proper_finding_pos</th>\n",
       "      <td>189</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.129108</td>\n",
       "      <td>0.051382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_proper_method_pos</th>\n",
       "      <td>190</td>\n",
       "      <td>0.349765</td>\n",
       "      <td>0.349765</td>\n",
       "      <td>0.144497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_proper_mechanism_pos</th>\n",
       "      <td>191</td>\n",
       "      <td>0.349765</td>\n",
       "      <td>0.349765</td>\n",
       "      <td>0.144497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_proper_background_pos</th>\n",
       "      <td>192</td>\n",
       "      <td>0.538080</td>\n",
       "      <td>0.538080</td>\n",
       "      <td>0.538080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     j  Coverage  Overlaps  Conflicts  TP  FP  \\\n",
       "LF_common_s_however                  0  0.014867  0.014867   0.012259  45  12   \n",
       "LF_common_s_nevertheless             1  0.000782  0.000782   0.000522   0   3   \n",
       "LF_common_s_despite                  2  0.003391  0.003391   0.002608   3  10   \n",
       "LF_common_s_spite                    3  0.000782  0.000782   0.000782   1   2   \n",
       "LF_common_s_yet                      4  0.004956  0.004956   0.004173   5  14   \n",
       "LF_common_s_but                      5  0.020866  0.020866   0.019823  10  70   \n",
       "LF_common_but_sometimes              6  0.000261  0.000261   0.000261   0   0   \n",
       "LF_common_but_also                   7  0.001304  0.001304   0.001304   0   0   \n",
       "LF_purpose_more_a_z_than             8  0.002869  0.002869   0.002869   1  10   \n",
       "LF_purpose_er_than                   9  0.012780  0.012780   0.012780   3  46   \n",
       "LF_purpose_in_order_to              10  0.004434  0.004434   0.003912   8   9   \n",
       "LF_purpose_implication              11  0.002347  0.002347   0.002347   0   9   \n",
       "LF_purpose_to_solve                 12  0.003130  0.003130   0.003130   5   7   \n",
       "LF_purpose_hypothesis               13  0.000522  0.000522   0.000522   0   2   \n",
       "LF_purpose_to_enable                14  0.001304  0.001304   0.001043   3   2   \n",
       "LF_purpose_to_aid                   15  0.000000  0.000000   0.000000   0   0   \n",
       "LF_purpose_to_produce               16  0.000522  0.000522   0.000522   0   2   \n",
       "LF_purpose_to_investigat            17  0.000261  0.000261   0.000261   1   0   \n",
       "LF_purpose_give                     18  0.025561  0.025561   0.024517  29  69   \n",
       "LF_purpose_that_can                 19  0.004956  0.004956   0.004434   4  15   \n",
       "LF_purpose_examine                  20  0.002869  0.002869   0.002869   3   8   \n",
       "LF_purpose_extend                   21  0.004956  0.004956   0.004956   3  16   \n",
       "LF_purpose_offer                    22  0.005738  0.005738   0.005738   7  15   \n",
       "LF_purpose_we_consider              23  0.005216  0.005216   0.004956  10  10   \n",
       "LF_purpose_how_a_z_can              24  0.026604  0.026604   0.026343  26  76   \n",
       "LF_purpose_we_study                 25  0.008086  0.008086   0.007825  17  14   \n",
       "LF_purpose_we_are_interested_in     26  0.001304  0.001304   0.001304   2   3   \n",
       "LF_purpose_way_to                   27  0.002869  0.002869   0.002869   4   7   \n",
       "LF_purpose_we_seek                  28  0.000522  0.000522   0.000522   2   0   \n",
       "LF_purpose_initiate                 29  0.001826  0.001826   0.001826   2   5   \n",
       "...                                ...       ...       ...        ...  ..  ..   \n",
       "LF_neg_method_evaluation           163  0.002869  0.002869   0.002087   0   0   \n",
       "LF_neg_method_network              164  0.044340  0.044340   0.036776   0   0   \n",
       "LF_neg_finding_we_a_z_show         165  0.034950  0.034950   0.026865   0   0   \n",
       "LF_neg_finding_results             166  0.003130  0.003130   0.001304   0   0   \n",
       "LF_neg_finding_we_a_z_prove        167  0.009650  0.009650   0.007825   0   0   \n",
       "LF_neg_finding_this_uncovers       168  0.000261  0.000261   0.000000   0   0   \n",
       "LF_neg_finding_observe_s_that      169  0.001304  0.001304   0.000782   0   0   \n",
       "LF_neg_finding_we_a_z_report       170  0.002869  0.002869   0.001304   0   0   \n",
       "LF_neg_finding_our_findings        171  0.001043  0.001043   0.000782   0   0   \n",
       "LF_neg_finding_spots               172  0.002087  0.002087   0.000782   0   0   \n",
       "LF_neg_finding_our_results         173  0.002869  0.002869   0.002347   0   0   \n",
       "LF_neg_finding_result_s_on         174  0.001304  0.001304   0.001043   0   0   \n",
       "LF_neg_finding_we_a_z_demonstrate  175  0.005999  0.005999   0.003652   0   0   \n",
       "LF_neg_finding_we_a_z_find         176  0.018779  0.018779   0.014345   0   0   \n",
       "LF_neg_finding_finally             177  0.012259  0.012259   0.006260   0   0   \n",
       "LF_neg_finding_our_experiments     178  0.002608  0.002608   0.001565   0   0   \n",
       "LF_neg_finding_show_s_that         179  0.034950  0.034950   0.026604   0   0   \n",
       "LF_neg_finding_our_a_z_result      180  0.005477  0.005477   0.003391   0   0   \n",
       "LF_neg_finding_indicate_s_that     181  0.000782  0.000782   0.000522   0   0   \n",
       "LF_neg_finding_yield               182  0.004173  0.004173   0.003130   0   0   \n",
       "LF_neg_finding_showcase            183  0.000782  0.000782   0.000522   0   0   \n",
       "LF_neg_finding_reveal_s_that       184  0.000261  0.000261   0.000261   0   0   \n",
       "LF_neg_finding_produce_s           185  0.006260  0.006260   0.005216   0   0   \n",
       "LF_neg_finding_we_a_z_provide      186  0.010694  0.010694   0.007042   0   0   \n",
       "LF_neg_finding_can_also            187  0.001565  0.001565   0.001043   0   0   \n",
       "LF_neg_finding_we_a_z_discover     188  0.003130  0.003130   0.003130   0   0   \n",
       "neg_proper_finding_pos             189  0.129108  0.129108   0.051382   0   0   \n",
       "neg_proper_method_pos              190  0.349765  0.349765   0.144497   0   0   \n",
       "neg_proper_mechanism_pos           191  0.349765  0.349765   0.144497   0   0   \n",
       "neg_proper_background_pos          192  0.538080  0.538080   0.538080   0   0   \n",
       "\n",
       "                                    FN    TN  Empirical Acc.  \\\n",
       "LF_common_s_however                815  2957        0.784017   \n",
       "LF_common_s_nevertheless           860  2966        0.774615   \n",
       "LF_common_s_despite                857  2959        0.773570   \n",
       "LF_common_s_spite                  859  2967        0.775137   \n",
       "LF_common_s_yet                    855  2955        0.773048   \n",
       "LF_common_s_but                    850  2899        0.759728   \n",
       "LF_common_but_sometimes            860  2969        0.775398   \n",
       "LF_common_but_also                 860  2969        0.775398   \n",
       "LF_purpose_more_a_z_than           859  2959        0.773048   \n",
       "LF_purpose_er_than                 857  2923        0.764168   \n",
       "LF_purpose_in_order_to             852  2960        0.775137   \n",
       "LF_purpose_implication             860  2960        0.773048   \n",
       "LF_purpose_to_solve                855  2962        0.774876   \n",
       "LF_purpose_hypothesis              860  2967        0.774876   \n",
       "LF_purpose_to_enable               857  2967        0.775659   \n",
       "LF_purpose_to_aid                  860  2969        0.775398   \n",
       "LF_purpose_to_produce              860  2967        0.774876   \n",
       "LF_purpose_to_investigat           859  2969        0.775659   \n",
       "LF_purpose_give                    831  2900        0.764952   \n",
       "LF_purpose_that_can                856  2954        0.772525   \n",
       "LF_purpose_examine                 857  2961        0.774092   \n",
       "LF_purpose_extend                  857  2953        0.772003   \n",
       "LF_purpose_offer                   853  2954        0.773309   \n",
       "LF_purpose_we_consider             850  2959        0.775398   \n",
       "LF_purpose_how_a_z_can             834  2893        0.762340   \n",
       "LF_purpose_we_study                843  2955        0.776182   \n",
       "LF_purpose_we_are_interested_in    858  2966        0.775137   \n",
       "LF_purpose_way_to                  856  2962        0.774615   \n",
       "LF_purpose_we_seek                 858  2969        0.775921   \n",
       "LF_purpose_initiate                858  2964        0.774615   \n",
       "...                                ...   ...             ...   \n",
       "LF_neg_method_evaluation           860  2969        0.775398   \n",
       "LF_neg_method_network              860  2969        0.775398   \n",
       "LF_neg_finding_we_a_z_show         860  2969        0.775398   \n",
       "LF_neg_finding_results             860  2969        0.775398   \n",
       "LF_neg_finding_we_a_z_prove        860  2969        0.775398   \n",
       "LF_neg_finding_this_uncovers       860  2969        0.775398   \n",
       "LF_neg_finding_observe_s_that      860  2969        0.775398   \n",
       "LF_neg_finding_we_a_z_report       860  2969        0.775398   \n",
       "LF_neg_finding_our_findings        860  2969        0.775398   \n",
       "LF_neg_finding_spots               860  2969        0.775398   \n",
       "LF_neg_finding_our_results         860  2969        0.775398   \n",
       "LF_neg_finding_result_s_on         860  2969        0.775398   \n",
       "LF_neg_finding_we_a_z_demonstrate  860  2969        0.775398   \n",
       "LF_neg_finding_we_a_z_find         860  2969        0.775398   \n",
       "LF_neg_finding_finally             860  2969        0.775398   \n",
       "LF_neg_finding_our_experiments     860  2969        0.775398   \n",
       "LF_neg_finding_show_s_that         860  2969        0.775398   \n",
       "LF_neg_finding_our_a_z_result      860  2969        0.775398   \n",
       "LF_neg_finding_indicate_s_that     860  2969        0.775398   \n",
       "LF_neg_finding_yield               860  2969        0.775398   \n",
       "LF_neg_finding_showcase            860  2969        0.775398   \n",
       "LF_neg_finding_reveal_s_that       860  2969        0.775398   \n",
       "LF_neg_finding_produce_s           860  2969        0.775398   \n",
       "LF_neg_finding_we_a_z_provide      860  2969        0.775398   \n",
       "LF_neg_finding_can_also            860  2969        0.775398   \n",
       "LF_neg_finding_we_a_z_discover     860  2969        0.775398   \n",
       "neg_proper_finding_pos             860  2969        0.775398   \n",
       "neg_proper_method_pos              860  2969        0.775398   \n",
       "neg_proper_mechanism_pos           860  2969        0.775398   \n",
       "neg_proper_background_pos          860  2969        0.775398   \n",
       "\n",
       "                                   Empirical Recall.  \n",
       "LF_common_s_however                         0.052326  \n",
       "LF_common_s_nevertheless                    0.000000  \n",
       "LF_common_s_despite                         0.003488  \n",
       "LF_common_s_spite                           0.001163  \n",
       "LF_common_s_yet                             0.005814  \n",
       "LF_common_s_but                             0.011628  \n",
       "LF_common_but_sometimes                     0.000000  \n",
       "LF_common_but_also                          0.000000  \n",
       "LF_purpose_more_a_z_than                    0.001163  \n",
       "LF_purpose_er_than                          0.003488  \n",
       "LF_purpose_in_order_to                      0.009302  \n",
       "LF_purpose_implication                      0.000000  \n",
       "LF_purpose_to_solve                         0.005814  \n",
       "LF_purpose_hypothesis                       0.000000  \n",
       "LF_purpose_to_enable                        0.003488  \n",
       "LF_purpose_to_aid                           0.000000  \n",
       "LF_purpose_to_produce                       0.000000  \n",
       "LF_purpose_to_investigat                    0.001163  \n",
       "LF_purpose_give                             0.033721  \n",
       "LF_purpose_that_can                         0.004651  \n",
       "LF_purpose_examine                          0.003488  \n",
       "LF_purpose_extend                           0.003488  \n",
       "LF_purpose_offer                            0.008140  \n",
       "LF_purpose_we_consider                      0.011628  \n",
       "LF_purpose_how_a_z_can                      0.030233  \n",
       "LF_purpose_we_study                         0.019767  \n",
       "LF_purpose_we_are_interested_in             0.002326  \n",
       "LF_purpose_way_to                           0.004651  \n",
       "LF_purpose_we_seek                          0.002326  \n",
       "LF_purpose_initiate                         0.002326  \n",
       "...                                              ...  \n",
       "LF_neg_method_evaluation                    0.000000  \n",
       "LF_neg_method_network                       0.000000  \n",
       "LF_neg_finding_we_a_z_show                  0.000000  \n",
       "LF_neg_finding_results                      0.000000  \n",
       "LF_neg_finding_we_a_z_prove                 0.000000  \n",
       "LF_neg_finding_this_uncovers                0.000000  \n",
       "LF_neg_finding_observe_s_that               0.000000  \n",
       "LF_neg_finding_we_a_z_report                0.000000  \n",
       "LF_neg_finding_our_findings                 0.000000  \n",
       "LF_neg_finding_spots                        0.000000  \n",
       "LF_neg_finding_our_results                  0.000000  \n",
       "LF_neg_finding_result_s_on                  0.000000  \n",
       "LF_neg_finding_we_a_z_demonstrate           0.000000  \n",
       "LF_neg_finding_we_a_z_find                  0.000000  \n",
       "LF_neg_finding_finally                      0.000000  \n",
       "LF_neg_finding_our_experiments              0.000000  \n",
       "LF_neg_finding_show_s_that                  0.000000  \n",
       "LF_neg_finding_our_a_z_result               0.000000  \n",
       "LF_neg_finding_indicate_s_that              0.000000  \n",
       "LF_neg_finding_yield                        0.000000  \n",
       "LF_neg_finding_showcase                     0.000000  \n",
       "LF_neg_finding_reveal_s_that                0.000000  \n",
       "LF_neg_finding_produce_s                    0.000000  \n",
       "LF_neg_finding_we_a_z_provide               0.000000  \n",
       "LF_neg_finding_can_also                     0.000000  \n",
       "LF_neg_finding_we_a_z_discover              0.000000  \n",
       "neg_proper_finding_pos                      0.000000  \n",
       "neg_proper_method_pos                       0.000000  \n",
       "neg_proper_mechanism_pos                    0.000000  \n",
       "neg_proper_background_pos                   0.000000  \n",
       "\n",
       "[193 rows x 10 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "np.random.seed(1702)\n",
    "labeler = LabelAnnotator(lfs=common_LFs+purpose_LFs+neg_for_purpose_LFs)\n",
    "%time L_dev = labeler.apply(split=1)\n",
    "printmd(\"**Total non_overlapping_coverage (percentage of labelled over all)**  \"+str(L_dev.non_overlapping_coverage()))\n",
    "printmd(\"**Total empirical recall (percentage of at least once True Positives over Positives)**  \"+str(L_dev.total_empirical_recall(L_gold_dev)))\n",
    "L_dev.lf_stats(session, L_gold_dev,set_unlabeled_as_neg=True,csv_path=\"data/L_dev_purpose_for_coverage_most_recent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cids_count 18911\n",
      "key_group 0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6min 7s, sys: 1.02 s, total: 6min 8s\n",
      "Wall time: 6min 9s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Total non_overlapping_coverage on L_train (percentage of labelled over all)**  1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n",
      "Finished training generative model, now checking performance against development set labels...\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train\n",
    "printmd(\"**Total non_overlapping_coverage on L_train (percentage of labelled over all)**  \"+str(L_train.non_overlapping_coverage()))\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=0.0)\n",
    "\n",
    "print(\"Finished training generative model, now checking performance against development set labels...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cids_count 3834\n",
      "key_group 0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 51s, sys: 660 ms, total: 1min 52s\n",
      "Wall time: 1min 58s\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 0.551\n",
      "Neg. class recall: 0.676\n",
      "Precision            0.329\n",
      "Recall               0.551\n",
      "F1                   0.412\n",
      "----------------------------------------\n",
      "TP: 474 | FP: 965 | TN: 2009 | FN: 386\n",
      "========================================\n",
      "\n",
      "cardinality is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py:214: SparseEfficiencyWarning: Comparing a sparse matrix with 0 using == is inefficient, try using != instead.\n",
      "  \", try using != instead.\", SparseEfficiencyWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Empirical Recall.</th>\n",
       "      <th>Learned Metric.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_common_s_however</th>\n",
       "      <td>0</td>\n",
       "      <td>0.014867</td>\n",
       "      <td>0.009911</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>815</td>\n",
       "      <td>2957</td>\n",
       "      <td>0.784017</td>\n",
       "      <td>0.052326</td>\n",
       "      <td>0.570696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_nevertheless</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>860</td>\n",
       "      <td>2966</td>\n",
       "      <td>0.774615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_despite</th>\n",
       "      <td>2</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>857</td>\n",
       "      <td>2959</td>\n",
       "      <td>0.773570</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.556020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_spite</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>859</td>\n",
       "      <td>2967</td>\n",
       "      <td>0.775137</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.555422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_yet</th>\n",
       "      <td>4</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>855</td>\n",
       "      <td>2955</td>\n",
       "      <td>0.773048</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.574994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_s_but</th>\n",
       "      <td>5</td>\n",
       "      <td>0.020866</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.007303</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>850</td>\n",
       "      <td>2899</td>\n",
       "      <td>0.759728</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.563316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_but_sometimes</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.554277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_common_but_also</th>\n",
       "      <td>7</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_more_than</th>\n",
       "      <td>8</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>858</td>\n",
       "      <td>2941</td>\n",
       "      <td>0.768608</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.560194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_er_than</th>\n",
       "      <td>9</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>857</td>\n",
       "      <td>2923</td>\n",
       "      <td>0.764168</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.559683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_in_order_to</th>\n",
       "      <td>10</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>852</td>\n",
       "      <td>2960</td>\n",
       "      <td>0.775137</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.556691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_implication</th>\n",
       "      <td>11</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>860</td>\n",
       "      <td>2960</td>\n",
       "      <td>0.773048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_solve</th>\n",
       "      <td>12</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>855</td>\n",
       "      <td>2962</td>\n",
       "      <td>0.774876</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.565312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_hypothesis</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>860</td>\n",
       "      <td>2967</td>\n",
       "      <td>0.774876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_enable</th>\n",
       "      <td>14</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>857</td>\n",
       "      <td>2967</td>\n",
       "      <td>0.775659</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.557002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_aid</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_produce</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>860</td>\n",
       "      <td>2967</td>\n",
       "      <td>0.774876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_to_investigat</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>859</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775659</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.566346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_give</th>\n",
       "      <td>18</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>29</td>\n",
       "      <td>69</td>\n",
       "      <td>831</td>\n",
       "      <td>2900</td>\n",
       "      <td>0.764952</td>\n",
       "      <td>0.033721</td>\n",
       "      <td>0.558880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_that_can</th>\n",
       "      <td>19</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>856</td>\n",
       "      <td>2954</td>\n",
       "      <td>0.772525</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.571358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_examine</th>\n",
       "      <td>20</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>857</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.774092</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.564364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_extend</th>\n",
       "      <td>21</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>857</td>\n",
       "      <td>2953</td>\n",
       "      <td>0.772003</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.571222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_offer</th>\n",
       "      <td>22</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>853</td>\n",
       "      <td>2954</td>\n",
       "      <td>0.773309</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>0.559816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_we_consider</th>\n",
       "      <td>23</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>850</td>\n",
       "      <td>2959</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_how_a_z_can</th>\n",
       "      <td>24</td>\n",
       "      <td>0.026604</td>\n",
       "      <td>0.025039</td>\n",
       "      <td>0.009390</td>\n",
       "      <td>26</td>\n",
       "      <td>76</td>\n",
       "      <td>834</td>\n",
       "      <td>2893</td>\n",
       "      <td>0.762340</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>0.581424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_we_study</th>\n",
       "      <td>25</td>\n",
       "      <td>0.008086</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>843</td>\n",
       "      <td>2955</td>\n",
       "      <td>0.776182</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>0.555170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_we_are_interested_in</th>\n",
       "      <td>26</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>858</td>\n",
       "      <td>2966</td>\n",
       "      <td>0.775137</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.573851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_way_to</th>\n",
       "      <td>27</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>856</td>\n",
       "      <td>2962</td>\n",
       "      <td>0.774615</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.571565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_we_seek</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>858</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775921</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.562842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_initiate</th>\n",
       "      <td>29</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>858</td>\n",
       "      <td>2964</td>\n",
       "      <td>0.774615</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.559347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_s_contribute</th>\n",
       "      <td>89</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_s_build</th>\n",
       "      <td>90</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_s_built</th>\n",
       "      <td>91</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_s_design</th>\n",
       "      <td>92</td>\n",
       "      <td>0.032603</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>0.016693</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_give</th>\n",
       "      <td>93</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_formalize</th>\n",
       "      <td>94</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_establish</th>\n",
       "      <td>95</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.563722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_reason</th>\n",
       "      <td>96</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_show</th>\n",
       "      <td>97</td>\n",
       "      <td>0.031821</td>\n",
       "      <td>0.019823</td>\n",
       "      <td>0.016171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_identify</th>\n",
       "      <td>98</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_characterize</th>\n",
       "      <td>99</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_proceed_to</th>\n",
       "      <td>100</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_our_algorithm</th>\n",
       "      <td>101</td>\n",
       "      <td>0.007303</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_the_method</th>\n",
       "      <td>102</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_the_algorithm</th>\n",
       "      <td>103</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_in_our</th>\n",
       "      <td>104</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_our</th>\n",
       "      <td>105</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>0.016171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_provide</th>\n",
       "      <td>106</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_derive</th>\n",
       "      <td>107</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_illustrate</th>\n",
       "      <td>108</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_than_existing_algorithms</th>\n",
       "      <td>109</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_extend</th>\n",
       "      <td>110</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_using_a_z_0_1</th>\n",
       "      <td>111</td>\n",
       "      <td>0.032603</td>\n",
       "      <td>0.032603</td>\n",
       "      <td>0.014867</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_adopt</th>\n",
       "      <td>112</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_argue</th>\n",
       "      <td>113</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.567745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_generalize</th>\n",
       "      <td>114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_novelty_is</th>\n",
       "      <td>115</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_a_a_z_algorithm</th>\n",
       "      <td>116</td>\n",
       "      <td>0.052686</td>\n",
       "      <td>0.045123</td>\n",
       "      <td>0.021909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_a_a_z_mechanism</th>\n",
       "      <td>117</td>\n",
       "      <td>0.017736</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_neg_mechanism_we_also_demonstrate</th>\n",
       "      <td>118</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>2969</td>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             j  Coverage  Overlaps  Conflicts  \\\n",
       "LF_common_s_however                          0  0.014867  0.009911   0.002608   \n",
       "LF_common_s_nevertheless                     1  0.000782  0.000522   0.000261   \n",
       "LF_common_s_despite                          2  0.003391  0.002608   0.001043   \n",
       "LF_common_s_spite                            3  0.000782  0.000261   0.000000   \n",
       "LF_common_s_yet                              4  0.004956  0.003391   0.002087   \n",
       "LF_common_s_but                              5  0.020866  0.015389   0.007303   \n",
       "LF_common_but_sometimes                      6  0.000261  0.000261   0.000261   \n",
       "LF_common_but_also                           7  0.001304  0.001304   0.001304   \n",
       "LF_purpose_more_than                         8  0.007825  0.004434   0.002608   \n",
       "LF_purpose_er_than                           9  0.012780  0.008607   0.006260   \n",
       "LF_purpose_in_order_to                      10  0.004434  0.002608   0.001565   \n",
       "LF_purpose_implication                      11  0.002347  0.001565   0.000782   \n",
       "LF_purpose_to_solve                         12  0.003130  0.003130   0.001043   \n",
       "LF_purpose_hypothesis                       13  0.000522  0.000522   0.000261   \n",
       "LF_purpose_to_enable                        14  0.001304  0.001304   0.000522   \n",
       "LF_purpose_to_aid                           15  0.000000  0.000000   0.000000   \n",
       "LF_purpose_to_produce                       16  0.000522  0.000261   0.000000   \n",
       "LF_purpose_to_investigat                    17  0.000261  0.000000   0.000000   \n",
       "LF_purpose_give                             18  0.025561  0.019040   0.010694   \n",
       "LF_purpose_that_can                         19  0.004956  0.003391   0.002347   \n",
       "LF_purpose_examine                          20  0.002869  0.001304   0.000522   \n",
       "LF_purpose_extend                           21  0.004956  0.004173   0.003652   \n",
       "LF_purpose_offer                            22  0.005738  0.003130   0.001826   \n",
       "LF_purpose_we_consider                      23  0.005216  0.002608   0.001304   \n",
       "LF_purpose_how_a_z_can                      24  0.026604  0.025039   0.009390   \n",
       "LF_purpose_we_study                         25  0.008086  0.004695   0.002347   \n",
       "LF_purpose_we_are_interested_in             26  0.001304  0.000782   0.000522   \n",
       "LF_purpose_way_to                           27  0.002869  0.002347   0.001043   \n",
       "LF_purpose_we_seek                          28  0.000522  0.000261   0.000261   \n",
       "LF_purpose_initiate                         29  0.001826  0.001826   0.001304   \n",
       "...                                        ...       ...       ...        ...   \n",
       "LF_neg_mechanism_s_contribute               89  0.000522  0.000261   0.000261   \n",
       "LF_neg_mechanism_s_build                    90  0.005216  0.004173   0.002869   \n",
       "LF_neg_mechanism_s_built                    91  0.001304  0.001043   0.001043   \n",
       "LF_neg_mechanism_s_design                   92  0.032603  0.025561   0.016693   \n",
       "LF_neg_mechanism_we_also_give               93  0.002869  0.002869   0.002869   \n",
       "LF_neg_mechanism_we_also_formalize          94  0.000522  0.000522   0.000522   \n",
       "LF_neg_mechanism_we_also_establish          95  0.002347  0.001565   0.001304   \n",
       "LF_neg_mechanism_we_also_reason             96  0.000261  0.000000   0.000000   \n",
       "LF_neg_mechanism_we_also_show               97  0.031821  0.019823   0.016171   \n",
       "LF_neg_mechanism_we_also_identify           98  0.001565  0.001304   0.001043   \n",
       "LF_neg_mechanism_we_also_characterize       99  0.000782  0.000522   0.000522   \n",
       "LF_neg_mechanism_we_also_proceed_to        100  0.000782  0.000782   0.000522   \n",
       "LF_neg_mechanism_our_algorithm             101  0.007303  0.005216   0.002347   \n",
       "LF_neg_mechanism_the_method                102  0.002608  0.002087   0.001826   \n",
       "LF_neg_mechanism_the_algorithm             103  0.004173  0.003652   0.002869   \n",
       "LF_neg_mechanism_in_our                    104  0.007042  0.004173   0.003652   \n",
       "LF_neg_mechanism_our                       105  0.038080  0.025561   0.016171   \n",
       "LF_neg_mechanism_we_also_provide           106  0.008346  0.003912   0.003130   \n",
       "LF_neg_mechanism_we_also_derive            107  0.003391  0.002608   0.001826   \n",
       "LF_neg_mechanism_we_also_illustrate        108  0.001043  0.000782   0.000782   \n",
       "LF_neg_mechanism_than_existing_algorithms  109  0.000261  0.000261   0.000261   \n",
       "LF_neg_mechanism_we_also_extend            110  0.001565  0.001565   0.001565   \n",
       "LF_neg_mechanism_using_a_z_0_1             111  0.032603  0.032603   0.014867   \n",
       "LF_neg_mechanism_we_also_adopt             112  0.001043  0.000522   0.000000   \n",
       "LF_neg_mechanism_we_also_argue             113  0.000782  0.000782   0.000782   \n",
       "LF_neg_mechanism_we_also_generalize        114  0.000000  0.000000   0.000000   \n",
       "LF_neg_mechanism_novelty_is                115  0.000261  0.000261   0.000261   \n",
       "LF_neg_mechanism_a_a_z_algorithm           116  0.052686  0.045123   0.021909   \n",
       "LF_neg_mechanism_a_a_z_mechanism           117  0.017736  0.015128   0.008607   \n",
       "LF_neg_mechanism_we_also_demonstrate       118  0.005477  0.002869   0.001304   \n",
       "\n",
       "                                           TP  FP   FN    TN  Empirical Acc.  \\\n",
       "LF_common_s_however                        45  12  815  2957        0.784017   \n",
       "LF_common_s_nevertheless                    0   3  860  2966        0.774615   \n",
       "LF_common_s_despite                         3  10  857  2959        0.773570   \n",
       "LF_common_s_spite                           1   2  859  2967        0.775137   \n",
       "LF_common_s_yet                             5  14  855  2955        0.773048   \n",
       "LF_common_s_but                            10  70  850  2899        0.759728   \n",
       "LF_common_but_sometimes                     0   0  860  2969        0.775398   \n",
       "LF_common_but_also                          0   0  860  2969        0.775398   \n",
       "LF_purpose_more_than                        2  28  858  2941        0.768608   \n",
       "LF_purpose_er_than                          3  46  857  2923        0.764168   \n",
       "LF_purpose_in_order_to                      8   9  852  2960        0.775137   \n",
       "LF_purpose_implication                      0   9  860  2960        0.773048   \n",
       "LF_purpose_to_solve                         5   7  855  2962        0.774876   \n",
       "LF_purpose_hypothesis                       0   2  860  2967        0.774876   \n",
       "LF_purpose_to_enable                        3   2  857  2967        0.775659   \n",
       "LF_purpose_to_aid                           0   0  860  2969        0.775398   \n",
       "LF_purpose_to_produce                       0   2  860  2967        0.774876   \n",
       "LF_purpose_to_investigat                    1   0  859  2969        0.775659   \n",
       "LF_purpose_give                            29  69  831  2900        0.764952   \n",
       "LF_purpose_that_can                         4  15  856  2954        0.772525   \n",
       "LF_purpose_examine                          3   8  857  2961        0.774092   \n",
       "LF_purpose_extend                           3  16  857  2953        0.772003   \n",
       "LF_purpose_offer                            7  15  853  2954        0.773309   \n",
       "LF_purpose_we_consider                     10  10  850  2959        0.775398   \n",
       "LF_purpose_how_a_z_can                     26  76  834  2893        0.762340   \n",
       "LF_purpose_we_study                        17  14  843  2955        0.776182   \n",
       "LF_purpose_we_are_interested_in             2   3  858  2966        0.775137   \n",
       "LF_purpose_way_to                           4   7  856  2962        0.774615   \n",
       "LF_purpose_we_seek                          2   0  858  2969        0.775921   \n",
       "LF_purpose_initiate                         2   5  858  2964        0.774615   \n",
       "...                                        ..  ..  ...   ...             ...   \n",
       "LF_neg_mechanism_s_contribute               0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_s_build                    0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_s_built                    0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_s_design                   0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_give               0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_formalize          0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_establish          0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_reason             0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_show               0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_identify           0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_characterize       0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_proceed_to         0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_our_algorithm              0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_the_method                 0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_the_algorithm              0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_in_our                     0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_our                        0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_provide            0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_derive             0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_illustrate         0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_than_existing_algorithms   0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_extend             0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_using_a_z_0_1              0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_adopt              0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_argue              0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_generalize         0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_novelty_is                 0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_a_a_z_algorithm            0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_a_a_z_mechanism            0   0  860  2969        0.775398   \n",
       "LF_neg_mechanism_we_also_demonstrate        0   0  860  2969        0.775398   \n",
       "\n",
       "                                           Empirical Recall.  Learned Metric.  \n",
       "LF_common_s_however                                 0.052326         0.570696  \n",
       "LF_common_s_nevertheless                            0.000000         0.558961  \n",
       "LF_common_s_despite                                 0.003488         0.556020  \n",
       "LF_common_s_spite                                   0.001163         0.555422  \n",
       "LF_common_s_yet                                     0.005814         0.574994  \n",
       "LF_common_s_but                                     0.011628         0.563316  \n",
       "LF_common_but_sometimes                             0.000000         0.554277  \n",
       "LF_common_but_also                                  0.000000         0.567547  \n",
       "LF_purpose_more_than                                0.002326         0.560194  \n",
       "LF_purpose_er_than                                  0.003488         0.559683  \n",
       "LF_purpose_in_order_to                              0.009302         0.556691  \n",
       "LF_purpose_implication                              0.000000         0.568306  \n",
       "LF_purpose_to_solve                                 0.005814         0.565312  \n",
       "LF_purpose_hypothesis                               0.000000         0.586215  \n",
       "LF_purpose_to_enable                                0.003488         0.557002  \n",
       "LF_purpose_to_aid                                   0.000000         0.564399  \n",
       "LF_purpose_to_produce                               0.000000         0.564644  \n",
       "LF_purpose_to_investigat                            0.001163         0.566346  \n",
       "LF_purpose_give                                     0.033721         0.558880  \n",
       "LF_purpose_that_can                                 0.004651         0.571358  \n",
       "LF_purpose_examine                                  0.003488         0.564364  \n",
       "LF_purpose_extend                                   0.003488         0.571222  \n",
       "LF_purpose_offer                                    0.008140         0.559816  \n",
       "LF_purpose_we_consider                              0.011628         0.562500  \n",
       "LF_purpose_how_a_z_can                              0.030233         0.581424  \n",
       "LF_purpose_we_study                                 0.019767         0.555170  \n",
       "LF_purpose_we_are_interested_in                     0.002326         0.573851  \n",
       "LF_purpose_way_to                                   0.004651         0.571565  \n",
       "LF_purpose_we_seek                                  0.002326         0.562842  \n",
       "LF_purpose_initiate                                 0.002326         0.559347  \n",
       "...                                                      ...              ...  \n",
       "LF_neg_mechanism_s_contribute                       0.000000         0.575098  \n",
       "LF_neg_mechanism_s_build                            0.000000         0.550439  \n",
       "LF_neg_mechanism_s_built                            0.000000         0.570045  \n",
       "LF_neg_mechanism_s_design                           0.000000         0.573623  \n",
       "LF_neg_mechanism_we_also_give                       0.000000         0.574333  \n",
       "LF_neg_mechanism_we_also_formalize                  0.000000         0.558525  \n",
       "LF_neg_mechanism_we_also_establish                  0.000000         0.563722  \n",
       "LF_neg_mechanism_we_also_reason                     0.000000         0.553228  \n",
       "LF_neg_mechanism_we_also_show                       0.000000         0.552308  \n",
       "LF_neg_mechanism_we_also_identify                   0.000000         0.566838  \n",
       "LF_neg_mechanism_we_also_characterize               0.000000         0.552381  \n",
       "LF_neg_mechanism_we_also_proceed_to                 0.000000         0.568318  \n",
       "LF_neg_mechanism_our_algorithm                      0.000000         0.573280  \n",
       "LF_neg_mechanism_the_method                         0.000000         0.562485  \n",
       "LF_neg_mechanism_the_algorithm                      0.000000         0.569008  \n",
       "LF_neg_mechanism_in_our                             0.000000         0.577420  \n",
       "LF_neg_mechanism_our                                0.000000         0.556859  \n",
       "LF_neg_mechanism_we_also_provide                    0.000000         0.566600  \n",
       "LF_neg_mechanism_we_also_derive                     0.000000         0.562392  \n",
       "LF_neg_mechanism_we_also_illustrate                 0.000000         0.561809  \n",
       "LF_neg_mechanism_than_existing_algorithms           0.000000         0.581583  \n",
       "LF_neg_mechanism_we_also_extend                     0.000000         0.562100  \n",
       "LF_neg_mechanism_using_a_z_0_1                      0.000000         0.576675  \n",
       "LF_neg_mechanism_we_also_adopt                      0.000000         0.562028  \n",
       "LF_neg_mechanism_we_also_argue                      0.000000         0.567745  \n",
       "LF_neg_mechanism_we_also_generalize                 0.000000         0.560722  \n",
       "LF_neg_mechanism_novelty_is                         0.000000         0.571151  \n",
       "LF_neg_mechanism_a_a_z_algorithm                    0.000000         0.584652  \n",
       "LF_neg_mechanism_a_a_z_mechanism                    0.000000         0.570721  \n",
       "LF_neg_mechanism_we_also_demonstrate                0.000000         0.564840  \n",
       "\n",
       "[119 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "%time L_dev = labeler.apply_existing(split=1)\n",
    "\n",
    "_ = gen_model.error_analysis(session, L_dev, L_gold_dev,set_unlabeled_as_neg=True)\n",
    "\n",
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'],set_unlabeled_as_neg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_marginals = gen_model.marginals(L_train)  # the marginal probability of each candidate being True\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=50)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18911\n"
     ]
    }
   ],
   "source": [
    "from util import get_candidate_text\n",
    "train_marginals_list=list(train_marginals)\n",
    "# print(train_marginals_list)\n",
    "# print(train_marginals)\n",
    "\n",
    "print(len(train_segments))\n",
    "# print(train_marginals_list)\n",
    "zipped=zip(train_segments,train_marginals_list)\n",
    "pairs=[pair for pair in zipped]\n",
    "# print(\"above 0.15 has \"+str(len([pair for pair in pairs if pair[1]>=0.4])))\n",
    "sorted_pairs=sorted(pairs,key=lambda x: x[1],reverse=True)\n",
    "\n",
    "f_write=open(\"data/ranked_high_to_low_generative_purpose\",\"w\")\n",
    "for element in sorted_pairs:\n",
    "    try:\n",
    "        f_write.write(get_candidate_text(element[0]).strip()+\"\\t\"+str(element[1])+\"\\n\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Total non_overlapping_coverage on L_train (percentage of labelled over all)**  1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFKJJREFUeJzt3X+M3Hd95/HnmxgCjSE2BFaR7avTq9sjYApkm0RCOq2TXmKSK87pksooBQe5Z12bo5WaqoRyKBwkwtydmxaV0rokwtBrl1xaFF8SilzjKXLVQAgJMUmOxiQWmERJix236wB3zr3vj/ksTJZZ78x4Zva7+Twf0mrn+/l+ZuY14/W+5vtjZiMzkSTV50WLHUCStDgsAEmqlAUgSZWyACSpUhaAJFXKApCkSlkAklQpC0CSKmUBSFKlli12gJM566yzcu3atQNd9/jx45xxxhnDDTQE5updEzOBufrVxFxNzATDy3Xffff9Y2a+esGJmdnYr/POOy8HtW/fvoGvO0rm6l0TM2Waq19NzNXETJnDywV8JXv4HesuIEmqlAUgSZWyACSpUhaAJFXKApCkSlkAklQpC0CSKmUBSFKlLABJqlSjPwriVK29/q6u44e2Xz7mJJLUPG4BSFKlLABJqpQFIEmV6qkAIuJQRByIiAci4itl7JURsSciHi3fV5bxiIiPRsTBiHgwIt7ccTtbyvxHI2LLaB6SJKkX/WwBbMjMN2bmZFm+HtibmeuAvWUZ4K3AuvK1Dfg4tAsDuAG4ADgfuGG2NCRJ43cqu4A2AbvK5V3AFR3jnyofS30PsCIizgYuBfZk5pHMPArsATaewv1Lkk5BtP92wAKTIh4HjgIJ/HFm7oyIZzJzRceco5m5MiLuBLZn5v4yvhd4DzAFvDQzbyzj7we+l5n/fc59baO95cDExMR509PTAz2wmZkZHj/2XNd161edOdBtDsPMzAzLly9ftPufTxNzNTETmKtfTczVxEwwvFwbNmy4r2Nvzbx6fR/AWzLziYh4DbAnIv73SeZGl7E8yfjzBzJ3AjsBJicnc2pqqseIz9dqtdix/3jXdYeuHuw2h6HVajHoYxqlJuZqYiYwV7+amKuJmWD8uXraBZSZT5TvTwOfpb0P/6mya4fy/eky/TCwpuPqq4EnTjIuSVoECxZARJwRES+fvQxcAnwd2A3MnsmzBbijXN4NvLOcDXQhcCwznwQ+D1wSESvLwd9LypgkaRH0sgtoAvhsRMzO/7PM/KuIuBe4LSK2At8Crirz7wYuAw4CzwLvAsjMIxHxIeDeMu+DmXlkaI9EktSXBQsgMx8Dfq7L+HeBi7uMJ3DtPLd1K3Br/zElScPmO4ElqVIWgCRVygKQpEpZAJJUKQtAkiplAUhSpSwASaqUBSBJlbIAJKlSFoAkVcoCkKRKWQCSVCkLQJIqZQFIUqUsAEmqlAUgSZWyACSpUhaAJFXKApCkSlkAklQpC0CSKmUBSFKlLABJqpQFIEmVsgAkqVIWgCRVygKQpEpZAJJUKQtAkiplAUhSpSwASapUzwUQEadFxP0RcWdZPicivhQRj0bEZyLiJWX89LJ8sKxf23Eb7y3j34iIS4f9YCRJvetnC+A3gEc6lj8C3JyZ64CjwNYyvhU4mpk/Ddxc5hER5wKbgdcBG4E/jIjTTi2+JGlQPRVARKwGLgc+UZYDuAi4vUzZBVxRLm8qy5T1F5f5m4DpzPxBZj4OHATOH8aDkCT1LzJz4UkRtwMfBl4O/BZwDXBPeZVPRKwBPpeZr4+IrwMbM/NwWfdN4ALgA+U6f1rGbynXuX3OfW0DtgFMTEycNz09PdADm5mZ4fFjz3Vdt37VmQPd5jDMzMywfPnyRbv/+TQxVxMzgbn61cRcTcwEw8u1YcOG+zJzcqF5yxaaEBH/Fng6M++LiKnZ4S5Tc4F1J7vOjwYydwI7ASYnJ3NqamrulJ60Wi127D/edd2hqwe7zWFotVoM+phGqYm5mpgJzNWvJuZqYiYYf64FCwB4C/C2iLgMeCnwCuD3gBURsSwzTwCrgSfK/MPAGuBwRCwDzgSOdIzP6ryOJGnMFjwGkJnvzczVmbmW9kHcL2Tm1cA+4MoybQtwR7m8uyxT1n8h2/uZdgOby1lC5wDrgC8P7ZFIkvrSyxbAfN4DTEfEjcD9wC1l/Bbg0xFxkPYr/80AmflQRNwGPAycAK7NzO476SVJI9dXAWRmC2iVy4/R5SyezPw+cNU8178JuKnfkJKk4fOdwJJUKQtAkiplAUhSpSwASaqUBSBJlbIAJKlSFoAkVcoCkKRKWQCSVCkLQJIqZQFIUqUsAEmqlAUgSZWyACSpUhaAJFXKApCkSlkAklQpC0CSKmUBSFKlLABJqpQFIEmVsgAkqVIWgCRVygKQpEpZAJJUKQtAkiplAUhSpSwASaqUBSBJlbIAJKlSCxZARLw0Ir4cEV+LiIci4r+U8XMi4ksR8WhEfCYiXlLGTy/LB8v6tR239d4y/o2IuHRUD0qStLBetgB+AFyUmT8HvBHYGBEXAh8Bbs7MdcBRYGuZvxU4mpk/Ddxc5hER5wKbgdcBG4E/jIjThvlgJEm9W7AAsm2mLL64fCVwEXB7Gd8FXFEubyrLlPUXR0SU8enM/EFmPg4cBM4fyqOQJPWtp2MAEXFaRDwAPA3sAb4JPJOZJ8qUw8CqcnkV8G2Asv4Y8KrO8S7XkSSN2bJeJmXmc8AbI2IF8Fngtd2mle8xz7r5xp8nIrYB2wAmJiZotVq9RPwxMzMzXLf+ua7rBr3NYZiZmVnU+59PE3M1MROYq19NzNXETDD+XD0VwKzMfCYiWsCFwIqIWFZe5a8GnijTDgNrgMMRsQw4EzjSMT6r8zqd97ET2AkwOTmZU1NT/UT8oVarxY79x7uuO3T1YLc5DK1Wi0Ef0yg1MVcTM4G5+tXEXE3MBOPP1ctZQK8ur/yJiJcBvwA8AuwDrizTtgB3lMu7yzJl/RcyM8v45nKW0DnAOuDLw3ogkqT+9LIFcDawq5yx8yLgtsy8MyIeBqYj4kbgfuCWMv8W4NMRcZD2K//NAJn5UETcBjwMnACuLbuWJEmLYMECyMwHgTd1GX+MLmfxZOb3gavmua2bgJv6jylJGjbfCSxJlbIAJKlSFoAkVcoCkKRKWQCSVCkLQJIqZQFIUqUsAEmqlAUgSZWyACSpUhaAJFXKApCkSlkAklQpC0CSKmUBSFKlLABJqpQFIEmVsgAkqVIWgCRVygKQpEpZAJJUKQtAkiplAUhSpSwASarUssUOIDXd2uvvet7ydetPcM31d3Fo++WLlEgaDrcAJKlSFoAkVcoCkKRKWQCSVCkLQJIqZQFIUqUWLICIWBMR+yLikYh4KCJ+o4y/MiL2RMSj5fvKMh4R8dGIOBgRD0bEmztua0uZ/2hEbBndw5IkLaSXLYATwHWZ+VrgQuDaiDgXuB7Ym5nrgL1lGeCtwLrytQ34OLQLA7gBuAA4H7hhtjQkSeO3YAFk5pOZ+dVy+Z+BR4BVwCZgV5m2C7iiXN4EfCrb7gFWRMTZwKXAnsw8kplHgT3AxqE+GklSz/o6BhARa4E3AV8CJjLzSWiXBPCaMm0V8O2Oqx0uY/ONS5IWQWRmbxMjlgN/A9yUmX8ZEc9k5oqO9Uczc2VE3AV8ODP3l/G9wG8DFwGnZ+aNZfz9wLOZuWPO/WyjveuIiYmJ86anpwd6YDMzMzx+7Lmu69avOnOg2xyGmZkZli9fvmj3P58m5mpKpgPfOfa85YmXwVPfW9yfo26a8nzN1cRcTcwEw8u1YcOG+zJzcqF5PX0WUES8GPgL4H9k5l+W4aci4uzMfLLs4nm6jB8G1nRcfTXwRBmfmjPemntfmbkT2AkwOTmZU1NTc6f0pNVqsWP/8a7rDl092G0OQ6vVYtDHNEpNzNWUTNd0+SygHQeWLerPUTdNeb7mamKuJmaC8efq5SygAG4BHsnM3+1YtRuYPZNnC3BHx/g7y9lAFwLHyi6izwOXRMTKcvD3kjImSVoEvWwBvAV4B3AgIh4oY78DbAdui4itwLeAq8q6u4HLgIPAs8C7ADLzSER8CLi3zPtgZh4ZyqOQJPVtwQIo+/JjntUXd5mfwLXz3NatwK39BJQkjYbvBJakSlkAklQpC0CSKmUBSFKlLABJqpQFIEmVsgAkqVIWgCRVygKQpEr19GFwtVg750O/Zh3afvmYk0jS6LkFIEmVsgAkqVIWgCRVygKQpEpVeRB4voO9klQTtwAkqVIWgCRVygKQpEpVeQxAWgp8Y6JGzS0ASaqUBSBJlbIAJKlSFoAkVcoCkKRKWQCSVCkLQJIqZQFIUqUsAEmqlAUgSZWyACSpUhaAJFVqwQKIiFsj4umI+HrH2CsjYk9EPFq+ryzjEREfjYiDEfFgRLy54zpbyvxHI2LLaB6OJKlXvWwBfBLYOGfsemBvZq4D9pZlgLcC68rXNuDj0C4M4AbgAuB84IbZ0pAkLY4FCyAzvwgcmTO8CdhVLu8CrugY/1S23QOsiIizgUuBPZl5JDOPAnv48VKRJI3RoMcAJjLzSYDy/TVlfBXw7Y55h8vYfOOSpEUSmbnwpIi1wJ2Z+fqy/ExmruhYfzQzV0bEXcCHM3N/Gd8L/DZwEXB6Zt5Yxt8PPJuZO7rc1zbau4+YmJg4b3p6eqAHNjMzw+PHnhvounOtX3XmUG4H2rmWL18+tNsblibmakqmA9859rzliZfBU98b7s9FL/c7a777bcrzNVcTczUxEwwv14YNG+7LzMmF5g36F8GeioizM/PJsovn6TJ+GFjTMW818EQZn5oz3up2w5m5E9gJMDk5mVNTU92mLajVarFj//GBrjvXoasHy9BNq9Vi0Mc0Sk3M1ZRM18z5y1zXrT/BjgPLhvpz0cv9zprvfpvyfM3VxFxNzATjzzXoLqDdwOyZPFuAOzrG31nOBroQOFZ2EX0euCQiVpaDv5eUMUnSIllwCyAi/pz2q/ezIuIw7bN5tgO3RcRW4FvAVWX63cBlwEHgWeBdAJl5JCI+BNxb5n0wM+ceWJYkjdGCBZCZb59n1cVd5iZw7Ty3cytwa1/pJEkj4zuBJalSFoAkVWrQs4Ak9WntfGf1bL98zEmkNrcAJKlSFoAkVcoCkKRKWQCSVCkLQJIq5VlAWtI8s0YanFsAklQpC0CSKmUBSFKlLABJqpQHgaUBeQBaS51bAJJUKbcAtCTM92pb0uDcApCkSlkAklQpC0CSKmUBSFKlLABJqpRnAalROs/2uW79Ca7x7J9T5vsVNB8LQCo81VS1sQCkF4gD3znmFpP6YgFopNz9IDWXBaChcPeJtPRYAHpBWkpbHotVnie73yY+Txo+C6AHS+mXiU7OLRXpRywA9cVfoItvvn+D69aPOYiWPAtA0ilzK3lpsgCkIXshbCX5C70OYy+AiNgI/D5wGvCJzNw+7gxafC+EX5LSUjfWAoiI04CPAf8GOAzcGxG7M/Phceao0UK/cOd+7IKv9KQXvnFvAZwPHMzMxwAiYhrYBCzJAmjaZvIwX1X7Cl3djPrnomn/p17oxl0Aq4BvdywfBi4Yc4aRO9l/Ej/gTDVZe/1dQ/mZH1YxzN6OW7xtkZnju7OIq4BLM/NXyvI7gPMz890dc7YB28rizwLfGPDuzgL+8RTijoq5etfETGCufjUxVxMzwfBy/WRmvnqhSePeAjgMrOlYXg080TkhM3cCO0/1jiLiK5k5eaq3M2zm6l0TM4G5+tXEXE3MBOPPNe4/CHMvsC4izomIlwCbgd1jziBJYsxbAJl5IiL+E/B52qeB3pqZD40zgySpbezvA8jMu4G7x3BXp7wbaUTM1bsmZgJz9auJuZqYCcaca6wHgSVJzeEfhZekSi35AoiIjRHxjYg4GBHXd1l/ekR8pqz/UkSsbUiufx0RX42IExFxZUMy/WZEPBwRD0bE3oj4yYbk+o8RcSAiHoiI/RFxbhNydcy7MiIyIsZy9kYPz9c1EfEP5fl6ICJ+ZbEzlTm/VH6+HoqIPxt1pl5yRcTNHc/T30fEMw3J9S8iYl9E3F/+P142kiCZuWS/aB9I/ibwU8BLgK8B586Z82vAH5XLm4HPNCTXWuANwKeAKxuSaQPwE+XyrzbouXpFx+W3AX/VhFxl3suBLwL3AJNNyAVcA/zBqLP0mWkdcD+wsiy/pgm55sx/N+0TUxY9F+1jAb9aLp8LHBpFlqW+BfDDj5bIzP8DzH60RKdNwK5y+Xbg4oiIxc6VmYcy80Hg/404Sz+Z9mXms2XxHtrv02hCrn/qWDwDGMeBq15+tgA+BPxX4PtjyNRPrnHqJdN/AD6WmUcBMvPphuTq9HbgzxuSK4FXlMtnMuf9UsOy1Aug20dLrJpvTmaeAI4Br2pArnHrN9NW4HMjTdTWU66IuDYivkn7l+2vNyFXRLwJWJOZd44hT8+5in9fdh3cHhFruqwfd6afAX4mIv42Iu4pnwo8aj3/zJfdnecAX2hIrg8AvxwRh2mfNfluRmCpF0C3V/JzXx32MmfYFuM+F9Jzpoj4ZWAS+G8jTVTursvYj+XKzI9l5r8E3gP855GnWiBXRLwIuBm4bgxZOvXyfP0vYG1mvgH4a360BbyYmZbR3g00RfuV9iciYkUDcs3aDNyemc+NMM+sXnK9HfhkZq4GLgM+XX7mhmqpF8CCHy3ROSciltHenDrSgFzj1lOmiPgF4H3A2zLzB03J1WEauGKkidoWyvVy4PVAKyIOARcCu8dwILiXj1P5bse/3Z8A5y12pjLnjsz8v5n5OO3P+FrXgFyzNjOe3T/QW66twG0Amfl3wEtpf07QcI36gMeID6YsAx6jvek2ezDldXPmXMvzDwLf1oRcHXM/yXgOAvfyXL2J9sGpdQ37N1zXcfkXga80Idec+S3GcxC4l+fr7I7L/w64pwGZNgK7yuWzaO8CedVi5yrzfhY4RHlfVEP+DT8HXFMuv5Z2QQw938gf7BiezMuAvy+/uN5Xxj5I+xUstJvzfwIHgS8DP9WQXD9P+5XAceC7wEMNyPTXwFPAA+Vrd0Oeq98HHiqZ9p3sF/E4c82ZO5YC6PH5+nB5vr5Wnq9/1YBMAfwu7b/9cQDY3ITnqix/ANg+jjx9PF/nAn9b/g0fAC4ZRQ7fCSxJlVrqxwAkSQOyACSpUhaAJFXKApCkSlkAklQpC0CSKmUBSFKlLABJqtT/BzYrPUjqmEmIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(\"**Total non_overlapping_coverage on L_train (percentage of labelled over all)**  \"+str(L_train.non_overlapping_coverage()))\n",
    "plt.hist(train_marginals, bins=50)\n",
    "plt.grid()\n",
    "plt.savefig(\"data/purpose_L_train_marginals_\"+str(len(purpose_LFs))+str(len(neg_for_purpose_LFs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment(Span(\"b'Our current domain of study is the film preview (the commercial advertisements primarily created to attract audiences).'\", sentence=11073, chars=[0,118], words=[0,19]))\n",
      "Sentence(Document 5834868425ff05a97b00d6d6,2,b'Our current domain of study is the film preview (the commercial advertisements primarily created to attract audiences).')\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x112419828>, 'entity_cids': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'stable_id': '5834868425ff05a97b00d6d6::sentence:293:412', 'pos_tags': ['PRP$', 'JJ', 'NN', 'IN', 'NN', 'VBZ', 'DT', 'NN', 'NN', '-LRB-', 'DT', 'JJ', 'NNS', 'RB', 'VBN', 'TO', 'VB', 'NNS', '-RRB-', '.'], 'abs_char_offsets': [293, 297, 305, 312, 315, 321, 324, 328, 333, 341, 342, 346, 357, 372, 382, 390, 393, 401, 410, 411], 'dep_parents': [3, 3, 6, 3, 4, 0, 9, 9, 6, 9, 13, 13, 9, 15, 13, 17, 15, 17, 15, 6], 'char_offsets': [0, 4, 12, 19, 22, 28, 31, 35, 40, 48, 49, 53, 64, 79, 89, 97, 100, 108, 117, 118], 'text': 'Our current domain of study is the film preview (the commercial advertisements primarily created to attract audiences).', 'document_id': 864, 'entity_types': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'dep_labels': ['poss', 'amod', 'nsubj', 'prep', 'pobj', 'ROOT', 'det', 'compound', 'attr', 'punct', 'det', 'amod', 'appos', 'advmod', 'acl', 'aux', 'xcomp', 'dobj', 'punct', 'punct'], 'type': 'sentence', 'lemmas': ['-PRON-', 'current', 'domain', 'of', 'study', 'be', 'the', 'film', 'preview', '(', 'the', 'commercial', 'advertisement', 'primarily', 'create', 'to', 'attract', 'audience', ')', '.'], 'ner_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'words': ['Our', 'current', 'domain', 'of', 'study', 'is', 'the', 'film', 'preview', '(', 'the', 'commercial', 'advertisements', 'primarily', 'created', 'to', 'attract', 'audiences', ')', '.'], 'position': 2, 'id': 11073, 'document': Document 5834868425ff05a97b00d6d6}\n",
      "[Sentence(Document 5834868425ff05a97b00d6d6,0,b'This paper presents a framework for the classification of feature films into genres, based on computable visual cues.'), Sentence(Document 5834868425ff05a97b00d6d6,1,b'The authors view the work as a step towards high-level semantic film interpretation, currently using low-level video features and knowledge of ubiquitous cinematic practices.'), Sentence(Document 5834868425ff05a97b00d6d6,2,b'Our current domain of study is the film preview (the commercial advertisements primarily created to attract audiences).'), Sentence(Document 5834868425ff05a97b00d6d6,3,b'A preview often emphasizes the theme of a film and hence provides suitable information for classification.\\n')]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "docid=194\n",
    "print(train_segments[docid])\n",
    "print(train_segments[docid].get_parent())\n",
    "print(train_segments[docid].get_parent().__dict__)\n",
    "print(train_segments[docid].get_parent().get_parent().sentences)\n",
    "print(train_segments[docid].get_parent().get_parent().sentences.index(train_segments[docid].get_parent()))\n",
    "# print(docs[1].sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
