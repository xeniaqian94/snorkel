{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run 3: Refine Labelling Function\n",
    "\n",
    "This notebook continues from Test Run 2, but addresses the following issues:\n",
    "\n",
    "1. Load more papers than toy examples as the toy Test Run 2. \n",
    "2. Complete/refine labelling functions.\n",
    "3. Load two evaluation datasets and calculate overlap: (1) 5 system/model/method papers + 5 empirical papers; (2) 20K labelled paper dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "debug_mode=1 # if not, debug_mode=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 5560\n",
      "Sentences: 19540\n",
      "The longest sentence has 157 tokens.\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "session = SnorkelSession()\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "\n",
    "sents = session.query(Sentence).all()\n",
    "n_max_corpus=0\n",
    "for sent in sents:\n",
    "    n_max_corpus=max(n_max_corpus,len(sent.words))\n",
    "\n",
    "print(\"The longest sentence has \"+str(n_max_corpus)+\" tokens.\")\n",
    "\n",
    "from snorkel.models import Document\n",
    "from util import number_of_people\n",
    "\n",
    "docs = session.query(Document).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if i % 10 == 8 and \"cscw18\"!=doc.name[:6]:\n",
    "            dev_sents.add(s)\n",
    "#         elif i % 10 == 9:\n",
    "        elif \"cscw18\"==doc.name[:6]:  # replace the earlier 10% test documents as cscw'18 annotation guideline 10 examples\n",
    "            test_sents.add(s)\n",
    "        elif \"cscw18\"!=doc.name[:6]:\n",
    "            train_sents.add(s)\n",
    "            \n",
    "print(len(test_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import *\n",
    "\n",
    "ngrams = Ngrams(n_max=n_max_corpus) # we define the maximum n value as n_max_corpus\n",
    "document_breakdown_map=dict() # maps doc_id into a dict of [\"Background\", \"Purpose\", \"Mechanism\", \"Method\", \"Finding\"]\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "def extract_and_display(matcher,candidate_class,candidate_class_name,document_breakdown_map=None,selected_split=0):  # split over train/dev/test but returns only train set\n",
    "    for (i, sents) in ([(0,train_sents), (1,dev_sents), (2,test_sents)] if selected_split==0 else ([(2,test_sents)] if selected_split==0 else [(1,dev_sents)])):\n",
    "        %time matcher.apply(sents, split=i)\n",
    "        printmd(\"**Split \"+str(i)+\" - number of candidates extracted: \"+str(session.query(candidate_class).filter(candidate_class.split == i).count())+\"**\\n\\n\")\n",
    "    cands = session.query(candidate_class).filter(candidate_class.split == selected_split).all()\n",
    "    for i in range(min(4,len(cands))): # to print all cands, range(len(cands))\n",
    "        printmd(\"**\"+str(i)+\"/\"+str(len(cands))+\" Candidate/Span:**\\t`\"+str(cands[i])+\"`\")\n",
    "        printmd(\"**Its parent Sentence's text:**\\t\"+str(cands[i].get_parent().text))\n",
    "        printmd(\"**Its parent Document's text:**\\t\"+str(cands[i].get_parent().get_parent().__dict__))\n",
    "        print() \n",
    "        \n",
    "    for cand in cands:\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in document_breakdown_map:\n",
    "            document_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in document_breakdown_map[doc_name]:\n",
    "            document_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        document_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "        \n",
    "    return cands\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to differentiate ``Background`` with ``Purpose``. Recall our ``Background`` definition from Google Doc:\n",
    "\n",
    "``Contains words that indicate prior work (e.g., “traditionally”, “researchers have…”), and then following sentence/span starts with some variant of “In this paper, we introduce…” (Exploring adjacency relationship through helper function in Snorkel)``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove if necessary \n",
    "# session.query(Background).all()\n",
    "import snorkel.models.candidate as candidate\n",
    "\n",
    "def del_defined_candidate_class(class_name):\n",
    "    print(\"Existing\", candidate.candidate_subclasses)\n",
    "    del(candidate.candidate_subclasses[class_name])\n",
    "    print(\"After deletion\", candidate.candidate_subclasses)\n",
    "    \n",
    "## Usage:    \n",
    "# del_defined_candidate_class('Purpose')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells describe our ``Span Matching`` phase: we respectively define the `Span` for the 5 segments are , and centrally stored in a document-id-indexed hashmap. This helps us to group the 5 segments back into documents.\n",
    "3. Only those documents that have at least 3 segments are considered as valid\n",
    "\n",
    "Following this ``Span Matching`` phase, we will be having an ``Document Aggregation`` phase, where adjacency relationship and (or) waterfall models are being emphasized through multiple criteria. For example, the following criteria prevents non-abstracts (e.g. proceeding cover letter, tutorial introduction, etc.) or less well-strctured abstracts from being included. \n",
    "\n",
    "1. Only those documents that have all 5 segments are considered as valid\n",
    "2. Only those documents that have at least 4 segments are cosidered as valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 1s, sys: 959 ms, total: 1min 1s\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 148**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 8.08 s, sys: 411 ms, total: 8.49 s\n",
      "Wall time: 8.58 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 15**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 472 ms, sys: 70.1 ms, total: 542 ms\n",
      "Wall time: 521 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 0**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/148 Candidate/Span:**\t`Background(Span(\"b'recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'\", sentence=24132, chars=[83,244], words=[13,36]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tWhile developing a suite of tools for statistical machine translation research, we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d9a19b0>, 'name': '59398333c50f90cdd3ae2761', 'stable_id': '59398333c50f90cdd3ae2761::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5295, 'type': 'document', 'sentences': [Sentence(Document 59398333c50f90cdd3ae2761,0,b'While developing a suite of tools for statistical machine translation research, we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'), Sentence(Document 59398333c50f90cdd3ae2761,1,b'We developed Cairo to fill this need.'), Sentence(Document 59398333c50f90cdd3ae2761,2,b'Cairo is a free, open-source, portable, user-friendly, GUI-driven program written in Java that provides a visual representation of word correspondences between bilingual pairs of sentences, as well as relevant translation model parameters.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/148 Candidate/Span:**\t`Background(Span(\"b'Researchers in psychometrics argue that before adding new labels to applications'\", sentence=12535, chars=[0,79], words=[0,10]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tResearchers in psychometrics argue that before adding new labels to applications, the labels must be empirically evaluated."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x112329978>, 'name': '5834868725ff05a97b014e76', 'stable_id': '5834868725ff05a97b014e76::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1958, 'type': 'document', 'sentences': [Sentence(Document 5834868725ff05a97b014e76,0,b'Linguistic labels such as high, medium, and low are commonly used in different applications.'), Sentence(Document 5834868725ff05a97b014e76,1,b'Researchers in psychometrics argue that before adding new labels to applications, the labels must be empirically evaluated.'), Sentence(Document 5834868725ff05a97b014e76,2,b'In this paper, we explain the process of selecting labels for a security assessment application.'), Sentence(Document 5834868725ff05a97b014e76,3,b'We also show how we evaluate the labels empirically using a sample population from Amazon Mechanical Turk users.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/148 Candidate/Span:**\t`Background(Span(\"b'Previous work explored semantic concepts for content analysis to assist retrieval.'\", sentence=12041, chars=[0,81], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tPrevious work explored semantic concepts for content analysis to assist retrieval."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122f1c18>, 'name': '5834868625ff05a97b013640', 'stable_id': '5834868625ff05a97b013640::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1818, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b013640,0,b'Huge amount of videos on the Internet have rare textual information, which makes video retrieval challenging given a text query.'), Sentence(Document 5834868625ff05a97b013640,1,b'Previous work explored semantic concepts for content analysis to assist retrieval.'), Sentence(Document 5834868625ff05a97b013640,2,b\"However, the human-defined concepts might fail to cover the data and there is a potential gap between these concepts and the semantics expected from user's query.\"), Sentence(Document 5834868625ff05a97b013640,3,b'Also, building a corpus is expensive and time-consuming.'), Sentence(Document 5834868625ff05a97b013640,4,b'To address these issues, we propose a semi-automatic framework to discover the semantic concepts.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/148 Candidate/Span:**\t`Background(Span(\"b'Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life'\", sentence=10746, chars=[0,116], words=[0,18]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tBecause researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life, they've made multidisciplinary teams the norm rather than the exception."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11c631eb8>, 'name': '5834868625ff05a97b011a7a', 'stable_id': '5834868625ff05a97b011a7a::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1438, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b011a7a,0,b\"Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life, they've made multidisciplinary teams the norm rather than the exception.\"), Sentence(Document 5834868625ff05a97b011a7a,1,b'However, overcoming boundaries between various disciplines is a significant challenge and in many cases represents a key factor for successful development.'), Sentence(Document 5834868625ff05a97b011a7a,2,b'Problem-solving approaches differ radically, and finding common ground for assessing results can be difficult.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Background: \n",
    "Background = candidate_subclass('Background', ['background_cue'])\n",
    "non_comma_matcher=DictionaryMatch(d=[','],longest_match_only=True,reverse=True)  \n",
    "transition_word=DictionaryMatch(d=['while','unlike','despite'],longest_match_only=True) \n",
    "transition_prev_work=DictionaryMatch(d=['previous','earlier','past'],longest_match_only=True) \n",
    "dict_background_matcher=DictionaryMatch(d=['previous work','traditionally','researchers'],longest_match_only=True) \n",
    "excluded_dict_background_matcher=DictionaryMatch(d=['we','unlike','our'],longest_match_only=True,reverse=True) \n",
    "non_comma_dict_background_matcher=CandidateExtractor(Background, [ngrams], [Intersection(non_comma_matcher,Union(dict_background_matcher,Intersection(transition_word,transition_prev_work)),excluded_dict_background_matcher)])\n",
    "background_cands=extract_and_display(non_comma_dict_background_matcher,Background,\"Background\",document_breakdown_map)\n",
    "print(document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 37s, sys: 1.68 s, total: 1min 38s\n",
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1501**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 11.5 s, sys: 477 ms, total: 12 s\n",
      "Wall time: 12.1 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 203**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 757 ms, sys: 118 ms, total: 876 ms\n",
      "Wall time: 865 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1501 Candidate/Span:**\t`Purpose(Span(\"b'In this paper'\", sentence=21545, chars=[0,12], words=[0,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d85b278>, 'name': '58cb6b8ec50f90cdd3875b14', 'stable_id': '58cb6b8ec50f90cdd3875b14::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4540, 'type': 'document', 'sentences': [Sentence(Document 58cb6b8ec50f90cdd3875b14,0,b'Visual inspection is, by far, the most widely used method in aircraft surface inspection.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,1,b'We are currently developing a prototype remote visual inspection system, designed to facilitate testing the hypothesized feasibility and advantages of remote visual inspection of aircraft surfaces.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,2,b'In this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1501 Candidate/Span:**\t`Purpose(Span(\"b'\\xe2\\x80\\x94here we examine shrinkage toward diagonality.\\n'\", sentence=11574, chars=[0,46], words=[0,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\t—here we examine shrinkage toward diagonality.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122c40f0>, 'name': '5834868625ff05a97b0122ab', 'stable_id': '5834868625ff05a97b0122ab::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1681, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b0122ab,0,b'The problem of estimating a covariance matrix in small samples has been considered by several authors following early work by Stein.'), Sentence(Document 5834868625ff05a97b0122ab,1,b'This problem can be especially important in hierarchical models where the standard errors of fixed and random effects depend on estimation of the covariance matrix of the distribution of the random effects.'), Sentence(Document 5834868625ff05a97b0122ab,2,b'We propose a set of hierarchical priors (HPs) for the covariance matrix that produce posterior shrinkage toward a specified structure'), Sentence(Document 5834868625ff05a97b0122ab,3,b'\\xe2\\x80\\x94here we examine shrinkage toward diagonality.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1501 Candidate/Span:**\t`Purpose(Span(\"b'which extend organisational boundaries.'\", sentence=6712, chars=[119,157], words=[19,23]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe quest for scalability, reliability and cost reduction has led to the development of massively distributed systems, which extend organisational boundaries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x111488240>, 'name': '5834868425ff05a97b00e0fe', 'stable_id': '5834868425ff05a97b00e0fe::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 315, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00e0fe,0,b'Modern computing paradigms have frequently adopted concepts from distributed systems.'), Sentence(Document 5834868425ff05a97b00e0fe,1,b'The quest for scalability, reliability and cost reduction has led to the development of massively distributed systems, which extend organisational boundaries.'), Sentence(Document 5834868425ff05a97b00e0fe,2,b'Voluntary computing environments (such as BOINC), Grids (such as EGEE and Globus), and more recently Cloud Computing (both open source and commercial) have established themselves as a range of distributed systems.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1501 Candidate/Span:**\t`Purpose(Span(\"b'however'\", sentence=12099, chars=[5,11], words=[2,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tFew, however, allow new interfaces to be created from scratch because they do not provide a means of demonstrating when a recorded macro should be invoked."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122fb828>, 'name': '5834868725ff05a97b013a8a', 'stable_id': '5834868725ff05a97b013a8a::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1836, 'type': 'document', 'sentences': [Sentence(Document 5834868725ff05a97b013a8a,0,b'Many programming by demonstration (PBD) systems elaborate on the idea of macro recording, and they allow users to extend existing applications.'), Sentence(Document 5834868725ff05a97b013a8a,1,b'Few, however, allow new interfaces to be created from scratch because they do not provide a means of demonstrating when a recorded macro should be invoked.'), Sentence(Document 5834868725ff05a97b013a8a,2,b'This paper discusses stimulus-response systems that allow both the when (stimulus/event) and the what (response macro) to be demonstrated.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Purpose:\n",
    "Purpose=candidate_subclass('Purpose',['purpose_cue'])\n",
    "\n",
    "transition_regex_matcher=RegexMatchSpan(rgx=\"((^|\\s)however.*$)|((^|\\s)but(?!(also))*$)\",longest_match_only=True)  # Correction: purpose \n",
    "excluded_dict_purpose_matcher=SentenceMatch(d=['but also','but without','but sometimes'],longest_match_only=True,reverse=True)  # the parent sentence shall not include \"but also\"\n",
    "transition_matcher=Intersection(transition_regex_matcher,excluded_dict_purpose_matcher)\n",
    "\n",
    "comparative_degree_matcher=Intersection(RegexMatchSpan(rgx=\"(.*more.*than.*$)|(.*er than.*$)\",longest_match_only=True),transition_prev_work)  # Correction: purpose \n",
    "other_regex_matcher=RegexMatchSpan(rgx=\"(.*extend.*$)|(.*offer.*$)\",longest_match_only=True)\n",
    "\n",
    "dict_purpose_matcher=DictionaryMatch(d=['in this paper','in the paper',' that can ','in this study','to examine','we examine','to investigate','implications'],longest_match_only=True) \n",
    "\n",
    "# Unit test\n",
    "# non_comma_dict_purpose_matcher=CandidateExtractor(Purpose, [ngrams], [Intersection(non_comma_matcher,other_regex_matcher)])\n",
    "\n",
    "non_comma_dict_purpose_matcher=CandidateExtractor(Purpose, [ngrams], [Intersection(non_comma_matcher,Union(comparative_degree_matcher,other_regex_matcher,dict_purpose_matcher,transition_matcher))]) #,intersection(excluded_dict_purpose_matcher,transition_regex_matcher)])\n",
    "purpose_cands=extract_and_display(non_comma_dict_purpose_matcher,Purpose,\"Purpose\",document_breakdown_map)\n",
    "print(document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 54.2 s, sys: 1 s, total: 55.2 s\n",
      "Wall time: 56.5 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1404**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 7.54 s, sys: 482 ms, total: 8.02 s\n",
      "Wall time: 8.23 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 169**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 504 ms, sys: 136 ms, total: 640 ms\n",
      "Wall time: 730 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1404 Candidate/Span:**\t`Mechanism(Span(\"b'We describe a Machine Translation (MT) approach that is specifically designed to enable rapid development of MT for languages with limited amounts of online resources.'\", sentence=17491, chars=[0,166], words=[0,27]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tWe describe a Machine Translation (MT) approach that is specifically designed to enable rapid development of MT for languages with limited amounts of online resources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x112113908>, 'name': '58ccadd5c50f90cdd3882822', 'stable_id': '58ccadd5c50f90cdd3882822::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3398, 'type': 'document', 'sentences': [Sentence(Document 58ccadd5c50f90cdd3882822,0,b'We describe a Machine Translation (MT) approach that is specifically designed to enable rapid development of MT for languages with limited amounts of online resources.'), Sentence(Document 58ccadd5c50f90cdd3882822,1,b'Our approach assumes the availability of a small number of bi-lingual speakers of the two languages, but these need not be linguistic experts.'), Sentence(Document 58ccadd5c50f90cdd3882822,2,b'The bi-lingual speakers create a comparatively small corpus of word aligned phrases and sentences (on the order of magnitude of a few thousand sentence pairs) using a specially designed elicitation tool.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1404 Candidate/Span:**\t`Mechanism(Span(\"b'Our approach introduces a moment-matching technique for trigonometric functions to approximate the Expectation Propagation messages efficiently.\\n'\", sentence=13390, chars=[0,144], words=[0,19]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOur approach introduces a moment-matching technique for trigonometric functions to approximate the Expectation Propagation messages efficiently.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11c635ac8>, 'name': '5834868625ff05a97b012ac9', 'stable_id': '5834868625ff05a97b012ac9::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 2200, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b012ac9,0,b'The von Mises model encodes a multivariate circular distribution as an undirected probabilistic graphical model.'), Sentence(Document 5834868625ff05a97b012ac9,1,b'Presently, the only algorithm for performing inference in the model is Gibbs sampling, which becomes inefficient for large graphs.'), Sentence(Document 5834868625ff05a97b012ac9,2,b'To address this issue, we introduce an Expectation Propagation based algorithm for performing inference in the von Mises graphical model.'), Sentence(Document 5834868625ff05a97b012ac9,3,b'Our approach introduces a moment-matching technique for trigonometric functions to approximate the Expectation Propagation messages efficiently.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1404 Candidate/Span:**\t`Mechanism(Span(\"b'Two new architectures are proposed for designing physical network nodes in packet-switched structures.'\", sentence=21079, chars=[0,101], words=[0,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tTwo new architectures are proposed for designing physical network nodes in packet-switched structures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d81b518>, 'name': '58c94b36c50f90cdd385f6ad', 'stable_id': '58c94b36c50f90cdd385f6ad::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4400, 'type': 'document', 'sentences': [Sentence(Document 58c94b36c50f90cdd385f6ad,0,b'Two new architectures are proposed for designing physical network nodes in packet-switched structures.'), Sentence(Document 58c94b36c50f90cdd385f6ad,1,b'They allow transparent optical packet networking and are based on the association of various subsystems, which have previously been proposed, and demonstrated.'), Sentence(Document 58c94b36c50f90cdd385f6ad,2,b'These elements are mainly based on the application of nonlinear behavior in semiconductor optical amplifiers.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1404 Candidate/Span:**\t`Mechanism(Span(\"b'an approach to abstract syntax with binding structure.'\", sentence=21019, chars=[124,177], words=[23,31]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tA recent SIGACT News Logic column, guest-written by James Cheney DOI= 10.1145/1107523.1107537, discussed nominal logic [1], an approach to abstract syntax with binding structure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d8109b0>, 'name': '58d08b9cc50f90cdd38a4f1d', 'stable_id': '58d08b9cc50f90cdd38a4f1d::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4383, 'type': 'document', 'sentences': [Sentence(Document 58d08b9cc50f90cdd38a4f1d,0,b'A recent SIGACT News Logic column, guest-written by James Cheney DOI= 10.1145/1107523.1107537, discussed nominal logic [1], an approach to abstract syntax with binding structure.'), Sentence(Document 58d08b9cc50f90cdd38a4f1d,1,b'In addition to providing a worthy tutorial on nominal logic, that column leveled five criticisms at higher-order abstract syntax, an alternative approach for dealing with binding structure in abstract syntax.'), Sentence(Document 58d08b9cc50f90cdd38a4f1d,2,b'We argue below that three of those criticisms are factually inaccurate, and the other two are misguided.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Mechanism: \n",
    "Mechanism = candidate_subclass('Mechanism', ['mechanism_cue']) \n",
    "dict_mechanism_matcher=DictionaryMatch(d=['introduce','introduces','propose','proposes','we propose','we develop','approach'],longest_match_only=True) \n",
    "non_comma_dict_mechanism_matcher=CandidateExtractor(Mechanism, [ngrams], [Intersection(non_comma_matcher,dict_mechanism_matcher)])\n",
    "mechanism_cands=extract_and_display(non_comma_dict_mechanism_matcher,Mechanism,\"Mechanism\",document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 2s, sys: 1.56 s, total: 1min 3s\n",
      "Wall time: 1min 6s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 752**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 7.53 s, sys: 456 ms, total: 7.99 s\n",
      "Wall time: 8.11 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 81**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 459 ms, sys: 92.3 ms, total: 551 ms\n",
      "Wall time: 542 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/752 Candidate/Span:**\t`Method(Span(\"b'we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection'\", sentence=21545, chars=[15,133], words=[4,19]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d85b278>, 'name': '58cb6b8ec50f90cdd3875b14', 'stable_id': '58cb6b8ec50f90cdd3875b14::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4540, 'type': 'document', 'sentences': [Sentence(Document 58cb6b8ec50f90cdd3875b14,0,b'Visual inspection is, by far, the most widely used method in aircraft surface inspection.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,1,b'We are currently developing a prototype remote visual inspection system, designed to facilitate testing the hypothesized feasibility and advantages of remote visual inspection of aircraft surfaces.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,2,b'In this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/752 Candidate/Span:**\t`Method(Span(\"b'it would still take almost a day to analyze the average sentence.'\", sentence=24074, chars=[79,143], words=[14,26]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIf every combination could be accurately judged in one thousandth of a second, it would still take almost a day to analyze the average sentence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d999cf8>, 'name': '5942b359c50f90cdd3b1f54d', 'stable_id': '5942b359c50f90cdd3b1f54d::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5276, 'type': 'document', 'sentences': [Sentence(Document 5942b359c50f90cdd3b1f54d,0,b'Fifty six million, six hundred eighty seven thousand, and forty.'), Sentence(Document 5942b359c50f90cdd3b1f54d,1,b'A big number, to be sure.'), Sentence(Document 5942b359c50f90cdd3b1f54d,2,b'This is the number of possible semantic analyses for an average sized sentence in the Mikrokosmos Machine Translation project.'), Sentence(Document 5942b359c50f90cdd3b1f54d,3,b'Complex sentences have gone past the trillions.'), Sentence(Document 5942b359c50f90cdd3b1f54d,4,b'If every combination could be accurately judged in one thousandth of a second, it would still take almost a day to analyze the average sentence.'), Sentence(Document 5942b359c50f90cdd3b1f54d,5,b'And you can forget about the hard ones.'), Sentence(Document 5942b359c50f90cdd3b1f54d,6,b'And yet, understanding natural language sentences is intuitively not an exponential a air.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/752 Candidate/Span:**\t`Method(Span(\"b'the robot played like the rest of the participants and'\", sentence=16575, chars=[18,71], words=[4,13]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn one condition, the robot played like the rest of the participants and, in the other, the robot moderated the game."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d619eb8>, 'name': '5834868825ff05a97b016ad8', 'stable_id': '5834868825ff05a97b016ad8::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3142, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b016ad8,0,b'We present initial findings from an experiment in which participants played Mafia, an established role-playing game, with our robot.'), Sentence(Document 5834868825ff05a97b016ad8,1,b'In one condition, the robot played like the rest of the participants and, in the other, the robot moderated the game.'), Sentence(Document 5834868825ff05a97b016ad8,2,b\"We discuss general aspects of the interaction, participants' perceptions, and the potential of this scenario for studying group spatial behavior from robotic platforms.\\n\")]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/752 Candidate/Span:**\t`Method(Span(\"b'along with experimental results from an empirical evaluation of the completed system.\\n'\", sentence=20724, chars=[70,155], words=[11,24]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe design and implementation of the diagnostic system are presented, along with experimental results from an empirical evaluation of the completed system.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d7e6f98>, 'name': '58ce2d95c50f90cdd388f04f', 'stable_id': '58ce2d95c50f90cdd388f04f::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4296, 'type': 'document', 'sentences': [Sentence(Document 58ce2d95c50f90cdd388f04f,0,b'This paper presents a grammar diagnostic system for controlled language checking.'), Sentence(Document 58ce2d95c50f90cdd388f04f,1,b'The implemented diagnostics were designed to address the most difficult rewrites for authors, based on an empirical analysis of log files containing over 180,000 sentences.'), Sentence(Document 58ce2d95c50f90cdd388f04f,2,b'The design and implementation of the diagnostic system are presented, along with experimental results from an empirical evaluation of the completed system.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compound Matcher for Method: \n",
    "Method = candidate_subclass('Method', ['method_cue'])\n",
    "\n",
    "dict_method_matcher=DictionaryMatch(d=['dataset','benchmark','experiment ','experiments',\"empirical\",\"participant\",\"survey\",\" conduct\",\" analyze\"],longest_match_only=True) \n",
    "\n",
    "non_comma_dict_method_matcher=CandidateExtractor(Method, [ngrams], [Intersection(non_comma_matcher,dict_method_matcher)])\n",
    "method_cands=extract_and_display(non_comma_dict_method_matcher,Method,\"Method\",document_breakdown_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[===================                     ] 45%"
     ]
    }
   ],
   "source": [
    "# TODO: Compound Matcher for Finding: \n",
    "Finding = candidate_subclass('Finding', ['finding_cue'])\n",
    "\n",
    "dict_finding_matcher=DictionaryMatch(d=['show that','shows that','found','indicate','results','performance','find'],longest_match_only=True) \n",
    "\n",
    "non_comma_dict_finding_matcher=CandidateExtractor(Finding, [ngrams], [Intersection(non_comma_matcher,dict_finding_matcher)])\n",
    "finding_cands=extract_and_display(non_comma_dict_finding_matcher,Finding,\"Finding\",document_breakdown_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the ``Document Aggregation`` phase. First we show a few document examples, of how ``document_breakdown_map`` looks like, as well as how do we aggregate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,docid in enumerate(document_breakdown_map.keys()):\n",
    "    print(\"\\n\\nDocument \"+str(ind)+\"/\"+str(len(document_breakdown_map.keys())))\n",
    "    print(\"Extracted segments\\n\\n\",docid,document_breakdown_map[docid],\"\\n\\n\")\n",
    "    print(\"Complete abstract\\n\\n\",document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent().sentences)\n",
    "    if ind>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we count the number of documents that contain exactly N segments, where N=1, 2, 3, 4, 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Below are one or two document examples that contain exactly 5 segments\n",
      "\n",
      "Total count is 0\n",
      "\n",
      "=================\n",
      "\n",
      "Below are one or two document examples that contain exactly 4 segments\n",
      "\n",
      "Total count is 0\n",
      "\n",
      "=================\n",
      "\n",
      "Below are one or two document examples that contain exactly 3 segments\n",
      "\n",
      "Total count is 0\n",
      "\n",
      "=================\n",
      "\n",
      "Below are one or two document examples that contain exactly 2 segments\n",
      "\n",
      "Total count is 0\n",
      "\n",
      "=================\n",
      "\n",
      "Below are one or two document examples that contain exactly 1 segments\n",
      "\n",
      "Total count is 0\n",
      "\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_document_text(doc):\n",
    "    text_string=\" \".join([str(sentence.text) for sentence in doc.sentences])\n",
    "    return text_string\n",
    "\n",
    "\n",
    "def rank_by_matched_segments(document_breakdown_map)\n",
    "    for n in [5,4,3,2,1]:\n",
    "        print(\"Below are one or two document examples that contain exactly \"+str(n)+\" segments\\n\")\n",
    "        showed_examples=0\n",
    "        count=0\n",
    "        for ind,docid in enumerate(document_breakdown_map.keys()):\n",
    "            if len(document_breakdown_map[docid].keys())==n:\n",
    "                count+=1\n",
    "                if showed_examples<2:\n",
    "                    showed_examples+=1\n",
    "                    print(docid,\": \", get_document_text(document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent()))\n",
    "                    print(\"Extracted segments\\n\\n\",docid,document_breakdown_map[docid],\"\\n\\n\")\n",
    "        print(\"Total count is \"+str(count)+\"\\n\\n=================\\n\")\n",
    "        \n",
    "rank_by_matched_segments(document_breakdown_map)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to some visualization of extracted Span as highlighted text on Document.\n",
    "\n",
    "Color notation: <b style=\"color:orange;\">Background</b> <b style=\"color:pink;\">Purpose</b> <b style=\"color:green;\">Mechanism</b> <b style=\"color:purple;\">Method</b> <b style=\"color:blue;\">Findings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The ability to argue is essential in many aspects of life, <font color='pink'>but</font> traditional face-to-face tutoring approaches do not scale up well. A solution for this dilemma may be computer-supported argumentation (CSA). <font color='blue'>The evaluation of CSA approaches in different domains has led to mixed results.</font> <font color='purple'>To gain insights into the challenges and future prospects of CSA we conducted a survey among teachers</font>, <font color='orange'>researchers</font>, and system developers. Our investigation points to optimism regarding the potential success and importance of CSA.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "    \n",
    "def print_colored_text(docid,document_breakdown_map=document_breakdown_map):\n",
    "    \n",
    "    color_mapping={\"Background\":\"orange\",\"Purpose\":\"pink\",\"Mechanism\":\"green\",\"Method\":\"purple\",\"Finding\":\"blue\"}\n",
    "    this_document=document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent()\n",
    "    document_text=get_document_text(this_document)\n",
    "\n",
    "    added_segment=[]\n",
    "    for segment in document_breakdown_map[docid]:\n",
    "        spans=document_breakdown_map[docid][segment]\n",
    "        for span in spans:\n",
    "            this_span=document_breakdown_map[docid][segment][0].__dict__[list(document_breakdown_map[docid][segment][0].__dict__)[6]]\n",
    "            span_sentence_text=this_span.sentence.text\n",
    "            span_text=str(span_sentence_text[this_span.char_start:(this_span.char_end+1)])\n",
    "            document_text=document_text.replace(span_text.strip(),\"<font color='\"+color_mapping[segment]+\"'>\"+span_text.strip()+\"</font>\")\n",
    "\n",
    "    printmd(document_text)\n",
    "\n",
    "docid='5834868625ff05a97b013016'\n",
    "print_colored_text(docid)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_breakdown_map={}\n",
    "test_background_cands=extract_and_display(non_comma_dict_background_matcher,Background,\"Background\",document_breakdown_map=test_doc_breakdown_map,selected_split=2)\n",
    "test_purpose_cands=extract_and_display(non_comma_dict_purpose_matcher,Purpose,\"Purpose\",document_breakdown_map=test_doc_breakdown_map,selected_split=2)\n",
    "test_mechanism_cands=extract_and_display(non_comma_dict_mechanism_matcher,Mechanism,\"Mechanism\",document_breakdown_map=test_doc_breakdown_map,selected_split=2)\n",
    "test_method_cands=extract_and_display(non_comma_dict_method_matcher,Method,\"Method\",document_breakdown_map=test_doc_breakdown_map,selected_split=2)\n",
    "test_finding_cands=extract_and_display(non_comma_dict_finding_matcher,Finding,\"Finding\",document_breakdown_map=test_doc_breakdown_map,selected_split=2)\n",
    "\n",
    "print_colored_text('cscw18social',document_breakdown_map=test_doc_breakdown_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
