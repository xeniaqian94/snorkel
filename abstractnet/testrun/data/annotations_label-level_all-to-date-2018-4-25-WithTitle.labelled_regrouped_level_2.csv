2K_test_0	Online communities have the potential to be supportive, cruel or anywhere in between The development of positive norms for interaction can help users build bonds, Consistent with aspects of imitation theory and deterrence theory, This work considers the intersection of tools, authority and types of behaviors, offering a new frame through which to consider the development of moderation strategies. We explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors. Users imitated examples of behavior that they saw, and more so for behaviors from high status users, Proactive moderation tools such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors. Using millions of messages sent in Twitch chatrooms.
2K_test_1	Recent research has demonstrated that ( a ) groups can be characterized by a collective intelligence ( CI ) factor that measures their ability to perform together on a wide range of different tasks, and ( b ) this factor can predict groups ' performance on other tasks in the future. The current study examines whether these results translate into the world of teams in competitive online video games where self-organized, time-pressured and intense collaboration occurs purely online. We find that CI does, indeed predict the competitive performance of teams controlling for the amount of time played as a team, We also find that CI is positively correlated with the presence of a female team member and with the team members ' average social perceptiveness, Finally unlike in prior studies, tacit coordination in this setting plays a larger role than verbal communication. In this study of teams playing the online game League of Legends.
2K_test_2	Forming work teams involves matching people with complementary skills and personalities, but requires obtaining such data a priori, for forming ad hoc teams accounting for interpersonal dynamics. Results show that participants evaluated their dates based on evidence beyond externally judged slogan quality, and relied heavily on their dyad-specific judgments in selecting teammates Results show that teams formed from preferred dates performed better on a final creative task compared to random dates or non-dates. We introduce team dating, where people interact on brief tasks before working with a dedicated partner for longer, more complex tasks Team dating provides a dynamic technique The initial interactions provided information that helped people select and work with an appropriate teammate. We studied team dating through two online experiments In Experiment 1, workers from a crowd platform independently wrote an ad slogan, discussed it with three consecutive people and evaluated their team date interactions, They then selected preferred teammates from a list showing average ratings for people they had dated and not dated In Experiment 2, we replicated the individual and team dating tasks, and formed teams either i ) by honoring pairwise team dating preferences, ii ) randomly from their pool of dates, or iii ) randomly from those not dated.
2K_test_3	Collective intelligence ( CI ), a group 's capacity to perform a wide variety of tasks, is a key factor in successful collaboration, Our results have important implications for online collaborations and distributed teams. Group composition particularly diversity and member social perceptiveness, are consistent predictors of CI, but we have limited knowledge about the mechanisms underlying their effects, To address this gap, we examine how physiological synchrony, as an indicator of coordination and rapport, relates to CI in computer-mediated teams, and if synchrony might serve as a mechanism explaining the effect of group composition on CI. We find that synchrony in facial expressions ( indicative of shared experience ) was associated with CI and synchrony in skin conductance ( indicative of shared arousal ) with group satisfaction, Furthermore various forms of synchrony mediated the effect of member diversity and social perceptiveness on CI and group satisfaction. We present results from a laboratory experiment where 60 dyads completed the Test of Collective Intelligence ( TCI ) together online and rated their group satisfaction, while wearing physiological sensors.
2K_test_4	Feedback is information that can improve task performance, Online communities educational forums, and crowd-based feedback platforms all support feedback exchange among a more diverse set of sources than ever before, with greater control over how to moderate this exchange Our findings provide design implications for platforms to support more fruitful feedback exchange. In this work we study how the power relationship between the source and receiver and the tone of language influence the recep-tivity, effort and work performance resulting from online feedback exchange. We found that critiques with positive affec-tive language increased positive emotions and reduced participants ' annoyance and frustration, which led to an increase in work quality, compared to critiques without positive language, Feedback without positive affective language led to more edits, but not better work outcomes, Participants reacted more positively to feedback from an anonymous source than from a peer or an authority. We conducted an online experiment manipulating affective language and source of feedback on a writing task.
2K_test_5	How do individuals perceive algorithmic vs. We investigated people 's perceptions of mathematically-proven fair division algorithms making social division decisions. About one third of the participants perceived algorithmic decisions as less than fair ( 30 % for self, 36 % for group ), often because algorithmic assumptions about users did not account for multiple concepts of fairness or social behaviors, and the process of quantifying preferences through interfaces was prone to error, algorithmic decisions were perceived to be less fair than discussion-based decisions, dependent on participants ' interpersonal power and computer programming knowledge, Our work suggests that for algorithmic mediation to be fair, algorithms and their interfaces should account for social and altruistic behaviors that may be difficult to define in mathematical terms. In our first qualitative study, In our second experiment.
2K_test_6	Social identities carry widely agreed upon meanings, called stereotypes that have important effects on social processes. In the present work, we develop a method to extract the stereotypes of Twitter users. Our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel. Our method is grounded in two distinct strands of theory, one that represents stereotypes as identities ' affective meanings and the other that represents stereotypes as semantic relationships between identities. After validating our approach via a prediction task, we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies.
2K_test_7	A key issue whenever people work together to solve a complex problem. How to divide the problem into parts done by different people and combine the parts into a solution for the whole problem, This paper presents a novel way of doing this. The early results suggest that the method can, indeed work at scale as intended. With groups of contests called contest webs Based on the analogy of supply chains for physical products, the method provides incentives for people to ( a ) reuse work done by themselves and others, ( b ) simultaneously explore multiple ways of combining interchangeable parts, and ( c ) work on parts of the problem where they can contribute the most. The paper also describes a field test of this method in an online community of over 50, 000 people who are developing proposals for what to do about global climate change.
2K_test_8	Crowd workers are distributed and decentralized. While decentralization is designed to utilize independent judgment to promote high-quality results, it paradoxically undercuts behaviors and institutions that are critical to high-quality work Reputation is one central example: crowdsourcing systems depend on reputation scores from decentralized workers and requesters, but these scores are notoriously inflated and uninformative. Crowd guilds produced reputation signals more strongly correlated with ground-truth worker quality than signals available on current crowd working platforms, and more accurate than in the traditional model. In this paper we draw inspiration from historical worker guilds ( e, in the silk trade ) to design and implement crowd guilds: centralized groups of crowd workers who collectively certify each other 's quality through double-blind peer assessment. A two-week field experiment compared crowd guilds to a traditional decentralized crowd work model.
2K_test_9	Social science researchers spend significant time annotating behavioral events in video data in order to quantitatively assess interactions [ 2 ], These behavioral events may be instantaneous changes, continuous actions that span unbounded periods of time, or behaviors that would be best described by severity or other scalar ratings. The complexity of these judgments, coupled with the time and effort required to meticulously assess video, results in a training and evaluation process that can take days or weeks, Computational analysis of video data is still limited due to the challenges introduced by objective interpretation and varied contexts. These new features allow analysts to acquire more specific information about events in video datasets. Glance [ 4 ] introduced a means of leveraging human intelligence by recruiting crowds of paid online workers to accurately analyze hours of video data in a matter of minutes, This approach has been shown to expedite work in human-centered fields, as well as generate training data for automated recognition systems, In this paper we describe an interactive demonstration of an improved, more expressive version of Glance that expands the initial set of supported annotation formats ( e, time range classification etc, ) from one to nine, Worker interfaces for each of these options are dynamically generated, along with tutorials based on the analyst 's question.
2K_test_10	How effective are call and SMS logs in modeling tie strength ? Frequency and duration of communication has long been cited as a major aspect of tie strength, Intuitively this makes sense: people communicate with those that they feel close to. Highly cited research papers have pushed this idea further, using communication as a direct proxy for tie strength, However this operationalization has not been validated, Our work evaluates this assumption. Consistent with theory we found that frequent or long-duration communication likely indicates a strong tie, However the use of call and SMS logs produced many errors in separating strong and weak ties, suggesting this approach is incomplete, Follow-up interviews indicate fundamental challenges for inferring tie strength from communication logs. We collected call and SMS logs and ground truth relationship data from 36 participants.
2K_test_11	Social influence is key in technology adoption. But its role in security-feature adoption is unique and remains unclear. Our results suggest that social influence affects one 's likelihood to adopt a security feature, but its effect varies based on the observability of the feature, the current feature adoption rate among a potential adopter 's friends, and the number of distinct social circles from which those feature-adopting friends originate Curiously, there may be a threshold higher than which having more security feature adopting friends predicts for higher adoption likelihood, but below which having more feature-adopting friends predicts for lower adoption likelihood, Furthermore the magnitude of this threshold is modulated by the attributes of a feature-features that are more noticeable ( Login Approvals, Trusted Contacts ) have lower thresholds. Here we analyzed how three Facebook security features ' Login Approvals, Login Notifications and Trusted Contacts-diffused through the social networks of 1.
2K_test_12	Feedback is an important component of the design process, but We conclude with implications for the design of crowd feedback services. Gaining access to high-quality critique outside a classroom or firm is challenging. In the first study, we compared crowd and expert critiques and found evidence that aggregated crowd critique approaches expert critique In a second study, we found that designers who got crowd feedback perceived that it improved their design process The third study showed that designers were enthusiastic about crowd critiques and used them to change their designs. We present CrowdCrit a web-based system that allows designers to receive design critiques from non-expert crowd workers. We evaluated CrowdCrit in three studies focusing on the designer 's experience and benefits of the critiques.
2K_test_13	Massive Open Online Courses ( MOOCs ) enable everyone to receive high-quality education. However current MOOC creators can not provide an effective, economical and scalable method to detect cheating on tests, which would be required for any certification. Our experiment show that ACD and PCD can detect usage of a cheat sheet with good accuracy and can reduce the overall human resources required to monitor MOOCs for cheating. In this paper we propose a Massive Open Online Proctoring ( MOOP ) framework, which combines both automatic and collaborative approaches to detect cheating behaviors in online tests The MOOP framework consists of three major components: Automatic Cheating Detector ( ACD ), Peer Cheating Detector ( PCD ), and Final Review Committee ( FRC ), ACD uses webcam video or other sensors to monitor students and automatically flag suspected cheating behavior, Ambiguous cases are then sent to the PCD, where students peer-review flagged webcam video to confirm suspicious cheating behaviors, Finally the list of suspicious cheating behaviors is sent to the FRC to make the final punishing decision.
2K_test_14	Online communities much like companies in the business world, often need to transfer best practices internally from one unit to another to improve their performance. Organizational scholars disagree about how much a recipient unit should modify a best practice when incorporating it Some evidence indicates that modifying a practice that has been successful in one environment will introduce problems, undercut its effectiveness and harm the performance of the recipient unit Other evidence, though suggests that recipients need to adapt the practice to fit their local environment. The current research introduces a contingency perspective on practice transfer, holding that the value of modifications depends on when they are introduced and who introduces them modifications are more helpful if they are introduced after the receiving project has had experience with the imported practice Furthermore, modifications are more effective if they are introduced by members who have experience in a variety of other projects. Empirical research on the transfer of a quality-improvement practice between projects within Wikipedia shows that.
2K_test_15	Online crowds are a promising source of new innovations, However crowd innovation quality does not always match its quantity. In this paper we explore how to improve crowd innovation with real-time expert guidance. A series of controlled experiments show that experienced facilitators increased the quantity and creativity of workers ' ideas compared to unfacilitated workers, while Novice facilitators reduced workers ' creativity, Analyses of inspiration strategies suggest these opposing results stem from differential use of successful inspiration strategies ( e, provoking mental simulations ), The results show that expert facilitation can significantly improve crowd innovation, but inexperienced facilitators may need scaffolding to be successful. One approach would for experts to provide personalized feed-back, but this scales poorly, and may lead to premature convergence during creative work, Drawing on strategies for facilitating face-to-face brainstorms, we introduce a crowd ideation system where experts monitor incoming ideas through a dashboard and offer high-level `` inspirations '' to guide ideation.
2K_test_16	Support a broad range of collaborative and cooperative tasks, discuss possible incentives and safeguards to context sharing from a user standpoint. Allows mobile devices to opportunistically share contextual information. Through two prototypes we demonstrate how GCF can be used to We then show how our framework 's architecture allows devices to opportunistically detect and collaborate with one another, even when running different applications Finally, we present two real-world domains that show how GCF 's ability to form groups increases users ' access to relevant and timely information. In this paper we present the Group Context Framework ( GCF ), a general-purpose toolkit that GCF provides a standardized way for developers to request contextual data for their applications The framework then intelligently groups with other devices to satisfy these requirements.
2K_test_17	Telepresence means business people can make deals in other countries, doctors can give remote medical advice, and soldiers can rescue someone from thousands of miles away, When interaction is mediated, people are removed from and lack context about the person they are making decisions about We discuss implications of our results for theory and future research. In this paper we explore the impact of technological mediation on risk and dehumanization in decision-making. The results suggest that technological mediation influences decision making, but its influence depends on an individual 's self-construal: participants who saw themselves as defined through their relationships ( interdependent self-construal ) recommended riskier and more painful treatments in video conferencing than when face-to-face. We conducted a laboratory experiment involving medical treatment decisions.
2K_test_18	Online question and answer ( Q & A ) sites, which are platforms for users to post and answer questions on a wide range of topics, are becoming large repositories of valuable knowledge and important to societies, In order to sustain success, Q & A sites face the challenges of ensuring content quality and encouraging user contributions, This work has implications for understanding and designing large-scale social computing systems. This paper examines a particular design decision in Q & A sites-allowing Wikipedia-like collaborative editing on questions and answers, and explores its beneficial effects on content quality and potential detrimental effects on users ' contributions. We found that the benefits of collaborative editing outweigh its risks, For example each substantive edit from other users can increase the number of positive votes by 181 % for the questions and 119 % for the answers, On the other hand, each edit only decreases askers and answerers ' subsequent contributions by no more than 5 %. By examining five years ' archival data of Stack Overflow.
2K_test_19	The Internet has the potential to accelerate scientific problem solving by engaging a global pool of contributors, Existing approaches focus on broadcasting problems to many independent solvers, A better understanding of such collaborative strategies can inform the design of tools to support distributed collaboration on complex problems. We investigate other approaches that may be advantageous. We contribute a simple taxonomy of collaborative acts derived from a Our results indicate a diversity of ways in which mathematicians are reaching a solution, including by iteratively advancing a solution. Process-level examination of collaborations and a quantitative analysis relating collaborative acts to solution quality. By examining a community for mathematical problem solving -- MathOverflow -- in which contributors communicate and collaborate to solve new mathematical 'micro-problems ' online.
2K_test_20	Crowdsourcing has become a popular and indispensable component of many problem-solving pipelines in the research literature, with crowd workers often treated as computational resources that can reliably solve problems that computers have trouble with, such as image labeling/classification, natural language processing or document writing, Yet obviously crowd workers are human, and long sequences of the same monotonous tasks might intuitively reduce the amount of good quality work done by the workers. Here we propose an investigation into how we can use to improve crowd workers ' experiences, to provide timely relief to workers during long sequences of micro-tasks, We hope to improve productivity by retaining workers to work on our tasks longer and to either improve or retain the quality of work. We find that micro-diversions can significantly improve worker retention rate while retaining the same work quality. Diversions containing small amounts of entertainment We call these small period of entertainment ``micro-diversions ''. We experimentally test micro-diversions on Amazon 's Mechanical Turk, a large paid-crowdsourcing platform.
2K_test_21	When health services involve long-term treatment over months or years, providers have the ability, not present in acute emergency care, to collaboratively reflect on clients ' changing health data and adjust interventions Our fieldwork in this context complements and provides contrasts to previous CSCW studies performed in time-critical hospital settings Current literature shows a bias toward standardized records and routines in the implementation of health information technology, a policy that may not be appropriate for long-term health services We discuss how the design of information systems should vary based on temporal factors. In this paper we discuss temporality as a factor in the design of health information technology. We define a temporal spectrum ranging from time-critical services that benefit from standardization to long-term services that require more flexibility. We provide empirical evidence from fieldwork that we performed in organizations providing long-term behavioral and mental health services for children.
2K_test_22	Inform a model of factors contributing to impression formation in this specific context, as well as experiments testing and providing design recommendations for improving members ' ability to interact and effectively learn about each other. My dissertation work focuses on understanding how and why professionals use activity traces generated by social work-sharing sites online to form impressions of fellow professionals ' expertise and inform personal interactions around work artifacts, These findings will then. I have conducted interviews with professionals in different domains who post and share their work online.
2K_test_23	Despite benefits and uses of social networking sites ( SNSs ) users are not always satisfied with their behaviors on the sites These desires for behavior change both provide insight into users ' perceptions of how SNSs impact their lives ( positively or negatively ) and can inform tools for helping users achieve desired behavior changes, Based on these results we provide insights both into how participants perceive different SNSs, as well as potential designs for behavior-change mechanisms to target SNS behaviors. Explore SNS users ' behavior-change goals for Facebook. While some participants want to reduce site use, others want to improve their use or increase a range of behaviors These desired changes differ by SNS, and for Twitter by participants ' levels of site use, Participants also expect a range of benefits from these goals, including increased time contact with others, intrinsic benefits better security/privacy, and improved self presentation. We use a 604-participant online survey.
2K_test_24	Analysts synthesize complex qualitative data to uncover themes and concepts, but the process is time-consuming, cognitively taxing and automated techniques show mixed success Crowdsourcing could help this process through on-demand harnessing of flexible and powerful human cognition, but incurs other challenges including limited attention and expertise Further, text data can be complex. We address two major challenges unsolved in prior crowd clustering work: scaffolding expertise for novice crowd workers, and creating consistent and accurate categories when each worker only sees a small portion of the data enable crowds to create an accurate and useful overview of a dataset:. We demonstrate a classification-plus-context approach elicits the most accurate categories at the most useful level of abstraction. To address these challenges we present an empirical study of a two-stage approach to A ) we draw on cognitive theory to assess how re-representing data can shorten and focus the data on salient dimensions ; and B ) introduce an iterative clustering approach that provides workers a global overview of data.
2K_test_25	Hackathons are events where people who are not normally collocated converge for a few days to write code together Hackathons, it seems are everywhere, Our findings have implications for technology support that needs to be in place for hackathons and for understanding the role of brief interludes of collocation in loosely-coupled. We know that long- term collocation helps advance technical work and facilitate enduring interpersonal relationships, but can similar benefits come from brief, hackathon-style collocation ? How do participants spend their time preparing, working face-to- face and following through these brief encounters ? Do the activities participants select suggest a tradeoff between the social and technical benefits of collocation ?. Suggest the way that hackathon-style collocation advances technical work varies across technical domain, community structure and expertise of participants, Building social ties in contrast, seems relatively constant across hackathons Results from different hackathon team formation strategies suggest a tradeoff between advancing technical work and building social ties. We present results from a multiple-case study that.
2K_test_26	A growing number of large collaborative idea generation platforms promise that by generating ideas together, people can create better ideas than any would have alone, But how might these platforms best leverage the number and diversity of contributors to help each contributor generate even better ideas ? Prior research suggests that seeing particularly creative or diverse ideas from others can inspire you, We see this work as a step toward building more effective online systems for supporting large scale collective ideation. But few scalable mechanisms exist to assess diversity, for evaluating the diversity of sets of ideas. Our validation study reveals that human raters agree with the estimates of dissimilarity derived from our idea map as much or more than they agree with each other, People seeing the diverse sets of examples from our idea map generate more diverse ideas than those seeing randomly selected examples, Our results also corroborate findings from prior research showing that people presented with creative examples generated more creative ideas than those who saw a set of random examples. We contribute a new scalable crowd-powered method The method relies on similarity comparisons ( is idea A more similar to B or C ) generated by non-experts to create an abstract spatial idea map.
2K_test_27	One main challenge in large creative online communities is helping their members find inspirational ideas from a large pool of ideas, A high-level approach to address this challenge is to create a synthesis of emerging solution space that can be used to provide participants with creative and diverse inspirational ideas of others. Existing approaches to generate the synthesis of solution space either require community members to engage in tasks that detract from the main activity of generating ideas or depend on external crowd workers to help organize the ideas. This feedback in turn helps the community identify diverse inspirational ideas that can prompt community members to generate more high-quality and diverse ideas. We built IDEAHOUND a collaborative idea generation system that demonstrates an alternative `` organic '' human computation approach, where community members ( rather than external crowds ) contribute feedback about ideas as a byproduct of an activity that naturally integrates into the ideation process.
2K_test_28	Previous work has shown the promise of crowdsourcing analogical idea generation, where distributing the stages of analogical processing across many people can reduce fixation, identify inspirations from more diverse domains, and lead to more creative ideas. However prior work has only considered problems with a single constraint, while many real-world problems involve multiple constraints for eliciting multiple constraints inherent in a problem and using those constraints to find inspirations useful in solving it To do so we identify methods to elicit useful constraints at different levels of abstraction, and empirical results that identify how the level of abstraction influences creative idea generation. Our results show that crowds find the most useful inspirations when the problem domain is represented abstractly and constraints are represented more concretely. This paper contributes a systematic crowdsourcing approach.
2K_test_29	People are more creative at solving difficult design problems when they use relevant examples from outside of the problem 's domain as inspirations. However finding such `` outside-the-box '' inspirations is difficult, particularly in large idea repositories such as the web, because without guidance people select domains to search based on surface similarity to the problem 's domain. We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own, or drew inspiration from domains closer to or not sharing structural correspondence with the original problem. In this paper we demonstrate an approach in which non-experts identify domains that have the potential to yield useful and non-obvious inspirations for solutions.
2K_test_30	Eye tracking is a compelling tool for revealing people 's spatial-temporal distribution of visual attention Such an approach will allow designers to evaluate and refine their visual design without requiring the use of limited/expensive eye trackers. But quality eye tracking hardware is expensive and can only be used with one person at a time, Further webcam eye tracking systems have significant limitations on head movement and lighting conditions that result in significant data loss and inaccuracies, To address these drawbacks. Which demonstrated good accuracy when compared to a real eye tracker, and showed that it accurately generated gaze heatmaps and trajectory maps. We introduce a new approach that harnesses the crowd to understand allocation of visual attention In our approach, crowdsourcing participants use mouse clicks to self-report the positions and trajectory for the following valuable eye tracking measures: first gaze, last gaze and all gazes. We validate our crowdsourcing approach with a user study, We then deployed our prototype, GazeCrowd in a crowdsourcing setting.
2K_test_31	Researchers and theorists have proposed that feelings of attachment to subgroups within a larger online community or site can increase users ' loyalty to the site, They have identified two types of attachment, with distinct causes and consequences With bond-based attachment, people feel connections to other group members, while with identity-based attachment they feel connections to the group as a whole. That these feelings of attachment to subgroups increase loyalty to the larger community. Communication with other people in a subgroup but not simple awareness of them increases attachment to the larger community, the experiments show that bond- and identity-based attachment have different causes, But the experiments show no evidence that bond and identity attachment have different consequences, We consider both theoretical and methodological reasons why the consequences of bond-based and identity-based attachment are so similar. By varying how the communication is structured, between dyads or with all group members simultaneously. In two experiments we show.
2K_test_32	People spend an enormous amount of time searching for complex information online ; for example, consumers researching new purchases or patients learning about their conditions for others with similar interests. As they search people build up rich mental schemas about their target domains ; which, if effectively shared could accelerate learning. Through a controlled experiment we show that having access to others ' schemas while foraging for information helps new users to induce more useful, prototypical and better-structured schemas than gathering information alone. In this paper we introduce a novel approach for integrating the schemas individuals develop as they gather information online and surfacing them for others with similar interests.
2K_test_33	Online collaboration tools enable developers of interactive systems to quickly reach potential users for usability testing Online needfinding may help designers create products and services that can target a more diverse user population. Can these technologies serve designers who seek feedback on user needs during the earliest stages of design We then introduce a tool for collecting early-stage design feedback from online participants and. We found that video can sufficiently capture nuanced reactions to preliminary concept storyboards, but that feedback providers need guidance and structure, The case study demonstrates that combining online crowdsourcing with a video survey tool provides a simple and cost-efficient way to collect early-stage feedback. To explore this we conducted a feasibility study to compare face-to-face methods with online needfinding sessions conduct a case study with a professional design team, The team conducted needfinding activities with local participants, as well as a cost-equivalent number of online participants.
2K_test_34	We examine several aggregate properties of deleted tweets, including their connections to other tweets ( e, whether they are replies or retweets ), the clients used to produce them, temporal aspects of deletion, and the presence of geotagging information. Some significant differences were discovered between the two collections, namely in the clients used to post them, their conversational aspects the sentiment vocabulary present in them, and the days of the week they were posted, However in other dimensions for which analysis was possible, no substantial differences were found, Finally we discuss some ramifications of this work for understanding Twitter usage and management of one 's privacy. This paper describes an empirical study of 1, 6M deleted tweets collected over a continuous one-week period from a set of 292K Twitter users.
2K_test_35	This paper presents a study of the life cycle of news articles posted online. We show that we can use this to characterize distinct classes of articles, We also find that social media reactions can help predict future visitation patterns early and accurately, We show that it is possible to model accurately the overall traffic articles will ultimately receive by observing the first ten to twenty minutes of social media reactions, Achieving the same prediction accuracy with visits alone would require to wait for three hours of data, We also describe significant improvements on the accuracy of the early prediction of shelf-life for news stories. We describe the interplay between website visitation patterns and social media reactions to news content hybrid observation method. We validate our methods using qualitative analysis as well as quantitative analysis on data from a large international news network, for a set of articles generating more than 3, 000 000 visits and 200, 000 social media reactions.
2K_test_36	A challenge for many online production communities is to direct their members to accomplish tasks that are important to the group, even when these tasks may not match individual members ' interests, Finally we discuss design and managerial implications based on our findings. Here we investigate how combining group identification and direction setting can motivate volunteers in online communities to accomplish tasks important to the success of the group as a whole We hypothesize that group identity, the perception of belonging to a group, triggers in-group favoritism ; and direction setting ( including explicit direction from group goals and implicit direction from role models ) focuses people 's group-oriented motivation towards the group 's important tasks. Results demonstrate that 1 ) publicizing important group goals via COTW can have a strong motivating influence on editors who have voluntarily identified themselves as group members compared to those who have not self-identified ; 2 ) the effects of goals spill over to non-goal related tasks ; and 3 ) editors exposed to group role models in COTW are more likely to perform similarly to the models on group-relevant citizenship behaviors. We tested our hypotheses in the context of Wikipedia 's Collaborations of the Week ( COTW ), a group goal setting mechanism and a social event within Wikiprojects.
2K_test_37	In crowd-collaborative innovation platforms, other contributors ' ideas can serve as sources of inspiration for creative ideas, We discuss implications for research and development of crowd-collaborative innovation platforms. But what patterns of interactions with others ' ideas are most helpful ? We investigate the hypothesis that building on inspiration sources that are conceptually far from one 's target domain are most helpful, a popular hypothesis with mixed empirical support. Surprisingly we find that innovators who cite conceptually near sources of inspiration achieve a higher success rate than those who prefer far sources. We predict the success rate of 2, 344 ideas for 12 different design challenges in a collaborative Web-based innovation platform based on their cited sources ' conceptual distance from the target domain ( measured using probabilistic topic modeling of the ideas ).
2K_test_38	Thus improving their ability to facilitate one-time or spontaneous exchanges of information. We present a new technique that allows mobile devices to opportunistically group with one another, we examine the limitations of using a single type of context to form groups. Show how leveraging multiple contexts improves our ability to detect and form relevant groupings Through two prototypes, we demonstrate how DIDJA enhances existing user experiences, and show how developers can use our toolkit to easily facilitate frictionless collaborations between users and their environment We then perform an extended experiment and show how DIDJA is able to accurately form groups under realistic conditions. In our approach devices share context with each other, and form groups when these readings are found to be similar to one another Through a formative study, We then present DIDJA, a robust software toolkit that automatically collects and analyzes contextual information in order to find and form groups.
2K_test_39	A significant challenge for crowdsourcing has been increasing worker engagement and output quality The findings of this paper provide strategies for harnessing the crowd to perform complex tasks, as well as insight into crowd workers ' motivation. We explore the effects of social, learning and financial strategies, and their combinations on increasing worker retention across tasks and change in the quality of worker output. We show that 1 ) using these strategies together increased workers ' engagement and the quality of their work ; 2 ) a social strategy was most effective for increasing engagement ; 3 ) a learning strategy was most effective in improving quality.
2K_test_40	HomeProxy is a research prototype that uses a physical proxy to support video messaging at home among distributed family members. We designed and implemented a prototype and our early experiences with it indicate the promise of offering quick video messaging at home and the challenges of a no-touch interface. A physical artifact dedicated to remote family members makes it easier to chat with them over video, HomeProxy combines a form factor designed for the home environment with a `` no-touch '' user experience and an interface that quickly transitions between recorded and live video communication.
2K_test_41	Sharing scientific data software, and instruments is becoming increasingly common as science moves toward large-scale, Sharing these resources requires extra work to make them generally useful, Although we know much about the extra work associated with sharing data, Our results have important implications for future empirical studies as well as funding policy. We know little about the work associated with sharing contributions to software, even though software is of vital importance to nearly every scientific result. Our findings indicate that they conduct a rich set of extra work around community management, code maintenance education and training, developer-user interaction and foreseeing user needs, We identify several conditions under which they are likely to do this work, as well as design principles that can facilitate it. This paper presents a qualitative, interview-based study of the extra work that developers and end users of scientific software undertake.
2K_test_42	Crowd feedback systems offer designers an emerging approach for improving their designs. But there is little empirical evidence of the benefit of these systems This paper reports the results of a study of using a crowd feedback system to iterate on visual designs. Results showed that the crowd feedback system prompted deep and cosmetic changes and led to improved designs, the crowd recognized the design improvements, and structured workflows generated more interpretative, diverse and critical feedback than free-form prompts. Users in an introductory visual design course created initial designs satisfying a design brief and received crowd feedback on the designs, Users revised the designs and the system was used to generate feedback again, This format enabled us to detect the changes between the initial and revised designs and how the feedback related to those changes, Further we analyzed the value of crowd feedback by comparing it with expert evaluation and feedback generated via free-form prompts.
2K_test_43	In a variety of peer production settings, from Wikipedia to open source software development to crowdsourcing, individuals may encounter edit, or review the work of unknown others, Typically this is done without much context to the person 's past behavior or performance, This work provides insight into the impact of activity history design factors on psychological and behavioral outcomes that can be of use in other related settings. To understand how exposure to an unknown individual 's activity history influences attitudes and behaviors. Surprisingly negative work history did not lead to negative outcomes, but in contrast a positive work history led to positive initial impressions that persisted in the face of contrary information. We conducted an online experiment on Mechanical Turk varying the content, quality and presentation of information about another Turker 's work history.
2K_test_44	When personalities clash teams operate less effectively Personality differences affect face-to-face collaboration and may lower trust in virtual teams This work demonstrates a simple personality matching strategy for forming more effective teams in crowdsourcing contexts. For relatively short-lived assignments, like those of online crowdsourcing, personality matching could provide a simple, scalable strategy for effective team formation, However it is not clear how ( or if ) personality differences affect teamwork in this novel context where the workforce is more transient and diverse This study examines how personality compatibility in crowd teams affects performance and individual perceptions. Results show that balancing for personality leads to significantly better performance on a collaborative task Balanced teams exhibited less conflict and their members reported higher levels of satisfaction and acceptance. Using the DISC personality test, we composed 14 five-person teams ( N=70 ) with either a harmonious coverage of personalities ( balanced ) or a surplus of leader-type personalities ( imbalanced.
2K_test_45	We close by identifying several ways in which crowd labor platform operators and/or individual task requestors could improve the accessibility of this increasingly important form of employment. We present the first formal study of crowdworkers who have disabilities. Our findings establish that people with a variety of disabilities currently participate in the crowd labor marketplace, despite challenges such as crowdsourcing workflow designs that inadvertently prohibit participation by, and may negatively affect the worker reputations of, people with disabilities Despite such challenges, we find that crowdwork potentially offers different opportunities for people with disabilities relative to the normative office environment, such as job flexibility and lack of a need to rely on public transit. Via in-depth open-ended interviews of 17 people ( disabled crowdworkers and job coaches for people with disabilities ) and a survey of 631 adults with disabilities.
2K_test_46	Increasingly the advice people receive on the Internet is socially transparent in the sense that it displays contextual information about the advice-givers or their actions CSCW research usually emphasizes how to increase information sharing ; this work suggests when shared information may be inappropriate We suggest ways to counter activity transparency 's potential downsides. We hypothesize that activity transparency -seeing an advice giver 's process while creating his or her recommendations - will increase advice taking, testing the effect of activity transparency on taking mediocre advice. We found that the presence of a web history increased the likelihood of following a financial advisor 's advice and reduced participant earnings ( Exp, 1 ) especially when the web history implied greater task focus ( Exp. We report three experiments.
2K_test_47	Social networking sites ( SNSs ) offer users a platform to build and maintain social connections, Understanding when people feel comfortable sharing information about themselves on SNSs is critical to a good user experience, because self-disclosure helps maintain friendships and increase relationship closeness. This observational research develops a machine learning model to measure self-disclosure in SNSs and uses it to understand the contexts where it is higher or lower, To validate the model and advance our understanding about online self-disclosure. Results show that women self-disclose more than men, People with a stronger desire to manage impressions self-disclose less, Network size is negatively associated with self-disclosure, while tie strength and network density are positively associated. Features include emotional valence, social distance between the poster and people mentioned in the post, the language similarity between the post and the community and post topic. We applied it to de-identified, aggregated status updates from Facebook users.
2K_test_48	Participatory sensing systems ( PSS ) require frequent injection of information that has a short shelf-life The use of crowds to gather information for PSS is therefore particularly challenging, Prior research has shown that request for help in crowdsourced system is an effective mechanism to increase contributions, Thus crowdsource system designers should consider imposing quid-pro-quo type policies for PSS that concentrate on fewer users, but makes them more productive. In this study we explore the impact of two policies on user contributions, A quid-pro-quo policy exchanges contributions from users for access to critical information in the system, A request policy simply reminds the user that information is needed to make the system function well. Our results confirmed that quid-pro-quo led to more contribution, but at a cost of faster departure from the study, When a participant was simply requested to contribute, but could still access community-generated data if they ignored a request, was largely ineffective and was statistically similar to the control condition where no request for contribution occurred. During a large-scale experimental study within a publicly deployed, crowdsourced transit information system, we analyzed metrics associated with frequency of contribution and commitment to long-term use over a 10-month period.
2K_test_49	Expert feedback is valuable but hard to obtain for many designers. Online crowds can provide fast and affordable feedback, but workers may lack relevant domain knowledge and experience Can expert rubrics address this issue and help novices provide expert-level feedback ?. We found that rubrics helped novice workers provide feedback that was rated nearly as valuable as expert feedback showed that student designers found feedback most helpful when it was emotionally positive and specific, and that a rubric increased the occurrence of these characteristics in feedback The analysis also found that expertise correlated with longer critiques, but not the other favorable characteristics, indicates that experts may instead have produced value by providing clearer justifications. To evaluate this we conducted an experiment with a 2x2 factorial design, Student designers received feedback on a visual design from both experts and novices, who produced feedback using either an expert rubric or no rubric A follow-up analysis on writing style An informal evaluation.
2K_test_50	To predict outcome measures such quality, errors and the likelihood of cheating, particularly as applied to crowd sourced tasks. A novel method of using task fingerprinting The technique focuses on the way workers work rather than the products they produce, The technique captures behavioral traces from online crowd workers and uses them to build predictive models of task performance. The effectiveness of the approach is evaluated across three contexts including classification, generation and comprehension tasks.
2K_test_53	Fast Fourier transform algorithms on large data sets achieve poor performance on various platforms because of the inefficient strided memory access patterns. These inefficient access patterns need to be reshaped to achieve high performance implementations we formally restructure memory access pattern efficient algorithms. We demonstrate DRAM-optimized accelerator designs over a large tradeoff space given various problem ( single/double precision 1D, 2D and 3D FFTs ) and hardware platform ( off-chip DRAM, 3D-stacked DRAM ASIC FPGA, We show that Spiral generated pareto optimal designs can achieve close to theoretical peak performance of the targeted platform offering 6x and 6, 5x system performance and power efficiency improvements respectively over conventional row-column FFT algorithms. In this paper 1D, 2D and 3D FFTs targeting a generic machine model with a two-level memory hierarchy requiring block data transfers, and derive using custom block data layouts These algorithms need to be carefully mapped to the targeted platform 's architecture, particularly the memory subsystem, to fully utilize performance and energy efficiency potentials Using the Kronecker product formalism, we integrate our optimizations into Spiral framework. And evaluate a family of DRAM-optimized FFT algorithms and their hardware implementation design space via automated techniques.
2K_test_54	Languages for music audio processing typically offer a large assortment of unit generators There is great duplication among different language implementations, as each language must implement many of the same ( or nearly the same ) unit generators, We suggest that these techniques might eliminate most of the effort of building unit generator libraries and could help with the implementation of embedded audio systems where unit generators are needed but a full embedded Csound engine is not required. Csound has a large library of unit generators and could be a useful source of reusable unit generators for other languages or for direct use in applications, In this study we consider how Csound unit generators can be exposed to direct access by other audio processing languages. Using Aura as an example, we modified Csound to allow efficient, dynamic allocation of individual unit generators without using the Csound compiler or writing Csound instruments, We then extended Aura using automatic code generation so that Csound unit generators can be accessed in the normal way from within Aura, In this scheme Csound details are completely hidden from Aura users.
2K_test_55	The results suggest that 1 ) the online disclosure of certain personal traits can influence the hiring decisions of U, firms and 2 ) the likelihood of hiring discrimination via online searches varies across employers, The findings also highlight the surprisingly lasting behavioral influence of traditional, offline networks in processes and scenarios where online interactions are becoming increasingly common. We investigate whether personal information posted by job candidates on social media sites is sought and used by prospective employers. We find evidence of employers searching online for the candidates, we find no difference in callback rates for the gay candidate compared to the straight candidate, but a 13 % lower callback rate for the Muslim candidate compared to the Christian candidate, While the difference is not significant at the national level, it exhibits significant and robust heterogeneity in bias at the local level, compatible with existing theories of discrimination, In particular employers in Republican areas exhibit significant bias both against the Muslim candidate, and in favor of the Christian candidate This bias is significantly larger than the bias in Democratic areas. We create profiles for job candidates on popular social networks, manipulating information protected under U, laws and submit job applications on their behalf to over 4, 000 employers After comparing interview invitations for a Muslim versus a Christian candidate, and a gay versus a straight candidate, The results are robust to using state- and county-level data, to controlling for firm, job and geographical characteristics, and to several model specifications.
2K_test_56	Motivated by a radically new peer review system that the National Science Foundation recently experimented with. We study peer review systems in which proposals are reviewed by PIs who have submitted proposals themselves. An ( m ; k ) -selection mechanism asks each PI to review m proposals, and uses these reviews to select ( at most ) k proposals, We are interested in impartial mechanisms, which guarantee that the ratings given by a PI to others ' proposals do not affect the likelihood of the PI 's own proposal being selected We design an impartial mechanism that selects a k-subset of proposals that is nearly as highly rated as the one selected by the non-impartial ( abstract version of ) the NSF pilot mechanism, even when the latter mechanism has the `` unfair '' advantage of eliciting honest reviews.
2K_test_57	The fairness notion of maximin share ( MMS ) guarantee underlies a deployed algorithm for allocating indivisible goods under additive valuations, Previous work has shown that such an MMS allocation may not exist, but the counterexample requires a number of goods that is exponential in the number of players ;. Our goal is to understand when we can expect to be able to give each player his MMS guarantee, that provably finds an MMS allocation with high probability. We give a new construction that uses only a linear number of goods, On the positive side, we formalize the intuition that these counterexamples are very delicate by designing an algorithm when valuations are drawn at random.
2K_test_58	A paradigmatic problem in social choice theory deals with the aggregation of subjective preferences of individuals represented as rankings of alternatives into a social ranking. We are interested in settings where individuals are uncertain about their own preferences, and represent their uncertainty as distributions over rankings. We show that ignoring uncertainty altogether can lead to suboptimal outcomes. Under the classic objective of minimizing the ( expected ) sum of Kendall tau distances between the input rankings and the output ranking, we establish that preference elicitation is surprisingly straightforward and near-optimal solutions can be obtained in polynomial time. Both in theory and using real data.
2K_test_60	Simultaneously reverse engineering a collection of condition-specific gene networks from gene expression microarray data to uncover dynamic mechanisms is a key challenge in systems biology. However existing methods for this task are very sensitive to variations in the size of the microarray samples across different biological conditions ( which we term sample size heterogeneity in network reconstruction ), and can potentially produce misleading results that can lead to incorrect biological interpretation that addresses this novel problem. We show the quantitative advantages of our approach reveals interesting results, some of which are confirmed by previously validated results. In this work we develop a more robust framework Just like microarray measurements across conditions must undergo proper normalization on their magnitudes before entering subsequent analysis, we argue that networks across conditions also need to be normalized on their density when they are constructed, and we provide an algorithm that allows such normalization to be facilitated while estimating the networks. On synthetic and real data Our analysis of a hematopoietic stem cell dataset.
2K_test_61	We study the envy-free allocation of indivisible goods between two players To rigorously quantify the efficiency gain from selling. We show that envy-free allocations of sellable goods are significantly more efficient than their unsellable counterparts. Our novel setting includes an option to sell each good for a fraction of the minimum value any player has for the good, we reason about the price of envy-freeness of allocations of sellable goods -- the ratio between the maximum social welfare and the social welfare of the best envy-free allocation.
2K_test_62	Some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives. It seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule, which simply counts the number of times each alternative was approved, We challenge this assertion. Results call attention to situations where approval voting is suboptimal. By proposing a probabilistic framework of noisy voting, and asking whether approval voting yields an alternative that is most likely to be the best alternative. While the answer is generally positive, our theoretical and empirical.
2K_test_63	Personal photos are enjoying explosive growth with the popularity of photo-taking devices and social media, The vast amount of online photos largely exhibit users ' interests. Mining user interests from personal photos can boost a number of utilities, such as advertising interest based community detection and photo recommendation, In this paper we study the problem of user interests mining from personal photos, to jointly model user interests and image contents. Demonstrate the effectiveness of our model. We propose a User Image Latent Space Model User interests are modeled as latent factors and each user is assumed to have a distribution over them, By inferring the latent factors and users ' distributions, we can discover what the users are interested in We model image contents with a four-level hierarchical structure where the layers correspond to themes, semantic regions visual words and pixels respectively, Users ' latent interests are embedded in the theme layer, Given image contents users ' interests can be discovered by doing posterior inference, We use variational inference to approximate the posteriors of latent variables and learn model parameters. Experiments on 180K Flickr photos.
2K_test_64	For cake cutting ( the fair allocation of a divisible good ), in which agents simultaneously send messages containing a sketch of their preferences over the cake. We introduce the simultaneous model We show that this model enables the computation of divisions that satisfy proportionality -- a popular fairness notion -- using a protocol that circumvents a standard lower bound via parallel information elicitation, Cake divisions satisfying another prominent fairness notion, envy-freeness are impossible to compute in the simultaneous model, but admit arbitrarily good approximations.
2K_test_65	Motivated by applications to crowdsourcing. We study voting rules that output a correct ranking of alternatives by quality from a large collection of noisy input rankings We seek voting rules that are supremely robust to noise, in the sense of being correct in the face of any `` reasonable '' type of noise. We show that there is such a voting rule, which we call the modal ranking rule, Moreover we establish that the modal ranking rule is the unique rule with the preceding robustness property within a large family of voting rules, which includes a slew of well-studied rules.
2K_test_66	Classic social choice theory assumes that votes are independent ( but possibly conditioned on an underlying objective ground truth ). This assumption is unrealistic in settings where the voters are connected via an underlying social network structure, as social interactions lead to correlated votes, for ranked voting to recover the ground truth. We establish a general framework -- based on random utility theory -- on a social network with arbitrarily many alternatives ( in contrast to previous work, which is restricted to two alternatives ), We identify a family of voting rules which, without knowledge of the social network structure, are guaranteed with high probability in large networks, with respect to a wide range of models of correlation among input votes.
2K_test_67	Limited lookahead has been studied for decades in perfect-information games. This paper initiates a new direction via two simultaneous deviation points : generalization to imperfect-information games and a game-theoretic approach The question of how one should act when facing an opponent whose lookahead is limited is studied along multiple axes : lookahead depth, whether the opponent ( s ), too have imperfect information, and how they break ties for computing optimal commitment strategies. The limited-lookahead player often obtains the value of the game if she knows the expected values of nodes in the game tree for some equilibrium, but we prove this is not sufficient in general, This uncovers a lookahead pathology. We characterize the hardness of finding a Nash equilibrium or an optimal commitment strategy for either player, showing that in some of these variations the problem can be solved in polynomial time while in others it is PPAD-hard or NP-hard, We proceed to design algorithms for when the opponent breaks ties 1 ) favorably, 2 ) according to a fixed rule, or 3 ) adversarially. The impact of limited lookahead is then investigated experimentally Finally, we study the impact of noise in those estimates and different lookahead depths.
2K_test_68	For solving a linear system arising from the 1-Laplacian corresponding to a collapsible simplicial complex with a known collapsing sequence. We present an efficient algorithm When combined with a result of Chillingworth, our algorithm is applicable to convex simplicial complexes embedded in R3 The running time of our algorithm is nearly-linear in the size of the complex and is logarithmic on its numerical properties, Our algorithm is based on projection operators and combinatorial steps for transferring between them The former relies on decomposing flows into circulations and potential flows using fast solvers for graph Laplacians, and the latter relates Gaussian elimination to topological properties of simplicial complexes.
2K_test_69	Classic cake cutting protocols are susceptible to manipulation, Do their strategic outcomes still guarantee fairness ? To address this question. Specifically we show that each protocol in the class of generalized cut and choose ( GCC ) protocols which includes the most important discrete cake cutting protocols is guaranteed to have approximate subgame perfect Nash equilibria, or even exact equilibria if the protocol 's tie-breaking rule is flexible, We further observe that the ( approximate ) equilibria of proportional protocols which guarantee each of the n agents a 1/n-fraction of the cake must be ( approximately ) proportional, thereby answering the above question in the positive ( at least for one common notion of fairness ). We adopt a novel algorithmic approach, proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model. We study the paradigmatic fair division problem of fairly allocating a divisible good among agents with heterogeneous preferences, commonly known as cake cutting.
2K_test_70	For performing accelerated data processing. This disclosure relates to a three-dimensional ( 3D ) integrated circuit ( 3DIC ) memory chip including computational logic-in-memory ( LiM ) Related memory systems and methods are also disclosed In one embodiment, the 3DIC memory chip includes at least one memory layer that provides a primary memory configured to store data, The 3DIC memory chip also includes a computational LiM layer, The computational LiM layer is a type of memory layer having application-specific computational logic integrated into local memory while externally appearing as regular memory The computational LiM layer and the primary memory are interconnected through through-silica vias ( TSVs ), In this manner the computational LiM layer may load data from the primary memory with the 3DIC memory chip without having to access an external bus coupling the 3DIC memory chip to a central processing unit ( CPU ) or other processors to computationally process the data and generate a computational result.
2K_test_71	An adversary who has obtained the cryptographic hash of a user 's password can mount an offline attack to crack the password by comparing this hash value with the cryptographic hashes of likely password guesses, This offline attacker is limited only by the resources he is willing to invest to crack the password, Key-stretching techniques like hash iteration and memory hard functions have been proposed to mitigate the threat of offline attacks by making each password guess more expensive for the adversary to verify. However these techniques also increase costs for a legitimate authentication server, which captures the essential elements of this interaction between a defender and an offline attacker that minimizes the fraction of passwords that would be cracked by a rational offline attacker. Our analysis shows that CASH can significantly reduce ( up to 50 % ) the fraction of password cracked by a rational offline adversary. We introduce a novel Stackelberg game model In the game the defender first commits to a key-stretching mechanism, and the offline attacker responds in a manner that optimizes his utility ( expected reward minus expected guessing costs ), We then introduce Cost Asymmetric Secure Hash ( CASH ), a randomized key-stretching mechanism without increasing amortized authentication costs for the legitimate authentication server CASH is motivated by the observation that the legitimate authentication server will typically run the authentication procedure to verify a correct password, while an offline adversary will typically use incorrect password guesses, By using randomization we can ensure that the amortized cost of running CASH to verify a correct password guess is significantly smaller than the cost of rejecting an incorrect password Using our Stackelberg game framework we can quantify the quality of the underlying CASH running time distribution in terms of the fraction of passwords that a rational offline adversary would crack, We provide an efficient algorithm to compute high quality CASH distributions for the defender. Finally we analyze CASH using empirical data from two large scale password frequency datasets.
2K_test_73	Abstract : Given a large network, changing over time how can we find patterns and anomalies ? which can discover both transient and periodic/ repeating communities. Com2 spots intuitive patterns, that is temporal communities ( comet communities ), We report our findings, which include large star-like patterns, near-bipartite-cores as well as tiny groups ( 5 users ), calling each other hundreds of times within a few days. We propose Com2 a novel and fast, incremental tensor analysis approach, The method is ( a ) scalable, being linear on the input size ( b ) general, ( c ) needs no user-defined parameters and ( d ) effective, returning results that agree with intuition. We apply our method on real datasets, including a phone-call network and a computer-traffic network The phone call network consists of 4 million mobile users, with 51 million edges ( phonecalls ).
2K_test_75	Many graph mining and analysis services have been deployed on the cloud, which can alleviate users from the burden of implementing and maintaining graph algorithms. However putting graph analytics on the cloud can invade users ' privacy, To solve this problem, to preserve the privacy of both users ' graph data and the analytic results, In this paper we present how to encrypt a graph using homomorphic encryption and how to query the structure of an encrypted graph by computing polynomials To solve the problem that certain operations are not executable on encrypted graphs, to seek help from users. We show how to apply our methods to perform analytics on encrypted graphs, demonstrate the correctness and feasibility of our methods. We propose CryptGraph which runs graph analytics on encrypted graph In CryptGraph, users encrypt their graphs before uploading them to the cloud, The cloud runs graph analysis on the encrypted graphs and obtains results which are also in encrypted form that the cloud can not decipher, During the process of computing, the encrypted graphs are never decrypted on the cloud side, The encrypted results are sent back to users and users perform the decryption to obtain the plaintext results, In this process users ' graphs and the analytics results are both encrypted and the cloud knows neither of them Thereby, users ' privacy can be strongly protected, Meanwhile with the help of homomorphic encryption, the results analyzed from the encrypted graphs are guaranteed to be correct, we propose hard computation outsourcing. Using two graph algorithms as examples, Experiments on two datasets.
2K_test_77	A key challenge in solving extensive-form games is dealing with large, or even infinite action spaces, In games of imperfect information, the leading approach is to find a Nash equilibrium in a smaller abstract version of the game that includes only a few actions at each decision point, and then map the solution back to the original game. However it is difficult to know which actions should be included in the abstraction without first solving the game, and it is infeasible to solve the game without first abstracting it. Show it can outperform fixed abstractions at every stage of the run : early on it improves as quickly as equilibrium finding in coarse abstractions, and later it converges to a better solution than does equilibrium finding in fine-grained abstractions. We introduce a method that combines abstraction with equilibrium finding by enabling actions to be added to the abstraction at run time, This allows an agent to begin learning with a coarse abstraction, and then to strategically insert actions at points that the strategy computed in the current abstraction deems important The algorithm can quickly add actions to the abstraction while provably not having to restart the equilibrium finding, It enables anytime convergence to a Nash equilibrium of the full game even in infinite games.
2K_test_78	The success of Amazon Mechanical Turk ( MTurk ) as an online research platform has come at a price : MTurk exhibits slowing rates of population replenishment, and growing participants non-naivety. Recently a number of alternative platforms have emerged, offering capabilities similar to MTurk while providing access to new and more naive populations. We found that both platforms participants were more naive and less dishonest compared to MTurk, CF showed the best response rate, but CF participants failed more attention-check questions and did not reproduce known effects replicated on ProA and MTurk Moreover, ProA participants produced data quality that was higher than CFs and comparable to MTurks. We examined two such platforms, CrowdFlower ( CF ) and Prolific Academic ( ProA ).
2K_test_79	Anecdotal evidence and scholarly research have shown that a significant portion of Internet users experience regrets over their online disclosures We discuss limitations of the current nudge designs and future directions for improvement. To help individuals avoid regrettable online disclosures to design mechanisms that `` nudge '' users to consider the content and context of their online disclosures before posting them. Our system logs results from exit surveys, and interviews suggest that privacy nudges could be a promising way to prevent unintended disclosure. We employed lessons from behavioral decision research and research on soft paternalism We developed three such privacy nudges on Facebook, The first nudge provides visual cues about the audience for a post, The second nudge introduces time delays before a post is published, The third nudge gives users feedback about their posts. We tested the nudges in a three-week exploratory field trial with 21 Facebook users, and conducted 13 follow-up interviews.
2K_test_81	We study cyber-physical systems subject to dynamic sensor attacks, relating them to the system 's strong observability. Finally we illustrate our results. First we find necessary and sufficient conditions for an attacker to create a dynamically undetectable sensor attack and relate these conditions to properties of the system dynamics eigenvectors, Next we provide an index that gives the minimum number of sensors that must be attacked in order for an attack to be undetectable. With a numerical example on the Quadruple Tank Process.
2K_test_82	Tree structured graphical models are powerful at expressing long range or hierarchical dependency among many variables, and have been widely applied in different areas of computer science and statistics. However existing methods for parameter estimation, inference and structure learning mainly rely on the Gaussian or discrete assumptions, which are restrictive under many applications, that can recover the latent tree structures, estimate the parameters and perform inference for high dimensional continuous and non-Gaussian variables. The usefulness of the proposed methods are illustrated. In this paper we propose new nonparametric methods based on reproducing kernel Hilbert space embeddings of distributions. By thorough numerical results.
2K_test_83	One common problem plaguing crowdsourcing tasks is tuning the set of worker responses : Depending on task requirements, requesters may want a large set of rich and varied worker responses ( typically in subjective evaluation tasks ) or a more convergent response-set ( typically for more objective tasks such as fact-checking ), This problem is especially salient in tasks that combine workers responses to present a single output : Divergence in these settings could either add richness and complexity to the unified answer, that allow requesters to tune different levels of convergence in worker participation for different tasks. In this paper we present HiveMind, a system of methods simply by adjusting the value of one variable.
2K_test_84	Active learning has shown to reduce the number of exper- iments needed to obtain high-confidence drug-target predictions, How- ever in order to actually save experiments using active learning, it is crucial to have a method to evaluate the quality of the current pre- diction and decide when to stop the experimentation process. Only by applying reliable stoping criteria to active learning, time and costs in the experimental process can be actually saved a for the accuracy of the active learner. That applying the stopping criteria can result in upto 40 % savings of the total experiments for highly accurate predictions. We compute active learning traces on simulated drug-target matrices in order to learn regression model By analyzing the perfor- mance of the regression model on simulated data, we design stopping criteria for previously unseen experimental matrices. We demonstrate on four previously characterized drug effect data sets.
2K_test_85	These results show that this system could be a valuable addition to vehicle anomaly detection and safety systems. That is capable of predicting the speed and gear position of a moving vehicle While audio classification is widely used in other research areas such as music information retrieval and bioacoustics, its application to vehicle sounds is rare. The experiment shows that the system is capable of predicting the vehicle speed and gear position with near-perfect accuracy over 99 %. This paper presents a machine learning system from the sound it makes Therefore, we investigate predicting the state of a vehicle using audio features in a classification task We improve the classification results using correlation matrices, calculated from signals correlating with the audio. In an experiment the sound of a moving vehicle is classified into discretized speed intervals and gear positions.
2K_test_86	Training large machine learning ( ML ) models with many variables or parameters can take a long time if one employs sequential procedures even with stochastic updates. A natural solution is to turn to distributed computing on a cluster ; however, naive unstructured parallelization of ML algorithms does not usually lead to a proportional speedup and can even result in divergence, because dependencies between model elements can attenuate the computational gains from parallelization and compromise correctness of inference Recent efforts toward this issue have benefited from exploiting the static, a priori block structures residing in ML algorithms, In this paper we take this path further for coordinating distributed updates in ML algorithms. We provide theoretical guarantees for our scheduler, and demonstrate its efficacy versus static block structures. We propose and showcase a general-purpose scheduler, STRADS  which harnesses the aforementioned opportunities in a systematic way. By exploring the dynamic block structures and workloads therein present during ML program execution, which offers new opportunities for improving convergence, correctness and load balancing in distributed ML, on Lasso and Matrix Factorization.
2K_test_87	Interface-confinement is a common mechanism that secures untrusted code by executing it inside a sandbox, The sandbox limits ( confines ) the code 's interaction with key system resources to a restricted set of interfaces This practice is seen in web browsers, hypervisors and other security-critical systems. For modeling and proving safety properties of systems that execute adversary-supplied code via interface-confinement, to specify the properties of interface-confined code. And prove the soundness of System M relative to the model. Motivated by these systems, we present a program logic, called System M In addition to using computation types to specify effects of computations, System M includes a novel invariant type The interpretation of invariant type includes terms whose effects satisfy an invariant We construct a step-indexed model built over traces System M is the first program logic that allows proofs of safety for programs that execute adversary-supplied code without forcing the adversarial code to be available for deep static analysis System M can be used to model and verify protocols as well as system designs. We demonstrate the reasoning principles of System M by verifying the state integrity property of the design of Memoir, a previously proposed trusted computing system.
2K_test_88	We investigate a notion of behavioral genericity in the context of session type disciplines. Combined our results confer strong correctness guarantees for communicating systems. To this end we develop a logically motivated theory of parametric polymorphism, reminiscent of the Girard-Reynolds polymorphic -calculus, but casted in the setting of concurrent processes, In our theory polymorphism accounts for the exchange of abstract communication protocols and dynamic instantiation of heterogeneous interfaces, as opposed to the exchange of data types and dynamic instantiation of individual message types, Our polymorphic session-typed process language satisfies strong forms of type preservation and global progress, is strongly normalizing and enjoys a relational parametricity principle In particular, parametricity is key to derive non-trivial results about internal protocol independence, a concurrent analogous of representation independence, and non-interference properties of modular.
2K_test_89	A dataset has been classified by some unknown classifier into two types of points, What were the most important factors in determining the classification outcome ?. In order to uniquely characterize an influence measure : a function that, given a set of classified points, outputs a value for each feature corresponding to its influence in determining the classification outcome. In this work we employ an axiomatic approach We show that our influence measure takes on an intuitive form when the unknown classifier is linear. Finally we employ our influence measure in order to analyze the effects of user profiling on Google 's online display advertising.
2K_test_90	The goal is to recover the support union of all regression vectors using $ l_1/l_2 $ -regularized Lasso. Namely if the sample size is above the threshold, then $ l_1/l_2 $ -regularized Lasso correctly recovers the support union ; and if the sample size is below the threshold, $ l_1/l_2 $ -regularized Lasso fails to recover the support union, In particular the threshold precisely captures the impact of the sparsity of regression vectors and the statistical properties of the design matrices on sample complexity Therefore, the threshold function also captures the advantages of joint support union recovery using multi-task Lasso over individual support recovery using single-task Lasso. We characterize sufficient and necessary conditions on sample complexity \emph { as a sharp threshold } to guarantee successful recovery of the support union. In this paper we investigate a multivariate multi-response ( MVMR ) linear regression problem, which contains multiple linear regression models with differently distributed design matrices, and different regression and output vectors.
2K_test_91	Upsampling of a multi-dimensional data-set is an operation with wide application in image processing and quantum mechanical calculations using density functional theory, For small up sampling factors as seen in the quantum chemistry code ONETEP, a time-shift based implementation that shifts samples by a fraction of the original grid spacing to fill in the intermediate values using a frequency domain Fourier property can be a good choice Readily available highly optimized multidimensional FFT implementations are leveraged at the expense of extra passes through the entire working set. An optimized variant of the time-shift based up sampling. We demonstrate speed-ups in isolation averaging 3x and within ONETEP of up to 15 %. In this paper we present Since ONETEP handles threading, we address the memory hierarchy and SIMD vectorization, and focus on problem dimensions relevant for ONETEP, We present a formalization of this operation within the SPIRAL framework and demonstrate auto-generated and auto-tuned interpolation libraries. We compare the performance of our generated code against the previous best implementations using highly optimized FFT libraries ( FFTW and MKL.
2K_test_92	When asked to mentally simulate coin tosses, people generate sequences that differ systematically from those generated by fair coins, It has been rarely noted that this divergence is apparent already in the very 1st mental toss This bias has far-reaching implications extending well beyond the context of randomness cognition ; in particular, to binary surveys ( e, reject ) and tests ( e, In binary choice there is an advantage to what presents first. We offer a comprehensive account. Reveals that about 80 % of respondents start their sequence with Heads, We attributed this to the linguistic convention describing coin toss outcomes as Heads or Tails, found the first-toss bias reversible. In terms of a novel response bias, which we call reachability, It is more general than the 1st-toss bias, and it reflects the relative ease of reaching 1 option compared to its alternative in any binary choice context, When faced with a choice between 2 options ( e, Heads and Tails when tossing mental coins ), whichever of the 2 is presented first by the choice architecture ( hence, is more reachable ) will be favored. Analysis of several existing data sets However, our subsequent experiments under minor changes in the experimental setup, such as mentioning Tails before Heads in the instructions.
2K_test_93	This paper proves the asymptotic convergence of the meansquared error ( MSE ). This result shows that our distributed Kalman filter can track with bounded MSE any arbitrary linear dynamics. Of a distributed Kalman filter that we have previously proposed.
2K_test_94	Given information about medical drugs and their properties, how can we automatically discover that Aspirin has blood-thinning properties, and thus prevents Expressed in more general terms, if we have a large in- formation network that integrates data from heterogeneous data sources, how can we extract semantic information that provides a better understanding of the in- tegrated data and also helps us to identify missing links ?. We propose to extract concepts that describe groups of objects and their common properties from the integrated data. We demonstrate the effectiveness and scalability of the proposed method. The discovered concepts provide semantic information as well as an abstract view on the integrated data and thus improve the understanding of complex systems, Our proposed method has the following desirable properties : ( a ) it is parameter-free and therefore requires no user-defined parameters ( b ) it is fault-tolerant, allowing for the detection of missing links and ( c ) it is scalable, being linear on the input size. On real publicly available graphs.
2K_test_95	Multilinear analysis is pervasive in a wide variety of fields, ranging from Signal Processing to Chemometrics, and from Machine Vision to Data Mining, Determining the quality of a given tensor decomposition is a task of utmost importance that spans all fields of application of tensors, This task by itself is hard in its nature, since even determining the rank of a tensor is an NP-hard problem, Fortunately there exist heuristics in the literature that can be effectively used for this task ; one of these heuristics is the so-called Core Consistency Diagnostic ( CORCONDIA ) which is very intuitive and simple, However simple computation of this diagnostic proves to be a very daunting task even for data of medium scale, let alone big tensor data. With the increase of the size of the tensor data that need to be analyzed there grows the need for efficient and scalable algorithms to compute diagnostics such as CORCONDIA, in order to assess the modelling quality. In this work we derive a fast and exact algorithm for CORCONDIA which exploits data sparsity and scales very well as the tensor size increases.
2K_test_96	As Machine Learning ( ML ) applications embrace greater data size and model complexity, practitioners turn to distributed clusters to satisfy the increased computational and memory demands Effective use of clusters for ML programs requires considerable expertise in writing distributed code, but existing highly-abstracted frameworks like Hadoop that pose low barriers to distributed-programming have not, in practice matched the performance seen in highly specialized and advanced ML implementations, The recent Parameter Server ( PS ) paradigm is a middle ground between these extremes, allowing easy conversion of single-machine parallel ML programs into distributed ones, while maintaining high throughput through relaxed `` consistency models '' that allow asynchronous ( and, hence inconsistent ) parameter reads. However due to insufficient theoretical study, it is not clear which of these consistency models can really ensure correct ML algorithm output ; at the same time, there remain many theoretically-motivated but undiscovered opportunities to maximize computational throughput that enables ML programs to reach their solution more quickly. We then use the gleaned insights to improve a consistency model using an `` eager '' PS communication mechanism, and implement it as a new PS system. Inspired by this challenge, we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models.
2K_test_97	Principal Component Analysis ( PCA ) has wide applications in machine learning, text mining and computer vision, Classical PCA based on a Gaussian noise model is fragile to noise of large magnitude. Laplace noise assumption based PCA methods can not deal with dense noise effectively. Experimental results demonstrate the robustness of Cauchy PCA to various noise patterns. In this paper we propose Cauchy Principal Component Analysis ( Cauchy PCA ), a very simple yet effective PCA method which is robust to various types of noise, We utilize Cauchy distribution to model noise and derive Cauchy PCA under the maximum likelihood estimation ( MLE ) framework with low rank constraint, Our method can robustly estimate the low rank matrix regardless of whether noise is large or small, We analyze the robustness of Cauchy PCA from a robust statistics view and present an efficient singular value projection optimization method. On both simulated data and real applications.
2K_test_98	People with disabilities can be reluctant to friendsource help from their own friends for fear of appearing dependent or annoying. Our social microvolunteering approach has volunteers post friendsourcing tasks on behalf of people with disabilities. We demonstrate this approach via a Facebook application that answers visual questions on behalf of blind users.
2K_test_99	In prior research we have developed a Curry-Howard interpretation of linear sequent calculus as session-typed processes. In this paper we uniformly integrate this computational interpretation in a functional language. Via a linear contextual monad that isolates session-based concurrency, Monadic values are open process expressions and are first class objects in the language, thus providing a logical foundation for higher-order session typed processes We illustrate how the combined use of the monad and recursive types allows us to cleanly write a rich variety of concurrent programs, including higher-order programs that communicate processes We show the standard metatheoretic result of type preservation, as well as a global progress theorem, which to the best of our knowledge, is new in the higher-order session typed setting.
2K_test_100	Multi-modal data is dramatically increasing with the fast growth of social media Learning a good distance measure for data with multiple modalities is of vital importance for many applications, including retrieval clustering classification and recommendation. An effective and scalable multi-modal distance metric learning framework. And present empirical results on retrieval and classification to demonstrate the effectiveness and scalability. In this paper we propose Based on the multi-wing harmonium model, our method provides a principled way to embed data of arbitrary modalities into a single latent space, of which an optimal distance metric can be learned under proper supervision, by minimizing the distance between similar pairs whereas maximizing the distance between dissimilar pairs, The parameters are learned by jointly optimizing the data likelihood under the latent space model and the loss induced by distance supervision, thereby our method seeks a balance between explaining the data and providing an effective distance metric, which naturally avoids overfitting. We apply our general framework to text/image data.
2K_test_101	Abstract : Suppose we are given a large graph in which, by some external process, a handful of nodes are marked, What can we say about these marked nodes ? Are they all close-by in the graph, or are they segregated into multiple groups ? How can we automatically determine how many, if any groups they form as well as find simple paths that connect the nodes in each group ? We formalize the problem for partitioning marked nodes as well as finding simple paths between nodes within parts. Shows DOT2DOT correctly groups nodes for which good connection paths can be constructed, while separating distant nodes. In terms of the Minimum Description Length principle : a set of paths is simple when we need few bits to describe each path from one node to another, For example we want to avoid high-degree nodes, unless we need to visit many of its spokes, As such the best partitioning requires the least number of bits to describe the paths that visit all marked nodes, We show that our formulation for finding simple paths between groups of nodes has connections to well-known other problems in graph theory, We propose fast effective solutions, and introduce DOT2DOT an efficient algorithm.
2K_test_102	We consider the problem of signal recovery on graphs graph signal recovery. We validate the proposed methods. Graphs model data with complex structure as signals on a graph, Graph signal recovery recovers one or multiple smooth graph signals from noisy, corrupted or incomplete measurements We formulate as an optimization problem, for which we provide a general solution through the alternating direction methods of multipliers We show how signal inpainting, matrix completion robust principal component analysis, and anomaly detection all relate to graph signal recovery and provide corresponding specific solutions and theoretical analysis. On real-world recovery problems, including online blog classification, bridge condition identification temperature estimation, recommender system for jokes, and expert opinion combination of online blog classification.
2K_test_103	For estimating the graph structure of graph signals. And the performance of the new method The adjacency matrices estimated using the new method are shown to be close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset. This paper presents a computationally tractable algorithm is presented. The algorithm is demonstrated on simulated and real network time series datasets is compared to that of related methods for estimating graph structure.
2K_test_104	That is pose-tolerant under unconstrained face matching scenarios. And solidly outperformed the state-of-the-art algorithms under four evaluation protocols with a high accuracy of 89, 69 % a top score among image-restricted and unsupervised protocols, The advancement of Spartans is also proven In addition, our learning method based on advanced correlation filters is much more effective, in both linear and non-linear cases. In this paper we investigate a single-sample periocular-based alignment-robust face recognition technique Our Spartans framework starts by utilizing one single sample per subject class, and generate new face images under a wide range of 3D rotations using the 3D generic elastic model which is both accurate and computationally economic, Then we focus on the periocular region where the most stable and discriminant features on human faces are retained, and marginalize out the regions beyond the periocular region since they are more susceptible to expression variations and occlusions A novel facial descriptor, high-dimensional Walsh local binary patterns, is uniformly sampled on facial images with robustness toward alignment, During the learning stage, subject-dependent advanced correlation filters are learned for pose-tolerant non-linear subspace modeling in kernel feature space followed by a coupled max-pooling mechanism which further improve the performance Given any unconstrained unseen face image, the Spartans can produce a highly discriminative matching score, thus achieving high verification rate. We have evaluated our method on the challenging Labeled Faces in the Wild database in the Face Recognition Grand Challenge and Multi-PIE databases in terms of learning subject-dependent pose-tolerant subspaces, compared with many well-established subspace methods.
2K_test_105	Summary form only given. What do graphs look like ? How do they evolve over time ? How does influence/news/viruses propagate. We show that fractals and self-similarity can explain several of the observed patterns, and we conclude and a surprising result on virus propagation and immunization. We present a long list of static and temporal laws. Some recent observations on real graphs.
2K_test_106	Facial hair detection and segmentation play an important role in forensic facial analysis. For beard/moustache detection and segmentation in challenging facial images. Results have demonstrated the robustness and effectiveness of our proposed system in detecting and segmenting facial hair. In this paper we propose a fast, robust fully automatic and self-training system In order to overcome the limitations of illumination, facial hair color and near-clear shaving, our facial hair detection self-learns a transformation vector to separate a hair class and a non-hair class from the testing image itself A feature vector, consisting of Histogram of Gabor ( HoG ) and Histogram of Oriented Gradient of Gabor ( HOGG ) at different directions and frequencies, is proposed for both beard/moustache detection and segmentation in this paper, A feature-based segmentation is then proposed to segment the beard/moustache from a region on the face that is discovered to contain facial hair. Experimental in images drawn from three entire databases i, the Multiple Biometric Grand Challenge ( MBGC ) still face database, the NIST color Facial Recognition Technology FERET database and a large subset from Pinellas County database.
2K_test_107	We consider the task of designing sparse control laws for large-scale systems by directly minimizing an infinite horizon quadratic cost with an $ \ell_1 $ penalty on the feedback controller gains that allows us to scale to large systems ( i, those where sparsity is most useful ). We demonstrate the appeal of this approach on synthetic examples and real power networks significantly larger than those previously considered in the literature. Our focus is on an improved algorithm with convergence times that are several orders of magnitude faster than existing algorithms, In particular we develop an efficient proximal Newton method which minimizes per-iteration cost with a coordinate descent active set approach and fast numerical solutions to the Lyapunov equations.
2K_test_108	To maximize the number of bugs found for black-box mutational fuzzing. Our result is promising : we found an average of 38, 6 % more bugs than three previous fuzzers over 8 applications using the same amount of fuzzing time. We present the design of an algorithm given a program and a seed input The major intuition is to leverage white-box symbolic analysis on an execution trace for a given program-seed pair to detect dependencies among the bit positions of an input, and then use this dependency relation to compute a probabilistically optimal mutation ratio for this program-seed pair.
2K_test_109	Crowdsourcing can solve problems beyond the reach of state-of-the-art fully automated systems, A common pattern found in many such systems is for the workers to discover, in parallel a number of candidate solutions and then vote on the best one to pass forward, often within a fixed amount of time. For eliciting from crowd workers the proper balance between solution discovery and selection. We present the propose-vote-abstain mechanism Each crowd worker is given a choice among proposing an answer, voting among the answers proposed so far, When a stopping condition is reached, the mechanism returns the answer with the most votes, Workers are paid a base amount, with bonuses if they propose or vote for the winning answer.
2K_test_110	Crowdsourcing systems leverage short bursts of focused attention from many contributors to achieve a goal. By requiring peoples full attention, existing crowdsourcing systems fail to leverage peoples cognitive surplus in the many settings for which they may be distracted, performing or waiting to perform another task, or barely paying attention for enabling low-effort crowdsourcing. We discuss the design space for low-effort crowdsourcing, and through a series of prototypes, demonstrate interaction techniques mechanisms. In this paper we study opportunities for low-effort crowdsourcing that enable people to contribute to problem solving in such settings.
2K_test_111	In large scale machine learning and data mining problems with high feature dimensionality, the Euclidean distance between data points can be uninformative, and Distance Metric Learning ( DML ) is often desired to learn a proper similarity measure ( using side information such as example data pairs being similar or dissimilar ). However high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al, ( 2002 ) and later extensions. And we show that, our program is able to complete a DML task, 22-thousand features and 200 million labeled data pairs, in 15 hours ; and the learned metric shows great effectiveness in properly measuring distances. In this paper we present a distributed algorithm for DML, and a large-scale implementation on a parameter server architecture, Our approach builds on a parallelizable reformulation of Xing et al, ( 2002 ) and an asynchronous stochastic gradient descent optimization procedure, To our knowledge this is the first distributed solution to DML. On a system with 256 CPU cores on a dataset with 1 million data points.
2K_test_112	Maintaining consistency is a difficult challenge in crowd-powered systems in which constituent crowd workers may change over time, Our goal is to provide consistency between long interactions with crowd-powered conversational assistants by using AI to augment crowd workers. We discuss an initial outline for Chorus : Mnemonic, a system that augments the crowd 's collective memory of a conversation by automatically recovering past knowledge based on topic, allowing the system to support consistent multi-session interactions We present the design of the system itself. And discuss methods for testing its effectiveness.
2K_test_113	Examples include adaptive front lighting in vehicles, dynamic stage performance lighting, adaptive dynamic range imaging and volumetric displays. We consider the class of projector-camera systems that adaptively image and illuminate a dynamic environment, to explore the design space of such Reactive Visual Systems. And demonstrate dis-illumination of falling snow-like particles and photography of fast moving scenes. A simulator is developed Simulations are conducted to characterize system performance by analyzing the effects of end-to-end latency, jitter and prediction algorithm complexity, Key operating points are identified where systems with simple prediction algorithms can outperform systems with more complex prediction algorithms Based on the lessons learned from simulations, a low latency and low jitter, tight closed-loop reactive visual system is built. For the first time, we measure end-to-end latency, perform jitter analysis investigate various prediction algorithms and their effect on system performance, compare our system 's performance to previous work.
2K_test_114	Thousands of web APIs expose data and services that would be useful to access with natural dialog, from weather and sports to Twitter and movies. The process of adapting each API to a robust dialog system is difficult and time-consuming, as it requires not only programming but also anticipating what is mostly likely to be asked and how it is likely to be asked. And present results for each stage. We present a crowd-powered system able to generate a natural languageinterface for arbitrary web APIs from scratch without domain-dependent training data or knowledge, Our approach combines two types of crowd workers : non-expert Mechanical Turk workers interpret the functions of the API and elicit information from the user, and expert oDesk workers provide a minimal sufficient scaffolding around the API to allow us to make general queries, We describe our multi-stage process.
2K_test_115	We have been investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous driving, to guarantee their safety and efficiency despite these impairments. And suggest required modifications. In this work we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols.
2K_test_116	Many big data applications collect a large number of time series, for example the financial data of companies quoted in a stock exchange, the health care data of all patients that visit the emergency room of a hospital, or the temperature sequences continuously measured by weather stations across the US, A first task in the analytics of these data is to derive a low dimensional representation, a graph or discrete manifold, that describes well the interrelations among the time series and their intrarelations across time. For estimating this graph structure from the available data. The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested. This paper presents a computationally tractable algorithm This graph is directed and weighted, possibly representing causation relations, not just correlations as in most existing approaches in the literature. The algorithm is demonstrated on random graph and real network time series datasets, and its performance is compared to that of related methods.
2K_test_117	How can we correlate neural activity in the human brain as it responds to words, with behavioral data expressed as answers to questions about these same words ?. In short we want to find latent variables, that explain both the brain activity, as well as the behavioral responses, that solves the CMTF problem. We show that this is an instance of the Coupled Matrix-Tensor Factorization ( CMTF ) problem, we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm for CMTF, along with a 5 fold increase in sparsity Scoup-SMT is able to find meaningful latent variables, as well as to predict brain activity with competitive accuracy Finally, we demonstrate the generality of Scoup-SMT there, Scoup-SMT spots spammer-like anomalies. We propose Scoup-SMT a novel, fast and parallel algorithm and produces a sparse latent low-rank subspace of the data Moreover, we extend Scoup-SMT to handle missing data without degradation of performance. In our experiments We apply Scoup-SMT to BrainQ, a dataset consisting of a ( nouns, brain voxels human subjects ) tensor and a ( nouns, properties ) matrix with coupling along the nouns dimension by applying it on a Facebook dataset ( users, friends wall-postings ) ;.
2K_test_118	For representing motion information for video classification and retrieval To generate such local descriptors, the video blocks they are based on must contain just the right amount of motion information, However current state-of-the-art local descriptor methods use video blocks with a single fixed size, which is insufficient for covering actions with varying speeds, thus covering motions with large speed variance. Results show that albeit simple, our model achieves state-of-the-arts results. We propose a method We improve upon local descriptor based methods that have been among the most popular and successful models for representing videos, The desired local descriptors need to satisfy two requirements : 1 ) to be representative, 2 ) to be discriminative, Therefore they need to occur frequently enough in the videos and to be be able to tell the difference among different types of motions, In this paper we introduce a long-short term motion feature that generates descriptors from video blocks with multiple lengths. Experimental on several benchmark datasets.
2K_test_119	How can we find useful patterns and anomalies in large scale real-world data with multiple attributes ? For example, network intrusion logs with ( source-ip, target-ip port-number timestamp ) ? Tensors are suitable for modeling these multi-dimensional data, and widely used for the analysis of social networks, web data network traffic, and in many other settings. However current tensor decomposition methods do not scale for tensors with millions and billions of rows, columns and fibers that often appear in real datasets. And discover hidden concepts. In this paper we propose HaTen2, a scalable distributed suite of tensor decomposition algorithms running on the MapReduce platform By carefully reordering the operations, and exploiting the sparsity of real world tensors, HaTen2 dramatically reduces the intermediate data, and the number of jobs, As a result using HaTen2. We analyze big real-world tensors that can not be handled by the current state of the art.
2K_test_121	Building information models ( BIMs ) provide opportunities to serve as an information repository to store and deliver as-built information, Since a building is not always constructed exactly as the design information specifies, there will be discrepancies between a BIM created in the design phase ( called as-designed BIM ) and the as-built conditions, Point clouds captured by laser scans can be used as a reference to update an as-designed BIM into an as-built BIM ( i, the BIM that captures the as-built information ), Occlusions and construction progress prevent a laser scan performed at a single point in time to capture a complete view of building components, Progressively scanning a building during the construction phase and combining the progressively captured point cloud data together can provide the geometric information missing in the point cloud data captured previously. However combining all point cloud data will result in large file sizes and might not always guarantee additional building component information, to help engineers decide on which progressively captured point cloud data to combine in order to get more geometric information and eliminate large file sizes due to redundant point clouds. This paper provides the details of an approach developed.
2K_test_122	Both SAT and # SAT can represent difficult problems in seemingly dissimilar areas such as planning, verification and probabilistic inference, # SAT problems require counting the number of satisfiable formulas in a concisely-describable set of existentially-quantified. Here we examine an expressive new language, # SAT that generalizes both of these languages. Our experiments show that, despite the formidable worst-case complexity of # PNP [ 1 ], many of the instances can be solved efficiently by noticing and exploiting a particular type of frequent structure. We characterize the expressiveness and worst-case difficulty of # SAT by proving it is complete for the complexity class # PNP [ 1 ], and relating this class to more familiar complexity classes. We also experiment with three new general-purpose # SAT solvers on a battery of problem distributions including a simple logistics domain.
2K_test_123	Classic cake cutting protocols -- which fairly allocate a divisible good among agents with heterogeneous preferences -- are susceptible to manipulation. Do their strategic outcomes still guarantee fairness ? To answer this question. GCC protocols are guaranteed to have exact subgame perfect Nash equilibria. We adopt a novel algorithmic approach, proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model Specifically, we show that each protocol in the class of generalized cut and choose ( GCC ) protocols -- which includes the most important discrete cake cutting protocols -- is guaranteed to have approximate subgame perfect Nash equilibria Moreover, we observe that the ( approximate ) equilibria of proportional protocols -- which guarantee each of the n agents a 1/n-fraction of the cake -- must be ( approximately ) proportional, and design a GCC protocol where all Nash equilibrium outcomes satisfy the stronger fairness notion of envy-freeness. Finally we show that under an obliviousness restriction, which still allows the computation of approximately envy-free allocations.
2K_test_124	To design the topology of a distributed sensor network that is minimal with respect to a communication cost function. The paper introduces a method by which In the scenario considered, sensor nodes communicate with each other within a graph structure to update their data according to linear dynamics using neighbor node data A subset of sensors can also report their state to a central location One physical interpretation of this situation would be a set of spatially distributed wireless sensors which can communicate with other sensors within range to update data and can possibly connect to a network backbone, The costs would then be related to transmission energy, The objective is to recover the vector of initial sensor measurements from the backbone outputs over time, which requires that the dynamics of the overall networked system be observable, The topology of the network is then determined by the nonzero elements of the optimal observable dynamics The following text contributes an efficient algorithm for designing the optimal observable dynamics and the network topology for a given set of sensors and cost function. Providing proof of correctness and example implementation.
2K_test_125	The Bayesian paradigm has provided a useful conceptual theory for understanding perceptual computation in the brain, While the detailed neural mechanisms of Bayesian inference are not fully understood, recent computational and neurophysiological works have illuminated the underlying computational principles and representational architecture, The fundamental insights are that the visual system is organized as a modular hierarchy to encode an internal model of the world, and that perception is realized by statistical inference based on such internal model. In this paper we will discuss and analyze the varieties of representational schemes of these internal models and how they might be used to perform learning and inference for relating the internal models to the observed neural phenomena and mechanisms in the visual cortex. We will argue for a unified theoretical framework.
2K_test_126	That can be used for time synchronization and indoor positioning of mobile devices. That this approach outperforms the Network Time Protocol ( NTP ) on smartphones by an order of magnitude, providing an average 720s synchronization accuracy with clock drift rates as low as 2ppm. In this paper we present the design and evaluation of a platform The platform uses the Time-Difference-Of-Arrival ( TDOA ) of multiple ultrasonic chirps broadcast from a network of beacons placed throughout the environment to find an initial location as well as synchronize a receivers clock with the infrastructure, These chirps encode identification data and ranging information that can be used to compute the receivers location Once the clocks have been synchronized, the system can continue performing localization directly using Time-of-Flight ( TOF ) ranging as opposed to TDOA, This provides similar position accuracy with fewer beacons ( for tens of minutes ) until the mobile device clock drifts enough that a TDOA signal is once again required, Our hardware platform uses RF-based time synchronization to distribute clock synchronization from a subset of infrastructure beacons connected to a GPS source, Mobile devices use a novel time synchronization technique leverages the continuously free-running audio sampling subsystem of a smartphone to synchronize with global time Once synchronized, each device can determine an accurate proximity from as little as one beacon using TOF measurements, This significantly decreases the number of beacons required to cover an indoor space and improves performance in the face of obstructions. We show through experiments.
2K_test_127	Document clustering and topic modeling are two closely related tasks which can mutually benefit each other, Topic modeling can project documents into a topic space which facilitates effective document clustering, Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. Into a unified framework and jointly performs the two tasks to achieve the overall best performance. Demonstrate the effectiveness of our model. In this paper we propose a multi-grain clustering topic model ( MGCTM ) which integrates document clustering and topic modeling Our model tightly couples two components : a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters We employ variational inference to approximate the posterior of hidden variables and learn model parameters. Experiments on two datasets.
2K_test_128	In the United States, over three billion dollars are spent due to office equipment being left on when not in use during the weekend and at night. There is very little incentive for office workers to save energy because utility bills are not directly their responsibility, Our goal is to find ways to reduce the negative impact of this pervasive phenomenon to create awareness and encourage office workers towards more environmentally sustainable behavior, that enable office workers to control energy-using components. By applying persuasive technologies We then proceeded to develop `` dashboard-controllers '' with expert feedback to save energy. To this end we conducted a literature review to investigate the persuasive methods appropriate to the field of building controls.
2K_test_130	Differential game logic ( dG L ) is a logic for specifying and verifying properties of hybrid games, games that combine discrete, continuous and adversarial dynamics, Unlike hybrid systems hybrid games allow choices in the system dynamics to be resolved adversarially by different players with different objectives. To study the existence of winning strategies for such hybrid games. Finally dG L is proved to be strictly more expressive than the corresponding logic of hybrid systems. The logic dG L can be used i, ways of resolving the players choices in some way so that he wins by achieving his objective for all choices of the opponent, Hybrid games are determined, from each state one player has a winning strategy, yet computing their winning regions may take transfinitely many steps, The logic dG L, nevertheless has a sound and complete axiomatization relative to any expressive logic, Separating axioms are identified that distinguish hybrid games from hybrid systems. By characterizing the expressiveness of both.
2K_test_131	In this paper we examine the emerging copycat issue in the mobile apps market. We detect two types of copycats ( deceptive and non-deceptive ) Our results indicate significant heterogeneity in the interactions between copycats and original apps over time : ( 1 ) Non-deceptive copycats are reluctant to enter the market when the original app is popular and free, However this negative effect does not hold in other cases ; ( 2 ) Copycats can be either friends or foes of the original apps High quality copycats always have a negative effect on the original app downloads, Interestingly low quality deceptive copycats have a positive effect on the original app downloads, suggesting a potential positive spillover effect. Using machine learning techniques on large-scale unstructured data, Based on our detected copycats, we model the key drivers of mobile app copycats as well as their major impacts. From 10 100 action game apps from iOS App Store over five years.
2K_test_132	Protocols for tasks such as authentication, electronic voting and secure multiparty computation ensure desirable security properties if agents follow their prescribed programs Thus, our definition applies to relevant security properties. However if some agents deviate from their prescribed programs and a security property is violated, it is important to hold agents accountable by determining which deviations actually caused the violation, Motivated by these applications, we initiate a formal study of program actions as actual causes, for establishing program actions as actual causes. First we prove that violations of a specific class of safety properties always have an actual cause. Specifically we define in an interacting program model what it means for a set of program actions to be an actual cause of a violation We present a sound technique We demonstrate the value of this formalism in two ways. Second we provide a cause analysis of a representative protocol designed to address weaknesses in the current public key certification infrastructure.
2K_test_133	Privacy has become a significant concern in modern society as personal information about individuals is increasingly collected, used and shared often using digital technologies, by a wide range of organizations To mitigate privacy concerns, organizations are required to respect privacy laws in regulated sectors e, HIPAA in healthcare GLBA in financial sector and to adhere to self-declared privacy policies in self-regulated sectors e, privacy policies of companies such as Google and Facebook in Web services. This article provides an overview of a body of work on formalizing and enforcing privacy policies. Producing the first complete logical specification and audit of all disclosure-related clauses of the HIPAA Privacy Rule. We formalize privacy policies that prescribe and proscribe flows of personal information as well as those that place restrictions on the purposes for which a governed entity may use personal information Recognizing that traditional preventive access control and information flow control mechanisms are inadequate for enforcing such privacy policies, we develop principled accountability mechanisms that seek to encourage policy-compliant behavior by detecting policy violations, assigning blame and punishing violators. We apply these techniques to several U, privacy laws and organizational privacy policies.
2K_test_134	When building large-scale machine learning ( ML ) programs, such as massive topic models or deep neural networks with up to trillions of parameters and training examples, one usually assumes that such massive tasks can only be attempted with industrial-sized clusters with thousands of nodes, which are out of reach for most practitioners and academic researchers, We consider this challenge in the context of topic modeling on web-scale corpora. With 1 million topics and a 1-million-word vocabulary ( for a total of 1 trillion parameters ), on a document collection with 200 billion tokens -- - a scale not yet reported even with thousands of machines, evidence showing how this development puts massive data and models within reach on a small cluster, while still enjoying proportional time cost reductions with increasing cluster size. And show that with a modest cluster of as few as 8 machines, we can train a topic model Our major contributions include : 1 ) a new, highly-efficient O ( 1 ) Metropolis-Hastings sampling algorithm, whose running cost is ( surprisingly ) agnostic of model size, and empirically converges nearly an order of magnitude more quickly than current state-of-the-art Gibbs samplers ; 2 ) a model-scheduling scheme to handle the big model challenge, where each worker machine schedules the fetch/use of sub-models as needed, resulting in a frugal use of limited memory capacity and network bandwidth ; 3 ) a differential data-structure for model storage, which uses separate data structures for high- and low-frequency words to allow extremely large models to fit in memory, while maintaining high inference speed, These contributions are built on top of the Petuum open-source distributed ML framework. And we provide experimental.
2K_test_136	In distributed ML applications, shared parameters are usually replicated among computing nodes to minimize network overhead, Therefore proper consistency model must be carefully chosen to ensure algorithm 's correctness and provide high throughput, Existing consistency models used in general-purpose databases and modern distributed ML systems are either too loose to guarantee correctness of the ML algorithms or too strict and thus fail to fully exploit the computing power of the underlying distributed system, Many ML algorithms fall into the category of \emph { iterative convergent algorithms } which start from a randomly chosen initial point and converge to optima by repeating iteratively a set of procedures. We 've found that many such algorithms are to a bounded amount of inconsistency and still converge correctly This property allows distributed ML to relax strict consistency models to improve system performance while theoretically guarantees algorithmic correctness, for asynchronous parallel computation and. In this paper we present several relaxed consistency models The proposed consistency models are implemented in a distributed parameter server. Theoretically prove their algorithmic correctness and evaluated in the context of a popular ML application : topic modeling.
2K_test_137	Modern organizations ( e, hospitals social networks government agencies ) rely heavily on audit to detect and punish insiders who inappropriately access and disclose confidential information. Recent work on audit games models the strategic interaction between an auditor with a single audit resource and auditees as a Stackelberg game, augmenting associated well-studied security games with a configurable punishment parameter, to account for multiple audit resources enabling application to practical auditing scenarios. That this transformation significantly speeds up computation of solutions for a class of audit games and security games. We significantly generalize this audit game model where each resource is restricted to audit a subset of all potential violations, thus We provide an FPTAS that computes an approximately optimal solution to the resulting non-convex optimization problem The main technical novelty is in the design and correctness proof of an optimization transformation that enables the construction of this FPTAS. In addition we experimentally demonstrate.
2K_test_138	To partly address people 's concerns over web tracking, Google has created the Ad Settings webpage to provide information about and some choice over the profiles Google creates on users Nevertheless, these results can form the starting point for deeper investigations by either the companies themselves or by regulatory bodies. That explores how user behaviors, Google 's ads and Ad Settings interact. In particular we found that visiting webpages associated with substance abuse changed the ads shown but not the settings page, We also found that setting the gender to female resulted in getting fewer instances of an ad related to high paying jobs than setting it to male We can not determine who caused these findings due to our limited visibility into the ad ecosystem, which includes Google advertisers. We present AdFisher an automated tool AdFisher can run browser-based experiments and analyze data using machine learning and significance tests, Our tool uses a rigorous experimental design and statistical analysis to ensure the statistical soundness of our results, We use AdFisher to find that the Ad Settings was opaque about some features of a user 's profile, that it does provide some choice on ads, and that these choices can lead to seemingly discriminatory ads.
2K_test_139	To guide the design of password management schemes systematic strate- gies to help users create and remember multiple passwords, for the complexity of password man- agement Observing that current pass- word management schemes are either insecure or unusable. Password reuse benefits users not only by reducing the number of passwords that the user has to memorize, but more importantly by in- creasing the natural rehearsal rate for each password. We introduce quantitative usability and security models In the same way that security proofs in cryptography are based on complexity- theoretic assumptions ( e, hardness of factoring and discrete loga- rithm ), we quantify usability by introducing usability assumptions, I n particular password management relies on assumptions about human memory, that a user who follows a particular rehearsal schedule will successfully maintain the corresponding memory These assumptions are informed by research in cognitive science and can be tested empirically Given rehearsal requirements and a user 's visitation schedule for each account, we use the total number of extra rehearsals that the user would have to do to remember all of his passwords as a measure of the usability of the password scheme Our usability model leads us to a key observa- tion : We also present a security model which accounts with multiple accounts and associated threats, including online offline and plaintext password leak attacks, we present Shared Cues a new scheme in which the underlying secret is strategi- cally shared across accounts to ensure that most rehearsal requirements are satisfied naturally while simultaneously providing strong security The construction uses the Chinese Remainder Theorem to achieve these competing goals.
2K_test_140	Many important applications fall into the broad class of iterative convergent algorithms, Parallel implementations of these algorithms are naturally expressed using the Bulk Synchronous Parallel ( BSP ) model of computation. However implementations using BSP are plagued by the straggler problem, where every transient slowdown of any given thread can delay all other threads, that preserves many of its advantages, while avoiding the straggler problem. This paper presents the Stale Synchronous Parallel ( SSP ) model as a generalization of BSP Algorithms using SSP can execute efficiently, even with significant delays in some threads, addressing the oft-faced straggler problem.
2K_test_141	Ductal Carcinoma In Situ ( DCIS ) is a precursor lesion of Invasive Ductal Carcinoma ( IDC ) of the breast Investigating its temporal progression could provide fundamental new insights for the development of better diagnostic tools to predict which cases of DCIS will progress to IDC, The approach provides new insights into mechanisms of clonal progression in breast cancers and helps illustrate the power of the ILP approach for similar problems in reconstructing tumor evolution scenarios under complex sets of constraints. We investigate the problem of reconstructing a plausible progression from single-cell sampled data of an individual with synchronous DCIS and IDC. Show that the corresponding predicted progression models are classifiable into categories having specific evolutionary characteristics. Specifically by using a number of assumptions derived from the observation of cellular atypia occurring in IDC, we design a possible predictive model using integer linear programming ( ILP ). Computational experiments carried out on a preexisting data set of 13 patients with simultaneous DCIS and IDC.
2K_test_142	A variety of problems in computing, service and manufacturing systems can be modeled via infinite repeating Markov chains with an infinite number of levels and a finite number of phases. Many such chains are quasi-birth-death processes with transitions that are skip-free in level, in that one can only transition between consecutive levels, and unidirectional in phase, in that one can only transition from lower-numbered phases to higher-numbered phases for determining the limiting probabilities of such Markov chains exactly. We present a procedure, which we call Clearing Analysis on Phases ( CAP ) The CAP method yields the limiting probability of each state in the repeating portion of the chain as a linear combination of scalar bases raised to a power corresponding to the level of the state, The weights in these linear combinations can be determined by solving a finite system of linear equations.
2K_test_143	Any strong Nash equilibrium outcome is Pareto efficient for each coalition. In this paper we consider strong Nash equilibria, in mixed strategies for finite games, In order to get our result. Our main result in its simplest form, states that if a game has a strong Nash equilibrium with full support ( that is, both players randomize among all pure strategies ), then the game is strictly competitive. We use the indifference principle fulfilled by any Nash equilibrium, and the classical KKT conditions ( in the vector setting ), that are necessary conditions for Pareto efficiency, Our characterization enables us to design a strong-Nash-equilibrium-finding algorithm with complexity in Smoothed- $ \mathcal { P } $, So this problem -- -that Conitzer and Sandholm [ Conitzer, New complexity results about Nash equilibria, 63 621 -- 641 ] proved to be computationally hard in the worst case -- -is generically easy, Hence although the worst case complexity of finding a strong Nash equilibrium is harder than that of finding a Nash equilibrium, once small perturbations are applied, finding a strong Nash is easier than finding a Nash equilibrium Next we switch to the setting with more than two players, We demonstrate that a strong Nash equilibrium can exist in which an outcome that is strictly Pareto dominated by a Nash equilibrium occurs with positive probability Finally, we prove that games that have a strong Nash equilibrium where at least one player puts positive probability on at least two pure strategies are extremely rare : they are of zero measure. First we analyze the two -- player setting.
2K_test_144	Many real world network problems often concern multivariate nodal attributes such as image, textual and multi-view feature vectors on nodes, rather than simple univariate nodal attributes. The existing graph estimation methods built on Gaussian graphical models and covariance selection algorithms can not handle such data, neither can the theories developed around such methods be directly applied, for estimating multi-attribute graphs. Demonstrate performance of our method under various conditions. In this paper we propose a new principled framework Instead of estimating the partial correlation as in current literature, our method estimates the partial canonical correlations that naturally accommodate complex nodal features Computationally, we provide an efficient algorithm which utilizes the multi-attribute structure, Theoretically we provide sufficient conditions which guarantee consistent graph recovery.
2K_test_145	Proceedings : AACR 106th Annual Meeting 2015 ; April 18-22, 2015 ; Philadelphia PA Intratumor heterogeneity has long been a confounding factor in interpreting cancer genomic data, but has also been useful in reconstructing progression processes based on variation between clonal populations in single tumors We previously developed a strategy of applying computational deconvolution algorithms to gene expression or DNA copy number data to reconstruct models of progression of cell populations from bulk tumor samples, Citation Format : Theodore Roman. Many tools have since been proposed for similar deconvolution analysis, but all are limited by the computational difficulty of unambiguously distinguishing small cell populations and variants from noise in genomic assays to infer tumor progression pathways from deconvolved genomic data to better deconvolve cell populations across tumor types, to perform tumor deconvolution Improved deconvolution of heterogeneous tumor data to reconstruct clonal evolution from bulk genomic samples. All methods perform comparably on unstructured data but the new method substantially outperforms the others on data consistent with simple scenarios for tumor progression along multiple discrete subtypes, The novel method however, shows lower tolerance for noisy data than a Gaussian mixture model, showed our method could partition tumors into discrete subcategories associated with HER2+, ER/PR+ and triple-negative status and could exploit the resulting substructure of the data to deconvolve tumor data and infer progression models reflecting partial sharing of progression states between subtypes, showed association of deconvolved cell populations with a variety of gene functional categories suggestive of distinct progression mechanisms of the subtypes. We present a novel approach designed to leverage the fact that tumors that partition into subtypes with similar evolutionary trajectories would be expected to lead to a mathematical substructure in the genomic data, known as a simplicial complex, which can be modeled computationally We have developed a computational pipeline while taking into consideration this kind of mathematical structure The pipeline clusters tumors to identify genetically distinct subgroups, fits mixture models to these subgroups, and uses overlap between them to infer a refined deconvolution of major cellular populations and possible pathways of progression between them. We apply our methods to a set of RNASeq data from the TCGA breast cancer data set and to synthetic data modeling distinct scenarios of tumor progression, We first compare our methods on the synthetic data to our earlier work and to a comparative Gaussian mixture model, Application to the TCGA RNASeq data Gene enrichment analysis.
2K_test_146	Signals and datasets that arise in physical and engineering applications, as well as social, genetics biomolecular and many other domains, are becoming increasingly larger and more complex, In contrast to traditional time and image signals, data in these domains are supported by arbitrary graphs, Signal processing on graphs extends concepts and techniques from traditional signal processing to data indexed by generic graphs, In traditional signal processing, these concepts are easily defined because of a natural frequency ordering that has a physical interpretation, For signals residing on graphs, in general there is no obvious frequency ordering. This paper studies the concepts of low and high frequencies on graphs, and low- high- and band-pass graph signals and graph filters. We propose a definition of total variation that naturally leads to a frequency ordering on graphs and defines low-, high- and band-pass graph signals and filters. We study the design of graph filters with specified frequency response, and illustrate our approach with applications to sensor malfunction detection and data classification.
2K_test_147	Block tridiagonal matrices arise in applied mathematics, physics and signal processing, Many applications require knowledge of eigenvalues and eigenvectors of block tridiagonal matrices, which can be prohibitively expensive for large matrix sizes. In this paper we address the problem of the eigendecomposition of block tridiagonal matrices. That our work can lead to fast algorithms for the eigenvector expansion for block tridiagonal matrices. By studying a connection between their eigenvalues and zeros of appropriate matrix polynomials We use this connection with matrix polynomials to derive a closed-form expression for the eigenvectors of block tridiagonal matrices, which eliminates the need for their direct calculation and can lead to a faster calculation of eigenvalues. We also demonstrate with an example.
2K_test_148	We aim to detect complex events in long Internet videos that may last for hours A major challenge in this setting is that only a few shots in a long video are relevant to the event of interest while others are irrelevant or even misleading. And confirm the effectiveness of the proposed approach. Instead of indifferently pooling the shots, we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event detector, Next we propose a new isotonic regularizer that is able to exploit the semantic ordering information The resulting nearly-isotonic SVM classifier exhibits higher discriminative power Computationally, we develop an efficient implementation using the proximal gradient algorithm, and we prove new. We conduct extensive experiments on three real-world video datasets.
2K_test_149	Which may be of independent interest. For approximate maximum flow in undirected graphs with good separator structures. We present faster algorithms such as bounded genus, minor free and geometric graphs Given such a graph with n vertices, m edges along with a recursive n-vertex separator structure, our algorithm finds an 1 -- e approximate maximum flow in time O ( m6/5poly ( e -- 1 ) ), ignoring poly-logarithmic terms Similar speedups are also achieved for separable graphs with larger size separators albeit with larger run times These bounds also apply to image problems in two and three dimensions, Key to our algorithm is an intermediate problem that we term grouped L2 flow, which exists between maximum flows and electrical flows, Our algorithm also makes use of spectral vertex sparsifiers in order to remove vertices while preserving the energy dissipation of electrical flows We also give faster spectral vertex sparsification algorithms on well separated graphs.
2K_test_150	The cake cutting problem models the fair division of a heterogeneous good between multiple agents, Previous work assumes that each agent derives value only from its own piece. However agents may also care about the pieces assigned to other agents ; such externalities naturally arise in fair division settings, to capture externalities and generalize the classical fairness notions of proportionality and envyfreeness. We extend the classical model Our technical results characterize the relationship between these generalized properties, establish the existence or nonexistence of fair allocations, and explore the computational feasibility of fairness in the face of externalities.
2K_test_151	If Alice has double the friends of Bob, will she also have dou- ble the phone-calls ( or wall-postings, or tweets ) ?. Our first contribution is the discovery that the relative frequencies obey a power-law ( sub-linear, or super-linear ) for a wide variety of diverse settings : tasks in a phone- call network, like count of friends, count of phone-calls total count of minutes ; tasks in a twitter-like network, like count of tweets, count of followees etc, We show how to use our observations to spot clusters and outliers, telemarketers in our phone-call network. The second contribution is that we further provide a full, digitized 2-d distribution which we call the Almond-DG model, thanks to the shape of its iso-surfaces, The Almond-DG model matches all our empirical observations : super-linear relationships among variables, and ( provably ) log-logistic marginals. We illustrate our observations on two large, real network datasets spanning 2, 1M individu- als with 5 features each.
2K_test_153	Kidney exchange provides a life-saving alternative to long waiting lists for patients in need of a new kidney, Fielded exchanges typically match under utilitarian or near-utilitarian rules ; this approach marginalizes certain classes of patients. In this paper we focus on improving access to kidneys for highly-sensitized, or hard-to-match patients Toward this end. We formally adapt a recently introduced measure of the tradeoff between fairness and efficiency -- -the price of fairness -- -to the standard kidney exchange model, We show that the price of fairness in the standard theoretical model is small, We then introduce two natural definitions of fairness. And empirically explore the tradeoff between matching more hard-to-match patients and the overall utility of a utilitarian matching, on real data from the UNOS nationwide kidney exchange and simulated data from each of the standard kidney exchange distributions.
2K_test_155	Abstract : Privacy policies in sectors as diverse as Web services, finance and healthcare often place restrictions on the purposes for which a governed entity may use personal information. Thus automated methods for enforcing privacy policies require a semantics of purpose restrictions to determine whether a governed agent used information for a purpose, We provide such a semantics to automate the enforcement of purpose restrictions. Using a formalism based on planning, We model planning using Partially Observable Markov Decision Processes ( POMDPs ), which supports an explicit model of information We argue that information use is for a purpose if and only if the information is used while planning to optimize the satisfaction of that purpose under the POMDP model We determine information use by simulating ignorance of the information prohibited by the purpose restriction, which we relate to noninterference, We use this semantics to develop a sound audit algorithm.
2K_test_156	For n-gram language modeling. PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task. We present power low rank ensembles ( PLRE ), a flexible framework where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context, Our method can be understood as a generalization of ngram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases.
2K_test_157	Contributing to the general theory of large deviations. We find large deviations rates for consensus-based distributed inference for directed networks finding the left eigenvector that achieves the highest value of the rate function. When the topology is deterministic, we establish the large deviations principle and find exactly the corresponding rate function, equal at all nodes, We show that the dependence of the rate function on the stochastic weight matrix associated with the network is fully captured by its left eigenvector corresponding to the unit eigenvalue, Further when the sensors observations are Gaussian, the rate function admits a closed-form expression, we show that the network design problem can be formulated as a semidefinite ( convex ) program, and hence can be solved efficiently the system exhibits an interesting property : the graph of the rate function always lies between the graphs of the rate function of an isolated node and the rate function of a fusion center that has access to all observations We prove that this fundamental property holds even when the topology and the associated system matrices change randomly over time. Motivated by these observations, we formulate the optimal network design problem of, for a given target accuracy, This eigenvector therefore minimizes the time that the inference algorithm needs to reach the desired accuracy, Due to the generality of its assumptions, the latter result requires more subtle techniques than the standard large deviations tools. For Gaussian observations When observations are identically distributed across agents.
2K_test_158	Our study has the potential to help advertisers design keyword portfolios, and help search engines improve the quality of sponsored ads. In this paper we explore how the contextual ambiguity of a search can affect a keyword 's performance, an automatic way of categorizing keywords and examining keyword contextual ambiguity. We find that consumer click behaviors vary significantly across keywords, and keyword category and contextual ambiguity significantly affect such variation, Specifically higher contextual ambiguity can lead to higher click-through rate ( CTR ) on top-positioned ads, but the CTR tends to decay faster with position. We propose based on topic models from machine learning and computational linguistics We quantify the effect of contextual ambiguity on keyword click-through performance using a hierarchical Bayesian model. And validate our study using a novel dataset from a major search engine containing information on click activities for 12, 790 keywords across multiple categories from over 4.
2K_test_159	Function to function regression ( FFR ) covers a large range of interesting applications including timeseries prediction problems, and also more general tasks like studying a mapping between two separate types of distributions. We analyze the problem of regression when both input covariates and output responses are functions from a nonparametric function class, However previous nonparametric estimators for FFR type problems scale badly computationally with the number of input/output pairs in a data-set Given the complexity of a mapping between general functions it may be necessary to consider large datasets in order to achieve a low estimation risk, To address this issue. Furthermore we show an improvement of several orders of magnitude in terms of prediction speed and a reduction in error over previous estimators. We develop a novel scalable nonparametric estimator, the Triple-Basis Estimator ( 3BE ), which is capable of operating over data-sets with many instances, To the best of our knowledge, the 3BE is the first nonparametric FFR estimator that can scale to massive data-sets. We analyze the 3BEs risk and derive an upperbound rate, in various real-world datasets.
2K_test_160	An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer. For a setting in which the human user can only receive assistance from a semi-trusted computer. For these schemes we prove that forging passwords is equivalent to recovering the secret mapping Thus, our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts. We propose several candidate authentication protocols -- - a computer that stores information and performs computations correctly but does not provide confidentiality, Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $, The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate, We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $, for a security parameter $ s ( f ) $ that depends on two key properties To obtain our results, we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $, Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate ), and generalize recent results of Feldman et al.
2K_test_161	To achieve maximum security, defenders must perfectly synchronize their randomized allocations of resources. We study security games with multiple defenders However, in real-life scenarios ( such as protection of the port of Boston ) this is not the case, Our goal is to quantify the loss incurred by miscoordination between defenders, both theoretically and empirically, that capture this loss. Indicate that the loss may be extremely high in the worst case, establish a smaller yet significant loss in practice. We introduce two notions under different assumptions : the price of miscoordination, and the price of sequential commitment. Generally speaking our theoretical bounds while our simulations.
2K_test_162	In real world industrial applications of topic modeling, the ability to capture gigantic conceptual space by learning an ultra-high dimensional topical representation, the so-called `` big model '', is becoming the next desideratum after enthusiasms on `` big data '', especially for fine-grained downstream tasks such as online advertising, where good performances are usually achieved by regression-based predictors built on millions if not billions of input features. The conventional data-parallel approach for training gigantic topic models turns out to be rather inefficient in utilizing the power of parallelism, due to the heavy dependency on a centralized image of `` model '', Big model size also poses another challenge on the storage, where available model size is bounded by the smallest RAM of nodes, To address these issues, which enables training of disjoint blocks of a big topic model in parallel. Demonstrate the ability of this system to handle topic modeling with unprecedented amount of 200 billion model variables only on a low-end cluster with very limited computational resources and bandwidth. We explore another type of parallelism, namely model-parallelism By integrating data-parallelism with model-parallelism, we show that dependencies between distributed elements can be handled seamlessly, achieving not only faster convergence but also an ability to tackle significantly bigger model size We describe an architecture for model-parallel inference of LDA, and present a variant of collapsed Gibbs sampling algorithm tailored for it.
2K_test_163	The proliferation of mobile devices that are capable of estimating their position, has lead to the emergence of a new class of social networks, namely location-based social networks ( LBSNs for short ), The main interaction between users in an LBSN is location sharing, To the best of our knowledge, this is the first attempt to model this problem using tensor analysis. While the latter can be realized through continuous tracking of a user 's whereabouts from the service provider, the majority of LBSNs allow users to voluntarily share their location, LBSNs provide incentives to users to perform check-ins, However these incentives can also lead to people faking their location, thus generating false information, for spotting anomalies in the check-in behavior of users. In this work we propose the use of tensor decomposition.
2K_test_164	The computational characterization of game-theoretic solution concepts is a central topic in artificial intelligence, with the aim of developing computationally efficient tools for finding optimal ways to behave in strategic interactions, The central solution concept in game theory is Nash equilibrium ( NE ), However it fails to capture the possibility that agents can form coalitions ( even in the 2-agent case ), Strong Nash equilibrium ( SNE ) refines NE to this setting, It is known that finding an SNE is NP-complete when the number of agents is constant, This hardness is solely due to the existence of mixed-strategy SNEs, given that the problem of enumerating all pure-strategy SNEs is trivially in P. Our central result is that, in order for a game to have at least one non-pure-strategy SNE, the agents ' payoffs restricted to the agents ' supports must, in the case of 2 agents, lie on the same line, and in the case of n agents, lie on an ( n - 1 ) -dimensional hyperplane, Leveraging this result we provide two contributions. First we develop worst-case instances for support-enumeration algorithms, These instances have only one SNE and the support size can be chosen to be of any size-in particular, Second we prove that, unlike NE finding an SNE is in smoothed polynomial time : generic game instances ( i, all instances except knife-edge cases ) have only pure-strategy SNEs.
2K_test_165	If we know most of Smith 's friends are from Boston, what can we say about the rest of Smith 's friends ? which is one of the most important topics in AI and Web communities. In this paper we focus on the node classification problem on networks. We also prove the theoretical connections of our algorithm to the semi-supervised learning ( SSL ) algorithms and to random-walks demonstrate the benefits of the proposed algorithm, where OMNI-Prop outperforms the top competitors. Our proposed algorithm which is referred to as OMNI-Prop has the following properties : ( a ) seamless and accurate ; it works well on any label correlations ( i, homophily het-erophily and mixture of them ) ( b ) fast ; it is efficient and guaranteed to converge on arbitrary graphs ( c ) quasi-parameter free ; it has just one well-interpretable parameter with heuristic default value of 1. Experiments on four real.
2K_test_166	Single virus epidemics over complete networks are widely explored in the literature as the fraction of infected nodes is, under appropriate microscopic modeling of the virus infection, With non-complete networks this macroscopic variable is no longer Markov. In this paper we study virus diffusion, in particular multi-virus epidemics, over non-complete stochastic networks, We focus on multipartite networks to analyze the qualitative behavior of these limiting dynamics, establishing conditions on the virus micro characteristics and network structure under which a virus persists or a natural selection phenomenon is observed. We show that the peer-to-peer local random rules of virus infection lead, in the limit of large multipartite networks, to the emergence of structured dynamics at the macroscale The exact fluid limit evolution of the fraction of nodes infected by each virus strain across islands obeys a set of nonlinear coupled differential equations, see this http URL. In this paper we develop methods. In companying work this http URL.
2K_test_167	Many modern machine learning ( ML ) algorithms are iterative, converging on a final solution via many iterations over the input data. This paper explores approaches to exploiting these algorithms ' convergent nature to improve performance, by allowing parallel and distributed threads to use loose consistency models for shared algorithm state. Show that both approaches significantly increase convergence speeds, behaving similarly when there are no stragglers, but SSP outperforms BSP in the presence of stragglers. Specifically we focus on bounded staleness, in which each thread can see a view of the current intermediate solution that may be a limited number of iterations out-of-date, Allowing staleness reduces communication costs ( batched updates and cached reads ) and synchronization ( less waiting for locks or straggling threads ) One approach is to increase the number of iterations between barriers in the oft-used Bulk Synchronous Parallel ( BSP ) model of parallelizing, which mitigates these costs when all threads proceed at the same speed, A more flexible approach, called Stale Synchronous Parallel ( SSP ), avoids barriers and allows threads to be a bounded number of iterations ahead of the current slowest thread. Extensive experiments with ML algorithms for topic modeling, collaborative filtering and PageRank.
2K_test_168	Determining a match between the subjects of first and second images can be used for face recognition, even where a small portion of a person 's face is captured in an image. As a function of decimal-number representations of regions of the first and second images, The decimal-number representations are generated by performing discrete transforms on the regions so as to obtain discrete-transform coefficients, performing local-bit-pattern encoding of the coefficients to create data streams, and converting the data streams to decimal numbers In one embodiment, the first and second images depict periocular facial regions, and the disclosed techniques Subspace modeling may be used to improve accuracy.
2K_test_169	Practical applications of Bayesian nonparametric ( BNP ) models have been limited, due to their high computational complexity and poor scaling on large data, and the near-linear scalability indicates great potential for even bigger problem sizes. In this paper we consider dependent nonparametric trees ( DNTs ), a powerful infinite model that captures time-evolving hierarchies. Our system learns a 10K-node DNT topic model on 8M documents that captures both high-frequency and longtail topics, Our data and model scales are orders-of-magnitude larger than recent results on the hierarchical Dirichlet process. And develop a large-scale distributed training system Our major contributions include : ( 1 ) an effective memoized variational inference for DNTs, with a novel birth-merge strategy for exploring the unbounded tree space ; ( 2 ) a model-parallel scheme for concurrent tree growing/pruning and efficient model alignment, through conflict-free model partitioning and lightweight synchronization ; ( 3 ) a data-parallel scheme for variational parameter updates that allows distributed processing of massive data. Using 64 cores in 36 hours.
2K_test_170	It is unsolved even for two bidders and two items for sale. Designing optimalthat is revenue-maximizingcombinatorial auctions ( CAs ) is an important elusive problem. Show that our algorithms create mechanisms that yield significantly higher revenue than the VCG and scale dramatically better than prior automated mechanism design algorithms The algorithms yielded deterministic mechanisms with the highest known revenues for the settings tested, including the canonical setting with two bidders, two items and uniform additive valuations. Rather than pursuing the manual approach of attempting to characterize the optimal CA, we introduce a family of CAs and then seek a high-revenue auction within that family, The family is based on bidder weighting and allocation boosting ; we coin such CAs virtual valuations combinatorial auctions ( VVCAs ) VVCAs are the Vickrey-Clarke-Groves ( VCG ) mechanism executed on virtual valuations that are affine transformations of the bidders valuations, The auction family is parameterized by the coefficients in the transformations The problem of designing a CA is thereby reduced to search in the parameter space of VVCAor the more general space of affine maximizer auctions, We first construct VVCAs with logarithmic approximation guarantees in canonical special settings : ( 1 ) limited supply with additive valuations and ( 2 ) unlimited supply, In the main part of the paper, we develop algorithms that design high-revenue CAs for general valuations using samples from the prior distribution over bidders valuations, ( Priors turn out to be necessary for achieving high revenue, ) We prove properties of the problem that guide our design of algorithms, We then introduce a series of algorithms that use economic insights to guide the search and thus reduce the computational complexity.
2K_test_172	How many listens will an artist receive on a online radio ? How about plays on a YouTube video ? How many of these visits are new or returning users ? Modeling and mining popularity dynamics of social activity has important implications for researchers, content creators and providers. We here investigate the effect of revisits ( successive visits from a single user ) on content popularity which captures the popularity dynamics of individual objects. We show the effect of revisits in the popularity evolution of such objects. Secondly we propose the Phoenix-R model Phoenix-R has the desired properties of being : ( 1 ) parsimonious, being based on the minimum description length principle, and achieving lower root mean squared error than state-of-the-art baselines ; ( 2 ) applicable, the model is effective for predicting future popularity values of objects. Using four datasets of social activity, with up to tens of millions media objects ( e, YouTube videos Twitter hashtags or LastFM artists ).
2K_test_173	Social microvolunteering lets people volunteer despite temporal, financial or physical limitations. To magnify their effort. We propose social microvolunteering, in which people can do charitable microwork themselves for free, but also grant access to their Facebook friends as additional volunteers.
2K_test_174	To estimate the potential benefits of smart electric vehicle ( EV ) charging, to identify the benefits of non-residential EV charging to the load aggregators and the distribution grid Using this extensive dataset, we aim to improve upon past studies focusing on the benefits of smart EV charging. For the year of 2013, we show a reduction of up to 24, 8 % in the monthly bill is possible we show that EV aggregations decrease their contribution to the system peak load by approximately 37 % ( median ) when charging is controlled within arrival and departure times, Our results also show that it could be expected to shift approximately 0, 8 % ) of energy per non-residential EV charging session from peak periods ( 12PM6PM ) to off-peak periods ( after 6PM ) in Northern California for the year of 2013. We develop a smart charging framework by relaxing the assumptions made in these studies regarding : ( i ) driving patterns, driver behavior and driver types ; ( ii ) the scalability of a limited number of simulated vehicles to represent different load aggregation points in the power system with different customer characteristics ; and ( iii ) the charging profile of EVs, Then following a similar aggregation strategy. In this paper we use data collected from over 2000 non-residential electric vehicle supply equipments ( EVSEs ) located in Northern California for the year of 2013 First, we study the benefits of EV aggregations behind-the-meter, where a time-of-use pricing schema is used to understand the benefits to the owner when EV aggregations shift load from high cost periods to lower cost periods.
2K_test_175	Given the pace of discovery in medicine, accessing the literature to make informed decisions at the point of care has become increasingly difficult Advances in social computation and human computer interactions offer a potential solution to this problem, Journal of Hospital Medicine 2014 ; 9:451456, 2014 Society of Hospital Medicine. Although the Internet creates unprecedented access to information, gaps in the medical literature and inefficient searches often leave healthcare providers ' questions unanswered to help providers problem solve by crowdsourcing from their peers. 85 registered users logged 1544 page views and sent 45 consult questions, The median initial first response from the crowd occurred within 19 minutes Review of the transcripts revealed several dominant themes, including complex medical decision making and inquiries related to prescription medication use, Feedback from the post-trial survey identified potential hurdles related to medical crowdsourcing, including a reluctance to expose personal knowledge gaps and the potential risk for distracted doctoring, Users also suggested program modifications that could support future adoption, including changes to the mobile interface and mechanisms that could expand the crowd of participating healthcare providers. We developed and piloted the mobile application DocCHIRP, which uses a system of point-to-multipoint push notifications designed. Over the 244-day pilot period.
2K_test_176	We conclude with a discussion of ways the modeling approach might be applied, along with caveats from limitations, and directions for future work. For modeling emerging social structure as it develops in threaded discussion forums. In each of three MOOCs we find evidence that participation in two to four subcommunities out of the twenty is associated with significantly higher or lower dropout rates than average, illustrates how the learned models can be used as a lens for understanding the values and focus of discussions within the subcommunities, and in the illustrative example to think about the association between those and detected higher or lower dropout rates than average in the three courses, demonstrates that the patterns that emerge make sense : It associates evidence of stronger expressed motivation to actively participate in the course as well as evidence of stronger cognitive engagement with the material in subcommunities associated with lower attrition, and the opposite in subcommunities associated with higher attrition. In this paper we describe a novel methodology, grounded in techniques from the field of machine learning, with an eye towards application in the threaded discussions of massive open online courses ( MOOCs ), This modeling approach integrates two simpler, well established prior techniques, namely one related to social network structure and another related to thematic structure of text, As an illustrative application of the integrated techniques use and utility, We then use a survival model to measure the impact of participation in identified subcommunities on attrition along the way for students who have participated in the course discussion forums of the three courses. We use it as a lens for exploring student dropout behavior in three different MOOCs, In particular we use the model to identify twenty emerging subcommunities within the threaded discussions of each of the three MOOCs A qualitative post-hoc analysis Our qualitative analysis.
2K_test_177	Given a large number of taxi trajectories, we would like to find interesting and unexpected patterns from the data, How can we summarize the major trends, and how can we spot anomalies ? The anal- ysis of trajectories has been an issue of considerable interest with many applications such as tracking trails of migrating animals and predicting the path of hurricanes. Several recent works propose methods on clus- tering and indexing trajectories data, However these approaches are not especially well suited to pattern discovery with respect to the dynamics of social and economic behavior To further analyze a huge collection of taxi trajectories, to find meaningful patterns and anomalies. In fact F-Trail does produce concise, informative and interesting patterns. We develop a novel method, called F-Trail w hich al- lows us Our approach has the following advantages : ( a ) it is fast, and scales linearly on the input size, ( b ) it is effective, leading to novel discoveries. We demonstrate the effectiveness of our approach, by performing exper- iments on real taxi trajectories.
2K_test_178	Supports the human evaluation of complex crowd work. The present invention discloses CrowdScape, a system that through interactive visualization and mixed initiative machine learning, The system combines information about worker behavior with worker outputs and aggregate worker behavioral traces to allow the isolation of target worker clusters This approach allows users to develop and test their mental models of tasks and worker behaviors, and then ground those models in worker outputs and majority or gold standard verifications.
2K_test_181	The increasing performance of modern processors makes virtualization a viable solution for consolidating real-time systems into a single hardware platform. Although real-time task scheduling in a virtual machine can benefit from hierarchical scheduling, unbounded interrupt handling time and vulnerability to interrupt storms make practitioners hesitant to virtualize interrupt-driven real-time applications, designed for real-time system virtualization. Our experimental results indicate that vINT achieves timely interrupt handling while providing as good task schedulability as when it is not used, shows that vINT yields significant benefits in reducing interrupt handling time and in protecting real-time tasks against interrupt storms permeating into the virtual machine. In this paper we propose vINT, an interrupt handling scheme vINT provides a pseudo-VCPU abstraction dedicated for interrupt handling, which overcomes the limits imposed by the timing parameters of virtual CPUs in an analyzable way vINT also accounts for and enforces interrupt handling and resulting execution flows within a guest virtual machine vINT does not require any change to the guest OS code, so it can be used for virtualizing proprietary. We analyze interrupt handling time as well as VCPU and task schedulability, with and without vINT, Our case study based on a prototype implementation on the KVM hyper visor.
2K_test_182	Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in incomplete-information games. However imperfect-recall abstractions are poorly understood, and only weak algorithm-specific guarantees on solution quality are known for Nash equilibria. In this paper we show the first general, algorithm-agnostic solution quality guarantees and approximate self-trembling equilibria computed in imperfect-recall abstractions, when implemented in the original ( perfect-recall ) game, Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions where any results had been obtained, Further our analysis is tighter in two ways, each of which can lead to an exponential reduction in the solution quality error bound We then show that for extensive-form games that satisfy certain properties, the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem, where the increase in our bound is the distance function This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds We proceed to show a divide in the class of abstraction problems If payoffs are at the same scale at all information sets considered for abstraction, the input forms a metric space, Conversely if this condition is not satisfied, we show that the input does not form a metric space. Finally we use these results to experimentally investigate the quality of our bound for single-level abstraction.
2K_test_183	In this paper we focus on complex event detection in internet videos while also providing the key evidences of the detection results, Convolutional Neural Networks ( CNNs ) have achieved promising performance in image classification and action recognition tasks, However it remains an open problem how to use CNNs for video event detection and recounting, mainly due to the complexity and diversity of video events. Demonstrate the promising performance of our method, both for event detection and evidence recounting. In this work we propose a flexible deep CNN infrastructure, namely Deep Event Network ( DevNet ), that simultaneously detects pre-defined events and provides key spatial-temporal evidences Taking key frames of videos as input, we first detect the event of interest at the video level by aggregating the CNN features of the key frames, The pieces of evidences which recount the detection results, are also automatically localized, both temporally and spatially, The challenge is that we only have video level labels, while the key evidences usually take place at the frame levels, Based on the intrinsic property of CNNs, we first generate a spatial-temporal saliency map by back passing through DevNet, which then can be used to find the key frames which are most indicative to the event, as well as to localize the specific spatial position, usually an object in the frame of the highly indicative area. Experiments on the large scale TRECVID 2014 MEDTest dataset.
2K_test_184	Formal verification of industrial systems is very challenging, due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams, More importantly industrial systems are rarely designed for verification, but rather for operational needs The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team. To formally verify ACAS X. In this paper we present an overview of our experience using hybrid systems theorem proving an airborne collision avoidance system for airliners scheduled to be operational around 2020 The methods and proof techniques presented here are an overview of the work already presented in [ 8 ], while the evaluation of ACAS X has been significantly expanded and updated to the most recent version of the system.
2K_test_185	Modern day law enforcement banks heavily on the use of commercial off-the-shelf ( COTS ) face recognition systems ( FRS ) as a tool for biometric evaluation and identification, However in many real-world scenarios, when the face of an individual is occluded or degraded in some way, commercial recognition systems fail to accept the face for evaluation or simply return unusable matched faces, In these kinds of cases, forensic experts rely on image processing techniques and tools, to make the face fit to be processed by the commercial recognition systems ( e, use partial face images from another subject to fill in the occluded parts of the face of interest, or have a tight crop around the face ). In this study we evaluate the sensitivity of commercial recognition systems to such forensic techniques, This study is meant to serve as an evaluation of the effect of a few forensic techniques intended to allow commercial recognition systems to process and match face images that were otherwise unusable. Our results indicate that COTS FRS can be sensitive to the subjectivity in facial part swapping and cropping, resulting in inconsistencies in the identification rankings and similarity scores. More specifically we study the change in the rank-1 identification result that is caused by forensic processing of faces-of-interest that are unusable by the commercial recognition systems Further, forensic processing of such faces is more of an art and it is extremely difficult to process faces consistently such that there is a predictable effect on the rank-n identification result.
2K_test_186	Third parties play a prominent role in network-based explanations for successful knowledge transfer Third parties can be either shared or unshared Shared third parties signal insider status and have a predictable positive effect on knowledge transfer Unshared third parties, however signal outsider status and are believed to undermine knowledge transfer Surprisingly, unshared third parties have been ignored in empirical analysis, and so we do not know if or how much unshared third parties contribute to the process, Our results provide a more complete view of how third parties contribute to knowledge sharing, The results also advance our understanding of network-based dynamics defined more broadly. We illustrate how unshared third parties affect the rate at which individuals initiate and sustain knowledge transfer relationships. Results indicate that unshared third parties undermine knowledge sharing, and they also indicate that the magnitude of the negative unshared-third-party effect declines the more unshared third parties overlap in what they know By documenting how knowledge overlap among unshared third parties moderates their negative influence, our results show when the benefits provided by third parties and by bridges i, relationships with outsiders will be opposed versus when both can be enjoyed. Using knowledge transfer data from an online technical forum Empirical.
2K_test_187	How much did a network change since yesterday ? How different is the wiring between Bob 's brain ( a left-handed male ) and Alice 's brain ( a right-handed female ) ? Graph similarity with known node correspondence, the detection of changes in the connectivity of graphs, arises in numerous settings. In this work we formally state the axioms and desired properties of the graph similarity functions, and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs, that assesses the similarity between two graphs on the same nodes. Showcase the advantages of our method over existing similarity measures. We propose DeltaCon a principled, intuitive and scalable algorithm ( e, g employees of a company, customers of a mobile carrier ). Experiments on various synthetic and real graphs Finally, we employ DeltaCon to real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs, and ( b ) do temporal anomaly detection in the who-emails-whom Enron graph.
2K_test_188	Why does Smith follow Johnson on Twitter ?. In most cases the reason why users follow other users is unavailable In this work, we answer this question. Results show that TagF uncovers different, but explainable reasons why users follow other users. By proposing TagF which analyzes the who-follows-whom network ( matrix ) and the who-tags-whom network ( tensor ) simultaneously Concretely, our method decomposes a coupled tensor constructed from these matrix and tensor. The experimental on million-scale Twitter networks.
2K_test_189	A lot of real-world data is spread across multiple domains, Handling such data has been a challenging task, Heterogeneous face biometrics has begun to receive attention in recent years, In real-world scenarios many surveillance cameras capture data in the NIR ( near infrared ) spectrum. However most datasets accessible to law enforcement have been collected in the VIS ( visible light ) domain, Thus there exists a need to match NIR to VIS face images In this paper, we approach the problem. Results report state-of-the-art results. By developing a method to reconstruct VIS images in the NIR domain and vice-versa This approach is more applicable to real-world scenarios since it does not involve having to project millions of VIS database images into learned common subspace for subsequent matching We present a cross-spectral joint l 0 minimization based dictionary learning approach to learn a mapping function between the two domains, One can then use the function to reconstruct facial images between the domains, Our method is open set and can reconstruct any face not present in the training data. We present on the CASIA NIR-VIS v2.
2K_test_190	How often do individuals perform a given communication activity in the Web, such as posting comments on blogs or news ? Could we have a generative model to create communication events with realistic inter-event time distributions ( IEDs ) ? Which properties should we strive to match ?. Current literature has seemingly contradictory results for IED : some studies claim good fits with power laws ; others with non-homogeneous Poisson processes, Given these two approaches, we ask : which is the correct one ? Can we reconcile them all ? We show here that, surprisingly both approaches are correct. Reveal that the SFP mimics their properties very well. Being corner cases of the proposed Self-Feeding Process ( SFP ), We show that the SFP ( a ) exhibits a unifying power, which generates power law tails ( including the so-called `` top-concavity '' that real data exhibits ), as well as short-term Poisson behavior ; ( b ) avoids the `` i, d fallacy '' which none of the prevailing models have studied before ; and ( c ) is extremely parsimonious, requiring usually only one, and in general at most two parameters. Experiments conducted on eight large, diverse real datasets ( e, Youtube and blog comments, e-mails SMSs etc ).
2K_test_191	Gaussian processes ( GPs ) are a flexible class of methods with state of the art performance on spatial statistics applications, However GPs require O ( n3 ) computations and O ( n2 ) storage, and popular GP kernels are typically limited to smoothing and interpolation. To address these difficulties, Kronecker methods have been used to exploit structure in the GP covariance matrix for scalability, while allowing for expressive kernel learning ( Wilson et al, 2014 However fast Kronecker methods have been confined to Gaussian likelihoods for Gaussian processes with non-Gaussian likelihoods. Using our model we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts. We propose new scalable Kronecker methods using a Laplace approximation which involves linear conjugate gradients for inference, and a lower bound on the GP marginal likelihood for kernel learning, Our approach has near linear scaling, requiring O ( DnD+1/D ) operations and O ( Dn2/D ) storage, for n training data-points on a dense D > 1 dimensional grid, Moreover we introduce a log Gaussian Cox process, with highly expressive kernels, for modelling spatiotemporal count processes.
2K_test_192	The difference is that hybrid games also provide all the features of hybrid systems and discrete games, but only deterministic differential equations, Differential games instead provide differential equations with continuous-time game input by both players, but not the luxury of hybrid games, such as mode switches and discrete-time or alternating adversarial interaction. For the combined dynamics of differential hybrid games It shows how hybrid games subsume differential games for proving properties of differential games inductively. This article introduces differential hybrid games, which combine differential games with hybrid games, In both kinds of games, two players interact with continuous dynamics This article augments differential game logic with modalities and introduces differential game invariants and differential game variants.
2K_test_193	Most state-of-the-art action feature extractors involve differential operators, which act as highpass filters and tend to attenuate low frequency action information, This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices. The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation, However at the core of the Gaussian Pyramid is a convolutional smoothing operation, which makes it incapable of generating new features at coarse scales, In order to address this problem. Performance on challenging action recognition and event detection tasks Specifically, our method exceeds and is comparable to MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost. We propose a novel feature enhancing technique called Multi-skIp Feature Stacking ( MIFS ), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space, MIFS compensates for information lost from using differential operators by recapturing information at coarse scales, This recaptured information allows us to match actions at different speeds and ranges of motion, We prove that MIFS enhances the learnability of differential-based features exponentially, The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods, results show significantly improved. Experimental the state-of-the-arts on Hollywood2, UCF101 and UCF50 datasets state-of-the-arts on HMDB51 and Olympics Sports datasets.
2K_test_194	Biometrics has come a long way over the past decade in terms of technologies and devices that are used to verify user identities, Three of the more well studied modalities in this field are the face, iris and fingerprint with the latter two reporting very high user identification/verification rates. In the biometric community there has been little work in studying biomedical signals for user recognition purposes. Our approach yields high values of verification rates, which shows the promise of using these modalities as user specific biometric signatures. In this paper we propose using electromyograph ( EMG ) signals as a person 's biometric signature, The EMG records the motor unit action potentials ( MUAP ) during any physical motion, Keypress timings alone if used as a biometric, are very easy to spoof and hence we fuse this modality with EMG signals, In order to classify these features, we use subspace modeling as well as Bayesian classifiers. Our study is done within the context of a person using a keyboard to type a password or any other fixed phrase, Along with EMG signals, we log key press times for the user and study the feasibility of using this data too as a biometric feature, The experiments have been performed within the context of a user typing a fixed pass phrase at a workstation, The idea is to monitor both biometric modalities when this action is performed and study user verification across data capture sessions and within capture sessions.
2K_test_195	Generative score spaces provide a principled method to exploit generative information, data distribution and hidden variables, The underlying methodology is to derive measures or score functions from generative models, The derived score functions, spanning the so-called score space, provide features of a fixed dimension for discriminative classification. Which is essentially the sufficient statistics of the adopted generative models and does not involve the parameters of generative models, for the score space that seeks to utilize label information. Shows that performance of the score space approach coupled with the proposed discriminative learning method is competitive with state-of-the-art classification methods. In this paper we propose a simple yet effective score space We further propose a discriminative learning method by constraining the classification margin over the score space, The form of score function allows the formulation of simple learning rules, which are essentially the same learning rules for a generative model with an extra posterior imposed over its hidden variables. Experimental evaluation of this approach over two generative models.
2K_test_196	A group 's collective action is an outcome of the group 's decision-making process, which may be reached by either averaging of the individual preferences or following the choices of certain members in the group. Our problem here is to decide which decision process the group has adopted given the data of the collective actions, to infer the group 's decision process to make inference about the group 's decision process. Results of those comparisons. We propose a generic statistical framework from the spatio-temporal data of group trajectories, where each `` trajectory '' is a sequence of group actions. This is achieved by systematically comparing each agent type 's influence on the group actions based on an array of spatio-temporal criteria, are then aggregated into a score.
2K_test_199	Bayesian nonparametric models such as Gaussian processes, provide a compelling framework for automatic statistical modelling : these models have a high degree of flexibility, and automatically calibrated complexity. However automating human expertise remains elusive ; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners to reverse engineer the inductive biases of human learners across a set of behavioral experiments. In this paper we create function extrapolation problems and acquire human responses, and then design a kernel learning framework We use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels. Finally we investigate Occam 's razor in human and Gaussian process based function learning.
2K_test_200	For differential dynamic logic ( dL ) for differential dynamic logic to internalize differential invariants, differential substitutions and derivations as first-class axioms in dL. This paper introduces a new proof calculus that is entirely based on uniform substitution, a proof rule that substitutes a formula for a predicate symbol everywhere, Uniform substitutions make it possible to rely on axioms rather than axiom schemata, Instead of nontrivial schema variables and soundness-critical side conditions on the occurrence patterns of variables, the resulting calculus adopts only a finite number of ordinary dL formulas as axioms, The static semantics of differential dynamic logic is captured exclusively in uniform substitutions and bound variable renamings as opposed to being spread in delicate ways across the prover implementation In addition to sound uniform substitutions, this paper introduces differential forms that make it possible.
2K_test_201	While the growth of the mobile apps market has created significant market opportunities and economic incentives for mobile app developers to innovate, it has also inevitably invited other developers to create rip-offs, Practitioners and developers of original apps claim that copycats steal the original apps idea and demand and have called for app platforms to take action against such copycats, Our study contributes to the growing literature on mobile app consumption by presenting a method to identify copycats and providing evidence of the impact of copycats on an original apps demand. Surprisingly however there has been little rigorous research analyzing whether and how copycats affect an original apps demand The primary deterrent to such research is the lack of an objective way to identify similarities between different apps to compare apps and detect two types of copycats : deceptive and non-deceptive. Based on the detection results Our final results indicate that the effect of copycats on an original apps demand is determined by the quality and level of imitation of the copycat, High-quality non-deceptive copycats negatively affect demand for the originals, In contrast low-quality deceptive copycats positively affect demand for the originals. Using a combination of machine learning techniques such as natural language processing, latent semantic analysis network-based clustering and image analysis, we propose a method. We conduct an econometric analysis to determine the impact of copycat apps on the demand for the original apps on a sample of 10, 100 action game apps by 5, 141 developers that were released in the iOS App Store over five years.
2K_test_202	Hidden information derived from probabilistic generative models of data distributions can be used to construct features for discriminative classifiers, This observation has motivated the development of approaches that attempt to couple generative and discriminative models together for classification. However existing approaches typically feed features derived from generative models to discriminative classifiers, and do not refine the generative models or the feature mapping functions based on classification results, to improve the classifier 's performance. This new framework produces a general classification tool with state-of-the-art performance. In this paper we propose a coupling mechanism developed under the PAC-Bayes framework that can fine-tune the generative models and the feature mapping functions iteratively In our approach, a stochastic feature mapping, which is a function over the random variables of a generative model, is derived to generate feature vectors for a stochastic classifier, We construct a stochastic classifier over the feature mapping and derive the PAC-Bayes generalization bound for the classifier, for both supervised and semi-supervised learning This allows us to jointly learn the feature mapping and the classifier by minimizing the bound with an EM-like iterative algorithm using labeled and unlabeled data, The resulting framework integrates the learning of the discriminative classifier and the generative model and allows iterative fine-tuning of the generative models, and the feedforward feature mappings based on task performance feedback. Our experiments show in three distinct applications.
2K_test_203	Community detection plays a key role in understanding the structure of real-life graphs with impact on recommendation systems, load balancing and routing, Previous community detection methods look for uniform blocks in adjacency matrices. What do real communities in social networks look like ? as a better representation of communities and the relationships between their members, to detect communities with hyperbolic structure. We provide empirical evidence that communities are best represented as having an hyperbolic structure, We show that our method is effective in finding communities with a similar structure to self-declared ones. We detail HyCoM - the Hyperbolic Community Model - and show improvements in compression compared to standard methods We also introduce HyCoM-FIT, a fast parameter free algorithm. However after studying four real networks with ground-truth communities in real social networks, including a community in a blogging platform with over 34 million edges in which more than 1000 users established over 300 000 relations.
2K_test_204	The focus of this paper is to effectively leverage deep Convolutional Neural Networks ( CNNs ) to advance event detection, where only frame level static descriptors can be extracted by the existing CNN toolkits. Our new representation improves the Mean Average Precision ( mAP ) from 27, 6 % to 36, 8 % for the TRECVID MEDTest 14 dataset and from 34, 0 % to 44, 6 % for the TRECVID MEDTest 13 dataset. In this paper we propose a discriminative video representation for event detection over a large scale video dataset when only limited hardware resources are available, This paper makes two contributions to the inference of CNN video representation, First while average pooling and max pooling have long been the standard approaches to aggregating frame level static features, we show that performance can be significantly improved by taking advantage of an appropriate encoding method, Second we propose using a set of latent concept descriptors as the frame descriptor, which enriches visual information while keeping it computationally affordable, The integration of the two contributions results in a new state-of-the-art performance in event detection over the largest video datasets. Compared to improved Dense Trajectories, which has been recognized as the best video representation for event detection.
2K_test_205	To encode the global relationship context of visual events across time and space to use the contextual information to modulate the analysis. We show that our model can outperform state-of-art performances of gated Boltzmann machines ( GBM ) Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information We show it achieves state-of-art performances and possesses the ability to interpolate missing frames, a function that is lacking in GBM. We propose a new neurally-inspired model that can learn and by synthesis process in a predictive coding framework, The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes, In contrast to standard predictive coding models, the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer, and is thus more consistent with neurophysiological observations. We establish the computational feasibility of this model by demonstrating its ability in several aspects, in estimation of contextual information, in terms of prediction accuracy in a variety of tasks.
2K_test_206	We address the problem how high-fidelity verification results about the hybrid systems dynamics of cyber-physical flow systems can be provided at the scale of large ( traffic ) networks without prohibitive analytic cost, for traffic flow components. We propose the use of contracts concisely capturing the conditions for a safe operation in the context of a traffic network This reduces the analysis of flows in the full traffic network to simple arithmetic checks of the local compatibility of the traffic component contracts, while retaining higher-fidelity correctness guarantees of the global hybrid systems models that inherits from correct contracts of the hybrid system components. We evaluate our approach in a case study of a modular traffic network and a prototypical implementation in a model-based analysis and design tool for traffic flow networks.
2K_test_207	Background Computer-assisted diagnosis of dermoscopic images of skin lesions has the potential to improve melanoma early detection, Conclusions Our classifier may aid clinicians in deciding if a skin lesion should be biopsied and can easily be incorporated into a portable tool ( that uses no proprietary equipment ) that could aid clinicians in noninvasively evaluating cutaneous lesions. Objective to generate a lesion severity score. Results The classifier sensitivity for melanoma was 97, 4 % ; specificity was 44, 2 % in a test set of images, In the reader study, the classifier 's sensitivity to melanoma was higher ( P P Limitations This is a retrospective study using existing images primarily chosen for biopsy by a dermatologist, The size of the test set is small. We sought to evaluate the performance of a novel classifier that uses decision forest classification of dermoscopic images. Methods Severity scores were calculated for 173 dermoscopic images of skin lesions with known histologic diagnosis ( 39 melanomas, 14 nonmelanoma skin cancers, and 120 benign lesions ), A threshold score was used to measure classifier sensitivity and specificity A reader study was conducted to compare the sensitivity and specificity of the classifier with those of 30 dermatology clinicians.
2K_test_208	Most Lamb wave localization techniques require that we know the waves velocity characteristics ; yet, in many practical scenarios, velocity estimates can be challenging to acquire, are unavailable or are unreliable because of the complexity of Lamb waves. As a result there is a significant need for new methods that can reduce a systems reliance on a priori velocity information This paper addresses this challenge. We show that both methods can achieve less than 1 cm localization error and have less systematic error than traditional time-of-arrival localization methods. Through two novel source localization methods designed for sparse sensor arrays in isotropic media Both methods exploit the fundamental sparse structure of a Lamb wave 's frequencywavenumber representation The first method uses sparse recovery techniques to extract velocities from calibration data, The second method uses kurtosis and the support earth movers distance to measure the sparseness of a Lamb waves approximate frequency-wavenumber representation These measures are then used to locate acoustic sources with no prior calibration data. We experimentally study each method with a collection of acoustic emission data measured from a 1, 22 m by 1, 22 m isotropic aluminum plate.
2K_test_209	To enable authors and crowd workers to work together. And found it was effective at producing reasonable drafts, However the workers often needed more structure and the authors more context. In this paper we introduce a paradigm for completing complex tasks from wearable devices by leveraging crowdsourcing, and demonstrate its validity for academic writing We explore this paradigm using a collaborative authoring system, called WearWrite which is designed using an Android smartwatch and Google Docs to produce academic papers, WearWrite allows expert authors who do not have access to large devices to contribute bits of expertise and big picture direction from their watch, while freeing them of the obligation of integrating their contributions into the overall document, Crowd workers on desktop computers actually write the document WearWrite addresses these issues by focusing workers on specific tasks and providing select context to authors on the watch. We used this approach to write several simple papers We demonstrate the system 's feasibility by writing this paper using it.
2K_test_210	Road intersections are considered to be serious bottlenecks in urban transportation, More than 44 % of all reported crashes in U, S Occur within intersection areas, which in turn lead to 8, 500 fatalities and approximately 1 million injuries every year Furthermore, because traffic traveling in one direction is generally stopped at busy intersections to allow traffic to flow in another direction, an intersection creates traffic congestion and frustration, The impact of road intersections on traffic delays leads to enormous waste of human and natural resources, According to the 2011 Urban Mobility Report, the delay endured by the average commuter was 34 hours, which costs in aggregate more than $ 100 billion each year in the U, With the advances in Cyber-Physical Systems ( CPS ), autonomous driving as a part of Intelligent Transportation Systems ( ITS ) is likely to be at the heart of urban transportation in the future, Autonomous vehicles have been demonstrated successfully at the DARPA Urban Challenge, General Motors ' Electrical-Networked Vehicle, CMU 's autonomous vehicle and Google 's car are just a few other recently unveiled examples. Therefore it is critical to address safety and throughput concerns as one of the main challenges for autonomous driving through intersections to manage the safe and efficient passage of autonomous vehicles through intersections To achieve high throughput at intersections. Results show that we are able to avoid collisions and increase the throughput of the intersections by up to 96, 24 % compared to common signalized intersections, Under BRIP the optimal intersection capacity utilization of 100 % is achievable in certain cases. In this paper we propose a spatio-temporal technique called the Ballroom Intersection Protocol ( BRIP ) BRIP aims to maximize the utilization of the capacity of the intersection area by increasing parallelism, By enforcing a synchronized arrival of autonomous vehicles at intersections, BRIP allows vehicles approaching from all directions to simultaneously and continuously cross without stopping behind or inside the intersection area.
2K_test_211	The harmful effects of cell phone usage on driver behavior have been well investigated and the growing problem has motivated several several research efforts aimed at developing automated cell phone usage detection systems, Computer vision based approaches for dealing with this problem have only emerged in recent years. To automatically determine if a driver is holding a cell phone close to one of his/her ears ( thus keeping only one hand on the steering wheel ). Demonstrate the method 's efficacy. In this paper we present a vision based method To the best of our knowledge, this is the first such evaluation carried out using this relatively new data, Our approach utilizes the Supervised Descent Method ( SDM ) based facial landmark tracking algorithm to track the locations of facial landmarks in order to extract a crop of the region of interest, Following this features are extracted from the crop and are classified using previously trained classifiers in order to determine if a driver is holding a cell phone. And quantitatively on challenging Strategic Highway Research Program ( SHRP2 ) face view videos from the head pose validation data that was acquired to monitor driver head pose variation under naturalistic driving conditions, We adopt a through approach and benchmark the performance obtained using raw pixels and Histogram of Oriented Gradients ( HOG ) features in combination with various classifiers.
2K_test_212	The use of deductive techniques, such as theorem provers, has several advantages in safety verification of hybrid systems ; however, state-of-the-art theorem provers require manual intervention to handle complex systems. Furthermore there is often a gap between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide directly for differential dynamic logic allows local reasoning to discover useful forward invariants to complete verification tasks. This paper presents an extension to KeYmaera, a deductive verification tool ; the new technique using system designer intuition about performance within particular modes as part of a proof task, Our approach allows the theorem prover to leverage forward invariants, discovered using numerical techniques, as part of a proof of safety, We introduce a new inference rule into the proof calculus of KeYmaera, the forward invariant cut rule, and we present a methodology, which are then used with the new cut rule. We demonstrate how our new approach can be used to complete verification tasks that lie out of the reach of existing automatic verification approaches using several examples, including one involving an automotive powertrain control system.
2K_test_213	In recent years many lawsuits have been filed by individuals seeking legal redress for harms caused by the loss or theft of their personal information, By providing the first comprehensive empirical analysis of data breach litigation, our findings offer insight into the debate over privacy litigation versus privacy regulation. However very little is known about the drivers, mechanics and outcomes of those lawsuits, making it difficult to assess the effectiveness of litigation at balancing organizations ' usage of personal data with individual privacy rights, We investigate two questions : Which data breaches are being litigated ? and Which data breach lawsuits are settling ?. Our results suggest that the odds of a firm being sued are 3, 5 times greater when individuals suffer financial harm, but 6 times lower when the firm provides free credit monitoring Moreover, defendants settle 30 percent more often when plaintiffs allege financial loss, or when faced with a certified class action suit. Using a unique and manually collected database, we analyze court dockets for more than 230 federal data breach lawsuits from 2000 to 2010.
2K_test_214	Epidemics in large complete networks is well established. In contrast we consider epidemics in non-complete networks. We establish the fluid limit macroscopic dynamics of a multi-virus spread over a multipartite network as the number of nodes at each partite or island grows large, The virus spread follows a peer-to-peer random rule of infection in line with the Harris contact process The model conforms to an SIS ( susceptible-infected-susceptible ) type, where a node is either infected or it is healthy and prone to be infected The local ( at node level ) random infection model induces the emergence of structured dynamics at the macroscale.
2K_test_215	To solve a problem posed by the NC DETECT team, North Carolina Division of Public Health ( NC DPH ) and UNC Department of Emergency Medicine Carolina Center for Health Informatics, and facilitated by the ISDS Technical Conventions Committee This use case identifies a need for methodology that detects emerging, potentially novel outbreaks in free-text emergency department ( ED ) chief complaint data. We apply a novel semantic scan statistic approach Our semantic scan approach successfully addresses this problem, eliminates the need for classifying cases into pre-defined syndromes and identifies emerging clusters that public health officials could not have predicted in advance.
2K_test_216	Given a large cloud of multi-dimensional points, and an off-the shelf outlier detection method, why does it take a week to finish ? to eliminate the problem. We discovered that duplicate points create subtle issues, that the literature has ignored : if d max is the multiplicity of the most over-plotted point, typical algorithms are quadratic on d max, we report wall-clock times and our time savings ; and we show that our methods give either exact results, or highly accurate approximate ones. We propose several ways ;.
2K_test_217	Instant access to computing, when and where we need it, has long been one of the aims of research areas such as ubiquitous computing. To make ordinary surfaces instantly interactive. In this paper we describe the WorldKit system, which makes use of a paired depth camera and projector Using this system, touch-based interactivity can without prior calibration, be placed on nearly any unmodified surface literally with a wave of the hand, as can other new forms of sensed interaction From a user perspective, such interfaces are easy enough to instantiate that they could, if desired be recreated or modified `` each time we sat down '' by `` painting '' them next to us From the programmer 's perspective, our system encapsulates these capabilities in a simple set of abstractions that make the creation of interfaces quick and easy, Further it is extensible to new, custom interactors in a way that closely mimics conventional 2D graphical user interfaces, hiding much of the complexity of working in this new domain. We detail the hardware and software implementation of our system, and several example applications built using the library.
2K_test_218	To quantify sharpness of tuning. We propose using the statistical measurement of the sample skewness of the distribution of mean firing rates of a tuning curve For some features, like binocular disparity tuning curves are best described by relatively complex and sometimes diverse functions, making it difficult to quantify sharpness with a single function and parameter, Skewness provides a robust nonparametric measure of tuning curve sharpness that is invariant with respect to the mean and variance of the tuning curve and is straightforward to apply to a wide range of tuning, including simple orientation tuning curves and complex object tuning curves that often can not even be described parametrically, Because skewness does not depend on a specific model or function of tuning, it is especially appealing to cases of sharpening where recurrent interactions among neurons produce sharper tuning curves that deviate in a complex manner from the feedforward function of tuning, Since tuning curves for all neurons are not typically well described by a single parametric function, this model independence additionally allows skewness to be applied to all recorded neurons, maximizing the statistical power of a set of data We also compare skewness with other nonparametric measures of tuning curve sharpness and selectivity Compared to these other nonparametric measures tested, skewness is best used for capturing the sharpness of multimodal tuning curves defined by narrow peaks maximum and broad valleys minima Finally, we provide a more formal definition of sharpness using a shape-based information gain measure and derive and show that skewness is correlated with this definition.
2K_test_219	How can we describe a large, dynamic graph over time ? Is it random ? If not, what are the most apparent deviations from randomness -- a dense block of actors that persists over time, or perhaps a star with many satellite nodes that appears with some fixed periodicity ? In practice, these deviations indicate patterns -- for example, botnet attackers forming a bipartite core with their victims over the duration of an attack, family members bonding in a clique-like fashion over a difficult period of time, or research collaborations forming and fading away over the years. Which patterns exist in real-world dynamic graphs, and how can we find and rank them in terms of importance ? These are exactly the problems we focus on in this work, Our main contributions are. We show that TIMECRUNCH is able to compress these graphs. ( a ) formulation : we show how to formalize this problem as minimizing the encoding cost in a data compression paradigm, ( b ) algorithm : we propose TIMECRUNCH, an effective scalable and parameter-free method for finding coherent, temporal patterns in dynamic graphs and ( c ) practicality : by summarizing important temporal structures and finds patterns that agree with intuition. We apply our method to several large, diverse real-world datasets with up to 36 million edges and 6.
2K_test_220	Generating three-dimensional ( 3D ) as-is Building Information Models ( BIMs ), representative of the existing conditions of buildings, from point cloud data collected by laser scanners is becoming common practice. However generation of such models currently is mostly performed manually, and errors can be introduced during data collection, for assessing the quality of as-is BIMs generated from point cloud data. The results show that the deviation analysis method is capable of identifying almost six times more errors with more than 40 % time savings compared to the physical measurement method. This paper presents a method by analyzing the patterns of geometric deviations between the model and the point cloud data The fundamental assumption is that the point cloud and the as-is BIM generated from the point cloud should corroborate in the depiction of the components and their spatial attributes Major geometric deviations between as-is models and point clouds can indicate potential errors introduced during data collection, processing and/or model generation The research described in this paper provides a taxonomy for patterns of deviations and sources of errors and demonstrates that it is possible to identify the source, magnitude and nature of errors by analyzing the deviation patterns. The method is validated through a comparison with the currently adopted physical measurement method in a case study.
2K_test_221	Learning whether motor sensory or cognitive, requires networks of neurons to generate new activity patterns, As some behaviours are easier to learn than others1, 2 These results suggest that the existing structure of a network can shape learning, On a timescale of hours, it seems to be difficult to learn to generate neural activity patterns that are not consistent with the existing network structure These findings offer a network-level explanation for the observation that we are more readily able to learn new skills when they are related to the skills that we already possess3. We asked if some neural activity patterns are easier to generate than others, Here we investigate whether an existing network constrains the patterns that a subset of its neurons is capable of exhibiting, and if so what principles define this constraint. Here we show that the animals could readily learn to proficiently control the cursor using neural activity patterns that were within the intrinsic manifold, However animals were less able to learn to proficiently control the cursor using activity patterns that were outside of the intrinsic manifold. We employed a closed-loop intracortical braincomputer interface learning paradigm in which Rhesus macaques ( Macaca mulatta ) controlled a computer cursor by modulating neural activity patterns in the primary motor cortex, Using the braincomputer interface paradigm, we could specify and alter how neural activity mapped to cursor velocity, The activity of a neural population can be represented in a high-dimensional space ( termed the neural space ), wherein each dimension corresponds to the activity of one neuron, These characteristic activity patterns comprise a low-dimensional subspace ( termed the intrinsic manifold ) within the neural space, The intrinsic manifold presumably reflects constraints imposed by the underlying neural circuitry. At the start of each session, we observed the characteristic activity patterns of the recorded neural population.
2K_test_222	AbstractUnderstanding the value that individuals assign to the protection of their personal data is of great importance for business, law and public policy, The results highlight the sensitivity of privacy valuations to contextual. To investigate individual privacy valuations and find evidence of endowment and order effects. Individuals assigned markedly different values to the privacy of their data depending on ( 1 ) whether they were asked to consider how much money they would accept to disclose otherwise private information or how much they would pay to protect otherwise public information and ( 2 ) the order in which they considered different offers for their data The gap between such values is large compared with that observed in comparable studies of consumer goods. We use a field experiment informed by behavioral economics and decision research.
2K_test_223	Modern offices are crowded with personal computers, While studies have shown these to be idle most of the time, they remain powered consuming up to 60p of their peak power Hardware-based solutions engendered by PC vendors ( e, low-power states Wake-on-LAN ) have proved unsuccessful because, in spite of user inactivity, these machines often need to remain network active in support of background applications that maintain network presence. Recent proposals have advocated the use of consolidation of idle desktop Virtual Machines ( VMs ), However desktop VMs are often large, requiring gigabytes of memory, Consolidating such VMs creates large network transfers lasting in the order of minutes and utilizes server memory inefficiently, When multiple VMs migrate concurrently, networks become congested and the resulting migration latencies are prohibitive, that transparently migrates only the working set of an idle VM. Can deliver 44 -- 91p energy savings during idle periods of at least 10 minutes, while providing low migration latencies of about 4 seconds and migrating minimal state that is under an order of magnitude of the VMs memory footprint. We present partial VM migration, an approach It creates a partial replica of the desktop VM on the consolidation server by copying only VM metadata, and it transfers pages to the server on-demand, as the VM accesses them, This approach places desktop PCs in low-power mode when inactive and switches them to running mode when pages are needed by the VM running on the consolidation server, To ensure that desktops save energy, we have developed sleep scheduling and prefetching algorithms, as well as the context-aware selective resume framework, a novel approach to reduce the latency of power mode transition operations in commodity PCs. Jettison our software prototype of partial VM migration for off-the-shelf PCs.
2K_test_224	Event detection from real surveillance videos with complicated background environment is always a very hard task. Different from the traditional retrospective and interactive systems designed on this task, which are mainly executed on video fragments located within the event-occurrence time, in this paper we propose a new interactive system constructed on the mid-level discriminative representations ( patches/shots ) which are closely related to the event ( might occur beyond the event-occurrence period ) and are easier to be detected than video fragments. By virtue of such easily-distinguished mid-level patterns, our framework realizes an effective labor division between computers and human participants, The task of computers is to train classifiers on a bunch of mid-level discriminative representations, and to sort all the possible mid-level representations in the evaluation sets based on the classifier scores, The task of human participants is then to readily search the events based on the clues offered by these sorted mid-level representations, For computers such mid-level representations, with more concise and consistent patterns, can be more accurately detected than video fragments utilized in the conventional framework, and on the other hand, a human participant can always much more easily search the events of interest implicated by these location-anchored mid-level representations than conventional video fragments containing entire scenes, Both of these two properties facilitate the availability of our framework in real surveillance event detection applications.
2K_test_226	These problems are motivated by the LASSO framework and have applications in machine learning and computer vision. We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems, Our work shows a close connection between these problems and core questions in algorithmic graph theory. While this connection demonstrates the difficulties of obtaining runtime guarantees, it also suggests an approach of using techniques originally developed for graph algorithms We then show that most of these problems can be formulated as a grouped least squares problem, and give efficient algorithms for this formulation Our algorithms rely on routines for solving quadratic minimization problems, which in turn are equivalent to solving linear systems. Some preliminary experimental work on image processing tasks are also presented.
2K_test_227	Fluctuations in the growth rate of a bacterial culture during unbalanced growth are generally considered undesirable in quantitative studies of bacterial physiology Under well-controlled experimental conditions, however these fluctuations are not random but instead reflect the interplay between intra-cellular networks underlying bacterial growth and the growth environment, Our method has implications for both basic understanding of bacterial physiology and for the classification of bacterial strains. Therefore these fluctuations could be considered quantitative phenotypes of the bacteria under a specific growth condition to identify phenotypic signatures. Here we present a method by time-frequency analysis of unbalanced growth curves measured with high temporal resolution, The signatures are then applied to differentiate amongst different bacterial strains or the same strain under different growth conditions, and to identify the essential architecture of the gene network underlying the observed growth dynamics.
2K_test_228	At the core of Machine Learning ( ML ) analytics is often an expert-suggested model, whose parameters are refined by iteratively processing a training dataset until convergence, The completion time ( i, convergence time ) and quality of the learned model not only depends on the rate at which the refinements are generated but also the quality of each refinement. While data-parallel ML applications often employ a loose consistency model when updating shared model parameters to maximize parallelism, the accumulated error may seriously impact the quality of refinements and thus delay completion time, a problem that usually gets worse with scale, Although more immediate propagation of updates reduces the accumulated error, this strategy is limited by physical network bandwidth, Additionally the performance of the widely used stochastic gradient descent ( SGD ) algorithm is sensitive to step size, Simply increasing communication often fails to bring improvement without tuning step size accordingly and tedious hand tuning is usually needed to achieve optimal performance. Show that our mechanism significantly improves upon static communication schedules. This paper presents Bosen, a system that maximizes the network communication efficiency under a given inter-machine network bandwidth budget to minimize parallel error, while ensuring theoretical convergence guarantees for large-scale data-parallel ML applications, Furthermore Bosen prioritizes messages most significant to algorithm convergence, further enhancing algorithm convergence, Finally Bosen is the first distributed implementation of the recently presented adaptive revision algorithm, which provides orders of magnitude improvement over a carefully tuned fixed schedule of step size refinements for some SGD algorithms. Experiments on two clusters with up to 1024 cores.
2K_test_229	Autonomous driving will play an important role in the future of transportation, Various autonomous vehicles have been demonstrated at the DARPA Urban Challenge [ 3 ], General Motors has recently unveiled their Electrical-Networked Vehicles ( EN-V ) in Shanghai, China [ 5 ], One of the main challenges of autonomous driving in urban areas is transition through cross-roads and intersections, In addition to safety concerns, current intersection management technologies such as stop signs and traffic lights can introduce significant traffic delays even under light traffic conditions. To avoid vehicle collisions at intersections and increase the traffic throughput The focus of this paper is investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous vehicles. And show significant improvements in throughput, We also prove that our protocols avoid deadlock situations inside the intersection area, results show that our new proposed V2V intersection protocols provide both safe passage through the intersection and significantly decrease the delay at the intersection and our latest V2V intersection protocol yields over 85 % overall performance improvement over the common traffic light models. Our goal is to design and develop efficient and reliable intersection protocols. We study how our proposed V2V intersection protocols can be beneficial for autonomous driving The simulation.
2K_test_230	Massive Open Online Courses ( MOOCs ) enable everyone to receive high-quality education. However current MOOC creators can not provide an effective, economical and scalable method to detect cheating on tests, which would be required for any certification, to detect cheating behaviors in online tests. Show that ACD and PCD can detect usage of a cheat sheet with good accuracy and can reduce the overall human resources required to monitor MOOCs for cheating. In this paper we propose a Massive Open Online Proctoring ( MOOP ) framework, which combines both automatic and collaborative approaches The MOOP framework consists of three major components : Automatic Cheating Detector ( ACD ), Peer Cheating Detector ( PCD ), and Final Review Committee ( FRC ), ACD uses webcam video or other sensors to monitor students and automatically flag suspected cheating behavior, Ambiguous cases are then sent to the PCD, where students peer-review flagged webcam video to confirm suspicious cheating behaviors, Finally the list of suspicious cheating behaviors is sent to the FRC to make the final punishing decision.
2K_test_231	Visible light communication ( VLC ) between LED light bulbs and smart-phone cameras has already begun to gain traction for identification and indoor localization applications, To support detection by cameras, the frequencies and data rates are typically limited to below 1kHz and tens of bytes per second ( Bps ). For transmitting data from solid-state luminaries, simultaneously to both cameras and low-power embedded devices. We show that we are able to reliably simultaneously transmit low-speed data at 1, 3 Bps to camera enabled devices and higher-speed data at 104 Bps to low-power embedded devices that consumes less then 204 uA and can be triggered in less then 10ms. In this paper we present a technique used for interior ambient lighting in a manner that is imperceptible to occupants This allows the camera communication VLC channel to also act as a higher speed downstream link and low-power wakeup mechanism for energy-constrained devices Our approach uses Manchester encoding and Binary Frequency Shift Keying ( BFSK ) to modulate the high-speed data stream and applies duty-cycle adjustment to generate the slower camera communication signal, We explore the trade-off between the performance of the two communication channels Our hybrid communication protocol is also compatible with existing IR receivers, This allows lights to communicate with low-cost commodity chipsets and control home appliances such as TVs, AV receivers AC window units, etc Since the majority of energy in many RF communication protocols often goes towards media access and receiving, VLC-triggered wakeup can significantly decrease system energy consumption. We also demonstrate a proof-of-concept wakeup circuit.
2K_test_233	Consider networks in harsh environments, where nodes may be lost due to failure, attack or infection -- how is the topology affected by such events ?. Can we mimic and measure the effect ? to evaluate robustness to construct secure networks operating within malicious environments. We propose a new generative model of network evolution in dynamic and harsh environments, Our model can reproduce the range of topologies observed across known robust and fragile biological networks, as well as several additional transport, communication and social networks, We also develop a new optimization measure based on preserving high connectivity following random or adversarial bursty node loss, propose a new distributed algorithm. Using this measure we evaluate the robustness of several real-world networks and.
2K_test_234	Such datasets arise in many social, economic biological and physical networks As a potential application of the graph Fourier transform, we consider the efficient representation of structured data that utilizes the sparseness of graph signals in the frequency domain. For the representation and analysis of datasets with complex structure. We demonstrate their relation to the generalized eigenvector basis of the graph adjacency matrix. We propose a novel discrete signal processing framework Our framework extends traditional discrete signal processing theory to structured datasets by viewing them as signals represented by graphs, so that signal coefficients are indexed by graph nodes and relations between them are represented by weighted graph edges We discuss the notions of signals and filters on graphs, and define the concepts of the spectrum and Fourier transform for graph signals. And study their properties.
2K_test_235	Computational methods have been widely used to infer properties of complex systems that one can not directly observe experimentally, Viral capsid assembly is a key model system for complex self-assembly for which we lack direct experimental data on critical information, such as kinetic parameters, needed to build models and reveal detailed assembly pathways, We previously sought to learn such hidden parameters with a heuristic optimization approach using gradient and response surface methods applied to the light scattering measurements of three in vitro viral assembly systems : human papillomavirus ( HPV ), hepatitis B virus ( HBV ), and cowpea chlorotic mottle virus ( CCMV ), This method successfully learned plausible kinetic parameters for all the three viruses leading to reconstruction of detailed models of assembly pathways Work is continuing on evaluating different DFO methods and customizing them to inference of kinetic parameters in order to determine the best strategies for inferring unobservable physical parameters in complex biological self-assembly systems. Significant computational challenges however, hinder our ability to construct more precise or detailed models and reliably quantify uncertainty in the inferences, First there is no closed form representation for the quality of fit of models to data, which therefore must be evaluated through computationally costly simulations, Second the problem requires stochastic simulations, and the resulting simulation trajectories must be averaged over many replicates to suppress noise, Third optimization of parameters must account for unknown factors and imprecision of experimental measurements. Preliminary tests show improvements over our custom gradient-based method using a DFO strategy. We explore here improvements based on the idea of derivative free optimization ( DFO ), a class of optimization algorithm that can achieve faster and more accurate fitting, especially on systems characterized by costly, noisy evaluations of quality of fit.
2K_test_236	Background : Intracortical electrode arrays that can record extracellular action potentials from small, targeted groups of neurons are critical for basic neuroscience research and emerging clinical applications In general, these electrode devices suffer from reliability and variability issues, which have led to comparative studies of existing and emerging electrode designs to optimize performance, Conclusions : A more extensive spatial and temporal insight into the chronic electrophysiological performance over time will help uncover the biological and mechanical failure mechanisms of the neural electrodes and direct future research toward the elucidation of design optimization for specific applications. Comparisons of different chronic recording devices have been limited to single-unit ( SU ) activity and employed a bulk averaging approach treating brain architecture as homogeneous with respect to electrode distribution, to quantify evoked multi-unit ( MU ) and local field potential ( LFP ) recordings. Results For example performance metrics in Layer V and stratum pyramidale were initially higher than Layer II/III, but decrease more rapidly, On the other hand, Layer II/III maintained recording metrics longer In addition, chronic changes at the level of layer IV are evaluated using visually evoked current source density, The use of MU and LFP activity for evaluation and tracking biological depth provides a more comprehensive characterization of the electrophysiological performance landscape of microelectrodes. New method : In this study, we optimize the methods and parameters in eight mice visual cortices. These findings quantify the large recording differences stemming from anatomical differences in depth and the layer dependent relative changes to SU and MU recording performance over 6-months, Comparison with existing method ( s ) :.
2K_test_237	Multimedia event detection ( MED ) is an emerging area of research. Previous work mainly focuses on simple event detection in sports and news videos, or abnormality detection in surveillance videos, In contrast we focus on detecting more complicated and generic events that gain more users ' interest, and we explore an effective solution for MED. The results show that our approach outperforms several other state-of-the-art detection algorithms. Moreover our solution only uses few positive examples since precisely labeled multimedia content is scarce in the real world, As the information from these few positive examples is limited, we propose using knowledge adaptation to facilitate event detection Different from the state of the art, our algorithm is able to adapt knowledge from another source for MED even if the features of the source and the target are partially different, Avoiding the requirement that the two domains are consistent in feature types is desirable as data collection platforms change or augment their capabilities and we should be able to respond to this with little or no effort. We perform extensive experiments on real-world multimedia archives consisting of several challenging events.
2K_test_238	ABSTRACT In spite of their many advantages, real -world application of guided -waves for structural health monitoring ( SHM ) of pipelines is still quite limited, The challenges can be discussed under three headings : ( 1 ) Multiple m odes, ( 2 ) Multi -path reflections, and ( 3 ) Sensitivity to environmental and operational conditions ( EOCs ) These challenges are UHYLHZHG LQ WKH DXWKRUV SUHYLRXV ZRUN This paper is part of a study whose objective is to overcome these challenges for damage diagnosis of pipes, while addressing the limitations of the current approaches, That is develop methods that simplify signal while retaining damage information, and perform well as EOC s vary, Moreover the potential of the proposed met hod for online monitoring is illustrated, for wide range of temperature variations and different damage scenarios. To extract a sparse subset of the ultrasonic guided -wave signal s that contain optimal damage information for detection purposes. The results suggest that, for practical ranges of monitoring and damage sizes of interest, the proposed method has low sensitivity to such training factors, High detection performances are obtained for temperature differences up to 14 (, The finding s reported in this paper suggest that although the proposed method is a supervised approach, labeling of the training data does not require prior knowledge about the damage characteristics ( e. In this paper a s upervised method is proposed That is, a discriminant vector is calculated so that the projection s of undamaged and damaged pipes on this vector is separated, In the training stage, data is recorded from intact pipe, and from a pipe with an artificial structural abnormality ( to simulate any variation from intact condition ), During the monitoring stage, test signals are projected on the discriminant vector, and these projections are used as damage -sensitive features for detection purposes, Being a supervised metho d, factors such as EOC variations, and difference in the characteristics of the structural abnormality in training and test data, may affect the detection performance. This paper reports the experiments investigating the extent to which the differences in damage size and damage location, as well as temperatures, can influence the discriminatory power of the extracted damage -sensitive features.
2K_test_239	We present an adaptive graph filtering approach Adaptive graph filters combine decisions from multiple graph filters using a weighting function that is optimized in a semi-supervised manner, We also demonstrate the multiresolution property of adaptive graph filters by connecting them to the diffusion wavelets. In our experiments we apply the adaptive graph filters to the classification of online blogs and damage identification in indirect bridge structural health monitoring.
2K_test_240	We address the problem of action recognition in unconstrained videos. Most noticeably the accuracy of our algorithm reaches 51, 8 % on the challenging HMDB dataset which outperforms the state-of-the-art of 7. We propose a novel content driven pooling that leverages space-time context while being robust toward global space-time transformations, Being robust to such transformations is of primary importance in unconstrained videos where the action localizations can drastically shift between frames, Our pooling identifies regions of interest using video structural cues estimated by different saliency functions To combine the different structural information, we introduce an iterative structure learning algorithm, WSVM ( weighted SVM ), that determines the optimal saliency layout of an action model through a sparse regularizer, A new optimization method is proposed to solve the WSVM ' highly non-smooth objective function. We evaluate our approach on standard action datasets ( KTH, UCF50 and HMDB ).
2K_test_241	Large scale integration of stochastic energy resources in power systems requires probabilistic analysis approaches for comprehensive system analysis, The large-varying grid condition on the aging and stressed power system infrastructures also requires merging of offline security analyses into online operation Meanwhile in computing, the recent rapid hardware performance growth comes from the more and more complicated architecture. Fully utilizing the computing power for specific applications becomes very difficult to the following fundamental tools for power system probabilistic and security analysis : for real-time distribution feeder probabilistic solutions for transmission grid probabilistic analysis, AC contingency calculation solver. Given the challenges and opportunities in both the power system and the computing fields, this paper presents the unique commodity high performance computing system solutions 1 ) a high performance Monte Carlo simulation ( MCS ) based distribution probabilistic load flow solver 2 ) A high performance MCS based transmission probabilistic load flow solver 3 ) A SIMD accelerated based on Woodbury matrix identity on multi-core CPUs By aggressive algorithm level and computer architecture level performance optimizations including optimized data structures, optimization for superscalar out-of-order execution, SIMDization and multi-core scheduling, our software fully utilizes the modern commodity computing systems, makes the critical and computational intensive power system probabilistic and security analysis problems solvable in real-time on commodity computing systems.
2K_test_242	Lamb waves are powerful tools in nondestructive evaluation and structural health monitoring, Researchers use Lamb waves to detect and locate damage across large areas, To best utilize Lamb waves, they are analyzed through two processing steps : baseline subtraction and velocity calibration, Baseline subtraction removes background information from our data and velocity calibration tunes our algorithms Yet, in many scenarios these steps are challenging to implement. Baseline subtraction is challenging due to variable environmental conditions, Velocity calibration is challenging due to multi-modal and dispersive velocity behavior in Lamb waves, To address both challenges. We show these combined approaches to be effective. We present two approaches that combine environmental compensation with self-calibrating localization We discuss temperature compensation strategies based on the scale transform and singular value decomposition We then integrate these with a localization framework known as data-driven matched field processing. In a variety of scenarios.
2K_test_243	Modern robots like todays smartphones, are complex devices with intricate software systems. Introductory robot programming courses must evolve to reflect this reality, by teaching students to make use of the sophisticated tools their robots provide rather than reimplementing basic algorithms. This paper focuses on teaching with Tekkotsu, an open source robot application development framework designed specifically for education But, the curriculum described here can also be taught using ROS, the Robot Operating System that is now widely used for robotics research.
2K_test_244	As airspace becomes ever more crowded, air traffic management must reduce both space and time between aircraft to increase throughput, making on-board collision avoidance systems ever more important, These safety-critical systems must be extremely reliable, and as such many resources are invested into ensuring that the protocols they implement are accurate, Still it is challenging to guarantee that such a controller works properly under every circumstance, This is an important step in formally verified, flyable and distributed air traffic control. In tough scenarios where a large number of aircraft must execute a collision avoidance maneuver, a human pilot under stress is not necessarily able to understand the complexity of the distributed system and may not take the right course, especially if actions must be taken quickly. We prove that the controllers never allow the aircraft to get too close to one another, even when new planes approach an in-progress avoidance maneuver that the new plane may not be aware of, Because these safety guarantees always hold, the aircraft are protected against unexpected emergent behavior which simulation and testing may miss. We consider a class of distributed collision avoidance controllers designed to work even in environments with arbitrarily many aircraft or UAVs.
2K_test_246	Brand Associations one of central concepts in marketing, describe customers ' top-of-mind attitudes or feelings toward a brand, Thus this consumer-driven brand equity often attains the grounds for purchasing products or services of the brand. Traditionally brand associations are measured by analyzing the text data from consumers ' responses to the survey or their online conversation logs, In this paper we propose to go beyond text data and leverage large-scale online photo collections contributed by the general public, which have not been explored so far. We demonstrate that our approach can discover complementary views on the brand associations that are hardly mined from the text data show that our approach outperforms other candidate methods on the both visualization tasks. As a first technical step toward the study of photo-based brand associations, we aim to jointly achieve the following two visualization tasks in a mutually-rewarding way : ( i ) detecting and visualizing core visual concepts associated with brands, and ( ii ) localizing the regions of brand in the images. With experiments on about five millions of images of 48 brands crawled from five popular online photo sharing sites.
2K_test_247	The importance of studying the implications of sampling is twofold : First, sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second, because every such data collection can be perceived as a sample of the real world To the best of our knowledge, our work represents the largest study of propagation patterns of executables. How does malware propagate ? Does it form spikes over time ? Does it resemble the propagation pattern of benign files, such as software patches ? Does it spread uniformly over countries ? How long does it take for a URL that distributes malware to be detected and shut down ? In this work, we answer these questions to efficiently extrapolate crucial properties of the data from a small sample. We discover the SharkFin temporal propagation pattern of executable files, the GeoSplit pattern in the geographical spread of machines that report executables to Symantecs servers, the Periodic Power Law ( Ppl ) distribution of the lifetime of URLs, and we show how. By analyzing patterns from 22 million malicious ( and benign ) files, 6 million hosts worldwide during the month of June 2011 We conduct this study using the WINE database available at Symantec Research Labs Additionally, we explore the research questions raised by sampling on such large databases of executables ; We further investigate the propagation pattern of benign and malicious executables, unveiling latent structures in the way these files spread.
2K_test_248	And outline ideas for future research in this. The idea that digital files can be stored and retrieved later from the memories of people in the crowd To explore and validate this idea. We demonstrate that crowd storage is feasible. This paper introduces the concept of crowd storage Similar to human memory, crowd storage is ephemeral, which means that storage is temporary and the quality of the stored information degrades over time, Crowd storage may be preferred over storing information directly in the cloud, or when it is desirable for information to degrade inline with normal human memories, we created WeStore a system that stores and then later retrieves digital files in the existing memories of crowd workers, WeStore does not store information directly, but rather encrypts the files using details of the existing memories elicited from individuals within the crowd as cryptographic keys, The fidelity of the retrieved information is tied to how well the crowd remembers the details of the memories they provided. Using an existing crowd marketplace ( Amazon Mechanical Turk ), explore design considerations important for building systems that use crowd storage area.
2K_test_249	Our approach is not limited to images, but they provide a convenient query space to test search optimizations. To opportunistic near real-time search of untagged images on smartphones. We present a cloud-based approach that is sensitive to bandwidth and energy constraints Our approach is inspired by the long-established practice of photographers using contact sheets to rapidly visualize a new collection of photographs, and then selecting a subset on which to focus attention, On behalf of each smartphone, the cloud maintains a virtual contact sheet of images that have been captured but not yet uploaded, The virtual contact sheet consists of a set of low-fidelity images as well as full or partial meta-data associated with each image, If search processing on the cloud indicates that a particular low-fidelity object is relevant, then its full-fidelity image can be obtained just-in-time from the corresponding smartphone for further search processing or presentation to the user.
2K_test_251	Autonomous driving is likely to be the heart of urban transportation in the future Autonomous vehicles have the potential to increase the safety of passengers and also to make road trips shorter and more enjoyable, As the first steps toward these goals, many car manufacturers are investing in designing and equipping their vehicles with advanced driver-assist systems, Road intersections are considered to be serious bottlenecks of urban transportation, as more than 44 % of all reported crashes in U, occur within intersection areas which in turn lead to 8, 500 fatalities and approximately 1 million injuries every year, Furthermore the impact of road intersections on traffic delays leads to enormous waste of human and natural resources. In this paper we therefore focus on intersection management in Intelligent Transportation Systems ( ITS ) research, In the future when dealing with autonomous vehicles, it is critical to address safety and throughput concerns that arise from autonomous driving through intersections and roundabouts Our goal is to provide vehicles with a safe and efficient passage method through intersections and roundabouts, to avoid vehicle collisions at intersections and increase traffic throughput, to achieve the above goals to guarantee their safety and efficiency despite these impairments. We show that in addition to intersections, these protocols are also applicable to vehicle crossings at roundabouts, results show that we are able to avoid collisions and also increase the throughput of the intersections up to 87, 82 % compared to common traffic-light signalized intersections. We have designed and developed efficient and reliable intersection protocols In this paper, we introduce new V2V intersection protocols and suggest required modifications. We have been investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous driving, Additionally we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols Our simulation.
2K_test_252	Parameterized probabilistic complex computational ( P 2 C 2 ) models are being increasingly used in computational systems biology for analyzing biological systems, A key challenge is to build mechanistic P 2 C 2 models by combining prior knowledge and empirical data, given that certain system properties are unknown. These unknown components are incorporated into a model as parameters and determining their values has traditionally been a process of trial and error, for discovering parameters in agent-based models of biological systems against behavioral specifications mined from large data-sets. That guarantee a set of desired clinical outcomes with high probability. We present a new algorithmic procedure Our approach uses Bayesian model checking, sequential hypothesis testing and stochastic optimization to synthesize parameters of P 2 C 2 models. We demonstrate our algorithm by discovering the amount and schedule of doses of bacterial lipopolysaccharide in a clinical agent-based model of the dynamics of acute inflammation.
2K_test_253	In this paper we address the problem of jointly summarizing large sets of Flickr images and YouTube videos, for creating not only high-quality video summaries but also novel structural summaries of online images as storyline graphs. We demonstrate that the proposed joint summarization approach outperforms other baselines and our own methods using videos or images only. Starting from the intuition that the characteristics of the two media types are different yet complementary, we develop a fast and easily-parallelizable approach The storyline graphs can illustrate various events or activities associated with the topic in a form of a branching network, The video summarization is achieved by diversity ranking on the similarity graphs between images and video frames, The reconstruction of storyline graphs is formulated as the inference of sparse time-varying directed graphs from a set of photo streams with assistance of videos. For evaluation we collect the datasets of 20 outdoor activities, 7M Flickr images and 16K YouTube videos, Due to the large-scale nature of our problem, we evaluate our algorithm via crowdsourcing using Amazon Mechanical Turk.
2K_test_254	Although widely touted as a replacement for glass slides and microscopes in pathology, digital slides present major challenges in data storage, transmission processing and interoperability OpenSlide is in use today by many academic and industrial organizations world-wide, including many research sites in the United States that are funded by the National Institutes of Health. Since no universal data format is in widespread use for these images today, each vendor defines its own proprietary data formats, analysis tools viewers and software libraries, This creates issues not only for pathologists, but also for interoperability for reading and manipulating digital slides of diverse vendor formats. Can transparently handle multiple vendor formats. In this paper we present the design and implementation of OpenSlide, a vendor-neutral C library The library is extensible and easily interfaced to various programming languages. An application written to the OpenSlide interface.
2K_test_255	Smartwatches are a promising new interactive platform, but their small size makes even basic actions cumbersome, Hence there is a great need for approaches that expand the interactive envelope around smartwatches, allowing human input to escape the small physical confines of the device, to render icons on the user 's skin. We show that these 'skin buttons ' can have high touch accuracy and recognizability, while being low cost and power-efficient. We propose using tiny projectors integrated into the smartwatch These icons can be made touch sensitive, significantly expanding the interactive region without increasing device size. Through a series of experiments.
2K_test_257	To solve the last mile problem for the synthesis of high assurance implementations of controllers for vehicular systems that are executed in todays and future embedded and high performance embedded system processors. We demonstrate High Assurance SPIRALs capability. In this paper we introduce High Assurance SPIRAL High Assurance SPIRAL is a scalable methodology to translate a high level specification of a high assurance controller into a highly resource-efficient, platform-adapted verified control software implementation for a given platform in a language like C or C++, High Assurance SPIRAL proves that the implementation is equivalent to the specification written in the control engineers domain language, Our approach scales to problems involving floating-point calculations and provides highly optimized synthesized code It is possible to estimate the available headroom to enable assurance/performance trade-offs under real-time constraints, and enables the synthesis of multiple implementation variants to make attacks harder, At the core of High Assurance SPIRAL is the Hybrid Control Operator Language ( HCOL ) that leverages advanced mathematical constructs expressing the controller specification to provide high quality translation capabilities, Combined with a verified/certified compiler, High Assurance SPIRAL provides a comprehensive complete solution to the efficient synthesis of verifiable high assurance controllers. By co-synthesizing proofs and implementations for attack detection and sensor spoofing algorithms and deploy the code as ROS nodes on the Landshark unmanned ground vehicle and on a Synthetic Car in a real-time simulator.
2K_test_258	A responsibility we have as researchers is to disseminate the results of our research widely, A primary way we do this is through research publications, We offer thoughts on research challenges and future work that may make our community 's research more accessible. When these publications are not accessible to everyone, some readers will be excluded and the impact of our research limited, In this paper we explore this problem in two ways. Second we reflect on our experience making papers accessible for any CHI 2015 author who requested it. First we report on the accessibility of 1, 811 papers in the technical program of several top conferences related to accessibility and human-computer interaction.
2K_test_260	The robust detection of small targets is one of the key techniques in infrared search and tracking applications. Show that under different clutter backgrounds the proposed method not only works more stably for different target sizes and signal-to-clutter ratio values, but also has better detection performance compared with conventional baseline methods. A novel small target detection method in a single infrared image is proposed in this paper Initially, the traditional infrared image model is generalized to a new infrared patch-image model using local patch construction, Then because of the non-local self-correlation property of the infrared background image, based on the new model small target detection is formulated as an optimization problem of recovering low-rank and sparse matrices, which is effectively solved using stable principle component pursuit, Finally a simple adaptive segmentation method is used to segment the target image and the segmentation result can be refined by post-processing. Extensive synthetic and real data experiments.
2K_test_261	In this paper we investigate a time-sensitive image retrieval problem. Our experimental results show that the proposed algorithm is more successful in time-sensitive image retrieval than other candidate methods, including ranking SVM a PageRank-based image ranking, and a generative temporal topic model. In which given a query keyword, a query time point, and optionally user information, we retrieve the most relevant and temporally suitable images from the database, Inspired by recently emerging interests on query dynamics in information retrieval research, our time-sensitive image retrieval algorithm can infer users ' implicit search intent better and provide more engaging and diverse search results according to temporal trends of Web user photos We model observed image streams as instances of multivariate point processes represented by several different descriptors, and develop a regularized multi-task regression framework that automatically selects and learns stochastic parametric models to solve the relations between image occurrence probabilities and various temporal factors that influence them. Using Flickr datasets of more than seven million images of 30 topics.
2K_test_262	As part of a collaboration with a major California school district. We study the problem of fairly allocating unused classrooms in public schools to charter schools. Show that a nontrivial implementation of the leximin mechanism scales gracefully in terms of running time ( even though the problem is intractable in theory ), and performs extremely well with respect to a number of efficiency objectives. Our approach revolves around the randomized leximin mechanism We extend previous work to the classroom allocation setting, showing that the leximin mechanism is proportional, envy-free efficient and group strategyproof, We also prove that the leximin mechanism provides a ( worst-case ) 4-approximation to the maximum number of classrooms that can possibly be allocated, We take great pains to establish the practicability of our approach, and discuss issues related to its deployment. Our experiments which are based on real data.
2K_test_263	The LD results answer a fundamental question on how to quantify the rate at which the distributed scheme approaches the centralized performance as the inter-sensor communication rate increases. This paper studies the convergence of the estimation error process and the characterization of the corresponding invariant measure in distributed Kalman filtering for potentially unstable and large linear dynamic systems. It is shown that the network achieves weak consensus, the conditional estimation error covariance at a randomly selected sensor converges weakly ( in distribution ) to a unique invariant measure, Further it is proved that as $ \overline { \gamma } \rightarrow \infty $ this invariant measure satisfies the Large Deviation ( LD ) upper and lower bounds, implying that this measure converges exponentially fast ( in probability ) to the Dirac measure $ \delta_ { P^* } $, where $ P^* $ is the stable error covariance of the centralized ( Kalman ) filtering setup. A gossip network protocol termed Modified Gossip Interactive Kalman Filtering ( M-GIKF ) is proposed, where sensors exchange their filtered states ( estimates and error covariances ) and propagate their observations via inter-sensor communications of rate $ \overline { \gamma } $ ; $ \overline { \gamma } $ is defined as the averaged number of inter-sensor message passages per signal evolution epoch The filtered states are interpreted as stochastic particles swapped through local interaction, The paper shows that the conditional estimation error covariance sequence at each sensor under M-GIKF evolves as a random Riccati equation ( RRE ) with Markov modulated switching. By formulating the RRE as a random dynamical system.
2K_test_264	Given a set of k networks, possibly with different sizes and no overlaps in nodes or links, how can we quickly assess similarity between them ? Analogously, are there a set of social theories which, when represented by a small number of descriptive, numerical features effectively serve as a `` signature '' for the network ?. Having such signatures will enable a wealth of graph mining and social network analysis tasks, including clustering outlier detection, visualization etc for solving the above problem. NETSIMILE outperforms baseline competitors. We propose a novel, effective and scalable method, called NETSIMILE Our approach has the following desirable properties : ( a ) It is supported by a set of social theories, ( b ) It gives similarity scores that are size-invariant, ( c ) It is scalable, being linear on the number of links for graph signature extraction our approach enables several mining tasks such as clustering, visualization discontinuity detection network transfer learning, and re-identification across networks. In extensive experiments on numerous synthetic and real networks from disparate domains, We also demonstrate how.
2K_test_265	Short-term forecasting is a ubiquitous practice in a wide range of energy systems, including forecasting demand renewable generation. Although it is known that probabilistic forecasts ( which give a distribution over possible future outcomes ) can improve planning and control, many forecasting systems in practice are just used as point forecast tools, as it is challenging to represent high-dimensional non-Gaussian distributions over multiple spatial and temporal points. We show that this probabilistic model greatly outperforms other methods on the task of accurately modeling potential distributions of power ( as would be necessary in a stochastic dispatch problem. In this paper we apply a recently-proposed algorithm for modeling high-dimensional conditional Gaussian distributions to forecasting wind power and extend it to the non-Gaussian case using the copula transform. On a wind power forecasting task.
2K_test_266	The omnipresence of indoor lighting makes it an ideal vehicle for pervasive communication with mobile devices. To send data to mobile devices. We show how a binary frequency shift keying modulation scheme can be used to transmit data at 1, 25 bytes per second ( fast enough to send an ID code ) from up to 29 unique light sources simultaneously in a single collision domain We also show how tags can demodulate the same signals using a light sensor instead of a camera for low-power applications. In this paper we present a communication scheme that enables interior ambient LED lighting systems using either cameras or light sensors, By exploiting rolling shutter camera sensors that are common on tablets, laptops and smartphones it is possible to detect high-frequency changes in light intensity reflected off of surfaces and in direct line-of-sight of the camera We present a demodulation approach that allows smartphones to accurately detect frequencies as high as 8kHz with 0, In order to avoid humanly perceivable flicker in the lighting, our system operates at frequencies above 2kHz and compensates for the non-ideal frequency response of standard LED drivers by adjusting the light 's duty-cycle, By modulating the PWM signal commonly used to drive LED lighting systems, we are able to encode data that can be used as localization landmarks.
2K_test_267	With the rapid increase in cloud services collecting and using user data to offer personalized experiences, ensuring that these services comply with their privacy policies has become a business imperative for building user trust. However most compliance efforts in industry today rely on manual review processes and audits designed to safeguard user data, and therefore are resource intensive and lack coverage, to automate privacy policy compliance checking in Bing, that allows specification of privacy policies that tracks how user data flows. In this paper we present our experience building and operating a system Central to the design of the system are ( a ) Legal ease-a language that impose restrictions on how user data is handled, and ( b ) Grok-a data inventory for Map-Reduce-like big data systems among programs, Grok maps code-level schema elements to data types in Legal ease, in essence annotating existing programs with information flow types with minimal human input, Compliance checking is thus reduced to information flow analysis of big data systems. The system bootstrapped by a small team, checks compliance daily of millions of lines of ever-changing source code written by several thousand developers.
2K_test_268	Question answering ( Q & A ) communities have been gaining popularity in the past few years, The success of such sites depends mainly on the contribution of a small number of expert users who provide a significant portion of the helpful answers. And so identifying users that have the potential of becoming strong contributers is an important task for owners of such communities, for detecting influential and anomalous users in the underlying user interaction network. Interestingly we find that while the majority of questions on the site are asked by low reputation users, on average a high reputation user asks more questions than a user with low reputation and find they are effective in detecting extreme behaviors such as those of spam users we predict who will become influential long-term contributors. We consider a number of graph analysis methods. We present a study of the popular Q & A website StackOverflow ( SO ), in which users ask and answer questions about software development, algorithms math and other technical topics, The dataset includes information on 3, 5 million questions and 6, 9 million answers created by 1, 3 million users in the years 2008 -- 2012, Participation in activities on the site ( such as asking and answering questions ) earns users reputation, which is an indicator of the value of that user to the site, We describe an analysis of the SO reputation system, and the participation patterns of high and low reputation users, The contributions of very high reputation users to the site indicate that they are the primary source of answers, and especially of high quality answers, Lastly we show an application of our analysis : by considering user contributions over first months of activity on the site.
2K_test_269	The results suggest that even if a notice contains information users care about, it is unlikely to be recalled if only shown in the app store. We examined how the timing impacts the salience of smartphone app privacy notices. Showing the notice during app use significantly increased recall rates over showing it in the app store, which improved recall but did not perform as well as notices shown during app use. In a series of experiments In a web survey and a field experiment, we isolated different timing conditions for displaying privacy notices : in the app store, when an app is started, during app use and after app use Participants installed and played a history quiz app, either virtually or on their phone, After a distraction or delay they were asked to recall the privacy notice 's content, Recall was used as a proxy for the attention paid to and salience of the notice, In a follow-up web survey, we tested alternative app store notices.
2K_test_270	In commercial-off-the-shelf ( COTS ) multi-core systems, a task running on one core can be delayed by other tasks running simultaneously on other cores due to interference in the shared DRAM main memory. Such memory interference delay can be large and highly variable, thereby posing a significant challenge for the design of predictable real-time systems to provide a tight upper bound on the worst-case memory interference. Results show that our approach provides an upper bound very close to our measured worst-case interference. In this paper we present techniques in a COTS-based multi-core system We explicitly model the major resources in the DRAM system, including banks buses and the memory controller By considering their timing characteristics, we analyze the worst-case memory interference delay imposed on a task by other tasks running in parallel, To the best of our knowledge, this is the first work bounding the request re-ordering effect of COTS memory controllers Our work also enables the quantification of the extent by which memory interference can be reduced by partitioning DRAM banks. We evaluate our approach on a commodity multi-core platform running Linux/RK Experimental.
2K_test_271	Virus capsid assembly has been widely studied as a biophysical system, both for its biological and medical significance and as an important model for complex self-assembly processes No current technology can monitor assembly in detail and what information we have on assembly kinetics comes exclusively from invitro studies, There are many differences between the intracellular environment and that of an invitro assembly assay, however that might be expected to alter assembly pathways These models may help us understand how complicated assembly systems may have evolved to function with high efficiency and fidelity in the densely crowded environment of the cell. Here we explore one specific feature characteristic of the intracellular environment and known to have large effects on macromolecular assembly processes : molecular crowding to examine possible effects of crowding on assembly pathways. Simulations suggest a striking difference depending on whether or not a system uses nucleation-limited assembly, with crowding tending to promote off-pathway growth in a nonnucleation-limited model but often enhancing assembly efficiency at high crowding levels even while impeding it at lower crowding levels in a nucleation-limited model. We combine prior particle simulation methods for estimating crowding effects with coarse-grained stochastic models of capsid assembly, using the crowding models to adjust kinetics of capsid simulations.
2K_test_272	Understanding and quantifying the impact of unobserved processes is one of the major challenges of analyzing multivariate time series data. We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models demonstrate the the superior performance of our proposed models. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it. In this paper we analyze a flexible stochastic process model, the generalized linear auto-regressive process ( GLARP ) and identify the conditions under which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood, In particular we examine three examples, including two popular models for count data, e Poisson and Conwey-Maxwell Poisson vector auto-regressive processes, and one powerful model for extreme value data, Gumbel vector auto-regressive processes Experiments on two synthetic datasets, one social network dataset and one climatology dataset.
2K_test_273	Kidney exchanges allow incompatible donor-patient pairs to swap kidneys, but each donation must pass three tests : blood, tissue and crossmatch In practice a matching is computed based on the first two tests, and then a single crossmatch test is performed for each matched patient. However if two crossmatches could be performed per patient, in principle significantly more successful exchanges could take place, In this paper we ask : If we were allowed to perform two crossmatches per patient, could we harness this additional power optimally and efficiently ? for this problem. Our main result is a polynomial time algorithm that almost surely computes optimal -- - up to lower order terms -- - solutions on random large kidney exchange instances.
2K_test_274	The Internet has the potential to accelerate scientific problem solving by engaging a global pool of contributors, A better understanding of such collaborative strategies can inform the design of tools to support distributed collaboration on complex problems. Existing approaches focus on broadcasting problems to many independent solvers, We investigate other approaches that may be advantageous by examining a community for mathematical problem solving. Our results indicate a diversity of ways in which mathematicians are reaching a solution, including by iteratively advancing a solution. -- MathOverflow -- in which contributors communicate and collaborate to solve new mathematical 'micro-problems ' online We contribute a simple taxonomy of collaborative acts derived from a process-level examination of collaborations and a quantitative analysis relating collaborative acts to solution quality.
2K_test_275	Recent computer systems research has proposed using redundant requests to reduce latency, The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request ). However there is no exact analysis of systems with redundancy, This paper presents the first exact analysis of systems with redundancy. We find some surprising results, First the response time of a fully redundant class follows a simple Exponential distribution and that of the non-redundant class follows a Generalized Hyperexponential, Second fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant, We find that in many cases, redundancy outperforms JSQ and Opt-Split with respect to overall response time, making it an attractive solution. We allow for any number of classes of redundant requests, any number of classes of non-redundant requests, any degree of redundancy, and any number of heterogeneous servers, In all cases we derive the limiting distribution on the state of the system, In small ( two or three server ) systems, we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes, and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy We also compare redundancy with other approaches for reducing latency, such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and Join-the-Shortest-Queue ( JSQ ) routing of a class.
2K_test_276	Detecting dyslexia is crucial so that people who have dyslexia can receive training to avoid associated high rates of academic failure, These differences suggest that Dytective could be used to help identify those likely to have dyslexia. Show significant differences between groups who played Dytective. In this paper we present Dytective. The results of a within-subjects experiment with 40 children ( 20 with dyslexia ).
2K_test_278	For reconstructing storyline graphs from large-scale collections of Internet images, and optionally other side information such as friendship graphs, In order to explore further the usefulness of the storyline graphs. We show that the proposed algorithm improves other candidate methods for both storyline reconstruction and image prediction tasks. In this paper we investigate an approach The storyline graphs can be an effective summary that visualizes various branching narrative structure of events or activities recurring across the input photo sets of a topic class we leverage them to perform the image sequential prediction tasks, from which photo recommendation applications can benefit, We formulate the storyline reconstruction problem as an inference of sparse time-varying directed graphs, and develop an optimization algorithm that successfully addresses a number of key challenges of Web-scale problems, including global optimality linear complexity. With experiments on more than 3, 3 millions of images of 24 classes and user studies via Amazon Mechanical Turk.
2K_test_279	For example the Project Tycho provides open access to the count infections for U, states from 1888 to 2013, for 56 contagious diseases ( e, measles influenza ) which include missing values, possible recording errors sudden spikes ( or dives ) of infections. Given a large collection of epidemiological data consisting of the count of d contagious diseases for l locations of duration n, how can we find patterns, rules and outliers ? So how can we find a combined model, for all these diseases, locations and time-ticks ? for large scale epidemiological data which solves the above problem. Demonstrate that FUNNELFIT does indeed discover important properties of epidemics : ( P1 ) disease seasonality, influenza spikes in January, Lyme disease spikes in July and the absence of yearly periodicity for gonorrhea ; ( P2 ) disease reduction effect, the appearance of vaccines ; ( P3 ) local/state-level sensitivity, many measles cases in NY ; ( P4 ) external shock events, historical flu pandemics ; ( P5 ) detect incongruous values. In this paper we present FUNNEL, a unifying analytical model as well as a novel fitting algorithm, FUNNELFIT Our method has the following properties : ( a ) Sense-making : it detects important patterns of epidemics, such as periodicities the appearance of vaccines, external shock events and more ; ( b ) Parameter-free : our modeling framework frees the user from providing parameter values ; ( c ) Scalable : FUNNELFIT is carefully designed to be linear on the input size ; ( d ) General : our model is general and practical, which can be applied to various types of epidemics, including computer-virus propagation as well as human diseases. Extensive experiments on real data.
2K_test_280	From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored : what can we learn about an article from the identities of its likely readers ? To address this question. Demonstrating that our approach is effective. We model the content of news articles and blog posts by attributes of the people who are likely to share them, For example many Twitter users describe themselves in a short profile, labeling themselves with phrases such as `` vegetarian '' or `` liberal, '' By assuming that a user 's labels correspond to topics in the articles he shares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to appear together in the output via a structured sparsity penalty, Finally we show that our approach yields a novel document representation that can be effectively used in many problem settings, from recommendation to modeling news dynamics For example, while the top politics stories will change drastically from one month to the next, the `` politics '' label will still be there to describe them. We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012.
2K_test_281	The M/M/k/setup model where there is a penalty for turning servers on, is common in data centers, call centers and manufacturing systems, Setup costs take the form of a time delay, and sometimes there is additionally a power penalty, as in the case of data centers. While the M/M/1/setup was exactly analyzed in 1964, no exact analysis exists to date for the M/M/k/setup with $ $ k > 1 $ $ k In this paper, we provide the first exact, closed-form analysis for the M/M/k/setup and some of its important variants including systems in which idle servers delay for a period of time before turning off or can be put to sleep. By a new way of combining renewal reward theory and recursive techniques to solve Markov chains with a repeating structure, Our renewal-based approach uses ideas from renewal reward theory and busy period analysis to obtain closed-form expressions for metrics of interest such as the transform of time in system and the transform of power consumed by the system, The simplicity intuitiveness and versatility of our renewal-based approach makes it useful for analyzing Markov chains far beyond the M/M/k/setup, In general our renewal-based approach should be used to reduce the analysis of any 2-dimensional Markov chain which is infinite in at most one dimension and repeating to the problem of solving a system of polynomial equations, In the case where all transitions in the repeating portion of the Markov chain are skip-free and all up/down arrows are unidirectional, the resulting system of equations will yield a closed-form solution. Our analysis is made possible.
2K_test_282	Multimedia event detection ( MED ) is an effective technique for video indexing and retrieval. Current classifier training for MED treats the negative videos equally, However many negative videos may resemble the positive videos in different degrees, Intuitively we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning, Aiming for this to get the decisive attributes of a specific event. Have validated the efficacy of our proposed approach. We use a statistical method on both the positive and negative examples Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation, The resulting fine-grained labels may be not accurate enough to characterize the negative videos, Hence we propose to jointly optimize the fine-grained labels with the knowledge from the visual features and the attributes representations, which brings mutual reciprocality, Our model obtains two kinds of classifiers, one from the attributes and one from the features, which incorporate the informative cues from the fine-grained labels The outputs of both classifiers on the testing videos are fused for detection. Extensive experiments on the challenging TRECVID MED 2012 development set.
2K_test_283	Autonomous driving technologies have been emerging over the past few years, and semi-autonomous driving functionalities have been deployed to vehicles available in the market, Since autonomous driving is realized by the intelligent processing of data from various types of sensors such as LIDAR, the complexity of designing a dependable real-time autonomous driving system is rather high, Although there has been much research on building a reliable real-time system using hardware replication, the resulting systems tend to add significant extra cost due to hardware replication, we summarize SAFER ( System-level Architecture for Failure Evasion in Real-time applications ) our previous work on flexible system design. Therefore an alternative solution would be helpful in building an autonomous vehicle in a cost-effective way, An autonomous driving system is different from the conventional reliable real-time system because it requires ( 1 ) flexible design, ( 2 ) adaptive graceful degradation and ( 3 ) effective use of different modalities of sensors and actuators, To address these characteristics, to provide adaptive graceful degradation and support for using different types of sensors/actuators when a failure happens. We then present a conceptual framework for autonomous vehicles. We motivate our proposed framework with various scenarios, and we describe how SAFER can be extended to support the proposed conceptual framework.
2K_test_284	Coding behavioral video is an important method used by researchers to understand social phenomenon, Recent work has shown that these tasks can be completed quickly by leveraging the parallelism of large online crowds This trade-off between coding quality and privacy protection suggests that researchers can use online crowds to code for some key behaviors in video without compromising participant identity, We conclude with a discussion of how researchers can balance privacy and accuracy on their own data using a system we introduce called Incognito. Unfortunately traditional hand-coding approaches can take days or weeks of time to complete, but using the crowd introduces new concerns about accuracy, reliability privacy and cost To explore these issues, to investigate common practices and challenges with video coding, To explore this more concretely, to investigate whether crowds can accurately recognize instances of commonly coded behaviors. We find accuracy and privacy to be the researchers ' primary concerns, and show that the crowd yields accurate results, and find as expected, that workers ' ability to identify participants decreases as blur level increases The workers ' ability to accurately and reliably code behaviors also decreases, but not as steeply as the identity test. Then we demonstrate a method for obfuscating participant identity with a video blur filter. We conducted interviews with 12 researchers who frequently code behavioral video, we used sample videos.
2K_test_285	For pipe guided wave inspection systems, it can often be difficult to achieve accurate localization performance due to the pipe 's geometry Many localization techniques focus on the first arrival for processing, but this often results in a poor circumferential resolution, Furthermore the pipe 's circular geometry generates multipath arrivals that make data interpretation difficult. In this paper however, we utilize this multipath behavior. We show that our method significantly improves circumferential resolution and reduces localization artifacts when compared with the standard delay-and-sum method. By combining the standard delay-and-sum localization method with a simple multipath model for a pipe. Using experimental data from a transmitting source.
2K_test_286	If Lisa visits Dr, Brown and there is no record of the drug he prescribed her, can we find it ? Data sources, much to analysts ' dismay, are too often plagued with incompleteness, making business analytics over the data difficult. Data entries with incomplete values are ignored, making some analytic queries fail to accurately describe how an organization is performing to choose a correct value after viewing possible values and why they were inferred. We introduce a principled way of performing value imputation on missing values, allowing a user We achieve this by turning our data into a graph network and performing link prediction on nodes of interest using the belief propagation algorithm.
2K_test_287	A cognitive assistance application combines a wearable device such as Google Glass with cloudlet processing to provide step-by-step guidance on a complex task, We then reflect on the difficulties we faced in building these applications, and suggest future research that could simplify the creation of similar applications. For narrow and well-defined tasks that require specialized knowledge and/or skills. In this paper we focus on user assistance We describe proof-of-concept implementations for four different tasks : assembling 2D Lego models, freehand sketching playing ping-pong, and recommending context-relevant YouTube tutorials.
2K_test_288	The utilization of Building Information Modeling ( BIM ) has been growing significantly and translating into the support of various tasks within the construction industry, In relation to such a growth, many approaches that leverage dimensions of information stored in BIM model are being developed, Through this it is possible to allow all stakeholders to retrieve and generate information from the same model, enabling them to work cohesively It is believed to benefit the industry by providing a computable BIM and enabling all project participants to extract any information required for decision making, Finally the framework is used to identify areas to extend BIM research. To identify gaps of existing work and evaluate new studies in this area. And the result reveals a research gap for BIM applications in the project domains of quality, safety and environmental management. A BIM application framework is developed and discussed in this paper Such a framework gives an overview of BIM applications in the construction industry, A computable multi-dimensional ( nD ) model is difficult to establish in these areas because with continuously changing conditions, the decision making rules for evaluating whether an individual component is considered good quality, or whether a construction site is safe, also vary as the construction progresses, A process of expanding from 3D to computable nD models, specifically a possible way to integrate safety, quality and carbon emission variables into BIM during the construction phase of a project is explained in this paper. A literature review within this framework, has been conducted As examples, the processes of utilizing nD models on real construction sites are described.
2K_test_289	Let us consider that someone is starting a research on a topic that is unfamiliar to them. Which seminal papers have influenced the topic the most ? What is the genealogy of the seminal papers in this topic ? These are the questions that they can raise, which we try to answer in this paper, that finds a set of seminal papers on a given topic, that constructs a genealogy of the seminal papers. We show the effectiveness and efficiency of our approach. First we propose an algorithm We also address the performance and scalability issues of this sophisticated algorithm Next, we discuss the measures to decide how much a paper is influenced by another paper, Then we propose an algorithm by using the influence measure and citation information. Finally through extensive experiments with a large volume of a real-world academic literature data.
2K_test_290	Online social networks and the World Wide Web lead to large underlying graphs that might not be completely known because of their size, To compute reliable statistics, we have to resort to sampling the network. In this paper we investigate four network sampling methods to estimate the network degree distribution and the so-called biased degree distribution of a 3, 7 million wireless subscriber network. Node sampling yields Pareto optimal sample sizes in terms of the Kolomogorov-Smirnov statistic for the degree distribution, while node-by-edge sampling yields optimal sample sizes for the biased distribution, We also find that random walk sampling performs better than the Metropolis-Hastings random walk. We measure the quality of our estimates of the degree distributions by using the Kolmogorov-Smirnov statistic, Among all four sampling methods.
2K_test_291	As kidney exchange programs are growing, manipulation by hospitals becomes more of an issue. Assuming that hospitals wish to maximize the number of their own patients who receive a kidney, they may have an incentive to withhold some of their incompatible donorpatient pairs and match them internally, thus harming social welfare, for hospitals to report all their incompatible pairs. Suggest that in practice our mechanism performs much closer to optimal. We study mechanisms for two-way exchanges that are strategyproof, make it a dominant strategy We establish lower bounds on the welfare loss of strategyproof mechanisms, both deterministic and randomized, and propose a randomized mechanism that guarantees at least half of the maximum social welfare in the worst case. Simulations using realistic distributions for blood types and other parameters.
2K_test_292	AbstractVulnerability assessment serves to identify vulnerabilities, develop responses and drive the risk-management process, In identifying vulnerabilities it is fundamental to identify and rank critical assets, which include vital systems, facilities processes and information necessary to maintain continuity of service, During emergencies in the facility management domain, first responders typically search for critical assets, both related to business continuity and value to the organization. To reason about building systems and content to support vulnerability assessment in building emergencies caused by failures in building systems ( e, sprinkler line leak power outage ). This paper presents a formalized approach The developed reasoning approach enables a first responder to perform flexible searches and prioritize critical spaces and pieces of equipment that need to be protected in an emergency by leveraging existing building and content representations found in building information models ( BIM ).
2K_test_293	Data locality and parallelism are critical optimization objectives for performance on modern multi-core machines. Both coarse-grain parallelism ( e, multi-core ) and fine-grain parallelism ( e, vector SIMD ) must be effectively exploited, but despite decades of progress at both ends, current compiler optimization schemes that attempt to address data locality and both kinds of parallelism often fail at one of the three objectives, We address this problem aims for integrated data locality, multi-core parallelism and SIMD execution of programs. Exhibiting significant performance improvements over existing compilers. By proposing a 3-step framework, which We define the concept of vectorizable codelets, with properties tailored to achieve effective SIMD code generation for the codelets, We leverage the power of a modern high-level transformation framework to restructure a program to expose good ISA-independent vectorizable codelets, exploiting multi-dimensional data reuse, Then we generate ISA-specific customized code for the codelets, using a collection of lower-level SIMD-focused optimizations. We demonstrate our approach on a collection of numerical kernels that we automatically tile.
2K_test_294	Abstract Guided wave ultrasonics is an attractive monitoring technique for damage diagnosis in large-scale plate and pipe structures, Damage can be detected by comparing incoming records with baseline records collected on intact structure, Researchers developed temperature compensation methods to eliminate the effects of temperature variation, but they have limitations in practical implementations. However during long-term monitoring, environmental and operational conditions often vary significantly and produce large changes in the ultrasonic signals, thereby challenging the baseline comparison based damage detection. We show that our method accurately detects the presence of a mass scatterer, and is robust to the environmental and operational variations exhibited in the practical system. In this paper we develop a robust damage detection method based on singular value decomposition ( SVD We show that the orthogonality of singular vectors ensures that the effect of damage and that of environmental and operational variations are separated into different singular vectors. We report on our field ultrasonic monitoring of a 273, 05mm outer diameter pipe segment, which belongs to a hot water piping system in continuous operation, We demonstrate the efficacy of our method on experimental pitchcatch records collected during seven months.
2K_test_295	For monitoring virtual machines ( VMs ) in the cloud. We propose a non-intrusive approach At the core of this approach is a mechanism for selective real-time monitoring of guest file updates within VM instances, This mechanism is agentless, requiring no guest VM support, It has low virtual I/O overhead, low latency for emitting file updates, and a scalable design, Its central design principle is distributed streaming of file updates inferred from introspected disk sector writes, The mechanism called DS-VMI, enables many system administration tasks that involve monitoring files to be performed outside VMs.
2K_test_297	In social settings individuals interact through webs of relationships, This paper extends to signals on graphs DSP and its basic tenets, including filters convolution z -transform, impulse response spectral representation, Fourier transform frequency response, and illustrates DSP on graphs by classifying blogs, linear predicting and compressing data from irregularly located weather stations, or predicting behavior of customers of a mobile service provider. Each individual is a node in a complex network ( or graph ) of interdependencies and generates data, to describe represent transform, analyze process or synthesize these well ordered time or image signals. We label the data by its source, or formally stated we index the data by the nodes of the graph, The resulting signals ( data indexed by the nodes ) are far removed from time or image signals indexed by well ordered time samples or pixels DSP, discrete signal processing provides a comprehensive, elegant and efficient methodology.
2K_test_298	Spatial Pyramid Matching ( SPM ) assumes that the spatial Bag-of-Words ( BoW ) representation is independent of data. However evidence has shown that the assumption usually leads to a suboptimal representation, to learn the BoW representation from data directly at the BoW level. Validate that JS Tiling outperforms the SPM and the state-of-the-art methods, The runtime comparison demonstrates that selecting BoW representations by JS Tiling is more than 1, 000 times faster than running classifiers, Besides JS Tiling is an important component contributing to CMU Teams ' final submission in TRECVID 2012 Multimedia Event Detection. In this paper we propose a novel method called Jensen-Shannon ( JS ) Tiling The proposed JS Tiling is especially appropriate for large-scale datasets as it is orders of magnitude faster than existing methods, but with comparable or even better classification precision. Experimental results on four benchmarks including two TRECVID12 datasets.
2K_test_299	Imply improved parallel randomized algorithms for several problems, including single-source shortest paths, maximum flow minimum-cost flow, and approximate maximum flow. For solving symmetric diagonally dominant ( SDD ) linear systems. We present the design and analysis of a nearly-linear work parallel algorithm On input an SDD n-by-n matrix A with m nonzero entries and a vector b, our algorithm computes a vector $ \tilde { x } $ such that $ \|\tilde { x } - A^ { + } b\|_ { A } \leq\varepsilon\cdot\| { A^ { + } b } \|_ { A } $ in $ O ( m\log^ { O ( 1 ) } { n } \log { \frac { 1 } { \varepsilon } } ) $ work and $ O ( m^ { 1/3+\theta } \log\frac { 1 } { \varepsilon } ) $ depth for any ? > 0, where A + denotes the Moore-Penrose pseudoinverse of A, The algorithm relies on a parallel algorithm for generating low-stretch spanning trees or spanning subgraphs To this end, we first develop a parallel decomposition algorithm that in O ( mlog O ( 1 ) n ) work and polylogarithmic depth, partitions a graph with n nodes and m edges into components with polylogarithmic diameter such that only a small fraction of the original edges are between the components, This can be used to generate low-stretch spanning trees with average stretch O ( n ? ) in O ( mlog O ( 1 ) n ) work and O ( n ? ) depth for any ? > 0, Alternatively it can be used to generate spanning subgraphs with polylogarithmic average stretch in O ( mlog O ( 1 ) n ) work and polylogarithmic depth, We apply this subgraph construction to derive a parallel linear solver. By using this solver in known applications.
2K_test_300	Heating Ventilation and Air-Conditioning ( HVAC ) systems account for more than 15 % of the total energy consumption in the US In order to improve the energy efficiency of HVAC systems, researchers have developed hundreds of algorithms to automatically analyze their performance. However the complex information, such as configurations of HVAC systems, layouts and materials of building elements and dynamic data from the control systems, required by these algorithms inhibits the process of deploying them in real-world facilities To address this challenge, to identify and document the information requirements from the publications that describe these algorithms This paper presents the extensions to the IDM approach and the results of using it to identify information requirements for performance analysis algorithms of HVAC systems. We envision a framework that automatically integrates the required information items and provides them to the performance analysis algorithms for HVAC systems, This paper presents an approach We extend the Information Delivery Manual ( IDM ) approach so that the identified information requirements can be mapped to multiple information sources that use various formats and schemas.
2K_test_301	A password composition policy restricts the space of allowable passwords to eliminate weak passwords that are vulnerable to statistical guessing attacks. Usability studies have demonstrated that existing password composition policies can sometimes result in weaker password distributions ; hence a more principled approach is needed, for optimizing password composition policies. We introduce the first theoretical model Our main positive result is an algorithm that -- with high probability -- - constructs almost optimal policies ( which are specified as a union of subsets of allowed passwords ), and requires only a small number of samples of users ' preferred passwords. We study the computational and sample complexity of this problem under different assumptions on the structure of policies and on users ' preferences over passwords, We complement our with simulations using a real-world dataset of 32 million passwords.
2K_test_302	Time synchronization in wireless sensor networks is important for event ordering and efficient communication scheduling. To improve synchronization and significantly reduce clock drift over long periods of time. We show that our new synchronization circuit consumes 60 % less power than the original design and is able to correct clock drift rates to within 0, 01 ppm without power hungry and expensive precision clocks. In this paper we introduce an external hardwarebased clock tuning circuit that can be used without waking up the host MCU This is accomplished through two main hardware sub-systems First, we improve upon the circuit presented in [ 1 ] that synchronizes clocks using the ambient magnetic fields emitted from power lines, The new circuit uses an electric field front-end as opposed to the original magnetic-field sensor, which makes the design more compact, lower-power lower-cost exhibit less jitter and improves robustness to noise generated by nearby appliances Second, we present a low-cost hardware tuning circuit that can be used to continuously trim a micro-controller 's low-power clock at runtime, Most time synchronization approaches require a CPU to periodically adjust internal counters to accommodate for clock drift, Periodic discrete updates can introduce interpolation errors as compared to continuous update approaches and they require the CPU to expend energy during these wake up periods, Our hardware-based external clock tuning circuit allows the main CPU to remain in a deep-sleep mode for extended periods while an external circuit compensates for clock drift.
2K_test_303	Split fabrication the process of splitting an IC into an untrusted and trusted tier, facilitates access to the most advanced semiconductor manufacturing capabilities available in the world without requiring disclosure of design intent. While researchers have investigated the security of logic blocks in the context of split fabrication, the security of IP blocks, another key component of an SoC, has not been examined to design these blocks efficiently and securely. Shows that they are vulnerable to recognition attacks at the untrusted foundry due to the use of standardized floorplans and leaf cell layouts, and demonstrate their effectiveness using 130nm split fabricated testchips. Our security analysis of IP block designs, specifically embedded memory and analog components We propose methodologies.
2K_test_304	In this paper we address the distributed estimation of a dynamic ( time varying ) random field, The dynamic field is globally observable ( by the entire sensor network ), but not locally observable ( at each sensor ), the estimate at each sensor is unbiased with bounded mean-squared estimation error. We present a distributed Kalman-type estimator such that The challenges with distributed estimation by a network of sensors lie in the estimation of fields with unstable dynamics Our distributed Kalman filter type estimator, which includes a consensus step on the pseudo-innovations, a modified version of the filter innovations, is able to track arbitrary unstable dynamics, as long as the sensor network connectivity is above a threshold determined by the degree of instability of the field dynamics, regardless of the specifics of the local observations.
2K_test_305	Self-powered vehicles that interact with the physical world, such as spacecraft require computing platforms with predictable timing behavior and a low energy demand, Energy consumption can be reduced by choosing energy-efficient designs for both hardware and software components of the platform. We address the problem of allocating real-time software components onto heterogeneous cores such that total energy is minimized. Shows that neither balancing the load nor assigning all load to the cheapest core is the best load distribution strategy, unless the cores are extremely alike or extremely different. We leverage the state-of-the-art in energy-efficient hardware design by adopting Heterogeneous Multi-core Processors with support for Dynamic Voltage and Frequency Scaling and Dynamic Power Management, Our approach is to start from an analytically justified target load distribution and find a task assignment heuristic that approximates it, The optimal load distribution is then formulated as a solution to a convex optimization problem A heuristic that approximates this load distribution and an alternative method that leverages the solution explicitly are proposed as viable task assignment methods. Our analysis The proposed methods are compared to state-of-the-art on simulated problem instances and in a case study of a soft-real-time application on an off-the-shelf ARM big.
2K_test_306	Harnessing crowds can be a powerful mechanism for increasing innovation, Our results have implications for improving creativity and building systems for distributed crowd innovation. However current approaches to crowd innovation rely on large numbers of contributors generating ideas independently in an unstructured way, which aims to make idea generation more effective and less reliant on chance. We show that distributed analogical idea generation leads to better ideas than example-based approaches. We introduce a new approach called distributed analogical idea generation Drawing from the literature in cognitive science on analogy and schema induction, our approach decomposes the creative process in a structured way amenable to using crowds. In three experiments and investigate the conditions under which crowds generate good schemas and ideas.
2K_test_308	Reranking has been a focal technique in multimedia retrieval due to its efficacy in improving initial retrieval results. Current reranking methods however, mainly rely on the heuristic weighting. Results validate the efficacy and the efficiency of the proposed method on both image and video search tasks, Notably SPaR achieves by far the best result on the challenging TRECVID multimedia event search task. In this paper we propose a novel reranking approach called Self-Paced Reranking ( SPaR ) As its name suggests, SPaR utilizes samples from easy to more complex ones in a self-paced fashion, SPaR is special in that it has a concise mathematical objective to optimize and useful properties that can be theoretically verified It on one hand offers a unified framework providing theoretical justifications for current reranking methods, and on the other hand generates a spectrum of new reranking schemes, This paper also advances the state-of-the-art self-paced learning research which potentially benefits applications in other fields.
2K_test_309	Memory layout transformations via data reorganization are very common operations, which occur as a part of the computation or as a performance optimization in data-intensive applications. These operations require inefficient memory access patterns and roundtrip data movement through the memory hierarchy, failing to utilize the performance and energy-efficiency potentials of the memory subsystem. And demonstrate that HAMLeT can achieve close to peak system utilization, offering up to an order of magnitude performance improvement compared to the CPU and GPU memory subsystems which does not employ HAMLeT. This paper proposes a high-bandwidth and energy-efficient hardware accelerated memory layout transform ( HAMLeT ) system integrated within a 3D-stacked DRAM, HAMLeT uses a low-overhead hardware that exploits the existing infrastructure in the logic layer of 3D-stacked DRAMs, and does not require any changes to the DRAM layers, yet it can fully exploit the locality and parallelism within the stack by implementing efficient layout transform algorithms. We analyze matrix layout transform operations ( such as matrix transpose, matrix blocking and 3D matrix rotation ).
2K_test_310	This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities. To develop integrate and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations ' information systems, to detect independently developed red team inserts of malicious insider activities. We defined over 100 data features in seven categories We have achieved area under the ROC curve values of up to 0, 979 and lift values of 65 on the top 50 user-days identified on two months of real data. Our system combines structural and semantic information from a real corporate database of monitored activity on their users ' computers We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns temporal sequences, and normal graph evolution Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams, We have also developed a visual language for specifying combinations of features, baselines peer groups time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. Based on approximately 5, 5 million actions per day from approximately 5.
2K_test_311	Support for multiple concurrent applications is an important enabler for promoting the use of sensor networks as an infrastructure technology, where multiple users can deploy their applications independently In such a scenario, different applications on a node may transmit packets at distinct periods, causing the node to change from sleep to active state more often, which negatively impacts the energy consumption of the whole network. In this paper we propose to batch the transmissions together. And show how it can achieve a duty-cycle comparable to an ideal TDMA approach. By defining a harmonizing period to align the transmissions from multiple applications at periodic boundaries, This harmonizing period is then leveraged to design a protocol that coordinates the transmissions across nodes and provides real-time guarantees in a multi-hop network This protocol, which we call Network- Harmonized Scheduling ( NHS ), takes advantage of the periodicity introduced to assign offsets to nodes at different hop-levels such that collisions are always avoided, and deterministic behavior is enforced NHS is a light-weight and distributed protocol that does not require any global state-keeping mechanism. We implemented NHS on the Contiki operating system.
2K_test_312	For computing the Voronoi diagram of a set of n points in constant-dimensional Euclidean space. We describe a new algorithm The running time of our algorithm is O ( f log n log ) where f is the output complexity of the Voronoi diagram and is the spread of the input, the ratio of largest to smallest pairwise distances, Despite the simplicity of the algorithm and its analysis, it improves on the state of the art for all inputs with polynomial spread and near-linear output size, The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation Then, the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity, yielding the output sensitive bound, The removal only involves local flips and is inspired by kinetic data structures.
2K_test_313	Color descriptors are one of the important features used in content-based image retrieval, The dominant color descriptor ( DCD ) represents a few perceptually dominant colors in an image through color quantization, For image retrieval based on DCD, the earth movers distance ( EMD ) and the optimal color composition distance were proposed to measure the dissimilarity between two images. Although providing good retrieval results, both methods are too time-consuming to be used in a large image database, To solve the problem. The results reveal that our approach achieves almost the same results with the EMD in linear time. We propose a new distance function that calculates an approximate earth movers distance in linear time To calculate the dissimilarity in linear time, the proposed approach employs the space-filling curve for multidimensional color space, To improve the accuracy, the proposed approach uses multiple curves and adjusts the color positions, As a result our approach achieves order-of-magnitude time improvement but incurs small errors. We have performed extensive experiments to show the effectiveness and efficiency of the proposed approach.
2K_test_314	In previous work we developed the scaled SIS process, which models the dynamics of SIS epidemics over networks, With the scaled SIS process, we can consider networks that are finite-sized and of arbitrary topology ( i, we are not restricted to specific classes of networks ), We derived for the scaled SIS process a closed-form expression for the time-asymptotic probability distribution of the states of all the agents in the network, This closed-form solution of the equilibrium distribution explicitly exhibits the underlying network topology through its adjacency matrix. This paper determines which network configuration is the most probable. We illustrate our results. We prove that for a range of epidemics parameters, this combinatorial problem leads to a submodular optimization problem, which is exactly solvable in polynomial time, We relate the most-probable configuration to the network structure, in particular to the existence of high density subgraphs Depending on the epidemics parameters, subset of agents may be more likely to be infected than others ; these more-vulnerable agents form subgraphs that are denser than the overall network. With a 193 node social network and the 4941 node Western US power grid under different epidemics parameters.
2K_test_315	For structured datasets that arise from social, economic biological and physical networks. Demonstrate that the proposed approach achieves high classification accuracy. We propose a novel discrete signal processing framework Our framework extends traditional discrete signal processing theory to datasets with complex structure that can be represented by graphs, so that data elements are indexed by graph nodes and relations between elements are represented by weighted graph edges, We interpret such datasets as signals on graphs, introduce the concept of graph filters for processing such signals, and discuss important properties of graph filters, including linearity shift-invariance and invertibility We then demonstrate the application of graph filters to data classification by demonstrating that a classifier can be interpreted as an adaptive graph filter.
2K_test_316	Semantic search in video is a novel and challenging problem in information and multimedia retrieval, We share our observations and lessons in building such a state-of-the-art system, which may be instrumental in guiding the design of the future system for semantic search in video. Existing solutions are mainly limited to text matching, in which the query words are matched against the textual metadata generated by users. Where the proposed system achieves the best performance. This paper presents a state-of-the-art system for event search without any textual metadata or example videos, The system relies on substantial video content understanding and allows for semantic search over a large collection of videos. The novelty and practicality is demonstrated by the evaluation in NIST TRECVID 2014.
2K_test_318	To identify identical twins. We have shown that the proposed ALDA method with the aid of facial asymmetry features significantly outperforms other well-established facial descriptors ( LBP, LTP LTrP ) and the ALDA subspace method does a much better job in distinguishing identical twins than LDA, We are able to achieve 48, 50 % VR at 0, 1 % FAR for identifying family membership of identical twin individuals in the crowd and an averaged 82, 58 % VR at 0, 1 % FAR for verifying identical twin individuals within the same family, a significant improvement over traditional descriptors and traditional LDA method. In this work we have proposed an Augmented Linear Discriminant Analysis ( ALDA ) approach It learns a common subspace that not only can identify from which family the individual comes, but also can distinguish between individuals within the same family. We evaluate the ALDA against the traditional LDA approach for subspace learning on the Notre Dame twin database.
2K_test_319	Accurate inference of molecular and functional interactions among genes, especially in multicellular organisms such as Drosophila, often requires statistical analysis of correlations not only between the magnitudes of gene expressions, but also between their temporal-spatial patterns, The ISH ( in-situ-hybridization ) -based gene expression micro-imaging technology offers an effective approach to perform large-scale spatial-temporal profiling of whole-body mRNA abundance. However analytical tools for discovering gene interactions from such data remain an open challenge due to various reasons, including difficulties in extracting canonical representations of gene activities from images, and in inference of statistically meaningful networks from such representations for inferring gene interaction networks from Drosophila embryonic ISH images. We demonstrate the effectiveness of our approach in network building Furthermore, we report results where GINI makes novel and interesting predictions of gene interactions. In this paper we present GINI, a machine learning system GINI builds on a computer-vision-inspired vector-space representation of the spatial pattern of gene expression in ISH images, enabled by our recently developed system ; and a new multi-instance-kernel algorithm that learns a sparse Markov network model, in which every gene ( i, node ) in the network is represented by a vector-valued spatial pattern rather than a scalar-valued gene intensity as in conventional approaches such as a Gaussian graphical model By capturing the notion of spatial similarity of gene expression, and at the same time properly taking into account the presence of multiple images per gene via multi-instance kernels, GINI is well-positioned to infer statistically sound, and biologically meaningful gene interaction networks from image data Software for GINI is available at http : //sailing. Using both synthetic data and a small manually curated data set, on a large publicly available collection of Drosophila embryonic ISH images from the Berkeley Drosophila Genome Project.
2K_test_320	Background : Association analysis using genome-wide expression quantitative trait locus ( eQTL ) data investigates the effect that genetic variation has on cellular pathways and leads to the discovery of candidate regulators Furthermore, this analysis demonstrates the potential of GFlasso as a powerful computational tool for eQTL studies that exploit the rich structural information among expression traits due to correlation, regulation or other forms of biological dependencies. Traditional analysis of eQTL data via pairwise statistical significance tests or linear regression does not leverage the availability of the structural information of the transcriptome, such as presence of gene networks that reveal correlation and potentially regulatory relationships among the study genes, to reanalyze a genome-wide yeast dataset. Results : While eQTL hotspots in yeast have been reported previously as genomic regions controlling multiple genes, our analysis reveals additional novel eQTL hotspots and, more interestingly uncovers groups of multiple contributing eQTL hotspots that affect the expression level of functional gene modules To our knowledge, our study is the first to report this type of gene regulation stemming from multiple eQTL hotspots Additionally, we report the results Not only do we find that many of these candidate regulators contain mutations in the promoter and coding regions of the genes, in the case of the Ribi group, we provide experimental evidence suggesting that the identified candidates do regulate the target genes predicted by GFlasso Conclusions : Thus, this structured association analysis of a yeast eQTL dataset via GFlasso, coupled with extensive bioinformatics analysis, discovers a novel regulation pattern between multiple eQTL hotspots and functional gene modules. We employ a new eQTL mapping algorithm, GFlasso which we have previously developed for sparse structured regression, GFlasso fully takes into account the dependencies among expression traits to suppress false positives and to enhance the signal/noise ratio, Thus GFlasso leverages the gene-interaction network to discover the pleiotropic effects of genetic loci that perturb the expression level of multiple ( rather than individual ) genes, which enables us to gain more power in detecting previously neglected signals that are marginally weak but pleiotropically significant We suggest candidate regulators for the functional gene modules that map to each group of hotspots. From in-depth bioinformatics analysis for three groups of these eQTL hotspots : ribosome biogenesis, telomere silencing and retrotransposon biology.
2K_test_321	Human action may be observed from multi-view, which are highly related but sometimes look different from each other. Traditional metric learning algorithms have achieved satisfactory performance in single-view, but they often fail or do not satisfy when they are utilized to fuse different views. Show that the exploring of consistency properties of different views by graph model is very useful, moreover GM-GS-DSDL for each view, which are learnt simultaneously, can further improve the fusion performance, demonstrate that our proposed algorithm can obtain competing performance against the state-of-the-art methods. Thus multi-view discriminative and structured dictionary learning with group sparsity and graph model ( GM-GS-DSDL ) is proposed to fuse different views and recognize human actions, First spatio-temporal interest points are extracted for each view, and then multi-view bag of words ( MVBoW ) representation is employed, at the same time, the graph model is also utilized to fuse different views, which will remove overlapped interest points to explore their consistency properties, Furthermore GM-GS-DSDL is formulated to discover the latent correlation among multiple views, In addition we also issue a new multi-view action dataset with RGB, depth and skeleton data ( called CVS-MV-RGBD ) Multi-view bag of words ( MVBoW ) representation is employed, The graph model is also utilized to fuse different views, We formulate the multi-view action recognition task as a joint dictionary learning ( DL ) problem, We release a new multi-view action dataset which contains RGB, depth and skeleton data. Large-scale experimental results on multi-view IXMAX and CVS-MV-RGBD datasets Comparative experiments.
2K_test_322	Analyzing and interpreting the activity of a heterogeneous population of neurons can be challenging, especially as the number of neurons, experimental trials and experimental conditions increases, One approach is to extract a set of latent variables that succinctly captures the prominent co-fluctuation patterns across the neural population, DataHigh was developed to fulfil a need for visualization in exploratory neural data analysis, which can provide intuition that is critical for building scientific hypotheses and models of population activity. Objective A key problem is that the number of latent variables needed to adequately describe the population activity is often greater than 3, thereby preventing direct visualization of the latent space, By visualizing a small number of 2-d projections of the latent space or each latent variable individually, it is easy to miss salient features of the population activity, To address this limitation that allows the user to quickly and smoothly navigate through a continuum of different 2-d projections of the latent space. Approach we developed a Matlab graphical user interface ( called DataHigh ) We also implemented a suite of additional visualization tools ( including playing out population activity timecourses as a movie and displaying summary statistics, such as covariance ellipses and average timecourses ) and an optional tool for performing dimensionality reduction. To demonstrate the utility and versatility of DataHigh, we used it to analyze single-trial spike count and single-trial timecourse population activity recorded using a multi-electrode array, as well as trial-averaged population activity recorded using single electrodes.
2K_test_323	Current applications have produced graphs on the order of hundreds of thousands of nodes and millions of edges, To take advantage of such graphs, one must be able to find patterns, These tasks are better performed in an interactive environment, where human expertise can guide the process. For large graphs though, there are some challenges : the excessive processing requirements are prohibitive, and drawing hundred-thousand nodes results in cluttered images hard to comprehend, To cope with these problems. We propose an innovative framework suited for any kind of tree-like graph visual design GMine integrates 1 ) a representation for graphs organized as hierarchies of partitions-the concepts of SuperGraph and Graph-Tree ; and 2 ) a graph summarization methodology-CEPS Our graph representation deals with the problem of tracing the connection aspects of a graph hierarchy with sub linear complexity, allowing one to grasp the neighborhood of a single node or of a group of nodes in a single click. As a proof of concept, the visual environment of GMine is instantiated as a system in which large graphs can be investigated globally and locally.
2K_test_324	Many content-based video search ( CBVS ) systems have been proposed to analyze the rapidly-increasing amount of user-generated videos on the Internet, which potentially opens the door to interactive web-scale CBVS for the general public. Though the accuracy of CBVS systems have drastically improved, these high accuracy systems tend to be too inefficient for interactive search, Therefore to strive for real-time web-scale CBVS, to understand the trade-offs between accuracy and speed of each component. Showed that can achieve an 10, 000-fold speedup while retaining 80 % accuracy of a state-of-the-art CBVS system, demonstrated that our system can complete the search in 0, 975 seconds with a single core. Through a combination of effective features, highly compressed representations and one iteration of reranking. We perform a comprehensive study on the different components in a CBVS system Directions investigated include exploring different low-level and semantics-based features, testing different compression factors and approximations during video search, and understanding the time v, accuracy trade-off of reranking Extensive experiments on data sets consisting of more than 1, 000 hours of video We further performed search over 1 million videos and.
2K_test_327	That enables LED lighting luminaires to communicate with cameras on mobile devices. We demonstrate a Visual Light Communication ( VLC ) system Each LED pulses at a frequency above the humanly perceivable flicker threshold where cameras and photodiodes can still detect changes in light intensity, Our modulation scheme supports multiple light sources in a single collision domain, and works for both, line-of-sight ( LOS ) operation as well as from reflected surfaces like those found in architectural lighting, The spatial confinement of light makes this system ideal for use as localization landmarks, A mobile device receiving and processing the signal displays the ID and RSSI of the closest landmark, Interacting with the system will allow users to see the practical effects of multiple-access, frequency of operation distance from the lights, camera parameters and camera motion. Our demonstration includes four LED ambient lights acting as location landmarks transmitting modulated data.
2K_test_328	The relationship between occupant activity and electricity consumption is inextricably linked, It has been difficult to both gather detailed energy data and information about occupants ' daily lives as well as understand their relationship quantitatively. There is significant past work on activity recognition in homes and load prediction, but there is limited understanding of how activities can inform consumption or vice versa, to extract explicit rules that may be useful for forming the basis of demand patterns. Initial findings include the identification of device groups but highlight the challenges of modeling complex patterns and event rarity. Our work begins by characterizing power data as provided by plug-level meters from one household, Association and sequential rule mining techniques are applied.
2K_test_329	To reduce costs organizations may outsource data storage and data processing to third-party clouds, This raises confidentiality concerns, since the outsourced data may have sensitive information. Although semantically secure encryption of the data prior to outsourcing alleviates these concerns, it also renders the outsourced data useless for any relational processing, Motivated by this problem, to support a wide-range of relational queries. And observe low to moderate overheads. We present two database encryption schemes that reveal just enough information about structured data Our main contribution is a definition and proof of security for the two schemes, This definition captures confidentiality offered by the schemes using a novel notion of equivalence of databases from the adversary 's perspective. As a specific application, we adapt an existing algorithm for finding violations of a rich class of privacy policies to run on logs encrypted under our schemes.
2K_test_330	Developers use cryptographic APIs in Android with the intent of securing data such as passwords and personal information on mobile devices These numbers show that applications do not use cryptographic APIs in a fashion that maximizes overall security We then suggest specific remediations based on our analysis towards improving overall cryptographic security in Android applications. In this paper we ask whether developers use the cryptographic APIs in a fashion that provides typical cryptographic notions of security, to automatically check programs. And find that 10, 327 out of 11, 748 applications that use cryptographic APIs -- 88 % overall -- make at least one mistake. We develop program analysis techniques on the Google Play marketplace.
2K_test_331	As a way to relieve the tedious work of manual annotation, active learning plays important roles in many applications of visual concept recognition, In typical active learning scenarios, the number of labelled data in the seed set is usually small. However most existing active learning algorithms only exploit the labelled data, which often suffers from over-fitting due to the small number of labelled examples, Besides while much progress has been made in binary class active learning, little research attention has been focused on multi-class active learning. In this paper we propose a semi-supervised batch mode multi-class active learning algorithm for visual concept recognition, Our algorithm exploits the whole active pool to evaluate the uncertainty of the data, Considering that uncertain data are always similar to each other, we propose to make the selected data as diverse as possible, for which we explicitly impose a diversity constraint on the objective function, As a multi-class active learning algorithm, our algorithm is able to exploit uncertainty across multiple classes, An efficient algorithm is used to optimize the objective function. Extensive experiments on action recognition, object classification scene recognition.
2K_test_333	Existing methods are typically superlinear in space or execution time. That looks for clusters in subspaces of multidimensional data. Effectiveness : it is accurate, providing results with equal or better quality compared to top related works Halite was in average at least 12 times faster than seven representative works, and always presented highly accurate results, Halite was at least 11 times faster than others, increasing their accuracy in up to 35 percent. This paper proposes Halite, a novel fast and scalable clustering method Halite 's strengths are that it is fast and scalable, while still giving highly accurate results, Specifically the main contributions of Halite are : 1 ) Scalability : it is linear or quasi linear in time and space regarding the data size and dimensionality, and the dimensionality of the clusters ' subspaces ; 2 ) Usability : it is deterministic, robust to noise does n't take the number of clusters as an input parameter, and detects clusters in subspaces generated by original axes or by their linear combinations, including space rotation ; 3 ) ; and 4 ) Generality : it includes a soft clustering approach. Experiments on synthetic data ranging from five to 30 axes and up to 1 \rm million points were performed On real data Finally, we report experiments in a real scenario where soft clustering is desirable.
2K_test_334	Analysts synthesize complex qualitative data to uncover themes and concepts, but the process is time-consuming, cognitively taxing and automated techniques show mixed success, Crowdsourcing could help this process through on-demand harnessing of flexible and powerful human cognition, but incurs other challenges including limited attention and expertise, Further text data can be complex. We address two major challenges unsolved in prior crowd clustering work : scaffolding expertise for novice crowd workers, and creating consistent and accurate categories when each worker only sees a small portion of the data. We demonstrate a classification-plus-context approach elicits the most accurate categories at the most useful level of abstraction. B ) introduce an iterative clustering approach that provides workers a global overview of data. To address these challenges we present an empirical study of a two-stage approach to enable crowds to create an accurate and useful overview of a dataset : A ) we draw on cognitive theory to assess how re-representing data can shorten and focus the data on salient dimensions ; and.
2K_test_335	Sequencing of RNAs ( RNA-Seq ) has revolutionized the field of transcriptomics, but the reads obtained often contain errors, Read error correction can have a large impact on our ability to accurately assemble transcripts, This is especially true for de novo transcriptome analysis, where a reference genome is not available, Supporting website : http : //sb. Current read error correction methods, developed for DNA sequence data, can not handle the overlapping effects of non-uniform abundance, polymorphisms and alternative splicing, the first to successfully address these problems. We show that SEECER greatly improves on previous methods in terms of quality of read alignment to the genome and assembly accuracy Our corrected assembled transcripts shed new light on two important stages in sea cucumber development, has also revealed novel transcripts that are unique to sea cucumber, some of which we have validated. Here we present SEquencing Error CorrEction in Rna-seq data ( SEECER ), a hidden Markov Model ( HMM ) based method, which is SEECER efficiently learns hundreds of thousands of HMMs and uses these to correct sequencing errors. Using human RNA-Seq data, To illustrate the usefulness of SEECER for de novo transcriptome studies, we generated new RNA-Seq data to study the development of the sea cucumber Parastichopus parvimensis, Comparison of the assembled transcripts to known transcripts in other species experimentally.
2K_test_336	The HMT3522 progression series of human breast cells have been used to discover how tissue architecture, microenvironment and signaling molecules affect breast cell growth and behaviors, Thus analysis of various reversion conditions ( including non-reverted ) of HMT3522 cells using Treegl can be a good model system to study drug effects on breast cancer. However much remains to be elucidated about malignant and phenotypic reversion behaviors of the HMT3522-T4-2 cells of this series. We found that different breast cell states contain distinct gene networks, The network specific to non-malignant HMT3522-S1 cells is dominated by genes involved in normal processes, whereas the T4-2-specific network is enriched with cancer-related genes, The networks specific to various conditions of the reverted T4-2 cells are enriched with pathways suggestive of compensatory effects, consistent with clinical data showing patient resistance to anticancer drugs, and showed that aberrant expression values of certain hubs in the identified networks are associated with poor clinical outcomes. We employed a pan-cell-state strategy, using a tree-lineage multi-network inference algorithm. And analyzed jointly microarray profiles obtained from different state-specific cell populations from this progression and reversion model of the breast cells We validated the findings using an external dataset.
2K_test_337	For urban driving knowledge of ego-vehicle 's position is a critical piece of information that enables ad- vanced driver-assistance systems or self-driving cars to execute safety-related, autonomous driving maneuvers This is because, without knowing the current location, it is very hard to autonomously execute any driving maneuvers for the future, The existing solutions for localization rely on a combination of Global Navigation Satellite System ( GNSS ), an inertial mea- surement unit, and a digital map However, on urban driving environments, due to poor satellite geometry and disruption of radio signal reception, their longitudinal and lateral errors are too significant to be used to guide an autonomous system. To enhance the existing system 's localization capability. Showed promising results in terms of counting the number of road-lanes and the indices of the current road-lanes. This work presents an effort of developing a vision-based lateral localization algorithm The algorithm aims at reliably counting, with or without observations of lane-markings, the number road-lanes and identifying the index of the road-lane on the roadway that our vehicle happens to be driving. Testings the proposed algorithms against inter-city and inter-state highway videos.
2K_test_338	We present a generic event detection system evaluated in the Surveillance Event Detection ( SED ) task of TRECVID 2012. This approach outperformed the results of all other teams submissions in TRECVID SED 2012 on four of the seven event types. We investigate a statistical approach with spatio-temporal features applied to seven event classes, which were defined by the SED task, This approach is based on local spatio-temporal descriptors, called MoSIFT and generated by pair-wise video frames A Gaussian Mixture Model ( GMM ) is learned to model the distribution of the low level features, Then for each sliding window, the Fisher vector encoding [ improvedFV ] is used to generate the sample representation, The model is learnt using a Linear SVM for each event, The main novelty of our system is the introduction of Fisher vector encoding into video event detection, Fisher vector encoding has demonstrated great success in image classification, The key idea is to model the low level visual features as a Gaussian Mixture Model and to generate an intermediate vector representation for bag of features, FV encoding uses higher order statistics in place of histograms in the standard BoW, FV has several good properties : ( a ) it can naturally separate the video specific information from the noisy local features and ( b ) we can use a linear model for this representation, We build an efficient implementation for FV encoding which can attain a 10 times speed-up over real-time, We also take advantage of non-trivial object localization techniques to feed into the video event detection, multi-scale detection and non-maximum suppression.
2K_test_339	Background : Drug discovery and development has been aided by high throughput screening methods that detect compound effects on a single target, However when using focused initial screening, undesirable secondary effects are often detected late in the development process after significant investment has been made, The methods described are also likely to find widespread application outside drug discovery, such as for characterizing the effects of a large number of compounds or inhibitory RNAs on a large number of cell or tissue phenotypes. An alternative approach would be to screen against undesired effects early in the process, but the number of possible secondary targets makes this prohibitively expensive, for making this global approach practical. Results Conclusions : An average of nearly 60 % of all hits in the dataset were found after exploring only 3 % of the experimental space which suggests that active learning can be used to enable more complete characterization of compound effects than otherwise affordable. This paper describes methods by constructing predictive models for many target responses to many compounds and using them to guide experimentation We demonstrate for the first time that by jointly modeling targets and compounds using descriptive features and using active machine learning methods, accurate models can be built by doing only a small fraction of possible experiments. The methods were evaluated by computational experiments using a dataset of 177 assays and 20, 000 compounds constructed from the PubChem database.
2K_test_340	With the increasing availability of metropolitan transportation data, such as those from vehicle Global Positioning Systems ( GPSs ) and road-side sensors. It has become viable for authorities, operators and individuals to analyze the data for better understanding of the transportation system and, possibly improved utilization and planning of the system. And offer first-hand lessons learned from developing the system VAIT beats state-of-the-art methods and systems in terms of scalability, efficiency and effectiveness and offers us an easy-to-use, efficient and scalable platform to shed more light on intelligent transportation research. We report our experience in building the Visual Analytics for Intelligent Transportation ( VAIT ) system, which is the first system on real-life large-scale data sets Our key observation is that metropolitan transportation data are inherently visual as they are spatio-temporal around road networks Therefore, we visualize and manage traffic data, together with digital maps, and support analytical queries through this interactive visual interface We discuss the technical challenges in data calibration, storage visualization and query processing. As a case study, we demonstrate VAIT on real-world taxi GPS and meter data sets from 15 000 taxis running for two months in a Chinese city of over 10 million people Based on our extensive empirical experiment results.
2K_test_341	Computers are often used in performance of popular music, but most often in very restricted ways, such as keyboard synthesizers where musicians are in complete control, or pre-recorded or sequenced music where musicians follow the computer 's drums or click track, An interesting and yet little-explored possibility is the computer as highly autonomous performer of popular music, capable of joining a mixed ensemble of computers and humans. Considering the skills and functional requirements of musicians leads to a number of predictions about future humancomputer music performance ( HCMP ) systems for popular music for such systems. And our experience with them. We describe a general architecture. And describe some early implementations.
2K_test_342	Fast Fourier transform algorithms on large data sets achieve poor performance on various platforms because of the inefficient strided memory access patterns. These inefficient access patterns need to be reshaped to achieve high performance implementations, we formally restructure 1D, 2D and 3D FFTs targeting a generic machine model and derive memory access pattern efficient algorithms. We demonstrate that Spiral generated hardware designs achieve close to theoretical peak performance of the targeted platform and offer significant speed-up ( up to 6, 5x ) compared to naive baseline algorithms. In this paper with a two-level memory hierarchy requiring block data transfers, using custom block data layouts, Using the Kronecker product formalism, we integrate our optimizations into Spiral framework.
2K_test_343	How can we detect suspicious users in large online networks ? Online popularity of a user or product ( via follows, ) can be monetized on the premise of higher ad click-through rates or increased sales, Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck, Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent ( but sometimes honest ) users. However small-scale stealthy attacks may go unnoticed due to the nature of low-rank Eigen analysis used in practice, In this work we take an adversarial approach to find and prove claims about the weaknesses of modern, state-of-the-art spectral methods to catch small-scale, stealth attacks that slip below the radar. ( b ) it is shown to be highly effective on real data and and with high precision identify many suspicious accounts which have persisted without suspension even to this day. And propose fBox an algorithm designed Our algorithm has the following desirable properties : ( a ) it has theoretical underpinnings, ( c ) it is scalable ( linear on the input size ). We evaluate fBox on a large, 7 million node 1, 5 billion edge who-follows-whom social graph from Twitter in 2010.
2K_test_344	As the complexity of software for Cyber-Physical Systems ( CPS ) rapidly increases, multi-core processors and parallel programming models such as OpenMP become appealing to CPS developers for guaranteeing timeliness Hence, a parallel task on multi-core processors is expected to become a vital component in CPS such as a self-driving car, where tasks must be scheduled in real-time. To be scheduled in real-time, where the number of parallel threads can vary depending on the physical attributes of the system To efficiently schedule the proposed task model. We achieve a resource augmentation bound of 3, 73 In other words, any task set that is feasible on m unit-speed processors can be scheduled by the proposed algorithm on m processors that are 3. In this paper we extend the fork-join parallel task model we develop the task stretch transform Using this transform for global Deadline Monotonic scheduling for fork-join real-time tasks. The proposed scheme is implemented on Linux/RK as a proof of concept, and ported to Boss, the self-driving vehicle that won the 2007 DARPA Urban Challenge, We evaluate our scheme on Boss by showing its driving quality, curvature and velocity profiles of the vehicle.
2K_test_345	A well-studied approach to the design of voting rules views them as maximum likelihood estimators ; given votes that are seen as noisy estimates of a true ranking of the alternatives, the rule must reconstruct the most likely true ranking. We argue that this is too stringent a requirement, and instead ask : How many votes does a voting rule need to reconstruct the true ranking ?. And show that for all rules in this family the number of samples required from the Mallows noise model is logarithmic in the number of alternatives, and that no rule can do asymptotically better ( while some rules like plurality do much worse ) and find voting rules that are accurate in the limit for all noise models in such general families. We define the family of pairwise-majority consistent rules Taking a more normative point of view, we consider voting rules that surely return the true ranking as the number of samples tends to infinity ( we call this property accuracy in the limit ) ; this allows us to move to a higher level of abstraction We characterize the distance functions that induce noise models for which pairwise-majority consistent rules are accurate in the limit, and provide a similar result for another novel family of position-dominance consistent rules These characterizations capture three well-known distance functions. We study families of noise models that are parametrized by distance functions.
2K_test_346	The quality and effectiveness of the load following services provided by centralized control of thermostatically controlled loads depend highly on the communication requirements and the underlying cyberinfrastructure characteristics, Specifically ensuring end-user comfort while providing real-time demand response services depends on the availability of the information provided from the thermostatically controlled loads to the main controller regarding their operating statuses and internal temperatures. State estimation techniques can be used to infer the necessary information from the aggregate power consumption of these loads, replacing the need for an upstream communication platform carrying information from appliances to the main controller in real-time, as an alternative to a Kalman filter approach. The results show that some improvement is possible for scenarios when loads are expected to be toggled frequently. In this paper we introduce a moving horizon mean squared error state estimator with constraints which assumes a linear model without constraints.
2K_test_347	Reading text on the Web is a challenging task for many people, such as those with cognitive impairments, reading difficulties or people who are learning a new language. To help with reading Spanish text on the Web. In this paper we present a web browser plug-in The plug-in is freely available for Chrome and presents definitions and simpler synonyms on demand for the selected web text. The tool was modified following the suggestions of 5 people ( 2 with diagnosed dyslexia ) who tested the tool using the think aloud protocol and undertook a subsequent interview.
2K_test_348	It also significantly contributes to CMU Team 's final submission in TRECVID-13 Multimedia Event Detection. For event search in video. The approach improves the baseline by up to 158 % in terms of the mean average precision. We propose a novel method MultiModal Pseudo Relevance Feedback ( MMPRF which requires no search examples from the user, Pseudo Relevance Feedback has shown great potential in retrieval tasks, but previous works are limited to unimodal tasks with only a single ranked list, To tackle the event search task which is inherently multimodal, our proposed MMPRF takes advantage of multiple modalities and multiple ranked lists to enhance event search performance in a principled way The approach is unique in that it leverages not only semantic features, but also non-semantic low-level features for event search in the absence of training data. Evaluated on the TRECVID MEDTest dataset.
2K_test_349	Big graphs are everywhere, ranging from social networks and mobile call networks to biological networks and the World Wide Web Mining big graphs leads to many interesting applications including cyber security, fraud detection Web search, recommendation and many more. How do we find patterns and anomalies in very large graphs with billions of nodes and edges ? How to mine such big graphs efficiently ? a big graph mining system data processing platform. Our findings include anomalous spikes in the connected component size distribution, the 7 degrees of separation in a Web graph, and anomalous adult advertisers in the who-follows-whom Twitter social network. In this paper we describe Pegasus built on top of MapReduce, a modern distributed We introduce GIM-V, an important primitive that Pegasus uses for its algorithms to analyze structures of large graphs We also introduce HEigen, a large scale eigensolver which is also a part of Pegasus Both GIM-V and HEigen are highly optimized, achieving linear scale up on the number of machines and edges, 2x and 76x faster performance than their naive counterparts. Using Pegasus we analyze very large, real world graphs with billions of nodes and edges.
2K_test_350	We address the challenging problem of utilizing related exemplars for complex event detection while multiple features are available, Related exemplars share certain positive elements of the event, but have no uniform pattern due to the huge variance of relevance levels among different related exemplars, None of the existing multiple feature fusion methods can deal with the related exemplars. And it gains promising performance. In this paper we propose an algorithm which adaptively utilizes the related exemplars by cross-feature learning, Ordinal labels are used to represent the multiple relevance levels of the related videos, Label candidates of related exemplars are generated by exploring the possible relevance levels of each related exemplar via a cross-feature voting strategy, Maximum margin criterion is then applied in our framework to discriminate the positive and negative exemplars, as well as the related exemplars from different relevance levels. We test our algorithm using the large scale TRECVID 2011 dataset.
2K_test_351	Multi-core CPUs with multiple levels of parallelism ( i, data level instruction level and task/core level ) have become the mainstream CPUs for commodity computing systems. To fully utilize the computing power of commodity systems for online and real time applications. The optimized ACCC solver achieves close to linear speedup ( SIMD width multiply core numbers ) comparing to scalar implementation and is able to solve a complete N-1 line outage AC contingency calculation of the Polish grid within one second on a commodity CPU, It enables the complete ACCC as a real-time application on commodity computing systems. Based on the multi-core CPUs, in this paper we developed a high performance computing framework for AC contingency calculation ( ACCC ) Using Woodbury matrix identity based compensation method, we transform and pack multiple contingency cases of different outages into a fine grained vectorized data parallel programming model, We implement the data parallel programming model using SIMD instruction extension on x86 CPUs, therefore fully taking advantages of the CPU core with SIMD floating point capability We also implement a thread pool scheduler for ACCC on multi-core CPUs which automatically balances the computing loads across CPU cores to fully utilize the multi-core capability. We test the ACCC solver on the IEEE test systems and on the Polish 3000-bus system using a quad-core Intel Sandy Bridge CPU.
2K_test_352	To accelerate important data-intensive computing. Demonstrate orders of magnitude of performance and power efficiency improvements compared with the traditional multithreaded software implementation on modern CPU. This paper introduces a 3D-stacked logic-in-memory ( LiM ) system that integrates the 3D die-stacked DRAM architecture with the application-specific LiM IC The proposed system comprises a fine-grained rank-level 3D die-stacked DRAM device and extra LiM layers implementing logic-enhanced SRAM blocks that are dedicated to a particular application, Through silicon vias ( TSVs ) are used for vertical interconnections providing the required bandwidth to support the high performance LiM computing We performed a comprehensive 3D DRAM design space exploration and exploit the efficient architectures to accelerate the computing that can balance the performance and power.
2K_test_354	This paper presents explicit convergence rates. Illustrate the analytical results. For a class of deterministic distributed augmented Lagrangian methods The expressions for the convergence rates show the dependence on the underlying network parameters.
2K_test_355	Brain ? computer interfaces ( BCIs ) are being developed to assist paralyzed people and amputees by translating neural activity into movements of a computer cursor or prosthetic limb, The use of the instructed path task has the potential to accelerate the development of BCI systems and their clinical translation. Intended to help accelerate improvements to BCI systems. Main results We demonstrate that monkeys are able to perform the instructed path task in a closed-loop BCI setting. Here we introduce a novel BCI task paradigm Through this task, we can push the performance limits of BCI systems, we can quantify more accurately how well a BCI system captures the user ? s intent, and we can increase the richness of the BCI movement repertoire Approach, We have implemented an instructed path task, wherein the user must drive a cursor along a visible path The instructed path task provides a versatile framework to increase the difficulty of the task and thereby push the limits of performance, Relative to traditional point-to-point tasks, the instructed path task allows more thorough analysis of decoding performance and greater richness of movement kinematics. We further investigate how the performance under BCI control compares to native arm control, whether users can decrease their movement variability in the face of a more demanding task, and how the kinematic richness is enhanced in this task.
2K_test_356	Guided waves such as Lamb waves, are attractive tools for monitoring large civil infrastructures due to their sensitivity to damage. Yet interpreting guided wave data and identifying effects resulting from damage is often complicated by the multimodal and dispersive characteristics of guided waves and multipath interference from the medium 's boundaries to decompose guided waves into a collection of multipath arrivals. Shows that the estimates all correspond to expected paths. In this paper we present a method by combining sparse wavenumber analysis, a methodology for accurately recovering multimodal and dispersive properties, with additional l 1 minimization techniques Its application to experimental Lamb wave data.
2K_test_357	Pipes carrying fluids under pressure are critical components in infrastructure and industry. Changes in ultrasonic signals detected by piezoelectric transducers can indicate scattering from flaws, but signals also change dramatically from environmental and operational variations, Extensive pitch-catch tests are performed on pressurized pipe segments in a working hot-water supply system that experiences ongoing variations in pressure, temperature and flow rate, Singular value decomposition is applied to differentiate the change caused by scatterer from the changes produced by benign variations. We build a singular value decomposition ( SVD ) based change detector that is sensitive to the mass scatterer but insensitive to the changes produced by operational and environmental variations. We show examples of its successful performance on field experiments data.
2K_test_359	The convergence of mobile computing and cloud computing is predicated on a reliable, This basic requirement is hard to guarantee in hostile environments such as military operations and disaster recovery This article is part of a special issue on the edge of the cloud. In this article the authors examine can overcome this challenge. How VM-based cloudlets that are located in close proximity to associated mobile devices.
2K_test_360	ABSTRACT One of the main challenges in real-world application of guided-waves based nondestructive evaluation ( NDE ) of pipelines is their sensitivity to changes in environmental and operational conditions ( EOC ) that these structures are subject to In spite of many favorable characteristics of gu ided-waves for NDE of pipes, their multi-modal dispersive and multi-path characteristics result in complex signa ls whose interpretation is a difficult task. Studies that have considered the effects of EOC varia tions either fail to reflect realistic EOC scenarios ( e, limited to particular effects of specific EOCs, like time shifting effects of temperature in plates ) or lack the necessary understanding of the effects of EOC variations on different aspects of the developed damage detection approaches, Such gaps limit the extensibility of these approaches to pipeline applications outside of controlled environments, This paper motivates the idea of analytically incorporating the effects of temperature and flow rate variations into damage diagnosis of pipes, through a number of case studie s, A review of the existing literature on guided-wave based testing is also provided. For damage detection a linear supervised classification method, namely linear discriminant analysis ( LDA ), Principal components obtained through principal component analysis ( PCA ), and Fourier transforms of the signals are two sets of damage-sensitive features ( DSF ) that are examined for LDA-based classification. Is applied to experimental guided-wave data recorded from a hot water piping system under regular operation, The effects of temperature and flow rate difference among testing and training datasets on ( A ) detection performance and ( B ) goodness of fit of the method to the data are investigated.
2K_test_362	Over the last several years there has been an explosion of powerful. This provides an outstanding opportunity for novel data visualization techniques that leverage new interaction methods and minimize their barriers to entry, for multivariate data visualization. We describe the results that suggest users of Kinetica were able to explore multiple dimensions of data at once, identify outliers and discover trends with minimal training. In this paper we describe an approach that uses physics-based affordances that are easy to intuit, constraints that are easy to apply and visualize, and a consistent view as data is manipulated in order to promote data exploration and interrogation, We provide a framework for exploring this problem space. An example proof of concept system called Kinetica, of a user study.
2K_test_364	Signal recovery from noisy measurements is an important task that arises in many areas of signal processing. In this paper we consider this problem for signals represented with graphs. Using a recently developed framework of discrete signal processing on graphs, We formulate graph signal denoising as an optimization problem and derive an exact closed-form solution expressed by an inverse graph filter, as well as an approximate iterative solution expressed by a standard graph filter. We evaluate the obtained algorithms by applying them to measurement denoising for temperature sensors and opinion combination for multiple experts.
2K_test_366	Design Finally we summarize key lessons learned and hypothesis about additional hardware that could be used to ease the tracing of faults like short circuits and downed lines within microgrids. In this paper we present the architecture, and experiences in rural Les Anglais, Haiti We believe that this fine-grained micro-payment model can enable sustainable power in otherwise unfeasible areas. From a wirelessly managed microgrid deployment The system consists of a three-tiered architecture with a cloud-based monitoring and control service, a local embedded gateway infrastructure and a mesh network of wireless smart meters deployed at 52 buildings Each smart meter device has an 802, 4 radio that enables remote monitoring and control of electrical service The meters communicate over a scalable multi-hop TDMA network back to a central gateway that manages load within the system, The gateway also provides an 802, 11 interface for an on-site operator and a cellular modem connection to a cloud-backend that manages and stores billing and usage data The cloud backend allows occupants in each home to pre-pay for electricity at a particular peak power limit using a text messaging service, The system activates each meter within seconds and locally enforces power limits with provisioning for theft detection This paper provides a chronology of our deployment and installation strategy that involved GPS-based site mapping along with various network conditioning actions required as the network evolved.
2K_test_367	Proceedings : AACR 104th Annual Meeting 2013 ; Apr 6-10, 2013 ; Washington DC Understanding tumors as evolutionary systems is an important area of study with far-reaching implications in diagnostic and treatment paradigms Computational phylogenetics is a valuable method for inferring tumor evolution in terms of evolutionary trees, phylogenies where paths in a tree correspond to possible tumor progression pathways The location of specific cell-types and patient samples in the tree provide information on tumor sub-types and development of heterogeneity, We previously developed a tumor phylogeny inference pipeline for array comparative genome hybridization ( aCGH ) -based tumor copy number profiles, Steps in the pipeline included extraction of robust progression markers from the data, which could differentiate stages of tumor evolution or the different paths in the tree, and assigning amplification states to the inferred markers in those stages We introduced a novel multi-sample model for amplicon identification and calling, HMMCNA which jointly extracted markers from and assigned amplification states to small sets of tumor aCGH profiles, HMMCNA employs a Hidden Markov Model ( HMM ), a probabilistic model to classify data into normal and amplified states based on an underlying distribution for the two copy number states and a hidden state space of possible amplification states We assumed two possible amplification states per sample : normal ( 0 ) or amplified ( 1 ), Joint segmentation and calling is performed by identifying a most likely sequence of amplification states across all genomic sites probes and samples, Further steps include tuning the parameters of the HMM to handle noise-levels across different datasets, Citation Format : Ayshwarya Subramanian, Stanley Shackney Russell Schwartz, Inference of tumor phylogenetic markers from large copy number datasets. This approach limits in the number of samples the HMM can handle since the number of possible hidden amplification states increases exponentially with the number of samples, Here we present an extension of the approach to handle large datasets, to reduce the hidden state space. The introduction of this heuristic reduces the state space on average by 99 %, gives a further reduction of 80 %, Our method was able to quickly segment the data into sets of robust normal and amplified segments suitable for downstream phylogeny building, The amplicons inferred carried several known markers of tumor progression. We incorporate a heuristic prior to the HMM classification by first screening out amplification states not strongly supported at any individual genome coordinates We further reduce the set of possible amplification states based on the frequency of occurrence of the states by only allowing those states occuring at multiple aCGH probes or array genome coordinate, This step accounts for the presence of random noise in the data and We demonstrate the method on a breast tumor aCGH dataset comprising copy number profiles derived from sectioned biopsy samples ( NCBI GEO [ GSE16672 ] [ 1 ].
2K_test_369	Offers provably fair solutions for the division of rent. Spliddit is a first-of-its-kind fair division website, which In this note, we discuss Spliddit 's goals.
2K_test_370	Hybrid systems with both discrete and continuous dynamics are an important model for real-world cyber-physical systems, The key challenge is to ensure their correct functioning w, Promising techniques to ensure safety seem to be model-driven engineering to develop hybrid systems in a well-defined and traceable manner, and formal verification to prove their correctness, Their combination forms the vision of verification-driven engineering, Often hybrid systems are rather complex in that they require expertise from many domains ( e, robotics control systems computer science, software engineering and mechanical engineering ). Moreover despite the remarkable progress in automating formal verification of hybrid systems, the construction of proofs of complex systems often requires nontrivial human guidance, since hybrid systems verification tools solve undecidable problems, It is thus not uncommon for development and verification teams to consist of many players with diverse expertise, tools for ( 1 ) graphical ( UML ) and textual modeling of hybrid systems, ( 2 ) exchanging and comparing models and proofs, and ( 3 ) managing verification tasks to tackle large-scale verification tasks. This paper introduces a verification-driven engineering toolset that extends our previous work on hybrid and arithmetic verification with This toolset makes it easier.
2K_test_371	Data imbalance problem often exists in our real life dataset, especial for massive video dataset. However the balanced data distribution and the same misclassification cost are assumed in traditional machine learning algorithms, thus it will be difficult for them to accurately describe the true data distribution, and resulting in misclassification. Demonstrate that our proposed algorithm has much more powerful performance than that of traditional machine learning algorithms, and keeps stable and robust when different kinds of features are employed also prove that our EHS algorithm is efficient and effective, and reaches top performance in four of seven events. In this paper the data imbalance problem in semantic extraction under massive video dataset is exploited, and enhanced and hierarchical structure ( called EHS ) algorithm is proposed In proposed algorithm, data sampling filtering and model training are considered and integrated together compactly via hierarchical structure algorithm, thus the performance of model can be improved step by step, and is robust and stability with the change of features and datasets. Experiments on TRECVID2010 Semantic Indexing Extended experiments on TRECVID2010 Surveillance Event Detection.
2K_test_372	Cake cutting is a common metaphor for the division of a heterogeneous divisible good, There are numerous papers that study the problem of fairly dividing a cake. ; a small number of them also take into account self-interested agents and consequent strategic issues, but these papers focus on fairness and consider a strikingly weak notion of truthfulness In this paper we investigate the problem of cutting a cake in a way that is truthful. Where for the first time our notion of dominant strategy truthfulness is the ubiquitous one in social choice and computer science, We design both deterministic and randomized cake cutting mechanisms that are truthful and fair under different assumptions with respect to the valuation functions of the agents.
2K_test_373	Mobile crowdsensing is becoming a vital technique for environment monitoring, infrastructure management and social computing. However deploying mobile crowdsensing applications in large-scale environments is not a trivial task, It creates a tremendous burden on application developers as well as mobile users, In this paper we try to reveal the barriers hampering the scale-up of mobile crowdsensing applications. And to offer our initial thoughts on the potential solutions to lowering the barriers.
2K_test_374	We have been investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous driving, to guarantee their safety and efficiency despite these impairments. In this work we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols and.
2K_test_375	Smart metering systems in distribution networks provide near real-time, two-way information exchange between end users and utilities, enabling many advanced smart grid technologies. However the fine grained real-time data as well as the various market functionalities also pose great risks to customer privacy privacy preserving smart metering system. The demonstration shows the feasibility of our proposed privacy preserving protocol for advanced smart grid technologies which includes load management and retail level electricity market support. In this work we propose a secure multi-party computation ( SMC ) based Using the proposed SMC protocol, a utility is able to perform advanced market based demand management algorithms without knowing the actual values of private end user consumption and configuration data, Using homomorphic encryption billing is secure and verifiable. We implemented a demonstration system that includes a graphical user interface and simulates real-world network communication of the proposed SMC-enabled smart meters.
2K_test_376	Rating data is ubiquitous on websites such as Amazon, Since ratings are not static but given at various points in time, a temporal analysis of rating data provides deeper insights into the evolution of a product 's quality. In this work we tackle the following question : Given the time stamped rating data for a product or service, how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous ? For learning our model. We show the effectiveness of our method and we present interesting discoveries on multiple real world datasets. We propose a Bayesian model that represents the rating data as sequence of categorical mixture models, In contrast to existing methods, our method does not require any aggregation of the input but it operates on the original time stamped data To capture the dynamic effects of the ratings, the categorical mixtures are temporally constrained : Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time, Our method automatically determines the intervals where anomalies occur, and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions we propose an efficient algorithm combining principles from variational inference and dynamic programming. In our experimental study.
2K_test_377	The goal of PLAID is to provide a public library for high-resolution appliance measurements that can be integrated into existing or novel appliance identification algorithms. We introduce the Plug-Level Appliance Identification Dataset ( PLAID ), a public and crowd-sourced dataset for load identification research consisting of short voltage and current measurements ( in the order of a few seconds ) for different residential appliances, PLAID currently contains measurements for more than 200 different appliance instances, representing 11 appliance classes, and totaling more than a thousand records, In this demo we summarize the existing dataset, demonstrate how new records can be added to the library using a web interface and. Walk through a live example of how the library can be integrated into an existing non-intrusive load monitoring ( NILM ) algorithm framework.
2K_test_378	Abstraction has emerged as a key component in solving extensive-form games of incomplete information, However lossless abstractions are typically too large to solve, so lossy abstraction is needed. All prior lossy abstraction algorithms for extensive-form games either 1 ) had no bounds on solution quality or 2 ) depended on specific equilibrium computation approaches, limited forms of abstraction, and only decreased the number of information sets rather than nodes in the game tree to give bounds on solution quality for any perfect-recall extensive-form game. Show that it finds a lossless abstraction when one is available and lossy abstractions when smaller abstractions are desired. We introduce a theoretical framework that can be used The framework uses a new notion for mapping abstract strategies to the original game, and it leverages a new equilibrium refinement for analysis, Using this framework we develop the first general lossy extensive-form game abstraction method with bounds While our framework can be used for lossy abstraction, it is also a powerful tool for lossless abstraction if we set the bound to zero, Prior abstraction algorithms typically operate level by level in the game tree, We introduce the extensive-form game tree isomorphism and action subset selection problems, both important problems for computing abstractions on a level-by-level basis We show that the former is graph isomorphism complete, and the latter NP-complete, We also prove that level-by-level abstraction can be too myopic and thus fail to find even obvious lossless abstractions.
2K_test_379	Abstract Image patterns at different spatial levels are well organized, such as regions within one image and feature points within one region, These classes of spatial structures are hierarchical in nature. The appropriate integration and utilization of such relationship are important to improve the performance of region tagging, Inspired by the recent advances of sparse coding methods. Demonstrate that the proposed approach has better performance of region tagging than the current state of the art methods. We propose an approach, called Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation This approach consists of two steps : region representation and region reconstruction, In the first step, rather than using the l 1 -norm as it is commonly done in sparse coding, we add a hierarchical structure to the process of sparse coding and form a framework of tree-guided dictionary learning, In this framework the hierarchical structures among feature points, regions and images are encoded by forming a tree-guided multi-task learning process, With the learned dictionary, we obtain a better representation of training and testing regions, In the second step, we propose to use a sub-hierarchical structure to guide the sparse reconstruction for testing regions, the structure between regions and images, Thanks to this hierarchy, the obtained reconstruction coefficients are more discriminate, Finally tags are propagated to testing regions by the learned reconstruction coefficients. Extensive experiments on three public benchmark image data sets.
2K_test_381	ABSTRACT Guided wave ultrasonics is an attractive technique for structural health monitoring, especially on pressurized pipes. However civil infrastructure components, including pipes are often subject to large environmental and operational variations that prevent traditional baseline subtraction-based approaches from detecting damage, We collect ultrasonic data on a large-scale pipe segment in its normal operating conditions and observe large environmental variations to detect the presence of a mass scatterer on the pipe at the same time that we co llect the data. The results show that the framework can effectively detect the presence of a scatterer and is robust to large environmental and operational variations. We developed a damage detection method based on singular value decomposition ( SVD ) that is robust to those benign variations We further develop an online novelty detection framework based on our SVD method. We examine the framework with both synthetic simulations and field experimental data.
2K_test_382	The execution of an agent 's complex activities, comprising sequences of simpler actions, sometimes leads to the clash of conflicting functions that must be optimized, These functions represent satisfaction, short-term as well as long-term objectives, costs and individual preferences, The way that these functions are weighted is usually unknown even to the decision maker, But if we were able to understand the individual motivations and compare such motivations among individuals, then we would be able to actively change the environment so as to increase satisfaction and/or improve performance. In this work we approach the problem of providing highlevel and intelligible descriptions of the motivations of an agent, for the analysis of observational records to converge towards a summary description of an agent 's behaviors. Our results show that our method is not only useful, but also performs much better than the previous methods, in terms of accuracy. A novel algorithm is proposed, We also present a methodology that allows researchers through the minimization of an error measure between the current description and the observed behaviors. Based on observations of such an agent during the fulfillment of a series of complex activities ( called sequential decisions in our work ), This work was validated using not only a synthetic dataset representing the motivations of a passenger in a public transportation network, but also real taxi drivers ' behaviors from their trips in an urban network.
2K_test_383	For distributed computation of average consensus. We propose a new framework The presented framework leads to a systematic design of iterative algorithms that compute the consensus exactly, are guaranteed to converge in finite time, are computationally efficient and require no online memory, We demonstrate that our approach is applicable to a broad class of networks For remaining networks, our framework leads to the construction of approximating algorithms for consensus that are also guaranteed to compute in finite time, Our approach is inspired by graph filters introduced by the theoretical framework of signal processing on graphs.
2K_test_384	The primary goal of a vehicular headlight is to improve safety in low-light and poor weather conditions The typical headlight however has very limited flexibility - switching between high and low beams, turning off beams toward the opposing lane or rotating the beam as the vehicle turns - and is not designed for all driving environments. Thus despite decades of innovation in light source technology, more than half of the vehicular accidents still happen at night even with much less traffic on the road, that can sense react and adapt quickly to any environment with the goal of increasing safety for all drivers on the road. In this talk we will lay out the engineering challenges in building this headlight and share our experiences. We will describe a new DMD-based design for a headlight that can be programmed to perform several tasks simultaneously and For example, we will be able to drive with high-beams without glaring any other driver and we will be able to see better during rain and snowstorms when the road is most treacherous to drive, The headlight can also increase contrast of lanes, markings and sidewalks and can alert drivers to sudden obstacles. With the prototypes developed over the past two years.
2K_test_385	Estimating the number of people within a room is important for a wide variety of applications including : HVAC load management, scheduling room allocations and guiding first responders to areas with trapped people. To estimate the number of occupants to recalibrate when we know the room is empty so that it can adapt dynamically over time. We show that our approach is able to capture the number of people in a wide-variety of room configurations with people counting accuracy below 10 % of the maximum room capacity count with as few as two training points. In this paper we present an active sensing technique that uses changes in a room 's acoustic properties Frequency dependent models of reverberation and room capacity are often used when designing auditoriums and concert halls, We leverage this property by using measured changes in the ultrasonic spectrum reflected back from a wide-band transmitter to estimate occupancy, A centrally located beacon transmits an ultrasonic chirp and then records how the signal dissipates over time By analyzing the frequency response over the chirp 's bandwidth at a few known occupancy levels, we are able to extrapolate the response as the number of people in the room changes We explore the design of an excitation signal that best senses the environment with the fewest number of training samples, Finally we provide a simple mechanism that allows our system.
2K_test_386	Organizations collect personal information from individuals to carry out their business functions, Federal privacy regulations such as the Health Insurance Portability and Accountability Act ( HIPAA ), mandate how this collected information can be shared by the organizations, It is thus incumbent upon the organizations to have means to check compliance with the applicable regulations, Prior work by Barth et, introduces two notions of compliance, weak compliance ( WC ) and strong compliance ( SC ), WC ensures that present requirements of the policy can be met whereas SC also ensures obligations can be met, An action is compliant with a privacy policy if it is both weakly and strongly compliant. However their definitions of compliance are restricted to only propositional linear temporal logic ( pLTL ), which can not feasibly specify HIPAA which can capture the privacy requirements of HIPAA. We prove that checking WC is feasible whereas checking SC is undecidable. To this end we present a policy specification language based on a restricted subset of first order temporal logic ( FOTL ) We then formally specify WC and SC for policies of our form, We then formally specify the property WC entails SC, denoted by which requires that each weakly compliant action is also strongly compliant, To check whether an action is compliant with such a policy, it is sufficient to only check whether the action is weakly compliant with that policy We also prove that when a policy has the -property, the present requirements of the policy reduce to the safety requirements imposed by We then develop a sound, semi-automated technique for checking whether practical policies have the -property. We finally use HIPAA as a case study to demonstrate the efficacy of our policy analysis technique.
2K_test_387	The increase in the number of bloggers and the amount of information diffused in the blogosphere makes the blogosphere an important medium through which to communicate and exchange information, Accordingly the interest in understanding the nature of the information diffusion in the blogosphere has also been increased. Existing studies in social networks have mainly focused on the information diffusion through explicit relationships between members, In this paper we analyze the causes for the information diffusion without explicit relationships in the blogosphere. BlogCast a functionality provided by blog-service providers to expose a high quality post on the portal main page, is found to be one of the main causes of the information diffusion without explicit relationships. We analyze the characteristics of the information diffusion through the BlogCast and its halo effect on the bloggers whose post has been exposed on the portal main page, In addition we examine the sustainability of the halo effect of the BlogCast over time.
2K_test_388	Many of the visual questions that blind people ask can not be easily answered with a single image or a short response, especially when questions are of an exploratory nature, what is in this area, or what tools are available on this work bench. To allow blind users to capture large areas of visual information, identify all of the objects within them, and explore their spatial layout. We introduce RegionSpeak with fewer interactions, RegionSpeak helps blind users capture all of the relevant visual information using an interface designed to support stitching multiple images together We use a parallel crowdsourcing workflow that asks workers to define and describe regions of interest, allowing even complex images to be described quickly The regions and descriptions are displayed on an auditory touchscreen interface, allowing users to know what is in a scene and how it is laid out.
2K_test_389	The proliferation of Bluetooth Low-Energy ( BLE ) chipsets on mobile devices has lead to a wide variety of user-installable tags and beacons designed for location-aware applications. That improves ranging accuracy and can help users configure indoor localization systems with minimal effort. That our system can estimate three-dimensional beacon location with a Euclidean distance error of 16, 1cm and can generate maps with room measurements with a two-dimensional Euclidean distance error of 19, we saw that the system can identify Non-Line-Of-Sight ( NLOS ) signals with over 80 % accuracy and track a user 's location to within less than 100cm. In this paper we present the Acoustic Location Processing System ( ALPS ), a platform that augments BLE transmitters with ultrasound in a manner A user places three or more beacons in an environment and then walks through a calibration sequence with their mobile device where they touch key points in the environment like the floor and the corners of the room This process automatically computes the room geometry as well as the precise beacon locations without needing auxiliary measurements, Once configured the system can track a user 's location referenced to a map, The platform consists of time-synchronized ultrasonic transmitters that utilize the bandwidth just above the human hearing limit, where mobile devices are still sensitive and can detect ranging signals, To aid in the mapping process, the beacons perform inter-beacon ranging during setup, Each beacon includes a BLE radio that can identify and trigger the ultrasonic signals, By using differences in propagation characteristics between ultrasound and radio, the system can classify if beacons are within Line-Of-Sight ( LOS ) to the mobile phone In cases where beacons are blocked, we show how the phone 's inertial measurement sensors can be used to supplement localization data. We experimentally evaluate When tested in six different environments.
2K_test_390	Real-time captioning provides deaf and hard of hearing ( DHH ) users with access to spoken content during live events, and the web has allowed these services to be provided via remotely- located captioning services, and for web content itself, and then suggest future work that builds on these insights. For improving the readability of real- time captions However, despite caption benefits spoken language reading rates often result in DHH users falling behind spoken content, especially when the audio is paired with visual references, This is particularly true in classroom settings, where multi-modal content is the norm, and captions are often poorly positioned in the room, Additionally this accommodation can benefit other students who face temporary or `` situational '' disabilities such as listening to unfamiliar speech accents, or if a student is in a location with poor acoustics. Show that by providing users with a tool to more easily track their place in a transcript while viewing live video, it is possible for them to follow visual content that might otherwise have been missed, Both pausing and highlighting have a positive impact on students ' scores on comprehension tests, but highlighting is preferred to pausing, and yields nearly twice as large of an improvement We then discuss several issues with captioning that we observed during our design process and user study. We explore methods by allowing users to more easily switch their gaze between multiple visual information sources, In this paper we explore pausing and highlighting as a means of helping DHH students keep up with live classroom content by helping them track their place when reading text involving visual references.
2K_test_391	To accelerate the processing of sparse matrix data that is held in a 3D DRAM system. Demonstrates more than two orders of magnitude of performance and energy efficiency improvements compared with the traditional multithreaded software implementation on modern processors. This paper introduces a 3D-stacked logic-in-memory ( LiM ) system We build a customized content addressable memory ( CAM ) hardware structure to exploit the inherent sparse data patterns and model the LiM based hardware accelerator layers that are stacked in between DRAM dies for the efficient sparse matrix operations, Through silicon vias ( TSVs ) are used to provide the required high inter-layer bandwidth Furthermore, we adapt the algorithm and data structure to fully leverage the underlying hardware capabilities, and develop the necessary design framework to facilitate the design space evaluation and LiM hardware synthesis.
2K_test_393	Automatic face recognition performance has been steadily improving over years of research. However it remains significantly affected by a number of factors such as illumination, pose expression resolution and other factors that can impact matching scores The focus of this paper is the pose problem which remains largely overlooked in most real-world applications, Specifically we focus on one-to-one matching scenarios where a query face image of a random pose is matched against a set of gallery images, to geometrically correct the viewpoint of the face. We show significant performance improvements in verification rates and also demonstrate the resilience of the proposed method with respect to degrading input quality, We find that the proposed technique is able to match non-frontal images to other non-frontal images of varying angles. We propose a method that relies on two fundamental components : ( a ) A 3D modeling step For this purpose, we extend a recent technique for efficient synthesis of 3D face models called 3D Generic Elastic Model, ( b ) A sparse feature extraction step using subspace modeling and l1-minimization to induce pose-tolerance in coefficient space This in return enables the synthesis of an equivalent frontal-looking face, which can be used towards recognition.
2K_test_394	Autonomous agents that operate as components of dynamic spatial systems are becoming increasingly popular and mainstream, Applications can be found in consumer robotics, in road rail and air transportation, manufacturing and military operations. Unfortunately the approaches to modeling and analyzing the behavior of dynamic spatial systems are just as diverse as these application domains for the medium-term control of autonomous agents in dynamic spatial systems for integrating different approaches of dynamic spatial system analysis to achieve coverage of all required features. In this article we discuss reasoning approaches, which requires a sufficiently detailed description of the agents behavior and environment but may still be conducted in a qualitative manner, We introduce a conceptual reference model, which summarizes the current understanding of the characteristics of dynamic spatial systems based on a catalog of evaluation criteria derived from the model We provide a comparative summary of the modeling features, discuss lessons learned and introduce a research roadmap. We survey logic-based qualitative and hybrid modeling and commonsense reasoning approaches with respect to their features for describing and analyzing dynamic spatial systems in general, and the actions of autonomous agents operating therein in particular, We assess the modeling features provided by logic-based qualitative commonsense and hybrid approaches for projection, planning simulation and verification of dynamic spatial systems.
2K_test_395	For decomposing an undirected unweighted graph into small diameter pieces. With the same asymptotic guarantees as the best sequential algorithm. We show an improved parallel algorithm with a small fraction of the edges in between These decompositions form critical subroutines in a number of graph algorithms, Our algorithm builds upon the shifted shortest path approach introduced in [ Blelloch, Gupta Koutis Miller Peng, Tangwongsan SPAA 2011 ], By combining various stages of the previous algorithm, we obtain a significantly simpler algorithm.
2K_test_396	Human computation allows computer systems to leverage human intelligence in computational processes. While it has primarily been used for tasks that are not time-sensitive, recent systems use crowdsourcing to get on-demand, real-time and even interactive results, for building real-time crowdsourcing systems Our goal is to provide system builders with the tools and insights they need to replicate the success of modern systems in order to further explore this new space. In this paper we present techniques, and then discuss how and when to use them.
2K_test_397	Large-scale collaboration systems often separate their content from the deliberation around how that content was produced. Surfacing this deliberation may engender trust in the content generation process if the deliberation process appears fair, Alternatively it could encourage doubts about content quality, especially if the process appears messy or biased. Where we found that surfacing deliberation generally led to decreases in perceptions of quality for the article under consideration, especially - but not only - if the discussion revealed conflict The effect size depends on the type of editors ' interactions Finally, this decrease in actual article quality rating was accompanied by self-reported improved perceptions of the article and Wikipedia overall. In this paper we report the results of an experiment.
2K_test_398	This can potentially lead to a deeper understanding of programming, bringing students closer to true computational thinking. We describe a three-stage model beginning with a simple, highly scaffolded programming environment ( Kodu ) and progressing to more challenging frameworks ( Alice and Lego NXT-G ), In moving between frameworks, students explore the similarities and differences in how concepts such as variables, conditionals and looping are realized Some novel strategies for teaching with Kodu are outlined. Finally we briefly report on our methodology and select preliminary results from a pilot study using this curriculum with students ages 10-17, including several with disabilities.
2K_test_399	Proceedings : AACR Annual Meeting 2014 ; April 5-9, 2014 ; San Diego, CA Experimental techniques for assessing heterogeneity in tumor cell populations have undergone great advances, In continuing work we are exploring extension of these approaches to better modeling and analysis of tumor evolution using single-cell sequencing data and to more detailed models of tumor evolution. To compute likely evolutionary histories from tumor single-cell copy number data and next generation sequencing data and apply the methods to data collected from diverse types of tumors, but these improvements have created a great need for more sophisticated computer algorithms capable of making sense of these data sources in terms of coherent models of tumor evolution We have addressed this problem. The evolutionary tree models reveal robust features of evolutionary processes distinguishing progression stages and predicting future progression that lead to improved classification accuracy relative to predictions from cellular heterogeneity data alone, Our software is freely available at ftp : //ftp. We describe computational methods by developing computer algorithms for building phylogenetic trees describing evolution of individual tumors based on copy numbers of fluorescence in situ hybridization ( FISH ) probes from single cells in these tumors, These algorithms reconstruct evolutionary trees for observed cell populations so as to heuristically minimize the number of mutational events needed to explain the observed combinations of probe counts by evolution from a common diploid ancestral cell We have extended this work from initial simple evolutionary models of evolution by single copy number changes to account for distinct mechanisms of evolution at the gene, chromosome or whole-genome scale, with potentially different rates of evolution by mutation type. We have applied these algorithms to several FISH data sets, including cervical cancers probed for four genes ( LAMP3, PROX1 PRKAA1 and CCND1 ) measured for up to 250 cells of paired primary and metastatic samples from 16 patients, head-and-neck cancers probed for four genes ( TERC, CCND1 EGFR and TP53 ) measured on up to 250 cells per patient for 65 patients at four tumor stages, prostate cancers probed for six genes ( TBL1XR1, CTTNBP2 MYC PTEN MEN1 and PDGFB ) measured for up to 407 cells in 6 non-progressive and 7 progressive carcinomas, and breast cancers probed for eight genes ( COX-2, MYC CCND1 HER-2 ZNF217, DBC2 CDH1 and TP53 ) measured on up to 220 cells of paired of ductal carcinoma in situ and invasive ductal carcinoma samples from 13 patients We have then applied statistical and machine learning analysis to examine the ability of these trees to classify tumors by stage or potential for progression.
2K_test_400	These ndings coupled with recent results on naturallyrehearsing password schemes, suggest that 4 PAO stories couldbe used to create usable and strong passwords for 14 sensitiveaccounts following this spaced repetition schedule, possibly witha few extra upfront rehearsals stories, These ndings yield concrete advice for improving constructionsof password management schemes and future user studies. AbstractWe report on a user study that provides evidencethat spaced repetition and a specic mnemonic technique enableusers to successfully recall multiple strong passwords over. The best results were obtained whenusers initially returned after 12 hours and then in 1:5 increasingintervals : 77 % of the participants successfully recalled all 4stories in 10 tests over a period of 158 days Much of theforgetting happened in the rst test period ( 12 hours ) : 89 % of participants who remembered their stories during the rsttest period successfully remembered them in every subsequentround In addition, we nd statisticallysignicant evidence that with 8 tests over 64 days users whowere asked to memorize 4 PAO stories outperform users whoare given 4 random action-object pairs, but with 9 tests over 128days the advantage is not signicant Furthermore, there is aninterference effect across multiple PAO stories : the recall rate of100 % ( resp, 90 % ) for participants who were asked to memorize1 PAO story ( resp, 2 PAO stories ) is signicantly better than therate for participants who were asked to memorize 4 PAO. Remote research participants were asked to memorize 4 Person-Action-Object ( PAO ) stories where they chose a famous personfrom a drop-down list and were given machine-generated randomaction-object pairs Users were also shown a photo of a scene andasked to imagine the PAO story taking place in the scene ( e, Bill Gatesswallowingbike on a beach ), Subsequently theywere asked to recall the action-object pairs when prompted withthe associated scene-person pairs following a spaced repetitionschedule over a period of 127+ days, While we evaluated severalspaced repetition schedules.
2K_test_401	Non-Intrusive Load Monitoring ( NILM ) is a set of techniques used to estimate the electricity consumed by individual appliances in a building from measurements of the total electrical consumption, Most commonly NILM works by first attributing any significant change in the total power consumption ( also known as an event ) to a specific load and subsequently using these attributions ( i, the labels for the events ) to estimate energy for each load. For this last step, most published work in the field makes simplifying assumptions to make the problem more tractable for creating appliance models to relax many of these assumptions. Show that the framework can learn data-driven models based on event labels and use that to estimate energy with lower error margins ( e, 3 % ) than when using the heuristic models used by others. In this paper we present a framework based on classification labels and aggregate power measurements that can help Our framework automatically builds models for appliances to perform energy estimation The model relies on feature extraction, clustering via affinity propagation, perturbation of extracted states to ensure that they mimic appliance behavior, creation of finite state models, correction of any errors in classification that might violate the model, and estimation of energy based on corrected labels. We evaluate our framework on 3 houses from standard datasets in the field and.
2K_test_402	It is becoming more and more evident that the mechanical forces previously taken for granted actually play a pivotal role in influencing biological phenomenon from cancer metastases to vasculogenesis, Recent literature provides strong evidence for a causative link between the mechanical stretching of the cytoskeleton and the release of mechanotransductive signaling molecules, Our model for the mechanotransductive release of signaling factors represents a potentially versatile mechanistic platform for examining biophysical interactions that link mechanical stimulus at the cellular level to response at the protein level. Understanding the links between mechanical input, the corresponding morphological changes in the actin cytoskeleton, and the resulting biochemical response is not well understood yet is a significant challenge in the field of mechanotransduction, to further elucidate the interplay between actin network morphology and resultant biochemical signaling. Here we present a model that integrates actin filament network remodeling under stretch with a novel biophysical model of molecular release stretch is applied to a model of the actin filament network, the distribution of bond angles in the network transitions from a more peaked to a flatter distribution, High variability is observed from site-to-site within the network upon an applied stretch, with a nearly uniform distribution of difference between stretched and unstretched angles ( delta angles ) at high levels of stretching, We used our approach to explore various thresholding models of how actin filament network deformations might influence rates of release of bound signaling molecules These models allow us to project how a biochemical response might appear from a given applied mechanical stimulus. We validate these simulations using experimental data and use our model to then test different predictive capabilities of how mechanotransduction may function.
2K_test_403	Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval. Existing methods index a video by the raw concept detection score that is dense and inconsistent, and thus can not scale to `` big data '' that are readily available on the Internet. Results validate the efficacy and the efficiency of the proposed method, The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance Specifically, the proposed method ( with reranking ) achieves the best result on the challenging TRECVID Multimedia Event Detection ( MED ) zero-example task, It only takes 0, 2 second on a single CPU core to search a collection of 100 million Internet videos. This paper proposes a scalable solution, The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index, The proposed adjustment model relies on a concise optimization framework with interpretations, The proposed index leverages the text-based inverted index for video retrieval.
2K_test_405	For multi-agent Markov decision processes ( MDPs ) ; the agents have no prior information on the global state transition and on the local agent cost statistics. We prove that $ { { \cal Q } { \cal D } } $ -learning, a $ \rm consensus + innovations $ algorithm with mixed time-scale stochastic dynamics, converges asymptotically almost surely to the desired value function and to the optimal stationary control policy at each network agent. The paper develops $ { { \cal Q } { \cal D } } $ -learning, a distributed version of reinforcement $ Q $ -learning, The network agents minimize a network-averaged infinite horizon discounted cost, by local processing and by collaborating through mutual information exchange over a sparse ( possibly stochastic ) communication network The agents respond differently ( depending on their instantaneous one-stage random costs ) to a global controlled state and the control actions of a remote controller. When each agent is aware only of its local online cost data and the inter-agent communication network is weakly connected.
2K_test_406	In a Stackelberg Security Game, a defender commits to a randomized deployment of security resources, and an attacker best-responds by attacking a target that maximizes his utility. While algorithms for computing an optimal strategy for the defender to commit to have had a striking real-world impact, deployed applications require significant information about potential attackers, We address this problem. Via an online learning approach, We are interested in algorithms that prescribe a randomized strategy for the defender at each step against an adversarially chosen sequence of attackers, and obtain feedback on their choices ( observing either the current attacker type or merely which target was attacked We design no-regret algorithms whose regret ( when compared to the best fixed strategy in hindsight ) is polynomial in the parameters of the game, and sublinear in the number of times steps.
2K_test_407	Detection of neuronal cell differentiation is essential to study cell fate decisions under various stimuli and/or environmental conditions, Many tools exist that quantify differentiation by neurite length measurements of single cells, However quantification of differentiation in whole cell populations remains elusive so far, In conclusion this enables long-term, large-scale studies of cell populations with minimized costs and efforts for detecting effects of external manipulation of neuronal cell differentiation. Because such populations can consist of both proliferating and differentiating cells, the task to assess the overall differentiation status is not trivial and requires a high-throughput, fully automated approach to analyze sufficient data for a statistically significant discrimination to determine cell differentiation We address the problem of detecting differentiation in a mixed population of proliferating and differentiating cells over time. Using nerve growth factor induced differentiation of PC12 cells, we monitor the changes in cell morphology over days by phase-contrast live-cell imaging, For general applicability the classification procedure starts out with many features to identify those that maximize discrimination of differentiated and undifferentiated cells and to eliminate features sensitive to systematic measurement artifacts, The resulting image analysis determines the optimal post treatment day for training and achieves a near perfect classification of differentiation, Our approach allows to monitor neuronal cell populations repeatedly over days without any interference, It requires only an initial calibration and training step and is thereafter capable to discriminate further experiments. In technically and biologically independent as well as differently designed experiments.
2K_test_408	Mechanotransduction has been divided into mechanotransmission. Although how a cell performs all three functions using the same set of structural components is still highly debated, Here we bridge the gap between emerging molecular and systems-level understandings of mechanotransduction. Our results suggest that filamin-FilGAP mechanotransduction response is best explained by a bandpass mechanism favoring release when crosslinking angles fall outside of a specific range and finds that a more disordered actin network may allow a cell to more finely tune control of molecular release enabling a more robust response. Through a multiscale model linking these three phases Our model incorporates a discrete network of actin filaments and associated proteins that responds to stretching through geometric relaxation Our model further investigates the difference between ordered versus disordered networks. We assess three potential activating mechanisms at mechanosensitive crosslinks as inputs to a mixture model of molecular release and benchmark each using experimental data of mechanically-induced Rho GTPase FilGAP release from actin-filamin crosslinks.
2K_test_409	Understanding the unique biochemical and physical differences between typical in vitro experimental systems and the in vivo environment of a living cell is a question of great importance in building and interpreting reliable models of complex reaction systems, Virus capsids make an excellent model system for such questions because they tend to have few components, making them amenable to in vitro and modeling studies, yet their assembly can be described by enormously complex networks of possible reactions that can not be resolved by any current experimental technology We have previously attempted to bridge the gap between the complexity of the system and the limitations of data for tracking detailed assembly pathways using simulation-based model inference, learning kinetic parameters of coarse-grained rule models by fitting simulations to light scattering data from in vitro capsid assembly systems. Here we describe extensions of that work to attempt to understand the influence of specific features of the cellular environment, individually or in concert, on assembly pathway selection We specifically focus on the effects of macromolecular crowding and nucleic acid on capsid assembly to adjust rate parameters learned from the in vitro system. Results suggest surprisingly complex and often counterintuitive mechanisms by which crowding or nucleic acids can alternately promote or inhibit assembly for different virus and assembly conditions. Using coarse-grained biophysical models and suggest how these adjustments to fine-scale interactions may alter high-level pathway selection. From a series of virus capsid models.
2K_test_411	Abstract Residential electricity users need more detail than monthly bills to reduce consumption, Using national average penetration rates, the Residential Energy Consumption Survey ( RECS ), estimates that 42 unique appliances account for 93 % of electricity consumption, while 12 appliances account for 80 % of average household electric load These results can be used to design and maximize the value of residential energy information and management systems. With the emergence of technologies that provide detailed usage estimates for energy consumption, First how many different energy-consuming appliances contribute to household electricity load, and secondly which appliances ?. Find that eight appliances are responsible for 80 % of a household 's electric load in the United States, It is concluded that RECS can not be used as a representative household as it overestimates the number of appliances that contribute to a household electric load, The number of significant appliances is affected by appliance ownership and use, which is more variable between homes than between census divisions. A typical scenario is developed from national and regional penetration rates and Four household scenarios are developed : a house that uses electric appliances, gas appliances the average household.
2K_test_412	A genome-wide association study involves examining a large number of single-nucleotide polymorphisms ( SNPs ) to identify SNPs that are significantly associated with the given phenotype, while trying to reduce the false positive rate. Although haplotype-based association methods have been proposed to accommodate correlation information across nearby SNPs that are in linkage disequilibrium, none of these methods directly incorporated the structural information such as recombination events along chromosome. Show that there is a significant advantage in incorporating the prior knowledge on linkage disequilibrium structure for marker identification under whole-genome association. In this paper we propose a new approach called stochastic block lasso that exploits prior knowledge on linkage disequilibrium structure in the genome such as recombination rates and distances between adjacent SNPs in order to increase the power of detecting true associations while reducing false positives Following a typical linear regression framework with the genotypes as inputs and the phenotype as output, our proposed method employs a sparsity-enforcing Laplacian prior for the regression coefficients, augmented by a first-order Markov process along the sequence of SNPs that incorporates the prior information on the linkage disequilibrium structure, The Markov-chain prior models the structural dependencies between a pair of adjacent SNPs, and allows us to look for association SNPs in a coupled manner, combining strength from multiple nearby SNPs. Our results on HapMap-simulated datasets and mouse datasets.
2K_test_413	In this paper we consider the problem of state estimation of a dynamical system in a multi-agent network to estimate any arbitrary linear dynamical system. The agents are sparsely connected and each of them observes a strict subset of the state vector, The distributed algorithm that we propose enables each agent with bounded mean-squared error, To achieve this the ratio of the algebraic connectivity and the largest eigenvalue of the graph Laplacian has to be larger than a lower bound determined by the spectral radius of the system 's dynamics matrix This extends the notion of Network Tracking Capacity introduced by other authors in prior work We accomplish this by introducing a new class of estimation algorithm of dynamical systems that, besides a ( consensus + innovations ) term, also includes consensus on the innovations.
2K_test_416	Acoustruments adds a new method to the toolbox HCI practitioners and researchers can draw upon, while introducing a cheap and passive method for adding interactive controls to consumer products. That can bring rich, tangible functionality to handheld devices. Show that Acoustruments can achieve 99 % accuracy with minimal training, is robust to noise, and can be rapidly prototyped. We introduce Acoustruments : low-cost, passive and power-less mechanisms, made from plastic Through a structured exploration, we identified an expansive vocabulary of design primitives, providing building blocks for the construction of tangible interfaces utilizing smartphones ' existing audio functionality By combining design primitives, familiar physical mechanisms can all be constructed from passive elements, On top of these, we can create end-user applications with rich.
2K_test_417	With the widespread availability of video cameras, we are facing an ever-growing enormous collection of unedited and unstructured video data. Due to lack of an automatic way to generate summaries from this large collection of consumer videos, they can be tedious and time consuming to index or search. Demonstrating the effectiveness of online video highlighting. In this work we propose online video highlighting, a principled way of generating short video summarizing the most important and interesting contents of an unedited and unstructured video, costly both time-wise and financially for manual processing, Specifically our method learns a dictionary from given video using group sparse coding, and updates atoms in the dictionary on-the-fly, A summary video is then generated by combining segments that can not be sparsely reconstructed using the learned dictionary, The online fashion of our proposed method enables it to process arbitrarily long videos and start generating summaries before seeing the end of the video, Moreover the processing time required by our proposed method is close to the original video length, achieving quasi real-time summarization speed. Theoretical analysis together with experimental results on more than 12 hours of surveillance and YouTube videos are provided.
2K_test_418	Finally we discuss some ramifications of this work for understanding Twitter usage and management of one 's privacy. An empirical study of 1. Some significant differences were discovered between the two collections, namely in the clients used to post them, their conversational aspects the sentiment vocabulary present in them, and the days of the week they were posted, However in other dimensions for which analysis was possible, no substantial differences were found. This paper describes collected over a continuous one-week period from a set of 292K Twitter users We examine several aggregate properties of deleted tweets, including their connections to other tweets ( e, whether they are replies or retweets ), the clients used to produce them, temporal aspects of deletion, and the presence of geotagging information.
2K_test_420	Background Disease progression in the absence of therapy varies significantly in HIV-1 infected individuals. Both viral and host cellular molecules are implicated ; however, the exact role of these factors and/or the mechanism involved remains elusive, To understand how microRNAs ( miRNAs ), which are regulators of transcription and translation, influence host cellular gene expression ( mRNA ) during HIV-1 infection. We performed a comparative miRNA and mRNA microarray analysis using PBMCs obtained from infected individuals with distinct viral load and CD4 counts.
2K_test_421	In the Architecture Engineering, and Construction ( AEC ) domain, semantically rich 3D information models are increasingly used throughout a facility 's life cycle for diverse applications, such as planning renovations, space usage planning and managing building maintenance, These models which are known as building information models ( BIMs ), are often constructed using dense, three dimensional ( 3D ) point measurements obtained from laser scanners, Laser scanners can rapidly capture the as-is conditions of a facility, which may differ significantly from the design drawings. Currently the conversion from laser scan data to BIM is primarily a manual operation, and it is labor-intensive and can be error-prone to automatically convert the raw 3D point data from a laser scanner positioned at multiple locations throughout a facility into a compact, semantically rich information model. This paper presents a method Our algorithm is capable of identifying and modeling the main visible structural components of an indoor environment ( walls, floors ceilings windows and doorways ) despite the presence of significant clutter and occlusion, which occur frequently in natural indoor environments, Our method begins by extracting planar patches from a voxelized version of the input point cloud, The algorithm learns the unique features of different types of surfaces and the contextual relationships between them and uses this knowledge to automatically label patches as walls, ceilings or floors Then, we perform a detailed analysis of the recognized surfaces to locate openings, such as windows and doorways, This process uses visibility reasoning to fuse measurements from different scan locations and to identify occluded regions and holes in the surface, Next we use a learning algorithm to intelligently estimate the shape of window and doorway openings even when partially occluded Finally, occluded surface regions are filled in using a 3D inpainting algorithm. We evaluated the method on a large, highly cluttered data set of a building with forty separate rooms.
2K_test_422	For improving performance in VM-based mobile computing systems implemented as thick clients on host PCs, to speed up performance-critical operations. Results confirm that TransPart offers low overhead and startup cost, while improving user experience. This article investigates the transient use of free local storage We use the term TransientPC systems to refer to these types of systems, The solution we propose, called TransPart uses the higher-performing local storage of host hardware Our solution constructs a virtual storage device on demand ( which we call transient storage ) by borrowing free disk blocks from the hosts storage, In this article we present the design. And evaluation of a TransPart prototype, which requires no modifications to the software or hardware of a host computer Experimental.
2K_test_423	Real-time system level implementations of complex Synthetic Aperture Radar ( SAR ) image reconstruction algorithms have always been challenging due to their data intensive characteristics. To alleviate the data intensity. Results indicate that this proposed algorithm/hardware co-optimized system can achieve an accuracy of 91 dB PSNR compared to a reference algorithm implemented in Matlab and energy efficiency of 72 GFLOPS/W for a 8k8k SAR image reconstruction. In this paper we propose a basis vector transform based novel algorithm and a 3D-stacked logic in memory based hardware accelerator as the implementation platform.
2K_test_424	The findings of this paper provide strategies for harnessing the crowd to perform complex tasks, as well as insight into crowd workers ' motivation. A significant challenge for crowdsourcing has been increasing worker engagement and output quality. We show that 1 ) using these strategies together increased workers ' engagement and the quality of their work ; 2 ) a social strategy was most effective for increasing engagement ; 3 ) a learning strategy was most effective in improving quality. We explore the effects of social, learning and financial strategies, and their combinations on increasing worker retention across tasks and change in the quality of worker output.
2K_test_425	Due to the recent penetration of distributed green energy, distributed intelligence and plug-in electric vehicles Finally, the proposed method can be implemented given recent advances in machine learning, which are becoming drivers and sources of data previously unavailable in the electric power industry. This paper is motivated by major needs for fast and accurate on-line data analysis tools in the emerging electric energy systems topology estimation approach for the smart grid. Results of the proposed method show that the new method produces a topology estimate excelling the current industrial approach. Instead of taking the traditional complex physical model based approach, this paper proposes a data-driven method, leading to an effective Specifically, we first introduce the data-driven topology estimation problem, Then a novel Logistic Kernel Regression is proposed in a Bayesian framework based on Nearest Neighbors search, Notably unlike many machine learning approaches that do not account for physical constraints, and distinctive from deterministic engineering modeling defined solely by physical laws, this paper for the first time combines the two into one single regression modeling for topology estimation.
2K_test_426	We close by identifying several ways in which crowd labor platform operators and/or individual task requestors could improve the accessibility of this increasingly important form of employment. We present the first formal study of crowdworkers who have disabilities. Our findings establish that people with a variety of disabilities currently participate in the crowd labor marketplace, despite challenges such as crowdsourcing workflow designs that inadvertently prohibit participation by, and may negatively affect the worker reputations of, people with disabilities Despite such challenges, we find that crowdwork potentially offers different opportunities for people with disabilities relative to the normative office environment, such as job flexibility and lack of a need to rely on public transit. Via in-depth open-ended interviews of 17 people ( disabled crowdworkers and job coaches for people with disabilities ) and a survey of 631 adults with disabilities.
2K_test_427	Effortless one-touch capture of video is a unique capability of wearable devices such as Google Glass. To create a new type of crowd-sourced system. We use this capability in which users receive queries relevant to their current location and opt-in preferences, In response they can send back live video snippets of their surroundings, A system of result caching, geolocation and query similarity detection shields users from being overwhelmed by a flood of queries.
2K_test_428	Researchers can easily evaluate their approaches across the different datasets and benchmark their results against prior work. In this paper we propose a common file format and API for public Non-Intrusive Load Monitoring ( NILM ) datasets such that The proposed file format enables storing the power demand of the whole house along with individual appliance consumption, and other relevant metadata in a single compact file, whereas the API supports the creation and manipulation of individual files and datasets in the proposed format.
2K_test_429	It is known that in this setting both revenue and social welfare can be maximized by a threshold policy, whereby customers are barred from entry once the queue length reaches a certain threshold, allowing for settings with multiple servers. We consider the social welfare model of Naor [ 20 ] and revenue-maximization model of Chen and Frank [ 7 ], where a single class of delay-sensitive customers seek service from a server with an observable queue, under state dependent pricing, However no explicit expression for this threshold has been found, for the ( maximum ) revenue under this optimal threshold. Finally we present a generalization of our results. This paper presents the first derivation of the optimal threshold in closed form, and a surprisingly simple formula Utilizing properties of the Lambert W function, we also provide explicit scaling results of the optimal threshold as the customer valuation grows.
2K_test_430	The virtualization of real-time systems has received much attention for its many benefits, such as the consolidation of individually developed real-time applications while maintaining their implementations. However the current state of the art still lacks properties required for resource sharing among real-time application tasks in a multi-core virtualization environment, for the virtualization of multi-core real-time systems. Results indicate that under vMPCP, deferrable server outperforms periodic server when overrun is used, with as much as 80 % more task sets being schedulable, shows that vMPCP yields significant benefits compared to a virtualization-unaware multi-core synchronization protocol, with 29 % shorter response time on average. In this paper we propose vMPCP, a synchronization framework Vmpcp exposes the executions of critical sections of tasks in a guest virtual machine to the hyper visor, Using this approach vMPCP reduces and bounds blocking time on accessing resources shared within and across virtual CPUs ( VCPUs ) assigned on different physical CPU cores Vmpcp supports periodic server and deferrable server policies for the VCPU budget replenish policy, with an optional budget overrun to reduce blocking times, We provide the VCPU and task schedulability analyses under vMPCP, with different VCPU budget supply policies, with and without overrun. Experimental The case study using our hyper visor implementation.
2K_test_432	Real-time captioning enables deaf and hard of hearing ( DHH ) people to follow classroom lectures and other aural speech by converting it into visual text with less than a five second delay, These results show the potential to reliably capture speech even during sudden bursts of speed, as well as for generating enhanced captions, unlike other human-powered captioning approaches. Keeping the delay short allows end-users to follow and participate in conversations, This article focuses on the fundamental problem that makes real-time captioning difficult : sequential keyboard typing is much slower than speaking. We show that both hearing and DHH participants preferred and followed collaborative captions better than those generated by automatic speech recognition ( ASR ) or professionals due to the more consistent flow of the resulting captions. We first surveyed the audio characteristics of 240 one-hour-long captioned lectures on YouTube, such as speed and duration of speaking bursts We then analyzed how these characteristics impact caption generation and readability, considering specifically our human-powered collaborative captioning approach, We note that most of these characteristics are also present in more general domains, For our caption comparison evaluation, we transcribed a classroom lecture in real-time using all three captioning approaches, We recruited 48 participants ( 24 DHH ) to watch these classroom transcripts in an eye-tracking laboratory, We presented these captions in a randomized.
2K_test_433	Event detection in social media is an important but challenging problem. Most existing approaches are based on burst detection, topic modeling or clustering techniques, which can not naturally model the implicit heterogeneous network structure in social media, As a result only limited information, such as terms and geographic locations. And present empirical evaluations illustrating the effectiveness and efficiency of our proposed approach. This paper presents Non-Parametric Heterogeneous Graph Scan ( NPHGS ), a new approach that considers the entire heterogeneous network : we first model the network as a `` sensor '' network, in which each node senses its `` neighborhood environment '' and reports an empirical p-value measuring its current level of anomalousness for each time interval ( e, hour or day ) Then, we efficiently maximize a nonparametric scan statistic over connected subgraphs to identify the most anomalous network clusters, Finally the event represented by each cluster is summarized with information such as type of event, geographical locations time and participants. As a case study, we consider two applications using Twitter data, civil unrest event detection and rare disease outbreak detection.
2K_test_434	Clustering is one of the fundamental data mining tasks. While traditional clustering techniques assign each object to a single cluster only, in many applications it has been observed that objects might belong to multiple clusters with different degrees, to tackle the challenge of mixed membership clustering for vector data For learning our model. We show the strengths of our novel clustering technique. In this work we present a Bayesian framework We exploit the ideas of subspace clustering where the relevance of dimensions might be different for each cluster, Combining the relevance of the dimensions with the cluster membership degree of the objects, we propose a novel type of mixture model able to represent data containing mixed membership subspace clusters, we develop an efficient algorithm based on variational inference allowing easy parallelization. In our empirical study on synthetic and real data.
2K_test_435	Monte Carlo simulation ( MCS ) is a numerical method to solve the probabilistic load flow ( PLF ) problem, Comparing to analytical methods, MCS for PLF has advantages such as flexibility, general purpose able to deal with large nonlinearity and large variances. However MCS also suffers from low convergence speed and high computational burden, especially for problems with multiple random variables to solve the PLF for radial distribution network. In this paper we showed that the PLF for radial distribution system has the similar properties and can be a good candidate for QMC method, have shown the effectiveness of the proposed method. In this paper we proposed a Quasi-Monte Carlo ( QMC ) based method QMC uses samples from low-discrepancy sequence intended to cover the high dimension random sample space as uniformly as possible The QMC method is particularly suitable for the high dimension problems with low effective dimensions, and has been successfully used to solve large scale problems in econometrics and statistical circuit design, The proposed method possesses the advantage of MCS method and significantly increases the convergence rate and overall speed. Numerical experiment results on IEEE test feeders.
2K_test_436	People spend an enormous amount of time searching for and saving information online, Existing tools capture only a small portion of the cognitive processing a user engages in while making sense of a new domain and practical implications for the development of tools to capture and share online information. For capturing online information in a structured but lightweight way. Our results contribute empirical knowledge relevant to theories of information seeking and sensemaking. In this paper we introduce a novel interface We use this interface as a platform. To experimentally characterize the costs and benefits of structuring information during the sensemaking process.
2K_test_437	ABSTRACT Multiple ultrasonic guided -wave modes propagating along a pipe travel with different velocities which are themselves a function of frequency, Reflections from the features of the structure ( e, boundaries pipe welding damage, ) and their complex s uperposition, adds to the complexity of guided -waves, Guided -wave based damage d iagnosis of pipelines becomes even more challenging when environmental and operational conditions ( EOCs ) vary ( e, temperature flow rate inner pressure, These complexities make guided -wave based damage diagnosis of operating pipelines a challenging task. This paper reviews the approaches to -date addressing these challenges, and highlights the preferred characteristics of a method that simplifies guided -wa ve signals for damage diagnosis purposes to extract a sparse subset of guided -wave signals in time -domain. In this paper the general concept of this method is proved The potential of the proposed method for real -time damage detection is illustrated, for wide range of temperature variation scenarios ( i, temperature difference between training and test data varying between -2 ( and 13 ( ). A method is proposed while retaining optimal damage information for detection purpose. T hrough an extensive set of experiments, Effects of temperature variation on detection performnce of the proposed method, and on discriminatory power of the extracted damage -sensitive features are investigated.
2K_test_438	That allows users to design virtual sensors with user-defined dataflow logic. Preliminary performance and scalability study is also reported. This paper reports the design and development of an HTML5-empowered Virtual Sensor Editor ( VSE ) over Internet of Things cloud VSE is a scalable tool by visually aggregating existing sensors, either physical sensors or user-defined virtual sensors, VSE supports a real-time and historical visualization of sensor values and analytical studies, and is a cross-platform and customizable tool equipped with ability to support verifiable sensor data service compos ability A discussion on design decisions is presented. Our preliminary work has been applied to NASA Sustainability Base for Smart Building monitoring.
2K_test_439	Sensor selection in nonparametric decentralized detection is investigated. Results are provided to demonstrate the advantages and properties of the proposed sensor selection approach. Kernel-based minimization framework with a weighted kernel is adopted, where the kernel weight parameters represent sensors ' contributions to decision making, L 1 regularization on weight parameters is introduced into the risk function so that the resulting optimal decision rule contains a sparse vector of nonzero weight parameters, In this way sensor selection is naturally performed because only sensors corresponding to nonzero weight parameters contribute to decision making, A gradient projection algorithm and a Gauss-Seidel algorithm are developed to jointly perform weight selection ( i, sensor selection ) and optimize decision rules, Both algorithms are shown to converge to critical points for this non-convex optimization problem.
2K_test_442	ABSTRACT Guided waves can propagate long distances and are sensitive to subtle structural damage Guided-wave based damage localization often requires extracting the scatter signal ( s ) produced by damage, which is typically obtained by subtracting an intact baseline record from a record to be tested However, in practical applications environmental and operational conditions ( EOC ) dramatically affect guided wave si gnals, In this case the baseline subtraction process can no longer perfectly remove the baseline, thereby defeating localization algorithms, In previous work we showed that singular value decomposition ( SVD ) can be used to detect the presence of damage under large EOC variations, because it can differentiate the tr ends of damage from other EO C variations. This capability of differentiation implies that SVD can also robustly extract a scatter signal, originating from damage in the structure, that is not affected by temperature vari ation, This process allows us to extract a scatterer signal without the challenges associated with traditional temperature compensation and baseline subtraction routines, to localize structural damage in large, spatially and temporally varying EOCs. We show that our SVD-based approach successfully localize damage while current temperature-compensated baseline subtraction methods fail. In this work we use to approach We collect pitch-catch records from randomly placed PZT transducers on an aluminum plate while undergoing temperature variations, Damage is introduced to the plate during the monitoring period, We then use our SVD method to extract the scatter signal from the records, and use the scatter signal to localize damage using the delay-and-sum method. To compare results we also apply several temperature compensation methods to the records and then perform baseline subtraction.
2K_test_443	To design faster parallel graph algorithms involving distances. We use exponential start time clustering Previous algorithms usually rely on graph decomposition routines with strict restrictions on the diameters of the decomposed pieces, We weaken these bounds in favor of stronger local probabilistic guarantees, This allows more direct analyses of the overall process, giving : Linear work parallel algorithms that construct spanners with O ( k ) stretch and size O ( n 1+1/ k ) in unweighted graphs, and size O ( n 1+1/ k log k ) in weighted graphs Hopsets that lead to the first parallel algorithm for approximating shortest paths in undirected graphs with O ( m poly log n ) work.
2K_test_444	Testing Cyber-Physical Systems is becoming increasingly challenging as they incorporate advanced autonomy features. To detect violations of critical system behavioral requirements on an automotive development platform. Beyond demonstrating feasibility the experience emphasized a number of remaining research challenges, including : approximating system intent based on limited system state observability, how to best balance the simplicity and expressiveness of the specification language used to define monitored properties, how to warm up monitoring of system variable state after mode change discontinuities, and managing the differences between simulation and real vehicles when conducting such tests. We investigate using an external runtime monitor as a partial test oracle Despite limited source code access and using only existing network messages, we were able to monitor a hardware-in-the-loop vehicle simulator and analyze prototype vehicle log data to detect violations of high-level critical properties Interface robustness testing was useful to further exercise the monitors.
2K_test_445	Disparity tuning measured in the primary visual cortex ( V1 ) is described well by the disparity energy model, but not all aspects of disparity tuning are fully explained by the model. Such deviations from the disparity energy model provide us with insight into how network interactions may play a role in disparity processing and help to solve the stereo correspondence problem, that provides a simple account of the observed deviations. Our model predicted sharper disparity tuning for larger stimuli, In this case our model predicted reduced sharpening and strength of inverted disparity tuning, the dynamics of disparity tuning observed from the neurophysiological recordings in macaque V1 matched model simulation predictions, Overall the results of this study support the notion that, while the disparity energy model provides a primary account of disparity tuning in V1 neurons, neural disparity processing in V1 neurons is refined by recurrent interactions among elements in the neural circuit. Here we propose a neuronal circuit model with recurrent connections The model is based on recurrent connections inferred from neurophysiological observations on spike timing correlations, and is in good accord with existing data on disparity tuning dynamics. We further performed two additional experiments to test predictions of the model, First we increased the size of stimuli to drive more neurons and provide a stronger recurrent input Second, we displayed anti-correlated stereograms, where dots of opposite luminance polarity are matched between the left- and right-eye images and result in inverted disparity tuning in the disparity energy model.
2K_test_446	Behavioral researchers spend considerable amount of time coding video data to systematically extract meaning from subtle human actions and emotions, - opening up new possibilities for naturally exploring video data. That allows researchers to rapidly query sample, and analyze large video datasets for behavioral events that are hard to detect automatically. Our show that Glance can code nearly 50 minutes of video in 5 minutes by recruiting over 60 workers simultaneously, and can get initial feedback to analysts in under 10 seconds for most clips, Glance 's rapid responses to natural language queries, feedback regarding question ambiguity and anomalies in the data, and ability to build on prior context in followup queries allow users to have a conversation-like interaction with their data. In this paper we present Glance, a tool Glance takes advantage of the parallelism available in paid online crowds to interpret natural language queries and then aggregates responses in a summary view of the video data, Glance provides analysts with rapid responses when initially exploring a dataset, and reliable codings when refining an analysis, We present and compare new methods for accurately aggregating the input of multiple workers marking the spans of events in video data, and for measuring the quality of their coding in real-time before a baseline is established by measuring the variance between workers.
2K_test_447	Inference of gene interaction networks from expression data usually focuses on either supervised or unsupervised edge prediction from a single data source. However in many real world applications, multiple data sources such as microarray and ISH ( in situ hybridization ) measurements of mRNA abundances, are available to offer multiview information about the same set of genes, to estimate a gene interaction network that is consistent with such multiple data sources. Results are reported where NP-MuScL outperforms baseline algorithms significantly, even in the presence of noisy data sources where NP-MuScL predicts a higher number of known gene interactions than existing techniques. We propose ISH which are expected to reflect the same underlying relationships between the genes, NP-MuScL casts the network estimation problem as estimating the structure of a sparse undirected graphical model We use the semiparametric Gaussian copula to model the distribution of the different data sources, with the different copulas sharing the same precision ( i, inverse covariance ) matrix, and we present an efficient algorithm to estimate such a model in the high-dimensional scenario. On synthetic data Experiments are also run on two real-world scenarios : two yeast microarray datasets and three Drosophila embryonic gene expression datasets.
2K_test_448	Intracortical braincomputer interface ( BCI ) decoders are typically retrained daily to maintain stable performance Significance, We believe that the development of classifiers that require no daily retraining will accelerate the clinical translation of BCI systems, Future work should test these results in a closed-loop setting. Objective Self-recalibrating decoders aim to remove the burden this may present in the clinic by training themselves autonomously during normal use but have only been developed for continuous control, Here we address the problem for discrete decoding ( classifiers ). Further drift is constrained across time, which is reflected in the performance of a standard classifier which does not progressively worsen if it is not retrained daily, though overall performance is reduced by more than 10 % compared to a daily retrained classifier, produce a increase in classification accuracy over that achieved by the non-retrained classifier to nearly recover the performance of the daily retrained classifier. We show that for the purposes of developing a self-recalibrating classifier, tuning parameters can be considered as fixed within days and that parameters on the same electrode move up and down together between days, Two novel self-recalibrating classifiers. We recorded threshold crossings from 96-electrode arrays implanted in the motor cortex of two rhesus macaques performing center-out reaches in 7 directions over 41 and 36 separate days spanning 48 and 58days in total for offline analysis.
2K_test_449	Low-cost genetic sequencing coupled with novel social media platforms and visualization techniques, present a new frontier for scientific participation, whereby people can learn, share and act on data embedded within their own bodies. Reveals how users make sense of and contextualize their genetic results, critique and evaluate the underlying research, and reflect on the broader implications of genetic testing. Our findings show that personal genetics serves as a site for public engagement with science, whereby communities of biological citizens creatively interpret, debate and act on professional research. We conclude with design trajectories at the intersection of genetics and creativity support tools : platforms for aggregating hybrid knowledge ; tools for creative reflection on professional science ; and strategies for supporting collaborations across communities. Our study of 23andMe, a popular genetic testing service, We frame user groups as citizen science publics groups that coalesce around scientific issues and work towards resolving shared concerns.
2K_test_450	In the real-world unconstrained face recognition scenarios, automatic facial landmarking scheme using the active shape model ( ASM ) usually gives non-ideal results, especially at the facial boundary This is because the local subspace methods such as the principal component analysis ( PCA ) used in the ASM does not properly discern skin texture and background with very similar photometric and textual properties, thus fails to accurately locate the facial boundary. To efficiently refine the landmarks on facial boundary. We have shown the effectiveness of our proposed methods where the refined landmarks yield much lower MSE from the ground truth. In this work we have novelly developed a robust image statistics approach Moreover, with the aid of banana wavelets to highlight the facial boundary, our proposed approach can deal with even more difficult task This algorithm can dramatically increase the accuracy of landmarks on facial boundary for unconstrained facial images with minimum computational expense since this method is purely based on image statistics with no training stages involved at all. On the GBU database.
2K_test_451	One of the most significant challenges for many online communities is increasing members ' contributions over time, Prior studies on peer feedback in online communities have suggested its impact on contribution, but have been limited by their correlational nature This research provides insights into the mechanisms underlying peer feedback in online communities and practical guidance to design more effective peer feedback systems. To test the effects of different feedback types ( positive feedback, negative feedback directive feedback, and social feedback ) on members ' contribution. Our results characterize the effects of different feedback types, and suggest trade-offs in the effects of feedback between the focal task and general motivation, as well as differences in how newcomers and experienced editors respond to peer feedback. In this paper we conducted a field experiment on Wikipedia.
2K_test_452	In commercial-off-the-shelf ( COTS ) multi-core systems, the execution times of tasks become hard to predict because of contention on shared resources in the memory hierarchy, In particular a task running in one processor core can delay the execution of another task running in another processor core, This is due to the fact that tasks can access data in the same cache set shared among processor cores or in the same memory bank in the DRAM memory ( or both ), Such cache and bank interference effects have motivated the need to create isolation mechanisms for resources accessed by more than one One popular isolation mechanism is cache coloring that divides the cache into multiple partitions, With cache coloring each task can be assigned exclusive cache partitions, thereby preventing cache interference from other tasks, Similarly bank coloring allows assigning exclusive bank partitions to tasks, While cache coloring and some bank coloring mechanisms have been studied separately, interactions between the two schemes have not been studied, Specifically while memory accesses to two different bank colors do not interfere with each other at the bank level, they may interact at the cache level, Similarly two different cache colors avoid cache interference but may not prevent bank interference. Therefore it is necessary to coordinate cache and bank coloring approaches, that is designed to prevent cache and bank interference simultaneously for configuring a virtual memory system to support our scheme. We observed that the execution time can increase by 60 % due to inter-task interference when we use only cache Our coordinated approach can reduce this figure down to 12 % ( an 80 % reduction ). In this paper we present a coordinated cache and bank coloring scheme We also developed color allocation algorithms. Which has been implemented in the Linux kernel.
2K_test_453	Anecdotal evidence and scholarly research have shown that Internet users may regret some of their online disclosures, We discuss implications and challenges for designing and evaluating systems to assist users with online disclosures. To help individuals avoid such regrets that nudge users to consider the content and audience of their online disclosures more carefully. We found that reminders about the audience of posts can prevent unintended disclosures without major burden ; however, introducing a time delay before publishing users ' posts can be perceived as both beneficial and annoying On balance, some participants found the nudges helpful while others found them unnecessary or overly intrusive. We designed two modifications to the Facebook web interface. We implemented and evaluated these two nudges in a 6-week field trial with 28 Facebook users We analyzed participants ' interactions with the nudges, the content of their posts, and opinions collected through surveys.
2K_test_456	Virus capsid assembly has been a powerful model system for biological self-assembly in general due to the combination of experimental tractability but complicated pathway space, Detailed experimental resolution of viral assembly processes, however has so far proven impossible, Computational approaches have provided a solution, allowing us to learn models of assembly consistent with indirect experimental measures of bulk in vitro assembly and thus fill the gaps between coarse-grained experimental measurements and detailed theoretical models, We previously developed a heuristic optimization approach to learn rate parameters of coat-coat interactions by minimizing the deviation between real and simulated static light scattering measurements, Advancing such simulation-based data fitting methods provides a general technology for greatly enhancing our ability to learn fine-scale details of complex assembly processes from experimental data, a strategy with potential application to developing accurate quantitative models of numerous other assembly systems found through biology. Nonetheless accurate simulation predictions rely on building accurate models, which has proven to be a challenging data-fitting problem due to the high computational cost of simulating capsid assembly trajectories, high stochastic noise inherent to the system, and limited and generally noisy experimental data available, We now show that one can substantially improve fitting to light scattering data to deal with challenges of costly. Suggest that other feasible technologies providing richer data on assembly progress can more precisely pin down true parameters and assembly pathways. Here we describe progress in learning accurate kinetic models of capsid assembly systems by computationally fitting assembly simulations to experimental data, using an alternative class of methods called derivative-free optimization. Simultaneously simulated exploration of potential alternative sources of experimental data for monitoring bulk assembly ( e, g non-covalent mass spectrometry ).
2K_test_458	Automatic understanding of human activities is a huge challenge in multimedia analysis field, This challenge is especially critical in small-scale activities, such as finger motions, and activities in complex scenes, For typical camera views, both global feature and local feature analysis methods are unsuitable, To solve this problem, many studies focus on using spatio-temporal features and feature selection methods to get video representation. However these spatio-temporal features are problematic for two reasons, First we are not sure whether these features are meaningful foreground or noise, Second we are unable to foresee where an activity will occur based on these features, Therefore a biological feature selection method is needed to reorganize these spatio-temporal features and represent the video in a feature space to select more efficient features for activity analysis. Validate our activity analysis approach is more effective than state-of-the-art methods. In this paper we propose a graph based Co-Attention model Without reducing the dimensionality, our Co-Attention model considers the number of interest points, Our model is derived from correlations among individual tiny activities, whose salient regions are identified by combining an integrated top-down and bottom-up visual attention model, and a motion attention model built by spatio-temporal features instead of optical flow directly, Different from typical attention models, the Co-Attention model allows multiple regions of interest in video co-existing for further analysis. Experimental results on the KTH dataset, YouTube dataset and a new tiny activity dataset, Pump dataset which consist of visual observation data from patients operating an infusion pump.
2K_test_459	Machine-learning ( ML ) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses and facial recognition, In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al, adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs. Show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people 's faces given only their name and access to the ML model The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions, Our new attacks are applicable in a variety of settings, and we explore two in depth : decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition, In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values.
2K_test_460	In our previous work, we derived the closed form description of the equilibrium distribution that explicitly accounts for the network topology and showe dt hat themost probable equilibrium statedemonstrates threshol db ehavior. SIS ( susceptible-infected-susceptible ) epidemics In this paper, we will show how subgraph structures in the network topology impact the most probable state of the long run behavior of aS IS epidemics ( i, stochastic diffusion process ) over any static. We model a over as tatic, fi nite-sized network as ac ontinuous-time Markov process using the scaled SIS epidemics model.
2K_test_461	Multimedia Event Detection ( MED ) is a multimedia retrieval task with the goal of finding videos of a particular event in a large-scale Internet video archive, given example videos and text descriptions. In this paper we mainly focus on an 'ad-hoc ' scenario in MED where we do not use any example video, We aim to retrieve test videos. Based on their visual semantics using a Visual Concept Signature ( VCS ) generated for each event only derived from the event description provided as the query, Visual semantics are described using the Semantic INdexing ( SIN ) feature which represents the likelihood of predefined visual concepts in a video, To generate a VCS for an event, we project the given event description to a visual concept list using the proposed textual semantic similarity, Exploring SIN feature properties, we harmonize the generated visual concept signature and the SIN feature to improve retrieval performance. We conduct different experiments to assess the quality of generated visual concept signatures with respect to human expectation, and in the context of the MED task to retrieve the SIN feature of videos in the test dataset when we have no or only very few training videos.
2K_test_462	Given a large graph, like who-calls-whom or who-likes-whom, what behavior is normal and what should be surprising, possibly due to fraudulent activity ? How do graphs evolve over time ? How does influence/news/viruses propagate, over time ? We conclude with some open research questions for graph mining. We focus on three topics : ( a ) anomaly detection in large static graphs ( b ) patterns and anomalies in large time-evolving graphs and ( c ) cascades and immunization. As well as some discoveries such settings, For the third we show that for virus propagation, a single number is enough to characterize the connectivity of graph. For the first we present a list of static and temporal laws, including advances patterns like 'eigenspokes ' ; we show how to use them to spot suspicious activities, in on-line buyer-and-seller settings, in FaceBook in twitter-like networks, For the second we show how to handle time-evolving graphs as tensors, how to handle large tensors in map-reduce environments, thus we show how to do efficient immunization for almost any type of virus ( SIS - no immunity ; SIR - lifetime immunity ; etc ).
2K_test_464	The space around the body provides a large interaction volume that can allow for big interactions on small mobile devices. However interaction techniques making use of this opportunity are underexplored, primarily focusing on distributing information in the space around the body. We demonstrate three types of around-body interaction including canvas, modal and context-aware interactions in six demonstration applications We also present using standard smartphone hardware : a phone 's front camera, accelerometer and inertia measurement units, Our solution allows a person to interact with a mobile device by holding and positioning it between a normal field of view and its vicinity around the body By leveraging a user 's proprioceptive sense, around-body Interaction opens a new input channel that enhances conventional interaction on a mobile device without requiring additional hardware.
2K_test_465	Recent improvements in content-based video search have led to systems with promising accuracy. Thus opening up the possibility for interactive content-based video search to the general public, to do multimodal text-to-video and video-to-video search in large video collections, and to incrementally refine queries. Shows that our system is capable of assisting users to incrementally build effective queries over complex event topics. We present an interactive system based on a state-of-the-art content-based video search pipeline which enables users through relevance feedback and model visualization, Also the comprehensive functionalities enhance a flexible formulation of multimodal queries with different characteristics. Quantitative and qualitative analysis.
2K_test_466	In order to reduce the offloading workload. We show that offload shaping can produce significant reduction in resource demand, with little loss of application-level fidelity. When offloading computation from a mobile device, we show that it can pay to perform additional on-device work We call this offload shaping. Demonstrate its application at many different levels of abstraction using a variety of techniques.
2K_test_467	In recent years progresses in data mining and business analytics have fostered the advent of recommender systems, behavioral advertising and other ways of using consumer data to personalize offers and products, Copyright Springer Science+Business Media New York 2014. We investigate the incentives for sellers to invest in systems that allow the tracking of consumers and then to truthfully report whether potential buyers will enjoy yet untried products. We find that there are two types of equilibria : sellers will target all potential buyers, hence their targeted ads or purchase recommendations provide no benefit to the consumer, ads and recommendations will be accurate will be inversely related. For some parameter values, But for other values In particular, the incentive for the seller to provide accurate ads and recommendations to the difference between the cost of producing the good and its average market evaluation.
2K_test_468	These interrelated issues underpin advanced correctness analysis in models of structured communications. We investigate strong normalization, confluence and behavioral equality in the realm of session-based concurrency, Strong normalization and confluence are established for session-typed processes. We prove that all proof conversions induced by the logic interpretation actually express observational equivalences, and explain how type isomorphisms resulting from linear logic equivalences are realized by coercions between interface types of session-based concurrent systems. By developing a theory of logical relations, Defined upon a linear type structure, our logical relations remain remarkably similar to those for functional languages, We also introduce a natural notion of observational equivalence Strong normalization and confluence come in handy in the associated coinductive reasoning :. The starting point for our study is an interpretation of linear logic propositions as session types for communicating processes, proposed in prior work.
2K_test_469	Given a large social graph, what can we say about its robustness ? Broadly speaking, the property of robustness is crucial in real graphs, since it is related to the structural behavior of graphs to retain their connectivity properties after losing a portion of their edges/nodes, Can we estimate a robustness index for a graph quickly ? Additionally, if the graph evolves over time, how this property changes ?. In this work we are trying to answer the above questions studying the expansion properties of large social graphs. And we observe interesting properties for both static and time-evolving social graphs, we show how to spot outliers and anomalies in graphs over time, Finally we examine how graph generating models that mimic several properties of real-world graphs and behave in terms of robustness dynamics. First we present a measure that characterizes the robustness properties of a graph and also serves as global measure of the community structure ( or lack thereof ), We show how to compute this measure efficiently by exploiting the special spectral properties of real-world networks. We apply our method on several diverse real networks with millions of nodes, As an application example.
2K_test_470	User provided rating data about products and services is one key feature of websites such as Amazon, Since these ratings are rather static but might change over time, a temporal analysis of rating distributions provides deeper insights into the evolution of a products ' quality. Given a time-series of rating distributions, in this work we answer the following questions : ( 1 ) How to detect the base behavior of users regarding a product 's evaluation over time ? ( 2 ) How to detect points in time where the rating distribution differs from this base behavior, due to attacks or spontaneous changes in the product 's quality ? To achieve these goals solving our objective. And we present interesting findings. We model the base behavior of users regarding a product as a latent multivariate autoregressive process, This latent behavior is mixed with a sparse anomaly signal finally leading to the observed data We propose an efficient algorithm. On various real world datasets.
2K_test_471	Self-reported behavioral data is frequently relied upon to understand the population of social network users, These data often consist of self-reported posting, commenting or general engagement frequency within the social network over the last few days or a month. When asking users to report on actions that tend to occur infrequently and irregularly ( e, profile field editing ). We show that these data can be quite inaccurate Indeed, the regularity of the behavior is a strong predictor of self-report accuracy, are the most accurate in their reporting our study suggests that questions should only refer to a very narrow and recent time window to improve response accuracy, Our study also highlights the importance of considering the granularity of privacy concern measurements when investigating the so-called privacy paradox, do take more privacy actions In particular, this group is less likely to enter profile information, more likely to limit the visibility of their posts and more likely to delete posts. Using a sample of 397 Google+ users Users who exhibit a behavior either very frequently or very infrequently For social networks, in which it is often the case that most users are `` lurkers '' who do not post or comment much, Within our sample those users who report that the ability to control post visibility and/or delete posts are more important than other.
2K_test_472	We study the problem of large-scale social identity linkage across different social media platforms, which is of critical importance to business intelligence by gaining from social data a deeper understanding and more accurate profiling of users. Demonstrate that HYDRA correctly identifies real user linkage across different platforms, and outperforms existing state-of-the-art algorithms by at least 20 % under different settings, and 4 times better in most settings. This paper proposes HYDRA, a solution framework which consists of three key steps : ( I ) modeling heterogeneous behavior by long-term behavior distribution analysis and multi-resolution temporal information matching ; ( II ) constructing structural consistency graph to measure the high-order structure consistency on users ' core social structures across different platforms ; and ( III ) learning the mapping function by multi-objective optimization composed of both the supervised learning on pair-wise ID linkage information and the cross-platform structure consistency maximization. Extensive experiments on 10 million users across seven popular social network platforms.
2K_test_473	While much progress has been made to multi-task classification and subspace learning, multi-task feature selection has long been largely unaddressed. Have demonstrated the effectiveness of the proposed algorithm. In this paper we propose a new multi-task feature selection algorithm and apply it to multimedia ( e, video and image ) analysis, Instead of evaluating the importance of each feature individually, our algorithm selects features in a batch mode, by which the feature correlation is considered, While feature selection has received much research attention, less effort has been made on improving the performance of feature selection by leveraging the shared knowledge from multiple related tasks, Our algorithm builds upon the assumption that different related tasks have common structures, Multiple feature selection functions of different tasks are simultaneously learned in a joint framework, which enables our algorithm to utilize the common knowledge of multiple tasks as supplementary information to facilitate decision making, An efficient iterative algorithm is proposed to optimize it, whose convergence is guaranteed. Experiments on different databases.
2K_test_474	Designed to collect a wide array of data on user and computer behavior. We present an architecture for the Security Behavior Observatory ( SBO ), a client-server infrastructure from hundreds of participants over several years, The SBO infrastructure had to be carefully designed to fulfill several requirements, First the SBO must scale with the desired length, breadth and depth of data collection Second, we must take extraordinary care to ensure the security of the collected data, which will inevitably include intimate participant behavioral data, Third the SBO must serve our research interests, which will inevitably change as collected data is analyzed and interpreted This short paper summarizes some of our design and implementation benefits and discusses a few hurdles and trade-offs to consider when designing such a data collection system.
2K_test_475	Very recently there has been a perfect storm of technical advances that has culminated in the emergence of a new interaction modality : on-body interfaces, Such systems enable the wearer to use their body as an input and output platform with interactive graphics, Projects such as PALMbit and Skinput sought to answer the initial and fundamental question : whether or not on-body interfaces were technologically possible, point the way towards more comfortable, efficacious and enjoyable on-body user experiences. Although considerable technical work remains, we believe it is important to begin shifting the question away from how and what, and towards where and ultimately why, These are the class of questions that inform the design of next generation systems, To better understand and explore this expansive space. The results of this complimentary. We employed a mixed-methods research process involving more than two thousand individuals, This started with high-resolution, but low-detail crowdsourced data, We then combined this with rich, expert interviews exploring aspects ranging from aesthetics to kinesthetics.
2K_test_476	Given a large image set, in which very few images have labels, how to guess labels for the remaining majority ? How to spot images that need brand new labels different from the predefined ones ? How to summarize these data to route the user 's attention to what really matters ? Here we answer all these questions, solution to two problems : ( i ) Low-labor labeling ( LLL ), find the most appropriate labels for the rest ; and ( ii ) Mining and attention routing - that best represent the data. QuMinS scales linearly on the data size, being up to 40 times faster than top competitors ( GCap ), still achieving better or equal accuracy, it spots images that potentially require unpredicted labels, and it works even with tiny initial label sets, nearly five examples to show that QuMinS is a viable tool for automatic coffee crop detection from remote sensing images. Specifically we propose QuMinS, a fast scalable - given an image set, very few images have labels in the same setting, find clusters the top-N '' O outlier images, and the N '' R images. Experiments on satellite images spanning up to 2, 25 GB show that, contrasting to the state-of-the-art labeling techniques, We also report a case study of our method 's practical usage.
2K_test_477	We show how a disruptive force in mobile computing can be created. By extending todays unmodified cloud to a second level consisting of self-managed data centers with no hard state called cloudlets These are located at the edge of the Internet, just one wireless hop away from associated mobile devices By leveraging lowlatency offload, cloudlets enable a new class of real-time cognitive assistive applications on wearable devices By processing high data rate sensor inputs such as video close to the point of capture, cloudlets can reduce ingress bandwidth demand into the cloud By serving as proxies for distant cloud services that are unavailable due to failures or cyberattacks, cloudlets can improve robustness and availability, We caution that proprietary software ecosytems surrounding cloudlets will lead to a fragmented marketplace that fails to realize the full business potential of mobile-cloud convergence, Instead we urge that the software ecosystem surrounding cloudlets be based on the same principles of openness and end-to-end design that have made the Internet so successful.
2K_test_478	Introductory programming activities for students often include graphical user interfaces or other visual media that are inaccessible to students with visual impairments, and suggests future directions for integrating data analysis and 3D printing into programming instruction for blind students. Digital fabrication techniques such as 3D printing offer an opportunity for students to write programs that produce tactile objects, providing an accessible way of exploring program output. This paper describes outcomes from our workshop. This paper describes the planning and execution of a four-day computer science education workshop in which blind and visually impaired students wrote Ruby programs to analyze data from Twitter regarding a fictional ecological crisis, Students then wrote code to produce accessible tactile visualizations of that data.
2K_test_479	Botnets are large networks of bots ( compromised machines ) that are under the control of a small number of bot masters, They pose a significant threat to Internets communications and applications, A botnet relies on command and control ( C2 ) communications channels traffic between its members for its attack execution, C2 traffic occurs prior to any attack ; hence, the detection of botnets C2 traffic enables the detection of members of the botnet before any real harm happens, This is due to the pre-programmed behavior of bots that check for updates to download them every T seconds. We analyze C2 traffic to detect C2 traffic. And find that it exhibits a periodic behavior. We exploit this periodic behavior The detection involves evaluating the periodogram of the monitored traffic, Then applying Walkers large sample test to the periodograms maximum ordinate in order to determine if it is due to a periodic component or not If the periodogram of the monitored traffic contains a periodic component, then it is highly likely that it is due to a bots C2 traffic, The test looks only at aggregate control plane traffic behavior, which makes it more scalable than techniques that involve deep packet inspection ( DPI ) or tracking the communication flows of different hosts. We apply the test to two types of botnet, tinyP2P and IRC that are generated by SLINGbot, We verify the periodic behavior of their C2 traffic and compare it to the results we get on real traffic that is obtained from a secured enterprise network, We further study the characteristics of the test in the presence of injected HTTP background traffic and the effect of the duty cycle on the periodic behavior.
2K_test_480	Memory reservations are used to provide real-time tasks with guaranteed memory access to a specified amount of physical memory However, previous work on memory reservation primarily focused on private pages, and did not pay attention to shared pages, which are widely used in current operating systems, With previous schemes a real-time task may experience unexpected timing delays from other tasks through shared pages that are shared by another process, even though the task has enough free pages in its own reservation. In this paper we first describe the problems that arise when real-time tasks share pages a shared-page management framework. We then propose which enhances the temporal isolation provided by memory reservations in resource kernels that use the resource reservation approach, Our proposed solution consists of two schemes, Shared-Page Conservation ( SPC ) and Shared-Page Eviction Lock ( SPEL ), each of which prevents timing penalties caused by the seemingly arbitrary eviction of shared pages, The framework can manage shared data for inter-process communication and shared libraries, as well as pages shared by the kernel 's copy-on-write technique and file caches. We have implemented and evaluated our schemes on the Linux/RK platform, but it can also be applied to other operating systems with paged virtual memory.
2K_test_481	The promise of affordable, automatic approaches to real-time captioning imagines a future in which deaf and hard of hearing ( DHH ) users have immediate access to speech in the world around them my simply picking up their phone or other mobile device, This is in contrast to current human-powered approaches, which use highly-trained professional captionists who can type up to 250 words per minute ( WPM ), but also can cost over $ 100/hr. While the challenges of processing highly variable natural language has prevented automated approaches from completing this task reliably enough for use in settings such as classrooms or workplaces [ 4 ], recent work in crowd-powered approaches have allowed groups of non-expert captionists to provide a similarly-flexible source of captions for DHH users to provide reliable captions with less than 5 seconds of latency. In this paper we describe a real-time demo of Legion : Scribe ( or just `` Scribe '' ), a crowd-powered captioning system that allows untrained participants and volunteers by computationally merging their input into a single collective answer that is more accurate and more complete than any one worker could have generated alone.
2K_test_482	Distributed augmented Lagrangian ( AL ) methods have good empirical performance on several signal processing and learning applications. We study distributed optimization where nodes cooperatively minimize the sum of their individual, locally known convex costs $ f_ { i } ( x ) $ 's ; $ x\in\BBR^ { d } $ is global, but there is limited understanding of their convergence rates and how it depends on the underlying network. Illustrate our analytical findings. This paper establishes globally linear ( geometric ) convergence rates of a class of deterministic and randomized distributed AL methods, when the $ f_ { i } $ 's are twice continuously differentiable and have a bounded Hessian We give explicit dependence of the convergence rates on the underlying network parameters.
2K_test_483	Dispersion curves characterize many propagation mediums, When known many methods use these curves to analyze waves. Yet in many scenarios, their exact values are unknown due to material and environmental uncertainty for recovering dispersion curves from data. In the results orthogonal matching pursuit provides two to three orders of magnitude improvement in speed and a small average reduction in prediction capability, The analysis is demonstrated. This paper presents a fast implementation of sparse wavenumber analysis. This approach based on orthogonal matching pursuit, is compared with a prior implementation, based on basis pursuit denoising across multiple scenarios and parameters.
2K_test_484	For categorizing human actions. Experimental results show that our method outperforms several state-of-the-art algorithms, Most notably much better performances have been achieved when there are only a few labeled training samples. This paper presents a semi-supervised method using multiple visual features, The proposed algorithm simultaneously learns multiple features from a small number of labeled videos, and automatically utilizes data distributions between labeled and unlabeled data to boost the recognition performance, Shared structural analysis is applied in our approach to discover a common subspace shared by each type of feature, In the subspace the proposed algorithm is able to characterize more discriminative information of each feature type, Additionally data distribution information of each type of feature has been preserved The aforementioned attributes make our algorithm robust for action recognition, especially when only limited labeled training samples are provided. Extensive experiments have been conducted on both the choreographed and the realistic video datasets, including KTH Youtube action and UCF50.
2K_test_485	Such performance guarantee makes the greedy algorithm very attractive in the practical scenario of multi-stage installations for utilities with limited budgets. To address the phasor measurement unit ( PMU ) placement problem in electric power systems. And show that it achieves an approximation ratio of ( 1-1/ e ) for any PMU placement budget We further show that the performance is the best that one can achieve, in the sense that it is NP-hard to achieve any approximation ratio beyond ( 1-1/ e ) results demonstrate near-optimal performance of the proposed PMU placement algorithm. This paper presents an information-theoretic approach Different from the conventional `topological observability ' based approaches, this paper advocates a much more refined, information-theoretic criterion namely the mutual information ( MI ) between PMU measurements and power system states, The proposed MI criterion not only includes observability as a special case, but also rigorously models the uncertainty reduction on power system states from PMU measurements, Thus it can generate highly informative PMU configurations, The MI criterion can also facilitate robust PMU placement by explicitly modeling probabilistic PMU outages We propose a greedy PMU placement algorithm.
2K_test_486	Robust facial hair detection and segmentation is a highly valued soft biometric attribute for carrying out forensic facial analysis. For beard/moustache detection and segmentation in challenging facial images. Results demonstrate the effectiveness of our proposed system in detecting and segmenting facial hair regions. In this paper we propose a novel and fully automatic system, called SparCLeS SparCLeS uses the multiscale self-quotient ( MSQ ) algorithm to preprocess facial images and deal with illumination variation, Histogram of oriented gradients ( HOG ) features are extracted from the preprocessed images and a dynamic sparse classifier is built using these features to classify a facial region as either containing skin or facial A level set based approach, which makes use of the advantages of both global and local information, is then used to segment the regions of a face containing. Experimental in images drawn from three databases, the NIST Multiple Biometric Grand Challenge ( MBGC ) still face database, the NIST Color Facial Recognition Technology FERET database, and the Labeled Faces in the Wild ( LFW ) database.
2K_test_487	A descending ( multi-item ) clock auction ( DCA ) is a mechanism for buying items from multiple potential sellers, In the DCA bidder-specific prices are decremented over the course of the auction, In each round each bidder might accept or decline his offer price, Accepting means the bidder is willing to sell at that price, Rejecting means the bidder will not sell at that price or a lower price DCAs have been proposed as the method for procuring spectrum from existing holders in the FCC 's imminent incentive auctions so spectrum can be repurposed to higher-value uses. However the DCA design has lacked a way to determine the prices to offer the bidders in each round, This is a recognized, important and timely problem, We present to our knowledge, the first techniques for this, to naturally reduce the offer prices to the bidders through the bidding rounds to minimize expected payment. An unexpected paradox about DCAs is that sometimes when the number of rounds allowed increases, the final payment increases, We provide an explanation for this. We develop a percentile-based approach which provides a means We also develop an optimization model for setting prices so as while stochastically satisfying the feasibility constraint, ( The DCA has a final adjustment round that obtains feasibility after feasibility has been lost in the final round of the main DCA, ) We prove attractive properties of this, such as symmetry and monotonicity, We develop computational methods for solving the model, ( We also develop optimization models with recourse, but they are not computationally practical. We present experiments both on the homogeneous items case and the case of FCC incentive auctions, where we use real interference constraint data to get a fully faithful model of feasibility.
2K_test_488	Aggregations of thermostatically controlled loads ( TCLs ) have been shown to hold promise as demand response resources. However the evaluation of these promises has relied on simulations of individual TCLs that make important assumptions about the thermal dynamics and properties of the loads, the end-users interactions with individual TCLs and the disturbances to their operation to simulate individual TCLsspecifically, household refrigeration units ( HRUs ) that allows us to relax some of these assumptions and evaluate the validity of the approaches proposed to date. Our results show that the effects of invalid assumptions about the disturbances and time-invariant properties of individual HRUs may be mitigated by a faster sampling of the state variables and that, when this is not possible, the proposed LTI system reduces the plant-model mismatch. In this paper we first propose a data-driven modeling strategy Specifically, we fit probability distributions to a year-long dataset of power measurements for HRUs and use these models to create more realistic simulations We then derive the aggregate system equations using a bottom-up approach that results in a more flexible [ linear time invariant ( LTI ) ] system. Finally we quantify the plant-model mismatch and evaluate the proposed strategy with the more realistic simulation.
2K_test_489	For solving symmetric diagonally dominant ( SDD ) linear systems with m non-zero entries to a relative error of e in O ( m log 1/2 n log c n log ( 1/ e ) ) time, for constructing optimal embeddings in snowflake spaces that runs in O ( m log log n ) time. We show an algorithm Our approach follows the recursive preconditioning framework, which aims to reduce graphs to trees using iterative methods, We improve two key components of this framework : random sampling and tree embeddings, Both of these components are used in a variety of other algorithms, and our approach also extends to the dual problem of computing electrical flows, We show that preconditioners constructed by random sampling can perform well without meeting the standard requirements of iterative methods, In the graph setting, this leads to ultra-sparsifiers that have optimal behavior in expectation The improved running time makes previous low stretch embedding algorithms the running time bottleneck in this framework In our analysis, we relax the requirement of these embeddings to snowflake spaces, We then obtain a two-pass approach algorithm This algorithm is also readily parallelizable.
2K_test_490	Social media offers a targeted way for mainstream technology companies to communicate with people with disabilities about the accessibility problems that they face, and suggests the extent to which a company is able to leverage this input depends greatly on how they choose to present themselves and interact on social media. While companies have started to engage with users on social media about accessibility, they differ greatly in terms of their approach and how well they support the ways in which their users want to engage. We find that while many users want to interact directly with companies about accessibility, companies prefer to redirect them to other channels and use Twitter for broadcast messages promoting their accessibility work instead Our analysis demonstrates that users want to use social media to become part of the process of improving accessibility of mainstream technology. In this paper we describe current use patterns of six corporate accessibility teams and their users on Twitter, and present an analysis of these interactions.
2K_test_491	We motivate the necessity of this framework in the context of self-driving technologies, and we believe that our frameworks for GPU programming are useful contributions given the increasing emphasis on parallel heterogeneous computing. To adjust their task execution times based on total workload. In this paper we present two conceptual frameworks for GPU applications These frameworks enable smart GPU resource management when many applications share GPU resources while the workloads of those applications vary Application developers can explicitly adjust the number of GPU cores depending on their needs An implicit adjustment will be supported by a run-time framework, which dynamically allocates the number of cores to tasks based on the total workload, The runtime support of the proposed system can be realized using functions which measure the execution times of the tasks on GPU and change the number of GPU cores.
2K_test_492	For very large dynamic networks, monitoring the behavior of a subset of agents provides an efficient framework for detecting changes in network topology In addition, we show how well-known tools in dynamic control systems may be useful for identifying abnormal events ; in particular. For example in mobile caller networks with millions of subscribers, we would like to monitor the dynamics of the smallest possible set of subscribers and still be able to infer abnormal events that occur over the entire network, To address this issue to reconstruct the dynamic state evolution of all the network agents at any given time and, simultaneously detect agent departure events to identify a small subset of agents that ensures such network observability regardless of any agent leaving, to identify agent departures. As a proof of concept. In general we assume that the temporal behavior of a network agent is captured by a ( local ) dynamic state, which may reflect either a physical property such as the number of connections or an abstract quantity such as opinions or beliefs, Further assuming coupled linear inter-agent dynamics in which the local agent states evolve as weighted linear combinations of the neighboring agents ' states, we focus on tracking network-wide agent dynamics, Due to the large-scale nature of the problem, directly monitoring data streams of the state dynamics for every individual agent is infeasible, we propose a method that identifies a relatively small subset of agents whose state streams enable us Using structural properties of the coupled inter-agent dynamics, we provide an algorithm, which is polynomial in the number of agents, we use a fault detection and isolation scheme. Finally we illustrate our method and algorithms in a small test network.
2K_test_493	That detects stop-lines and tracks the detected stop-line over time. This method demonstrated promising results, in terms of the accuracy of the initial detection accuracy and the reliability of the tracking. This paper presents a computer vision algorithm, by analyzing lane-marking detection results using an unscented Kalman filter To detect lateral and longitudinal lane-markings, this method applies a spatial filter emphasizing the intensity contrast between lane-marking pixels and their neighboring pixels The authors then examine the detected lane-markings to identify perpendicular, geometry layouts between longitudinal and lateral lane-markings for stop-line detection, To provide reliable stop-line recognition, the authors developed an unscented Kalman filter to track the detected stop-line over frames. Through the tests with real-world, busy urban street videos.
2K_test_494	Despite the popularity of home medical devices, serious safety concerns have been raised, because the use-errors of home medical devices have linked to a large number of fatal hazards, To resolve the problem, to automatically monitor the use of home medical devices. Results show significant performance improvements over the baseline methods by using the estimated ROI for action recognition. We introduce a cognitive assistive system Being able to accurately recognize user operations is one of the most important functionalities of the proposed system, However even though various action recognition algorithms have been proposed in recent years, it is still unknown whether they are adequate for recognizing operations in using home medical devices, Since the lack of the corresponding database is the main reason causing the situation, at the first part of this paper, we present a database specially designed for studying the use of home medical devices, Then we evaluate the performance of the existing approaches on the proposed database, Although using state-of-art approaches which have demonstrated near perfect performance in recognizing certain general human actions, we observe significant performance drop when applying it to recognize device operations, We conclude that the tiny action involved in using devices is one of the most important reasons leading to the performance decrease, To accurately recognize tiny actions, it 's critical to focus on where the target action happens, namely the region of interest ( ROI ) and have more elaborate action modeling based on the ROI, Therefore in the second part of this paper, we introduce a simple but effective approach to estimating ROI for recognizing tiny actions, The key idea of this method is to analyze the correlation between an action and the sub-regions of a frame, The estimated ROI is then used as a filter for building more accurate action representations.
2K_test_495	Given a real world graph, how should we lay-out its edges ? How can we compress it ? These questions are closely related, and the typical approach so far is to find clique-like communities, like the cavemen graph. We show that the block-diagonal mental image of the cavemen graph is the wrong paradigm, in full agreement with earlier results that real world graphs have no good cuts Instead, we propose to envision graphs as a collection of hubs connecting spokes, with super-hubs connecting the hubs, and so on recursively. We show that SlashBurn consistently outperforms other methods for all data sets, resulting in better compression and faster running time, Moreover we show that SlashBurn with the appropriate spokes ordering can further improve compression while hardly sacrificing the running time. Based on the idea, we propose the SlashBurn method to recursively split a graph into hubs and spokes connected only by the hubs, We also propose techniques to select the hubs and give an ordering to the spokes, in addition to the basic SlashBurn, We give theoretical analysis of the proposed hub selection methods, Our view point has several advantages : ( a ) it avoids the no good cuts problem, ( b ) it gives better compression, and ( c ) it leads to faster execution times for matrix-vector operations, which are the back-bone of most graph processing tools.
2K_test_497	That can track unstable dynamics with bounded mean-squared error ( MSE ). We propose and study a new distributed Kalman filter algorithm The Network Tracking Capacity ( NTC ) of this algorithm depends only on the diffusion rate of the network and is independent of the local observation patterns, only requiring global observability. We analyze and compare the NTC for different network models.
2K_test_498	Big data has been becoming ubiquitous and applied in numerous fields recently. The challenges to solve a large-scale machine learning problem in big data scenario generally lie in three aspects, Firstly a proposed machine learning algorithm has to be appropriated for the distributed optimization problem, Secondly it needs a platform for the distributed implementation, Finally the communication delays different machines may cause problems in convergence even though the non-distributed algorithm shows a good convergence rate, In order to solve these challenges, to combine the advantages of sparse representation in an over-complete dictionary. Our method achieves very high classification accuracies in face recognition in the presence of occlusions It also outperforms the state of the art methods in object recognition It hence shows its applicability to general computer vision and pattern recognition problems show our distributed method achieves high speedup of 7, 85x with just 10 machine nodes and can gain even more with more computing resources. We propose a new machine learning approach named Distributed Class-dependent Feature Analysis ( DCFA ) The classifier is based on the estimation of class-specific optimal filters, by solving an i-norm optimization problem We demonstrate how this problem is solved using the Alternating Direction Method of Multipliers and also explore relevant convergency details, More importantly our proposed framework can be efficiently implemented on a robust distributed framework, Thus it improves both accuracy and computational time in large-scale databases. On AR database on two challenging large-scale object databases, In addition computational time experiments on Caltech256 databases compared to the non-distributed version.
2K_test_499	In this paper we study the intertwined propagation of two competing `` memes '' ( or data, ) in a composite network, Within the constraints of this scenario, we ask two key questions : ( a ) which meme will prevail ? and ( b ) can one influence the outcome of the propagations ?. Our model is underpinned by two key concepts, a structural graph model ( composite network ) and a viral propagation model ( SI 1 I 2 S ), Using this framework we formulate a non-linear dynamic system and perform an eigenvalue analysis to identify the tipping point of the epidemic behavior Based on insights gained from this analysis, we demonstrate an effective and accurate prediction method to determine viral dominance, which we call the EigenPredictor. Next using a combination of synthetic and real composite networks, we evaluate the effectiveness of various viral suppression techniques by either a ) concurrently suppressing both memes or b ) unilaterally suppressing a single meme while leaving the other relatively unaffected.
2K_test_500	The Gates Hillman prediction market ( GHPM ) was an internet prediction market designed to predict the opening day of the Gates and Hillman Centers, the new computer science complex at Carnegie Mellon University Unlike a traditional continuous double auction format, the GHPM was mediated by an automated market maker, a central agent responsible for pricing transactions with traders over the possible opening days, The GHPMs event partition was, at the time the largest ever elicited in any prediction market by an order of magnitude, and dealing with the markets size required new advances, including a novel span-based elicitation interface that simplified interactions with the market maker. To examine issues of trader performance and market microstructure, including how the market both reacted to and anticipated official news releases about the buildings opening day. We use the large set of identity-linked trades generated by the GHPM.
2K_test_501	Braincomputer interfaces ( BCIs ) are a promising technology for restoring motor ability to paralyzed patients, Spiking-based BCIs have successfully been used in clinical trials to control multi-degree-of-freedom robotic devices, Current implementations of these devices require a lengthy spike-sorting step, which is an obstacle to moving this technology from the lab to the clinic, Our results indicate that simple automated spike-sorting performs as well as the more computationally or manually intensive methods used here, Even basic spike-sorting adds value to the low-threshold waveform-crossing methods often employed in BCI decoding. Objective A viable alternative is to avoid spike-sorting, treating all threshold crossings of the voltage waveform on an electrode as coming from one putative neuron, It is not known, however how much decoding information might be lost by ignoring spike identity. Main results no spikes should be discarded spike-sorting is useful a fast and simple method is competitive, Decoding using the joint waveform and tuning model shows promise but is not consistently superior. Approach We present a full analysis of the effects of spike-sorting schemes on decoding performance, Specifically we compare how well two common decoders, the optimal linear estimator and the Kalman filter, reconstruct the arm movements of non-human primates performing reaching tasks, when receiving input from various sorting schemes The schemes we tested included : using threshold crossings without spike-sorting ; expert-sorting discarding the noise ; expert-sorting, including the noise as if it were another neuron ; and automatic spike-sorting using waveform features, We also decoded from a joint statistical model for the waveforms and tuning curves, which does not involve an explicit spike-sorting step. Discarding the threshold crossings that can not be assigned to neurons degrades decoding Decoding based on spike-sorted units outperforms decoding based on electrodes voltage crossings The four waveform based spike-sorting methods tested here yield similar decoding efficiencies.
2K_test_502	A common approach in crowdsourcing is to break large tasks into small microtasks so that they can be parallelized across many crowd workers and so that redundant work can be more easily compared for quality control. In practice this can result in the microtasks being presented out of their natural order and often introduces delays between individual microtasks to improve both worker performance and realized pay. That non-sequential microtasks and the introduction of delays significantly decreases worker performance, We show that interruptions where a large delay occurs between two related tasks can cause up to a 102 % slowdown in completion time, and interruptions where workers are asked to perform different tasks in sequence can slow down completion time by 57 %. We conclude with a set of design guidelines and instructions for implementing these changes in existing interfaces for crowd work. In this paper we demonstrate in a study of 338 crowd workers.
2K_test_503	For computing the Voronoi diagram of a set of $ $ n $ $ n points in constant-dimensional Euclidean space. We describe a new algorithm The running time of our algorithm is $ $ O ( f \log n \log \varDelta ) $ $ O ( flognlog ) where $ $ f $ $ f is the output complexity of the Voronoi diagram and $ $ \varDelta $ $ is the spread of the input, the ratio of largest to smallest pairwise distances Despite the simplicity of the algorithm and its analysis, it improves on the state of the art for all inputs with polynomial spread and near-linear output size, The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation, Then the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity, yielding the output sensitive bound, The removal only involves local flips and is inspired by kinetic data structures.
2K_test_504	The paper studies the qualitative behavior of a set of ordinary differential equations. We prove our results we prove the result for general networks. ( ODE ) that models the dynamics of bi-virus epidemics over bilayer networks, Each layer is a weighted digraph associated with a strain of virus ; the weights $ \gamma ^ { z } _ { ij } $ represent the rates of infection from node $ i $ to node $ j $ of strain $ z $, We establish a sufficient condition on the $ \gamma $ s that guarantees survival of the fittestonly one strain survives, We propose an ordering of the weighted digraphs, the $ \star $ -order, and show that if the weighted digraph of strain $ y $ is $ \star $ -dominated by the weighted digraph of strain $ x $, then $ y $ dies out in the long run, We prove that the orbits of the ODE accumulate to an attractor that captures the survival of the fittest phenomenon, Due to the coupled nonlinear high-dimension nature of the ODEs, there is no natural Lyapunov function to study their global qualitative behavior. By combining two important properties of these ODEs : ( i ) monotonicity under a partial ordering on the set of graphs ; and ( ii ) dimension-reduction under symmetry of the graphs, Property ( ii ) allows us to fully address the survival of the fittest for regular graphs, Then by bounding the epidemics dynamics for generic networks by the dynamics on regular networks.
2K_test_505	The Internet of Things ( IoT ) offers the promise of integrating the digital world of the Internet with the physical world in which we live. But realizing this promise necessitates a systematic approach to integrating the sensors, actuators and information on which they operate into the Internet we know today to support federated sensor data as a service. This paper reports the design and development of an open community-oriented platform aiming, featuring interoperability and reusability of heterogeneous sensor data and data services The concepts of virtual sensors and virtual devices are identified as central autonomic units to model scalable and context-aware configurable/reconfigurable sensor data and services The decoupling of the storage and management of sensor data and platform-oriented metadata enables the handling of both discrete and streaming sensor data. A cloud computing-empowered prototyping system has been established as a proof of concept to host smart community-oriented sensor data and services.
2K_test_507	The game played by hospitals participating in pairwise kidney exchange programs. Has an approximation ratio of 3/2 to the maximum cardinality matching, This is an improvement over a recent upper bound of 2 ( Ashlagi et al, 2010 2 ] ) and, furthermore our mechanism beats for the first time the lower bound on the approximation ratio of deterministic truthful mechanisms We complement our positive result with new lower bounds, Among other statements we prove that the weaker incentive compatibility property of truthfulness in expectation in our mechanism is necessary ; universally truthful mechanisms that have an inclusion-maximality property have an approximation ratio of at least 2. We study a mechanism design version of matching computation in graphs that models We present a new randomized matching mechanism for two agents which is truthful in expectation and.
2K_test_508	Self-powered systems that interact with the physical world require computing platforms with predictable timing behavior and a low energy demand. Energy consumption can be reduced by choosing energy-efficient designs for both hardware and software components of the platform, for identifying the energy-efficient frequency range. We expose the hardware characteristics that violate assumptions of conventional energy models Our analysis shows that neither balancing the load nor assigning all load to the `` cheapest '' core is the best load distribution strategy, unless the cores are extremely alike or extremely different. We leverage the state-of-the-art in hardware design by adopting Heterogeneous Multi-core Processors with support for Dynamic Voltage and Frequency Scaling and Dynamic Power Management and propose a revised model suitable We then address the problem of allocating real-time software components onto heterogeneous cores such that total energy is minimized, Our approach is to start from an analytically justified target load distribution and find a task assignment heuristic that approximates it, The optimal load distribution is then formulated as a solution to a convex optimization problem A heuristic that approximates this load distribution and an alternative method that leverages the solution explicitly are proposed as viable task assignment methods. Through experiments on one such platform The proposed methods are compared to state-of-the-art on simulated problem instances and in a case study of a soft-real-time application on an off-the-shelf ARM big.
2K_test_509	Poor posture and incorrect muscle usage are a leading cause of many injuries in sports and fitness, For this reason non- invasive, fine-grained sensing and monitoring of human motion and muscles is important for mitigating injury and improving fitness efficacy, Current sensing systems either de- pend on invasive techniques or unscalable approaches whose accuracy is highly dependent on body sensor placement. As a result these systems are not suitable for use in active sports or fitness training where sensing needs to be scalable, accurate and un-inhibitive to the activity being performed, detects both body motion and individual muscle group activity during physical human activity. The system achieves greater than 95 % accuracy in identifying muscle groups. We present MARS a system that by only using unobtrusive, non-invasive in- ertial sensors MARS not only accurately senses and recreates human motion down to the muscles, but also allows for fast personalized system setup by determining the individual identities of the instrumented muscles, obtained with minimal system training. In a real world human study con- ducted to evaluate MARS.
2K_test_510	The openness of wireless communication and the recent development of software-defined radio technology, respectively provide a low barrier and a wide range of capabilities for misbehavior, attacks and defenses against attacks. That allows a jammer and sender to choose ( 1 ) whether to transmit or sleep, ( 2 ) a power level to transmit with, and ( 3 ) what channel to transmit on, to choose on how many channels it simultaneously attacks. Matching our intuition the aggressiveness of an attacker is related to how much of a discount is placed on data delay, This results in the defender often choosing to sleep despite the latency implication, because the threat of jamming is high, We also present several other findings. In this work we present finite-energy jamming games, a game model We also allow the jammer A major addition in finite-energy jamming games is that the jammer and sender both have a limited amount of energy which is drained according to the actions a player takes We develop a model of our system as a zero-sum finite-horizon stochastic game with deterministic transitions, We leverage the zero-sum and finite-horizon properties of our model to design a simple polynomial-time algorithm to compute optimal randomized strategies for both players, The utility function of our game model can be decoupled into a recursive equation, Our algorithm exploits this fact to use dynamic programming to construct solutions in a bottom-up fashion, For each state of energy levels, a linear program is solved to find Nash equilibrium strategies for the subgame, With these techniques our algorithm has only a linear dependence on the number of states, and quadratic dependence on the number of actions, allowing us to solve very large instances, By computing Nash equilibria for our game models, we explore what kind of performance guarantees can be achieved both for the sender and jammer, when playing against an optimal opponent, We also use the optimal strategies to simulate finite-energy jamming games and provide insights into robust communication among reconfigurable, yet energy-limited radio systems. To test the performance of the optimal strategies we compare their performance with a random and adaptive strategy from simulations where we vary the strategies for one or both of the players.
2K_test_511	Multimedia event detection ( MED ) and multimedia event recounting ( MER ) are fundamental tasks in managing large amounts of unconstrained web videos, and have attracted a lot of attention in recent years. Most existing systems perform MER as a post-processing step on top of the MED results, In order to leverage the mutual benefits of the two tasks. And obtain very promising results for both MED and MER. We propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events, Our premise is that a good recounting algorithm should not only explain the detection result, but should also be able to assist detection in the first place, Coupled in a joint optimization framework, recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences, To better utilize the powerful and interpretable semantic video representation, we segment each video into several shots and exploit the rich temporal structures at shot level, The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm, which after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps, enables us to efficiently process extremely large video corpora. We test the proposed method on the large scale TRECVID MEDTest 2014 and MEDTest 2013 datasets.
2K_test_512	Despite persistent effort many web pages are still not accessible to everyone. Fixing web accessibility problems can be complicated, Developers need to have extensive knowledge not only of possible accessibility problems but also of approaches for fixing them, This paper is about using the large number of accessibility issues on real websites and crowd-sourced fixes for them as a unique source of learning materials for web developers to learn how to build accessible components in a cost-efficient manner. We also present our user study results where web developers who had varying knowledge of web accessibility all found our system an effective and interesting platform to learning web accessibility. In this paper we present the design, development and study of CAN ( Composable Accessibility Infrastructure ), a crowdsourcing infrastructure that collects web accessibility issues and their fixes, dynamically composes solutions on-the-fly, and delivers the crowd-sourced content as teaching materials Our unique CAN user interaction and system design enables end users with disabilities to both benefit from and contribute to the system without additional effort in their daily web browsing, and allows web developers to experience real accessibility issues and initiate a learning process with first-hand materials CAN also provides an opportunity for data-driven discovery of the common implementation practices that cause accessibility issues. We show how CAN addresses a set of accessibility issues on the top 100 popular websites.
2K_test_514	If the people belong to multiple online communities, their joint membership can influence the survival of each of the communities to which they belong, Communities with many joint memberships may struggle to get enough of their members ' time and attention, but find it easy to import best practices from other communities, Our contributions are two-fold, Theoretically by examining the impact of membership overlap on the survival of online communities we identified an important mechanism underlying the success of online communities, Practically our findings may guide community creators on how to effectively manage their members, and tool designers on how to support this task. In this paper we study the effects of membership overlap on the survival of online communities. We find that higher levels of membership overlap are positively associated with higher survival rates of online communities, Furthermore we find that it is beneficial for young communities to have shared members who play a central role in other mature communities. By analyzing the historical data of 5673 Wikia communities.
2K_test_517	That allows for precise and robust indoor localization. We demonstrate the Acoustic Location Processing System ( ALPS ), a platform that augments BLE proximity beacons with ultrasonic transmitters in a manner { \em ALPS } uses Time-Difference-Of-Arrival ( TDOA ) and Time-Of-Flight ( TOF ) ranging to accurately localize mobile devices such as off-the-shelf smartphones and tablets in 2D space, Users inside the demo area will be able to determine their location and can directly plot it relatively to a map of the area using our app Once a receiving device has determined its initial position, it can synchronize its audio clock with the transmission infrastructure to perform TOF-based localization, which provides similar position accuracy to TDOA based localization with fewer beacons, Multilateration and trilateration processing for each device 's location is offloaded onto a cloud-based solver that can provide localization as a service to ALPS and similar TOF/TDOA based systems.
2K_test_518	These results suggest that the instantaneous details of single-trial movement speed are difficult to extract using commonly assumed coding schemes, This apparent paucity of speed information takes particular importance in the context of brain-machine interfaces ( BMIs ), which rely on extracting kinematic information from motor cortex Previous studies have highlighted subjects ' difficulties in holding a BMI cursor stable at targets, These studies along with our finding of relatively little speed information in motor cortex, BMI systems enabling stable stops will be more effective and user-friendly when translated into clinical applications. Motor cortex plays a substantial role in driving movement, yet the details underlying this control remain unresolved We analyzed the extent to which movement-related information could be extracted from single-trial motor cortical activity recorded while monkeys performed center-out reaching enhances speed control. We found that single units carry relatively little speed-related information compared with direction-related information This result is not mitigated at the population level : simultaneously recorded population activity predicted speed with significantly lower accuracy relative to direction predictions revealed that speed accuracy would likely remain lower than direction accuracy, even given larger populations, SDKF improved success rates by a factor of 1, 7 relative to a standard Kalman filter in a closed-loop BMI task requiring stable stops at targets. Inspired a speed-dampening Kalman filter ( SDKF ) that automatically slows the cursor upon detecting changes in decoded movement direction Effectively, SDKF by using prevalent directional signals, rather than requiring speed to be directly decoded from neural activity. Using information theoretic techniques, Furthermore a unit-dropping analysis.
2K_test_519	Traditional hard real-time scheduling algorithms require the use of the worst-case execution times to guarantee that deadlines will be met. Unfortunately many algorithms with parameters derived from sensing the physical world suffer large variations in execution time, leading to pessimistic overall utilization, such as visual recognition tasks to maximize total system utility. Showing that ZS-QRAM is able to obtain 4 as much UDR as ZSRM, a previous overbooking approach, and almost 2 as much UDR as Rate-Monotonic with Period Transformation ( RM/TP We show that, by using our approach, we are able to keep the tasks that render the most utility by degrading lower-utility ones even in the presence of highly dynamic execution times. In this article we present ZS-QRAM, a scheduling approach that enables the use of flexible execution times and application-derived utility to tasks in order In particular, we provide a detailed description of the algorithm, the formal proofs for its temporal protection, and a detailed evaluation. Our evaluation uses the Utility Degradation Resilience ( UDR ) We then evaluate a Linux kernel module implementation of our scheduler on an Unmanned Air Vehicle ( UAV ) platform.
2K_test_520	Complex events essentially include human, scenes objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection, Many works have exploited attributes at image level for various applications. However attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Validate the efficacy of the proposed approach. Hence we propose to leverage attributes at video level ( named as video attributes in this work ), the semantic labels of external videos are used as attributes, Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events, Specifically building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset.
2K_test_521	Traditional power system state estimation methods lack the ability to track and manage increasing uncertainties inherent in the new technologies, such as recent and ongoing massive penetration of renewable energy, distribution intelligence and plug-in electric vehicles, To deal with the inability, a recent work proposes to utilize the unused historical data for power system state estimation First, because the power systems are with periodic patterns, which create clustered measurement data. Although able to achieve much higher accuracy, the new approach is slow due to the burden by sequential similarity check over large volumes of high dimensional historical measurements, making it unsuitable for online services, This calls for a general approach to preprocess the historical data, In this paper we propose to achieve such a goal with three steps, to remove redundancy To further reduce the computational time to group the clustered power system data into a tree structure. Results show that the new method can dramatically reduce the necessary computational time for online data-driven state estimation, while producing a highly accurate state estimate. Dimension reduction is proposed, but still able to retrieve similar measurements the k-dimensional tree indexing approach is employed in step two resulting in a log-reduction over searching time. Finally we verify the obtained historical power system states via AC power system model and the current measurements to filter out bad historical data.
2K_test_523	The Internet of Things ( IoT ) aims to integrate the digital world of the Internet with our encompassing physical world. However existing IoT lacks to provide considerate services, meaning that sensors dynamically `` collaborate '' to provide context-aware federated sensor data, to record and study historical interaction patterns among sensors, to leverage in-memory database to monitor and manage real-time sensor service provisioning, to assure scalability of sensor service communication, to leverage in-memory database to monitor and manage real-time sensor service provisioning. This paper reports our on-going work developing a sensor service federation and provisioning infrastructure novel approach has been presented to build social sensor networks A two-way publish/subscribe pattern on top of a message bus is established Workflow provenance is carried by a dynamic virtual device concept that we have introduced. A case study is reported A case study is reported.
2K_test_524	A key idea in object-oriented programming is that objects encapsulate state and interact with each other by message exchange This perspective suggests a model of computation that is inherently concurrent ( to facilitate simultaneous message exchange ) and that accounts for the effect of message exchange on an object 's state ( to express valid sequences of state transitions ). In this paper we show that such a model of computation arises naturally from session-based communication to express the protocols of message exchange and to reason about concurrency and state. We show that our language supports the typical patterns of object-oriented programming ( e, encapsulation dynamic dispatch and subtyping ) while guaranteeing session fidelity in a concurrent setting In addition, we show that our language facilitates new forms of expression ( e, type-directed reuse internal choice ), which are not available in current object-oriented languages. We introduce an object-oriented programming language that has processes as its only objects and employs linear session types. Based on various examples We have implemented our language in a prototype compiler.
2K_test_525	The Pascaline was the first working mechanical calculator, created in 1642 by the French polymath Blaise Pascal, Over the next two decades Pascal built 40 of these machines, of which nine survive today, Several good web resources describe the Pascaline, but to properly appreciate the sautoir, Pascal 's kinetic energy solution to jam-free ripple carry, building a working replica is invaluable. Thanks to the growing availability of rapid prototyping tools, it has become relatively easy for CS educators to fabricate physical artifacts to help students explore computational ideas. The Pascaline kit designed in SolidWorks, is open source and available at http : //www. I 've created a Pascaline kit using laser-cut acrylic and standard fasteners that can be assembled with just a screwdriver, pliers and Loctite High school or college students with minimal skills can put it together in a few hours and have a functioning calculator, Exploring the Pascaline 's design is an engaging way to connect a milestone in the early history of computing with more modern theoretical concepts, Students can investigate questions such as : What makes a device `` digital '' ? ( Slide rules have numeric scales but are analog devices, ) How does nonlinearity produce discrete states in a continuous world ? How are nonlinearities induced in the Pascaline vs, in digital electronics ? How do the logic design concepts `` half adder '' and `` full adder '' map onto the components of the Pascaline ? Is the Pascaline really adding, or merely counting ? How does the Pascaline use nines complement arithmetic to perform subtraction, and why is n't it tens complement ?.
2K_test_526	User review is a crucial component of open mobile app markets such as the Google Play Store, How do we automatically summarize millions of user reviews and make sense out of them ? We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users. Unfortunately beyond simple summaries such as histograms of user ratings, there are few analytic tools that can provide insights into user reviews. Results using our techniques are reported. In this paper we propose Wiscom, a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail, Our system is able to ( a ) discover inconsistencies in reviews ; ( b ) identify reasons why users like or dislike a given app, and provide an interactive, zoomable view of how users ' reviews evolve over time ; and ( c ) provide valuable insights into the entire app market, identifying users ' major concerns and preferences of different types of apps. On a 32GB dataset consisting of over 13 million user reviews of 171, 493 Android apps in the Google Play Store.
2K_test_527	In an effort to address persistent consumer privacy concerns, policy makers and the data industry seem to have found common grounds in proposals that aim at making online privacy more `` transparent, '' Such self-regulatory approaches rely on, among other things providing more and better information to users of Internet services about how their data is used These findings cast doubts on the likelihood of initiatives predicated around notices and transparency to address, by themselves online privacy concerns. However we illustrate that even simple privacy notices do not consistently impact disclosure behavior, and may in fact be used to nudge individuals to disclose variable amounts of personal information. We demonstrate that the impact of privacy notices on disclosure is sensitive to relative judgments, even when the objective risks of disclosure actually stay constant, we show that the impact of privacy notices on disclosure can be muted by introducing simple misdirections that do not alter the objective risk of disclosure. In a series of experiments In a first experiment, In a second experiment.
2K_test_528	The commoditization of wireless sensing systems makes it feasible to include BAS functionality in small and medium-sized buildings. The configuration complexity and cost of installation is now the dominant barrier to adoption focuses on ease-of-installation, secure configuration and management of BAS sub-systems in a manner that can scale from small to large installations. In this demo we introduce a platform called Mortar, io which Unlike cloud-reliant systems, io distributes storage and control functionality across end devices making it robust to network and internet outages The system, once initialized can run autonomously on a low-cost controller within a building or connect to the cloud for remote monitoring and configuration, We will also show our efficient multi-resolution data store that buffers data locally and replicates aggregate data across devices for reliability, A publish-subscribe model built on top of XMPP is used for messaging with per-device access control and a transducer schema, Finally a web portal provides an interface to monitor and schedule lighting, plug-loads environmental sensors and HVAC from a single uniform interface.
2K_test_529	ApplianceReader broadly demonstrates the potential of hybrid approaches that combine human and machine intelligence to effectively realize intelligent, interactive access technology today. Visually impaired people can struggle to use everyday appliances with inaccessible control panels, To address this problem, to make appliance interfaces accessible. We present ApplianceReader - a system that combines a wearable point-of-view camera with on-demand crowdsourcing and computer vision ApplianceReader sends photos of appliance interfaces that it has not seen previously to the crowd, who work in parallel to quickly label and describe elements of the interface, Computer vision techniques then track the user 's finger pointing at the controls and read out the labels previously provided by the crowd This enables visually impaired users to interactively explore and use appliances without asking the crowd repetitively.
2K_test_530	Advances in real-time embedded and distributed systems along with control and communication theory have catalyzed the rapid emergence of cyber-physical systems such as a self-driving car The importance of fault-tolerance support on a cyber-physical system ( CPS ) has been greatly emphasized by recent research due to the nature of CPS that senses its surroundings, processes sensor data and reacts using its actuators, In order to tackle this challenge, we proposed SAFER ( System-level Architecture for Failure Evasion in Real-time Applications ) in our previous work, SAFER is able to tolerate fail-stop processor and/or task failures for distributed embedded real-time systems. One of its limitations, however is that SAFER is not capable of tolerating a failure of a processor with a dedicated connection to an actuator, that relaxes this limitation. This paper provides a method by ( 1 ) deploying a small piece of hardware to avoid a dedicated connection between a processor and an actuator, ( 2 ) adding a software module that monitors and controls the hardware, and ( 3 ) enhancing the failure detection and recovery mechanisms of SAFER to support these changes. The detailed implementation and evaluation of the SAFER extension is on-going work.
2K_test_531	Behavioral coding is a common technique in the social sciences and human computer interaction for extracting meaning from video data [ 3 Rapid coding allows participants to have a `` conversation with their data '' to rapidly develop and refine research hypotheses in ways not previously possible. Since computer vision can not yet reliably interpret human actions and emotions, video coding remains a time-consuming manual process done by a small team of researchers, that allows researchers to rapidly analyze video datasets for behavioral events that are difficult to detect automatically. We show that Glance can accurately code events in video in a fraction of the time it would take a single person showing that Glance is able to code 80 % of an hour-long video in just 5 minutes. We present Glance a tool Glance uses the crowd to interpret natural language queries, and then aggregates and summarizes the content of the video. We also investigate speed improvements made possible by recruiting large crowds.
2K_test_532	To achieve three goals -- modular extensibility, automated verification and high performance. The model checker CBMC automatically verifies 5208 lines of C code in about 80 seconds using less than 2GB of RAM indicate that XMHF 's performance is comparable to popular high-performance general-purpose hypervisors for the single guest that it supports. We present the design, implementation and verification of XMHF- an eXtensible and Modular Hypervisor Framework XMHF is designed XMHF includes a core that provides functionality common to many hypervisor-based security architectures and supports extensions that augment the core with additional security or functional properties while preserving the fundamental hypervisor security property of memory integrity ( i, ensuring that the hypervisor 's memory is not modified by software running at a lower privilege level ). We verify the memory integrity of the XMHF core -- 6018 lines of code -- using a combination of automated and manual techniques We manually audit the remaining 422 lines of C code and 388 lines of assembly language code that are stable and unlikely to change as development proceeds.
2K_test_533	The promise of `` smart '' homes, workplaces schools and other environments has long been championed, Unattractive however has been the cost to run wires and install sensors, More critically raw sensor data tends not to align with the types of questions humans wish to ask, do I need to restock my pantry ? Through our API, Zensors can enable a variety of rich end-user applications and moves us closer to the vision of responsive. Although techniques like computer vision can answer some of these questions, it requires significant effort to build and train appropriate classifiers, Even then these systems are often brittle, with limited ability to handle new or unexpected situations, including being repositioned and environmental changes ( e, lighting furniture seasons ) to provide robust, adaptive and readily deployable intelligent sensors. We propose Zensors a new sensing approach that fuses real-time human intelligence from online crowd workers with automatic approaches With Zensors, users can go from question to live sensor feed in less than 60 seconds.
2K_test_534	Video analysis has been attracting increasing research due to the proliferation of internet videos. In this paper we investigate how to improve the performance on internet quality video analysis, Particularly we work on the scenario of few labeled training videos being provided, which is less focused in multimedia, To being with we consider how to more effectively harness the evidences from the low-level features Researchers have developed several promising features to represent videos to capture the semantic information, However as videos usually characterize rich semantic contents, the analysis performance by using one single feature is potentially limited, Simply combining multiple features through early fusion or late fusion to incorporate more informative cues is doable but not optimal due to the heterogeneity and different predicting capability of these features. And the comparison to other state-of-the-art multi-feature learning algorithms has validated the efficacy of our framework. For better exploitation of multiple features, we propose to mine the importance of different features and cast it into the learning of the classification model, Our method is based on multiple graphs from different features and uses the Riemannian metric to evaluate the feature importance, On the other hand, to be able to use limited labeled training videos for a respectable accuracy we formulate our method in a semi-supervised way The main contribution of this paper is a novel scheme of evaluating the feature importance that is further casted into a unified framework of harnessing multiple weighted features with limited labeled training videos. We perform extensive experiments on video action recognition and multimedia event recognition.
2K_test_535	For collaborative multi-agent Markov decision processes ( MDPs ) are presented and analyzed With the objective of jointly learning the optimal stationary control policy ( in the absence of global state transition and local agent cost statistics ) that minimizes network-averaged infinite horizon discounted cost. The proposed distributed algorithms are shown to achieve optimal learning asymptotically, almost surely ( a, ) each network agent is shown to learn the value function and the optimal stationary control policy of the collaborative MDP asymptotically Further, convergence rate estimates for the proposed class of distributed learning algorithms are obtained. Distributed reinforcement learning algorithms The networked setup consists of a collection of agents ( learners ) which respond differently ( depending on their instantaneous one-stage random costs ) to a global controlled state and the control actions of a remote controller, the paper presents distributed variants of Q-learning of the consensus + innovations type in which each agent sequentially refines its learning parameters by locally processing its instantaneous payoff data and the information received from neighboring agents. Under broad conditions on the multi-agent decision model and mean connectivity of the inter-agent communication network.
2K_test_537	With the advancement of information systems, means of communications are becoming cheaper, faster and more available, Today millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want, They can access their e-mails, comment on weblogs watch and post videos and photos ( as well as comment on them ), and make phone calls or text messages almost ubiquitously, We also show three potential applications of the SFP : as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications, as a technique to detect anomalies, and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems. Given this scenario in this article, we tackle a fundamental aspect of this new era of communication : How the time intervals between communication events behave for different technologies and means of communications Are there universal patterns for the Inter-Event Time Distribution ( IED ) q How do inter-event times behave differently among particular technologiesq To answer these questions to generate inter-event times between communications. Moreover we propose the use of the Self-Feeding Process ( SFP ) The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data. We analyzed eight different datasets from real and modern communication data and found four well-defined patterns seen in all the eight datasets.
2K_test_539	Multimedia Event Detection ( MED ) is a multimedia retrieval task with the goal of finding videos of a particular event in video archives, given example videos and event descriptions ; different from MED, multimedia classification is a task that classifies given videos into specified classes Generally, early fusion and late fusion are two popular combination strategies, The former one fuses features before performing classification and the latter one combines output of classifiers from different features, Early fusion can better capture the relationship among features yet is prone to over-fit the training data, Late fusion deals with the over-fitting problem better but does not allow classifiers to train on all the data at the same time. Both tasks require mining features of example videos to learn the most discriminative features, with best performance resulting from a combination of multiple complementary features, How to combine different features is the focus of this paper. Results are reported For the MED 2010 dataset, we get a mean minimal normalized detection cost ( MMNDC ) of 0, 49 which exceeds the state-of-the-art performance by more than 12 percent, On the TRECVID MED 2011 test dataset, we achieve a MMNDC of 0, 51 which is the second best among all 19 participants, On UCF50 and HMDB51, we obtain classification accuracy of 88, 1 % and 48, 7 % respectively which are the best reported results to date. In this paper we introduce a fusion scheme named double fusion, which simply combines early fusion and late fusion together to incorporate their advantages. On the TRECVID MED 2010, MED 2011 UCF50 and HMDB51 datasets.
2K_test_540	Given a large collection of co-evolving multiple time-series, which contains an unknown number of patterns of different durations, how can we efficiently and effectively find typical patterns and the points of variation ? How can we statistically summarize all the sequences, and achieve a meaningful segmentation ? for co-evolving time sequences. Demonstrate that AutoPlait does indeed detect meaningful patterns correctly, and it outperforms state-of-the-art competitors as regards accuracy and speed : AutoPlait achieves near-perfect, over 95 % precision and recall, and it is up to 472 times faster than its competitors. In this paper we present AutoPlait, a fully automatic mining algorithm Our method has the following properties : ( a ) effectiveness : it operates on large collections of time-series, and finds similar segment groups that agree with human intuition ; ( b ) scalability : it is linear with the input size, and thus scales up very well ; and ( c ) AutoPlait is parameter-free, and requires no user intervention, no prior training and no parameter tuning. Extensive experiments on 67GB of real datasets.
2K_test_541	In this paper we focus on common data reorganization operations such as shuffle, pack/unpack swap transpose and layout transformations Although these operations simply relocate the data in the memory, they are costly on conventional systems mainly due to inefficient access patterns, limited data reuse and roundtrip data traversal throughout the memory hierarchy for efficient data reorganization. For the various test cases, in-memory data reorganization provides orders of magnitude performance and energy efficiency improvements via low overhead hardware. This paper presents a two pronged approach, which combines ( i ) a proposed DRAM-aware reshape accelerator integrated within 3D-stacked DRAM, and ( ii ) a mathematical framework that is used to represent and optimize the reorganization operations. We evaluate our proposed system through two major use cases First, we demonstrate the reshape accelerator in performing a physical address remapping via data layout transform to utilize the internal parallelism/locality of the 3D-stacked DRAM structure more efficiently for general purpose workloads Then, we focus on offloading and accelerating commonly used data reorganization routines selected from the Intel Math Kernel Library package We evaluate the energy and performance benefits of our approach by comparing it against existing optimized implementations on state-of-the-art GPUs and CPUs.
2K_test_542	Can we identify patterns of temporal activities caused by human communications in social media ? Is it possible to model these patterns and tell if a user is a human or a bot based only on the timing of their postings ? Social media services allow users to make postings, generating large datasets of human activity time-stamps. In this paper we analyze time-stamp data from social media services that is able to match all four discovered patterns. And find that the distribution of postings inter-arrival times ( IAT ) is characterized by four patterns : ( i ) positive correlation between consecutive IATs, ( ii ) heavy tails, ( iii ) periodic spikes and ( iv ) bimodal distribution, by showing that it can accurately fit real time-stamp data from Reddit and Twitter, We also show that RSC can be used to spot outliers and detect users with non-human behavior, RSC consistently provides a better fit to real data and clearly outperform existing models for human dynamics, RSC was also able to detect bots with a precision higher than 94 %. Based on our findings, we propose Rest-Sleep-and-Comment ( RSC ). We demonstrate the utility of RSC We validate RSC using real data consisting of over 35 million postings from Twitter and Reddit.
2K_test_543	Detecting and quantifying the timing and the genetic contributions of parental populations to a hybrid population is an important but challenging problem in reconstructing evolutionary histories from genetic variation data, With the advent of high throughput genotyping technologies, new methods suitable for large-scale data are especially needed. Furthermore existing methods typically assume the assignment of individuals into subpopulations is known, when that itself is a difficult problem often unresolved for real data, to both identify population assignments and learn divergence times and admixture proportions for those populations. On simulated sequences our methods show better accuracy and faster runtime than leading competitive methods in estimating admixture fractions and divergence times Analysis on the real data further shows our methods to be effective at matching our best current knowledge about the relevant populations. Here we propose a novel method that combines prior work for inferring nonreticulate population structures with an MCMC scheme for sampling over admixture scenarios using genome-scale admixed genetic variation data. We validated our method using coalescent simulations and a collection of real bovine and human variation data.
2K_test_544	From a technical viewpoint, the proposed distributed estimator leads to non-Markovian mixed timescale stochastic recursions and the analytical methods developed in the paper contribute to the general theory of distributed stochastic approximation. The paper studies the problem of distributed parameter estimation. Is shown to yield consistent parameter estimates at each network agent, Further it is shown that the distributed estimator is asymptotically efficient, in that the asymptotic covariances of the agent estimates coincide with that of the optimal centralized estimator, the inverse of the centralized Fisher information rate. In multi-agent networks with exponential family observation statistics A certainty-equivalence type distributed estimator of the consensus + innovations form is proposed in which, at each each observation sampling epoch agents update their local parameter estimates by appropriately combining the data received from their neighbors and the locally sensed new information ( innovation ), Under global observability of the networked sensing model, the ability to distinguish between different instances of the parameter value based on the joint observation statistics, and mean connectivity of the inter-agent communication network.
2K_test_545	Thus it appears that disorders of the Rb/E2F axis can arise at multiple organ sites and produce tumors that simultaneously overexpress multiple E2F-responsive genes. Reasoning that overexpression of multiple E2F-responsive genes might be a useful marker for RB1 dysfunction. In breast cancer a group of tumors was identified, each of which simultaneously overexpressed multiple E2F-responsive genes, Seventy percent of these genes were concerned with cell cycle progression, DNA repair or mitosis These E2F-responsive gene overexpressing ( ERGO ) tumors frequently exhibited additional evidence of Rb/E2F axis dysfunction, were mostly triple negative, and preferentially overexpressed multiple basal cytokeratins, suggesting that they overlapped substantially with the basal-like tumor subset, ERGO tumors were also identified in serous ovarian cancer and prostate cancer, In these cancer types, there was no evidence for a tumor subset comparable to the breast cancer basal-like subset, A core group of about 30 E2F-responsive genes were overexpressed in all three cancer types. We compiled a list of E2F-responsive genes from the literature and evaluated their expression in publicly available gene expression microarray data of patients with breast cancer, serous ovarian cancer and prostate cancer.
2K_test_546	We conclude with example applications and thoughts on future avenues of research. We explore 3D printing physical controls whose tactile response can be manipulated programmatically through pneumatic actuation. Which demonstrate the feasibility of our approach. In particular by manipulating the internal air pressure of various pneumatic elements, we can create mechanisms that require different levels of actuation force and can also change their shape, We describe the challenges that we faced and the methods that we used to overcome some of the limitations of current 3D printing technology. We introduce and discuss a series of example 3D printed pneumatic controls This includes conventional controls, such as buttons knobs and sliders, but also extends to domains such as toys and deformable interfaces.
2K_test_547	Many people would find the Web easier to use if content was a little bigger, even those who already find the Web possible to use now We believe this concept applies generally across a wide range of accessibility improvements designed to help people with diverse abilities. Improvements intended to make a web page easier to access, such as magnification are automatically applied to the extent that they can be without causing negative side effects. By magnifying existing web pages 1, 6x on average without introducing negative side effects. This paper introduces the idea of opportunistic accessibility improvement in which We explore this idea with oppaccess, js an easily-deployed system for magnifying web pages that iteratively increases magnification until it notices negative side effects, such as horizontal scrolling or overlapping text. We validate this approach.
2K_test_548	These theoretical results have direct practical implications. We consider the problem of fairly allocating indivisible goods, focusing on a recently-introduced notion of fairness called maximin share guarantee : Each player 's value for his allocation should be at least as high as what he can guarantee by dividing the items into as many bundles as there are players and receiving his least desirable bundle. We show that such allocations may not exist, but allocations guaranteeing each player 2/3 of the above value always exist. And can be computed in polynomial time when the number of players is constant. Assuming additive valuation functions.
2K_test_551	To improve accuracy in differentially private data analysis. In particular while our generic procedure is computationally inefficient, for the specific definition of H as graphs of bounded degree, we exhibit efficient ways of constructing f H using different projection-based techniques, We demonstrate that the restricted sensitivity of such queries can be significantly lower than their smooth sensitivity Thus, using restricted sensitivity we can maintain privacy whether or not D HH, while providing more accurate results in the event that HH holds true. We demonstrate the usefulness of this notion by considering the task of answering queries regarding social-networks, We then analyze two important query classes : subgraph counting queries ( e, number of triangles ) and local profile queries ( e, number of people who know a spy and a computer-scientist who know each other ).
2K_test_552	Activity recognition can provide computers with the context underlying user inputs, enabling more relevant responses and more fluid interaction, Prior work has enabled the crowd to provide labels in real-time to train automated systems on-the-fly, but numerous examples are still needed before the system can recognize an activity on its own. However training these systems is difficult because it requires observing every possible sequence of actions that comprise a given activity, To reduce the need to collect this data by observing users. Show that over seven times as many examples can be collected using our approach versus relying on direct observation alone, demonstrating that by leveraging the understanding of the crowd, it is possible to more easily train automated systems. We introduce ARchitect a system that uses the crowd to capture the dependency structure of the actions that make up activities.
2K_test_553	Multimedia event detection ( MED ) plays an important role in many applications such as video indexing and retrieval. Current event detection works mainly focus on sports and news event detection or abnormality detection in surveillance videos, Differently our research aims to detect more complicated and generic events within a longer video sequence, In the past researchers have proposed using intermediate concept classifiers with concept lexica to help understand the videos, Yet it is difficult to judge how many and what concepts would be sufficient for the particular video analysis task, Additionally obtaining robust semantic concept classifiers requires a large number of positive training examples, which in turn has high human annotation cost. Demonstrate the effectiveness of the proposed approach. In this paper we propose an approach that exploits the external concepts-based videos and event-based videos simultaneously to learn an intermediate representation from video features, Our algorithm integrates the classifier inference and latent intermediate representation into a joint framework, The joint optimization of the intermediate representation and the classifier makes them mutually beneficial and reciprocal, Effectively the intermediate representation and the classifier are tightly correlated, The classifier dependent intermediate representation not only accurately reflects the task semantics but is also more suitable for the specific classifier, Thus we have created a discriminative semantic analysis framework based on a tightly coupled intermediate representation. Extensive experiments on multimedia event detection using real-world videos.
2K_test_554	Ultrasonic guided-waves propagating in pipes with varying environmental and operational conditions ( EOCs ) are usually the results of complex superposition of multiple modes travelling in multiple paths, Among all of the components forming a complex guided-wave signal, the arrivals scattered by damage ( so called scatter signal ) are of importance for damage diagnosis purposes, Current approaches for extracting scatter signal can be categorized as ( A ) baseline subtraction methods, and ( B ) linear decomposition. This paper evaluates the potentials of nonlinear decomposition methods for extracting the scatter signal from a multi-modal signal recorded from a pipe under varying temperatures In this paper, we first illustrate experimentally, the challenges for applying these methods on multi-modal signals at varying temperatures, In this paper for removing the nonlinear relation between the components of a multi-modal guided-wave signal, and thus extracting the scatter signal. The simulation results show that different wave modes may have significantly different sensitivities to temperature variations This brings about challenges such as shape distortion and nonlinear relations between the signals recorded at different temperatures, which prevent the aforementioned methods to be extensible to wide range of temperatures, It is observed that NLPCA can successfully remove nonlinear relations between the signal bases, hence extract scatter signal, for temperature variations up to 10, with detection accuracies above 99 %. We examine the potential of a nonlinear decomposition method, namely nonlinear principal component analysis ( NLPCA ). To better analyze the experimental results, the effects of temperature on multi-modal signals are simulated Ultrasonic pitch-catch measurements from an aluminum pipe segment in a thermally controlled laboratory are used to evaluate the detection performance of the damage-sensitive features extracted by the proposed approach.
2K_test_555	Abstract In case of an emergency in a building, first responders need to know current blockages in the building ( e, blocked passageways and exits ) and safe evacuating paths so that the occupants can be guided to the unblocked exits and safe paths toward those exits The estimated blockage information can be used to create a topological map of the damaged building, indicating safe paths toward available unblocked exits. To automatically determine blockage levels at buildings to fuse sensor and video camera data for estimating the level of blockage in the hallway for different damage combinations applied on building components. The results demonstrated the technical feasibility of the proposed system and the findings of the decision tree highlight that by using less number of sensors, a cost-effective configuration can be achieved. A system that fuses data from multiple sensors and video camera was proposed A prototype was developed and a decision tree method was used. And tested on an experimental model of a pilot building 's hallway, A series of damage tests were conducted on the hallway model and recorded by the sensors and the video camera, Individual performances of sensors and video camera were evaluated.
2K_test_556	Topic models are effective probabilistic tools for processing large collections of unstructured data, The open-source C++ implementation of the proposed algorithm is available at https : //github. With the exponential growth of modern industrial data, and consequentially also with our ambition to explore much bigger models, there is a real pressing need to significantly scale up topic modeling algorithms, which has been taken up in lots of previous works, culminating in the recent fast Markov chain Monte Carlo sampling algorithms in [ 10, 23 ] for the unsupervised latent Dirichlet allocation ( LDA ) formulations, In this work we extend the recent sampling advances for unsupervised LDA models to supervised tasks. We focus on the Gibbs MedLDA model [ 27 ] that is able to simultaneously discover latent structures and make accurate predictions, By combining a set of sampling techniques we are able to reduce the O ( K 3 + DK 2 + DNK complexity in [ 27 ] to O ( DK + DN ) when there are K topics and D documents with average length N, To our best knowledge, this is the first linear time sampling algorithm for supervised topic models, Our algorithm requires minimal modifications to incorporate most loss functions in a variety of supervised tasks, and we observe in our experiments an order of magnitude speedup over the current state-of-the-art implementation, while achieving similar prediction performances.
2K_test_557	For extending prior work where both are assumed to be deterministic. A target in stationary random multipath clutter. We show that the Time Reversal Likelihood Ratio Test performs much better than the conventional Weighted Energy Detector, results show that the Linear Quadratic detector is a good approximation to the Time Reversal Likelihood Ratio Test and that both show a significant improvement of 4 to 7 dB effective signal-to-noise ratio gain over the Weighted Energy Detector, This gain is dependent on the target and clutter power spectral densities. We derive the time reversal likelihood ratio optimal detector We suppress the stationary clutter through adaptive power allocation. We provide analytical on the performance of the time reversal detector by approximating it with the Time Reversal Linear Quadratic detector, which models the received signal as a Complex Gaussian Our simulations.
2K_test_559	Text can often be complex and difficult to read, especially for people with cognitive impairments or low literacy skills, Text simplification is a process that reduces the complexity of both wording and structure in a sentence, while retaining its meaning, This may allow simplification systems and system builders to get better feedback about how well content is being simplified, as compared to standard measures which classify content into 'simplified ' or 'not simplified ' categories Our study provides evidence that the crowd could be used to evaluate English text simplification, as well as to create simplified text in future work. However this is currently a challenging task for machines, and thus providing effective on-demand text simplification to those who need it remains an unsolved problem, Even evaluating the simplicity of text remains a challenging problem for both computers, which can not understand the meaning of text, and humans who often struggle to agree on what constitutes a good simplification. We show that leveraging crowds can result in a collective decision that is accurate and converges to a consensus rating, show that the crowd can effectively rate levels of simplicity. This paper focuses on the evaluation of English text simplification using the crowd, Our results from 2.
2K_test_560	Seeking solutions from one domain to solve problems in another is an effective process of innovation, This work provides a useful method for finding analogies, and can streamline innovation for both novices and professional designers. This process of analogy searching is difficult for both humans and machines for re-presenting a problem in terms of its abstract structure, and then allowing people to use this structural representation to find analogies. We show the benefits of using abstract structural representations to search for ideas that are analogous to a source problem, and that these analogies result in better solutions than alternative approaches. In this paper we present a novel approach We propose a crowdsourcing process that helps people navigate a large dataset to find analogies.
2K_test_562	In 1876 Charles Lutwidge Dodgson suggested the intriguing voting rule that today bears his name Although Dodgsons rule is one of the most well-studied voting rules, it suffers from serious deficiencies, both from the computational point of viewit is NP-hard even to approximate the Dodgson score within sublogarithmic factorsand from the social choice point of viewit fails basic social choice desiderata such as monotonicity and homogeneity. However this does not preclude the existence of approximation algorithms for Dodgson that are monotonic or homogeneous, and indeed it is natural to ask whether such algorithms exist, In this article we give definitive answers to these questions. Furthermore we show that a slight variation on a known voting rule yields a monotonic, homogeneous polynomial-time O ( m log m ) -approximation algorithm and establish that it is impossible to achieve a better approximation ratio even if one just asks for homogeneity we prove that algorithms with an approximation ratio that depends only on m do not exist. We design a monotonic exponential-time algorithm that yields a 2-approximation to the Dodgson score, while matching this result with a tight lower bound We also present a monotonic polynomial-time O ( log m ) -approximation algorithm ( where m is the number of alternatives ) ; this result is tight as well due to a complexity-theoretic lower bound. We complete the picture by studying several additional social choice properties ; for these properties.
2K_test_563	Given a snapshot of a large graph, in which an infection has been spreading for some time, can we identify those nodes from which the infection started to spread ? In other words, can we reliably tell who the culprits are ? In this paper, we answer this question affirmatively Essentially, we are after that set of seed nodes that best explain the given snapshot, to identify the best set of seed nodes and virus propagation ripple to identify likely sets of seed nodes. Shows high accuracy in the detection of seed nodes, in addition to the correct automatic identification of their number Moreover, NetSleuth scales linearly in the number of nodes of the graph. And give an efficient method called NetSleuth for the well-known susceptible-infected virus propagation model, We propose to employ the minimum description length principle as the one by which we can most succinctly describe the infected graph, We give an highly efficient algorithm given a snapshot, Then given these seed nodes, we show we can optimize the virus propagation ripple in a principled way by maximizing likelihood, With all three combined, NetSleuth can automatically identify the correct number of seed nodes, as well as which nodes are the culprits. Experimentation on our method.
2K_test_566	As sensor networks gain traction and begin to scale, we will be increasingly faced with challenges associated with managing large-scale time-series data, that is capable of serving large amounts of time-series data from a continuously updating datastore with access latencies low enough to support interactive real-time visualization. That Respawn is able to run on ARM-based edge node devices connected to a cloud-backend with the ability to serve thousands of clients and terabytes of data with sub-second latencies. In this paper we present a cloud-to-edge partitioned architecture called Respawn Respawn targets sensing systems where resource-constrained edge node devices may only have limited or intermittent network connections linking them to a cloud-backend, The cloud-backend provides aggregate storage and transparent dispatching of data queries to edge node devices, Data is downsampled as it enters the system creating a multi-resolution representation capable of lowlatency range-base queries, Lower-resolution aggregate data is automatically migrated from edge nodes to the cloud-backend both for improved consistency and caching, In order to further mask latency from users, edge nodes automatically identify and migrate blocks of data that contain statistically interesting features. We show through simulation and micro-benchmarking.
2K_test_567	Online instructional videos have become a popular way for people to learn new skills encompassing art. As watching instructional videos is a natural way for humans to learn, analogously machines can also gain knowledge from these videos, to harvest examples of various actions in an unsupervised fashion. Show that the examples harvested are of reasonably good quality, and action detectors trained on data collected by our unsupervised method yields comparable performance with detectors trained with manually collected data on the TRECVID Multimedia Event Detection task. We propose to utilize the large amount of instructional videos available online The key observation is that in instructional videos, the instructor 's action is highly correlated with the instructor 's narration, By leveraging this correlation, we can exploit the timing of action corresponding terms in the speech transcript to temporally localize actions in the video and harvest action examples The proposed method is scalable as it requires no human intervention.
2K_test_568	A way of preventing automated offline dictionary attacks against user selected passwords. We introduce GOTCHAs ( Generating panOptic Turing Tests to Tell Computers and Humans Apart ) as A GOTCHA is a randomized puzzle generation protocol, which involves interaction between a computer and a human, Informally a GOTCHA should satisfy two key properties : ( 1 ) The puzzles are easy for the human to solve, ( 2 ) The puzzles are hard for a computer to solve even if it has the random bits used by the computer to generate the final puzzle -- - unlike a CAPTCHA [ 44 ], Our main theorem demonstrates that GOTCHAs can be used to mitigate the threat of offline dictionary attacks against passwords by ensuring that a password cracker must receive constant feedback from a human being while mounting an attack Finally, we provide a candidate construction of GOTCHAs based on Inkblot images, Our construction relies on the usability assumption that users can recognize the phrases that they originally used to describe each Inkblot image -- - a much weaker usability assumption than previous password systems based on Inkblots which required users to recall their phrase exactly. We conduct a user study to evaluate the usability of our GOTCHA construction, We also generate a GOTCHA challenge where we encourage artificial intelligence and security researchers to try to crack several passwords protected with our scheme.
2K_test_569	Viral videos that gain popularity through the process of Internet sharing are having a profound impact on society The application of our work is not only important for advertising agencies to plan advertising campaigns and estimate costs, but also for companies to be able to quickly respond to rivals in viral marketing campaigns The proposed method is unique in that it is the first attempt to incorporate video metadata into the peak day prediction. Existing studies on viral videos have only been on small or confidential datasets to forecast the future peak day of viral videos. We discover some interesting characteristics of viral videos, The empirical results demonstrate that the proposed method outperforms the state-of-the-art methods, with statistically significant differences. Based on our analysis, in the second half of the paper, we propose a model. We collect by far the largest open benchmark for viral video study called CMU Viral Video Dataset, and share it with researchers from both academia and industry, Having verified existing observations on the dataset.
2K_test_570	Space-Time Adaptive Processing ( STAP ) is a technique for processing signals from multiple antenna elements over multiple time periods for target detection, As STAP algorithms are typical run on airborne platforms, they need to be both high performance and energy-efficient Due to the high rate of processing required, many existing algorithms focus on reducing the dimensionality of the data, or exploiting structure in the underlying mathematical formulation in order to reduce the total number of floating-point operations ( FLOPs ), and consequently the time for computation. While such algorithms target the FLOPs-intensive operations within the STAP algorithm, a significant portion of the compute time for most STAP algorithms is actually spent in low-FLOPs, memory-bounded operations we address the computation of these memory-bounded operations within the STAP algorithm. We show that more than 11x improvement in time, and 77x improvement in energy efficiency can be expected when a 3D stack is used together with memory-side accelerators to target the memory-bounded operations within STAP. In this paper using a 3D stacked Logic-in-Memory system The imminent arrival of 3D stacked memory makes avail high memory bandwidth, which opens up a new and orthogonal dimension for optimizing STAP algorithms.
2K_test_571	For deeply scaled digital integrated systems, the power required for transporting data between memory and logic can exceed the power needed for computation, thereby limiting the efficacy of synthesizing logic and compiling memory independently Logic-in-Memory ( LiM ) architectures address this challenge by embedding logic within the memory block to perform basic operations on data locally for specific functions. While custom smart memories have been successfully constructed for various applications, a fully automated LiM synthesis flow enables architectural exploration that has heretofore not been possible that performs co-design of algorithms and architectures to explore system level trade-offs. Results shown in this paper demonstrate a 250x performance improvement and 310x energy savings for a data-intensive application example. In this paper we present a tool and design methodology for LiM physical synthesis The resulting layouts and timing models can be incorporated within any physical synthesis tool.
2K_test_572	Control decisions are made. We consider an adaptive cruise control system in which based on position and velocity information received from other vehicles via V2V wireless communication If the vehicles follow each other at a close distance, they have better wireless reception but collisions may occur when a follower car does not receive notice about the decelerations of the leader car fast enough to react before it is too late, If the vehicles are farther apart, they would have a bigger safety margin, but the wireless communication drops out more often, so that the follower car no longer receives what the leader car is doing, In order to guarantee safety, such a system must return control to the driver if it does not receive an update from a nearby vehicle within some timeout period, The value of this timeout parameter encodes a tradeoff between the likelihood that an update is received and the maximum safe acceleration Combining formal verification techniques for hybrid systems with a wireless communication model. We analyze how the expected efficiency of a provably-safe adaptive cruise control system is affected by the value of this timeout.
2K_test_573	Accurate models of the cross-talk between signaling pathways and transcriptional regulatory networks within cells are essential to understand complex response programs, Consequently our method is widely applicable and can be used to derive accurate, dynamic response models in several species. To reconstruct dynamic and causal stress response networks. Demonstrate the predictive power of our method. We present a new computational method that combines condition-specific time-series expression data with general protein interaction data These networks characterize the pathways involved in the response, their time of activation, and the affected genes The signaling and regulatory components of our networks are linked via a set of common transcription factors that serve as targets in the signaling network and as regulators of the transcriptional response network, Our method correctly identifies the core signaling proteins and transcription factors of the response programs, It further predicts the involvement of additional transcription factors and other proteins not previously implicated in the response pathways, Our approach requires little condition-specific data : only a partial set of upstream initiators and time-series gene expression data, which are readily available for many conditions and species. Detailed case studies of stress responses in budding yeast We experimentally verify several of these predictions for the osmotic stress response network.
2K_test_574	People spend an enormous amount of time searching for complex information online ; for example, consumers researching new purchases or patients learning about their conditions. As they search people build up rich mental schemas about their target domains ; which, if effectively shared could accelerate learning for others with similar interests, for integrating the schemas individuals develop as they gather information online and surfacing them for others with similar interests. We show that having access to others ' schemas while foraging for information helps new users to induce more useful, prototypical and better-structured schemas than gathering information alone. In this paper we introduce a novel approach. Through a controlled experiment.
2K_test_575	Multimedia event detection ( MED ) is a retrieval task with the goal of finding videos of a particular event in a large scale internet video archive, given example videos and text descriptions. Nowadays different multimodal fusion schemes of low-level and high-level features are extensively investigated and evaluated for MED, For most of events in MED, people are usually the central subjects in videos, The face of a person can be considered as the most important factor which brings a lot of information describing the video events, However face information has not been systematically investigated in the previous research for MED. Show that our proposed method outperforms the state-of-the-art methods by up to 4 %. In this paper we investigate the possibility of using the high-level face information to assist multimedia event detection, Moreover since the labeled data in TRECVID MED dataset are limited, we propose a semi-supervised kernel ridge regression which works well in practice to explore the useful information from unlabeled data to assist the event detection. Extensive experimental results on TRECVID MED dataset.
2K_test_576	Current research is interested in identifying how topology impacts epidemics in networks. We can obtain a closed form description of the equilibrium distribution. We show that for k-regular graph topologies, the most probable network state transitions from the state where everyone is healthy to one where everyone is infected at a threshold that depends on k but not on the size of the graph. In this paper we model SIS ( susceptible-infected-susceptible ) epidemics as a continuous-time Markov process and for which Such distribution describes the long-run behavior of the epidemics, The adjacency matrix of the network topology is reflected explicitly in the formulation of the equilibrium distribution. Secondly we are interested in analyzing the model in the regime where the topology dependent infection process opposes the topology independent healing process, Specifically how will network topology affect the most probable long-run network state ?.
2K_test_577	Provision of training data sets is one of the core requirements for event-based supervised NILM ( Non-Intrusive Load Monitoring ) algorithms, Due to diversity in appliances ' technologies, in-situ training by users is often required, This process might require continuous user-interaction to ensure that a high quality training data set is provided. Pre-populating a training data set could potentially reduce the need for user-system interaction to enable autonomous partitioning of appliances signature space ( i, e feature space ) for applications in electricity consumption disaggregation. The algorithm performance in accurate partitioning of the feature space and the effect of different feature extraction techniques were presented and discussed. In this study a heuristic unsupervised clustering algorithm is presented and evaluated The algorithm is based on hierarchical clustering and uses the characteristics of a cluster binary tree to determine the distance threshold for pruning the tree without a priori information, The algorithm determines the partition of a feature space recursively to account for multi-scale nature of the binary cluster tree. Evaluation of the algorithm was carried out using metrics for accuracy and cluster quality ( proposed in this study ) on a fully labeled data set that was collected and processed in a real residential setting.
2K_test_578	Given a table where rows correspond to records and columns correspond to attributes, we want to find a small number of patterns that succinctly summarize the dataset, For example given a set of patient records with several attributes each, how can we find ( a ) that the `` most representative '' pattern is, say ( male adult, * ) followed by ( *, child low-cholesterol ) etc ? that provides a sequence of patterns. Demonstrate the effectiveness and intuitiveness of our discovered patterns. We propose TSum a method ordered by their `` representativeness It can decide both which these patterns are, as well as how many are necessary to properly summarize the data, Our main contribution is formulating a general framework, TSum using compression principles, TSum can easily accommodate different optimization strategies for selecting and refining patterns, The discovered patterns can be used to both represent the data efficiently, as well as interpret it quickly.
2K_test_579	Matched field processing is a powerful tool for accurately localizing targets in dispersive media, However matched field processing requires a precise model of the medium under test, In underwater acoustics where matched field processing has been extensively studied, authors often resort to extremely detailed numerical models of the propagation medium, which are computationally expensive and impractical for many applications. To construct an accurate model of a plate medium. We demonstrate the effectiveness of this model The results visually illustrate our approach to significantly improve localization accuracy and reduce artifacts when compared to a conventional narrowband technique. As an alternative this paper uses convex sparse recovery techniques directly from measured data based on its dispersion characteristics, From this data-driven model, the Green 's function between two points can be readily predicted. By localizing a source in a dispersive plate medium.
2K_test_580	Restricted Boltzmann Machine ( RBM ) has shown great effectiveness in document modeling, It utilizes hidden units to discover the latent topics and can learn compact semantic representations for documents which greatly facilitate document retrieval. The popularity ( or frequency ) of topics in text corpora usually follow a power-law distribution where a few dominant topics occur very frequently while most topics ( in the long-tail region ) have low probabilities, Due to this imbalance, RBM tends to learn multiple redundant hidden units to best represent dominant topics and ignore those in the long-tail region, which renders the learned representations to be redundant and non-informative, To solve this problem. Demonstrate that with diversification, the document modeling power of DRBM can be greatly improved. We propose Diversified RBM ( DRBM ) which diversifies the hidden units, to make them cover not only the dominant topics, but also those in the long-tail region, We define a diversity metric and use it as a regularizer to encourage the hidden units to be diverse, Since the diversity metric is hard to optimize directly, we instead optimize its lower bound and prove that maximizing the lower bound with projected gradient ascent can increase this diversity metric. Experiments on document retrieval and clustering.
2K_test_582	Despite benefits and uses of social networking sites ( SNSs ) users are not always satisfied with their behaviors on the sites, Based on these results we provide insights both into how participants perceive different SNSs, as well as potential designs for behavior-change mechanisms to target SNS behaviors. These desires for behavior change both provide insight into users ' perceptions of how SNSs impact their lives ( positively or negatively ) and can inform tools for helping users achieve desired behavior changes to explore SNS users ' behavior-change goals for Facebook. While some participants want to reduce site use, others want to improve their use or increase a range of behaviors, These desired changes differ by SNS, and for Twitter by participants ' levels of site use Participants also expect a range of benefits from these goals, including increased time contact with others, intrinsic benefits better security/privacy, and improved self presentation. We use a 604-participant online survey.
2K_test_583	Non-Intrusive Load Monitoring ( NILM ) has been studied for a few decades now as a method of disaggregating information about appliance level power consumption in a building from aggregate measurements of voltage and/or current obtained at a centralized location in the electrical system When such information is provided to the electricity consumer as feedback, they can then take the necessary steps to modify their behavior and conserve Research has shown potential for savings of up to 20 % through this kind of feedback, The training phase required to allow the algorithms to recognize appliances in the home at the beginning of a NILM setup is a big hindrance to wide adoption of the technique, One of the recent advances in this research area includes the addition of an Electro-Magnetic Field ( EMF ) sensor that measures the electric and magnetic field nearby an appliance to detect its operational state, This information when coupled with the aggregate power consumption data for the home, can help to train a NILM system, which is a significant step forward in automating the training phase, Possible reasons behind the findings are discussed and areas for further exploration are proposed. This paper explores the theory behind the operation of the EMF sensor and discusses the feasibility of. A vector subspace obtained using Independent Component Analysis ( ICA ), along with a k-NN classifier, was found to perform best among the different alternatives explored. Automating the training and classification process using these devices, Various dimensionality reduction techniques are applied to the collected data, and the resulting feature vectors are used to train a variety of common machine learning classifiers. A case study is presented, where magnetic field measurements of eight appliances are analyzed to determine the viability of using these signals alone to determine the type of appliance that the EMF sensor has been placed next to.
2K_test_584	High-data-rate sensors such as video cameras, are becoming ubiquitous in the Internet of Things, This article is part of a special issue on smart spaces. An Internet-scale repository of crowd-sourced video content. This article describes GigaSight, with strong enforcement of privacy preferences and access controls The GigaSight architecture is a federated system of VM-based cloudlets that perform video analytics at the edge of the Internet, thus reducing the demand for ingress bandwidth into the cloud, Denaturing which is an owner-specific reduction in fidelity of video content to preserve privacy, is one form of analytics on cloudlets, Content-based indexing for search is another form of cloudlet-based analytics.
2K_test_586	Many vision tasks require a multi-class classifier to discriminate multiple categories, on the order of hundreds or thousands. A principled way for large-scale multi-class classification. Empirical results demonstrate the effectiveness of our proposed approach. In this paper we propose sparse output coding, by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem, Specifically sparse output coding is composed of two steps : efficient coding matrix learning with scalability to thousands of classes. On object recognition and scene classification.
2K_test_587	Computers have the potential to significantly extend the practice of popular music based on steady tempo and mostly determined form. There are significant challenges to overcome, however due to constraints including accurate timing based on beats and adherence to a form or structure despite possible changes that may occur, possibly even during performance, takes into account latency due to communication delays and audio buffering, the problem of mapping from a conventional score. We describe an approach to synchronization across media that We also address with repeats and other structures to an actual performance, which can involve both flattening the score and rearranging it, as is common in popular music, Finally we illustrate the possibilities of the score as a bidirectional user interface in a real-time system for music performance, allowing the user to direct the computer through a digitally displayed score, and allowing the computer to indicate score position back to human performers.
2K_test_588	Noisy high-dimensional time series observations can often be described by a set of low-dimensional latent variables, Commonly used methods to extract these latent variables typically assume instantaneous relationships between the latent and observed variables, In many physical systems, changes in the latent variables manifest as changes in the observed variables after time delays. Techniques that do not account for these delays can recover a larger number of latent variables than are present in the system, thereby making the latent representation more difficult to interpret that performs dimensionality reduction. Demonstrate that when time delays are present, TD-GPFA is able to correctly identify these delays and recover the latent space, TD-GPFA is able to better describe the neural activity using a more parsimonious latent space than GPFA, a method that has been used to interpret motor cortex data but does not account for time delays. In this work we introduce a novel probabilistic technique, time-delay gaussian-process factor analysis TD-GPFA in the presence of a different time delay between each pair of latent and observed variables We demonstrate how using a gaussian process to model the evolution of each latent variable allows us to tractably learn these delays over a continuous domain, Additionally we show how TD-GPFA combines temporal smoothing and dimensionality reduction into a common probabilistic framework, We present an expectation/conditional maximization either ECME algorithm to learn the model parameters More broadly, TD-GPFA can help to unravel the mechanisms underlying high-dimensional time series data by taking into account physical delays in the system. Our simulations We then applied TD-GPFA to the activity of tens of neurons recorded simultaneously in the macaque motor cortex during a reaching task.
2K_test_589	There has been significant interest and progress recently in algorithms that solve regression problems involving tall and thin matrices in input sparsity time, Our results build upon the close connection between randomized matrix algorithms, iterative methods and graph sparsification. Given a n * d matrix where n g d, these algorithms find an approximation with fewer rows, allowing one to solve a poly ( d ) sized problem instead, In practice the best performances are often obtained by invoking these routines in an iterative fashion to give theoretical guarantees comparable to and better than the current state of the art. We show these iterative methods can be adapted Our approaches are based on computing the importances of the rows, known as leverage scores, in an iterative manner We show that alternating between computing a short matrix estimate and finding more accurate approximate leverage scores leads to a series of geometrically smaller instances, This gives an algorithm whose runtime is input sparsity plus an overhead comparable to the cost of solving a regression problem on the smaller approximation.
2K_test_590	Abstract Genes are often combinatorially regulated by multiple transcription factors ( TFs ), Such combinatorial regulation plays an important role in development and facilitates the ability of cells to respond to different stresses. While a number of approaches have utilized sequence and ChIP-based datasets to study combinational regulation, these have often ignored the combinational logic and the dynamics associated with such regulation, for reconstructing dynamic models of combinatorial regulation. We show that the predicted combinatorial sets agree with other high throughput genomic datasets and improve upon prior methods. Here we present cDREM, a new method cDREM integrates time series gene expression data with ( static ) protein interaction data, The method is based on a hidden Markov model and utilizes the sparse group Lasso to identify small subsets of combinatorially active TFs, their time of activation, and the logical function they implement. We tested cDREM on yeast and human data sets Using yeast.
2K_test_591	This technical note studies the impact of side initial state information on the detectability of data deception attacks against cyber-physical systems, that detects detectable attacks. Finally we design a dynamic attack detector. We assume the attack detector has access to a linear function of the initial system state that can not be altered by an attacker, First we provide a necessary and sufficient condition for an attack to be undetectable by any dynamic attack detector under each specific side information pattern, Second we characterize attacks that can be sustained for arbitrarily long periods without being detected, Third we define the zero state inducing attack, the only type of attack that remains dynamically undetectable regardless of the side initial state information available to the attack detector.
2K_test_593	Several researchers proposed using non-Euclidean metrics on point sets in Euclidean space for clustering noisy data, Almost always a distance function is desired that recognizes the closeness of the points in the same cluster, even if the Euclidean cluster diameter is large. There- fore it is preferred to assign smaller costs to the paths that stay close to the input points, In this paper we consider a natural metric with this property. Which we call the nearest neighbor metric, Given a point set P and a path, t his metric is the integral of the distance to P along, W e describe a ( 3 +e ) - approximation algorithm and a more intricate ( 1 + e ) -approximation algorithm to compute the nearest neighbor metric Both approximation algorithms work in near-linear time, The former uses shortest paths on a sparse graph defined over the input points, The latter uses a sparse sample of the ambient space, to find good approximate geodesic paths.
2K_test_594	On input of an $ n $ -vertex $ m $ -edge weighted graph $ G $ and a value $ k $ produces an incremental sparsifier $ \hat { G } $ with $ n-1 + m/k $ edges, such that the relative condition number of $ G $ with $ \hat { G } $ is bounded above by $ \tilde { O } ( k\log^2 n ) $, with probability $ 1-p $ ( we use the $ \tilde { O } ( ) $ notation to hide a factor of at most $ ( \log\log n ) ^4 $ ). We present an algorithm that The algorithm runs in time $ \tilde { O } ( ( m \log { n } + n\log^2 { n } ) \log ( 1/p ) ), $ As a result, we obtain an algorithm that on input of an $ n\times n $ symmetric diagonally dominant matrix $ A $ with $ m $ nonzero entries and a vector $ b $ computes a vector $ { x } $ satisfying $ || { x } -A^ { + } b||_A < \epsilon ||A^ { + } b||_A $, in expected time $ \tilde { O } ( m\log^2 { n } \log ( 1/\epsilon ) ), $ The solver is based on repeated applications of the incremental sparsifier that produces a chain of graphs which is then used as input to the recursive preconditioned Chebyshev iteration.
2K_test_595	Linear subspace learning methods such as Fisher 's Linear Discriminant Analysis ( LDA ), Unsupervised Discriminant Projection ( UDP ), and Locality Preserving Projections ( LPP ) have been widely used in face recognition applications as a tool to capture low dimensional discriminant information However, when these methods are applied in the context of face recognition, they often encounter the small-sample-size problem, In order to overcome this problem, a separate Principal Component Analysis ( PCA ) step is usually adopted to reduce the dimensionality of the data. However such a step may discard dimensions that contain important discriminative information that can aid classification performance for maximizing class separation criteria in LDA. That our proposed FKDA significantly outperforms traditional linear discriminant subspace learning methods as well as five other competing algorithms. In this work we propose a new idea which we named Multi-class Fukunaga Koontz Discriminant Analysis ( FKDA ) by incorporating the Fukunaga Koontz Transform within the optimization In contrast to traditional LDA, UDP and LPP our approach can work with very high dimensional data as input, without requiring a separate dimensionality reduction step to make the scatter matrices full rank In addition, the FKDA formulation seeks optimal projection direction vectors that are orthogonal which the existing methods can not guarantee, and it has the capability of finding the exact solutions to the `` trace ratio '' objective in discriminant analysis problems while traditional methods can only deal with a relaxed and inexact `` ratio trace '' objective, HighlightsSolve small-sample-size problem in LDA, UDP LPP using FKT formulation, Can work with high dimensional data without inverting any scatter matrices, Finds optimal projection direction vectors that are orthogonal, Finds exact solutions to the objective in the form of trace ratio, Improvement in unconstrained face recognition scenarios. We have shown using six face database, in the context of large scale unconstrained face recognition, face recognition with occlusions, and illumination invariant face recognition, under `` closed set '', `` semi-open set '', and `` open set '' recognition scenarios.
2K_test_596	Online consumers are uncertain about subjective product quality e, fit and feel of clothing and texture of materials because of the absence of experiential information, This implies that online consumers are reluctant to buy expensive products with only digitally transferred information, whereas they tend to purchase more of the cheaper products online along with their accumulated online shopping experience, Our study on the dynamics in the set of products purchased online expands the understanding of consumer purchase behavior under uncertainty. In this paper we examine the dynamic change of the products purchased online over time in the presence of this type of uncertainty. We find that consumers purchase products with a high degree of product uncertainty as their online shopping experiences help them better estimate product quality, Our results also show that the average and highest prices of market baskets decrease around 1 % when online shopping experience increases 10 %, When online consumers buy products priced under $ 50, they readily buy products with a high degree of product uncertainty regardless of their online shopping experience, But consumers are unlikely to buy expensive products online if there is a high degree of product uncertainty, even when they have accumulated much online shopping experience, In addition we find that online vendors can effectively overcome product-level uncertainty by taking advantage of retailer reputation in the physical world and through the use of digitized video commercials This paper was accepted by Lorin Hitt. Using individual-level transaction data We also verify the interaction effects of product uncertainty and product price on online consumers ' purchase decision.
2K_test_597	For performing hybrid symbolic execution to detect exploitable bugs in binary code. The systems and methods determine that resources associated with an execution client performing symbolic execution of a target program are below, at or above a threshold performance level, generate checkpoints for active executing paths of the online symbolic execution, and cause the execution client to perform symbolic execution in response to the determination that the resources are at or above the threshold performance level. Systems and methods are described. In some example embodiments.
2K_test_598	For continuous collection of crowd-sourced video. Reveal the bottlenecks for video upload, denaturing indexing and content-based search, They also provide insight on how parameters such as frame rate and resolution impact scalability. We propose a scalable Internet system from devices such as Google Glass, Our hybrid cloud architecture, GigaSight is effectively a Content Delivery Network ( CDN ) in reverse It achieves scalability by decentralizing the collection infrastructure using cloudlets based on virtual machines~ ( VMs ), Based on time location, and content privacy sensitive information is automatically removed from the video, This process which we refer to as denaturing, is executed in a user-specific VM on the cloudlet Users can perform content-based searches on the total catalog of denatured videos.
2K_test_599	Self-paced learning ( SPL ) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning : diversity, To incorporate this information. We demonstrate that our method significantly outperforms the conventional SPL Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets. We propose an approach called self-paced learning with diversity ( SPLD ) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks, Albeit non-convex the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. On three real-world datasets.
2K_test_600	Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns ? to quantify both concepts ( `` synchronicity '' and `` normality '' ). CatchSync consistently outperforms existing competitors, both in detection accuracy by 36 % on Twitter and 20 % on Tencent Weibo, as well as in speed. We propose a fast and effective method, CatchSync which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior pattern, because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority, We introduce novel measures and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots Thanks to careful design, CatchSync has the following desirable properties : ( a ) it is scalable to large datasets, being linear on the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology, without needing labeled data, nor timing information etc, while still capable of using side information. We applied CatchSync on two large, real datasets 1-billion-edge Twitter social graph and 3-billion-edge Tencent Weibo social graph, and several synthetic ones.
2K_test_601	Regularization has played a key role in deriving sensible estimators in high dimensional statistical inference, A substantial amount of recent works has argued for nonconvex regularizers in favor of their superior theoretical properties and excellent practical performances, In a dierent but analogous vein, nonconvex loss functions are promoted because of their robustness against \outliers ''. However these nonconvex formulations are computationally more challenging, especially in the presence of nonsmoothness and nonseparability, To address this issue. Prove its nice convergence properties, and illustrate its eectiveness demonstrate that our method compares favorably against other alternatives. We propose a new proximal gradient meta-algorithm by rigorously extending the proximal average to the nonconvex setting. We formally on two applications : multi-task graph-guided fused lasso and robust support vector machines Experiments.
2K_test_602	We investigate the power of voting among diverse, randomized software agents allows us to reason about a collection of agents with different biases ( determined by the first-stage noise models ). That a uniform team, consisting of multiple instances of any single agent, must make a significant number of mistakes, whereas a diverse team converges to perfection as the number of agents grows provide evidence for the effectiveness of voting when agents are diverse. With teams of computer Go agents in mind, we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning, This model which furthermore, apply randomized algorithms to evaluate alternatives and produce votes ( captured by the second-stage noise models ). We analytically demonstrate Our experiments, which pit teams of computer Go agents against strong agents.
2K_test_603	Navigation models are explicit representations of geometrical and topological information of physical environments that can be utilized for map-matching of indoor positioning data. For automated generation of three different types of navigation models, namely centerline-based network metric-based and grid-based navigation models, for map-matching of indoor positioning data, to extract 2D geometry and topology from IFC-based BIM to generate grid-based navigation models. This research paper presents algorithms The abovementioned navigation models have been generated in an automated fashion from Industry Foundation Classes ( IFC ) -based building information models ( BIM ), Specifically we have 1 ) built on and targeted addressing limitations of existing algorithms that generate centerline-based network navigation models for polygonal shapes, 2 ) developed an approach for creating metric-based navigation models, and 3 ) modified an existing algorithm using geometry and topology extracted from BIM. The abovementioned three types of navigation models have been generated for six different testbeds with varying shape, size and density of spaces We have validated the generality of the developed algorithms by evaluating the accuracy of geometrical and topological information contained within the three types of navigation models generated from testbeds with varying spatial characteristics.
2K_test_606	Motivation : Synaptic connections underlie learning and memory in the brain and are dynamically formed and eliminated during development and in response to stimuli, Quantifying changes in overall density and strength of synapses is an important pre-requisite for studying connectivity and plasticity in these cases or in diseased conditions. Unfortunately most techniques to detect such changes are either low-throughput ( e, electrophysiology ) prone to error and difficult to automate ( e, standard electron microscopy ) or too coarse ( e, magnetic resonance imaging ) to provide accurate and large-scale measurements, : To facilitate high-throughput analyses, to automatically detect synapses in these images, to overcome sample heterogeneity and improve performance. Results We detected thousands of synapses in these images and Our algorithms are highly efficient and scalable and are freely available for others to use, Availability : Code is available at http : //www. And we developed a machine-learning framework We also used a semi-supervised algorithm that leverages unlabeled data. We used a 50-year-old experimental technique to selectively stain for synapses in electron microscopy images, To validate our method, we experimentally imaged brain tissue of the somatosensory cortex in six mice, demonstrate the accuracy of our approach using crossvalidation with manually labeled data and by comparing against existing algorithms and against tools that process standard electron microscopy images.
2K_test_607	For fast distributed learning on big ( i, high-dimensional ) models applied to big datasets. Which demonstrate that our method scales well with large data and model sizes, while beating learning strategies that fail to take both data and model partitioning into account. We present a scheme Unlike algorithms that focus on distributed learning in either the big data or big model setting ( but not both ), our scheme partitions both the data and model variables simultaneously This not only leads to faster learning on distributed clusters, but also enables machine learning applications where both data and model are too large to fit within the memory of a single machine, Furthermore our scheme allows worker machines to perform additional updates while waiting for slow workers to finish, which provides users with a tunable synchronization strategy that can be set based on learning needs and cluster conditions. We prove the correctness of such strategies, as well as provide bounds on the variance of the model variables under our scheme, Finally we present empirical results for latent space models such as topic models.
2K_test_608	The integration of synthetic and cell-free biology has made tremendous strides towards creating artificial cellular nanosystems using concepts from solution-based chemistry : only the concentrations of reacting species modulate gene expression rates However, it is known that macromolecular crowding, a key feature of natural cells, can dramatically influence biochemical kinetics by volume exclusion effects that reduce diffusion rates and enhance binding rates of macromolecules, our work has implications for efficient and robust control of both synthetic and natural cellular circuits. Here we demonstrate that macromolecular crowding can increase the robustness of gene expression In addition, we reveal how ubiquitous cellular modules, including genetic components a negative feedback loop, and the size of crowding molecules, can fine tune gene circuit response to molecular crowding, By bridging a key gap between artificial and living cells. Through integrating synthetic cellular components of biological circuits and artificial cellular nanosystems.
2K_test_609	Phylogenetic algorithms have begun to see widespread use in cancer research to reconstruct processes of evolution in tumor progression, Developing reliable phylogenies for tumor data requires quantitative models of cancer evolution that include the unusual genetic mechanisms by which tumors evolve, such as chromosome abnormalities, and allow for heterogeneity between tumor types and individual patients. Motivation Previous work on inferring phylogenies of single tumors by copy number evolution assumed models of uniform rates of genomic gain and loss across different genomic sites and scales, a substantial oversimplification necessitated by a lack of algorithms and quantitative parameters for fitting to more realistic tumor evolution models, for inferring models of tumor progression for identification of most parsimonious combinations of single gene and single chromosome events to estimate mutation-specific and tumor-specific event rates concurrently with tree reconstruction. Results identifies key genomic events in disease progression consistent with prior literature, lead to improved prediction accuracy for the metastasis of primary cervical cancers and for tongue cancer survival, Availability and implementation : Our software ( FISHtrees ) and two datasets are available at ftp : //ftp. We propose a framework from single-cell gene copy number data, including variable rates for different gain and loss events We propose a new algorithm We extend it via dynamic programming to include genome duplications, We implement an expectation maximization ( EM ) -like method. Application of our algorithms to real cervical cancer data Classification experiments on cervical and tongue cancer datasets.
2K_test_610	Studying temporal dynamics of topics in social media is very useful to understand online user behaviors, Most of the existing work on this subject usually monitors the global trends, ignoring variation among communities Since users from different communities tend to have varying tastes and interests, capturing communitylevel temporal change can improve the understanding and management of social content, Additionally it can further facilitate the applications such as community discovery, temporal prediction and online marketing. However this kind of extraction becomes challenging due to the intricate interactions between community and topic, and intractable computational complexity. And demonstrate the superiority of proposed model on tasks of time stamp prediction, link prediction and topic perplexity. In this paper we take a unified solution towards the communitylevel topic dynamic extraction, A probabilistic model CosTot ( Community Specific Topics-over-Time ) is proposed to uncover the hidden topics and communities, as well as capture community-specific temporal dynamics, Specifically CosTot considers text, time and network information simultaneously, and well discovers the interactions between community and topic over time We then discuss the approximate inference implementation to enable scalable computation of model parameters, especially for large social data. Based on this the application layer support for multi-scale temporal analysis and community exploration is also investigated, We conduct extensive experimental studies on a large real microblog dataset.
2K_test_611	The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator ( Lasso ) allows computationally efficient feature selection based on linear dependency between input features and output values, for capturing non-linear input-output dependency. In this paper we consider a feature-wise kernelized Lasso We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures such as the Hilbert-Schmidt independence criterion ( HSIC ) We then show that the globally optimal solution can be efficiently computed ; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments for classification and regression with thousands of features.
2K_test_612	We also discuss potential future research directions in this area. As our society is increasingly aging, it is urgent to develop computer aided techniques to improve the quality-of-care ( QoC ) and quality-of-life ( QoL ) of geriatric patients. Demonstrate the proposed approach is effective for human activity analysis. In this paper we focus on automatic human activities analysis in video surveillance recorded in complicated environments at a nursing home, This will enable the automatic exploration of the statistical patterns between patients ' daily activities and their clinical diagnosis.
2K_test_613	That is capable of a wide range of autonomous and intelligent behaviors. We present an autonomous driving research vehicle with minimal appearance modifications including smooth and comfortable trajectory generation and following ; lane keeping and lane changing ; intersection handling with or without V2I and V2V ; and pedestrian, bicyclist and workzone detection, Safety and reliability features include a fault-tolerant computing system ; smooth and intuitive autonomous-manual switching ; and the ability to fully disengage and power down the drive-by-wire and computing system upon E-stop. The vehicle has been tested extensively on both a closed test field and public roads.
2K_test_614	Game-theoretic algorithms for physical security have made an impressive real-world impact These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game, where the attacker observes the defender 's strategy and best-responds. In order to build the game model, though the payoffs of potential attackers for various outcomes must be estimated ; inaccurate estimates can lead to significant inefficiencies that optimizes the defender 's strategy with no prior information. We design an algorithm, by observing the attacker 's responses to randomized deployments of resources and learning his priorities In contrast to previous work, our algorithm requires a number of queries that is polynomial in the representation of the game.
2K_test_615	When integrating sensors and cloud computing. The paper explains how to use sensors as the eyes, ears hands and feet for the cloud This paper describes the opportunities and challenges.
2K_test_616	Tablet computers are often called upon to emulate classical pen-and-paper input. However touchscreens typically lack the means to distinguish between legitimate stylus and finger touches and touches with the palm or other parts of the hand, This forces users to rest their palms elsewhere or hover above the screen, resulting in ergonomic and usability problems. Our system improves upon previous approaches, reducing accidental palm inputs to 0, 016 per pen stroke, while correctly passing 98 % of stylus inputs. We present a probabilistic touch filtering approach that uses the temporal evolution of touch contacts.
2K_test_617	Most algorithmic matches in fielded kidney exchanges do not result in an actual transplant. In this paper we address the problem of cycles and chains in a proposed match failing after the matching algorithm has committed to them, for the probabilistic exchange clearing problem. We show that failure-aware kidney exchange can significantly increase the expected number of lives saved and show that this new solver scales well. From the computational viewpoint, we design a branch-and-price-based optimal clearing algorithm specifically. ( i ) in theory, on random graph models ; ( ii ) on real data from kidney exchange match runs between 2010 and 2012 ; ( iii ) on synthetic data generated via a model of dynamic kidney exchange on large simulated data, unlike prior clearing algorithms.
2K_test_618	Cloud offload is an important technique in mobile computing, VM-based cloudlets have been proposed as offload sites for the resource-intensive and latency-sensitive computations typically associated with mobile multimedia applications. Since cloud offload relies on precisely-configured back-end software, it is difficult to support at global scale across cloudlets in multiple domains To address this problem. We demonstrate a prototype system that is capable of provisioning a cloudlet with a non-trivial VM image in 10 seconds. We describe just-in-time ( JIT ) provisioning of cloudlets under the control of an associated mobile device This speed is achieved through dynamic VM synthesis and a series of optimizations to aggressively reduce transfer costs and startup latency. Using a suite of five representative mobile applications.
2K_test_619	The average person can skillfully manipulate a plethora of tools, from hammers to tweezers. However despite this remarkable dexterity, gestures on today 's touch devices are simplistic, relying primarily on the chording of fingers : one -finger pan, two -finger pinch four -finger swipe and similar, to build a rich set of gestures for touch interaction. Users were able to summon a variety of virtual tools by replicating their corresponding real-world grasps. We propose that touch gesture design be inspired by the manipulation of physical tools from the real world, In this way we can leverage user familiarity and fluency with such tools. With only a few minutes of training on a proof-of-concept system.
2K_test_620	Location sharing is a popular feature of online social networks, but challenges remain in the effective presentation of privacy choices to users, whose location sharing preferences are complex and diverse, One proposed approach for capturing these nuances builds on the observation that key attributes of users ' location sharing preferences can be represented by a small number of privacy profiles, which can provide a basis for configuring individual preferences, This further suggests that the provision of profiles for privacy settings must be carefully considered, as they can substantially alter sharing behavior. However the impact of this approach on how users view their privacy is relatively unknown evaluating the impact of this approach on users ' location sharing preferences and their satisfaction with the decisions made by their resulting settings. The results suggest that this approach can influence users to share significantly more without a substantial difference in comfort. We present a study.
2K_test_621	The proliferation of touchscreen devices has made soft keyboards a routine part of life. However ultra-small computing platforms like the Sony SmartWatch and Apple iPod Nano lack a means of text entry, This limits their potential, despite the fact they are quite capable computers, that enables text entry on ultra-small devices. After eight practice trials, users achieved an average of 9, 3 words per minute, with accuracy comparable to a full-sized physical keyboard This compares favorably to existing mobile text input methods. In this work we present a soft keyboard interaction technique called ZoomBoard Our approach uses iterative zooming to enlarge otherwise impossibly tiny keys to comfortable size, We based our design on a QWERTY layout, so that it is immediately familiar to users and leverages existing skill. As the ultimate test, we ran a text entry experiment on a keyboard measuring just 16 x 6mm - smaller than a US penny.
2K_test_622	Meeting service level objectives ( SLOs ) for tail latency is an important and challenging open problem in cloud computing infrastructures, The challenges are exacerbated by burstiness in the workloads to provide tail latency QoS for shared networked storage. PriorityMeister outperforms most recent reactive request scheduling approaches, with more workloads satisfying latency SLOs at higher latency percentiles. This paper describes PriorityMeister -- a system that employs a combination of per-workload priorities and rate limits, even with bursty workloads, PriorityMeister automatically and proactively configures workload priorities and rate limits across multiple stages ( e, a shared storage stage followed by a shared network stage ) to meet end-to-end tail latency SLOs PriorityMeister is also robust to mis-estimation of underlying storage device performance and contains the effect of misbehaving workloads. In real system experiments and under production trace workloads.
2K_test_623	With an explosion of popularity of online photo sharing, we can trivially collect a huge number of photo streams for any interesting topics such as scuba diving as an outdoor recreational activity class. Obviously the retrieved photo streams are neither aligned nor calibrated since they are taken in different temporal, spatial and personal perspectives, However at the same time, they are likely to share common storylines that consist of sequences of events and activities frequently recurred within the topic, as a first technical step to detect such collective storylines, to jointly aligning and segmenting uncalibrated multiple photo streams. Our empirical results show that the proposed algorithms are more successful than other candidate methods for both tasks. In this paper we propose an approach The alignment task discovers the matched images between different photo streams, and the image segmentation task parses each image into multiple meaningful regions to facilitate the image understanding, We close a loop between the two tasks so that solving one task helps enhance the performance of the other in a mutually rewarding way, To this end we design a scalable message-passing based optimization framework to jointly achieve both tasks for the whole input image set at. With evaluation on the new Flickr dataset of 15 outdoor activities that consist of 1, 5 millions of images of 13 thousands of photo streams.
2K_test_624	Regret-based methods have largely been favored in practice, in spite of their theoretically inferior convergence rates. We study the problem of computing a Nash equilibrium in large-scale two-player zero-sum extensive-form games While this problem can be solved in polynomial time, first-order or regret-based methods are usually preferred for large games. We find that mirror prox and the excessive gap technique outperform the prior regret-based methods for finding medium accuracy solutions. In this paper we investigate the acceleration of first-order methods both theoretically and experimentally An important component of many first-order methods is a distance-generating function, Motivated by this we investigate a specific distance-generating function, namely the dilated entropy function, over treeplexes which are convex polytopes that encompass the strategy spaces of perfect-recall extensive-form games, We develop significantly stronger bounds on the associated strong convexity parameter, In terms of extensive-form game solving, this improves the convergence rate of several first-order methods by a factor of O ( ( # information sets depth M ) / ( 2 depth ) ) where M is the maximum value of the l 1 norm over the treeplex encoding the strategy spaces, In order to instantiate stochastic mirror prox, we develop a class of gradient sampling schemes for game trees, Equipped with our distance-generating function and sampling scheme. Experimentally we investigate the performance of three first-order methods ( the excessive gap technique, mirror prox and stochastic mirror prox ) and compare their performance to the regret-based algorithms.
2K_test_625	How does a new startup drive the popularity of competing websites into oblivion like Facebook famously did to MySpace ? This question is of great interest to academics, technologists and financial investors alike. In this work we exploit the singular way in which Facebook wiped out the popularity of MySpace, Hi5 Friendster and Multiply to guide the design of a new popularity competition model. The resulting model not only accurately fits the observed Daily Active Users ( DAU ) of Facebook and its competitors but also predicts their fate four years into the future. Our model provides new insights into what Nobel Laure- ate Herbert A, Simon called the `` marketplace of attention, '' which we recast as the attention-activity marketplace. Our model design is further substantiated by user-level activity of 250, 000 MySpace users obtained between 2004 and 2009.
2K_test_626	For disease outbreak detection. We present a new method the `` Non-Parametric Heterogeneous Graph Scan ( NPHGS ) '', NPHGS enables fast and accurate detection of emerging space-time clusters using Twitter and other social media streams where standard parametric model assumptions are incorrect.
2K_test_627	Distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness. A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms, where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates, We argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment and theoretical analysis. In this paper we develop a system for model-parallelism, STRADS that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs, STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ML. We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling, matrix factorization and Lasso.
2K_test_628	Computational cancer phylogenetics seeks to enumerate the temporal sequences of aberrations in tumor evolution, thereby delineating the evolution of possible tumor progression pathways, molecular subtypes and mechanisms of action, We previously developed a pipeline for constructing phylogenies describing evolution between major recurring cell types computationally inferred from whole-genome tumor profiles. The accuracy and detail of the phylogenies, however depend on the identification of accurate, high-resolution molecular markers of progression, reproducible regions of aberration that robustly differentiate different subtypes and stages of progression, for the problem of inferring such phylogenetically significant markers. Which confirms its effectiveness for tumor phylogeny inference and suggests avenues for future advances. Here we present a novel hidden Markov model ( HMM ) scheme through joint segmentation and calling of multisample tumor data Our method classifies sets of genome-wide DNA copy number measurements into a partitioning of samples into normal ( diploid ) or amplified at each probe, It differs from other similar HMM methods in its design specifically for the needs of tumor phylogenetics, by seeking to identify robust markers of progression conserved across a set of copy number profiles. We show an analysis of our method in comparison to other methods on both synthetic and real tumor data.
2K_test_630	Offering a unified input modality with expressiveness greater than each input modality alone. We present Air+Touch a new class of interactions that interweave touch events with in-air gestures, We demonstrate how air and touch are highly complementary : touch is used to designate targets and segment in-air gestures, while in-air gestures add expressivity to touch events, For example a user can draw a circle in the air and tap to trigger a context menu, do a finger 'high jump ' between two touches to select a region of text, or drag and in-air 'pigtail ' to copy text to the clipboard we devised a basic taxonomy of Air+Touch interactions, based on whether the in-air component occurs before, between or after touches. Through an observational study, To illustrate the potential of our approach, we built four applications that showcase seven exemplar Air+Touch interactions we created.
2K_test_631	Recent trends in System-on-a-Chip show that an increasing number of special-purpose processors are being added to improve the efficiency of common operations, Unfortunately the use of these processors may introduce suspension delays incurred by communication, synchronization and external I/O operations When these processors are used in real-time systems, conventional schedulability analyses incorporate these delays in the worst-case execution/response time, hence significantly reducing the schedulable utilization. In this paper we provide schedulability analyses for self-suspending tasks. While RMS is shown to not be optimal, it can be used effectively in some special cases that we have identified, show that the proposed scheme provides up to 40 times more schedulable utilization than RMS. And propose segment-fixed priority scheduling We model the tasks as segments of execution separated by suspensions, We then derive a utilization bound for the cases as a function of the ratio of the suspension duration to the period of the tasks, For general cases we develop a segment-fixed priority scheduling scheme Our scheme assigns individual segments different priorities and phase offsets that are used for phase enforcement to control the unexpected self-suspending nature. We start from providing response-time analyses for self-suspending tasks under Rate Monotonic Scheduling ( RMS ) With the exact schedulability analysis designed for our scheme.
2K_test_632	Cloud-sourced virtual appliances ( VAs ) have been touted as powerful solutions for many software maintenance, mobility backward compatibility and security challenges. In this paper we ask whether it is possible that supports fluid, interactive user experience even over mobile networks. Supports fluid interaction even in challenging network conditions, such as 4G LTE. To create a VA cloud service More specifically, we wish to support a YouTube-like streaming service for executable content, such as games interactive books, Users should be able to post, browse through and interact with executable content swiftly and without long interruptions Intuitively, this seems impossible ; the bandwidths, latencies and costs of last-mile networks would be prohibitive given the sheer sizes of virtual machines ! Yet, we show that a set of carefully crafted, novel prefetching and streaming techniques can bring this goal surprisingly close to reality. We show that vTube, a VA streaming system that incorporates our techniques.
2K_test_633	To learn a high-dimensional conditional distribution of outputs given inputs. Improving upon past bounds with convergence rates that depend logarithmically on the data dimension, and demonstrating state-of-the-art performance on two real-world tasks. This paper considers the sparse Gaussian conditional random field, a discriminative extension of sparse inverse covariance estimation, where we use convex methods The model has been proposed by multiple researchers within the past year, yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems In this paper, we make three contributions : 1 ) we develop a second-order active-set method which is several orders of magnitude faster than previously proposed optimization approaches for this problem. 2 ) we analyze the model from a theoretical standpoint, 3 ) we apply the method to large-scale energy forecasting problems.
2K_test_634	Previously it has been shown that, under some conditions on the distribution of votes, if the number of manipulators is o ( n ), where n is the number of voters, then the probability that a random profile is manipulable by the coalition goes to zero as the number of voters goes to infinity, whereas if the number of manipulators is ( n ), then the probability that a random profile is manipulable goes to one This result analytically validates recent empirical results, and suggests that deciding the coalitional manipulation problem may be of limited computational hardness in practice. We study the phase transition of the coalitional manipulation problem for generalized scoring rules. And we show that as c goes from zero to infinity, the limiting probability that a random profile is manipulable goes from zero to one in a smooth fashion, there is a smooth phase transition between the two regimes. Here we consider the critical window, where a coalition has size cn.
2K_test_635	Real-time captioning provides people who are deaf or hard of hearing access to speech in settings such as classrooms and live events. The most reliable approach to provide these captions is to recruit an expert stenographer who is able to type at natural speaking rates, but they charge more than $ 100 USD per hour and must be scheduled in advance, to jointly caption speech in real-time. We have shown that the accuracy of Scribe captions approaches that of a professional stenographer, while its latency and cost is dramatically lower. We introduce Legion Scribe ( Scribe ), a system that allows 3-5 ordinary people who can hear and type Each person is unable to type at natural speaking rates, and so is asked only to type part of what they hear Scribe automatically stitches all of the partial captions together to form a complete caption stream.
2K_test_636	Distributed online groups have great potential for generating interdependent and complex products like encyclopedia articles or product design, These results have implications for small group theory and crowdsourcing research. However coordinating multiple group members to work together effectively while minimizing process losses remains an open challenge. Our results indicate that, contrary to prior work, a sequential work structure was more effective than a simultaneous work structure as the size of the group increased, suggests that social processes such as territoriality partially accounts for these results, mitigated the detrimental effects of the simultaneous work structure. We conducted an experiment comparing the effectiveness of two coordination strategies ( simultaneous vs, sequential work ) on a complex creative task as the number of group members increased, A mediation analysis A follow up experiment giving workers specific roles.
2K_test_637	In a multimillion-node network of who-follows-whom like Twitter, since a high count of followers leads to higher profits, users have the incentive to boost their in-degree. Can we spot the suspicious following behavior, which may indicate zombie followers and suspicious followees ? To answer the above question. Moreover we show it is effective. We propose CatchSync which exploits two tell-tale signs of the suspicious behavior : ( a ) synchronized behavior : the zombie followers have extremely similar following behavior pattern, because say they are generated by a script ; and ( b ) abnormal behavior : their behavior pattern is very different from the majority, Our CatchSync introduces novel measures to quantify both concepts and catches the suspicious behavior. In a real-world social network.
2K_test_638	Contemporary parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized operators relying on graphical representations of ML programs. How can one build a distributed framework that allows efficient deployment of a wide spectrum of modern advanced machine learning ( ML ) programs for industrial-scale problems using Big Models ( 100s of billions of parameters ) on Big Data ( terabytes or petabytes ) - The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of different ML programs at scale. We demonstrate how such a design in light of ML-first principles leads to significant performance improvements versus well-known implementations of several ML programs, allowing them to run in much less time and at considerably larger model sizes. We propose a general-purpose framework that systematically addresses data- and model-parallel challenges in large-scale ML, by leveraging several fundamental properties underlying ML programs that make them different from conventional operation-centric programs : error tolerance, dynamic structure and nonuniform convergence ; all stem from the optimization-centric nature shared in ML programs ' mathematical definitions, and the iterative-convergent behavior of their algorithmic solutions, These properties present unique opportunities for an integrative system design, built on bounded-latency network synchronization and dynamic load-balancing scheduling, which is efficient programmable, and enjoys provable correctness guarantees. On modestly-sized computer clusters.
2K_test_639	In this paper we address the problem of discovering topically meaningful, yet compact ( densely connected ) communities in a social network. Our method outperforms other well known baselines Finally, we also provide a fast, parallel approximation of the same. Assuming the social network to be an integer-weighted graph ( where the weights can be intuitively defined as the number of common friends, followers documents exchanged etc, ) we transform the social network to a more efficient representation, In this new representation, each user is a bag of her one-hop neighbors, We propose a mixed-membership model to identify compact communities using this transformation, Next we augment the representation and the model to incorporate user-content information imposing topical consistency in the communities, In our model a user can belong to multiple communities and a community can participate in multiple topics This allows us to discover community memberships as well as community and user interests. On two real-world social networks.
2K_test_640	Recently data with complex characteristics such as epilepsy electroencephalography ( EEG ) time series has emerged, Epilepsy EEG data has special characteristics including nonlinearity. Therefore it is important to find a suitable forecasting method that covers these special characteristics. Results show that when compared to previous methods, the proposed method can forecast faster and accurately. In this paper we propose a coercively adjusted autoregression ( CA-AR ) method that forecasts future values from a multivariable epilepsy EEG time series, We use the technique of random coefficients, which forcefully adjusts the coefficients with 1 and 1, The fractal dimension is used to determine the order of the CA-AR model, We applied the CA-AR method reflecting special characteristics of data to forecast the future value of epilepsy EEG data.
2K_test_641	With the continuous improvement in genotyping and molecular phenotyping technology and the decreasing typing cost, it is expected that in a few years, more and more clinical studies of complex diseases will recruit thousands of individuals for pan-omic genetic association analyses. Hence there is a great need for algorithms and software tools that could scale up to the whole omic level, integrate different omic data, leverage rich structure information, and be easily accessible to non-technical users. And report some interesting findings. We present GenAMap an interactive analytics software platform that 1 ) automates the execution of principled machine learning methods that detect genome- and phenome-wide associations among genotypes, gene expression data and clinical or other macroscopic traits, and 2 ) provides new visualization tools specifically designed to aid in the exploration of association mapping results Algorithmically, GenAMap is based on a new paradigm for GWAS and PheWAS analysis, termed structured association mapping, which leverages various structures in the omic data GenAMap is available from http : //sailing. We demonstrate the function of GenAMap via a case study of the Brem and Kruglyak yeast dataset, and then apply it on a comprehensive eQTL analysis of the NIH heterogeneous stock mice dataset.
2K_test_643	Oral tongue squamous cell carcinoma ( OTSCC ) is associated with poor prognosis, Whats new ? Oral tongue squamous cell carcinoma ( OTSCC ) is a rare head and neck cancer that typically is asymptomatic in early stages. To improve prognostication Hence, in order to improve prognosis in OTSCC, predictive biomarkers that are independent of tumor stage must be identified. Unsupervised hierarchical clustering of the FISH data distinguished three clusters related to smoking status Copy number increases of all five markers were found to be correlated to non-smoking habits, while smokers in this cohort had low-level copy number gains The patients whose tumors were modeled as progressing by a more diverse distribution of copy number changes across the four genes have poorer prognosis, This is consistent with the view that multiple genetic pathways need to become deregulated in order for cancer to progress, Analyses of the models showed that the more diverse the changes within the four marker genes, the worse the outcome in OTSCC, The markers predicted survival independent of smoking behavior and tumor stage. Using the phylogenetic modeling software FISHtrees, we constructed models of tumor progression for each patient based on the four gene probes, Then we derived test statistics on the models that are significant predictors of disease-free and overall survival, independent of tumor stage and smoking status in multivariate analysis, Here using four fluorescence in situ hybridization ( FISH ) gene probes and the software FISHtrees, phylogenetic tree models of tumor progression in OTSCC patients were constructed. We analyzed four gene probes ( TERC, CCND1 EGFR and TP53 ) and the centromere probe CEP4 as a marker of chromosomal instability, using fluorescence in situ hybridization ( FISH ) in single cells from the tumors of sixty-five OTSCC patients ( Stage I, n=15 ; Stage II, n=30 ; Stage III, n=7 ; Stage IV.
2K_test_644	And derive some recommendations, Smartphone users are often unaware of the data collected by apps running on their devices. We report on a study intended to raise their awareness of the data collected by their apps. Participants reassessing their permissions, and 58 % of them further restricting some of their permissions We discuss how participants interacted both with the permission manager and the privacy nudges, analyze the effectiveness of both solutions, Our study provides both qualitative and quantitative evidence that these approaches are complementary and can each play a significant role in empowering users to more effectively control their privacy, For instance even after a week with access to the permission manager, participants benefited from nudges showing them how often some of their sensitive data was being accessed by apps, with 95 % of. That evaluates the benefits of giving users an app permission manager and sending them nudges.
2K_test_645	When a free catchy application shows up, how quickly will people notify their friends about it ? Will the enthusiasm drop exponentially with time, or oscillate ? What other patterns emerge ? Here we answer these questions which generate networks that mimic our discovered patterns. We report surprising patterns, the most striking of which are : ( a ) the FIZZLE pattern, excitement about Polly shows a power-law decay over time with ex- ponent of -1, 2 ; ( b ) the RENDEZVOUS pattern, that obeys a power law ( we explain RENDEZVOUS in the text ) ; ( c ) the DISPERSION pattern, we find that the more a person uses Polly, the fewer friends he will use it with, but in a reciprocal fashion. Finally we also propose a generator of influence networks. Using data from the Polly telephone-based application, a large influence network of 72, 000 people with about 173, 000 in- teractions spanning 500MB of log data and 200 GB of audio data.
2K_test_646	Inadequate information interoperability in facility management ( FM ) activities costs time and money being wasted for searching for the needed information in many different data sources, An integrated building information model ( BIM ), depicting as-is conditions has a high potential to minimize such wastes, However facility managers still face the challenge of generating as-is building information models for existing facilities. A main problem with current approaches for generating as-is BIMs is that they mainly focus on capturing and providing geometric information, Many other types of information are missing, such as equipment warranty and technical parameters, the generation of accurate and semantically-rich as-is BIMs. The comparative analysis results reveal several information gaps among different sources. The research described in this paper targets by leveraging heterogeneous existing information sources, such as drawings and operation and maintenance manuals. This approach was investigated through detailed case studies done in two old academic buildings, Existing information obtained from documents has been compared to Industry Foundation Classes ( IFC ), COBie and the data generated from a BIM authoring system.
2K_test_647	We approach the problem of estimating the parameters of a latent tree graphical model from a hierarchical tensor decomposition point of view. For correctly specified latent tree graphical models, we show that a global optimum of the optimization problem can be obtained via a recursive decomposition algorithm, This algorithm recovers previous spectral algorithms for hidden Markov models ( Hsu et al, 2009 ; Foster et al, 2012 ) and latent tree graphical models ( Parikh et al, 2011 ; Song et al, 2011 ) as special cases, elucidating the global objective these algorithms are optimizing, this new estimator significantly improves over the state-of-the-art. In this new view, the marginal probability table of the observed variables is treated as a tensor, and we show that : ( i ) the latent variables induce low rank structures in various matricizations of the tensor ; ( ii ) this collection of low rank matricizations induces a hierarchical low rank decomposition of the tensor, We further derive an optimization problem for estimating ( alternative ) parameters of a latent tree graphical model, allowing us to represent the marginal probability table of the observed variables in a compact and robust way, The optimization problem aims to find the best hierarchical low rank approximation of a tensor in Frobenius norm, For misspecified latent tree graphical models, we derive a novel decomposition based on our framework, and provide approximation guarantee and computational complexity analysis. In both synthetic and real world data.
2K_test_648	Much work in optimal control and inverse control has assumed that the controller has perfect knowledge of plant dynamics, Due to sensory feedback delay, the subject uses an internal model to generate an internal prediction of the current plant state, which may differ from the actual plant state. However if the controller is a human or animal subject, the subject 's internal dynamics model may differ from the true plant dynamics, Here we consider the problem of learning the subject 's internal model from demonstrations of control and knowledge of task goals, to jointly estimate the internal model, internal state trajectories and feedback delay. We discovered that the subject 's internal model deviated from the true BMI plant dynamics and provided significantly better explanation of the recorded neural control signals than did the true plant dynamics. We develop a probabilistic framework and exact EM algorithm. We applied this framework to demonstrations by a nonhuman primate of brain-machine interface ( BMI ) control.
2K_test_649	For users in cognitive decline. We describe the architecture and prototype implementation of an assistive system based on Google Glass devices It combines the first-person image capture and sensing capabilities of Glass with remote processing to perform real-time scene interpretation, The system architecture is multi-tiered It offers tight end-to-end latency bounds on compute-intensive operations, while addressing concerns such as limited battery capacity and limited processing capability of wearable devices The system gracefully degrades services in the face of network failures and unavailability of distant architectural tiers.
2K_test_650	Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown. But inference in such models can be slow, Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate to perform MCMC using the correct equilibrium distribution, in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods. In this paper we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us.
2K_test_651	Nowadays facility management ( FM ) teams are facing challenges to generate accurate and semantically-rich as-is BIMs for existing buildings, In order to address these challenges, formalized approaches are required to support conflict resolution, data extraction and integration. Current model creation approaches, such as model generation based on point cloud data, mainly capture geometric information of a building and lack to provide additional semantic information about components and other project information, This paper provides the results of a detailed case study. The initial findings from the case study highlighted two main challenges associated with model generation from existing data sources : information extraction and integration Existing information for different components is typically stored in heterogeneous data sources with various formats and quality, and hence requires different approaches to extract information, The findings also showed that almost 40 % of the component attributes investigated had conflicting values in existing sources. That aimed at leveraging existing data sources ( e, archived documents and data in FM systems ) to generate accurate and semanticallyrich as-is BIMs.
2K_test_652	Formal verification and validation play a crucial role in making cyber-physical systems ( CPS ) safe, Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained, including models of the controller and of the physical dynamics, In CPS models are essential ; but any model we could possibly build necessarily deviates from the real world. If the real system fits to the model, its behavior is guaranteed to satisfy the correctness properties verified with respect to the model, Otherwise all bets are off, ensuring that verification results about models apply to CPS implementations, to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic. Overall ModelPlex generates provably correct monitor conditions that, are provably guaranteed to imply that the offline safety verification results about the CPS model apply to the present run of the actual CPS implementation. This article introduces ModelPlex, a method ModelPlex provides correctness guarantees for CPS executions at runtime : it combines offline verification of CPS models with runtime validation of system executions for compliance with the model, ModelPlex ensures in a provably correct way that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model If, at some point the observed behavior no longer complies with the model so that offline verification results no longer apply, ModelPlex initiates provably safe fallback actions, assuming the system dynamics deviation is bounded This article, furthermore develops a systematic technique by a correct-by-construction approach, leading to verifiably correct runtime model validation. If checked to hold at runtime.
2K_test_654	Demand response has gained significant attention in recent years as it demonstrates potentials to enhance the power system 's operational flexibility in a cost-effective way, Industrial loads such as steel manufacturing plants consume large amounts of electric energy, and their electricity bills account for a remarkable percentage of their total operation cost, Meanwhile lots of industrial loads are very flexible in terms of adjusting their power consumption rate, through switching the transformer tap position. Hence industrial loads such as the steel plants have both the motivation and the ability to support power system operation through demand response to maximize its profits. In this paper we focus on the steel plant and optimize its scheduling from both the energy and the spinning reserve markets.
2K_test_655	Black-box mutational fuzzing is a simple yet effective technique to find bugs in software. Given a set of program-seed pairs, we ask how to schedule the fuzzings of these pairs in order to maximize the number of unique bugs found at any point in time. Show that one of our new scheduling algorithms outperforms the multi-armed bandit algorithm in the current version of the CERT Basic Fuzzing Framework ( BFF ) by finding 1, 5x more unique bugs in the same amount of time. We develop an analytic framework using a mathematical model of black-box mutational fuzzing. And use it to evaluate 26 existing and new randomized online scheduling algorithms.
2K_test_656	Much research on human action recognition has been oriented toward the performance gain on lab-collected datasets. Yet real-world videos are more diverse, with more complicated actions and often only a few of them are precisely labeled, Thus recognizing actions from these videos is a tough mission, to facilitate the action recognition in real-world videos. The paucity of labeled real-world videos motivates us to `` borrow '' strength from other resources, Specifically considering that many lab datasets are available, we propose to harness lab datasets given that the lab and real-world datasets are related As their action categories are usually inconsistent, we design a multi-task learning framework to jointly optimize the classifiers for both sides, The general Schatten $ $ p $ $ p -norm is exerted on the two classifiers to explore the shared knowledge between them, In this way our framework is able to mine the shared knowledge between two datasets even if the two have different action categories, which is a major virtue of our method, The shared knowledge is further used to improve the action recognition in the real-world videos. Extensive experiments are performed on real-world datasets.
2K_test_658	The costs are convex, have Lipschitz continuous gradient ( with constant L ). We study distributed optimization problems when N nodes minimize the sum of their individual costs subject to a common vector variable. Achieves rates O ( logK/K ) and O ( logk/k ), It achieves rates O ( 1/ K 2- ) and O ( 1/k 2 ) ( > 0 arbitrarily small ), examples illustrate our findings. We propose two fast distributed gradient algorithms based on the centralized Nesterov gradient algorithm and establish their convergence rates in terms of the per-node communications K and the per-node gradient evaluations k, Our first method Distributed Nesterov Gradient, Our second method Distributed Nesterov gradient with Consensus iterations, assumes at all nodes knowledge of L and ( W ) - the second largest singular value of the N N doubly stochastic weight matrix W. Further we give for both methods explicit dependence of the convergence constants on N and W.
2K_test_659	In spite of many favorable characteristics of guided-waves for Nondestructive Evaluation ( NDE ) of pipes, real-world application of these systems is still quite limited. Beside the complexities derived from multi-modal, dispersive and multi-path characteristics of guided-waves, one of the main challenges in guided-wave based NDE of pipelines is sensitivity of these systems to variations of environmental and operational conditions ( EOC ), This paper investigates the effects of varying EOCs on guilded-wave based NDE of pipelines. It is observed that masking effects of flow rate for damage detection can be at least as significant as temperature effects, and that such effects become more dominant when flow rate and temperature variations co-occur. We first provide a review of the studies to date in the field of guided-wave based testing to identify research gaps for enhancing the application of these systems in pipeline NDE To study the identified gaps, guided-wave data from a fully operational piping system, with continuously varying flow rate and temperature, is used Time-shift and amplitude drift effects due to flow rate variations are evaluated along with those of temperature.
2K_test_660	Given a graph with billions of nodes and edges, how can we find patterns and anomalies ? Are there nodes that participate in too many or too few triangles ? Are there close-knit near-cliques ? These questions are expensive to answer unless we have the first several eigenvalues and eigenvectors of the graph adjacency matrix. However eigensolvers suffer from subtle problems ( e, convergence ) for large sparse matrices, let alone for billion-scale ones, We address this problem. We report important discoveries about nearcliques and triangles on several real-world graphs, including a snapshot of the Twitter social network ( 56 Gb, 2 billion edges ) and the YahooWeb data set, one of the largest publicly available graphs ( 120 Gb, 4 billion nodes 6, 6 billion edges ). With the proposed HEIGEN algorithm, which we carefully design to be accurate, efficient and able to run on the highly scalable MAPREDUCE ( HADOOP ) environment, This enables HEIGEN to handle matrices more than 1 ; 000 larger than those which can be analyzed by existing algorithms. We implement HEIGEN and run it on the M45 cluster, one of the top 50 supercomputers in the world.
2K_test_661	For synthetic aperture radar ( SAR ) image formation, to match the capabilities of the application-specific logic-in-memory processing paradigm. Results show that the logic-in-memory approach has the potential to enable substantial improvements in energy efficiency without sacrificing image quality. In this paper we present a local interpolation-based variant of the well-known polar format algorithm used We develop the algorithm, which off-loads lightweight computation directly into the SRAM and DRAM Our proposed algorithm performs filtering, an image perspective transformation, and a local 2D interpolation, and supports partial and low-resolution reconstruction We implement our customized SAR grid interpolation logic-in-memory hardware in advanced 14 nm silicon technology, Our high-level design tools allow to instantiate various optimized design choices to fit image processing and hardware needs of application designers.
2K_test_662	Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets. However the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time : We expect both the structure of our hierarchy, and the parameters of the clusters, to evolve with time that can be used to model evolving hierarchies for performing approximate inference in such a model. We demonstrate the efficacy of our model and inference algorithm. In this paper we present a distribution over collections of time-dependent, infinite-dimensional trees and present an efficient and scalable algorithm. On both synthetic data and real-world document corpora.
2K_test_663	In modern crowdsourcing markets, requesters face the challenge of training and managing large transient workforces, Requesters can hire peer workers to review others ' work, but the value may be marginal, especially if the reviewers lack requisite knowledge. Our research explores if and how workers learn and improve their performance in a task domain by serving as peer reviewers, Further we investigate whether peer reviewing may be more effective in teams where the reviewers can reach consensus through discussion. The results show that workers who review others ' work perform better on subsequent tasks than workers who just produce, We also find that interactive reviewer teams outperform individual reviewers on all quality measures, However aggregating individual reviewers into nominal groups produces better quality assessments than interactive teams, except in task domains where discussion helps overcome individual misconceptions. An online between-subjects experiment compares the trade-offs of reviewing versus producing work using three different organization strategies : working individually, working as an interactive team, and aggregating individuals into nominal groups.
2K_test_664	Understanding where electricity is being used in buildings is an important tool for Cyber-Physical Systems ( CPS ) used in building energy conservation and efficiency. Current approaches for appliance-level energy metering typically require the installation of plug-through power meters, which is often difficult and costly for devices with inaccessible wires or outlets, or appliances that draw large amounts of current, that estimates the energy consumption of individual appliances which can detect appliance state transitions within close proximity. The system is able to detect appliance state transitions with an accuracy of 95, 8 % and estimate the overall energy with an accuracy of 98. In this paper we present an energy measurement system using a wireless sensor network consisting of contactless electromagnetic field ( EMF ) sensors deployed near each appliance, and a whole-house power meter, We present the design of a battery-operated EMF sensor, based on magnetic and electric field fluctuations, Each detector wirelessly transmits state change events to a circuit-panel energy meter, in a time-synchronized fashion, so that the overall power measurements can be used to estimate appliance-level energy usage, Our EMF sensors are able to detect significant power state changes from a few inches away, thus making it possible to externally monitor in-wall wiring to devices. We experimentally evaluate our proposed EMF sensor, three-phase power meter and communication protocol in a residential building collecting data for over a week.
2K_test_665	Methods for the analysis of chromatin immunoprecipitation sequencing ( ChIP-seq ) data start by aligning the short reads to a reference genome. While often successful they are not appropriate for cases where a reference genome is not available. Indicates that our method outperforms alignment based methods that utilize closely related species. Here we develop methods for de novo analysis of ChIP-seq data, Our methods combine de novo assembly with statistical tests enabling motif discovery without the use of a reference genome. We validate the performance of our method using human and mouse data, Analysis of fly data.
2K_test_666	Consumers who are close to one another in a social network often make similar purchase decisions This similarity can result from latent homophily or social influence, as well as common exogenous factors, Latent homophily means consumers who are connected to one another are likely to have similar characteristics and product preferences Social influence refers to the ability of one consumer to directly influence another consumer 's decision based upon their communication. Of purchases of caller ring-back tones using data from an Asian mobile network that predicts consumers ' purchase timing and choice decisions. Identification is achieved due to our dynamic, panel data structure and the availability of detailed communication data, We find strong influence effects and latent homophily effects in both the purchase timing and product choice decisions of consumers, This paper was accepted by Sandra Slaughter. We present an empirical study We simultaneously measure latent homophily and social influence, while also accounting for exogenous factors.
2K_test_667	How can web services that depend on user generated content discern fraudulent input by spammers from legitimate input ? as well as potential extensions to anomaly detection problems in other domains. In this paper we focus on the social network Facebook and the problem of discerning ill-gotten Page Likes, made by spammers hoping to turn a profit, from legitimate Page Likes detects lockstep Page Like patterns on Facebook to find such suspicious lockstep behavior. Finally we demonstrate and discuss the effectiveness of CopyCatch. Our method which we refer to as CopyCatch, by analyzing only the social graph between users and Pages and the times at which the edges in the graph ( the Likes ) were created We offer the following contributions : ( 1 ) We give a novel problem formulation, with a simple concrete definition of suspicious behavior in terms of graph structure and edge constraints ( 2 ) We offer two algorithms - one provably-convergent iterative algorithm and one approximate, scalable MapReduce implementation 3 ) We show that our method severely limits `` greedy attacks '' and analyze the bounds from the application of the Zarankiewicz problem to our setting CopyCatch is actively in use at Facebook, searching for attacks on Facebook 's social graph of over a billion users, many millions of Pages, and billions of Page Likes. At Facebook and on synthetic data.
2K_test_668	Iris masks play an important role in iris recognition They indicate which part of the iris texture map is useful and which part is occluded or contaminated by noisy image artifacts such as eyelashes, eyelids eyeglasses frames and specular reflections, The accuracy of the iris mask is extremely important, The performance of the iris recognition system will decrease dramatically when the iris mask is inaccurate, even when the best recognition algorithm is used. Traditionally people used the rule-based algorithms to estimate iris masks from iris images, However the accuracy of the iris masks generated this way is questionable to model the underlying probabilistic distributions of both valid and invalid regions on iris images. Results show that the masks generated by the proposed algorithm increase the iris recognition rate, verifying the effectiveness and importance of our proposed method for iris occlusion estimation. In this work we propose to use Figueiredo and Jain 's Gaussian Mixture Models ( FJ-GMMs ) We also explored possible features and found that Gabor Filter Bank ( GFB ) provides the most discriminative information for our goal Finally, we applied Simulated Annealing ( SA ) technique to optimize the parameters of GFB in order to achieve the best recognition rate. Experimental on both ICE2 and UBIRIS dataset.
2K_test_669	Given a simple noun such as { \em apple }, and a question such as `` is it edible ? ``, what processes take place in the human brain ?. More specifically given the stimulus, what are the interactions between ( groups of ) neurons ( also known as functional connectivity ) and how can we automatically infer those interactions, given measurements of the brain activity ? Furthermore, how does this connectivity differ across different human subjects ? which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity. GeBM produces brain activity patterns that are strikingly similar to the real ones, and the inferred functional connectivity is able to provide neuroscientific insights towards a better understanding of the way that neurons interact with each other, as well as detect regularities and outliers in multi-subject brain activity measurements. In this work we present a simple, novel good-enough brain model, or GeBM in short, and a novel algorithm Sparse-SysId Moreover, GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text ). We evaluate GeBM by using both synthetic and real brain data, Using the real data.
2K_test_670	Given a network with attributed edges, how can we identify anomalous behavior ? Networks with edge attributes are ubiquitous, and capture rich information about interactions between nodes. In this paper we aim to utilize exactly this information to discern suspicious from typical behavior in an unsupervised fashion, lending well to the traditional scarcity of ground-truth labels in practical anomaly detection scenarios, for detecting edge-attributed graph anomalies. : we show that EdgeCentric successfully spots numerous such anomalies where it achieved 0, 87 precision over the top 100 results. Our work has a number of notable contributions, including ( a ) formulation : while most other graph-based anomaly detection works use structural graph connectivity or node information, we focus on the new problem of leveraging edge information, ( b ) methodology : we introduce EdgeCentric, an intuitive and scalable compression-based approach and ( c ) practicality. In several large edge-attributed real-world graphs, including the Flipkart e-commerce graph with over 3 million product reviews between 1, 1 million users and 545 thousand products.
2K_test_671	We consider the problem of recovering a symmetric, positive semidefinite ( SPSD ) matrix from a subset of its entries, possibly corrupted by noise. Finally we demonstrate the algorithm 's utility. In contrast to previous matrix recovery work, we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix, We develop a set of sufficient conditions for the recovery of a SPSD matrix from a set of its principal submatrices, present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met, The proposed algorithm is naturally generalized to the problem of noisy matrix recovery, and we provide a worst-case bound on reconstruction error for this scenario. On noiseless and noisy simulated datasets.
2K_test_672	Can we predict the values of unseen coalitions ?. This paper explores a PAC ( probably approximately correct ) learning model in cooperative games, Specifically we are given m random samples of coalitions and their values, taken from some unknown cooperative game ; We also establish a novel connection between PAC learnability and core stability : for games that are efficiently learnable, it is possible to find payoff divisions that are likely to be stable using a polynomial number of samples. We study the PAC learnability of several well-known classes of cooperative games, such as network flow games, threshold task games and induced subgraph games.
2K_test_673	For discovering brand associations. We then demonstrate that our approach can discover complementary views on the brand associations that are hardly obtained from text data, show the superior performance of our algorithm for the two tasks over other candidate methods. In this paper we study an approach by leveraging large-scale online photo collections contributed by the general public, Brand Associations one of central concepts in marketing, describe customers ' top-of-mind attitudes or feelings toward a brand, what comes to mind when you think of Burberry ? ) Traditionally, brand associations are measured by analyzing the text data from consumers ' responses to the survey or their online conversation logs In this paper, we go beyond textual media and take advantage of large-scale photos shared on the Web More specifically, we jointly achieve the following two fundamental tasks in a mutually-rewarding way : ( i ) detecting exemplar images as key visual concepts associated with brands, and ( ii ) localizing the regions of brand in images. For experiments we collect about five millions of images of 48 brands crawled from five popular online photo sharing sites We also quantitatively.
2K_test_674	Abstract : Security-sensitive applications that execute untrusted code often check the codes integrity by comparing its syntax to a known good value or sandbox the code to contain its effects. For reasoning about such security-sensitive applications, to trace safety properties and, additionally contains two new reasoning principles. We illustrate both new reasoning principles of System M. System M is a new program logic System M extends Hoare Type Theory ( HTT ) First, its type system internalizes logical equality, facilitating reasoning about applications that check code integrity, Second a confinement rule assigns an effect type to a computation based solely on knowledge of the computations sandbox. We prove the sound-ness of System M relative to a step-indexed trace-based semantic model, by verifying the main integrity property of the design of Memoir, a previously proposed trusted computing system for ensuring state continuity of isolated security-sensitive applications.
2K_test_675	There has been recent interest in applying Stackelberg games to infrastructure security, in which a defender must protect targets from attack by an adaptive adversary, In real-world security settings the adversaries are humans and are thus boundedly rational. Most existing approaches for computing defender strategies against boundedly rational adversaries try to optimize against specific behavioral models of adversaries, and provide no quality guarantee when the estimated model is inaccurate, which provides guarantees against all adversary behavior models satisfying monotonicity, including all in the family of Regular Quantal Response functions for computing monotonic maximin. We propose a new solution concept, monotonic maximin We propose a mixed-integer linear program formulation We also consider top-monotonic maximin, a related solution concept that is more conservative, and propose a polynomial-time algorithm for top-monotonic maximin.
2K_test_676	The development of accurate clinical biomarkers has been challenging in part due to the diversity between patients and diseases, MSS provides a straightforward approach for modeling highly divergent subclasses of patients, which may be adaptable for diverse applications. One approach to account for the diversity is to use multiple markers to classify patients, based on the concept that each individual marker contributes information from its respective subclass of patients, for developing biomarker panels. That yielded similar classification accuracy to several other classification algorithms, revealed subclasses of patients based on distinct marker states. Here we present a new strategy that accounts for completely distinct patient subclasses, Marker State Space ( MSS ) defines marker states based on all possible patterns of high and low values among a panel of markers, Each marker state is defined as either a case state or a control state, and a sample is classified as case or control based on the state it occupies. MSS was used to define multi-marker panels that were robust in cross validation and training-set/test-set analyses and A three-marker panel for discriminating pancreatic cancer patients from control subjects.
2K_test_677	Background Serum albumin is a major pharmacokinetic effector of. To gain further insight into albumin binding chemistry. The crystal structures of six oncology agents were determined in complex with human serum albumin at resolutions of 2, 0 A : camptothecin, 9-amino-camptothecin etoposide teniposide bicalutamide and idarubicin.
2K_test_678	Multimedia data are usually represented by multiple features. For multimedia semantics understanding. Results show that it is beneficial to combine multiple features, The performance of the proposed algorithm is remarkable when only a small amount of labeled training data are available. In this paper we propose a new algorithm, namely Multi-feature Learning via Hierarchical Regression where two issues are considered, First labeling large amount of training data is labor-intensive, It is meaningful to effectively leverage unlabeled data to facilitate multimedia semantics understanding, Second given that multimedia data can be represented by multiple features, it is advantageous to develop an algorithm which combines evidence obtained from different features to infer reliable multimedia semantic concept classifiers, We design a hierarchical regression model to exploit the information derived from each type of feature, which is then collaboratively fused to obtain a multimedia semantic concept classifier, Both label information and data distribution of different features representing multimedia data are considered, The algorithm can be applied to a wide range of multimedia applications and. Experiments are conducted on video data for video concept annotation and action recognition, Using Trecvid and CareMedia video datasets.
2K_test_679	Matrix-parametrized models including multiclass logistic regression and sparse coding, are used in machine learning ( ML ) applications ranging from computer vision to computational biology. When these models are applied to large-scale ML problems starting at millions of samples and tens of thousands of classes, their parameter matrix can grow at an unexpected rate, resulting in high parameter synchronization costs that greatly slow down distributed learning, To address this issue. We propose a Sufficient Factor Broadcasting ( SFB ) computation model for efficient distributed learning of a large family of matrix-parameterized models, which share the following property : the parameter update computed on each data sample is a rank-1 matrix, the outer product of two `` sufficient factors '' ( SFs By broadcasting the SFs among worker machines and reconstructing the update matrices locally at each worker, SFB improves communication efficiency -- - communication costs are linear in the parameter matrix 's dimensions, rather than quadratic -- - without affecting computational correctness. We present a theoretical convergence analysis of SFB, and empirically on four different matrix-parametrized ML models.
2K_test_680	BACKGROUND There is a need of such studies in African Americans, because they display a higher incidence of aggressive CRC tumors, This WES study in African American patients with CRC provides insight into the identification of novel somatic mutations in APC Our data suggest an association between specific mutations in the Wnt signaling pathway and an increased risk of CRC, The analysis of the pathogenicity of these novel variants may shed light on the aggressive nature of CRC in African Americans. The purpose of this study was to identify genome-wide single nucleotide variants and mutations in African American patients with colorectal cancer ( CRC ). RESULTS We identified somatic mutations in genes that are known targets in CRC such as APC, BRAF KRAS and PIK3CA, We detected novel alterations in the Wnt pathway gene, APC within its exon 15, of which mutations are highly associated with CRC CONCLUSIONS. METHODS We performed whole exome sequencing ( WES ) on DNA from 12 normal/tumor pairs of African American CRC patient tissues, Data analysis was performed using the software package GATK ( Genome Analysis Tool Kit ), Normative population databases ( eg, 1000 Genomes SNP database, dbSNP and HapMap ) were used for comparison, Variants were annotated using analysis of variance and were validated via Sanger sequencing.
2K_test_681	Numeric time series data has unique storage requirements and access patterns that can benefit from specialized support, given its importance in Big Data analyses. Popular frameworks and databases focus on addressing other needs, making them a suboptimal fit, for numeric time series for efficient time series storage. And illustrates its potential for satisfying key requirements. This paper describes the support needed suggests an architecture.
2K_test_682	A well-spaced superset of points conforming to a given input set on the output points. We present a new algorithm that produces in any dimension with guaranteed optimal output size We also provide an approximate Delaunay graph Our algorithm runs in expected time O ( 2 O ( d ) ( n log n + m ) ), where n is the input size, m is the output point set size, and d is the ambient dimension The constants only depend on the desired element quality bounds, To gain this new efficiency, the algorithm approximately maintains the Voronoi diagram of the current set of points by storing a superset of the Delaunay neighbors of each point By retaining quality of the Voronoi diagram and avoiding the storage of the full Voronoi diagram, a simple exponential dependence on d is obtained in the running time Thus, if one only wants the approximate neighbors structure of a refined Delaunay mesh conforming to a set of input points, the algorithm will return a size 2 O ( d ) m graph in 2 O ( d ) ( n log n + m ) expected time, If m is superlinear in n, then we can produce a hierarchically well-spaced superset of size 2 O ( d ) n in 2 O ( d ) n log n expected time.
2K_test_683	Distributions over matrices with exchangeable rows and infinitely many columns are useful in constructing nonparametric latent variable models. However the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. And can achieve better performance. In this paper we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models Such models allow us to specify the distribution over the number of features per data point. On data sets where the number of features is not well-modeled by the original distribution.
2K_test_684	Stochastic Differential Equation SDE models are used to describe the dynamics of complex systems with inherent randomness, The primary purpose of these models is to study rare but interesting or important behaviours, such as the formation of a tumour. Stochastic simulations are the most common means for estimating or bounding the probability of rare behaviours, but the cost of simulations increases with the rarity of events To address this problem to quantify the likelihood of rare behaviours in SDE models. We introduce a new algorithm specifically designed Our approach relies on temporal logics for specifying rare behaviours of interest, and on the ability of bit-vector decision procedures to reason exhaustively about fixed-precision arithmetic. We apply our algorithm to a minimal parameterised model of the cell cycle, and take Brownian noise into account while investigating the likelihood of irregularities in cell size and time between cell divisions.
2K_test_685	For making inference about latent spaces of large networks. Our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. We propose a scalable approach With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efficient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches.
2K_test_686	As renewable generation capacity in the power grid increases, keeping the balance between the supply and demand becomes difficult, This threatens the grids stability and security, Existing power reserve assets and regulation methodologies fail to provide the short-term responses required to keep the load and generation balanced as the amount of renewable generation increases. Hence researchers proposed to increase the information exchange within the power network and to introduce realtime demand control to ensure robustness while accommodating the intermittent nature of these generation resources, Constituting a significant portion of the electrical demand of buildings, thermostatically controlled loads ( TCLs ) are wellsuited to provide real-time demand control, In this paper we shed light on challenges associated with engaging TCLs to the power grid to select parameters when simulating a TCL population. Using a centralized control strategy, to propose a strategy. We focus on the challenges associated with simulating a realistic TCL population using the models that are proposed in the literature, Specifically we use data collected from residential refrigeration units operating in 214 different households.
2K_test_687	Fusion of multiple features can boost the performance of large-scale visual classification and detection tasks like TRECVID Multimedia Event Detection ( MED ) competition [ 1 ]. To effectively fuse various features. Our approach achieves promising improvements on HMDB [ 8 ] action recognition dataset and CCV [ 5 ] video classification dataset show that our approach outperforms the state-of-the-art fusion methods for complex event detection. In this paper we propose a novel feature fusion approach, namely Feature Weighting via Optimal Thresholding ( FWOT ) FWOT learns the weights, thresholding and smoothing parameters in a joint framework to combine the decision values obtained from all the individual features and the early fusion, To the best of our knowledge, this is the first work to consider the weight and threshold factors of fusion problem simultaneously. Compared to state-of-the-art fusion algorithms, In addition experiments on two TRECVID MED 2011 collections.
2K_test_688	Background Aging infrastructure in the US has gained quite a bit of attention in the past decade, Being one type of a critical infrastructure, embankment dams in the US require significant investment to upgrade the deteriorated parts, Due to limited budgets, understanding the behavior of structures over time through risk assessment is essential to prioritize dams, During the risk assessment for embankment dams, engineers utilize current and historical data from the design, construction and operation phases of these structures The challenge is that during risk assessment, various engineers from different disciplines ( e, geotechnical hydraulics ) come together, and how they would like to visualize the available datasets changes based on the discipline-specific analyses they need to perform. The objective of this research study is to understand the discipline-specific visualization needs of engineers from US Army Corps of Engineers ( USACE ) who are involved in risk assessment of embankment dams when they deal with large set of data accumulated since the inception of dams.
2K_test_689	Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded ( e, This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function. There is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions ( termed Group Latent Auto-Regressive Analysis. And found that gLARA provides a better description of the recordings than pCCA. Rather than attempting to identify direct interactions between neurons ( where the number of interactions grows with the number of neurons squared ), we propose to extract a smaller number of latent variables from each population Specifically, we propose extensions to probabilistic canonical correlation analysis ( pCCA ). And study how these latent variables interact, We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2.
2K_test_690	Our findings have both behavioral and policy implications, as they highlight how technologies that make individuals feel more in control over the publication of personal information may have the paradoxical and unintended consequence of eliciting their disclosure of more sensitive information. We introduce and test the hypothesis that control over publication of private information may influence individuals privacy concerns and affect their propensity to disclose sensitive information, even when the objective risks associated with such disclosures do not change or worsen. Our findings suggest paradoxically, that more control over the publication of their private information decreases individuals privacy concerns and increases their willingness to publish sensitive information, even when the probability that strangers will access and use that information stays the same or, On the other hand, less control over the publication of personal information increases individuals privacy concerns and decreases their willingness to publish sensitive information, even when the probability that strangers will access and use information actually decreases. We designed three experiments in the form of online surveys administered to students at a North-American University, In all experiments we manipulated the participants control over information publication, but not their control over the actual access to and usage by others of the published information.
2K_test_691	A large number of facility management tasks relying on sensor measurements require knowledge of the context under which the readings were collected, However this context information ( i, 'spatial metadata ' ) is generally recorded manually, a process that is error-prone and time consuming considering the number of sensors located in a building, Therefore as other researchers have pointed out, there is a need to automatically determine the location information. Inferring the relative locations of sensors with respect to each other is arguably the first step to take and previous work in this area has already shown promising initial results In this paper, we explore whether linear correlation or a statistical dependency measure ( in this case distance correlation ) are better suited to infer spatial relations between a pair of temperature sensors in a commercial building. We conclude that a linear correlation ( the normalized covariance matrix ) captures the spatial relationship in most situations although it is significantly sensitive to choosing the appropriate window size. We conducted analyses on three different test beds where temperature measurements from 10 sensors were collected every minute, We consider every possible size of data subsets within a year to explore time-windowing effects, We also examine how different physical distances between sensors affect the results.
2K_test_692	Most everyday electrical and electromechanical objects emit small amounts of electromagnetic ( EM ) noise during regular operation, Our studies show that discrimination between dozens of objects is feasible, independent of wearer time and local environment. When a user makes physical contact with such an object, this EM signal propagates through the user, owing to the conductivity of the human body. By modifying a small, low-cost software-defined radio we can detect and classify these signals in real-time, enabling robust on-touch object detection, Unlike prior work our approach requires no instrumentation of objects or the environment ; our sensor is self-contained and can be worn unobtrusively on the body, We call our technique EM-Sense and built a proof-of-concept smartwatch implementation.
2K_test_694	For detecting irregularly-shaped spatial clusters. Given a small amount of labeled training data, StarScan learns appropriate penalties for both compact and elongated clusters, resulting in improved detection performance. We propose StarScan a new star-shaped scan statistic StarScan generalizes the traditional, circular spatial scan statistic by allowing the radius of the cluster around a center location to vary continuously with the angle, but penalizes the log-likelihood ratio score proportional to the total change in radius. StarScan was compared with circular scan and fast subset scan on simulated respiratory outbreaks and bioterrorist anthrax attacks injected into real-world Emergency Department data.
2K_test_695	Crowd-powered systems that help people are difficult to scale and sustain because human labor is expensive and worker pools are difficult to grow To address this problem. Which collected 618 high-quality answers to questions asked over 12 days, illustrating the feasibility of the approach. We introduce the idea of social microvolunteering, a type of intermediated friendsourcing in which a person can provide access to their friends as potential workers for microtasks supporting causes that they care about, We explore this idea by creating Visual Answers, an exemplar social microvolunteering application for Facebook that posts visual questions from people who are blind. We present results of a survey of 350 participants on the concept of social microvolunteering, and a deployment of the Visual Answers application with 91 participants.
2K_test_697	Communication costs resulting from synchronization requirements during learning, can greatly slow down many parallel machine learning algorithms. We prove that our algorithm generates asymptotically exact samples and demonstrate its ability to parallelize burn-in and sampling in several models. In this paper we present a parallel Markov chain Monte Carlo ( MCMC ) algorithm in which subsets of data are processed independently, with very little communication, First we arbitrarily partition data onto multiple machines, Then on each machine, any classical MCMC method ( e, Gibbs sampling ) may be used to draw samples from a posterior distribution given the data subset, Finally the samples from each machine are combined to form samples from the full posterior, This embarrassingly parallel algorithm allows each machine to act independently on a subset of the data ( without communication ) until the final combination stage.
2K_test_698	Stochastic gradient optimization is a class of widely used algorithms for training machine learning models, To optimize an objective, it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset. However when the variance of the noisy gradient is large, the algorithm might spend much time bouncing around, leading to slower convergence and worse performance for variance reduction in stochastic gradient. On both problems our approach shows faster convergence and better performance than the classical approach. In this paper we develop a general approach of using control variate Data statistics such as low-order moments ( pre-computed or estimated online ) is used to form the control variate. We demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization, One is convexthe MAP estimation for logistic regression, and the other is non-convexstochastic variational inference for latent Dirichlet allocation.
2K_test_699	For detecting anomalous patterns in general categorical data sets. And demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain FGSS substantially decreased run time and improved detection power for massive multivariate data sets. We propose Fast Generalized Subset Scan ( FGSS ), a new method We frame the pattern detection problem as a search over subsets of data records and attributes, maximizing a nonparametric scan statistic over all such subsets We prove that the nonparametric scan statistics possess a novel property that allows for efficient optimization over the exponentially many subsets of the data without an exhaustive search, enabling FGSS to scale to massive and high-dimensional data sets. We evaluate the performance of FGSS in three real-world application domains ( customs monitoring, disease surveillance and network intrusion detection ), As compared to three other recently proposed detection algorithms.
2K_test_700	Paid crowd work offers remarkable opportunities for improving productivity, social mobility and the global economy by engaging a geographically distributed workforce to complete complex tasks on demand and at scale. But it is also possible that crowd work will fail to achieve its potential, focusing on assembly-line piecework, Can we foresee a future crowd workplace in which we would want our children to participate ? This paper frames the major challenges that stand in the way of this goal, that will enable crowd work that is complex. Drawing on theory from organizational behavior and distributed computing, as well as direct feedback from workers, we outline a framework The framework lays out research challenges in twelve major areas : workflow, task assignment hierarchy real-time response, synchronous collaboration quality control, crowds guiding AIs AIs guiding crowds, platforms job design reputation.
2K_test_701	We consider the problem of adaptively routing a fleet of cooperative vehicles within a road network in the presence of uncertain and dynamic congestion conditions, To tackle this problem to minimize the collective travel time of all vehicles in the system. We show that our congestion model is effective in modeling dynamic congestion conditions, We also show that our routing algorithm generates significantly faster routes compared to standard baselines, and achieves near-optimal performance We also present the results which showcases the efficacy of our approach. We first propose a Gaussian Process Dynamic Congestion Model that can effectively characterize both the dynamics and the uncertainty of congestion conditions, Our model is efficient and thus facilitates real-time adaptive routing in the face of uncertainty, Using this congestion model, we develop an efficient algorithm for non-myopic adaptive routing A key property of our approach is the ability to efficiently reason about the long-term value of exploration, which enables collectively balancing the exploration/exploitation trade-off for entire fleets of vehicles. We validate our approach based on traffic data from two large Asian cities compared to an omniscient routing algorithm, from a preliminary field study.
2K_test_702	This enables radial interactions in an area many times larger than a mobile device ; for example, virtual buttons that lie above, below and to the left and right. The simple fact that human fingers are large and mobile devices are small has led to the perennial issue of limited surface area for touch-based interactive tasks, In response that extends touch interaction beyond the small confines of a mobile device. Which shows that Toffee can accurately resolve the bearings of touch events ( mean error of 4, 3 with a laptop prototype ). We have developed Toffee, a sensing approach and onto ad hoc adjacent surfaces, most notably tabletops This is achieved using a novel application of acoustic time differences of arrival ( TDOA ) correlation, Previous time-of-arrival based systems have required semi-permanent instrumentation of the surface and were too large for use in mobile devices, Our approach requires only a hard tabletop and gravity -- the latter acoustically couples mobile devices to surfaces. We conducted an evaluation.
2K_test_704	A novel motion tracking technology. That highlight its immediate feasibility and utility. We present Lumitrack that uses projected structured patterns and linear optical sensors, Each sensor unit is capable of recovering 2D location within the projection area, while multiple sensors can be combined for up to six degree of freedom ( DOF ) tracking, Our structured light approach is based on special patterns, called m-sequences in which any consecutive sub-sequence of m bits is unique, Lumitrack can utilize both digital and static projectors, as well as scalable embedded sensing configurations, The resulting system enables high-speed, high precision and low-cost motion tracking for a wide range of interactive applications. We detail the hardware, operation and performance characteristics of our approach, as well as a series of example applications.
2K_test_705	There are many security tools and techniques for analyzing software, but many of them require access to source code, A decompiler should focus on two properties to be used for security, First it should recover abstractions as much as possible to minimize the complexity that must be handled by the security analysis that follows, Second it should aim to recover these abstractions correctly, Previous work in control-flow structuring, an abstraction recovery problem used in decompilers, does not provide either of these properties. To apply existing source-based tools and techniques to compiled programs, Specifically existing structuring algorithms are not semantics-preserving, which means that they can not safely be used for decompilation without modification, Existing structural algorithms also miss opportunities for recovering control flow structure, that addresses these problems. Our evaluation is an order of magnitude larger than previous systematic studies of endto-end decompilers, We show that our decompiler outperforms the de facto industry standard decompiler Hex-Rays in correctness by 114 %, and recovers 30 more control-flow structure than existing structuring algorithms in the literature. We propose leveraging decompilation, the study of recovering abstractions from compiled code, We propose a new structuring algorithm in this paper. We evaluate our decompiler, Phoenix and our new structuring algorithm, on a set of 107 real world programs from GNU coreutils.
2K_test_706	Identifying a suspect wearing a mask ( where only the suspect 's periocular region is visible ) is one of the toughest real-world challenges in biometrics that exist, This is an important problem faced in many law-enforcement applications on almost a daily basis. To hallucinate the full frontal face given only the periocular region of a face In such real-world situations, we only have access to the periocular region of a person 's face, Unfortunately commercial matchers are unable to process these images successfully. Show that our reconstruction technique, based on a modified sparsifying dictionary learning algorithm, can effectively reconstruct faces which we show are actually very similar to the original ground-truth faces, We show the real-world applicability of method to show that they still match competitively. In this paper we present a practical method We propose in this paper, an approach that will reconstruct the entire frontal face using just the periocular region Further, our method is open set, thus can reconstruct any face not seen in training. We empirically by benchmarking face verification results using the reconstructed faces compared to the original faces when evaluated under a large-scale face verification protocol such as NIST 's FRGC protocol where over 256 million face matches are made.
2K_test_707	A device just like Harry Potter 's Marauder 's Map, which pinpoints the location of each person-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. Show that our algorithm performs robust localization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem, Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection face recognition and non-background information to perform tracking, Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints, Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual.
2K_test_708	Using this method optimal strategies can be designed for control resource allocation to manage risk in a business process, Our work contributes to the literature on both ex ante risk management-based business process design and ex post risk assessments of existing business processes and control models This research applies not only to the literature on and practice of process design and risk management but also to business decision support systems in general. This article investigates the economic consequences of data errors in the information flows associated with business processes for managing the risks associated with such data errors. We develop a process modeling-based methodology Our method focuses on the topological structure of a process and takes into account its effect on error propagation and risk mitigation using both expected loss and conditional value-at-risk risk measures. An order-fulfillment process of an online pharmacy is used to illustrate the methodology.
2K_test_709	Design construction and operation of building heating, ventilation and air conditioning ( HVAC ) systems are complicated processes that generally involve several stakeholders, such as mechanical designers, control system integrators commissioning agents and facilities managers, It is important for all these stakeholders at various phases of the project to have a thorough understanding of the system components as well as the control strategy according to the design intent of the mechanical designers, For example when assessing the behavior of a HVAC system during operation phase, it is important for facilities managers to check for the correctness of every components behavior and its control logic against the design specifications, Challenges such as missing information for controlled parameters as well as textual descriptions that are open to interpretations are common and result in inaccurate interpretation of the system behavior, This may adversely affect the overall performance of systems and lead to energy inefficiencies. The control sequences and logic of HVAC systems are primarily conveyed through schematic diagrams and textual descriptions called sequence of operations ( SOOs ) in construction documents ( ASHRAE, Several challenges are associated with extracting and interpreting the information contained in these SOOs, Through a detailed analysis of a case-study conducted in relation to the information provided in the SOOs for the air handling unit ( AHU ) in a building, the research described in this paper highlights these challenges.
2K_test_710	Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets, It optimizes the variational objective with stochastic optimization, following noisy estimates of the natural gradient Operationally, stochastic inference iteratively subsamples from the data, analyzes the subsample and updates parameters with a decreasing learning rate. However the algorithm is sensitive to that rate, which usually requires hand-tuning to each application We solve this problem. Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates. By developing an adaptive learning rate for stochastic variational inference, Our method requires no tuning and is easily implemented with computations already made in the algorithm. We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora.
2K_test_711	Confessions are people 's way of coming clean, sharing unethical acts with others, Although confessions are traditionally viewed as categorical one either comes clean or not people often confess to only part of their transgression Such partial confessions may seem attractive, because they offer an opportunity to relieve ones guilt without having to own up to the full consequences of the transgression It seems that although partial confessions seem attractive, they come at an emotional cost. We found a high frequency of partial confessions, especially among people cheating to the full extent possible, People found partial confessions attractive because they ( correctly ) expected partial confessions to be more believable than not confessing People failed, however to anticipate the emotional costs associated with partially confessing In fact, partial confessions made people feel worse than not confessing or fully confessing, a finding corroborated in a laboratory setting as well as in a study assessing peoples everyday confessions. Using a novel experimental design. In this paper we explored the occurrence, antecedents consequences and everyday prevalence of partial confessions.
2K_test_712	Compared to visual concepts such as actions, scenes and objects complex event is a higher level abstraction of longer video sequences, For example a `` marriage proposal '' event is described by multiple objects ( e, ring faces ) scenes ( e, in a restaurant outdoor ) and actions ( e. The positive exemplars which exactly convey the precise semantic of an event are hard to obtain, It would be beneficial to utilize the related exemplars for complex event detection, However the semantic correlations between related exemplars and the target event vary substantially as relatedness assessment is subjective, Two related exemplars can be about completely different events, in the TRECVID MED dataset, both bicycle riding and equestrianism are labeled as related to `` attempting a bike trick '' event, To tackle the subjectiveness of human assessment. Demonstrate that our algorithm is able to utilize related exemplars adaptively, and the algorithm gains good performance for complex event detection. Our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis.
2K_test_713	Summary : T cells are activated through interaction with antigen-presenting cells ( APCs ), During activation receptors and signalingintermediates accumulate in diverse spatiotemporal distributions, Thesedistributions control the probability of signaling interactions and thusgovern information ow through the signaling system, Spatiotemporal-ly resolved system-scale investigation of signaling can extract the regu-latory information thus encoded, allowing unique insight into thecontrol of T-cell function, Substantial technical challenges exist, andthese are briey discussed herein. While much of the work assessingT-cell spatiotemporal organization uses planar APC substitutes, wefocus here on B-cell APCs with often stark differences. We suggest that the regulation ofactin dynamics, by controlling signaling distributions and membranetopology, is an important rheostat of T-cell signaling. Spatiotemporalsignaling distributions are driven by cell biologically distinct structures, a large protein assembly at the interface center, a large invagination the actin-supported interface periphery as extended by smaller indi-vidual lamella, and a newly discovered whole-interface actin-drivenlamellum, The more than 60 elements of T-cell activation studied todate are dynamically distributed between these structures, generating acomplex organization of the signaling system, Signal initiation and coresignaling prefer the interface center, while signal amplication islocalized in the transient lamellum, Actin dynamics control signalingdistributions through regulation of the underlying structures and drivea highly undulating T-cell/APC interface that imposes substantialconstraints on T-cell organization.
2K_test_714	This setting was recently studied in [ 15 ], where the \KernelKernel '' estimator was introduced and shown to have a polynomial rate of convergence. However evaluating a new prediction with the Kernel-Kernel estimator scales as ( N ), This causes the dicult situation where a large amount of data may be necessary for a low estimation risk, but the computation cost of estimation becomes infeasible when the data-set is too large to alleviate this big data problem. To this end we propose the Double-Basis estimator, which looks in two ways : rst, the Double-Basis estimator is shown to have a computation complexity that is independent of the number of of instances N when evaluating new predictions after training ; secondly, the Double-Basis estimator is shown to have a fast rate of convergence for a general class of mappings f2F.
2K_test_715	Primary motor-cortex multi-unit activity ( MUA ) and local-field potentials ( LFPs ) have both been suggested as potential control signals for brain-computer interfaces ( BCIs ) aimed at movement restoration, Some studies report that LFP-based decoding is comparable to spiking-based decoding, while others offer contradicting evidence, Our results suggest that a velocity and speed encoding model is most appropriate for both MUA and LFP H, whereas a speed only encoding model is adequate for LFP L. Differences in experimental paradigms, tuning models and decoding techniques make it hard to directly compare these results, to study how MUA and LFP encode various kinematic parameters during reaching movements. We find that in addition to previously reported directional tuning, MUA also contains prominent speed tuning, LFP activity in low-frequency bands ( 15-40Hz, LFP L ) is primarily speed tuned, and contains more speed information than both high-frequency LFP ( 100-300Hz, LFP H ) and MUA, LFP H contains more directional information compared to LFP L, but less information when compared with MUA. Here we use regression and mutual information analyses.
2K_test_716	Stencil computations are an integral component of applications in a number of scientific computing domains Short-vector SIMD instruction sets are ubiquitous on modern processors and can be used to significantly increase the performance of stencil computations. Traditional approaches to optimizing stencils on these platforms have focused on either short-vector SIMD or data locality optimizations, for stencil computations that allows specification of stencils in a concise manner to enhance data locality and enable load-balanced parallelism to effectively increase the performance of stencil computations. Performance increases are demonstrated for a number of stencils. In this paper we propose a domain specific language and compiler and automates both locality and short-vector SIMD optimizations, along with effective utilization of multi-core parallelism Loop transformations are combined with a data layout transformation. On several modern SIMD architectures.
2K_test_717	We discuss how design teams can use our approach to reflect on prototypes or begin user studies within seconds, and how over time, Apparition prototypes can become fully-implemented versions of the systems they simulate. Prototyping allows designers to quickly iterate and gather feedback, but the time it takes to create even a Wizard-of-Oz prototype reduces the utility of the process for prototyping interactive systems in the time it takes to describe the idea. In this paper we introduce crowdsourcing techniques and tools Our Apparition system uses paid microtask crowds to make even hard-to-automate functions work immediately, allowing more fluid prototyping of interfaces that contain interactive elements and complex behaviors As users sketch their interface and describe it aloud in natural language, crowd workers and sketch recognition algorithms translate the input into user interface elements, add animations and provide Wizard-of-Oz functionality, Powering Apparition is the first self-coordinated, real-time crowdsourcing infrastructure We anchor this infrastructure on a new, lightweight write-locking mechanism that workers can use to signal their intentions to each other.
2K_test_718	These context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself. Our model consistently outperforms competing models. We present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features, We learn our model in an efficient online fashion that is scalable for large. With five streaming datasets from two different genres economics news articles and social mediawe evaluate our model on the task of sequential language modeling.
2K_test_719	Embankment dams like most other civil infrastructure systems, are exposed to harsh and largely unpredictable environments, However unlike bridges buildings and other structures, their design specifications and as-is properties are not generally known in the same level of detail due to, among other things their age and the difficulties associated with assessing their internal structure, Hence making sense of measurements collected from instruments used to monitor their behavior requires sound engineering judgment and analysis, as well as robust statistical analysis techniques to prevent misinterpretation, In the United States ( US ), the current practice of analyzing the structural integrity of embankment dams relies primarily on manual a posteriori analysis of instrument data by engineers, leaving much room for improvement through the application of automated data analysis techniques, In our previous work, we presented the effectiveness of applying statistical anomaly detection techniques such as Principal Component Analysis and Robust Regression Analysis when analyzing piezometer data collected from embankment dams. In this paper we present how we could improve our work by testing with simulated anomalies that are indicative of internal erosion problems, In order to closely replicate more realistic anomalous scenarios. The detection accuracy came out to be 98. A physics-based model of an embankment dam was developed. By varying a hydraulic conductivity of a soil material in the model, corresponding detection accuracies and sensitivities of the statistical anomaly detection algorithm were evaluated When we applied our proposed anomaly detection on more realistically simulated anomalous data using the numerical model.
2K_test_720	It is typically expected that if a mechanism is truthful, then the agents would, indeed truthfully report their private information. But why would an agent believe that the mechanism is truthful ? truthfulness can be verified efficiently ( in the computational sense ). We wish to design truthful mechanisms that are simple, that is whose Our approach involves three steps : ( i ) specifying the structure of mechanisms, ( ii ) constructing a verification algorithm, and ( iii ) measuring the quality of verifiably truthful mechanisms. We demonstrate this approach using a case study : approximate mechanism design without money for facility location.
2K_test_721	Given the deluge of multimedia content that is becoming available over the Internet, it is increasingly important to be able to effectively examine and organize these large stores of information in ways that go beyond browsing or collaborative filtering of solving the TOMS problem. And present results of a pilot evaluation of our initial system. In this paper we review previous work on audio and video processing, and define the task of topic-oriented multimedia summarization ( TOMS ) using natural language generation ( NLG ) : given a set of automatically extracted features from a video, a TOMS system will automatically generate a paragraph of natural language, which summarizes the important information in a video belonging to a certain topic, and for example provides explanations for why a video was matched and retrieved, Possible features include visual semantic concepts, objects and actions environmental sounds, and transcripts from automatic speech recognition ( ASR ), We see this as a first step towards systems that will be able to discriminate visually similar, but semantically different videos, compare two videos and provide textual output or summarize a large number of videos at once In this paper, we introduce our approach We extract various visual concept features, environmental sounds and ASR transcription features from a given video, and develop a template-based NLG system to produce a textual recounting based on the extracted features. We also propose possible experimental designs for continuously evaluating and improving TOMS systems.
2K_test_722	From this comparison of Twitter and in-person regrets, we provide preliminary ideas for tools to help Twitter users avoid and cope with regret. Comparing messages individuals regretted. Participants generally reported similar types of regrets in person and on Twitter, In particular they often regretted messages that were critical of others However, regretted messages that were cathartic/expressive or revealed too much information were reported at a higher rate for Twitter, Regretted messages on Twitter also reached broader audiences, In addition we found that participants who posted on Twitter became aware of, and tried to repair, regret more slowly than those reporting in-person regrets. We present the results of an online survey of 1, 221 Twitter users either saying during in-person conversations or posting on Twitter.
2K_test_723	Fragments of first-order temporal logic are useful for representing many practical privacy and security policies, Past work has proposed two strategies for checking event trace ( audit log ) compliance with policies : online monitoring and offline audit. Although online monitoring is space- and time-efficient, existing techniques insist that satisfying instances of all subformulas of the policy be amenable to caching, which limits expressiveness when some subformulas have infinite support, In contrast offline audit is brute force and can handle more policies but is not as efficient that caches satisfying instances when it can, and falls back to the brute force search when it can not, flow- and time-sensitive static check of variable groundedness. We prove the correctness of our algorithm. This paper proposes a new online monitoring algorithm Our key technical insight is a new called the temporal mode check, which determines subformulas for which such caching is feasible and those for which it is not and, hence guides our algorithm. And evaluate its performance over synthetic traces and realistic policies.
2K_test_724	Abstract Host factor protein Cyclophilin A ( CypA ) regulates HIV-1 viral infectivity through direct interactions with the viral capsid, by an unknown mechanism, CypA can either promote or inhibit viral infection, depending on host cell type and HIV-1 capsid ( CA ) protein sequence, These results suggest that CypA loop dynamics is a determining factor in HIV-1 's escape from CypA dependence. We have examined the role of conformational dynamics on the nanosecond to millisecond timescale in HIV-1 CA assemblies in the escape from CypA dependence, by magic-angle spinning ( MAS ) NMR and molecular dynamics ( MD ). We demonstrate that assembled CA is dynamic, particularly in loop regions exhibits unprecedented mobility on the nanosecond to microsecond timescales, and the experimental NMR dipolar order parameters are in quantitative agreement with those calculated from MD trajectories Remarkably, the CypA loop dynamics of wild-type CA HXB2 assembly is significantly attenuated upon CypA binding, and the dynamics profiles of the A92E and G94D CypA escape mutants closely resemble that of wild-type CA assembly in complex with CypA. Through the analysis of backbone 1H-15N and 1H-13C dipolar tensors and peak intensities from 3D MAS NMR spectra of wild-type and the A92E and G94D CypA escape mutants, The CypA loop in assembled wild-type CA from two strains.
2K_test_725	To analyze a control algorithm designed to provide directional force feedback for a surgical robot, that provides safe operation along with directional force feedback. That guarantees the new algorithm is safe for all possible inputs. We then applied QdL to guide the development of a new algorithm. We applied quantified differential-dynamic logic ( QdL ) We identified problems with the algorithm, proved that it was in general unsafe, and described exactly what could go wrong, Using \KeYmaeraD ( a tool that mechanizes QdL ), we created a machine-checked proof.
2K_test_726	The rise of socially targeted marketing suggests that decisions made by consumers can be predicted not only from their personal tastes and characteristics, but also from the decisions of people who are close to them in their networks. One obstacle to consider is that there may be several different measures for closeness that are appropriate, either through different types of friendships, or different functions of distance on one kind of friendship, where only a subset of these networks may actually be relevant, Another is that these decisions are often binary and more difficult to model with conventional approaches, both conceptually and computationally, To address these issues. We present a hierarchical auto-probit model for individual binary outcomes that uses and extends the machinery of the auto-probit method for binary data. We demonstrate the behavior of the parameters estimated by the multiple network-regime auto-probit model ( m-NAP ) under various sensitivity conditions, such as the impact of the prior distribution and the nature of the structure of the network, We also demonstrate several examples of correlated binary data outcomes in networks of interest to information systems, including the adoption of caller ring-back tones, whose use is governed by direct connection but explained by additional network topologies.
2K_test_728	I010178 Complex software systems are becoming increasingly prevalent in aerospace applications : in particular, to accomplish critical tasks, Ensuring the safety of these systems is crucial, as they can have subtly different behaviors under slight variations in operating and proposals are given on how to address these issues. This paper advocates the use of formal verification techniques and in particulartheoremprovingfor hybridsoftware-intensivesystemsasawell-foundedcomplementaryapproachtothe classical aerospace verification and validation techniques, such as testing or simulation. The challenges that naturally arise when applying such technology to industrial-scale applications is then detailed. As an illustration of these techniques, a novel lateral midair collision-avoidance maneuver is studied in an ideal setting, without accounting for the uncertainties of the physical reality.
2K_test_729	Existing Bayesian models especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations, Such results contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community. While priors affect posterior distributions through Bayes ' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. Which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. In this paper we present regularized Bayesian inference ( RegBayes ), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation, RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks, When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes, Furthermore we present two concrete examples of RegBayes, infinite latent support vector machines ( iLSVM ) and multi-task infinite latent support vector machines ( MT-iLSVM ), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, We present efficient inference methods and. Report empirical studies on several benchmark data sets.
2K_test_730	Network robustness is an important principle in biology and engineering, Previous studies of global networks have identified both redundancy and sparseness as topological properties used by robust networks, Modules internal to the cell that are less exposed to environmental noise are more connected and less robust than external modules, A similar design principle is used by several other biological networks. By focusing on molecular subnetworks, or modules we show that module topology is tightly linked to the level of environmental variability ( noise ) the module expects to encounter, which gives rise to the rich range of module topologies observed within real networks. Leads to novel algorithms and insights benefiting both fields. We propose a simple change to the evolutionary gene duplication model. We apply these observations to evaluate and design communication networks that are specifically optimized for noisy or malicious environments Combined, joint analysis of biological and computational networks.
2K_test_731	The convergence of mobile computing and cloud computing enables new multimedia applications that are both resource-intensive and interaction-intensive, For these applications end-to-end network bandwidth and latency matter greatly when cloud resources are used to augment the computational power and battery life of a mobile device. That this crucial design consideration to meet interactive performance criteria limits data center consolidation. We then describe an architectural solution that is a seamless extension of today 's cloud computing infrastructure. We first present quantitative evidence.
2K_test_732	For smooth convex optimization subject to block-separable constraints. Our analysis reveals how the potential speedups over BCFW depend on the minibatch size and how one can provably obtain large problem dependent speedups, obtaining significant speedups over competing state-of-the-art ( and synchronous ) methods on structural SVMs. We develop mini-batched parallel Frank-Wolfe ( conditional gradient ) methods Our work includes the basic ( batch ) Frank-Wolfe algorithm as well as the recently proposed Block-Coordinate Frank-Wolfe ( BCFW ) method\citep { lacoste2012block } as special cases Our algorithm permits asynchronous updates within the minibatch, and is robust to stragglers and faulty worker threads. We present several experiments to indicate empirical behavior of our methods.
2K_test_734	How can we tell when accounts are fake or real in a social network ? And how can we tell which accounts belong to liberal, conservative or centrist users ? Often, we can answer such questions and label nodes in a network based on the labels of their neighbors and appropriate assumptions of homophily ( `` birds of a feather flock together '' ) or heterophily ( `` opposites attract '' ), One of the most widely used methods for this kind of inference is Belief Propagation ( BP ) which iteratively propagates the information from a few nodes with explicit labels throughout a network until convergence. A well-known problem with BP, however is that there are no known exact guarantees of convergence in graphs with loops that allows a closed-form solution that propagates information across every edge at most once. Show that LinBP and SBP are orders of magnitude faster than standard BP, while leading to almost identical node labels. This paper introduces Linearized Belief Propagation ( LinBP ), a linearization of BP via intuitive matrix equations and, thus comes with exact convergence guarantees, It handles homophily heterophily, and more general cases that arise in multi-class settings, Plus it allows a compact implementation in SQL, The paper also introduces Single-pass Belief Propagation ( SBP ), a localized ( or `` myopic '' ) version of LinBP and for which the final class assignments depend only on the nearest labeled neighbors In addition, SBP allows fast incremental updates in dynamic networks.
2K_test_735	For monitoring temporal evolution of market competition. We show that the proposed approach is more successful than other candidate methods for the topic modeling of competition, We also demonstrate the generalization power of the proposed method for three prediction tasks. We propose a dynamic topic model by jointly leveraging tweets and their associated images, For a market of interest ( e, luxury goods ) we aim at automatically detecting the latent topics ( e, bags clothes luxurious ) that are competitively shared by multiple brands ( e, Burberry Prada and Chanel ), and tracking temporal evolution of the brands ' stakes over the shared topics, One of key applications of our work is social media monitoring that can provide companies with temporal summaries of highly overlapped or discriminative topics with their major competitors We design our model to correctly address three major challenges : multiview representation of text and images, modeling of competitiveness of multiple brands over shared topics, and tracking their temporal evolution, As far as we know, no previous model can satisfy all the three challenges. For evaluation we analyze about 10 millions of tweets and 8 millions of associated images of the 23 brands in the two categories of luxury and beer.
2K_test_736	The stochastic matching problem deals with finding a maximum matching in a graph whose edges are unknown but can be accessed via queries, This is a special case of stochastic k-set packing, where the problem is to find a maximum packing of sets, each of which exists with some probability, for these two problems. We show that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches. In this paper we provide edge and set query algorithms that provably achieve some fraction of the omniscient optimal solution, Our main theoretical result for the stochastic matching ( i, 2-set packing ) problem is the design of an adaptive algorithm that queries only a constant number of edges per vertex and achieves a ( 1-e ) fraction of the omniscient optimal solution, for an arbitrarily small e > 0 Moreover, this adaptive algorithm performs the queries in only a constant number of rounds, We complement this result with a non-adaptive ( i, one round of queries ) algorithm that achieves a ( 0, 5 - e ) fraction of the omniscient optimum, We also extend both our results to stochastic k-set packing by designing an adaptive algorithm that achieves a ( 2/k - e ) fraction of the omniscient optimal solution, again with only O ( 1 ) queries per element, This guarantee is close to the best known polynomial-time approximation ratio of 3/k+1 -e for the deterministic k-set packing problem [ Furer 2013. We empirically explore the application of ( adaptations of ) these algorithms to the kidney exchange problem, where patients with end-stage renal failure swap willing but incompatible donors on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange.
2K_test_738	Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data, For example they are often used to explore complex systems where connections between entities are not well understood, such as in functional brain networks or genetic networks. Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable, such as a binary neural activation state or a continuous mRNA abundance measurement, even though in many real world problems, nodes can represent multivariate variables with much richer meanings, such as whole images, text documents or multi-view feature vectors, for estimating the structure of undirected graphical models from such multivariate ( or multi-attribute ) nodal data. Demonstrate the effectiveness of the method under various conditions. In this paper we propose a new principled framework The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes, Under a Gaussian model, this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection ( Dempster, We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective, We provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles, and uncovering brain connectivity graph from positron emission tomography data, Finally we provide sufficient conditions under which the true graphical structure can be recovered correctly.
2K_test_739	Many large-scale machine learning ( ML ) applications use iterative algorithms to converge on parameter values that make the chosen model fit the input data. Often this approach results in the same sequence of accesses to parameters repeating each iteration to improve the efficiency of the parallel and distributed ML applications that will be a mainstay in cloud computing environments. Show that such exploitation reduces per-iteration time by 33 -- 98 %, for three real ML workloads, and that these improvements are robust to variation in the patterns over time. This paper shows that these repeating patterns can and should be exploited Focusing on the increasingly popular `` parameter server '' approach to sharing model parameters among worker threads, we describe and demonstrate how the repeating patterns can be exploited Examples include replacing dynamic cache and server structures with static pre-serialized structures, informing prefetch and partitioning decisions, and determining which data should be cached at each thread to avoid both contention and slow accesses to memory banks attached to other sockets.
2K_test_740	Dashboards are increasingly being used in commercial buildings to show building data in an intuitive way to occupants and facility operators, Such dashboards make relevant parties aware of the impact that they have on a buildings behavior and enable them to understand the dynamics of building systems and current/historical energy use, and as a result, support reduction in energy use and improvement of operations of such systems. For designing and implementing an energy dashboard. The findings show that effective building energy dashboards should contain query-based and quick-access based functionalities for showing building energy data through the use of various widgets and user interactions Such dashboards should also enable decomposing data within spatial and temporal dimensions, interacting with static and dynamic data sources, and providing information about directly measurable energy usage vs, resulting energy use indicators. This paper gives an overview of an approach for a monitored building that includes highly sensed building automation systems and sensors for energy consumption, This project is part of the Energy Efficient Buildings Hub in the Philadelphia Navy Yard, The developed approach incorporates formalization of a taxonomy for building energy dashboards, identification of visualization aids and requirements through a questionnaire. And a prototype implementation.
2K_test_742	Many modern multi-core processors sport a large shared cache with the primary goal of enhancing the statistic performance of computing workloads, However due to resulting cache interference among tasks, the uncontrolled use of such a shared cache can significantly hamper the predictability and analyzability of multi-core real-time systems, Software cache partitioning has been considered as an attractive approach to address this issue because it does not require any hardware support beyond that available on many modern processors. However the state-of-the-art software cache partitioning techniques face two challenges : ( 1 ) the memory co-partitioning problem, which results in page swapping or waste of memory, and ( 2 ) the availability of a limited number of cache partitions, which causes degraded performance, These are major impediments to the practical adoption of software cache partitioning, for multi-core real-time systems. Results indicate that compared to the traditional approaches, our scheme is up to 39 % more memory space efficient and consumes up to 25 % less cache partitions while maintaining cache predictability, Our scheme also yields a significant utilization benefit that increases with the number of tasks. In this paper we propose a practical OS-level cache management scheme Our scheme provides predictable cache performance, addresses the aforementioned problems of existing software cache partitioning, and efficiently allocates cache partitions to schedule a given task set. We have implemented and evaluated our scheme in Linux/RK running on the Intel Core i7 quad-core processor.
2K_test_743	Existing algorithms for trajectory-based clustering usually rely on simplex representation and a single proximity-related distance ( or similarity ) measure Consequently, additional information markers ( e, social interactions or the semantics of the spatial layout ) are usually ignored, leading to the inability to fully discover the communities in the trajectory database. This is especially true for human-generated trajectories, where additional fine-grained markers ( e, movement velocity at certain locations, or the sequence of semantic spaces visited ) can help capture latent relationships between cluster members, To address this limitation, for computing semantic level similarity to discover the set of distinct communities. Experimental results demonstrate that TODMIS correctly and efficiently discovers the real grouping behaviors in these diverse settings. We propose TODMIS : a general framework for Trajectory cOmmunity Discovery using Multiple Information Sources, TODMIS combines additional information with raw trajectory data and creates multiple similarity metrics In our proposed approach, we first develop a novel approach by constructing a Markov Random Walk model from the semantically-labeled trajectory data, and then measuring similarity at the distribution level In addition, we also extract and compute pair-wise similarity measures related to three additional markers, namely trajectory level spatial alignment ( proximity ), temporal patterns and multi-scale velocity statistics Finally, after creating a single similarity metric from the weighted combination of these multiple measures, we apply dense sub-graph detection. We evaluated TODMIS extensively using traces of ( i ) student movement data in a campus, ( ii ) customer trajectories in a shopping mall, and ( iii ) city-scale taxi movement data.
2K_test_745	Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence, and may enable a useful new way of interacting with the crowds powering other systems. Despite decades of research attempting to establish conversational interaction between humans and computers, the capabilities of automated conversational systems are still limited. Demonstrate that Chorus can provide accurate, topical responses answering nearly 93 % of user queries appropriately, and staying on-topic in over 95 % of responses We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed, quality and breadth of assistance. In this paper we introduce Chorus, a crowd-powered conversational assistant, When using Chorus end users converse continuously with what appears to be a single conversational partner, Behind the scenes Chorus leverages multiple crowd workers to propose and vote on responses, A shared memory space helps the dynamic crowd workforce maintain consistency, and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting. Studies with 12 end users and 100 crowd workers.
2K_test_746	Organizations that collect and use large volumes of personal information often use security audits to protect data subjects from inappropriate uses of this information by authorized insiders, In face of unknown incentives of employees, a reasonable audit strategy for the organization is one that minimizes its regret. While regret minimization has been extensively studied in repeated games, the standard notion of regret for repeated games can not capture the complexity of the interaction between the organization ( defender ) and an adversary, which arises from dependence of rewards and actions on history, To account for this generality, which can provide a more accurate model of the audit process. We introduce a richer class of games called bounded-memory games, We introduce the notion of k-adaptive regret, which compares the reward obtained by playing actions prescribed by the algorithm against a hypothetical k-adaptive adversary with the reward obtained by the best expert in hindsight against the same adversary, Roughly a hypothetical k-adaptive adversary adapts her strategy to the defender 's actions exactly as the real adversary would within each window of karounds, A k-adaptive adversary is a natural model for temporary adversaries ( e, company employees ) who stay for a certain number of audit cycles and are then replaced by a different person Our definition is parameterized by a set of experts, which can include both fixed and adaptive defender strategies, We investigate the inherent complexity of and design algorithms for adaptive regret minimization in bounded-memory games of perfect and imperfect information.
2K_test_747	But the task of determining which SNPs have functional consequences remains an open challenge. This brought the effective coverage in the assemblies to eightfold, reducing the number and size of gaps in the final assembly over what would be obtained with 5, The two assembly strategies yielded very similar results that largely agree with independent mapping data The assemblies effectively cover the euchromatic regions of the human chromosomes, More than 90 % of the genome is in scaffold assemblies of 100, 000 bp or more, and 25 % of the genome is in scaffolds of 10 million bp or larger Analysis of the genome sequence revealed 26, 588 protein-encoding transcripts for which there was strong corroborating evidence and an additional 12, 000 computationally derived genes with mouse matches or other weak supporting evidence Although gene-dense clusters are obvious, almost half the genes are dispersed in low G+C sequence separated by large tracts of apparently noncoding sequence, 1 % of the genome is spanned by exons, whereas 24 % is in introns, with 75 % of the genome being intergenic DNA, Duplications of segmental blocks, ranging in size up to chromosomal lengths, are abundant throughout the genome and reveal a complex evolutionary history indicates vertebrate expansions of genes associated with neuronal function, with tissue-specific developmental regulation, and with the hemostasis and immune systems provided locations of 2, 1 million single-nucleotide polymorphisms ( SNPs ) A random pair of human haploid genomes differed at a rate of 1 bp per 1250 on average, but there was marked heterogeneity in the level of polymorphism across the genome, Less than 1 % of all SNPs resulted in variation in proteins. 91-billion base pair ( bp ) consensus sequence of the euchromatic portion of the human genome was generated by the whole-genome shotgun sequencing method, 8-billion bp DNA sequence was generated over 9 months from 27, 271 853 high-quality sequence reads ( 5, 11-fold coverage of the genome ) from both ends of plasmid clones made from the DNA of five individuals, Two assembly strategiesa whole-genome assembly and a regional chromosome assemblywere used, each combining sequence data from Celera and the publicly funded genome effort, The public data were shredded into 550-bp segments to create a 2, 9-fold coverage of those genome regions that had been sequenced, without including biases inherent in the cloning and assembly procedure used by the publicly funded group. Comparative genomic analysis DNA sequence comparisons between the consensus sequence and publicly funded genome data.
2K_test_748	Developments in health information technology have encouraged the establishment of distributed systems known as Health Information Exchanges ( HIEs ) to enable the sharing of patient records between institutions In many cases, the parties running these exchanges wish to limit the amount of information they are responsible for holding because of sensitivities about patient information, Hence there is an interest in broker-based HIEs that keep limited information in the exchange repositories. However it is essential to audit these exchanges carefully due to risks of inappropriate data sharing for auditing broker-based HIEs. In this paper we consider some of the requirements and present a design in a way that controls the information available in audit logs and regulates their release for investigations Our approach is based on formal rules for audit and the use of Hierarchical Identity-Based Encryption ( HIBE ) to support staged release of data needed in audits and a balance between automated and manual reviews. We test our methodology via an extension of a standard for auditing HIEs called the Audit Trail and Node Authentication Profile ( ATNA ) protocol.
2K_test_749	These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures. We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods, Specifically we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points and structure exploiting ( Kronecker and Toeplitz ) algebra for a scalable kernel representation, We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process, Inference and learning cost $ O ( n ) $ for $ n $ training points, and predictions cost $ O ( 1 ) $ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples.
2K_test_750	Significant efforts are being made in the improvement of building automation and control systems in order to optimize the performance of buildings ( e, reduction of energy consumption ), As sensor networks in buildings increase, the complexity of managing them also increases, For instance the generation and maintenance of metadata about sensors, such as their location within a building, currently requires significant manual labor, Being able to understand the relationships between different measurement types and building characteristics is fundamental in achieving an automatic mapping of sensors in buildings. The research described in this paper explores the relationship between different HVAC system sensor measurements and physical characteristics of spaces, and its potential application in streamlining the identification of sensor location within a facility, and this paper describes initial observations and results towards this goal. The energy contained in the conditioned air delivered to each room is presented as a characteristic feature in order to understand the differences between rooms.
2K_test_751	Given a large dataset of users ' ratings of movies, what is the best model to accurately predict which movies a person will like ? And how can we prevent spammers from tricking our algorithms into suggesting a bad movie ? Is it possible to infer structure between movies simultaneously ? that accomplishes all of these goals. ( 2 ) We provide proof of our model 's robustness to spam and anomalous behavior, to demonstrate the model 's effectiveness in accurately predicting user 's ratings, avoiding prediction skew in the face of injected spam, and finding interesting patterns in real world ratings data. In this paper we describe a unified Bayesian approach to Collaborative Filtering It models the discrete structure of ratings and is flexible to the often non-Gaussian shape of the distribution, Additionally our method finds a co-clustering of the users and items, which improves the model 's accuracy and makes the model robust to fraud, We offer three main contributions : ( 1 ) We provide a novel model and Gibbs sampling algorithm that accurately models the quirks of real world ratings, such as convex ratings distributions. 3 ) We use several real world datasets.
2K_test_753	Standard approaches to stochastic discrete systems require numerical solutions for large optimization problems and quickly become infeasible with larger state spaces, Generalizations of these techniques to hybrid systems with stochastic effects are even more challenging It is in principle applicable to a variety of stochastic models from other domains. We address the problem of model checking stochastic systems, checking whether a stochastic system satisfies a certain temporal property with a probability greater ( or smaller ) than a fixed threshold. While the answer to the verification problem is not guaranteed to be correct, we prove that Bayesian SMC can make the probability of giving a wrong answer arbitrarily small We show that our technique enables faster verification than state-of-the-art statistical techniques. In particular we present a Statistical Model Checking ( SMC ) approach based on Bayesian statistics We show that our approach is feasible for a certain class of hybrid systems with stochastic transitions, a generalization of Simulink/Stateflow models, The SMC approach was pioneered by Younes and Simmons in the discrete and non-Bayesian case, It solves the verification problem by combining randomized sampling of system traces ( which is very efficient for Simulink/Stateflow ) with hypothesis testing ( i, testing against a probability threshold ) or estimation ( i, computing with high probability a value close to the true probability ), We believe SMC is essential for scaling up to large Stateflow/Simulink models, The advantage is that answers can usually be obtained much faster than with standard, exhaustive model checking techniques We emphasize that Bayesian SMC is by no means restricted to Stateflow/Simulink models. We apply our Bayesian SMC approach to a representative example of stochastic discrete-time hybrid system models in Stateflow/Simulink : a fuel control system featuring hybrid behavior and fault tolerance.
2K_test_754	Existing literature proposes distributed gradient-like methods that are computationally cheap and resilient to link failures, but have slow convergence rates, For comparison the standard distributed gradient method can not do better than ( 1/k 2/3 ) and ( 1/ K 2/3 ), on the same class of cost functions ( even for static networks ). We consider distributed optimization in random networks where N nodes cooperatively minimize the sum i=1 N f i ( x ) of their individual convex costs. We prove their convergence rates Then the modified D-NG achieves rates O ( logk/k ) and O ( logK/ K ), and the modified D-NC rates O ( 1/k 2 ) and O ( 1/ K 2- ), where > 0 is arbitrarily small illustrate our analytical findings. In this paper we propose accelerated distributed gradient methods that 1 ) are resilient to link failures ; 2 ) computationally cheap ; and 3 ) improve convergence rates over other gradient methods, We model the network by a sequence of independent, identically distributed random matrices { W ( k ) } drawn from the set of symmetric, stochastic matrices with positive diagonals, The network is connected on average and the cost functions are convex, differentiable with Lipschitz continuous and bounded gradients, We design two distributed Nesterov-like gradient methods that modify the D-NG and D-NC methods that we proposed for static networks. In terms of the expected optimality gap at the cost function, Let k and K be the number of per-node gradient evaluations and per-node communications.
2K_test_756	In search advertising the search engine needs to select the most profitable advertisements to display, which can be formulated as an instance of online learning with partial feedback, also known as the stochastic multi-armed bandit ( MAB ) problem. In this paper we show that the naive application of MAB algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and `` estimation of the largest mean '' ( ELM ) bias that harms the advertisers by increasing game-theoretic player-regret. We then propose simple bias-correction methods with benefits to both the search engine and the advertisers.
2K_test_758	Smartwatches promise to bring enhanced convenience to common communication, creation and information retrieval tasks. Due to their prominent placement on the wrist, they must be small and otherwise unobtrusive, which limits the sophistication of interactions we can perform, This problem is particularly acute if the smartwatch relies on a touchscreen for input, as the display is small and our fingers are relatively large. In this work we propose a complementary input approach : using the watch face as a multi-degree-of-freedom, We developed a proof of concept smartwatch that supports continuous 2D panning and twist, as well as binary tilt and click. To illustrate the potential of our approach, we developed a series of example applications, many of which are cumbersome -- or even impossible -- on today 's smartwatch devices.
2K_test_759	Which song will Smith listen to next ? Which restaurant will Alice go to tomorrow ? Which product will John click next. These applications have in common the prediction of user trajectories that are in a constant state of flux over a hidden network ( e, website links geographic location ), Moreover what users are doing now may be unrelated to what they will be doing in an hour from now to cope with the complex challenges of learning personalized predictive models of non-stationary, transient and time-heterogeneous user trajectories. TribeFlow is more accurate and up to 413x faster than top competitors. Mindful of these challenges we propose TribeFlow, a method designed TribeFlow is a general method that can perform next product recommendation, next song recommendation next location prediction, and general arbitrary-length user trajectory prediction without domain-specific knowledge.
2K_test_760	For instance although `` eats '' and `` stares at '' seem unrelated in text, they share semantics visually, When people are eating something, they also tend to stare at the food. To learn visually grounded word embeddings ( vis-w2v ) to capture visual notions of semantic relatedness, While word embeddings trained using text have been extremely successful, they can not uncover notions of semantic relatedness implicit in our visual world, Grounding diverse relations like `` eats '' and `` stares at '' into vision remains challenging, despite recent progress in vision. We show improvements on three tasks : common-sense assertion classification, visual paraphrasing and text-based image retrieval, Our code and datasets are available online. We propose a model We note that the visual grounding of words depends on semantics, and not the literal pixels, We thus use abstract scenes created from clipart to provide the visual grounding, We find that the embeddings we learn capture fine-grained, visually grounded notions of semantic relatedness. Over text-only word embeddings ( word2vec ).
2K_test_761	Learning about a new area of knowledge is challenging for novices partly because they are not yet aware of which topics are most important. The Internet contains a wealth of information for learning the underlying structure of a domain, but relevant sources often have diverse structures and emphases, making it hard to discern what is widely considered essential knowledge vs, Crowdsourcing offers a potential solution because humans are skilled at evaluating high-level structure, but most crowd micro-tasks provide limited context and time, To address these challenges. The first experiment shows that a high context, low structure interface helps crowdworkers perform faster, higher quality synthesis while the second experiment shows that a tournament-style ( parallelized ) crowd workflow produces faster, higher quality more diverse outlines than a linear ( serial/iterative ) workflow. We present Crowdlines a system that uses crowdsourcing to help people synthesize diverse online information, Crowdworkers make connections across sources to produce a rich outline that surfaces diverse perspectives within important topics. We evaluate Crowdlines with two experiments.
2K_test_762	Review fraud is a pervasive problem in online commerce, in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services. Fake reviews are often detected based on several signs, including 1 ) they occur in short bursts of time ; 2 ) fraudulent user accounts have skewed rating distributions, However these may both be true in any given dataset, for detecting fraudulent reviews. Show that BIRDNEST successfully spots review fraud in large real-world graphs : the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart. Hence in this paper, we propose an approach which combines these 2 approaches in a principled manner, allowing successful detection even when one of these signs is not present To combine these 2 approaches, we formulate our Bayesian Inference for Rating Data ( BIRD ) model, a flexible Bayesian model of user rating behavior, Based on our model we formulate a likelihood-based suspiciousness metric, Normalized Expected Surprise Total ( NEST ), We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric. Experiments on real data.
2K_test_763	For estimating the pitch and yaw of fingers relative to a touchscreen 's surface. That illustrate the value and immediate feasibility of our approach. We describe a novel approach offering two additional, analog degrees of freedom for interactive functions, Further we show that our approach can be achieved on off-the-shelf consumer touchscreen devices : a smartphone and smartwatch. We validate our technique though a user study on both devices and conclude with several demo applications.
2K_test_764	User identification and differentiation have implications in many application domains, including security personalization and co-located multiuser systems, In response dozens of approaches have been developed, from fingerprint and retinal scans, to hand gestures and RFID tags. To provide real-time authentication and even identification of users. Our user study demonstrates twenty-participant authentication accuracies of 99, For twenty-user identification our software achieved 94, 0 % accuracy and 98, 2 % on groups of four. In this work we propose CapAuth, a technique that uses existing, low-level touchscreen data combined with machine learning classifiers. As a proof-of-concept we ran our software on an off-the-shelf Nexus 5 smartphone.
2K_test_765	Health information exchanges ( HIEs ) are healthcare information technology efforts designed to foster coordination of patient care across the fragmented U, Their purpose is to improve efficiency and quality of care through enhanced sharing of patient data, Across the United States, numerous states have enacted laws that provide various forms of incentives for HIEs and address growing privacy concerns associated with the sharing of patient data Our results contribute to the burgeoning literature on health information technology and the debate on the impact of privacy regulation on technology innovation, In particular they show that the impact of privacy regulation on the success of information technology efforts is heterogeneous : both positive and negative effects can arise from regulation, depending on the specific attributes of privacy laws, This paper was accepted by Anandhi Bharadwaj. We investigate the impact on the emergence of HIEs of state laws that incentivize HIE efforts and state laws that include different types of privacy requirements for sharing healthcare data. Although we observe that privacy regulation alone can result in a decrease in planning and operational HIEs, we also find that, when coupled with incentives, privacy regulation with requirements for patient consent can actually positively impact the development of HIE efforts Among all states with laws creating HIE incentives, only states that combined incentives with consent requirements saw a net increase in operational HIEs ; HIEs in those states also reported decreased levels of privacy concern relative to HIEs in states with other legislative approaches. Focusing on the impact of laws that include requirements for patient consent.
2K_test_766	Natural language dialog is an important and intuitive way for people to access information and services This hybrid systems approach will help make dialog systems both more general and more robust going forward. However current dialog systems are limited in scope, brittle to the richness of natural language, and expensive to produce. This paper introduces Guardian, a crowdpowered framework that wraps existing Web APIs into immediately usable spoken dialog systems Guardian takes as input the Web API and desired task, and the crowd determines the parameters necessary to complete it, how to ask for them, and interprets the responses from the API, The system is structured so that, over time it can learn to take over for the crowd.
2K_test_767	Recent research has highlighted the need for upstream planning in healthcare service delivery systems, patient scheduling and resource allocation in the hospital inpatient setting DRGs are a payment scheme employed at patients discharge, where the DRG and length of stay determine the revenue that the hospital obtains. This study examines the value of upstream planning within hospital-wide resource allocation decisions focusing on prediction of diagnosis-related groups ( DRGs ) and the use of these predictions for allocating scarce hospital resources. The largest improvements were observed at and before admission, when information such as procedures and diagnoses is typically incomplete, but performance was improved even after a substantial portion of the patients length of stay, and under multiple scenarios making different assumptions about the available information, Using the improved DRG predictions within our resource allocation model improves contribution margin by 2, 9 % and the utilization of scarce resources such as operating rooms and beds from 66, 3 % to 67, 3 % and from 70, 7 % to 71, 0 % more nonurgent elective patients to be admitted as compared to the baseline. Based on machine learning ( ML ) and mixed-integer programming ( MIP ), We show that early and accurate DRG classification using ML methods, incorporated into an MIP-based resource allocation model, can increase the hospitals contribution margin, the number of admitted patients, and the utilization of resources such as operating rooms and beds. We test these methods on hospital data containing more than 16, 000 inpatient records and demonstrate improved DRG classification accuracy as compared to the hospitals current approach.
2K_test_768	Touchscreens with dynamic electrostatic friction are a com-pelling, low-latency and solid-state haptic feedback technology, Work to date has focused on minimum perceptual difference, texture rendering and fingertip-surface models. However no work to date has quantified how electrostatic feedback can be used to improve user performance, in par-ticular targeting where virtual objects rendered on touchscreens can offer tactile feedback. Our results show that can improve targeting speed by 7, 5 % compared to conventional flat touchscreens.
2K_test_769	Background Effective management and treatment of cancer continues to be complicated by the rapid evolution and resulting heterogeneity of tumors, Phylogenetic study of cell populations in single tumors provides a way to delineate intra-tumoral heterogeneity and identify robust features of evolutionary processes. The introduction of single-cell sequencing has shown great promise for advancing single-tumor phylogenetics ; however, the volume and high noise in these data present challenges for inference, especially with regard to chromosome abnormalities that typically dominate tumor evolution to use such data to track differences in tumor cell genomic content during progression. Here we investigate a strategy.
2K_test_770	When training large machine learning models with many variables or parameters, a single machine is often inadequate since the model may be too large to fit in memory, while training can take a long time even with stochastic updates, A natural recourse is to turn to distributed cluster computing, in order to harness additional memory and processors. However naive unstructured parallelization of ML algorithms can make inefficient use of distributed memory, while failing to obtain proportional convergence speedups or can even result in divergence, for dynamic model-parallelism in order to explore partitioning and update scheduling of model variables in distributed ML algorithms. We develop a framework of primitives, STRADS thus improving their memory efficiency while presenting new opportunities to speed up convergence without compromising inference correctness. We demonstrate the efficacy of model-parallel algorithms implemented in STRADS versus popular implementations for Topic Modeling, Matrix Factorization and Lasso.
2K_test_771	The ap- proach can, generate nontrivial algebraic invariant equations capturing the airplane behavior during take-off or landing in longitudinal motion. We prove that any invariant algebraic set of a given polynomial vector field can be algebraically represented to efficiently automate the generation. By one polynomial and a finite set of its successive Lie derivatives, This so-called differential radical characterization re- lies on a sound abstraction of the reachable set of solutions by the smallest variety that contains it, The characterization leads to a differential radical invariant proof rule that is sound and complete, which implies that invariance of algebraic equa- tions over real-closed fields is decidable Furthermore, the problem of generating invariant varieties is shown to be as hard as minimizing the rank of a symbolic matrix, and is therefore NP-hard. We investigate symbolic linear algebra tools based on Gaussian elimination.
2K_test_772	When companies operate on the graphs with monetary incentives to sell Twitter `` Followers '' and Facebook page `` Likes '', the graphs show strange connectivity patterns. Given multimillion-node graphs such as `` who-follows-whom '', `` patent-cites-patent '' `` user-likes-page '' and `` actor/director-makes-movie '' networks, how can we find unexpected behaviors ? to detect users who offer the lockstep behaviors in undirected/directed/bipartite graphs. We report strange deviations from typical patterns like smooth degree distributions, We find that such deviations are often due to `` lockstep behavior '' that large groups of followers connect to the same groups of followees, We discover that ( a ) the lockstep behaviors on the graph shape dense `` block '' in its adjacency matrix and creates `` rays '' in spectral subspaces, and ( b ) partially overlapping of the behaviors shape `` staircase '' in its adjacency matrix and creates `` pearls '' in spectral subspaces The results demonstrate the scalability and effectiveness of our proposed algorithm. The second contribution is that we provide a fast algorithm, using the discovery as a guide for practitioners. In this paper we study a complete graph from a large Twitter-style social network, spanning up to 3, Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep, We carry out extensive experiments on both synthetic and real datasets, as well as public datasets from IMDb and US Patent.
2K_test_773	Characterizing the spatial distribution of proteins directly from microscopy images is a difficult problem with numerous applications in cell biology ( e, identifying motor-related proteins ) and clinical research ( e, identification of cancer biomarkers ) Such models are expected to be valuable for representing and summarizing each pattern and for constructing systems biology simulations of cell behaviors. That provides automated analysis of punctate protein patterns in microscope images, that captures the essential characteristics of the distinct patterns. We were able to show that these patterns could be distinguished from each other with high accuracy, and we were able to assign to one of these subclasses hundreds of proteins whose subcellular localization had not previously been well defined. Here we describe the design of a system including quantification of their relationships to microtubules, We constructed the system using confocal immunofluorescence microscopy images from the Human Protein Atlas project for 11 punctate proteins in three cultured cell lines These proteins have previously been characterized as being primarily located in punctate structures, but their images had all been annotated by visual examination as being simply vesicular, In addition to providing these novel annotations, we built a generative approach to modeling of punctate distributions.
2K_test_774	Revenue maximization in combinatorial auctions ( and other multidimensional selling settings ) is one of the most important and elusive problems in mechanism design, Such priors do not exist in most applications, Rather in many applications ( such as premium display advertising markets ), there is essentially a point prior, which may not be accurate. The optimal design is unknown, and is known to include features that are not acceptable in many applications, such as favoring some bidders over others and randomization A second challenge in mechanism design for combinatorial auctions is that the prior distribution on each bidder 's valuation can be doubly exponential, for branching upper bounding, lower bounding and lazy bounding. Validate the approach and show that our techniques dramatically improve scalability over a leading general-purpose MIP solver. In this paper we instead study a common revenue-enhancement approach - bundling - in the context of the most commonly studied combinatorial auction mechanism, the Vickrey-Clarke-Groves ( VCG ) mechanism, We adopt the point prior model, and prove robustness to inaccuracy in the prior, Then we present a branch-and-bound framework for finding the optimal bundling, We introduce several techniques. Experiments on CATS distributions.
2K_test_775	We address the problem of action recognition in unconstrained videos. Most noticeably the accuracy of our algorithm reaches 51, 8 % on the challenging HMDB dataset which outperforms the state-of-the-art of 7. We propose a novel content driven pooling that leverages space-time context while being robust toward global space-time transformations, Being robust to such transformations is of primary importance in unconstrained videos where the action localizations can drastically shift between frames, Our pooling identifies regions of interest using video structural cues estimated by different saliency functions, To combine the different structural information, we introduce an iterative structure learning algorithm, WSVM ( weighted SVM ), that determines the optimal saliency layout of an action model through a sparse regularizer, A new optimization method is proposed to solve the WSVM highly non-smooth objective function. We evaluate our approach on standard action datasets ( KTH, UCF50 and HMDB ).
2K_test_776	It is well known that strategic behavior in elections is essentially unavoidable ; we therefore ask : how bad can the rational outcome be ? We answer this question. We provide very positive results for plurality and very negative results for Borda, and place veto in the middle of this spectrum. Via the notion of the price of anarchy, using the scores of alternatives as a proxy for their quality and bounding the ratio between the score of the optimal alternative and the score of the winning alternative in Nash equilibrium Specifically, we are interested in Nash equilibria that are obtained via sequences of rational strategic moves, Focusing on three common voting rules -- plurality, veto and Borda --.
2K_test_777	Given a large graph, like a computer communication network, which $ k $ nodes should we immunize ( or monitor, or remove ) to make it as robust as possible against a computer virus attack ? This problem, referred to as the node immunization problem, is the core building block in many high-impact applications, ranging from public health, cybersecurity to viral marketing, A central component in node immunization is to find the best $ k $ bridges of a given graph. In this setting we typically want to determine the relative importance of a node ( or a set of nodes ) within the graph, for example how valuable ( as a bridge ) a person or a group of persons is in a social network. ( 1 ) the proposed bridging score gives mining results consistent with intuition ; and ( 2 ) the proposed fast solution is up to seven orders of magnitude faster than straightforward alternatives. First of all we propose a novel bridging score $ \Delta \lambda $, inspired by immunology and we show that its results agree with intuition for several realistic settings, Since the straightforward way to compute $ \Delta \lambda $ is computationally intractable, we then focus on the computational issues and propose a surprisingly efficient way ( $ O ( nk^2+m ) $ ) to estimate it. Experimental results on real graphs show that.
2K_test_778	To the best of our knowledge, our work represents the largest study of propagation patterns of executables. How does malware propagate ? Does it form spikes over time ? Does it resemble the propagation pattern of benign files, such as software patches ? Does it spread uniformly over countries ? How long does it take for a URL that distributes malware to be detected and shut down ? In this work, we answer these questions. Finally we discover the SharkFin temporal propagation pattern of executable files, the GeoSplit pattern in the geographical spread of machines that report executables to Symantec 's servers, the Periodic Power Law ( Ppl ) distribution of the life-time of URLs, and we show how to efficiently extrapolate crucial properties of the data from a small sample. By analyzing patterns from 22 million malicious ( and benign ) files, 6 million hosts worldwide during the month of June 2011, We conduct this study using the WINE database available at Symantec Research Labs Additionally, we explore the research questions raised by sampling on such large databases of executables ; the importance of studying the implications of sampling is twofold : First, sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second, because every such data collection can be perceived as a sample of the real world.
2K_test_779	Given a large cloud of multi-dimensional points, and an off-theshelf outlier detection method, why does it take a week to finish ? to eliminate the problem. We discovered that duplicate points create subtle issues, that the literature has ignored : if dmax is the multiplicity of the most over-plotted point, typical algorithms are quadratic on dmax, we show that our methods give either exact results, or highly accurate approximate ones. We propose several ways we report wall-clock times and our time savings ; and.
2K_test_780	Counterfactual Regret Minimization ( CFR ) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games, CFR is an iterative algorithm that repeatedly traverses the game tree, updating regrets at each information set. An improvement that prunes any path of play in the tree, and its descendants that has negative regret. Show an order of magnitude speed improvement, and the relative speed improvement increases with the size of the game. We introduce to CFR It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive, had that path been explored on every iteration The new algorithm maintains CFR 's convergence guarantees while making iterations significantly fastereven if previously known pruning techniques are used in the comparison, This improvement carries over to CFR+, a recent variant of CFR.
2K_test_781	Influence maximization is a problem of maximizing the aggregate adoption of products, technologies or even beliefs, Most past algorithms leveraged an assumption of submodularity that captures diminishing returns to scale. While submodularity is natural in many domains, early stages of innovation adoption are often better characterized by convexity, which is evident for renewable technologies, such as rooftop solar to scale over a finite time horizon, in which the decision maker faces a budget constraint. Prove that this policy is optimal in a very general setting, that the proposed `` best-time '' algorithm remains quite effective the `` best-time '' policy becomes suboptimal, and is significantly outperformed by our more general heuristic. We formulate a dynamic influence maximization problem under increasing returns We propose a simple algorithm in this model which chooses the best time period to use up the entire budget ( called Best-Stage ), and We also propose a heuristic algorithm for this problem of which Best-Stage decision is a special case. Additionally we experimentally verify even as we relax the assumptions under which optimality can be proved, However we find that when we add a `` learning-by-doing '' effect, in which the adoption costs decrease not as a function of time, but as a function of aggregate adoption.
2K_test_782	Pareto efficiency is a widely used property in solution concepts for cooperative and non -- cooperative game -- theoretic settings and, more generally in multi -- objective problems. However finding or even approximating ( when the objective functions are not convex ) the Pareto curve is hard, Most of the literature focuses on computing concise representations to approximate the Pareto curve or on exploiting evolutionary approaches to generate approximately Pareto efficient samples of the curve for game-theoretic solution concepts that incorporate Pareto efficiency. In this paper we show that the Pareto curve of a bimatrix game can be found exactly in polynomial time and that it is composed of a polynomial number of pieces, Furthermore each piece is a quadratic function We use this result to provide algorithms.
2K_test_783	Extensive-form games are a powerful tool for modeling a large range of multiagent scenarios Finally we discuss how our theory applies to several practical problems for which no solution quality bounds could be derived before. However most solution algorithms require discrete, In contrast many real-world domains require modeling with continuous action spaces, This is usually handled by heuristically discretizing the continuous action space without solution quality bounds, In this paper we address this issue, for providing bounds on solution quality for discretization of continuous action spaces in extensive-form games. Leveraging recent results on abstraction solution quality, we develop the first framework For games where the error is Lipschitz-continuous in the distance of a continuous point to its nearest discrete point, we show that a uniform discretization of the space is optimal, When the error is monotonically increasing in distance to nearest discrete point, we develop an integer program for finding the optimal discretization when the error is described by piecewise linear functions, This result can further be used to approximate optimal solutions to general monotonic error functions.
2K_test_784	Kidney exchange where candidates with organ failure trade incompatible but willing donors, is a life-saving alternative to the deceased donor waitlist, which has inadequate supply to meet demand We conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view. While fielded kidney exchanges see huge benefit from altruistic kidney donors ( who give an organ without a paired needy candidate ), a significantly higher medical risk to the donor deters similar altruism with livers. In this paper we begin by proposing the idea of liver exchange, and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level, We then explore cross-organ donation where kidneys and livers can be bartered for each other, We show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges ; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool. We support this result experimentally on demographically accurate multi-organ exchanges.
2K_test_785	The human brain is widely hypothesized to construct inner beliefs about how the world works, It is thought that we need this conception to coordinate our movements and anticipate rapid events that go on around us, A driver for example, needs to predict how the car should behave in response to every turn of the steering wheel and every tap on the brake, But on icy roads, these predictions will often not reflect how the car would behave, Applying the brakes sharply in these conditions could send the car skidding uncontrollably rather than stopping, The brain constructs such inner beliefs over time through experience and learning Taken together, this work provides a framework for understanding how the brain transforms sensory information into instructions for movement, The findings could also help improve the performance of brain-machine interfaces and suggest how we can learn new skills more rapidly and proficiently in everyday life. In general a mismatch between ones inner beliefs and reality is thought to cause errors and accidents, Yet this compelling hypothesis has not yet been fully investigated, investigated this hypothesis To study this learning process. The monkeys cursor movements were remarkably precise, In fact the experiment showed that the monkeys could internally predict their cursor movements just as a driver predicts how a car will move when turning the steering wheel These findings indicate that the monkeys have likely developed inner beliefs to predict how their neural signals drive the cursor, and that these beliefs helped coordinate their performance, In addition when the monkeys did make mistakes, their neural signals were not entirely wrongin fact they were typically consistent with the monkeys inner beliefs about how the cursor moves, A mismatch between these inner beliefs and reality explained most of the monkeys mistakes, This experiment uncovered that, during the course of learning, the monkeys inner beliefs realigned to better match the movements of the new cursor. By conducting a brain-machine interface experiment, In this experiment neural signals from the brains of two rhesus macaques were recorded using arrays of electrodes and translated into movements of a cursor on a computer screen, The monkeys were then trained to mentally move the cursor to hit targets on the screen, next conducted an experiment in which the cursor moved in a way that was substantially different from the monkeys inner beliefs.
2K_test_786	A multi-faceted graph defines several facets on a set of nodes, Each facet is a set of edges that represent the relationships between the nodes in a specific context. Mining multi-faceted graphs have several applications, including finding fraudster rings that launch advertising traffic fraud attacks, tracking IP addresses of botnets over time, analyzing interactions on social networks and co-authorship of scientific papers that does soft clustering on individual facets, to discover communities across facets. Where NeSim is shown to be superior to MCL, JP and AP the well-established clustering algorithms We also report the success stories of MuFace in finding advertisement click rings. We propose NeSim a distributed efficient clustering algorithm We also propose optimizations to further improve the scalability, the efficiency and the clusters quality We employ generalpurpose graph-clustering algorithms in a novel way Due to the qualities of NeSim, we employ it as a backbone in the distributed MuFace algorithm, which discovers multi-faceted communities. We evaluate the proposed algorithms on several real and synthetic datasets.
2K_test_787	The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm. Which enables CFR to scale to dramatically larger abstractions and numbers of cores, for generating such abstractions. It won the 2014 Annual Computer Poker Competition, beating each opponent with statistical significance. We introduce a distributed version of the most commonly used equilibrium-finding algorithm, counterfactual regret minimization ( CFR ), The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint We introduce an algorithm while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and the earth-mover's-distance similarity metric, Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency, Prior approaches run slowly on this architecture Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency, Finally we introduce a family of post-processing techniques that outperform prior ones. We applied these techniques to generate an agent for two-player no-limit Texas Hold'em.
2K_test_788	How can we succinctly describe a million-node graph with a few simple sentences ? Given a large graph, how can we find its most `` important '' structures, so that we can summarize it and easily visualize it ? How can we measure the `` importance '' of a set of discovered subgraphs in a large graph ?. Starting with the observation that real graphs often consist of stars, bipartite cores cliques and chains, our main idea is to find the most succinct description of a graph in these `` vocabulary '' terms. To this end we first mine candidate subgraphs using one or more graph partitioning algorithms Next, we identify the optimal summarization using the minimum description length MDL principle, picking only those subgraphs from the candidates that together yield the best lossless compression of the graph-or, equivalently that most succinctly describe its adjacency matrix.
2K_test_789	For generating interpretable descriptions of inputs that cause faults in high-dimensional software interfaces. Demonstrates superior performance and reliability compared to a basic decision tree approach We also briefly discuss how the method has assisted in debugging a commercial autonomous ground vehicle system. We propose a method Our method models the set of fault-triggering inputs as a Cartesian product and identifies this set by actively querying the system under test, The active sampling scheme is very efficient in the common case that few fields in the interface are relevant to causing the fault This scheme also solves the problem of efficiently finding sufficient examples to model rare faults, which is problematic for other learning-based methods, Compared to other techniques, ours requires no parameter turning or post-processing in order to produce useful results. We analyze the method qualitatively, theoretically and empirically An experimental evaluation.
2K_test_790	Latent Variable Models ( LVMs ) are a large family of machine learning models providing a principled and effective way to extract underlying patterns, structure and knowledge from observed data. Due to the dramatic growth of volume and complexity of data, several new challenges have emerged and can not be effectively addressed by existing LVMs : ( 1 ) How to capture long-tail patterns that carry crucial information when the popularity of patterns is distributed in a power-law fashion ? ( 2 ) How to reduce model complexity and computational cost without compromising the modeling power of LVMs ? ( 3 ) How to improve the interpretability and reduce the redundancy of discovered patterns ? To addresses the three challenges discussed above. We show that the monotonicity of the lower bound is closely aligned with the MAR to qualify the lower bound as a desirable surrogate of the MAR we demonstrate that MAR can effectively capture long-tail patterns, reduce model complexity without sacrificing expressivity and improve interpretability. We develop a novel regularization technique for LVMs, which controls the geometry of the latent space during learning to enable the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually different from each other, to accomplish long-tail coverage, low redundancy and better interpretability We propose a mutual angular regularizer ( MAR ) to encourage the components in LVMs to have larger mutual angles, The MAR is non-convex and non-smooth, entailing great challenges for optimization, To cope with this issue, we derive a smooth lower bound of the MAR and optimize the lower bound instead. Using neural network ( NN ) as an instance, we analyze how the MAR affects the generalization performance of NN On two popular latent variable models -- - restricted Boltzmann machine and distance metric learning.
2K_test_791	That is able to jointly deal with the presence of facial pose variation, partial occlusion of the face, and varying illumination and expressions. And exhibits a higher fitting accuracy on all of them. We propose a facial alignment algorithm Our approach proceeds from sparse to dense landmarking steps using a set of specific models trained to best account for the shape and texture variation manifested by facial landmarks and facial shapes across pose and various expressions We also propose the use of a novel $ \ell _1 $ -regularized least squares approach that we incorporate into our shape model, which is an improvement over the shape model used by several prior Active Shape Model ( ASM ) based facial landmark localization algorithms. Our approach is compared against several state-of-the-art methods on many challenging test datasets.
2K_test_792	To capture the relevant rotational and translation invariances in geometric data. Showing good performance for modeling previously unseen molecular configurations we show substantial improvement over the state of the art in molecular energy optimization. Motivated by problems such as molecular energy prediction, we derive an ( improper ) kernel between geometric inputs, that is able Since many physical simulations based upon geometric data produce derivatives of the output quantity with respect to the input positions, we derive an approach that incorporates derivative information into our kernel learning, We further show how to exploit the low rank structure of the resulting kernel matrices to speed up learning. Finally we evaluated the method in the context of molecular energy prediction, Integrating the approach into a Bayesian optimization.
2K_test_793	The leading approach for computing strong game-theoretic strategies in large imperfect-information games is to first solve an abstracted version of the game offline, then perform a table lookup during game play. A modification to this approach for performing endgame solving in large imperfect-information games, for evaluating the performance of an agent that uses endgame solving. Show that our algorithm leads to significantly stronger performance against the strongest agents from the 2013 AAAI Annual Computer Poker Competition. We consider where we solve the portion of the game that we have actually reached in real time to a greater degree of accuracy than in the initial computation, We call this approach endgame solving, Theoretically we show that endgame solving can produce highly exploitable strategies in some games ; however, we show that it can guarantee a low exploitability in certain games where the opponent is given sufficient exploitative power within the endgame, Furthermore despite the lack of a general worst-case guarantee, we describe benefits of endgame solving, We present an efficient algorithm and present a new variance-reduction technique. Experiments on no-limit Texas Hold'em.
2K_test_794	The proliferation of mobile technologies makes it possible for mobile advertisers to go beyond the realtime snapshot of the static location and contextual information about consumers, This indicates closely targeted mobile ads may constrict consumer focus and significantly reduce the impulsive purchase behavior, Our finding suggests marketers should carefully design mobile advertising strategy, depending on different business contexts. That leverages full information on consumers offline moving trajectories. We found the new mobile trajectory-based advertising is significantly more effective for focal advertising store compared to several existing baselines, It is especially effective in attracting highincome consumers, Interestingly it becomes less effective during the weekend. In this study we propose a novel mobile advertising strategy. To evaluate the effectiveness of this strategy, we design a large-scale randomized field experiment in a large shopping mall in Asia based on 83, 370 unique user responses for two weeks in 2014.
2K_test_795	Simultaneously achieving high level of faithfulness and expressiveness is very rare among other methods. For extreme face illumination normalization. The illumination normalized faces using our proposed Pokerface not only exhibit very high fidelity against neutrally illuminated face, but also allow for a significant improvement in face verification experiments using even the simplest classifier. We propose a new method called the Pokerface The Pokerface is a two-phase approach, It first aims at maximizing the minimum gap between adjacently-valued pixels while keeping the partial ordering of the pixels in the face image under extreme illumination condition, an intuitive effort based on order theory to unveil the underlying structure of a dark image, This optimization can be formulated as a feasibility search problem and can be efficiently solved by linear programming, It then smooths the intermediate representation by repressing the energy of the gradient map, The smoothing step is carried out by total variation minimization and sparse approximation. These conclusions are drawn after benchmarking our algorithm against 22 prevailing illumination normalization techniques on both the CMU Multi-PIE database and Extended YaleB database that are widely adopted for face illumination problems.
2K_test_796	Living organisms adapt to challenges through evolution and adaptation, Potential application classes include therapeutics at the population, individual and molecular levels ( drug design ), as well as cell repurposing and synthetic biology. This has proven to be a key difficulty in developing therapies, since the organisms develop resistance. Propose the wild idea of computational game theory for ( typically incomplete-information ) multistage games and opponent exploitation techniques, A sequential contingency plan for steering is constructed computationally for the setting at hand In the biological context, the opponent ( e, a disease ) has a systematic handicap because it evolves myopically, This can be exploited by computing trapping strategies that cause the opponent to evolve into states where it can be handled effectively.
2K_test_797	Content-based medical image retrieval ( CBMIR ) is an active research area for disease diagnosis and treatment but it can be problematic given the small visual variations between anatomical structures. And it showed improved retrieval accuracy and efficiency. We propose a retrieval method based on a bag-of-visual-words ( BoVW ) to identify discriminative characteristics between different medical images with Pruned Dictionary based on Latent Semantic Topic description We refer to this as the PD-LST retrieval, Our method has two main components, First we calculate a topic-word significance value for each visual word given a certain latent topic to evaluate how the word is connected to this latent topic, The latent topics are learnt, based on the relationship between the images and words, and are employed to bridge the gap between low-level visual features and high-level semantics, These latent topics describe the images and words semantically and can thus facilitate more meaningful comparisons between the words, Second we compute an overall-word significance value to evaluate the significance of a visual word within the entire dictionary, We designed an iterative ranking method to measure overall-word significance by considering the relationship between all latent topics and words, The words with higher values are considered meaningful with more significant discriminative power in differentiating medical images. We evaluated our method on two public medical imaging datasets.
2K_test_798	For locating tracking and monitoring resource in large-scale facilities is disclosed herein to effectively eliminate long-range communications, In order to perform resource localization and tracking. A system and method The system is based on a sensor network and is efficient, scalable and requires only short-range communication, The system allows for sensor-to-sensor communication as well as the traditional sensor-to-anchor communication, the present invention pairs each resource with an inexpensive, low-powered sensor possessing minimal communication and computation capabilities, The sensors communicate with only nearby resources or anchors and those resources communicate with their nearby resources or anchors until a wireless, linked network of resources and anchors is formed.
2K_test_799	Deep learning ( DL ) has achieved notable successes in many machine learning tasks, A number of frameworks have been developed to expedite the process of designing and training deep neural networks ( DNNs ), such as Caffe Torch and Theano. Currently they can harness multiple GPUs on a single machine, but are unable to use GPUs that are distributed across multiple machines ; as even average-sized DNNs can take days to train on a single GPU with 100s of GBs to TBs of data, distributed GPUs present a prime opportunity for scaling up DL, However the limited bandwidth available on commodity Ethernet networks presents a bottleneck to distributed GPU training, and prevents its trivial realization To investigate how to adapt existing frameworks to efficiently support distributed GPUs. Show that Poseidon converges to same objectives as a single machine, and achieves state-of-art training speedup Poseidon with 8 nodes achieves better speedup and competitive accuracy to recent CPU-based distributed systems such as Adam and Le et al, which use 10s to 1000s of nodes. We propose Poseidon a scalable system architecture for distributed inter-machine communication in existing DL frameworks We integrate Poseidon with Caffe Poseidon features three key contributions that accelerate DNN training on clusters : ( 1 ) a three-level hybrid architecture that allows Poseidon to support both CPU-only and GPU-equipped clusters, ( 2 ) a distributed wait-free backpropagation ( DWBP ) algorithm to improve GPU utilization and to balance communication, and ( 3 ) a structure-aware communication protocol ( SACP ) to minimize communication overheads. And evaluate its performance at training DNNs for object recognition, We empirically across multiple models and well-established datasets using a commodity GPU cluster of 8 nodes ( e, 5x speedup on AlexNet, 4x on GoogLeNet 4x on CIFAR-10 ), On the much larger ImageNet22K dataset.
2K_test_800	We investigate the welfare implications and the allocative effects of different consumer data '' handling regimes in online targeted advertisin g. We find that there exist conditions under which the intermediary obtains the highest proportion of benefits from targeting and, in general the intermediary 's incentives regarding the type of consumer information to be used for targeting are misaligned with the incentives of firms and/or consumers Furthermore, consumers ' surplus from targeting is higher when only specific types of personal information are made available during the targeting process. We develop a three '' players model that includes firms, consumers and an intermediary `` the ad exchange. And analyze three scenarios that differ in the type of consumers ' data available during the targeting : a case in which only the horizontal information ( consumers ' brand preferences ) is available ; a case in which only vertical information ( consumers ' purchasing power ) is available ; a case in which both pieces of information are available.
2K_test_801	The kernel-based nonparametric approach proposed by Nguyen, Wainwright and Jordan is further investigated for decentralized detection. Results are provided to demonstrate the advantages and properties of the proposed approach based on weighted kernel. In contrast with the uniform kernel used in the previous work, a weighted kernel is proposed, where weight parameters serve to selectively incorporate sensors information into the fusion centers decision rule based on quality of sensors observations, Furthermore weight parameters also serve as sensor selection parameters with nonzero parameters corresponding to sensors being selected, By introducing the $ l_1 $ regularization on weight parameters into the risk minimization framework, sensor selection is jointly performed with decision rules for sensors and the fusion center with the resulting optimal decision rule having only sparse nonzero weight parameters A gradient projection algorithm and a Gauss-Seidel algorithm are developed to solve the risk minimization problem, which is nonconvex and both algorithms are shown to converge to critical points, Conditions on the sample complexity to guarantee asymptotically small estimation error are characterized based on analysis of Rademacher complexity, Connection between the probability of error and the risk function is also studied.
2K_test_802	We ultimately envision this technique being integrated into future smartwatches, allowing hand gestures and direct touch manipulation to work synergistically to support interactive tasks on small screens. To recover the interior impedance geometry of a user 's arm. Our wrist location achieved 97 % and 87 % accuracies on these gesture sets respectively, while our arm location achieved 93 % and 81 %. We present Tomo a wearable, low-cost system using Electrical Impedance Tomography ( EIT ) This is achieved by measuring the cross-sectional impedances between all pairs of eight electrodes resting on a user 's skin, Our approach is sufficiently compact and low-powered that we integrated the technology into a prototype wrist- and armband, which can monitor and classify gestures in real-time. We conducted a user study that evaluated two gesture sets, one focused on gross hand gestures and another using thumb-to-finger pinches.
2K_test_803	Next-generation information technologies will process unprecedented amounts of loosely structured data that overwhelm existing computing systems. Improves the energy efficiency of abundant-data applications. N3XT by using new logic and memory technologies, 3D integration with fine-grained connectivity, and new architectures for computation immersed in memory.
2K_test_804	The rise of big data has led to new demands for machine learning ( ML ) systems to learn complex models, with millions to billions of parameters, that promise adequate capacity to digest massive datasets and offer powerful predictive analytics ( such as high-dimensional latent features, intermediate representations and decision functions ) thereupon, we present opportunities for ML researchers and practitioners to further shape and enlarge the area that lies between ML and systems. In order to run ML algorithms at such scales, on a distributed cluster with tens to thousands of machines, it is often the case that significant engineering efforts are requiredand one might fairly ask whether such engineering truly falls within the domain of ML research Taking the view that big ML systems can benefit greatly from ML-rooted statistical and algorithmic insightsand that ML researchers should therefore not shy away from such systems designwe How can an ML program be distributed over a cluster ? How can ML computation be bridged with inter-machine communication ? How can such communication be performed ? What should be communicated between machines ?. Discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions, These principles and strategies span a continuum from application, to engineering and to theoretical research and development of big ML systems and architectures, with the goal of understanding how to make them efficient, generally applicable and supported with convergence and scaling guarantees, They concern four key questions that traditionally receive little attention in ML research : By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs, and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks.
2K_test_805	Semantic search or text-to-video search in video is a novel and challenging problem in information and multimedia retrieval, We share our observations and lessons in building such a state-of-the-art system, which may be instrumental in guiding the design of the future system for video search and analysis. Existing solutions are mainly limited to text-to-text matching, in which the query words are matched against the user-generated metadata, This kind of text-to-text search, though simple is of limited functionality as it provides no understanding about the video content. The novelty and practicality are demonstrated by where the proposed system achieves the best performance. This paper presents a state-of-the-art system for event search without any user-generated metadata or example videos, known as text-to-video search, The system relies on substantial video content understanding and allows for searching complex events over a large collection of videos, The proposed text-to-video search can be used to augment the existing text-to-text search for video. The evaluation in NIST TRECVID 2014.
2K_test_806	For dynamic barter marketsand kidney exchange. We show that the mechanism results in significant gains on data from a national kidney exchange that includes 59 % of all US transplant centers. We present a credit-based matching mechanism in particularthat is both strategy proof and efficient, that is it guarantees truthful disclosure of donor-patient pairs from the transplant centers and results in the maximum global matching Furthermore, the mechanism is individually rational in the sense that, in the long run, it guarantees each transplant center more matches than the center could have achieved alone, The mechanism does not require assumptions about the underlying distribution of compatibility graphsa nuance that has previously produced conflicting results in other aspects of theoretical kidney exchange Our results apply not only to matching via 2-cycles : the matchings can also include cycles of any length and altruist-initiated chains, which is important at least in kidney exchanges, The mechanism can also be adjusted to guarantee immediate individual rationality at the expense of economic efficiency, while preserving strategy proofness via the credits, This circumvents a well-known impossibility result in static kidney exchange concerning the existence of an individually rational, strategy-proof and maximal mechanism.
2K_test_807	The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm. Which enables CFR to scale to dramatically larger abstractions and numbers of cores, for generating such abstractions. That won the 2014 Annual Computer Poker Competition, beating each opponent with statistical significance. We introduce a distributed version of the most commonly used equilibrium-finding algorithm, counterfactual regret minimization ( CFR ), The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint We introduce an algorithm while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and earth-mover 's distance, Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency, Prior approaches run slowly on this architecture, Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency, Finally we introduce a family of post-processing techniques that outperform prior ones. We applied these techniques to generate an agent for two-player no-limit Texas Hold'em.
2K_test_808	For lips/mouth segmentation in the wild. The results from our studies indicate that the proposed SC-FAC model is reliable and accurately perform prior shape weak object segmentation, The average performance of the mouth segmentation using proposed SC-FAC on 1918 images from the MBGC database under different illuminations, expressions and complex background reaches to a Precision of 91, 30 % a Recall of 91, 32 % and an F-measure of 90. In this paper we propose a novel joint formulation of feature-based active contour ( FAC ) and prior shape constraints ( CS Our proposed SC-FAC model is able to robustly segment the lips/mouth that belongs to a given mouth shape space while minimizing the energy functional, The shape space is defined by a 2D Modified Active Shape Model ( MASM ) whereas the active contour model is based on the Chan-Vese functional Our SC-FAC energy functional is able to overcome the drawback of noise while minimizing the fitting forces under the shape constraints HighlightsPropose a novel joint formulation of contour and shape for lips segmentation, Robustly segment lips under challenging environment and complex background, The energy functional is composed of 5 terms based on the Chan-Vese functional, The shape space is defined by a 2D, Experiments are conducted from the MBGC, VidTIMIT JAFFE and LFW databases. We conducted our experiments on images captured under challenging conditions such as varying illumination, low contrast facial expression, low resolution blurring wearing beard/moustache and cosmetic affection from the MBGC, VidTIMIT JAFFE and LFW databases.
2K_test_809	Bayesian theory has provided a compelling conceptualization for perceptual inference in the brain, Central to Bayesian inference is the notion of statistical priors, To understand the neural mechanisms of Bayesian inference, we need to understand the neural representation of statistical regularities in the natural environment These findings demonstrate that there is a relationship between the functional connectivity observed in the visual cortex and the statistics of natural scenes, They also suggest that the Boltzmann machine can be a viable model for conceptualizing computations in the visual cortex and, as such can be used to predict neural circuits in the visual cortex from natural scene statistics. In this paper we investigated empirically how statistical regularities in natural 3D scenes are represented in the functional connectivity of disparity-tuned neurons in the primary visual cortex of primates, to learn from 3D natural scenes. And found that the units in the model exhibited cooperative and competitive interactions, forming a disparity association field, analogous to the contour association field The cooperative and competitive interactions in the disparity association field are consistent with constraints of computational models for stereo matching, and found the results to be consistent with neurophysiological data in terms of the functional connectivity measurements between disparity-tuned neurons in the macaque primary visual cortex. A Boltzmann machine model. We applied In addition, we simulated neurophysiological experiments on the model.
2K_test_810	AbstractEngineering analysis to quantify the effects of earthquake forces on the structural strength of components requires determining the damage mode and severity of the components. The analysis requires strength computations and visual damage assessments, which are information intensive, potentially error-prone and slow, to support the engineering analysis of reinforced concrete structures, to perform strength analysis and visual assessment tasks. This study develops a building-information-modeling ( BIM ) based approach In the proposed approach, the damage information is represented along with the geometric, topological and structural information, Transformation and reasoning mechanisms are proposed to utilize the information contained in the BIM. The approach is validated on a case study building, which contains 42 damaged piers and spandrels.
2K_test_811	Of optimal voting under adversarial noise. Show that our approach produces significantly more accurate rankings than alternative approaches. We present the first model From this viewpoint, voting rules are seen as error-correcting codes : their goal is to correct errors in the input rankings and recover a ranking that is close to the ground truth We derive worst-case bounds on the relation between the average accuracy of the input votes, and the accuracy of the output ranking. Empirical results from real data.
2K_test_812	The use of deductive techniques, such as theorem provers, has several advantages in safety verification of hybrid systems. There is often a gap, however between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide, To address this deficiency that allows the theorem prover KeYmaera to locally reason about behaviors. We present an extension to the deductive verification framework of differential dynamic logic by leveraging forward invariant sets provided by external methods, such as numerical techniques and designer insights Our key contribution is a new inference rule, the forward invariant cut rule, introduced into the proof calculus of KeYmaera. We demonstrate the cut rule in action on an example involving an automotive powertrain control systems, in which we make use of a simulation-driven numerical technique to compute a local barrier function.
2K_test_813	Kernel methods are ubiquitous tools in machine learning, They have proven to be effective in many domains and tasks. Yet kernel methods often require the user to select a predefined kernel to build an estimator with, However there is often little reason for the a priori selection of a kernel, Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly effected by the choice of kernel, Furthermore when directly applying kernel methods, one typically needs to compute a $ N \times N $ Gram matrix of pairwise kernel evaluations to work with a dataset of $ N $ instances, The computation of this Gram matrix precludes the direct application of kernel methods on large datasets for scalable learning of kernels. Furthermore we show that BaNK outperforms several other scalable approaches for kernel learning. In this paper we introduce Bayesian nonparmetric kernel ( BaNK ) learning, a generic data-driven framework We show that this framework can be used for performing both regression and classification tasks and scale to large datasets. On a variety of real world datasets.
2K_test_814	When solving extensive-form games with large action spaces, typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective, When this occurs a procedure is needed to interpret actions of the opponent that fall outside of our abstraction ( by mapping them to actions in our abstraction ), This is called an action translation mapping. Prior action translation mappings have been based on heuristics without theoretical justification, We show that the prior mappings are highly exploitable and that most of them violate certain natural desiderata, that satisfies these desiderata. Our mapping performs competitively with the prior mappings. We present a new mapping and has significantly lower exploitability than the prior mappings Furthermore, we observe that the cost of this worst-case performance benefit ( low exploitability ) is not high in practice ; We also observe several paradoxes that can arise when performing action abstraction and translation ; for example, we show that it is possible to improve performance by including suboptimal actions in our abstraction and excluding optimal actions. Against no-limit Texas Hold'em agents submitted to the 2012 Annual Computer Poker Competition.
2K_test_815	Problems of this nature arise in formal verification of continuous and hybrid dynamical systems, where there is an increasing need for methods to expedite formal proofs. This paper studies sound proof rules for checking positive invariance of algebraic and semi-algebraic sets, that is sets satisfying polynomial equalities and those satisfying finite boolean combinations of polynomial equalities and inequalities, under the flow of polynomial ordinary differential equations. The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting. We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of benchmarks.
2K_test_817	How can we efficiently decompose a tensor into sparse factors, when the data do not fit in memory ?, enabling reproducibility of our work. Tensor decompositions have gained a steadily increasing popularity in data-mining applications ; however, the current state-of-art decomposition algorithms operate on main memory and do not scale to truly large datasets, for speeding up tensor decompositions. Indicate over 90p sparser outputs and 14 times faster execution, with approximation error close to the current state of the art irrespective of computation and memory requirements, demonstrating its effectiveness for data-mining practitioners. In this work we propose P ar C ube, a new and highly parallelizable method that is well suited to produce sparse approximations In particular, we are the first to analyze the very large N ell dataset using a sparse tensor decomposition, demonstrating that P ar C ube enables us to handle effectively and efficiently very large datasets Finally, we make our highly scalable parallel implementation publicly available. Experiments with even moderately large data We provide theoretical guarantees for the algorithms correctness and we experimentally validate our claims through extensive experiments, including four different real world datasets ( E nron, L bnl F acebook and N ell ).
2K_test_818	The current practice of analyzing the structural integrity of embankment dams relies primarily on manual a posteriori analysis of instrument data by engineers, leaving much room for improvement through the application of advanced data analysis techniques. To propose which data analytics are appropriate for various anomaly scenarios as well as piezometer locations. In general KL performs better than MPCA and AR, and delivers more consistent results throughout the different piezometers and anomaly scenarios, Given that KL is a nonparametric technique, the authors conclude that the prior assumptions about piezometer data do not always provide the best performance for anomaly prediction. In this research different types of anomaly detection techniques are examined in an effort Moreover, both the parametric ( Auto Regressive AR and Moving Principal Component Analysis MPCA ) and nonparametric ( Kullback-Leibler Divergence KL ) techniques are applied in order to test if the widely-held assumptions about piezometer data, linearity between piezometer data and pool levels, as well as normally distributed piezometer data, are necessary in the anomaly detection task.
2K_test_819	Modeling cell shape variation is critical to our understanding of cell biology The open-source tools provide a powerful basis for future studies of the molecular basis of cell organization. Previous work has demonstrated the utility of nonrigid image registration methods for the construction of nonparametric nuclear shape models in which pairwise deformation distances are measured between all shapes and are embedded into a low-dimensional shape space, to predict cell and nuclear shapes, To reduce the computational cost of learning these models. We find that these are frequently dependent on each other and show that tagged C1QBP reduces the correlation between cell and nuclear shape. Use this as the motivation for the development of combined cell and nuclear shape space models, extending nonparametric cell representations to multiple-component three-dimensional cellular shapes and identifying modes of joint shape variation We learn a first-order dynamics model given shapes at a previous time point. Using these methods we explore the relationship between cell shape and nuclear shape, We use this to determine the effects of endogenous protein tags or drugs on the shape dynamics of cell lines and we demonstrate the ability to reconstruct shape spaces using a fraction of computed pairwise distances.
2K_test_821	Building automation systems are believed to hold the key to significantly reducing the average energy consumption of our residential and commercial building stock, which in the U, is responsible for 41 % of the total annual energy use in 2014, As these systems become more widespread and inexpensive, the complexity and challenges associated with their installation, maintenance and upkeep will increase, One of the primary challenges is the generation and update of the meta-data associated with the sensors and control points distributed throughout the facility Previous research has attempted to reduce the human input required to perform these activities, by leveraging different signal processing and statistical analysis approaches to infer the sensor types and locations from measurements and/or tags obtained through a BAS, provide recommendations for future work in this area. However because of the relatively small sample size, the feasibility of applying these type approaches on large buildings, as well as their generalizability, remain as unsolved questions to learn from BAS measurement data in a semi-automated way. Show the feasibility of applying data driven approaches in the real world, We present the results of our study and. In this paper we propose a meta-data inference framework. Furthermore we evaluate the framework on two large buildings instrumented with thousands sensors and.
2K_test_822	This paper considers the dynamics of edges in a network. And show that it converges to a stationary distribution, In particular we show that, depending on the local dynamical rule, different network substructures such as hub or triangle subgraphs, are more prone to failure. The Dynamic Bond Percolation ( DBP ) process models, through stochastic local rules, the dependence of an edge $ ( a, b ) $ in a network on the states of its neighboring edges Unlike previous models, DBP does not assume statistical independence between different edges In applications, this means for example that failures of transmission lines in a power grid are not statistically independent, or alternatively relationships between individuals ( dyads ) can lead to changes in other dyads in a social network. We consider the time evolution of the probability distribution of the network state, the collective states of all the edges ( bonds ) We use this distribution to study the emergence of global behaviors like consensus ( i, catastrophic failure or full recovery of the entire grid ) or coexistence ( i, some failed and some operating substructures in the grid.
2K_test_823	Image clustering and visual codebook learning are two fundamental problems in computer vision and they are tightly related, On one hand a good codebook can generate effective feature representations which largely affect clustering performance, On the other hand, class labels obtained from image clustering can serve as supervised information to guide codebook learning. Traditionally these two processes are conducted separately and their correlation is generally ignored, to simultaneously perform image clustering and codebook learning. Demonstrate the effectiveness of two models. In this paper we propose a Double Layer Gaussian Mixture Model ( DLGMM ) In DLGMM, two tasks are seamlessly coupled and can mutually promote each other, Cluster labels and codebook are jointly estimated to achieve the overall best performance, To incorporate the spatial coherence between neighboring visual patches, we propose a Spatially Coherent DL-GMM which uses a Markov Random Field to encourage neighboring patches to share the same visual word label, We use variational inference to approximate the posterior of latent variables and learn model parameters. Experiments on two datasets.
2K_test_824	While prior research has studied the motivations of individuals to consume content on social media platforms Implications for research and practice are also discussed. Limited work exists on how contributors are motivated to create content We examine the role of peer influence in content production on YouTube, where content creators are competing for attention to analyze discrete choice decisions ( such as creating content or not ) in a networked environment to identify peer influence among content creators on YouTube. Given that content creation efforts are driven not only by their personal preferences, but also by the content creation decisions of others in the network neighbors, we develop a new method with panel data, We face a novel set of big data challenges, both statistical and quantitative, in estimating peer influence, We face computational challenges in that we can not reasonably estimate peer influence over the entire YouTube network, which has billions of nodes, We employ graph sampling methods to address this issue, Identification of social influence in large-scale social networks such as YouTube is difficult due to the interdependence in decisions of users, correlations between the video 's observable and unobservable characteristics and attributes over time These patterns can not be modeled with existing autocorrelation models, We design a new method, the Network Auto-Probit Model with Fixed Effects ( NAFE ).
2K_test_825	Despite the enormous medical impact of cancers and intensive study of their biology, detailed characterization of tumor growth and development remains elusive, This difficulty occurs in large part because of enormous heterogeneity in the molecular mechanisms of cancer progression, both tumor-to-tumor and cell-to-cell in single tumors, Advances in genomic technologies, especially at the single-cell level, are improving the situation, but these approaches are held back by limitations of the biotechnologies for gathering genomic data from heterogeneous cell populations and the computational methods for making sense of those data, One popular way to gain the advantages of whole-genome methods without the cost of single-cell genomics has been the use of computational deconvolution ( unmixing ) methods to reconstruct clonal heterogeneity from bulk genomic data, a key step in the process of accurately deconvolving tumor genomic data and inferring clonal heterogeneity from bulk data. These methods too are limited by the difficulty of inferring genomic profiles of rare or subtly varying clonal subpopulations from bulk data, a problem that can be computationally reduced to that of reconstructing the geometry of point clouds of tumor samples in a genome space, to improve that reconstruction. We show that this new method substantially improves our ability to resolve discrete tumor subgroups. Here we present a new method by better identifying subspaces corresponding to tumors produced from mixtures of distinct combinations of clonal subpopulations We develop a nonparametric clustering method based on medoidshift clustering for identifying subgroups of tumors expected to correspond to distinct trajectories of evolutionary progression. On synthetic and real tumor copy-number data.
2K_test_826	Over the last decade, the looming power wall has spurred a flurry of interest in developing heterogeneous systems with hardware accelerators. The questions then are what and how accelerators should be designed, and what software support is required, that explicitly targets the memory-bounded operations within high performance libraries. On average the legacy code using our proposed MEmory Accelerated Library ( MEALib ) improves performance and energy efficiency for individual operations in Intel 's Math Kernel Library ( MKL ) by 38 and 75, MEALib attains more than 10 better energy efficiency. Our accelerator design approach stems from the observation that many efficient and portable software implementations rely on high performance software libraries with well-established application programming interfaces ( APIs ) We propose the integration of hardware accelerators on 3D-stacked memory The fixed APIs with limited configurability simplify the design of the accelerators, while ensuring that the accelerators have wide applicability, With our software support that automatically converts library APIs to accelerator invocations, an additional advantage of our approach is that library-based legacy code automatically gains the benefit of memory-side accelerators without requiring a reimplementation. For a real-world signal processing application that employs Intel MKL.
2K_test_827	The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees in addressing practical issues such as ( 1 ) unavailability of transformed versions of labelled data and ( 2 ) not observing all transformations. To illustrate and validate our methods. In this paper we study kernels that are invariant to the unitary group while having theoretical guarantees We present a theoretically motivated alternate approach to the invariant kernel SVM Unlike previous approaches to the invariant SVM, the proposed formulation solves both issues mentioned, We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability. We present experiments on the UCI ML datasets.
2K_test_828	The assembly of virus capsids proceeds by a complicated cascade of association and dissociation steps, the great majority of which can not be directly experimentally observed This has made capsid assembly a rich field for computational models. But there are substantial obstacles to model inference for such systems a difficult data-fitting problem because of the high computational cost of simulating assembly trajectories, the stochastic noise inherent to the models, and the limited and noisy data available for fitting We further explore the advantages of alternative data sources through simulation of for monitoring bulk capsid assembly. The results show that advances in both the data and the algorithms can improve model inference More informative data sources lead to high-quality fits for all methods, but DFO methods show substantial advantages on less informative data sources that better represent current experimental practice. Here we describe progress on fitting kinetic rate constants defining capsid assembly models to experimental data a model of time-resolved mass spectrometry data, a technology that can be expected to provide much richer data than previously used static light scattering approaches. We evaluate the merits of data-fitting methods based on derivative-free optimization ( DFO ) relative to gradient-based methods used in prior work.
2K_test_829	To probabilistic segmentation and modeling of time series data for solving the resulting ( large ) optimization problems for estimating recurring clusters. The resulting methods often perform as well or better than existing latent variable models, while being substantially easier to train. We present a convex approach Our approach builds upon recent advances in multivariate total variation regularization, and seeks to learn a separate set of parameters for the distribution over the observations at each time point, but with an additional penalty that encourages the parameters to remain constant over time, We propose efficient optimization methods, and a two-stage procedure under such models, based upon kernel density estimation. Finally we show on a number of real-world segmentation tasks.
2K_test_830	This typically arises in the tourism setting where attractions can often be bundled and sold as a package to visitors, While the problem of predicting future locations given the current and past trajectories is well-established. We propose the problem of predicting a bundle of goods, where the goods considered is a set of spatial locations that an agent wishes to visit. Our predictions show improved accuracies by at least 20, one of which comes from the spatiotemporal analysis domain. We take a radical approach by looking at it from an economic point of view, We view an agent 's past trajectories as revealed preference ( RP ) data, where the choice of locations is a solution to an optimisation problem according to some unknown utility function and subject to the prevailing prices and budget constraint We approximate the prices and budget constraint as the time costs to finish visiting the chosen locations We leverage on a recent line of work that has established algorithms to efficiently learn from RP data ( i, recover the utility functions ) and make predictions of future purchasing behaviours, We adopt and adapt those work to our original setting while incorporating techniques from spatiotemporal analysis. We experiment with real-world trajectory data collected from a theme park, in comparison with the baseline methods.
2K_test_831	Given a large collection of co-evolving online activities, such as searches for the keywords `` Xbox '', `` PlayStation '' and `` Wii '', how can we find patterns and rules ? Are these keywords related ? If so, are they competing against each other ? Can we forecast the volume of user activity for the coming month ?. We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food for mining large-scale co-evolving online activities. Show that ECOWEB is effective, in that it can capture long-range dynamics and meaningful patterns such as seasonalities, and practical in that it can provide accurate long-range forecasts, ECOWEB consistently outperforms existing methods in terms of both accuracy and execution speed. We present ECOWEB ( i, Ecosystem on the Web ), which is an intuitive model designed as a non-linear dynamical system Our second contribution is a novel, parameter-free and scalable fitting algorithm, ECOWEB-FIT that estimates the parameters of ECOWEB. Extensive experiments on real data.
2K_test_832	Software lineage refers to the evolutionary relationship among a collection of software, The goal of software lineage inference is to recover the lineage given a set of program binaries, Software lineage can provide extremely useful information in many security scenarios such as malware triage and software vulnerability tracking. In this paper we systematically study software lineage inference by exploring four fundamental questions not addressed by prior work, First how do we automatically infer software lineage from program binaries ? Second, how do we measure the quality of lineage inference algorithms ? Third, how useful are existing approaches to binary similarity analysis for inferring lineage in reality, and how about in an idealized setting ? Fourth, what are the limitations that any software lineage inference algorithm must cope with ? Towards these goals for automatic software lineage inference of program binaries for scientific assessment of lineage quality. Our results reveal that partial order mismatches and graph arc edit distance often yield the most meaningful comparisons Even without assuming any prior information about the data sets, ILINE proved to be effective in lineage inference -- it achieves a mean accuracy of over 84 % for goodware and over 72 % for malware in our data sets. We build ILINE a system, and also IEVAL a system. We evaluated ILINE on two types of lineage -- straight line and directed acyclic graph -- with large-scale real-world programs : 1, 777 goodware spanning over a combined 110 years of development history and 114 malware with known lineage collected by the DARPA Cyber Genome program We used IEVAL to study seven metrics to assess the diverse properties of lineage.
2K_test_833	With the rise of online social networks and smartphones that record the user 's location, a new type of online social network has gained popularity during the last few years, the so called Location-based Social Networks ( LBSNs ), In such networks users voluntarily share their location with their friends via a check-in, In exchange they get recommendations tailored to their particular location as well as special deals that businesses offer when users check-in frequently, LBSNs started as specialized platforms such as Gowalla and Foursquare, however their immense popularity has led online social networking giants like Facebook to adopt this functionality, The spatial aspect of LBSNs directly ties the physical with the online world, creating a very rich ecosystem where users interact with their friends both online as well as declare their physical ( co- ) presence in various locations. Such a rich environment calls for novel analytic tools that can model the aforementioned types of interactions, By doing so we identify tightly knit, hidden communities of users and locations which they frequent to study the temporal dynamics of hidden communities in LBSNs. In this work we propose to model and analyze LBSNs using Tensors and Tensor Decompositions, powerful analytical tools that have enjoyed great growth and success in fields like Machine Learning, Data Mining and Signal Processing alike, In addition to Tensor Decompositions, we use Signal Processing tools that have been previously used in Direction of Arrival ( DOA ) estimations.
2K_test_834	Prior work has among other techniques, used canonical correlation analysis to project pre-trained vectors in two languages into a common space. This work focuses on the task of finding latent vector representations of the words in a corpus, In particular we address the issue of what to do when there are multiple languages in the corpus. That our method outperforms prior work on multilingual tasks, matches the performance of prior work on monolingual tasks, and scales linearly with the size of the input data ( and thus the number of languages being embedded ). We propose a simple and scalable method that is inspired by the notion that the learned vector representations should be invariant to translation between languages.
2K_test_836	For unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery. Our algorithm performs favorably without the need for careful initialization. We propose a spectral approach Our approach is grammarless we directly learn the bracketing structure of a given sentence without using a grammar model The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples, Although finding the minimal latent tree is NP-hard in general, for the case of projective trees we find that it can be found using bilexical parsing algorithms. Empirically compared to the constituent context model of Klein and Manning ( 2002 ).
2K_test_838	For general convex programming. Show that this often improves running times by an order of magnitude or more vs, existing approaches based on conic solvers. We present Epsilon a system using fast linear and proximal operators, As with existing convex programming frameworks, users specify convex optimization problems using a natural grammar for mathematical expressions, composing functions in a way that is guaranteed to be convex by the rules of disciplined convex programming Given such an input, the Epsilon compiler transforms the optimization problem into a mathematically equivalent form consisting only of functions with ecient proximal operators|an intermediate representation we refer to as prox-ane form, By reducing problems to this form, Epsilon enables solving general convex problems using a large library of fast proximal and linear operators ;. Numerical examples on many popular problems from statistics and machine learning.
2K_test_839	This paper focuses on recursive nonlinear least-squares parameter estimation in multiagent networks. It is shown that, at every network agent, $ \mathcal { CIWNLS } $ leads to consistent parameter estimates the distributed estimator is shown to yield order-optimal convergence rates, as far as the order of pathwise convergence is concerned, the local parameter estimates at each agent are as good as the optimal centralized nonlinear least-squares estimator that requires access to all the observations across all the agents at all times. Where the individual agents observe sequentially over time an independent and identically distributed time-series consisting of a nonlinear function of the true but unknown parameter corrupted by noise A distributed recursive estimator of the consensus+innovations type, namely $ \mathcal { CIWNLS } $, is proposed in which the agents update their parameter estimates at each observation sampling epoch in a collaborative way by simultaneously processing the latest locally sensed information ( innovations ) and the parameter estimates from other agents ( consensus ) in the local neighborhood conforming to a prespecified interagent communication topology. Under rather weak conditions on the connectivity of the interagent communication and a global observability criterion Furthermore, under standard smoothness assumptions on the local observation functions, To benchmark the performance of the $ \mathcal { CIWNLS } $ estimator with that of the centralized nonlinear least-squares estimator, the asymptotic normality of the estimate sequence is established, and the asymptotic covariance of the distributed estimator is evaluated.
2K_test_840	Curriculum learning ( CL ) or self-paced learning ( SPL ) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training, The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes, In CL the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner, However SPL is unable to deal with prior knowledge, rendering it prone to overfitting, In this paper we discover the missing link between CL and SPL. In SPL the curriculum is dynamically determined to adjust to the learning pace of the leaner and propose a unified framework named self-paced curriculum leaning ( SPCL ), SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to `` instructor-student-collaborative '' learning mode, as opposed to `` instructor-driven '' in CL or `` student-driven '' in SPL Empirically, we show that the advantage of SPCL on two tasks.
2K_test_841	Image and video classification research has made great progress through the development of handcrafted local features and learning based features. These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history, However they are typically viewed as distinct approaches. Show that by focusing on the structure of CNNs, rather than end-to-end training methods, we are able to design an efficient and powerful video feature learning algorithm. In this paper we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness, We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks ( CNNs ) share the same convolution-pooling network structure, We then propose a two-stream Convolutional ISA ( ConvISA ) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm, Through custom designed network structures for pixels and optical flow, our method also reflects distinctive characteristics of these two data sources. As an example we study the problem of designing efficient video feature learning algorithms for action recognition, Our experimental results on standard action recognition benchmarks.
2K_test_842	Early detection and precise characterization of emerging topics in text streams can be highly useful in applications such as timely and targeted public health interventions and discovering evolving regional business trends, Many methods have been proposed for detecting emerging events in text streams using topic modeling. However these methods have numerous shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams, to overcome these shortcomings in detecting new spatially compact events in text streams. On both tasks we find that Semantic Scan provides significantly better event detection and characterization accuracy than competing approaches, while providing up to an order of magnitude speedup. In this paper we describe Semantic Scan ( SS ) that has been developed specifically Semantic Scan integrates novel contrastive topic modeling with online document assignment and principled likelihood ratio-based spatial scanning to identify emerging events with unexpected patterns of keywords hidden in text streams, This enables more timely and accurate detection and characterization of anomalous, spatially localized emerging events Semantic Scan does not require manual intervention or labeled training data, and is robust to noise in real-world text data since it identifies anomalous text patterns that occur in a cluster of new documents rather than an anomaly in a single new document. We compare Semantic Scan to alternative state-of-the-art methods such as Topics over Time, Online LDA and Labeled LDA on two real-world tasks : ( i ) a disease surveillance task monitoring free-text Emergency Department chief complaints in Allegheny County, and ( ii ) an emerging business trend detection task based on Yelp reviews.
2K_test_843	Conditional Gaussian graphical models generalize the well-known Gaussian graphical models to conditional distributions to model the output network influenced by conditioning input variables. This paper addresses the problem of scalable optimization for L1-regularized conditional Gaussian graphical models, While highly scalable optimization methods exist for sparse Gaussian graphical model estimation, state-of-the-art methods for conditional Gaussian graphical models are not efficient enough and more importantly, fail due to memory constraints for very large problems that efficiently iterates over two sub-problems. We show that our methods can solve one million dimensional problems to high accuracy in a little over a day on a single machine. In this paper we propose a new optimization procedure based on a Newton method leading to drastic improvement in computation time compared to the previous methods We then extend our method to scale to large problems under memory constraints, using block coordinate descent to limit memory usage while achieving fast convergence. Using synthetic and genomic data.
2K_test_844	Unease over data privacy will retard consumer acceptance of IoT deployments, The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud. This is a direct consequence of the over-centralization of today 's cloud-based IoT hub designs, We propose a solution. That interposes a locally-controlled software component called a privacy mediator on every raw sensor stream Each mediator is in the same administrative domain as the sensors whose data is being collected, and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain This solution necessitates a logical point of presence for mediators within the administrative boundaries of each organization, Such points of presence are provided by cloudlets, which are small locally-administered data centers at the edge of the Internet that can support code mobility The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility.
2K_test_845	For use in data-distributed settings, where each machine only has access to a subset of data and runs VI independently, without communicating with other machines, This type of `` embarrassingly parallel '' procedure has recently been developed for MCMC inference algorithms ; however, in many cases it is not possible to directly extend this procedure to VI methods without requiring certain restrictive exponential family conditions on the form of the model, Furthermore most existing ( nonparallel ) VI methods are restricted to use on conditionally conjugate models, which limits their To combat these issues. And demonstrate our method. We develop a parallel variational inference ( VI ) procedure we make use of the recently proposed nonparametric VI to facilitate an embarrassingly parallel VI procedure that can be applied to a wider scope of models, including to nonconjugate models, We derive our embarrassingly parallel VI algorithm. Analyze our method theoretically empirically on a few nonconjugate models.
2K_test_846	Conclusions : A solution meeting the specification of the use case described above could improve human monitoring efficiency with expedited warning of events requiring follow-up, including otherwise overlooked events with no syndromic indicators, This approach can remove obstacles to collaboration with efficient, minimal data-sharing and without costly overhead. Introduction : We document a funded effort to bridge the gap between constrained scientific challenges of public health surveillance and methodologies from academia and industry. Results : Direct communication between public health problem owners and analytic developers was informative to both groups and constructive for the solution development process, The consultancy achieved refinement of the asyndromic detection challenge and of solution requirements, Participants summarized and evaluated solution approaches and discussed dissemination and collaboration strategies. Materials and Methods : Supported by the Defense Threat Reduction Agency Biosurveillance Ecosystem project, the International Society for Disease Surveillance formed an advisory group to select tractable use case problems and convene inter-disciplinary consultancies to translate analytic needs into well-defined problems and to promote development of applicable solution methods, The initial consultancys focus was a problem originated by the North Carolina Department of Health and its NC DETECT surveillance system : Derive a method for detection of patient record clusters worthy of follow-up based on free-text chief complaints and without syndromic classification. Component tasks are the collection of epidemiologists use case problems, multidisciplinary consultancies to refine them, and dissemination of problem requirements and shareable datasets We describe an initial use case and consultancy as a concrete example and challenge to developers.
2K_test_847	To guide the clustering process, that minimizes the overall number of such pairwise judgments needed. We show that with significantly lower number of pairwise judgments and feature-engineering effort, we can achieve competitive coreference performance. In this paper we define the problem of coreference resolution in text as one of clustering with pairwise constraints where human experts are asked to provide pairwise constraints ( pairwise judgments of coreferentiality ) Further, we describe an active learning strategy by asking the most informative questions to human experts at each step of coreference resolution. Positing that these pairwise judgments are easy to obtain from humans given the right context, We evaluate this hypothesis and our algorithms on both entity and event coreference tasks and on two languages.
2K_test_848	The Restricted Isometric Property ( R, ) is a very important condition for recovering sparse vectors from high dimensional space, Traditional methods often rely on R, P or its relaxed variants. However in real applications, features are often correlated to each other, which makes these assumptions too strong to be useful. The proposed algorithm converges geometrically, achieves nearly optimal recovery bound O ( s2 log ( d ) ) where s is the sparsity and d is the nominal dimension. We prove that when features exhibit cluster structures, which often happens in real applications, we are able to recover the sparse vector consistently, The consistency comes from our proposed density correction algorithm, which removes the variance of estimated cluster centers using cluster density. In this paper we study the sparse recovery problem in which the feature matrix is strictly non-R.
2K_test_849	To enhance the performance of current state-of-the-art human activity recognition systems. We demonstrate that our methods significantly improve the performance of state-of-the-art motion features. We propose two well-motivated ranking-based methods First, as an improvement over the classic power normalization method, we propose a parameter-free ranking technique called rank normalization ( RaN ), RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD, Second inspired by curriculum learning, we introduce a training-free re-ranking technique called multi-class iterative re-ranking ( MIR ), MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly. On six real-world datasets.
2K_test_852	Propagation of contagion in networks depends on the graph topology The only known analytical characterization of the equilibrium distribution of this process is for complete networks, For large networks with arbitrary topology, it is infeasible to numerically solve for the equilibrium distribution since it requires solving the eigenvalue-eigenvector problem of a matrix that is exponential in N, the size of the network. This paper is concerned with studying the time-asymptotic behavior of the extended contact processes. We confirm this result. On static undirected finite-size networks, This is a contact process with nonzero exogenous infection rate ( also known as the { \epsilon } -SIS, { \epsilon } susceptible-infected-susceptible, model [ 1 ] ) We show that, for a certain range of the network process parameters, the equilibrium distribution of the extended contact process on arbitrary, finite-size networks is well approximated by the equilibrium distribution of the scaled SIS process, which we derived in closed-form in prior work, We use this approximation to decide, in polynomial-time which agents and network substructures are more susceptible to infection by the extended contact process. With numerical simulations comparing the equilibrium distribution of the extended contact process with that of a scaled SIS process.
2K_test_853	The widespread use of social networks enables the rapid diffusion of information, news among users in very large communities, It is a substantial challenge to be able to observe and understand such diffusion processes, which may be modeled as networks that are both large and dynamic A key tool in this regard is data summarization, However few existing studies aim to summarize graphs/networks for dynamics, Dynamic networks raise new challenges not found in static settings, including time sensitivity and the needs for online interestingness evaluation and summary traceability, which render existing techniques inapplicable, The study offers insight into the effectiveness and design properties of OSNet. We study the topic of dynamic network summarization : how to summarize dynamic networks with millions of nodes by only capturing the few most interesting nodes or edges over time, and we address the problem by finding interestingness-driven diffusion processes. Based on the concepts of diffusion radius and scope, we define interestingness measures for dynamic networks, and we propose OSNet, an online summarization framework. We report on extensive experiments with both synthetic and real-life data.
2K_test_854	Session types provide a means to prescribe the communication behavior between concurrent message-passing processes However, in a distributed setting, some processes may be written in languages that do not support static typing of sessions or may be compromised by a malicious intruder, violating invariants of the session types. In such a setting, dynamically monitoring communication between processes becomes a necessity for identifying undesirable actions, In this paper we show how to dynamically monitor communication to enforce adherence to session types in a higher-order setting. We prove that dynamic monitoring does not change system behavior for welltyped processes, and that one of an indicated set of possible culprits must have been compromised in case of an alarm. We present a system of in the case when the monitor detects an undesirable action and an alarm is raised.
2K_test_856	To train deep neural networks ( DNNs ). Which has guaranteed convergence and great scalability : close to 6 times faster on instance of ImageNet data set when run with 6 machines. We propose a distributed approach The proposed scheme is close to optimally scalable in terms of number of machines, and guaranteed to converge to the same optima as the undistributed setting, The convergence and scalability of the distributed setting is The convergence analysis provides novel insights into this complex learning scheme, including : 1 ) layerwise convergence, and 2 ) convergence of the weights in probability. Theoretically empirically empirically across di ? erent datasets ( TIMIT and ImageNet ) and machine learning tasks ( image classi ? cation and phoneme extraction ).
2K_test_857	We compute approximate solutions to L0 regularized linear regression problems. Results of consistency under orthogonality and appropriate handling of redundant features, we demonstrate on synthetic data that the Lass-0 solutions are closer to the true sparse support than L1 regularization models, Lass-0 finds more parsimonious solutions that L1 regularization while maintaining similar predictive accuracy. Using convex relaxation of L1 regularization, also known as the Lasso, as an initialization step Our algorithm, the Lass-0 ( `` Lass-zero '' ), uses a computationally efficient stepwise search to determine a locally optimal L0 solution given any L1 regularization solution. We present theoretical Empirically Additionally.
2K_test_859	Overall this technique extends the capabilities of 3D printing in a new and interesting way, without requiring any new hardware. For furbricating 3D printed hair. Demonstrating the immediate feasibility of our approach using a low cost. We introduce a technique by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling Our approach offers a range of design parameters for controlling the properties of single strands and also of hair bundles, We further detail a list of post-processing techniques for refining the behavior and appearance of printed strands. We provide several examples of output.
2K_test_860	One typically proves infeasibility in satisfiability/ constraint satisfaction ( or optimality in integer programming ) by constructing a tree certificate. However deciding how to branch in the search tree is hard, and impacts search time drastically, using information gathered by that dart to guide what to do next. We explore the power of a simple paradigm, that of throwing random darts into the assignment space and then This method seems to work well when the number of short certificates of infeasibility is moderate, suggesting that the overhead of throwing darts is more than paid for by the information gained by these darts.
2K_test_863	Smartwatches are becoming increasingly powerful, but limited input makes completing complex tasks impractical. For enabling a watch user to contribute to complex tasks. And found it was effective at producing reasonable drafts. Our WearWrite system introduces a new paradigm not through new hardware or input methods, but by directing a crowd to work on their behalf from their wearable device WearWrite lets authors give writing instructions and provide bits of expertise and big picture directions from their smartwatch, while crowd workers actually write the document on more powerful devices. We used this approach to write three academic papers.
2K_test_864	In applied fields practitioners hoping to apply causal structure learning or causal orientation algorithms face an important question : which independence test is appropriate for my data ? In the case of real-valued iid data, linear dependencies and Gaussian error terms, partial correlation is sufficient, But once any of these assumptions is modified, the situation becomes more complex. Kernel-based tests of independence have gained popularity to deal with nonlinear dependencies in recent years, but testing for conditional independence remains a challenging problem We highlight the important issue of non-iid observations : when data are observed in space, time or on a network, nearby observations are likely to be similar, This fact biases estimates of dependence between variables, for certain variables and to obtain residuals, to test for independence. We show how properly accounting for spatial and temporal variation can lead to more reasonable causal graphs, We also show how highly structured data, like images and text, can be used in a causal inference framework using a novel structured input/output Gaussian process formulation. Inspired by the success of Gaussian process regression for handling non-iid observations in a wide variety of areas and by the usefulness of the Hilbert-Schmidt Independence Criterion ( HSIC ), a kernel-based independence test, we propose a simple framework to address all of these issues : first, use Gaussian process regression to control Second. We illustrate this on two classic datasets, one spatial the other temporal, that are usually treated as iid, We demonstrate this idea on a dataset of translated sentences, trying to predict the source language.
2K_test_868	To provide indoor ranging information to modern mobile devices like smartphones and tablets. An indoor ultrasonic location tracking system that can utilize standard audio speakers The method uses a communication scheme based on linearly increasing frequency modulated chirps in the audio bandwidth just above the human hearing frequency range where mobile devices are still sensitive The method uses gradual frequency and amplitude changes that minimize human perceivable ( psychoacoustic ) artifacts derived from the non-ideal impulse response of audio speakers, Chirps also benefit from Pulse Compression, which improves ranging resolution and resilience to both Doppler shifts and multi-path propagation that plague indoor environments, The method supports the decoding of multiple unique identifier packets simultaneously, A Time-Difference-of-Arrival pseudo-ranging technique allows for localization without explicit synchronization with the broadcasting infrastructure, An alternate received signal strength indicator based localization technique allows less accurate localization at the benefit of sparser transmission infrastructure.
2K_test_869	Online discussion forums are complex webs of overlapping subcommunities ( macrolevel structure, across threads ) in which users enact different roles depending on which subcommunity they are participating in within a particular time point ( microlevel structure. This sub-network structure is implicit in massive collections of threads, To uncover this structure. We demonstrate that our model can provide useful explanations of microlevel and macrolevel user presentation characteristics in different communities using the topics discovered from posts, we show that our model does better than MMSB and LDA in predicting user reply structure within threads that the proposed active sub-network discovery model is stable and recovers the original parameters of the experimental setup with high probability. We develop a scalable algorithm based on stochastic variational inference and leverage topic models ( LDA ) along with mixed membership stochastic block ( MMSB ) models. We evaluate our model on three large-scale datasets, Cancer-ThreadStarter ( 22K users and 14, 4K threads ) Cancer-NameMention ( 15, 1K users and 12, 4K threads ) and StackOverFlow ( 1, 19 million users and 4, 55 million threads ) Qualitatively, Quantitatively In addition we demonstrate via synthetic data experiments.
2K_test_871	This paper introduces reasoning about lawful behavior as an important computational thinking skill and provides examples from a novel introductory programming curriculum using Microsoft 's Kodu Game Lab. Showing that rising 5th and 6th graders can understand the lawfulness of Kodu programs, We also discuss some misconceptions students may develop about Kodu, their causes and potential remedies. We present an analysis of assessment data.
2K_test_872	While the version of LegionTools discussed here focuses on Amazon 's Mechanical Turk platform, it can be easily extended to other platforms as APIs become available. To coordinate synchronous crowds of online workers for their systems and studies. We introduce LegionTools a toolkit and interface for managing large, synchronous crowds of online workers for experiments This poster contributes the design and implementation of a state-of-the-art crowd management tool, along with a publicly-available, open-source toolkit that future system builders can use We describe the toolkit itself, along with the underlying design rationale, in order to make it clear to the community of system builders at UIST when and how this tool may be beneficial to their project. We also describe initial deployments of the system in which workers were synchronously recruited to support real-time crowdsourcing systems, including the largest synchronous recruitment and routing of workers from Mechanical Turk that we are aware of.
2K_test_873	Obtaining per-device energy consumption estimates in Non-Intrusive Load Monitoring ( NILM ) has proven to be a challenging task. That estimates the energy consumed by devices operating in different power ranges, data-driven solution to the PCC-NILM problem. We show that reliable energy estimates can be obtained by crowdsourcing the results from using 1, 456 event detectors applied to the publicly available BLUED dataset. We present Power Consumption Clustered Non-Intrusive Load Monitoring ( PCC-NILM ), a relaxation of the NILM problem The Approximate Power Trace Decomposition Algorithm ( APTDA ) is presented as an unsupervised.
2K_test_874	Recently diversity-inducing regularization methods for latent variable models ( LVMs ), which encourage the components in LVMs to be diverse. To address several issues involved in latent variable modeling : ( 1 ) how to capture long-tail patterns underlying data ; ( 2 ) how to reduce model complexity without sacrificing expressivity ; ( 3 ) how to improve the interpretability of learned patterns, While the effectiveness of diversity-inducing regularizers such as the mutual angular regularizer has been demonstrated empirically, a rigorous theoretical analysis of them is still missing, In this paper we aim to bridge this gap and analyze how the mutual angular regularizer ( MAR ) affects the generalization performance of supervised LVMs. Which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis. We use neural network ( NN ) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error. In addition to theoretical analysis, we also present empirical study.
2K_test_875	For identifying and characterizing smooth multidimensional changepoints, and automatically learning changes in expressive covariance structure. Where we identify previously unknown heterogeneous changes in space and time. We present a scalable Gaussian process model We use Random Kitchen Sink features to flexibly define a change surface in combination with expressive spectral mixture kernels to capture the complex statistical structure Finally, through the use of novel methods for additive non-separable kernels, we can scale the model to large datasets. We demonstrate the model on numerical and real world data, including a large spatio-temporal disease dataset.
2K_test_876	User-generated online reviews can play a significant role in the success of retail products. However review systems are often targeted by opinion spammers who seek to distort the perceived quality of a product by creating fraudulent reviews, for spotting fraudsters and fake reviews in online review datasets. Where FRAUDEAGLE successfully reveals fraud-bots in a large online app review database. We propose a fast and effective framework, FRAUDEAGLE Our method has several advantages : ( 1 ) it exploits the network effect among reviewers and products, unlike the vast majority of existing methods that focus on review text or behavioral analysis, ( 2 ) it consists of two complementary steps ; scoring users and reviews for fraud detection, and grouping for visualization and sensemaking, ( 3 ) it operates in a completely unsupervised fashion requiring no labeled data, while still incorporating side information if available, and ( 4 ) it is scalable to large datasets as its run time grows linearly with network size. We demonstrate the effectiveness of our framework on syntheticand real datasets ;.
2K_test_877	Networked or telematic music performances take many forms, ranging from small laptop ensembles using local area networks to long-distance musical collaborations using audio and video links, Two important concerns for any networked performance are :. ( 1 ) what is the role of communication in the music performance ? In particular, what are the esthetic and pragmatic justifications for performing music at a distance, and ( 2 ) how are the effects of communication latency ameliorated or incorporated into the performance ? In addition to addressing these two concerns. Which achieved a coordinated performance involving 68 computer musicians, each with their own connection to the network. A recent project the Global Net Orchestra, the technical aspects of the project.
2K_test_878	Given a large graph with several millions or billions of nodes and edges, such as a social network, how can we explore it efficiently and find out what is in the data ? that enables the comprehensive analysis of large graphs It automatically extracts graph invariants It interactively visualizes univariate and bivariate distributions for those invariants It summarizes the properties of the nodes that the user selects It efficiently visualizes the induced subgraph of a selected node and its neighbors. In this demo we present Perseus, a large-scale system by supporting the coupled summarization of graph properties and structures, guiding attention to outliers, and allowing the user to interactively explore normal and anomalous node behaviors Specifically, Perseus provides for the following operations : 1 ) ( e, degree PageRank real eigenvectors ) by performing scalable, offline batch processing on Hadoop ; 2 ) ; 3 ) ; 4 ), by incrementally revealing its neighbors. In our demonstration we invite the audience to interact with Perseus to explore a variety of multi-million-edge social networks including a Wikipedia vote network, a friendship/foeship network in Slashdot, and a trust network based on the consumer review website Epinions.
2K_test_879	The long-term goal of connecting scales in biological simulation can be facilitated by scale-agnostic methods. Applies effectively to spatially resolved cell-scale simulations. Show that although WE has important limitations, it can achieve performance significantly exceeding standard parallel simulationby orders of magnitude for some observables. We demonstrate that the weighted ensemble ( WE ) strategy, initially developed for molecular simulations The WE approach runs an ensemble of parallel trajectories with assigned weights and uses a statistical resampling strategy of replicating and pruning trajectories to focus computational effort on difficult-to-sample regions, The method can also generate unbiased estimates of non-equilibrium and equilibrium observables, sometimes with significantly less aggregate computing time than would be possible using standard parallelization Here, we use WE to orchestrate particle-based kinetic Monte Carlo simulations, which include spatial geometry ( e, of organelles plasma membrane ) and biochemical interactions among mobile molecular species. We study a series of models exhibiting spatial, temporal and biochemical complexity and.
2K_test_880	Besides the application to password generation, our proposed Human Usability Model ( HUM ) will have other applications. What can a human compute in his/her head that a powerful adversary can not infer ? To answer this question, we define a model of human computation and a measure of security. We show that our password generation methods are humanly computable and, to a well-defined extent. Then motivated by the special case of password creation, we propose a collection of well-defined password-generation methods, For the proof of security, we posit that password generation methods are public, but that the humans privately chosen seed is not, and that the adversary will have observed only a few input-output pairs.
2K_test_881	The spatial pyramid and its variants have been very popular feature models due to their success in balancing spatial location encoding and spatial invariance. We address the problem of generating video features for action recognition Although it seems straightforward to extend spatial pyramid to the temporal domain ( spatio-temporal pyramid ), the large spatio-temporal diversity of unconstrained videos and the resulting significantly higher dimensional representations make it less appealing to include the spatio-temporal location into the video features. Results show that despite its simplicity, this method achieves comparable or better results than spatio-temporal pyramid. This paper introduces the space-time extended descriptor, a simple but efficient alternative way Instead of only coding motion information and leaving the spatio-temporal location to be represented at the pooling stage, location information is used as part of the encoding step, This method is a much more effective and efficient location encoding method as compared to the fixed grid model because it avoids the danger of over committing to artificial boundaries and its dimension is relatively low. Experimental on several benchmark datasets.
2K_test_882	Suppose you are a teacher, and have to convey a set of object-property pairs ( 'lions eat meat ', or 'aspirin is a blood-thinner ' ), A good teacher will convey a lot of information, with little effort on the student side, Specifically given a list of objects ( like animals or medical drugs ) and their associated properties, what is the best and most intuitive way to convey this information to the student, without the student being overwhelmed ? A related, harder problem is : how can we assign a numerical score to each lesson plan ( i, way of conveying information ) ?. Here we give a formal definition of this problem of forming learning units. It is effective achieving excellent results on real data, both with respect to our proposed metric, but also with respect to encoding length, demonstrate the effectiveness of HYTRA. And we provide a metric for comparing different approaches based on information theory, We also design a multi-pronged algorithm, HYTRA for this problem, Our proposed HYTRA is scalable ( near-linear in the dataset size ), and it is intuitive, conforming to well-known educational principles, such as grouping related concepts, and `` comparing '' and `` contrasting ''. Experiments on real and synthetic datasets.
2K_test_883	The engineering analysis for determining the remaining seismic capacity of buildings following earthquakes requires performing structural calculations, observations of the actual damage, and applying extensive engineering judgment, Additionally the analysis should often be performed under stringent time requirements, The results of the study can be used to develop formal representation of damage information in information models and potentially allow better allocation of data collection time in the field. This study identifies the information requirements for representing the damage information and performing the visual damage assessment of structural walls. The study showed that the information required to represent the damaged conditions can be grouped under five broad categories and using seventeen damage parameters, showed that the damage parameters have varying degrees of importance. The damage descriptions for seven common damage modes of structural walls were studied by employing the affinity diagramming method.
2K_test_884	Differential dynamic logic is a logic for specifying and verifying safety, liveness and other properties about models of cyber-physical systems, Theorem provers based on differential dynamic logic have been used to verify safety properties for models of self-driving cars and collision avoidance protocols for aircraft, Examples include : an unambiguous separation between proof checking and proof search, the ability to extract program traces corresponding to counter-examples, and synthesis of surely-live deterministic programs from liveness proofs for nondeterministic programs. Unfortunately these theorem provers do not have explicit proof terms, which makes the implementation of a number of important features unnecessarily complicated without soundness-critical and extra-logical extensions to the theorem prover, with such an explicit representation of proofs, To support axiomatic theorem proving. This paper presents a differential dynamic logic The resulting logic extends both the syntax and semantics of differential dynamic logic with proof terms -- syntactic representations of logical deductions the logic allows equivalence rewriting deep within formulas and supports both uniform renaming and uniform substitutions.
2K_test_886	We study parallel and distributed Frank-Wolfe algorithms ;. And observe significant speedups over competing state-of-the-art ( and synchronous ) methods. The former on shared memory machines with mini-batching, and the latter in a delayed update framework, In both cases we perform computations asynchronously whenever possible, We assume block-separable constraints as in Block-Coordinate Frank-Wolfe ( BCFW ) method ( Lacoste-Julien et al, 2013 ) but our analysis subsumes BCFW and reveals problemdependent quantities that govern the speedups of our methods over BCFW, A notable feature of our algorithms is that they do not depend on worst-case bounded delays, but only ( mildly ) on expected delays, making them robust to stragglers and faulty worker threads. We present experiments on structural SVM and Group Fused Lasso.
2K_test_887	To infer the past, describe the present and predict the future. Demonstrate that our approach significantly outperforms the compared baselines. In this work we introduce Video Question Answering in temporal domain We present an encoder-decoder approach using Recurrent Neural Networks to learn temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions. We explore approaches for finer understanding of video content using question form of `` fill-in-the-blank '', and managed to collect 109, 895 video clips with duration over 1, 000 hours from TACoS, MPII-MD MEDTest 14 datasets, while the corresponding 390, 744 questions are generated from annotations Extensive experiments.
2K_test_888	Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet. In this article we exploit the relations among different modalities in a multimedia database and for general multimodal data mining problem In addition, in order to reduce the demanding computation to develop an effective and efficient solution to the multimodal data mining problem. Present a framework where image annotation and image retrieval are considered as the special cases Specifically, the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables, we propose a new max margin structure learning approach called Enhanced Max Margin Learning ( EMML ) framework, which is much more efficient with a much faster convergence rate than the existing max margin learning methods, as verified through empirical evaluations, Furthermore we apply EMML framework that is highly scalable in the sense that the query response time is independent of the database scale The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database, and excels many existing multimodal data mining methods in the literature that do not scale up at all. The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases.
2K_test_889	To specify temporal properties of hybrid systems, to reason about the intermediate states reached by a hybrid system. And show its use- fulness for nontrivial temporal properties of hybrid systems solving an open problem formulated in previous work. The differential temporal dynamic logic dTL 2 is a logic It combines differential dynamic logic with temporal logic The logic dTL 2 supports some linear time temporal properties of LTL, It extends differential temporal dynamic logic dTL with nested temporalities, We take particular care to handle the case of alternating universal dynamic and existential temporal modalities and its dual. We provide a semantics and a proof system for the logic dTL 2.
2K_test_890	The interest in distributed control methods for power systems is motivated by the need for scalable solutions to handle the coordination of an increasing number of distributed resources. To solve the DC Optimal Power Flow problem ( DC-OPF ). This paper presents a fully distributed multilevel method Our proposed approach constitutes a distributed iterative mechanism to solve the first order optimality conditions of the DC-OPF problem using the fact that optimality conditions involve local variable couplings The proposed distributed structure requires each bus to update a few local variables and exchange information with neighboring buses Our multilevel distributed approach distributes the computation at several levels, nodes subareas and areas, It allows for synchronous information exchanges, after each iteration at the nodal level and asynchronous communication, after multiple iterations between subareas and areas, To define meaningful subareas, we are using a graph theoretic partitioning method derived from an epidemics model. We compare the performance of the proposed partitioning method over a random partitioning method using the IEEE 118-bus system.
2K_test_891	We work with the `` NYC Taxi Data Set, '' a historical repository of 750 million rides of taxi medallions over a period of four years ( 20102013 ), This data set provides rich ( batch ) information on the movements in an urban network as its citizens go about their daily life. Preliminary results show that our method allows us to pinpoint locations of co-behavior for traffic in the Manhattan road network. We present a spectral analysis of taxi movement based on the graph Fourier transform, which necessitates the spectral decomposition of a large directed, sparse matrix Important considerations toward handling this matrix are discussed.
2K_test_892	Plug-meters benefit many grid and building-level energy management applications like automated load control and load scheduling However, installing and maintaining large and/or long term deployments of such meters requires assignment and updating of the identity ( labels ) of electrical loads connected to them. Although the literature on electricity disaggregation and appliance identification is extensive, there is no consensus on the generalizability of the proposed solutions, especially with respect to the features that are extracted from voltage and current measurements In this paper, we begin to address this problem by comparing the discriminative power of commonly used features, to understand the feasibility and design of a hardware setup that can perform these calculations in near real-time. Specifically we carry out tests on PLAID, a publicly available high-frequency dataset of hundreds of residential appliances By examining how the classification accuracy changes with sampling frequency, we also explore the computational complexity of these techniques.
2K_test_893	Many big data applications collect large numbers of time series A first task in analyzing such data is to find a low- dimensional representation, a graph which faithfully describes relations among the measured processes and through time, The processes are often affected by a relatively small number of unmeasured trends. For jointly estimating these trends and underlying weighted. This paper presents a computationally tractable algorithm from the collected data. The algorithm is demonstrated on simulated time series datasets.
2K_test_894	Metering of electricity consumption, both at the building-level and appliance-level, provides stakeholders like residents, facility managers building owners, with information requisite to engage in energy efficient practices. Currently available solutions for appliance-level energy metering require the installation of plug-through power meters ; this is often difficult and costly for appliances with inaccessible wires/outlets, or for appliances that draw large amounts of current, in order to substitute the need of plug-level sensors with cheap and easily deployable contactless sensors ( e, light sound magnetic field sensors ). Find that the inferred energy values have an average error of 10. In this paper we utilize a framework that performs the step of energy estimation for load disaggregation The framework combines data from these contactless sensors and aggregate metering, in order to virtually meter the electricity consumption of specific appliances The solution requires minimal calibration, and is easily performed using commercially available sensors. We test it in a commercial building with 6 appliances that are monitored using magnetic field sensors and.
2K_test_895	Over two widely separated frequency bands. This paper presents the first reported in-situ reconfiguration of a narrowband CMOS low noise amplifier ( LNA ) using a GeTe phase-change ( PC ) switch In this work, we present a robust realization of a reconfigurable 3/5 GHz LNA designed and fabricated in a 0, 13 m CMOS process and flip-chip integrated with a four-terminal PC switch fabricated using an in-house process.
2K_test_896	Devices can be made more intelligent if they have the ability to sense their surroundings and physical configuration. However adding extra special purpose sensors increases size, price and build complexity, to open new sensing opportunities. And demonstrates high accuracy. Instead we use speakers and microphones already present in a wide variety of devices Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment, chiefly the materials and geometry of proximate surfaces. We offer several example uses, two of which we implemented as self-contained demos, and conclude with an evaluation that quantifies their performance.
2K_test_897	People are more creative at solving difficult design problems when they use relevant examples from outside of the problem s domain as inspirations. However finding such outside-the-box inspirations is difficult, particularly in large idea repositories such as the web, because without guidance people select domains to search based on surface similarity to the problem s domain, to yield useful and non-obvious inspirations for solutions. Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own, or drew inspiration from domains closer to or not sharing structural correspondence with the original problem. In this paper we demonstrate an approach in which non-experts identify domains that have the potential. We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains.
2K_test_898	To study signals on networks, to detect epidemics or to predict blackouts, we need to understand network topology and its impact on the behavior of network processes, The high dimensionality of large networks presents significant analytical and computational challenges ; only specific network structures have been studied without approximation. We consider the impact of network topology on the limiting behavior of a dynamical process obeying the stochastic rules of SIS ( susceptible-infected-susceptible ) epidemics using the scaled SIS process which captures the preference of individual agents versus the preference of society ( i, network ) and investigate its effects. We introduce the network effect ratio.
2K_test_899	3D-stacked integration of DRAM and logic layers using through-silicon via ( TSV ) technology has given rise to a new interpretation of near-data processing ( NDP ) concepts that were proposed decades ago, However processing capability within the stack is limited by stringent power and thermal constraints, Simple processing mechanisms with intensive memory accesses, such as data reorganization, are an effective means of exploiting 3D stacking-based NDP, Data reorganization handled completely in memory improves the host processor 's memory access performance. However in-memory data reorganization performed in parallel with host memory accesses raises issues, including interference bandwidth allocation, Previous work has mainly focused on performing data reorganization while blocking host accesses to address host/NDP interference, flexible bandwidth allocation and in-memory coherence. This article details data reorganization performed in parallel with host memory accesses.
2K_test_900	How do people interact with their Facebook wall ? At a high level, this question captures the essence of our work, While most prior efforts focus on Twitter, the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study, Our work provides a solid step towards a systematic and quantitative wall-centric profiling of Facebook user activity. In this work we model Facebook user behavior : we analyze the wall activities of users focusing on identifying common patterns and surprising phenomena to fit our data. Our key results can be summarized in the following points, First we find that many wall activities, including number of posts, number of likes number of posts of type photo, can be described by the PowerWall distribution, What is more surprising is that most of these distributions have similar slope, with a value close to 1 ! Second, we show how our patterns and metrics can help us spot surprising behaviors and anomalies, For example we find a user posting every two days, exactly the same count of posts ; another user posting at midnight, with no other activity before or after. We propose PowerWall a lesser known heavy-tailed distribution. We conduct an extensive study of roughly 7K users over three years during four month intervals each year.
2K_test_901	Biomedical scientists have invested significant effort into making it easy to perform lots of experiments quickly and cheaply, These high throughput methods are the workhorses of modern systems biology efforts, However we simply can not perform an experiment for every possible combination of different cell type, genetic mutation and other conditions, In practice this has led researchers to either exhaustively test a few conditions or targets, or to try to pick the experiments that best allow a particular problem to be explored, But which experiments should we pick ? The ones we think we can predict the outcome of accurately, the ones for which we are uncertain what the results will be, or a combination of the two ? Humans are not particularly well suited for this task because it requires reasoning about many possible outcomes at the same time, However computers are much better at handling statistics for many experiments, and machine learning algorithms allow computers to learn how to make predictions and decisions based on the data theyve previously processed The next challenge is to apply these methods to reduce the cost of achieving the goals of large projects, such as The Cancer Genome Atlas. Previous computer simulations showed that a machine learning approach termed active learning could do a good job of picking a series of experiments to perform in order to efficiently learn a model that predicts the results of experiments that were not done. Showed that the active learning approach outperforms strategies a human might use, even when the potential outcomes of individual experiments are not known beforehand. Now Naik et al, have performed cell biology experiments in which experiments were chosen by an active learning algorithm and then performed using liquid handling robots and an automated microscope, The key idea behind the approach is that you learn more from an experiment you cant predict ( or that you predicted incorrectly ) than from just confirming your confident predictions. The results of the robot-driven experiments.
2K_test_902	In large-scale complex networks, the underlying nonlinear dynamical system is high-dimensional and performing qualitative analysis of the differential equation becomes prohibitive, The study of such systems is often deferred to numerical simulations or local analysis about equilibrium points of the system. We study the spread of two strains of virus competing for space to formally establish a simple sufficient condition for ( exponentially fast ) survival of the fittest in a bi-layer weighted digraph :. In a network modeled by the classical logistic ordinary differential equations In this paper, we extend the work developed in [ 1 ], the weaker strain dies out regardless of the initial conditions if its maximum in-flow rate of infection across nodes is smaller than the minimum in-flow rate of the stronger strain, We bound any solution of the logistic ODE by one- dimensional solutions over certain homogeneous networks, for which the system is well understood Our global stability approach via bounds readily applies to the discrete-time logistic model counterpart.
2K_test_903	The preferred treatment for kidney failure is a transplant ; however, demand for donor kidneys far outstrips supply, Kidney exchange an innovation where willing but incompatible patient-donor pairs can exchange organsvia barter cycles and altruist-initiated chainsprovides a life-saving alternative, Typically fielded exchanges act myopically, considering only the current pool of pairs when planning the cycles and chains. Yet kidney exchange is inherently dynamic, with participants arriving and departing, Also many planned exchange transplants do not go to surgery due to various failures, So it is important to consider the future when matching, for learning to match in a general dynamic model. It results in higher values of the objective it yields better solutions for the efficient objective ( which does not incorporate equity ) than traditional myopic matching that uses the efficiency objective. Motivated by our experience running the computational side of a large nationwide kidney exchange, we present FUTURE-MATCH a framework FUTUREMATCH takes as input a high-level objective ( e, `` maximize graft survival of transplants over time '' ) decided on by experts, then automatically ( i ) learns based on data how to make this objective concrete and ( ii ) learns the `` means '' to accomplish this goala task, in our experience that humans handle poorly It uses data from all live kidney transplants in the US since 1987 to learn the quality of each possible match ; it then learns the potentials of elements of the current input graph offline ( e, potentials of pairs based on features such as donor and patient blood types ), translates these to weights, and performs a computationally feasible batch matching that incorporates dynamic, failure-aware considerations through the weights. We validate FUTUREMATCH on real fielded exchange data Furthermore, even under economically inefficient objectives that enforce equity.
2K_test_904	Humans rely on eye gaze and hand manipulations extensively in their everyday activities, Most often users gaze at an object to perceive it and then use their hands to manipulate it. To enable rapid precise and expressive touch-free interactions. Results show that gaze+gesture can outperform systems using gaze or gesture alone, and in general approach the performance of `` gold standard '' input systems, such as the mouse and trackpad. We propose applying a multimodal, gaze plus free-space gesture approach We show the input methods are highly complementary, mitigating issues of imprecision and limited expressivity in gaze-alone systems, and issues of targeting speed in gesture-alone systems, We extend an existing interaction taxonomy that naturally divides the gaze+gesture interaction space, which we then populate with a series of example interaction techniques to illustrate the character and utility of each method. We contextualize these interaction techniques in three example scenarios, In our user study, we pit our approach against five contemporary approaches ;.
2K_test_906	There is often a large disparity between the size of a game we wish to solve and the size of the largest instances solvable by the best algorithms ; for example, a popular variant of poker has about 10165 nodes in its game tree, while the currently best approximate equilibrium-finding algorithms scale to games with around 1012 nodes, In order to approximate equilibrium strategies in these games, the leading approach is to create a sufficiently small strategic approximation of the full game, called an abstraction and to solve that smaller game instead, The leading abstraction algorithm for imperfect-information games generates abstractions that have imperfect recall and are distribution aware, using k-means with the earth mover 's distance metric to cluster similar states together, A distribution-aware abstraction groups states together at a given round if their full distributions over future strength are similar ( as opposed to, for example just the expectation of their strength ), The leading algorithm considers distributions over future strength at the final round of the game. However one might benefit by considering the trajectory of distributions over strength in all future rounds, not just the final round, An abstraction algorithm that takes all future rounds into account is called potential aware, for computing potential-aware imperfect-recall abstractions. Show that our algorithm improves performance over the previously best approach. We present the first algorithm using earth mover 's distance. Experiments on no-limit Texas Hold'em.
2K_test_907	Non-technical loss ( NTL ) represents a major challenge when providing reliable electrical service in developing countries, where it often accounts for 11-15 % of total generation capacity [ 1 ], NTL is caused by a variety of factors such as theft, unmetered homes and inability to pay which at volume can lead to system instability, grid failure and major financial losses for providers. In this paper we investigate error sources and techniques for separating NTL from total losses in microgrids. We show that the model can be used to determine uncertainty bounds that can help in separating NTL from total losses. Our approach models the primary sources of state uncertainty including line losses, transformer losses meter calibration error, packet loss and sample synchronization error. We conduct an extensive data-driven simulation on 72 days of wireless meter data from a 430-home microgrid deployed in Les Anglais.
2K_test_908	For the important task of binocular depth perception from complex natural-image stimuli, the neurophysiological basis for disambiguating multiple matches between the eyes across similar features has remained a long-standing problem, Recurrent interactions among binocular disparity-tuned neurons in the primary visual cortex ( V1 ) could play a role in stereoscopic computationsbyalteringresponsesto favorthemost likelydepthinterpretation fora givenimagepair, Psychophysicalresearch has shown that binocular disparity stimuli displayed in 1 region of the visualfield can be extrapolated into neighboring regions that contain ambiguous depth information, by cooperative algorithms play an important role in solving the stereo correspondence problem. We tested whether neurons in macaque V1 interact in a similar manner. And found that unambiguous binocular disparity stimuli displayed in the surrounding visualfields of disparity-selective V1 neurons indeed modified their responses when either bistable stereoscopic or uniform featureless stimuli were presented within their receptivefield centers, The delayed timing of the response behavior compared with the timing of classical surround suppression and multiple control experiments suggests that these modulations are carried out by slower disparity-specific recurrentconnectionsamongV1neurons.
2K_test_909	Regret matching is a widely-used algorithm for learning how to. We begin by proving that regrets on actions in one setting ( game ) can be transferred to warm start the regrets for solving a different setting with same structure but different payoffs that can be written as a function of parameters, for large incomplete-information games. We prove how this can be done by carefully discounting the prior regrets, This provides to our knowledge, the first principled warm-starting method It also extends to warm-starting the widely-adopted counterfactual regret minimization ( CFR ) algorithm We then study optimizing a parameter vector for a player in a two-player zero-sum game ( e, optimizing bet sizes to use in poker ), We propose a custom gradient descent algorithm that provably finds a locally optimal parameter vector while leveraging our warm-start theory to significantly save regret-matching iterations at each step, It optimizes the parameter vector while simultaneously finding an equilibrium This amounts to the first action abstraction algorithm ( algorithm for selecting a small number of discrete actions to use from a continuum of actions -- a key preprocessing step for solving large games using current equilibrium-finding algorithms ) with convergence guarantees for extensive-form games. We show this experimentally as well We present experiments in no-limit Leduc Hold'em and nolimit Texas Hold'em to optimize bet sizing.
2K_test_910	For many reasons one might consider mechanisms, or social choice functions, that only have access to the ordinal rankings of alternatives by the individual agents rather than their utility functions. We adopt a utilitarian perspective on social choice, assuming that agents have ( possibly latent ) utility functions over some space of alternatives, In this context one possible objective for a social choice function is the maximization of ( expected ) social welfare relative to the information contained in these rankings We study such optimal social choice functions and underscore the important role played by scoring functions. Sample complexity results for the class of scoring functions. Under three different models, In our worst-case model, no assumptions are made about the underlying distribution and we analyze the worst-case distortion-or degree to which the selected alternative does not maximize social welfare-of optimal ( randomized ) social choice functions, In our average-case model, we derive optimal functions under neutral ( or impartial culture ) probabilistic models Finally, a very general learning-theoretic model allows for the computation of optimal social choice functions ( i, ones that maximize expected social welfare ) under arbitrary, sampleable distributions In the latter case, we provide both algorithms and. And further validate the approach empirically.
2K_test_911	For elementary science test. Shows that our framework outperforms several strong baselines. We provide a solution using instructional materials, We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures ( given a corpus of question-answer pairs and instructional materials ), and uses what it learns to answer novel elementary science questions.
2K_test_912	In Massively Open Online Courses ( MOOCs ) TA resources are limited ; most MOOCs use peer assessments to grade assignments, Students have to divide up their time between working on their own homework and grading others, If there is no risk of being caught and penalized, students have no reason to spend any time grading others, Course staff want to incentivize students to balance their time between course work and peer grading. They may do so by auditing students, ensuring that they perform grading correctly, One would not want students to invest too much time on peer grading, as this would result in poor course performance of strategic auditing in peer grading. We present the first model, modeling the student 's choice of effort in response to a grader 's audit levels as a Stackelberg game with multiple followers We demonstrate that computing the equilibrium for this game is computationally hard, We then provide a PTAS in order to compute an approximate solution to the problem of allocating audit levels, However we show that this allocation does not necessarily maximize social welfare ; in fact, there exist settings where course auditor utility is arbitrarily far from optimal under an approximately optimal allocation To circumvent this issue, we present a natural condition that guarantees that approximately optimal TA allocations guarantee approximately optimal welfare for the course auditors.
2K_test_913	For decades researchers have struggled with the problem of envy-free cake cutting : how to divide a divisible good between multiple agents so that each agent likes his own allocation best. Although an envy-free cake cutting protocol was ultimately devised, it is unbounded in the sense that the number of operations can be arbitrarily large, depending on the preferences of the agents, We ask whether bounded protocols exist when the agents ' preferences are restricted. Our main result is an envy-free cake cutting protocol for agents with piecewise linear valuations, which requires a number of operations that is polynomial in natural parameters of the given instance.
2K_test_914	Achieving high performance for compute bounded numerical kernels typically requires an expert to hand select an appropriate set of Single-instruction multiple-data ( SIMD ) instructions, then statically scheduling them in order to hide their latency while avoiding register spilling in the process, Unfortunately this level of control over the code forces the expert to trade programming abstraction for performance which is why many performance critical kernels are written in assembly language, An alternative is to either resort to auto-vectorization ( see Figure 1 ) or to use intrinsic functions, both features offered by compilers However, in both scenarios the expert loses control over which instructions are selected, which optimizations are applied to the code and moreover how the instructions are scheduled for a target architecture. Ideally the expert would need assembly-like control over their SIMD instructions beyond what intrinsics provide while maintaining a C-level abstraction for the non-performance critical parts we bridge the gap between performance and abstraction for SIMD instructions. In this paper through the use of custom macro intrinsics that provide the programmer control over the instruction selection, and scheduling while leveraging the compiler to manage the registers This provides the best of both assembly and vector intrinsics programming so that a programmer can obtain high performance implementations within the C programming language.
2K_test_915	For differential dynamic logic. The resulting axiomatization of differential dynamic logic is proved to be sound and relatively complete. This article introduces a relatively complete proof calculus ( dL ) that is entirely based on uniform substitution, a proof rule that substitutes a formula for a predicate symbol everywhere Uniform substitutions make it possible to use axioms instead of axiom schemata, thereby substantially simplifying implementations Instead of subtle schema variables and soundness-critical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones, the resulting calculus adopts only a finite number of ordinary dLformulas as axioms, which uniform substitutions instantiate soundly, The static semantics of differential dynamic logic and the soundness-critical restrictions it imposes on proof steps is captured exclusively in uniform substitutions and variable renamings as opposed to being spread in delicate ways across the prover implementation In addition to sound uniform substitutions, this article introduces differential forms for differential dynamic logic that make it possible to internalize differential invariants, differential substitutions and derivatives as first-class axioms to reason about differential equations axiomatically.
2K_test_916	Complex event detection is a retrieval task with the goal of finding videos of a particular event in a large-scale unconstrained internet video archive, given example videos and text descriptions. Nowadays different multimodal fusion schemes of low-level and high-level features are extensively investigated and evaluated for the complex event detection task, However how to effectively select the high-level semantic meaningful concepts from a large pool to assist complex event detection is rarely studied in the literature to automatically select semantic meaningful concepts for the event detection task. Demonstrate the efficacy of our proposed method. In this paper we propose two novel strategies based on both the events-kit text descriptions and the concepts high-level feature descriptions, Moreover we introduce a novel event oriented dictionary representation based on the selected semantic concepts Towards this goal, we leverage training samples of selected concepts from the Semantic Indexing ( SIN ) dataset with a pool of 346 concepts, into a novel supervised multitask dictionary learning framework. Extensive experimental results on TRECVID Multimedia Event Detection ( MED ) dataset.
2K_test_917	CPS security though well studied. To support resilient design and active detection of adversaries in cyber physical systems ( CPS, In this paper we consider control systems as an abstraction of CPS to obtain a unified framework that captures and extends results in control system security. We demonstrate an ability to investigate and extend existing results through the proposed information flow analyses. This paper considers the development of information flow analyses Here, we use information flow analysis, a well established set of methods developed in software security, Specifically we propose the Kullback Liebler ( KL ) divergence as a causal measure of information flow, which quantifies the effect of adversarial inputs on sensor outputs, We show that the proposed measure characterizes the resilience of control systems to specific attack strategies by relating the KL divergence to optimal detection, We then relate information flows to stealthy attack scenarios where an adversary can bypass detection. Finally this article examines active detection mechanisms where a defender intelligently manipulates control inputs or the system itself to elicit information flows from an attacker 's malicious behavior, In all previous cases.
2K_test_920	The system and method of the present invention enables high-speed, high precision and low-cost motion tracking for a wide range of applications. According to embodiments of the present invention are a system and method that use projected structured patterns of light and linear optical sensors Sensors are capable of recovering two-dimensional location within the projection area, while several sensors can be combined for up to six degrees of freedom tracking, The structure patterns are based on m-sequences, in which any consecutive subsequence of m bits is unique, Both digital and static light sources can be used.
2K_test_923	Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models, capable of enhancing various types of neural networks ( e, CNNs and RNNs ) with declarative first-order logic rules. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems. We propose a general framework Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition.
2K_test_924	Independent thermal actuation ( switching ). We show that an AlN barrier decreases the switch parasitic capacitance with minimal increases in the switching power, results in switches with an improvement in cutoff frequency, $ f_ { \mathrm { CO } } $ from 5, 3 to 8 THz, a $ C_ { \mathrm { \scriptscriptstyle OFF } } $ improvement from 15 to 10 fF, while maintaining the $ R_ { \mathrm { \scriptscriptstyle ON } } $ at 2 $ \Omega $ This improvement was accompanied by normalized minimum power to amorphize increases of only 14 % ( from 1, 7 W ) for a 100-ns heater pulse. We demonstrate four-terminal GeTe-based RF switches with These devices incorporate an AlN-based dielectric separating high-conductivity W micro-heaters from the RF signal path, Decoupling these design variables, with the high thermal conductivity of the AlN, makes it possible to increase the electrical separation with a thicker AlN film for lower parasitic capacitance with minimal decrease in desirable thermal coupling Increasing the AlN thickness from 105 to 170 nm. With dc pulsed and RF testing.
2K_test_925	The LD results ans wer a fundamental question on how to quantify the rate at which the distributed scheme approaches the centralized performance as the inter-sensor communication rate increases. This paper studies the convergence of the estimation error process and the characterization of the corresponding invariant measure. It is shown that the network achieves weak consensus, the conditional estimation error covariance at a randomly selected sensor converges weakly ( in distribution ) to a unique invariant measure, Further it is proved that as ! 1 this invariant measure satisfies the Large Deviation ( LD ) upper and lower bounds, implying that this measure converges exponentially fast ( in probability ) to the Dirac measureP, where Pis the stable error covariance of the centralized ( Kalman ) filtering setup. In distributed Kalman filtering for poten tially unstable and large linear dynamic systems A gossip network protocol termed Modified Gossip Interactive Kalman Filteri ng ( M-GIKF ) is proposed, where sensors exchange their filtered states ( estimates and error covariances ) and propagate their observations via inter-sensor communications of rate ; is defined as the averaged number of inter-sensor message passa ges per signal evolution epoch The filtered states are interpre ted as stochastic particles swapped through local interaction, The paper shows that the conditional estimation error covariance sequence at each sensor under M-GIKF evolves as a random Riccati equa- tion ( RRE ) with Markov modulated switching. By formulating the RRE as a random dynamical system.
2K_test_926	How can we analyze large-scale real-world data with various attributes ? Many real-world data ( e, network traffic logs web data, social networks knowledge bases, and sensor streams ) with multiple attributes are represented as multi-dimensional arrays, For analyzing a tensor, tensor decompositions are widely used in many data mining applications : detecting malicious attackers in network traffic logs ( with source IP, destination IP port-number timestamp ), finding telemarketers in a phone call history ( with sender, receiver date ) and identifying interesting concepts in a knowledge base ( with subject. However current tensor decomposition methods do not scale to large and sparse real-world tensors with millions of rows and columns and `fibers for large-scale tensor decompositions. And discover hidden concepts. In this paper we propose HaTen2, a distributed method that runs on the MapReduce framework, Our careful design and implementation of HaTen2 dramatically reduce the size of intermediate data and the number of jobs leading to achieve high scalability compared with the state-of-the-art method. Thanks to HaTen2 we analyze big real-world sparse tensors that can not be handled by the current state of the art.
2K_test_927	Mental simulation is an important skill for program understanding and prediction of program behavior Finally, we present recommendations for question prompt design to foster better student simulation of program execution. Assessing students ' ability to mentally simulate program execution can be challenging in graphical programming environments and on paper-based assessments, for assessing students ability to mentally simulate and predict code behavior. Analysis of student responses suggest that this type of question can be used to identify misconceptions and misinterpretation of instructions. This poster presents the iterative design and refinement process using a novel introductory computational thinking curriculum for Microsoft 's Kodu Game Lab. We present an analysis of question prompts and student responses from data collected from three rising 3rd - 6th graders where the curriculum was implemented.
2K_test_928	We use the notion of balance to give a more fine-grained understanding of several well-studied routing questions that are considerably harder in directed graphs for computing low-radius decompositions of directed graphs parameterized by balance. We show that using our approximate maximum flow algorithm, we can efficiently determine whether a given directed graph is -balanced. We introduce the notion of balance for directed graphs : aweighted directed graph is -balanced if for every cut S V, the total weight of edges going from S to V S is within factor of the total weight of edges going from V S to S, We first revisit oblivious routings in directed graphs, Our main algorithmic result is an oblivious routing scheme for single-source instances that achieve an O ( log 3 n / loglog n ) competitive ratio, In the process we make several technical contributions which may be of independent interest, In particular we give an efficient algorithm We also define and construct low-stretch arborescences, a generalization of low-stretch spanning trees to directed graphs, On the negative side, we present new lower bounds for oblivious routing problems on directed graphs We show that the competitive ratio of oblivious routing algorithms for directed graphs is ( n ) in general ; this result improves upon the long-standing best known lower bound of ( n ) by Hajiaghayi et al, We also show that our restriction to single-source instances is necessary by showing an ( n ) lower bound for multiple-source oblivious routing in Eulerian graphs, We also study the maximum flow problem in balanced directed graphs with arbitrary capacities We develop an efficient algorithm that finds an ( 1+ ) -approximate maximum flows in -balanced graphs in time O ( m 2 / 2 ). Additionally we give an application to the directed sparsest cut problem.
2K_test_929	Virtual machine ( VM ) migration demands distinct properties under resource oversubscription and workload surges. For VMs under contention. We show that our implementation, resolves VM contention up to several times faster than live migration. We present enlightened post-copy, a new mechanism that evicts the target VM with fast execution transfer and short total duration This design contrasts with common live migration, which uses the down time of the migrated VM as its primary metric ; it instead focuses on recovering the aggregate performance of the VMs being affected, In enlightened post-copy the guest OS identifies memory state that is expected to encompass the VM 's working set The hypervisor accordingly transfers its state, mitigating the performance impact on the migrated VM resulting from post-copy transfer. With modest instrumentation in guest Linux.
2K_test_930	The context of consumer search is often unobserved and the prediction of it can be nontrivial, Consumers arrive at search engines with diverse interests, and their search context may vary even when they are searching using the same keyword Our study has the potential to help advertisers design keyword portfolios and bidding strategy by extracting contextual ambiguity and other semantic characteristics of keywords based on large-scale analytics from unstructured data, It can also help search engines improve the quality of displayed ads in response to a consumer search query. In this paper we explore how the contextual ambiguity of a search can affect a keyword 's performance an automatic way of examining keyword contextual ambiguity We examine the effect of contextual ambiguity on keyword performance. We find that consumer click behavior varies significantly across keywords, and such variation can be partially explained by keyword category and the contextual ambiguity of keywords, Specifically higher contextual ambiguity is associated with higher CTR on top-positioned ads, but also a faster decay in CTR with screen position, Therefore the overall effect of contextual ambiguity on CTR varies across positions. In our study we propose based on probabilistic topic models from machine learning and computational linguistics using a hierarchical Bayesian approach that allows for topic-specific effects and nonlinear position effects, and jointly models click-through rate ( CTR ) and ad position ( rank ). We validate our study using a novel data set from a major search engine that contains information on consumer click activities for 2, 625 distinct keywords across multiple product categories from 10.
2K_test_931	Pipes carrying pressurized fluids are an important part of the civil infrastructure, and structural health monitoring ( SHM ) could ensure structural integrity by predicting and preventing structural failures Guided wave ultrasonics is a good candidate for use in pipe SHM because guided waves can propagate long distances and are sensitive to structural damage such as cracks and corrosion losses. However the multi-modal and dispersive characteristics of guided waves make it difficult to interpret their arrival records, Moreover guided waves are also sensitive to environmental and operational variations, limiting the effectiveness of ultrasonic methods to detect pipe damage in a real environment, that can identify a change of interest. We introduce a damage detector based on singular value decomposition ( SVD ), caused by a mass scatterer that simulates subtle damage, under realistic environmental variations. We show the effectiveness and robustness of this method on experimental data collected on a pipe segment under realistic environmental and operational variations over a time period of several months.
2K_test_933	Multimedia event detection has been one of the major endeavors in video event analysis, A variety of approaches have been proposed recently to tackle this problem, Among others using semantic representation has been accredited for its promising performance and desirable ability for human-understandable reasoning, To generate semantic representation, we usually utilize several external image/video archives and apply the concept detectors trained on them to the event videos, Due to the intrinsic difference of these archives, the resulted representation is presumable to have different predicting capabilities for a certain event. Notwithstanding not much work is available for assessing the efficacy of semantic representation from the source-level, On the other hand, it is plausible to perceive that some concepts are noisy for detecting a specific event. With encouraging results that validate the efficacy of our proposed approach. Motivated by these two shortcomings, we propose a bi-level semantic representation analyzing method, Regarding source-level our method learns weights of semantic representation attained from different multimedia archives, Meanwhile it restrains the negative influence of noisy or irrelevant concepts in the overall concept-level, In addition we particularly focus on efficient multimedia event detection with few positive examples, which is highly appreciated in the real-world scenario. We perform extensive experiments on the challenging TRECVID MED 2013 and 2014 datasets.
2K_test_935	In multi-core systems main memory is a major shared resource among processor cores A task running on one core can be delayed by other tasks running simultaneously on other cores due to interference in the shared main memory system. Such memory interference delay can be large and highly variable, thereby posing a significant challenge for the design of predictable real-time systems, to reduce this interference and provide an upper bound on the worst-case interference for reducing memory interference. We find that memory interference can be significantly reduced by ( i ) partitioning DRAM banks, and ( ii ) co-locating memory-intensive tasks on the same processing core Experimental results show that the predictions made by our approach are close to the measured worst-case interference under workloads with both high and low memory contention In addition, our memory interference-aware task allocation algorithm provides a significant improvement in task schedulability over previous work, with as much as 96 % more tasksets being schedulable. In this paper we present techniques on a multi-core platform that uses a commercial-off-the-shelf ( COTS ) DRAM system, We explicitly model the major resources in the DRAM system, including banks buses and the memory controller Based on these observations, we develop a memory interference-aware task allocation algorithm. By considering their timing characteristics, we analyze the worst-case memory interference delay imposed on a task by other tasks running in parallel We evaluate our approach on a COTS-based multi-core platform running Linux/RK.
2K_test_936	Sustainable building system design techniques aim to find an optimal balance between occupant comfort and the energy performance of HVAC systems, Design and implementation of effective heating ventilating and air conditioning ( HVAC ) controls is the key to achieve these optimal design conditions, Any anomalies in the functioning of a system component or a control system would result in occupant discomfort and/or energy wastage, While occupant discomfort can be directly sensed by occupants, measurement of waste in energy use would require additional sensing and analysis infrastructure One way of identifying such a waste is to compare asdesigned system requirements with the actual performance of the systems The findings in this paper substantiate the need to formally define the sequence of operations and also point to the need to verify the implemented controls in a given project to detect any deviations from the actual design intent. This paper presents an analysis of an air handling unit ( AHU ) in a five story office building and provides the comparison results of design requirements against the sensor data corresponding to the AHU parameters. Any deviation in the sensor data as compared to the expected operation pattern of the design intent indicated incorrect operation of the system with incorrectly implemented controls. One year sensor data for the AHU parameters was analyzed to assess the correctness of the implementation of the design intent, The design intent was interpreted from the sequence of operations ( SOOs ) and confirmed with a commissioning engineer, who worked on the project, The design intent was then graphically represented as a pattern that the sensor data corresponding to the controls is expected to follow if it follows the design intent.
2K_test_937	Elucidating assembly pathways of complex macromolecular structures, such as virus capsids, is an important problem for understanding the many cellular processes dependent on self-assembly but also challenging given limited experimental technologies for observing such systems, We have previously addressed this problem through simulation-based data fitting, learning rate parameters of coarse-grained stochastic simulation models to match light scattering data from bulk assembly of purified coat protein in vitro providing an unprecedented view of the fine-scale reaction pathways that might have produced those data, These simulation results help us understand how RNA viral coat and genome may interact in assembly to promote rapid growth while avoiding kinetic traps expected from much prior theory, bringing us a step closer to the goal of understanding how viral assembly in the cell may differ from our current conception based largely on in vitro models. A key question raised by such models, though is how well they might reflect assembly under more natural cellular conditions where factors such as local concentration changes, non-specific crowding and often the influence of nucleic acid during assembly become relevant In the present study, we examine the latter issue, how would influence overall pathways and kinetics, primarily with reference to cowpea chlorotic mottle virus ( CCMV ). We find a surprising complexity and synergy of interaction effects, Energetic effects that gain or lower free energy tend to disrupt successful assembly relative to the in vitro model individually, while the full combination of positive and negative effects collectively promotes greatly accelerated assembly without loss of yield, Furthermore it accomplishes this change in kinetics while substantially altering the ensemble of assembly pathways open to the system. Using analytical models of various contributions of RNA folding to assembly.
2K_test_940	Machine learning ( ML ) algorithms are commonly applied to big data, using distributed systems that partition the data across machines and allow each machine to read and update all ML model parameters -- - a strategy known as data parallelism, An alternative and complimentary strategy, model parallelism partitions the model parameters for non-shared parallel access and updates, and may periodically repartition the parameters to facilitate communication. Model parallelism is motivated by two challenges that data-parallelism does not usually address : ( 1 ) parameters may be dependent, thus naive concurrent updates can introduce errors that slow convergence or even cause algorithm failure ; ( 2 ) model parameters converge at different rates, thus a small subset of parameters can bottleneck ML algorithm completion, that improves ML algorithm convergence speed. We show that SchMP programs running on STRADS outperform non-model-parallel ML implementations : for example, SchMP LDA and SchMP Lasso respectively achieve 10x and 5x faster convergence than recent. We propose scheduled model parallelism ( SchMP ), a programming approach by efficiently scheduling parameter updates, taking into account parameter dependencies and uneven convergence, To support SchMP at scale, we develop a distributed framework STRADS which optimizes the throughput of SchMP programs, and benchmark four common ML applications written as SchMP programs : LDA topic modeling, matrix factorization sparse least-squares ( Lasso ) regression and sparse logistic regression By improving ML progress per iteration through SchMP programming whilst improving iteration throughput through STRADS.
2K_test_941	Sequential games of perfect information can be solved by backward induction, where solutions to endgames are propagated up the game tree. However this does not work in imperfect-information games because different endgames can contain states that belong to the same information set and can not be treated independently, In fact we show that this approach can fail even in a simple game with a unique equilibrium and a single endgame, to conduct endgame solving in a scalable way. Show that our approach leads to significant performance improvements in practice. Nonetheless we show that endgame solving can have significant benefits in imperfectinformation games with large state and action spaces : computation of exact ( rather than approximate ) equilibrium strategies, computation of relevant equilibrium refinements, significantly finer-grained action and information abstraction, new information abstraction algorithms that take into account the relevant distribution of players types entering the endgame, being able to select the coarseness of the action abstraction dynamically, additional abstraction techniques for speeding up endgame solving, a solution to the off-tree problem, and using different degrees of probability thresholding in modeling versus playing, We discuss each of these topics in detail, and introduce techniques that enable one even when the number of states and actions in the game is large. Our experiments on two-player no-limit Texas Holdem poker.
2K_test_943	Privacy decision making has been examined from various perspectives, A dominant normative perspective has focused on rational processes by which consumers with stable preferences for privacy weigh the expected benefits of privacy choices against their potential costs More recently, an alternate behavioral perspective has leveraged theories from behavioral decision research to construe privacy decision making as a process in which cognitive heuristics and biases predictably occur Our results suggest a way to integrate diverse streams of IS literature on privacy decision making : consumers may both over-estimate their response to normative factors and under-estimate their response to behavioral factors in hypothetical choice contexts relative to actual choice contexts. We compare the predictive power of these two perspectives. We find that both relative and objective risks can, in fact impact consumer privacy decisions, However and surprisingly the impact of objective changes in risk diminishes between hypothetical and actual choice settings, Vice versa the impact of relative risk is more pronounced going from hypothetical to actual choice settings. In a series of experiments by evaluating the impact of changes in objective risk of disclosure and the impact of changes in relative perceptions of risk of disclosure on both hypothetical and actual consumer privacy choices.
2K_test_944	Communication and coordination play a major role in the ability of bacterial cells to adapt to ever changing environments and conditions. Recent work has shown that such coordination underlies several aspects of bacterial responses including their ability to develop antibiotic resistance, that helps explain how bacterial cells collectively search for food in harsh environments using extremely limited communication and com- putational complexity. Prove that the method we propose leads to convergence even when using a dynamically changing interaction network, illustrate the ability of the method to explain and further predict several aspects of bacterial swarm food search. Here we develop a new distributed gradient descent method This method can also be used for computational tasks when agents are facing similarly restricted conditions We formalize the communication and computation assumptions re- quired for successful coordination and The proposed method improves upon prior models suggested for bacterial foraging despite making fewer assumptions. Simulation studies and analysis of experimental data.
2K_test_945	ABSTRACT Clad steel refers to a thick carbon steel structural plate bonded to a corrosion resistant alloy ( CRA ) plate, such as stainless steel or titanium, and is widely used in industry to construct pressure vessels, The CRA resists the chemically aggressive environment on the interior, but can not prevent the development of corrosion losses and cracks that limit the continued safe operation of such vessels, In previous resear ch, sponsored by industry to detect and localize damage in pressurized piping systems under operational and environmental changes, we investigated a number of data-driven signal processing methods to extract damage information from ultrasonic guided wave pitch-catch records ; we also discuss observations of plate-like mode properties implied by these results. At present there are no practical methods to detect such defects from the exposed outer surface of the thick carbon steel plate, often necessitating removing such vessels from service and inspecting them visually from the interior. We discuss conditions under which localization is achieved by relatively simple first-arrival methods, and other conditions for which data-driven methods are needed. We now apply those methods to relatively large clad steel plate specimens. We study a sparse array of wafer-type ultrasonic transducers adhered to the carbon steel surface, attempting to localize mass scatterers grease-coupled to the stainless steel surface.
2K_test_946	Many commercial products and academic research activities are embracing behavior analysis as a technique for improving detection of attacks of many sortsfrom retweet boosting, hashtag hijacking to link advertising, Traditional approaches focus on detecting dense blocks in the adjacency matrix of graph data, and recently the tensors of multimodal data. No method gives a principled way to score the suspiciousness of dense blocks with different numbers of modes and rank them to draw human attention accordingly, to spot dense blocks that are worth inspecting. Where it improves the F1 score over previous techniques by 68percent and finds suspicious behavioral patterns in social datasets spanning 0. In this paper we first give a list of axioms that any metric of suspiciousness should satisfy ; we propose an intuitive, principled metric that satisfies the axioms, and is fast to compute ; moreover, we propose CrossSpot an algorithm typically indicating fraud or some other noteworthy deviation from the usual, and sort them in the order of importance ( suspiciousness ). Finally we apply CrossSpot to the real data.
2K_test_947	A traditional goal of neural recording with extracellular electrodes is to isolate action potential waveforms of an individual neuron, Recently in braincomputer interfaces ( BCIs ), it has been recognized that threshold crossing events of the voltage waveform also convey rich information, To date the threshold for detecting threshold crossings has been selected to preserve single-neuron isolation, How neural signals are processed impacts the information that can be extracted from them, Both the type and quality of information contained in threshold crossings depend on the threshold setting, There is more information available in these signals than is typically extracted, Adjusting the detection threshold to the parameter of interest in a BCI context should improve our ability to decode motor intent, and thus enhance BCI control, Further by sweeping the detection threshold, one can gain insights into the topographic organization of the nearby neural tissue. Objective However the optimal threshold for single-neuron identification is not necessarily the optimal threshold for information extraction to determine the best threshold for extracting information from extracellular recordings. The optimal threshold depends on the desired information, In M1 velocity is optimally encoded at higher thresholds than speed ; in both cases the optimal thresholds are lower than are typically used in BCI applications, In V1 information about the orientation of a visual stimulus is optimally encoded at higher thresholds than is visual contrast. Here we introduce a procedure We apply this procedure in two distinct contexts : the encoding of kinematic parameters from neural activity in primary motor cortex ( M1 ), and visual stimulus parameters from neural activity in primary visual cortex ( V1 Approach We record extracellularly from multi-electrode arrays implanted in M1 or V1 in monkeys, Then we systematically sweep the voltage detection threshold and quantify the information conveyed by the corresponding threshold crossings A conceptual model explains these results as a consequence of cortical topography.
2K_test_948	Although widely used Multilinear PCA ( MPCA ), one of the leading multilinear analysis methods, still suffers from four major drawbacks, First it is very sensitive to outliers and noise, Second it is unable to cope with missing values, Third it is computationally expensive since MPCA deals with large multi-dimensional datasets, Finally it is unable to maintain the local geometrical structures due to the averaging process, to solve the four problems mentioned above. We show that CSMA method can achieve good results and is very efficient in the inpainting problem as compared to [ 1 ], [ 2 ] Our method also achieves higher face recognition rates. This paper proposes a novel approach named Compressed Submanifold Multifactor Analysis ( CSMA ) Our approach can deal with the problem of missing values and outliers via SVD-L1 The Random Projection method is used to obtain the fast low-rank approximation of a given multifactor dataset, In addition it is able to preserve the geometry of the original data Our CSMA method can be used efficiently for multiple purposes, noise and outlier removal, estimation of missing values. Compared to LRTC SPMA MPCA and some other methods, PCA LDA and LPP, on three challenging face databases, CMU-MPIE CMU-PIE and Extended YALE-B.
2K_test_949	Consumer privacy decision making is often layered : different interrelated decisions determine, together a final privacy outcome and its associated benefits and costs Layered privacy choices are particularly common online, where consumers are frequently tasked with multiple, sequential choices ( such as first selecting a services privacy settings, and then engaging in privacy-sensitive behaviors ) that will ultimately impact their privacy trade-offs Implications for privacy decision research as well as policy makers are discussed. The layered nature of online privacy choices has important implications for models of privacy decision making and for consumers assumption of privacy risks, In this manuscript we investigate how changes in the architecture of privacy choices affect an initial layer of privacy choice, and how that effect percolates through subsequent layers of privacy choices. We find that various manipulations of decision frames, common to privacy contexts, can significantly alter individual choice of privacy protective options, Further and importantly we find that participants subsequent disclosure behavior stays constant despite the shifts in chosen privacy protections induced by choice framing. Specifically in a series of experiments, we investigate the impact of framing on participants initial privacy choices, and whether participants subsequent behaviors take account of, and neutralize that impact.
2K_test_950	Motivation : As cancer researchers have come to appreciate the importance of intratumor heterogeneity, much attention has focused on the challenges of accurately profiling heterogeneity in individual patients Experimental technologies for directly profiling genomes of single cells are rapidly improving, but they are still impractical for large-scale sampling, Bulk genomic assays remain the standard for population-scale studies, but conflate the influences of mixtures of genetically distinct tumor, stromal and infiltrating immune cells, Many computational approaches have been developed to deconvolute these mixed samples and reconstruct the genomics of genetically homogeneous clonal subpopulations, All such methods however, are limited to reconstructing only coarse approximations to a few major subpopulations, In prior work we showed that one can improve deconvolution of genomic data by leveraging substructure in cellular mixtures through a strategy called simplicial complex inference. This strategy however is also limited by the difficulty of inferring mixture structure from sparse, We improve on past work to better decompose mixture model substructure to better identify substructure in sparse. Results We show that these improvements lead to more accurate inference of cell populations and mixture proportions We further demonstrate their effectiveness in identifying mixture substructure Availability : Source code is available at this http URL. By introducing enhancements to automate learning of substructured genomic mixtures, with specific emphasis on genome-wide copy number variation ( CNV ) data, We introduce methods for dimensionality estimation fuzzy clustering and automated model inference methods for other key model parameters. In real tumor CNV data.
2K_test_951	The pervasiveness of mobile technologies today have facilitated the creation of massive crowdsourced and geotagged data from individual users in real time and at different locations in the city, Such ubiquitous user-generated data allow us to infer various patterns of human behavior, which help us understand the interactions between humans and cities, Our study demonstrates the potential of how to best make use of the large volumes and diverse sources of crowdsourced and geotagged user-generated data to create matrices to predict local economic demand in a manner that is fast, cheap accurate and meaningful. In this study we focus on understanding users economic behavior in the city by examining the economic value from crowdsourced and geotaggged data. Our results suggest that foot traffic can increase local popularity and business performance, while mobility and traffic from automobiles may hurt local businesses, especially the well-established chains and high-end restaurants We also find that on average one more street closure nearby leads to a 4, 7 % decrease in the probability of a restaurant being fully booked during the dinner peak. Our study is instantiated on a unique dataset of restaurant bookings from OpenTable for 3, 187 restaurants in New York City from November 2013 to March 2014. Specifically we extract multiple traffic and human mobility features from publicly available data sources using NLP and geo-mapping techniques, and examine the effects of both static and dynamic features on economic outcome of local businesses.
2K_test_952	How much has a network changed since yesterday ? How different is the wiring of Bobs brain ( a left-handed male ) and Alices brain ( a right-handed female ), and how is it different ? Graph similarity with given node correspondence, the detection of changes in the connectivity of graphs, arises in numerous settings. In this work we formally state the axioms and desired properties of the graph similarity functions, that assesses the similarity between two graphs on the same nodes that enables attribution of change or dissimilarity to responsible nodes and edges. Showcase the advantages of our method over existing similarity measures. We propose D elta C on, a principled intuitive and scalable algorithm ( e, employees of a company, customers of a mobile carrier In conjunction, we propose D elta C on -A ttr. And evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs, Experiments on various synthetic and real graphs Finally, we employ D elta C on and D elta C on -A ttr on real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs, ( b ) do temporal anomaly detection in the who-emails-whom Enron graph and find the top culprits for the changes in the temporal corporate email graph, and ( c ) recover pairs of test-retest large brain scans ( 17M edges, up to 90M edges ) for 21 subjects.
2K_test_953	Effective enforcement of laws and policies requires expending resources to prevent and detect offenders, as well as appropriate punishment schemes to deter violators, In particular enforcement of privacy laws and policies in modern organizations that hold large volumes of personal information ( e, hospitals banks ) relies heavily on internal audit mechanisms. We study economic considerations in the design of these mechanisms, focusing in particular on effective resource allocation and appropriate punishment schemes, for resource allocation with an additional punishment parameter. We present an audit game model that is a natural generalization of a standard security game model Computing the Stackelberg equilibrium for this game is challenging because it involves solving an optimization problem with non-convex quadratic constraints We present an additive FPTAS that efficiently computes the solution.
2K_test_954	Admixture-introduced linkage disequilibrium ( LD ) has recently been introduced into the inference of the histories of complex admixtures, Our method is a considerable improvement over other current methods and further facilitates the inference of the histories of complex population admixtures. However the influence of ancestral source populations on the LD pattern in admixed populations is not properly taken into consideration by currently available methods, which affects the estimation of several gene flow parameters from empirical data. And it was shown to be more accurate than MALDER, a state-of-the-art method that was recently developed for similar purposes, under various admixture models, Interestingly we were able to identify more than one admixture events in several populations, which have yet to be reported, For example two major admixture events were identified in the Xinjiang Uyghur, occurring around 27 ? ? ? 30 generations ago and 182 ? ? ? 195 generations ago, In an African population ( MKK ), three recent major admixtures occurring 13 ? ? ? 16, 50 ? ? ? 67, and 107 ? ? ? 139 generations ago were detected. We first illustrated the dynamic changes of LD in admixed populations and mathematically formulated the LD under a generalized admixture model with finite population size, We next developed a new method, MALDmef by fitting LD with multiple exponential functions for inferring and dating multiple-wave admixtures MALDmef takes into account the effects of source populations which substantially affect modeling LD in admixed population, which renders it capable of efficiently detecting and dating multiple-wave admixture events. The performance of MALDmef was evaluated by simulation We further applied MALDmef to analyzing genome-wide data from the Human Genome Diversity Project ( HGDP ) and the HapMap Project.
2K_test_955	Large-scale deep learning requires huge computational resources to train a multi-layer neural network, Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections. While the computation involved can be done more efficiently on GPUs than on more traditional CPU cores, training such networks on a single GPU is too slow and training on distributed GPUs can be inefficient, due to data movement overheads, GPU stalls and limited GPU memory. We show that GeePS enables a state-of-the-art single-node GPU implementation to scale well, such as to 13 times the number of training images processed per second on 16 machines ( relative to the original optimized single-node code ) Moreover, GeePS achieves a higher training throughput with just four GPU machines than that a state-of-the-art CPU-only system achieves with 108 machines. This paper describes a new parameter server, called GeePS that supports scalable deep learning across GPUs distributed among multiple machines.
2K_test_956	Often Big Data applications collect a large number of time series, for example the financial data of companies quoted in a stock exchange, the health care data of all patients that visit the emergency room of a hospital, or the temperature sequences continuously measured by weather stations across the US, A first task in the analytics of these data is to derive a low dimensional representation, a graph or discrete manifold, that describes well the interrelations among the time series and their intrarelations across time. For estimating this graph structure from the available data. The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested. This paper presents a computationally tractable algorithm This graph is directed and weighted, possibly representing causal relations, not just reciprocal correlations as in many existing approaches in the literature, A detailed convergence analysis is carried out. The algorithm is demonstrated on random graph and real network time series datasets, and its performance is compared to that of related methods.
2K_test_957	Given a large collection of time-evolving activities, such as Google search queries, which consist of d keywords/activities for m locations of duration n, how can we analyze temporal patterns and relationships among all these activities and find location-specific trends ? How do we go about capturing non-linear evolutions of local activities and forecasting future patterns ? For example, assume that we have the online search volume for multiple keywords, `` Nokia/Nexus/Kindle '' or `` CNN/BBC '' for 236 countries/territories, from 2004 to 2015. Our goal is to analyze a large collection of multi-evolving activities, and specifically to answer the following questions : ( a ) Is there any sign of interaction/competition between two different keywords If so, who competes with whom ? ( b ) In which country is the competition strong ? ( c ) Are there any seasonal/annual activities ? ( d ) How can we automatically detect important world-wide ( or local ) events ?. Demonstrate that COMPCUBE consistently outperforms the best state-of- the-art methods in terms of both accuracy and execution speed. We present COMPCUBE a unifying non-linear model, which provides a compact and powerful representation of co-evolving activities ; and also a novel fitting algorithm, COMPCUBE-FIT which is parameter-free and scalable Our method captures the following important patterns : ( B ) asic trends, non-linear dynamics of co-evolving activities, signs of ( C ) ompetition and latent interaction, Nexus ( S ) easonality, a Christmas spike for iPod in the U, and Europe and ( D ) eltas, unrepeated local events such as the U, election in 2008 Thanks to its concise but effective summarization, COMPCUBE can also forecast long-range future activities. Extensive experiments on real datasets.
2K_test_959	Given a large collection of co-evolving online activities, such as searches for the keywords `` Xbox '', `` PlayStation '' and `` Wii '', how can we find patterns and rules ? Are these keywords related ? If so, are they competing against each other ?. Can we forecast the volume of user activity for the coming month ? We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food for mining large-scale co-evolving online activities. Show that EcoWeb is effective, in that it can capture long-range dynamics and meaningful patterns such as seasonalities, and practical in that it can provide accurate long-range forecasts, EcoWeb consistently outperforms existing methods in terms of both accuracy and execution speed. We present EcoWeb ( i, Ecosystem on the Web ), which is an intuitive model designed as a non-linear dynamical system Our second contribution is a novel, parameter-free and scalable fitting algorithm, EcoWeb-Fit that estimates the parameters of EcoWeb. Extensive experiments on real data.
2K_test_961	T cells must receive signals through the T cell receptor ( TCR ) and the costimulatory receptor CD28 to become fully activated, This combination of imaging and computational analysis could be applied to other systems to determine the spatiotemporal dynamics of signaling molecules. Critical to this process is the reorganization of plasma membrane actin at the immunological synapse, the interface between a T cell and an antigen-presenting cell. The regulatory proteins WAVE2 and cofilin were efficiently recruited to the immunological synapse only when both TCR and CD28 signaled Constitutive activation of either protein in TCR-stimulated T cells enabled normal actin reorganization even when CD28 signaling was blocked. Imaged actin and fluorescently tagged actin regulatory proteins in T cells activated through the TCR in the absence or presence of CD28 signaling, Computational image processing to normalize differences in cell shape enabled tracking of the fluorescent proteins.
2K_test_962	A grand challenge for state estimation in newly built smart grid lies in how to deal with the increasing uncertainties, To solve the problem. Results show that the proposed data-driven approach works well in a smart grid setting with increasing uncertainties and it produces an online state estimate excelling current industrial approach. We propose a data-driven state estimation approach based on recent targeted investment on sensors, data storage and computing devices, An architecture is proposed to use power system physics and pattern to systematically clean historical data and conduct supervised learning, where historical similar measurements and their states are used to learn the relationship between the current measurement and the state, In order to deal with nonlinearity, kernel trick is used to produce linear mapping in a carefully selected higher dimensional space To speed up the data-driven approach for online services, we analyze power system data set and discover its clustering property due to the periodic pattern of power systems, This leads to significant dimension reduction and the idea of preorganizing data points in a tree structure for inquiry, leading to 1000 times speedup.
2K_test_963	Multi-person tracking plays a critical role in the analysis of surveillance video thus potentially opening the door to automatic summarization of the vast amount of surveillance video generated every day. However most existing work focus on shorter-term ( e, minute-long or hour-long ) video sequences. And we were able to localize a person 53, 2 % of the time with 69, Results showed that we were able to generate a reasonable visual diary ( i, a summary of what a person did ) for different people. Therefore we propose a multi-person tracking algorithm for very long-term ( e, month-long ) multi-camera surveillance scenarios, Long-term tracking is challenging because 1 ) the apparel/appearance of the same person will vary greatly over multiple days and 2 ) a person will leave and re-enter the scene numerous times, To tackle these challenges, we leverage face recognition information, which is robust to apparel change, to automatically reinitialize our tracker over multiple days of recordings, Unfortunately recognized faces are unavailable oftentimes Therefore, our tracker propagates identity information to frames without recognized faces by uncovering the appearance and spatial manifold formed by person detections. We tested our algorithm on a 23-day 15-camera data set ( 4, 935 hours total ), We further performed video summarization experiments based on our tracking output, 25 hours of video.
2K_test_964	Cyber-physical systems ( CPS ) are heterogeneous, be- cause they tightly couple computation, communication and control along with physical dynamics, which are traditionally considered separately, Without a comprehensive modeling formalism, model- based development of CPS involves using a multitude of models in a variety of formalisms that capture various aspects of the system design, such as software design, networking design physical mod- els. Without a rigorous unifying framework, system integration and integration of the analysis results for vari- ous models remains ad hoc to ensure consistency and enable system-level verification in a hierarchical and compositional manner. In this paper we propose a multi-view architecture framework that treats models as views of the under- lying system structure and uses structural and semantic mappings Index TermsControl design, control engineering formal veri- fication. Throughout the paper the theoretical concepts are illustrated using two examples : a quad- rotor and an automotive intersection collision avoidance system.
2K_test_965	That allows end-users to instruct the crowd to create trigger-action ( `` if, then '' ) rules based on their needs. In this paper we introduce InstructableCrowd, a system We create a framework which enables users to converse with the crowd using their phone and describe a problem which they might have, We create an interface for a crowd worker to both chat with the user and compose a rule with an `` IF '' part connected to the user 's phone sensors ( e, incoming emails GPS location, meeting calendar weather information etc, ) and a `` THEN '' part connected to user 's phone effectors ( e, sending an email creating an alarm, posting a tweet etc, The system then sends the rules created by the crowd to the user 's phone in order to help the user solve his problem.
2K_test_966	The maximum Nash welfare ( MNW ) solution -- - which selects an allocation that maximizes the product of utilities -- - is known to provide outstanding fairness guarantees when allocating divisible goods, These results lead us to believe that MNW is the ultimate solution for allocating indivisible goods, and underlie its deployment on a popular fair division website. In particular we prove that it selects allocations that are envy free up to one good -- - a compelling notion that is quite elusive when coupled with economic efficiency. Demonstrate that it scales well. And while it seems to lose its luster when applied to indivisible goods, we show that in fact, the MNW solution is unexpectedly, strikingly fair even in that setting, We also establish that the MNW solution provides a good approximation to another popular ( yet possibly infeasible ) fairness property, the maximin share guarantee, in theory and -- - even more so -- - in practice, While finding the MNW solution is computationally hard, we develop a nontrivial implementation.
2K_test_968	The safety of mobile robots in dynamic environments is predicated on making sure that they do not collide with obstacles, Our verification results are generic in the sense that they are not limited to the particul ar choices of one specific control algorithm but identify conditions that make them simultaneously apply to a broad class of control algorithms. In support of such safety arguments, we analyze fo r avoiding both stationary and moving obstacles that describe and formally verify the robots discrete control decisions along with it s continuous. We prove that provably safe motion is flexible enough to let the r obot still navigate waypoints and pass intersections Moreover, we formally prove that safety can still be guaranteed despite s ensor uncertainty and actuator perturbation, and when control choices for more aggressive maneuvers are introduced. And formally verify a series of increasingly powerful safety properties of controllers ( i ) static safety, which ensures that no collisions can happen with stationary obstacles, ( ii ) passive safety, which ensures that no collisions can happen with stationary or moving obstacles while the robot moves, ( iii ) the stronger passive friendly safety in which the robot further maintains sufficient maneuvering distance for obstacles to avoid collision as well, and ( iv ) passive orientation safety, which allows for imperfect sensor coverage of the robot, the robot is aw are that not everything in its environment will be visible, We complement these provably correct safety properties with liveness properties : We use hybrid system models and theorem proving techniques.
2K_test_970	Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral, ) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph, Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look `` normal '', Even worse some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. ( c ) is effective in real-world data, show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud, FRAUDAR successfully detected a subgraph of more than 4000 detected accounts, of which a majority had tweets showing that they used follower-buying services. We propose FRAUDAR an algorithm that ( a ) is camouflage-resistant, ( b ) provides upper bounds on the effectiveness of fraudsters. Experimental results under various attacks Additionally, in real-world experiments with a Twitter follower-followee graph of 1.
2K_test_973	The underlying motivating application is epidemics like computer virus spreading, for example in wide campus local networks. Abstract : We study the emergence of global behavior in large scale networks. We consider multiple classes of viruses, each type bearing their own statistical characterization -- exogenous contamination, contagious propagation and healing, The network state ( distribution of nodes infected by each class in the network ) is a jump Markov process, not necessarily reversible making it a challenge to obtain its invariant distribution, By suitable renormalization in the limit of a large network ( number of nodes ), we describe the macroscopic or emergent behavior of the network by the solution of a set of deterministic nonlinear differential equations These nonlinear differential equations are obtained by mean field analysis of the microscopic random dynamics. We study the qualitative behavior of the nonlinear differential equations describing the mean field dynamics.
2K_test_974	The ubiquitous deployment of mobile and sensor technologies has led to both the capacity to observe human behavior in physical ( offline ) settings as well as to record it, Finally our study has important welfare implications in that efficient information sharing leads to an income increase among all drivers, instead of a redistribution of income between different types of drivers Our work allows us not only to explain driver decision making behavior using these detailed behavioral traces, but also to prescribe information sharing strategy for the firm in order to improve the overall market efficiency. This provides researchers with a new lens to study and better understand the individual decision processes that were previously unobserved. We find strong heterogeneity in individual learning behavior and driving decisions, which is significantly associated with individual economic outcomes, Drivers with higher incomes benefit significantly from their ability to learn from not only demand information directly observable in the local market, but also aggregate information on demand flows across markets, Interestingly our policy simulations indicate information that is noisy at the individual level becomes valuable after being aggregated across various spatial and temporal dimensions Moreover, the value of information does not increase monotonically with the scale and frequency of information sharing. This capacity to use data where occupancy of the taxi is known is a distinctive feature of our data set and sets this work apart from prior work which has attempted to study driver behavior We conduct our study using a heterogeneous Bayesian learning model. In this paper we study decision making behavior of 11, 196 taxi drivers in a large Asian city using a rich data set consisting of 10, 6 million fine-grained GPS trip records These records include detailed taxi GPS trajectories, taxi occupancy data ( i, whether a taxi was occupied with a passenger or was vacant ) and taxi drivers daily incomes The specific decision we focus on pertains to actions drivers take to find new passengers after they have dropped off their current passengers, In particular we study the role of information derivable from the GPS trace data ( e, where passengers are dropped off, where passengers are picked up, longitudinal taxicab travel history with fine-grained time stamps ) observable by or made available to drivers in enabling them to learn the distribution of demand for their services over space and time.
2K_test_976	For approximate maximum a posteriori ( MAP ) inference on factor graphs. Show that AD3 compares favorably with the state-of-the-art. We present AD3 a new algorithm, based on the alternating directions method of multipliers, Like other dual decomposition algorithms, AD3 has a modular architecture, where local subproblems are solved independently, and their solutions are gathered to compute a global update, The key characteristic of AD3 is that each local subproblem has a quadratic regularizer, leading to faster convergence, both theoretically and in practice, We provide closed-form solutions for these AD3 subproblems for binary pairwise factors and factors imposing first-order logic constraints, For arbitrary factors ( large or combinatorial ), we introduce an active set method which requires only an oracle for computing a local MAP configuration, making AD3 applicable to a wide range of problems. Experiments on synthetic and real-world problems.
2K_test_977	To execute software long after its creation to encapsulate legacy software. We describe a system called Olive that freezes and precisely reproduces the environment necessary It uses virtual machine ( VM ) technology, complete with all its software dependencies, This legacy world can be completely closed-source : there is no requirement for availability of source code, nor a requirement for recompilation or relinking, The entire VM is streamed over the Internet from a web server, much as video is streamed today.
2K_test_978	What is the growth pattern of social networks, like Facebook and WeChat ? Does it truly exhibit exponential early growth, as predicted by textbook models like the Bass model, SI or the Branching Process ? How about the count of links, over time for which there are few published models ?. ; and we observe power law growth for both nodes and links, a fact that completely breaks the sigmoid models ( like SI, where NETTIDE gives good fitting accuracy, and more importantly applied on the WeChat data, our NETTIDE forecasted more than 730 days into the future, with 3 % error. In its place we propose NETTIDE, along with differential equations for the growth of the count of nodes, as well as links, Our model accurately fits the growth patterns of real graphs ; it is general, encompassing as special cases all the known, traditional models ( including Bass, SI log-logistic growth ) ; while still remaining parsimonious, requiring only a handful of parameters Moreover, our NETTIDE for link growth is the first one of its kind, accurately fitting real data, and naturally leading to the densification phenomenon. We examine the growth of several real networks, including one of the world 's largest online social network, `` WeChat '' with 300 million nodes and 4, 75 billion links by 2013 We validate our model with four real.
2K_test_981	How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time ? How do people join the social groups, uniformly or with burst ? What is the pattern of people quitting from groups ? Is there a simple universal model to depict the come-and-go patterns of various groups ? for group evolution. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR For example, we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore we propose a new model comeNgo, a concise yet flexible dynamic model Our model has the following advantages : ( a ) unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation, ( b ) succinctness and interpretability : it contains only six parameters with clear physical meanings, ( c ) accuracy : it can capture various kinds of group evolution patterns preciously and the goodness of fit increase by 58 % over baseline, ( d ) usefulness : it can be used in multiple application scenarios such as forecasting and pattern discovery. In this paper we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users.
2K_test_982	System management includes the selection of maintenance actions depending on the available observations : when a system is made up by components known to be similar, data collected on one is also relevant for the management of others, This is typically the case of wind farms, which are made up by similar turbines, Optimal management of wind farms is an important task due to high cost of turbines operation and maintenance : in this context, we recently proposed a method for planning and learning at system-level, called PLUS built upon the Partially Observable Markov Decision Process ( POMDP ) framework, which treats transition and emission probabilities as random variables, and is therefore suitable for including model uncertainty. PLUS models the components as independent or identical, In this paper we extend that formulation, allowing for a weaker similarity among components. And discuss its potential and computational complexity. The proposed approach called Multiple Uncertain POMDP ( MU-POMDP ), models the components as POMDPs, and assumes the corresponding parameters as dependent random variables Through this framework, we can calibrate specific degradation and emission models for each component while, at the same time, process observations at system-level. We compare the performance of the proposed MU-POMDP with PLUS.
2K_test_983	Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type, and the tensor-based summaries look for high-order dense blocks by clustering the values ( including timestamps ) in each dimension. However the human behaviors are multicontextual and dynamic : ( 1 ) each behavior takes place within multiple contexts in a few dimensions, which requires the representation to enable non-value and set-values for each dimension ; ( 2 ) many behavior collections, such as tweets or papers, evolve over time we represent the behavioral data for behavioral summary to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way. CatchTartan outperforms the baselines on both the accuracy and speed, providing comprehensive summaries for the events, human life and scientific development. In this paper as a two-level matrix ( temporal-behaviors by dimensional-values ) and propose a novel representation called Tartan that includes a set of dimensions, the values in each dimension, a list of consecutive time slices and the behaviors in each slice, We further develop a propagation method CatchTartan it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner. We apply CatchTartan to four Twitter datasets up to 10 million tweets and the DBLP data.
2K_test_984	In this paper we address the distributed filtering and prediction of time-varying random fields represented by linear time-invariant ( LTI ) dynamical systems. We prove that the mean-squared error of the estimator asymptotically converges if the degree of instability of the field dynamics is within a prespecified threshold defined as tracking capacity of the estimator The tracking capacity is a function of the local observation models and the agent communication network yielding distributed estimates with minimized mean-squared error, we show that the distributed estimator with optimal gains converges faster and with approximately 3dB better mean-squared error performance than previous distributed estimators. We develop a Kalman filter type consensus + innovations distributed linear estimator of the dynamic field termed as Consensus+Innovations Kalman Filter We design the optimal consensus and innovation gain matrices. The field is observed by a sparsely connected network of agents/sensors collaborating among themselves We analyze the convergence properties of this distributed estimator Through numerical evaluations.
2K_test_985	In this report we describe CMU-SMUs participation in the Video Hyperlinking task of TRECVID 2015. Our experiments mainly focus on the study of different features on the performance of video hyperlinking, including subtitle metadata audio and visual features, as well as the consideration of surrounding context. Results show that ( 1 ) the context does not generally improve results, ( 2 ) the search performance mainly rely on textual features, and the combination of audio and visual feature can not provide improvements ; ( 3 ) due to the lack of training examples, machine learning techniques can not provide contributions. We treat video hyperlinking as ad-hoc retrieval scenario and use a variety of retrieval methods, Different combination strategies are used to combine those features, Besides we also attempt to categorize the queries and use different search strategies for different categories.
2K_test_986	The physical constraints of smartwatches limit the range and complexity of tasks that can be completed, WearWrite represents a new approach to getting work done from wearables using the crowd. Despite interface improvements on smartwatches, the promise of enabling productive work remains largely unrealized users to write documents from their smartwatches. We validate that it is possible to manage the crowd writing process from a watch. This paper presents WearWrite, a system that enables by leveraging a crowd to help translate their ideas into text, WearWrite users dictate tasks, respond to questions and receive notifications of major edits on their watch, Using a dynamic task queue, the crowd receives tasks issued by the watch user and generic tasks from the system Watch users captured new ideas as they came to mind and managed a crowd during spare moments while going about their daily routine. In a week-long study with seven smartwatch users supported by approximately 29 crowd workers each.
2K_test_987	To the group fused lasso, a convex model that approximates a multi-dimensional signal via an approximately piecewise-constant signal. We show that on these problems the proposed method performs very well, solving the problems faster than state-of-the-art methods and to higher accuracy. We present a new algorithmic approach This model has found many applications in multiple change point detection, signal compression and total variation denoising, though existing algorithms typically using first-order or alternating minimization schemes, In this paper we instead develop a specialized projected Newton method, combined with a primal active set approach, which we show to be substantially faster that existing methods, Furthermore we present two applications that use this algorithm as a fast subroutine for a more complex outer loop : segmenting linear regression models for time series data, and color image denoising.
2K_test_988	Crowdsourced clustering approaches present a promising way to harness deep semantic knowledge for clustering complex information. However existing approaches have difficulties supporting the global context needed for workers to generate meaningful categories, and are costly because all items require human judgments. We introduce Alloy a hybrid approach that combines the richness of human judgments with the power of machine algorithms, Alloy supports greater global context through a new `` sample and search '' crowd pattern which changes the crowd 's task from classifying a fixed subset of items to actively sampling and querying the entire dataset It also improves efficiency through a two phase process in which crowds provide examples to help a machine cluster the head of the distribution, then classify low-confidence examples in the tail To accomplish this, Alloy introduces a modular `` cast and gather '' approach which leverages a machine learning backbone to stitch together different types of judgment tasks.
2K_test_989	As our approach is compact, non-invasive low-cost and low-powered, we envision the technology being integrated into future smartwatches, supporting rich touch interactions beyond the confines of the small touchscreen. That enables continuous touch tracking on the skin. Our approach can segment touch events at 99 % accuracy, and resolve the 2D location of touches with a mean error of 7. SkinTrack is a wearable system It consists of a ring, which emits a continuous high frequency AC signal, and a sensing wristband with multiple electrodes, Due to the phase delay inherent in a high-frequency AC signal propagating through the body, a phase difference can be observed between pairs of electrodes, SkinTrack measures these phase differences to compute a 2D finger touch coordinate.
2K_test_990	Strong Nash equilibrium ( SNE ) is an appealing solution concept when rational agents can form coalitions, A strategy profile is an SNE if no coalition of agents can benefit by deviating. For SNE finding in games with more than two agents. Validate the overall approach and show that the new conditions significantly reduce search tree size compared to using NE conditions alone. We present the first general-purpose algorithms An SNE must simultaneously be a Nash equilibrium ( NE ) and the optimal solution of multiple non-convex optimization problems, This makes even the derivation of necessary and sufficient mathematical equilibrium constraints difficult, We show that forcing an SNE to be resilient only to pure-strategy deviations by coalitions, unlike for NEs is only a necessary condition here, Second we show that the application of Karush-Kuhn-Tucker conditions leads to another set of necessary conditions that are not sufficient, Third we show that forcing the Pareto efficiency of an SNE for each coalition with respect to coalition correlated strategies is sufficient but not necessary We then develop a tree search algorithm for SNE finding At each node, it calls an oracle to suggest a candidate SNE and then verifies the candidate, We show that our new necessary conditions can be leveraged to make the oracle more powerful.
2K_test_991	How can we predict Smith 's main hobby if we know the main hobby of Smith 's friends ? Can we measure the confidence in our predic- tion if we are given the main hobby of only a few of Smith 's friends ?. In this paper we focus on how to estimate the confidence on the node classi- fication problem. Results demonstrate that our algorithm outperforms other algorithms on graphs with less smoothness and low label density. Providing a confidence level for the classification prob- lem is important because most nodes in real world networks tend to have few neighbors, and thus a small amount of evidence, Our contributions are three-fold : ( a ) novel algorithm ; we propose a semi-supervised learning algorithm that converges fast, and provides the confidence estimate. ( b ) theoretical analysis ; we show the solid theoretical foundation of our algo- rithm and the connections to label propagation and Bayesian inference ( c ) empirical analysis ; we perform extensive experiments on three dif- ferent real networks Specifically.
2K_test_992	Given their large energy footprints and the availability of building energy management systems, airports are uniquely positioned to take advantage of demand response ( DR ) programs, Therefore further studies should be carried out to conclude the potential of flight schedules in improving accuracies of energy prediction baselines. Although a baselinethe estimation of what the load would have been without load reductionis essential to assess the performance of DR strategies, there has been very little published research on developing baselines for airports, Therefore the research described in this paper aims to develop baseline models specially intended for airport facilities, for predicting electricity demand using time-of-week, temperature and flight schedule information. Test results reveals that a model, which has trained over specific seasonal data with only time-of-week and temperature as inputs, has the best prediction performance The number of passengers of departure flight schedules is shown to have a positive relationship to the load, but does not improve the model accuracy significantly, However since this study is done for the spring season, when heating ventilating and air conditioning ( HVAC ) systems run the least, the results may not represent other seasons with high cooling or heating demand. Specifically the authors propose piece-wise linear regression models. For the given period of April and May.
2K_test_993	The fair division of indivisible goods has long been an important topic in economics and, more recently computer science. That is allocations where each player values her own allocated set of goods at least as highly as any other player 's allocated set of goods. We show that even when the number of goods is larger than the number of agents by a linear fraction, envy-free allocations are unlikely to exist, We then show that when the number of goods is larger by a logarithmic factor, such allocations exist with high probability show that the asymptotic behavior of the theory holds even when the number of goods and agents is quite small, We demonstrate that there is a sharp phase transition from nonexistence to existence of envy-free allocations, and that on average the computational problem is hardest at that transition. We investigate the existence of envyfree allocations of indivisible goods. Under additive valuations We support these results experimentally and.
2K_test_996	Non-Technical Loss ( NTL ) represents a major challenge when providing reliable electrical service in developing countries, where it often accounts for 11-15 % of total generation capacity [ 1 ], NTL is caused by a variety of factors such as theft, unmetered homes and inability to pay, which at volume can lead to system instability, grid failure and major financial losses for providers. For separating NTL from total losses in microgrids. Both classes of approaches can provide a confidence interval based on the amount of detected NTL, We see that both are quite effective, but that the data-driven class is significantly easier to implement. In this paper we investigate error sources and techniques The model-driven class considers the primary sources of state uncertainty including line losses, meter consumption meter calibration error, packet loss and sample synchronization error, In the data-driven class, we use two approaches that learn grid state based on training data The first approach uses a regression technique on an NTL-free period of grid operation to capture the relationship between state error and total consumption The second approach uses an SVM trained on synthetic NTL data. We adopt and compare two classes of approaches for detecting NTL : ( 1 ) model- driven and ( 2 ) data- driven, We experimentally evaluate and compare the approaches on wireless meter data collected from a 525-home microgrid deployed in Les Anglais, In both cases we are able to experimentally evaluate to what degree we can reliably separate NTL from total losses.
2K_test_997	Kidney exchanges are organized markets where patients swap willing but incompatible donors, In the last decade, kidney exchanges grew from small and regional to large and national -- -and soon, This growth results in more lives saved, but exacerbates the empirical hardness of the $ \mathcal { NP } $ -complete problem of optimally matching patients to donors. State-of-the-art matching engines use integer programming techniques to clear fielded kidney exchanges, but these methods must be tailored to specific models and objective functions, and may fail to scale to larger exchanges the clearing problem is solvable in polynomial time. Show that indeed small numbers of attributes suffice. In this paper we observe that if the kidney exchange compatibility graph can be encoded by a constant number of patient and donor attributes, We give necessary and sufficient conditions for losslessly shrinking the representation of an arbitrary compatibility graph, Then using real compatibility graphs from the UNOS nationwide kidney exchange, we show how many attributes are needed to encode real compatibility graphs.
2K_test_998	Friendsourcing consists of broadcasting questions and help requests to friends on social networking sites. Despite its potential value, friendsourcing requests often fall on deaf ears, One way to improve response rates and motivate friends to undertake more effortful tasks may be to offer extrinsic rewards, such as money or a gift, for responding to friendsourcing requests, However past research suggests that these extrinsic rewards can have unintended consequences, including undermining intrinsic motivations and undercutting the relationship between people, To explore the effects of extrinsic reward on friends ' response rate and perceived relationship. Results indicate that large extrinsic rewards increase friends ' response rates without reducing the relationship strength between friends, Additionally the extrinsic rewards allow requesters to explain away the failure of friendsourcing requests and thus preserve their perceptions of relationship ties with friends. We conducted an experiment on a new friendsourcing platform - Mobilyzr.
2K_test_1000	Crowdsourcing offers a powerful new paradigm for online work, We also contribute a set of design patterns that may be informative for other systems aimed at supporting big picture thinking in small pieces. However real world tasks are often interdependent, requiring a big picture view of the difference pieces involved Existing crowdsourcing approaches that support such tasks -- ranging from Wikipedia to flash teams -- are bottlenecked by relying on a small number of individuals to maintain the big picture. In this paper we explore the idea that a computational system can scaffold an emerging interdependent, big picture view entirely through the small contributions of individuals, each of whom sees only a part of the whole. To investigate the viability, strengths and weaknesses of this approach we instantiate the idea in a prototype system for accomplishing distributed information synthesis and evaluate its output across a variety of topics.
2K_test_1001	We investigate synergy or lack thereof, between agents in co-operative games, building on the popular notion of Shapley value. Our main theoretical result is that any graph specifying synergistic and antagonistic pairs can arise even from a restricted class of cooperative games. We think of a pair of agents as synergistic ( resp, antagonistic ) if the Shapley value of one agent when the other agent participates in a joint effort is higher ( resp, lower ) than when the other agent does not participate We also study the computational complexity of determining whether a given pair of agents is synergistic. Finally we use the concepts developed in the paper to uncover the structure of synergies in two real-world organizations, the European Union and the International Monetary Fund.
2K_test_1002	How do users behave if they can tag each other in social networks ? Twitter lists can be regarded as the tagging process ; a user ( i, tagger ) creates a list with a name ( i, tag ) and adds other users ( i, tagged users ) into the list, This tagging network is by nature different from the resource tagging networks ( e, Flickr and Delicious ) because users on this network can tag each other, This study sheds light on the underlying characteristics of the interactive tagging network, which is relevant to the social scientists and the system designers of the tagging systems. In this paper we answer this question by studying the interactive tagging network constructed by Twitter lists, We address the following research questions : ( RQ1 ) What is the common patterns and the difference between the interactive tagging network and the resource tagging networks ? ( RQ2 ) Do users tag each other on the interactive tagging network ? And if so, to what extent ? ( RQ3 ) What is the difference between the two types of relationships on Twitter : who-tags-whom and who-follows-whom ?. We found the pervasive patterns across the different tagging networks, and the interactive patterns within the interactive tagging network. By quantitatively studying million-scale networks.
2K_test_1003	Social media is an increasingly important part of modern life We propose changes that Twitter and other social platforms should make to promote fuller access to users with visual impairments. We investigate the use of and usability of Twitter by blind users, While Twitter has traditionally been thought of as the most accessible social media platform for blind users, Twitter 's increasing integration of image content and users ' diverse uses for images have presented emergent accessibility challenges. Our findings illuminate the importance of the ability to use social media for people who are blind, while also highlighting the many challenges such media currently present this user base, including difficulty in creating profiles, in awareness of available features and settings, in controlling revelations of one 's disability status, and in dealing with the increasing pervasiveness of image-based content. Via a combination of surveys of blind Twitter users, large-scale analysis of tweets from and Twitter profiles of blind and sighted users, and analysis of tweets containing embedded imagery.
2K_test_1005	We focus on detecting complex events in unconstrained Internet videos, While most existing works rely on the abundance of labeled training data, we consider a more difficult zero-shot setting where no training data is supplied, To address the challenging optimization formulation. Verify the superiority of the proposed approach. We first pre-train a number of concept classifiers using data from other sources, Then we evaluate the semantic correlation of each concept w, the event of interest, After further refinement to take prediction inaccuracy and discriminative power into account, we apply the discovered concept classifiers on all test videos and obtain multiple score vectors, These distinct score vectors are converted into pairwise comparison matrices and the nuclear norm rank aggregation framework is adopted to seek consensus, we propose an efficient, highly scalable algorithm that is an order of magnitude faster than existing alternatives. Experiments on recent TRECVID datasets.
2K_test_1006	That can significantly lower the barrier for modelers to specify and solve convex stochastic optimization problems. We introduce disciplined convex stochastic programming ( DCSP ), a modeling framework by allowing modelers to naturally express a wide variety of convex stochastic programs in a manner that reflects their underlying mathematical representation DCSP allows modelers to express expectations of arbitrary expressions, partial optimizations and chance constraints across a wide variety of convex optimization problem families ( e, linear quadratic second order cone. We illustrate DCSP 's expressivity through a number of sample implementations of problems drawn from the operations research, finance and machine learning literatures.
2K_test_1007	Nudging behaviors through user interface design is a practice that is well-studied in HCI research, Corporations often use this knowledge to modify online interfaces to influence user information disclosure. The impact of a norm-shaping design patterns on information divulging behavior. We show that ( 1 ) a set of images, biased toward more revealing figures, change subjects ' personal views of appropriate information to share ; ( 2 ) that shifts in perceptions significantly increases the probability that a subject divulges personal information ; and ( 3 ) that these shift also increases the probability that the subject advises others to do so, Our main contribution is a key mechanism by which norm-shaping designs can change beliefs and subsequent disclosure behaviors. In this paper we experimentally test empirically identifying.
2K_test_1008	For describing sets of time series generated by interacting agents for estimating the graph adjacency matrix of this model. A class of models using directed, weighted graphs is introduced, A computationally tractable algorithm from observed time series data is presented The performance guarantees of this algorithm for prediction are outlined under several assumptions on the properties of the dynamics of the system of agents and on the true values of the parameters. These guarantees are tested empirically through simulation studies using several random graph models.
2K_test_1009	Finding densely connected subgraphs, also called communities in networks are of interest for many applications In previous work, we showed an optimization method for efficiently finding subgraphs denser than the overall network [ 1 ], This result is derived from our studies of network processes, dynamical processes that model interactions between individual agents in networks ( i, spread of infection or cascading failures ). In this paper we prove are also unique. That these subgraphs in the sense that there are no other subgraphs in the network isomorphic to these subgraphs.
2K_test_1010	This is one of the oldest non-trivial problems in computational geometry yet despite a long history of research the previous fastest running times for computing a ( 1+ ) -approximate geometric median were O ( d n 4/3 8/3 ) by Chin et, al O ( d exp 4 log 1 ) by Badoiu et, al O ( nd + poly ( d, 1 ) ) by Feldman and Langberg, and the polynomial running time of O ( ( nd ) O ( 1 ) log1/ ) by Parrilo and Sturmfels and Xue and Ye. For solving the geometric median problem given n points in d compute a point that minimizes the sum of Euclidean distances to the points In this paper we show how to compute such an approximate geometric median in time O ( nd log 3 n / ) and O ( d 2 ). In this paper we provide faster algorithms While our O ( d 2 ) is a fairly straightforward application of stochastic subgradient descent, our O ( nd log 3 n / ) time algorithm is a novel long step interior point method, We start with a simple O ( ( nd ) O ( 1 ) log1/ ) time interior point method and show how to improve it, ultimately building an algorithm that is quite non-standard from the perspective of interior point literature, Our result is one of few cases of outperforming standard interior point theory, Furthermore it is the only case we know of where interior point methods yield a nearly linear time algorithm for a canonical optimization problem that traditionally requires superlinear time.
2K_test_1011	Computer security problems often occur when there are disconnects between users understanding of their role in computer security and what is expected of them, that inform future directions for better design and research into security interventions, Our findings emphasize the need for better understanding of how users computers get infected, so that we can more effectively design user-centered mitigations. To help users make good security decisions more easily, we need insights into the challenges they face in their daily computer usage to collect data on user behavior and machine configurations. Produced engagement as the overarching theme, whereby participants with greater engagement in computer security and maintenance did not necessarily have more secure computer states Thus, user engagement alone may not be predictive of computer security, We identify several other themes. We built and deployed the Security Behavior Observatory ( SBO ) from participants home computers. Combining SBO data with user interviews, this paper presents a qualitative study comparing users attitudes, behaviors and understanding of computer security to the actual states of their computers Qualitative inductive thematic analysis of the interviews.
2K_test_1013	The environment of a living cell is vastly different from that of an in vitro reaction system, an issue that presents great challenges to the use of in vitro models, or computer simulations based on them, for understanding biochemistry in vivo, Virus capsids make an excellent model system for such questions because they typically have few distinct components, making them amenable to in vitro and modeling studies, yet their assembly can involve complex networks of possible reactions that can not be resolved in detail by any current experimental technology, We previously fit kinetic simulation parameters to bulk in vitro assembly data to yield a close match between simulated and real data, and then used the simulations to study features of assembly that can not be monitored experimentally The work demonstrates how computer simulations can help us understand how assembly might differ between the in vitro and in vivo environments and what features of the cellular environment account for these differences. The present work seeks to project how assembly in these simulations fit to in vitro data would be altered by computationally adding features of the cellular environment to the system, specifically the presence of nucleic acid about which many capsids assemble The major challenge of such work is computational : simulating fine-scale assembly pathways on the scale and in the parameter domains of real viruses is far too computationally costly to allow for explicit models of nucleic acid interaction. Exhibit surprising behavioral complexity, with distinct effects often acting synergistically to drive efficient assembly and alter pathways relative to the in vitro model. We bypass that limitation by applying analytical models of nucleic acid effects to adjust kinetic rate parameters learned from in vitro data to see how these adjustments, singly or in combination, might affect fine-scale assembly progress.
2K_test_1014	A novel extension of normal form games. Our main result is that biased games satisfying certain mild conditions always admit an equilibrium. We present that we call biased games, In these games a player 's utility is influenced by the distance between his mixed strategy and a given base strategy, We argue that biased games capture important aspects of the interaction between software agents We also tackle the computation of equilibria in biased games.
2K_test_1015	A kidney exchange is an organized barter market where patients in need of a kidney swap willing but incompatible donors, Determining an optimal set of exchanges is theoretically and empirically hard Traditionally, exchanges took place in cycles, with each participating patient-donor pair both giving and receiving a kidney, The recent introduction of chains, where a donor without a paired patient triggers a sequence of donations without requiring a kidney in return, increased the efficacy of fielded kidney exchanges -- -while also dramatically raising the empirical computational hardness of clearing the market in practice Finally, we note that our position-indexed chain-edge formulation can be modified in a straightforward way to take post-match edge failure into account, under the restriction that edges have equal probabilities of failure, Post-match edge failure is a primary source of inefficiency in presently-fielded kidney exchanges. While chains can be quite long, unbounded-length chains are not desirable : planned donations can fail before transplant for a variety of reasons, and the failure of a single donation causes the rest of that chain to fail, so parallel shorter chains are better in practice. We show that our new models are competitive with all existing solvers -- -in many cases outperforming all other solvers by orders of magnitude. In this paper we address the tractable clearing of kidney exchanges with short cycles and chains that are long but bounded, This corresponds to the practice at most modern fielded kidney exchanges, We introduce three new integer programming formulations, two of which are compact Furthermore, one of these models has a linear programming relaxation that is exactly as tight as the previous tightest formulation ( which was not compact ) for instances in which each donor has a paired patient, We show how to implement such failure-aware matching in our model, and also extend the state-of-the-art general branch-and-price-based non-compact formulation for the failure-aware problem to run its pricing problem in polynomial time. On real data from the UNOS nationwide exchange in the United States and the NLDKSS nationwide exchange in the United Kingdom, as well as on generated realistic large-scale data.
2K_test_1016	Kidney exchange is a barter market where patients trade willing but medically incompatible donors, These trades occur via cycles, where each patient-donor pair both gives and receives a kidney, and via chains which begin with an altruistic donor who does not require a kidney in return For logistical reasons, the maximum length of a cycle is typically limited to a small constant, while chains can be much longer, Given a compatibility graph of patient-donor pairs, altruists and feasible potential transplants between them, finding even a maximum-cardinality set of vertex-disjoint cycles and chains is NP-hard, There has been much work on developing provably optimal solvers that are efficient in practice, One of the leading techniques has been branch and price, where column generation is used to incrementally bring cycles and chains into the optimization model on an as-needed basis, This shows incorrectness of two leading branch-and-price solvers that suggested polynomial-time chain pricing algorithms. We prove that finding a positive-price chain is NP-complete. In particular only positive-price columns need to be brought into the model.
2K_test_1017	An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy, In a system with redundant requests, each job that arrives to the system is copied and dispatched to multiple servers, As soon as the first copy completes service, the job is considered complete, and all remaining copies are deleted. A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists. We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity. We propose a theoretical model of redundancy, the Redundancy-d system in which each job sends redundant copies to d servers chosen uniformly at random, We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers.
2K_test_1018	Complex networks have been shown to exhibit universal properties, with one of the most consistent patterns being the scale-free degree distribution. But are there regularities obeyed by the r-hop neighborhood in real networks ? We answer this question. And we show the pervasiveness of the power-hop. By identifying another power-law pattern that describes the relationship between the fractions of node pairs C ( r ) within r hops and the hop count r, This scale-free distribution is pervasive and describes a large variety of networks, ranging from social and urban to technological and biological networks In particular, inspired by the definition of the fractal correlation dimension D2 on a point-set, we consider the hop-count r to be the underlying distance metric between two vertices of the network, and we examine the scaling of C ( r ) with r, We find that this relationship follows a power-law in real networks within the range 2 r d, where d is the effective diameter of the network, that is the 90-th percentile distance We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. We provide theoretical justification for this pattern under successful existing network models, while we analyze a large set of real and synthetic network datasets.
2K_test_1019	Modernsmartphoneplatformshavemillionsofapps manyofwhich request permissions to access private data and resources, like user accounts or location, Prior research has shown that users are often unaware of, if not uncomfortable with, many of their permission settings, Prior work also suggests that it is theoretically possible to predict many of the privacy settings a user would want by asking the user a small number of questions, We discuss the implications of our results for mobile permission management and the design of personalized privacy assistant solutions. While these smartphone platforms provide varying degrees of control over these permissions, the sheer number of decisions that users are expected to manage has been shown to be unrealistically high, However this approach has neither been operationalized nor evaluated with actual users before. The results of our study are encouraging, We find that 78, 7 % of the recommendations made by the PPA were adopted by users, Following initial recommendations on permission settings, participants were motivated to further review and modify their settings with daily privacy nudges, Despite showing substantial engagement with these nudges, participants only changed 5, 1 % of the settings previously adopted based on the PPAs recommendations, The PPA and its recommendations were perceived as useful and usable. In which we implemented and evaluated a Personalized Privacy Assistant ( PPA ). We report on a field study ( n=72 ) with participants using their own Android devices.
2K_test_1020	The rise of Internet-scale networks, such as web graphs and social media with hundreds of millions to billions of nodes, presents new scientific opportunities, such as overlapping community detection to discover the structure of the Internet, or to analyze trends in online social behavior. However many existing probabilistic network models are difficult or impossible to deploy at these massive scales for modeling and inferring latent spaces in Internet-scale networks, with an eye towards overlapping community detection as a key application. We demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours, our method is several orders of magnitude faster, with competitive or improved accuracy at overlapping community detection. We propose a scalable approach By applying a succinct representation of networks as a bag of triangular motifs, developing a parsimonious statistical model, deriving an efficient stochastic variational inference algorithm. And implementing it as a distributed cluster program via the Petuum parameter server system, Compared to other state-of-the-art probabilistic network approaches.
2K_test_1021	Different propagation characteristics of the wave modes, their distinctive sensitivities to different types and ranges of EOCs, and to different damage scenarios, make the interpretation of diffuse-field guided-wave signals a challenging task. This work addresses the main challenges in real-world application of guided-waves for damage detection of pipelines, namely their complex nature and sensitivity to environmental and operational conditions ( EOCs ) The objective is to simplify diffuse-field guided-wave signals to a sparse subset of the arrivals that contains the majority of the energy carried by the signal. We show that such a subset is less affected by EOCs compared to the complete time-traces of the signals, Moreover it is shown that the effects of damage on the energy of this subset suppress those of EOCs. This paper proposes an unsupervised feature-extraction method for online damage detection of pipelines under varying EOCs, A set of signals from the undamaged state of a pipe are used as reference records, The reference dataset is used to extract the aforementioned sparse representation, During the monitoring stage, the sparse subset representing the undamaged pipe, will not accurately reconstruct the energy of a signal from a damaged pipe In other words, such a sparse representation of guided-waves is sensitive to occurrence of damage, Therefore the energy estimation errors are used as damage-sensitive features for damage detection purposes. A diverse set of experimental analyses are conducted to verify the hypotheses of the proposed feature-extraction approach, and to validate the detection performance of the damage-sensitive features, The empirical validation of the proposed method includes ( 1 ) detecting a structural abnormality in an aluminum pipe, under varying temperature at different ranges, ( 2 ) detecting multiple small damages of different types, at different locations in a steel pipe, under varying temperature ( 3 ) detecting a structural abnormality in an operating hot-water piping system, under multiple varying EOCs, such as temperature water flow rate, and inner pressure ; and ( 4 ) detecting a structural abnormality as the ratio of the damaged pipe 's signals in the reference dataset increases.
2K_test_1022	While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice priors that allow for computationally convenient or tractable inference are more commonly used. In this paper we investigate the following question : for a given model, is it possible to use any convenient prior to infer a false posterior, and afterwards given some true prior of interest, quickly transform this result into the true posterior ? to carry out this task. We prove that our method can generate asymptotically exact samples. We present a procedure : given an inferred false posterior and true prior, our algorithm generates samples from the true posterior, This transformation procedure which we call `` prior swapping '' works for arbitrary priors, Notably its cost is independent of data size, It therefore allows us, in some cases to apply significantly less-costly inference procedures to more-sophisticated models than previously possible, It also lets us quickly perform any additional inferences, such as with updated priors or for many different hyperparameter settings, without touching the data. Empirically on a number of models and priors.
2K_test_1023	To study the evolution of ties in a network of interacting agents. Results that capture e, the conspicuous phenomenon of emergence and downfall of leaders in social networks. We propose a family of models by reinforcement and penalization of their connections according to certain local laws of interaction, The family of stochastic dynamical systems, on the edges of a graph, exhibits \emph { good } convergence properties, in particular we prove a strong-stability result : a subset of binary matrices or graphs -- characterized by certain compatibility properties -- is a global almost sure attractor of the family of stochastic dynamical systems. To illustrate finer properties of the corresponding strong attractor, we present some simulation.
2K_test_1025	An age-old problem in the design of server farms is the choice of the task assignment policy, This is the algorithm that determines how to assign incoming jobs to servers, Popular policies include Round-Robin assignment, Join-the-Shortest-Queue Join-Queue-with-Least-Work and so on. While much research has studied assignment policies, little has taken into account server-side variability -- the fact that the server we choose might be temporarily and unpredictably slow that replicates each arrival to d servers chosen at random task assignment policy. We show that when server-side variability dominates runtime, replication of jobs can be very beneficial. We introduce the Replication-d algorithm, where the job is considered `` done '' as soon as the first replica completes, We provide an exact closed-form analysis of Replication-d, We next introduce a much more general model, one which takes both the inherent job size distribution and the server-side variability into account, This is a departure from traditional queueing models which only allow for one `` size '' distribution We propose and analyze a new, Replicate-Idle-Queue ( RIQ ), which is designed to perform well given these dual sources of variability.
2K_test_1026	Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled a myriad of new applications in many different domains from personal entertainment to process and infrastructure online monitoring in large industrial sites. To provide extended reach to an online video monitoring system. We show that this platform is non-omnidirectional in the flight plane and that UAV-to-UAV communication ceases around 75m transmitting payloads up to 200m ( over 802, 11g @ 54MBps ). Our work focuses on how one can use several small UAVs collaboratively We demonstrate how a TDMA overlay using 802, 11 radios on low-cost commercial-off-the-shelf ( COTS ) UAVs can be used to enable high channel utilization in multi-hop networks, by avoiding mutual interference This paper presents an extensive network characterisation and modelling of the quality of the UAV-to-UAV link, in terms of packet delivery ratio as a function of distance, packet size and orientation, Then we solve the mathematical problem of finding the optimal link length and number of hops that maximize the end-to-end throughput, as we extend the network. We validate our mathematical model with extensive experimental campaigns.
2K_test_1027	More than 10 % of the population has dyslexia, and most are diagnosed only after they fail in school, Currently we are working with schools to put our approach into practice at scale to reduce school failure as a primary way dyslexia is diagnosed. This work seeks to change this through scalable early that predict reading and writing difficulties. Revealed differences in how people with dyslexia read and write, with 83 % accuracy in a held-out test set with 100 participants. Detection via machine learning models by watching how people interact with a linguistic web-based game : Dytective The design of Dytective is based on ( i ) the empirical linguistic analysis of the errors that people with dyslexia make, ( ii ) principles of language acquisition, and ( iii ) specific linguistic skills related to dyslexia We trained a machine learning model that was able to predict dyslexia. Experiments with 243 children and adults ( 95 with diagnosed dyslexia ).
2K_test_1028	Motivation : Most methods for reconstructing response networks from high throughput data generate static models which can not distinguish between early and late response stages, to reconstruct dynamic models of host response to stimulus. Results led to accurate reconstruction of several known regulatory and signaling pathways and to novel mechanistic insights highlighting the usefulness of temporal models. We present TimePath a new method that integrates time series and static datasets TimePath uses an Integer Programming formulation to select a subset of pathways that, together explain the observed dynamic responses. Applying TimePath to study human response to HIV-1 We experimentally validated several of TimePaths predictions.
2K_test_1029	To generate invariance to nuisance transformations modeled as unitary to handle non-unitary transformations as well, to generate subject-specific pose-invariant features. Results extend the reach of a recent theory of invariance to discriminative and kernelized features based on unitary kernels and outperform previous work in almost all cases on off-angle face matching while we are on par with the previous state-of-the-art on the LFW unsupervised and image-restricted protocols, without any low-level image descriptors other than raw-pixels. We propose an explicitly discriminative and 'simple ' approach, the approach works well As a special case, a single common framework can be used for face recognition and vice-versa for pose estimation We show that our main proposed method ( DIKF ) can perform well under very challenging large-scale semisynthetic face matching and pose estimation protocols with unaligned faces using no landmarking whatsoever. In practice Our theoretical We additionally benchmark on CMU MPIE.
2K_test_1030	Large graphs are prevalent in many applications and enable a variety of information dissemination processes, meme virus and influence propagation, How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way ( e, stop a virus propagation, facilitate the propagation of a piece of good idea, etc ) ? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models. In this paper we study the problem of how to optimally place a set of edges ( e, edge deletion and edge addition ) to optimize the leading eigenvalue of the underlying graph, so that we can guide the dissemination process in a desired way for edge deletion and edge addition. In addition we reveal the intrinsic relationship between edge deletion and node deletion problems, results validate the effectiveness and efficiency of the proposed algorithms. We propose effective scalable algorithms.
2K_test_1031	Robust face detection in the wild is one of the ultimate components to support various facial related problems, unconstrained face recognition facial periocular recognition, facial landmarking and pose estimation, facial expression recognition 3D facial model construction. Although the face detection problem has been intensely studied for decades with various commercial applications, it still meets problems in some real-world scenarios due to numerous challenges, heavy facial occlusions extremely low resolutions, strong illumination exceptionally pose variations, image or video compression artifacts, etc to robustly solve the problems mentioned above. Results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin, and consistently achieves competitive results on FDDB against the recent state-of-the-art face detection methods. In this paper we present a face detection approach named Contextual Multi-Scale Region-based Convolution Neural Network ( CMS-RCNN ) Similar to the region-based CNNs, our proposed network consists of the region proposal component and the region-of-interest ( RoI ) detection component, However far apart of that network, there are two main contributions in our proposed network that play a significant role to achieve the state-of-the-art performance in face detection Firstly, the multi-scale information is grouped both in region proposal and RoI detection to deal with tiny face regions, Secondly our proposed network allows explicit body contextual reasoning in the network inspired from the intuition of human vision system. The proposed approach is benchmarked on two recent challenging face detection databases, the WIDER FACE Dataset which contains high degree of variability, as well as the Face Detection Dataset and Benchmark ( FDDB ) The experimental.
2K_test_1032	Recently fair division theory has emerged as a promising approach for allocation of multiple computational resources among agents While in reality agents are not all present in the system simultaneously, previous work has studied static settings where all relevant information is known upfront, We believe that our work informs the design of superior multiagent systems, and at the same time expands the scope of fair division theory by initiating the study of dynamic and fair resource allocation mechanisms. Our goal is to better understand the dynamic setting for dynamic resource allocation mechanisms. On the conceptual level, we develop a dynamic model of fair division, and propose desirable axiomatic properties On the technical level, we construct two novel mechanisms that provably satisfy some of these properties. And analyze their performance using real data.
2K_test_1034	We revisit the classic problem of estimating the population mean of an unknown single-dimensional distribution from samples Our key question is whether the sample median is the best ( in terms of mean squared error ) truthful estimator of the population mean. Our main result is a characterization of worst-case optimal truthful estimators, which provably outperform the median, for possibly asymmetric distributions with bounded support. Taking a game-theoretic viewpoint In our setting, samples are supplied by strategic agents, who wish to pull the estimate as close as possible to their own value In this setting, the sample mean gives rise to manipulation opportunities, whereas the sample median does not We show that when the underlying distribution is symmetric, there are truthful estimators that dominate the median.
2K_test_1035	For efficiently solving general convex optimization problems specified as disciplined convex programs ( DCP ). And show it often achieves order of magnitude speedups over existing general-purpose optimization solvers. This paper develops an approach a common general-purpose modeling framework, Specifically we develop an algorithm based upon fast epigraph projections, projections onto the epigraph of a convex function, an approach closely linked to proximal operator methods, We show that by using these operators, we can solve any disciplined convex program without transforming the problem to a standard cone form, as is done by current DCP libraries We then develop a large library of efficient epigraph projection operators, mirroring and extending work on fast proximal algorithms, for many common convex functions. Finally we evaluate the performance of the algorithm.
2K_test_1036	Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines, which could be of interest to studies on website profiling, Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models. Yet there has been relatively little work in modeling the behaviors and temporal properties of websites, as most research focuses on detecting whether a website distributes malware, In this paper we ask : How does web-based malware spread ? In order to conduct this study, to distinguish between compromised vs. First we find that legitimate but compromised websites constitute 33, 1 % of the malicious websites in our dataset, with an accuracy of 95, 3 % Second we find that malicious URLs can be surprisingly long-lived, with 10 % of malicious sites staying active for three months or more Third, we observe that a significant number of URLs exhibit the same temporal pattern that suggests a flush-crowd behavior, inflicting most of their damage during the first few days of appearance, Finally the distribution of the visits to malicious sites per user is skewed, 4 % of users visiting more than 10 malicious sites in 8 months. We develop a classifier. We conduct an extensive study and follow a website-centric and user-centric point of view We collect data from four online databases, including Symantec 's WINE Project, for a total of more than 600K malicious URLs and over 500K users.
2K_test_1037	The design of revenue-maximizing combinatorial auctions, multi item auctions over bundles of goods, is one of the most fundamental problems in computational economics, unsolved even for two bidders and two items for sale, In the traditional economic models, it is assumed that the bidders ' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution Despite this strong and oftentimes unrealistic assumption, it is remarkable that the revenue-maximizing combinatorial auction remains unknown The most scalable automated mechanism design algorithms take as input samples from the bidders ' valuation distribution and then search for a high-revenue auction in a rich auction class. In recent years automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design. In this work we provide the first sample complexity analysis In particular, we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying, unknown distribution over bidder valuations, for each of the auction classes in the hierarchy In addition to helping set automated mechanism design on firm foundations, our results also push the boundaries of learning theory, In particular the hypothesis functions used in our contexts are defined through multi stage combinatorial optimization procedures, rather than simple decision boundaries, as are common in machine learning.
2K_test_1038	Online content have become an important medium to disseminate information and express opinions, This paper is an extended abstract of the 2012 ACM SIGKDD best doctoral dissertation award of Ahmed [ 2011 ]. With their proliferation users are faced with the problem of missing the big picture in a sea of irrelevant and/or diverse content, In this paper we addresses the problem of information organization of online document collections. And provide algorithms that create a structured representation of the otherwise unstructured content, We leverage the expressiveness of latent probabilistic models ( e, topic models ) and non-parametric Bayes techniques ( e, Dirichlet processes ) and give online and distributed inference algorithms that scale to terabyte datasets and adapt the inferred representation with the arrival of new documents.
2K_test_1039	It remains a challenge to detect associations between genotypes and phenotypes because of insufficient sample sizes and complex underlying mechanisms involved in associations, Availability and implementation : Software is available at http : //www. Motivation Fortunately it is becoming more feasible to obtain gene expression data in addition to genotypes and phenotypes, giving us new opportunities to detect true genotypephenotype associations while unveiling their association mechanisms, that accurately detects associations between SNPs and phenotypes, as well as gene traits involved in such associations. Results we show that NETAM finds significantly more phenotype-associated SNPs than traditional genotypephenotype association analysis under false positive control, taking advantage of gene expression data and identified 477 significant path associations, among which we analyzed paths related to beta-amyloid, estrogen and nicotine pathways. In this article we propose a novel method, NETAM We take a network-driven approach : NETAM first constructs an association network, where nodes represent SNPs, gene traits or phenotypes, and edges represent the strength of association between two nodes, NETAM assigns a score to each path from an SNP to a phenotype, and then identifies significant paths based on the scores. In our simulation study, Furthermore we applied NETAM on late-onset Alzheimers disease data We also provide hypothetical biological pathways to explain our findings.
2K_test_1040	For learning sparse graphical models, for fitting the MQGM for sampling from the joint distribution that underlies the MQGM estimate. That demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data. We introduce the Multiple Quantile Graphical Model ( MQGM ), which extends the neighborhood selection approach of Meinshausen and Buhlmann The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others, Our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates We establish that, under suitable regularity conditions, the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic Gaussian data model, We develop an efficient algorithm using the alternating direction method of multipliers, We also describe a strategy. Lastly we present detailed experiments.
2K_test_1041	'Alice ' is submitting one web search per five minutes, for three hours in a row - is it normal ? How to detect abnormal search behaviors, among Alice and other users ? Is there any distinct pattern in Alice 's ( or other users ' ) search behavior ? to describe such an IAT distribution to capture and explain the two-dimensional, heavy-tail distribution of the parameters. For each user we discover and explain a surprising, bi-modal pattern of the inter-arrival time ( IAT ) of landed queries ( queries with user click-through ) we then notice the correlations among its parameters at the group level. In this paper we present a novel, user-and group-level framework M3A : Model, MetaModel and Anomaly detection Specifically, the model Camel-Log is proposed Thus, we further propose the metamodel Meta-Click, Combining Camel-Log and Meta-Click, the proposed M3A has the following strong points : ( 1 ) the accurate modeling of marginal IAT distribution, ( 2 ) quantitative interpretations, and ( 3 ) anomaly detection. We studied what is probably the largest, publicly available query log that contains more than 30 million queries from 0.
2K_test_1043	The large number of user-generated videos uploaded on to the Internet everyday has led to many commercial video search engines, which mainly rely on text metadata for search. However metadata is often lacking for user-generated videos, thus these videos are unsearchable by current search engines, Therefore content-based video retrieval ( CBVR ) tackles this metadata-scarcity problem by directly analyzing the visual and audio streams of each video, CBVR encompasses multiple research topics, including low-level feature design, feature fusion semantic detector training and video search/reranking, to enhance CBVR in both accuracy speed. Where our system outperformed other submissions in both text queries and video example queries, thus demonstrating the effectiveness of our proposed approaches. We present novel strategies in these topics and under different query inputs, including pure textual queries and query by video examples. Our proposed strategies have been incorporated into our submission for the TRECVID 2014 Multimedia Event Detection evaluation.
2K_test_1044	Recent computer systems research has proposed using redundant requests to reduce latency, The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request. However there is no exact analysis of systems with redundancy. We find some surprising results First, the response time of a fully redundant class follows a simple exponential distribution and that of the non-redundant class follows a generalized hyperexponential, Second fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant We find that, in many cases redundancy outperforms JSQ and Opt-Split with respect to overall response time, making it an attractive solution. This paper presents the first exact analysis of systems with redundancy We allow for any number of classes of redundant requests, any number of classes of non-redundant requests, any degree of redundancy, and any number of heterogeneous servers, In all cases we derive the limiting distribution of the state of the system, In small ( two or three server ) systems, we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes, and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy. We also compare redundancy with other approaches for reducing latency, such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and join-the-shortest-queue ( JSQ ) routing of a class.
2K_test_1045	Work in human-computer interaction has generally assumed either a single user or a group of users working together in a shared virtual space Recent crowd-powered systems use a different model in which a dynamic group of individuals ( the crowd ) collectively form a single actor that responds to real-time performance tasks, controlling an on-screen character, driving a robot or operating an existing desktop interface, Nowhere is the focus on the individual performer more finely resolved than in the study of the human psychomotor system, a mainstay topic in psychology that, largely owing to Fitts law, also has a legacy in HCI, This work contributes to the beginning of a predictive science for the general crowd actor model. To model coordination strategies and resulting collective performance, and discuss how the crowd actor is influenced not only by the domain on which it is asked to operate but also by the personality endowed to it by algorithms used to combine the inputs of constituent participants, Therefore we explored our notion of a crowd actor. In this paper we introduce the idea of the crowd actor as a way by modeling the crowd as a individual motor system performing pointing tasks. We combined the input of 200 participants in a controlled offline experiment to demonstrate the inherent trade-offs between speed and errors based on personality, the number of constituent individuals, and the mechanism used to distribute work across the group, Finally 10 workers participated in a synchronous experiment to explore how the crowd actor responds in a real online setting.
2K_test_1046	Identity-coherent trajectories to pinpoint potential tracking errors. Show that not only is our proposed tracker effective, but also the solution path enables automatic pinpointing of potential tracking failures, which can be readily utilized in an active learning framework to improve identity-aware multi-object tracking. We propose an identity-aware multi-object tracker based on the solution path algorithm, Our tracker not only produces based on cues such as recognition, but also has the ability The tracker is formulated as a quadratic optimization problem with l0 norm constraints, which we propose to solve with the solution path algorithm, The algorithm successively solves the same optimization problem but under different lp norm constraints, where p gradually decreases from 1 to 0, Inspired by the success of the solution path algorithm in various machine learning tasks, this strategy is expected to converge to a better local minimum than directly minimizing the hardly solvable l0 norm or the roughly approximated l1 norm constraints, Furthermore the acquired solution path complies with the `` decision making process '' of the tracker, which provides more insight to locating potential tracking errors.
2K_test_1047	In this work we have undertaken the task of occlusion and low-resolution robust facial gender classification to enforce the attention shift during the learning process, The hope is to enable the network to attend to particular high-profile regions ( e, the periocular region ) without the need to change the network architecture itself. With the progressively trained CNN models, we have achieved better gender classification results on the large-scale PCSO mugshot database with 400K images under occlusion and low-resolution settings, compared to the one undergone traditional training. Inspired by the trainable attention model via deep architecture, and the fact that the periocular region is proven to be the most salient region for gender classification purposes, we are able to design a progressive convolutional neural network training paradigm The network benefits from this attention shift and becomes more robust towards occlusions and low-resolution degradations, In addition our progressively trained network is sufficiently generalized so that it can be robust to occlusions of arbitrary types and at arbitrary locations, as well as low resolution.
2K_test_1051	Advances in fluorescence in situ hybridization ( FISH ) make it feasible to detect multiple copy-number changes in hundreds of cells of solid tumors, Studies using FISH sequencing, and other technologies have revealed substantial intra-tumor heterogeneity The evolution of subclones in tumors may be modeled by phylogenies, Tumors often harbor aneuploid or polyploid cell populations. Using a FISH probe to estimate changes in ploidy can guide the creation of trees that model changes in ploidy and individual gene copy-number variations. Tests on simulated data show improved accuracy of the ploidy-based approach relative to prior ploidyless methods Tests on real data further demonstrate novel insights these methods offer into tumor progression processes, Trees for DCIS samples are significantly less complex than trees for paired IDC samples Consensus graphs show substantial divergence among most paired samples from both sets, Low consensus between DCIS and IDC trees may help explain the difficulty in finding biomarkers that predict which DCIS cases are at most risk to progress to IDC, The FISHtrees software is available at ftp : //ftp. We present FISHtrees 3, 0 which implements a ploidy-based tree building method based on mixed integer linear programming ( MILP ) The ploidy-based modeling in FISHtrees includes a new formulation of the problem of merging trees for changes of a single gene into trees modeling changes in multiple genes and the ploidy, When multiple samples are collected from each patient, varying over time or tumor regions, it is useful to evaluate similarities in tumor progression among the samples, Therefore we further implemented in FISHtrees 3, 0 a new method to build consensus graphs for multiple samples. We validate FISHtrees 3, 0 on a simulated data and on FISH data from paired cases of cervical primary and metastatic tumors and on paired breast ductal carcinoma in situ ( DCIS ) and invasive ductal carcinoma ( IDC ).
2K_test_1052	In learning latent variable models ( LVMs ), it is important to effectively capture infrequent patterns and shrink model size without sacrificing modeling power, Various studies have been done to `` diversify '' a LVM, which aim to learn a diverse set of latent components in LVMs. Most existing studies fall into a frequentist-style regularization framework, where the components are learned via point estimation, In this paper we investigate how to `` diversify '' LVMs in the paradigm of Bayesian learning, which has advantages complementary to point estimation, such as alleviating overfitting via model averaging and quantifying uncertainty. And experimental results demonstrate the effectiveness and efficiency of our methods. We propose two approaches that have complementary advantages, One is to define diversity-promoting mutual angular priors which assign larger density to components with larger mutual angles based on Bayesian network and von Mises-Fisher distribution and use these priors to affect the posterior via Bayes rule, We develop two efficient approximate posterior inference algorithms based on variational inference and Markov chain Monte Carlo sampling, The other approach is to impose diversity-promoting regularization directly over the post-data distribution of components. These two methods are applied to the Bayesian mixture of experts model to encourage the `` experts '' to be diverse.
2K_test_1053	We study the problem of automatically building hypernym taxonomies from textual and visual data, Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Where our system outperforms previous approaches by a large gap. Instead we propose a probabilistic model by jointly leveraging text and images, To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies.
2K_test_1055	Besides appearance information the video contains temporal evolution, which represents an important and useful source of information about its content, Many video representation approaches are based on the motion information within the video, The common approach to extract the motion information is to compute the optical flow from the vertical and the horizontal temporal evolution of two consecutive frames. However the computation of optical flow is very demanding in terms of computational cost, in many cases being the most significant processing step within the overall pipeline of the target video analysis application, to capture the motion information within the video. Our HMG pipeline with several additional speed-ups is able to achieve real-time video processing and outperforms several well-known descriptors including descriptors based on the costly optical flow. In this work we propose a very efficient approach Our method is based on a simple temporal and spatial derivation, which captures the changes between two consecutive frames. The proposed descriptor Histograms of Motion Gradients ( HMG ), is validated on the UCF50 human action recognition dataset.
2K_test_1056	The applications of laser scanning technology are rapidly expanding in the civil engineering domain, LiDAR technology is now commonly used in the surveying and monitoring of large infrastructures In particular, tunnels have become key transport infrastructures, subjected to maintenance processes that allow quality checks for tunnel modifications or tunnel clearance and profile checks, demonstrating that tunnel management activities can definitely benefit from using mobile LiDAR by minimizing survey time and increasing productivity in dangerous environments. To semi-automatically retrieve the tunnel vertical clearance. An accuracy of 100 % in detection of cross sections is achieved, Only one of the cross sections shows a relative error in vertical clearance measurement higher than 1 %, The results demonstrated the effectiveness of the developed approach for computing vertical clearances and. The research described in this paper targets developing an approach based on ground based mobile LiDAR data, The steps of this approach include extraction of cross sections orthogonal to the vehicle trajectory and road markings based on radiometric information, and conversion of cross section to a two-dimensional profile to estimate the vertical clearance. The validation of the developed approach is done using real-life case study, a road tunnel in southern Galicia.
2K_test_1057	Complex event detection on unconstrained Internet videos has seen much progress in recent years. However state-of-the-art performance degrades dramatically when the number of positive training exemplars falls short, Since label acquisition is costly, laborious and time-consuming there is a real need to consider the much more challenging semantic event search problem, where no example video is given. And achieve state-of-the-art performances. In this paper we present a state-of-the-art event search system without any example videos, Relying on the key observation that events ( e, dog show ) are usually compositions of multiple mid-level concepts ( e, `` dog '' `` theater, '' and `` dog jumping '' ), we first train a skip-gram model to measure the relevance of each concept with the event of interest The relevant concept classifiers then cast votes on the test videos but their reliability, due to lack of labeled training videos, has been largely unaddressed We propose to combine the concept classifiers based on a principled estimate of their accuracy on the unlabeled test videos A novel warping technique is proposed to improve the performance and an efficient highly-scalable algorithm is provided to quickly solve the resulting optimization. We conduct extensive experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV datasets.
2K_test_1058	Kidney exchange where candidates with organ failure trade incompatible but willing donors, is a lifesaving alternative to the deceased donor waitlist, which has inadequate supply to meet demand While fielded kidney exchanges see huge benefit from altruistic kidney donors ( who give an organ without a paired needy candidate ), a significantly higher medical risk to the donor deters similar altruism with livers, We conclude with thoughts regarding the fielding of a nationwide liver or joint liverkidney exchange from a legal and computational point of view. Vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level. In this paper we begin by proposing the idea of liver exchange, and show on demographically accurate data that We then explore crossorgan donation where kidneys and livers can be bartered for each other, We show theoretically that this multiorgan exchange provides linearly more transplants than running separate kidney and liver exchanges ; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool. We support this result experimentally on demographically accurate multi-organ exchanges.
2K_test_1059	Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns ? to quantify both concepts ( synchronicity and normality ). C atch S ync consistently outperforms existing competitors, both in detection accuracy by 36p on Twitter and 20p on Tencent Weibo, as well as in speed. We propose a fast and effective method, C atch S ync, which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority We introduce novel measures and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots, Thanks to careful design, C atch S ync has the following desirable properties : ( a ) it is scalable to large datasets, being linear in the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology, without needing labeled data, nor timing information and the like, while still capable of using side information if available. We applied C atch S ync on three large, real datasets 1-billion-edge Twitter social graph, 3-billion-edge and 12-billion-edge Tencent Weibo social graphs, and several synthetic ones ;.
2K_test_1060	Action recognition ( AR ) is one of the most important tasks in video analysis and computer vision, Recently a large number of related methods have been proposed, leaving a reasonable space for further exploring the insights underlying such type of infrared AR problem and accordingly designing proper techniques to further promote the performance on this specifically constructed InfAR dataset. While most of these methods are investigated on AR datasets collected from the visible spectrum, the AR problem under infrared scenarios still has not attracted much attention, There is even few public infrared datasets available for supporting the fundamental evaluation requirements of this research To this issue, this work aims to emphasize the importance of the infrared AR problem in applications and arouse researchers ' attention on this task. Our results reveal : ( 1 ) In all, dense trajectory feature can achieve the best performance while the appearance features, HOG have relatively poorer performance ; ( 2 ) the encoding method of vector of locally aggregated descriptors is evidently better than that of the widely-used Fisher Vector ; ( 3 ) the late fusion facilitates a better performance than early fusion ; ( 4 ) action videos captured in winter is more discriminable than in summer ; ( 5 ) compared to appearance information, the motion information is more essential for infrared action recognition and utilizing this information through deep CNN can improve greatly the performance, The best performance achieved on our dataset is 76, 66 % ( Average Precision ). Specifically we construct a new Infrared Action Recognition ( InfAR ) dataset captured at different times, including in summer and winter, and explore how discriminable actions in our InfAR dataset are with the state-of-the-art pipelines based on low-level features and deep convolutional neural network ( CNN ).
2K_test_1061	Suppose you are a teacher, and have to convey a set of object-property pairs 'lions eat meat ', A good teacher will convey a lot of information, with little effort on the student side, What is the best and most intuitive way to convey this information to the student, without the student being overwhelmed ?. A related harder problem is : how can we assign a numerical score to each lesson plan i, way of conveying information ? Here, we give a formal definition of this problem of forming learning units and for comparing different approaches for this problem. It is effective achieving excellent results on real data, both with respect to our proposed metric, but also with respect to encoding length demonstrate the effectiveness of groupNteach. We provide a metric based on information theory, We also design an algorithm, groupNteach Our proposed groupNteach is scalable near-linear in the dataset size ; and it is intuitive, conforming to well-known educational principles. Experiments on real and synthetic datasets.
2K_test_1062	It is common that users are interested in finding video segments, which contain further information about the video contents in a segment of interest. To facilitate users to find and browse related video contents, video hyperlinking aims at constructing links among video segments with relevant information in a large video collection. Results show that ( 1 ) text features play a crucial role in search performance, and the combination of audio and visual features can not provide improvements ; ( 2 ) the consideration of contexts can not obtain better results ; and ( 3 ) due to the lack of training examples, machine learning techniques can not improve the performance. In this study we explore the effectiveness of various video features on the performance of video hyperlinking, including subtitle metadata content features ( i, audio and visual ), surrounding context as well as the combinations of those features, Besides we also test different search strategies over different types of queries, which are categorized according to their video contents, Comprehensive experimental studies have been conducted on the dataset of TRECVID 2015 video hyperlinking task.
2K_test_1063	Clustering is the task of grouping a set of objects so that objects in the same cluster are more similar to each other than to those in other clusters, The crucial step in most clustering algorithms is to find an appropriate similarity metric, which is both challenging and problem-dependent. Supervised clustering approaches which can exploit labeled clustered training data that share a common metric with the test set, have thus been proposed, Unfortunately current metric learning approaches for supervised clustering do not scale to large or even medium-sized datasets. Confirm several orders of magnitude speedup while still achieving state-of-the-art performance. In this paper we propose a new structured Mahalanobis Distance Metric Learning method We formulate our problem as an instance of large margin structured prediction and prove that it can be solved very efficiently in closed-form, The complexity of our method is ( in most cases ) linear in the size of the training dataset We further reveal a striking similarity between our approach and multivariate linear regression. Experiments on both synthetic and real datasets.
2K_test_1066	Matrix sketching is aimed at finding close approximations of a matrix by factors of much smaller dimensions, which has important applications in optimization and machine learning, Given a matrix A of size m by n, state-of-the-art randomized algorithms take O ( m * n ) time and space to obtain its low-rank decomposition. Although quite useful the need to store or manipulate the entire matrix makes it a computational bottleneck for truly large and dense inputs, Can we sketch an m-by-n matrix in O ( m + n ) cost by accessing only a small fraction of its rows and columns, without knowing anything about the remaining data ? to solve this problem. Fully demonstrate the potential of our methods in large scale matrix sketching and related areas. In this paper we propose the cascaded bilateral sampling ( CABS ) framework We start from demonstrating how the approximation quality of bilateral matrix sketching depends on the encoding powers of sampling, In particular the sampled rows and columns should correspond to the code-vectors in the ground truth decompositions Motivated by this analysis, we propose to first generate a pilot-sketch using simple random sampling, and then pursue more advanced, `` follow-up '' sampling on the pilot-sketch factors seeking maximal encoding powers In this cascading process, the rise of approximation quality is shown to be lower-bounded by the improvement of encoding powers in the follow-up sampling step, thus theoretically guarantees the algorithmic boosting property, Computationally our framework only takes linear time and space, and at the same time its performance rivals the quality of state-of-the-art algorithms consuming a quadratic amount of resources. Empirical evaluations on benchmark data.
2K_test_1067	This paper studies attackers with control objectives against cyber-physical systems ( CPS ). The system is equipped with its own controller and attack detector, and the goal of the attacker is to move the system to a target state while altering the system 's actuator input and sensor output to avoid detection, We formulate a cost function that reflects the attacker 's goals, and using dynamic programming, we show that the optimal attack strategy reduces to a linear feedback of the attacker 's state estimate By changing the parameters of the cost function, we show how an attacker can design optimal attacks to balance the control objective and the detection avoidance objective. Finally we provide a numerical illustration based on a remotely-controlled helicopter under attack.
2K_test_1068	At least 10 % of the global population has dyslexia, In the United States and Spain, dyslexia is associated with a large percentage of school drop out Our results suggest that Dytective is able to differentiate school age children with and without dyslexia in both English and Spanish speakers. Current methods to detect risk of dyslexia are language specific, expensive or do not scale well because they require a professional or extensive equipment, A central challenge to detecting dyslexia is handling its differing manifestations across languages. We found children with and without dyslexia differed significantly in their performance on the game. We designed a browser-based game, Dytective to detect risk of dyslexia across the English and Spanish languages Dytective consists of linguistic tasks informed by analysis of common errors made by persons with dyslexia. To evaluate Dytective we conducted a user study with 60 English and Spanish speaking children between 7 and 12 years old.
2K_test_1070	Acute hypotensive episodes ( AHEs ) are serious clinical events in intensive care units ( ICUs ), and require immediate treatment to prevent patient injury. Reducing the risks associated with an AHE requires effective and efficient mining of data generated from multiple physiological time series, to effectively predict AHE. HeartCast was found to outperform other state-of-the-art methods found in the literature with a 13, 7 % improvement in classification accuracy. We propose HeartCast a model that extracts essential features from such data HeartCast combines a non-linear support vector machine with best-feature extraction via analysis of the baseline threshold, quartile parameters and window size of the physiological signals, Our approach has the following benefits : ( a ) it extracts the most relevant features ; ( b ) it provides the best results for identification of an AHE event ; ( c ) it is fast and scales with linear complexity over the length of the window ; and ( d ) it can manage missing values and noise/outliers by using a best-feature extraction method. We performed experiments on data continuously captured from physiological time series of ICU patients ( roughly 3 GB of processed data ).
2K_test_1071	Summary An important experimental design question for high-throughput time series studies is the number of replicates required for accurate reconstruction of the profiles, Due to budget and sample availability constraints, more replicates imply fewer time points and vice versa These results provide theoretical support to the large number of high-throughput time series experiments that do not use replicates. We analyze the performance of dense and replicate sampling to determine the best replicate strategy given the expected noise. We observe that under reasonable noise levels, autocorrelations in the time series data allow dense sampling to better determine the correct levels of non-sampled points when compared to replicate sampling. By developing a theoretical framework that focuses on a restricted yet expressive set of possible curves over a wide range of noise levels and by analyzing real expression data, A Java implementation of our framework can be used. For both the theoretical analysis and experimental data.
2K_test_1072	To automatically determine whether a driver is using a cell-phone as well as detect if his/her hands are on the steering wheel ( i, counting the number of hands on the wheel ) To robustly detect small objects such as hands. Results show that our method archives better performance than Faster R-CNN on both hands on wheel detection and cell-phone usage detection while remaining at similar testing cost our approach obtains higher accuracy, is less time consuming and is independent to landmarking, The groundtruth database will be publicly available. In this paper we present an advanced deep learning based approach we propose Multiple Scale Faster-RCNN ( MSFRCNN ) approach that uses a standard Region Proposal Network ( RPN ) generation and incorporates feature maps from shallower convolution feature maps, conv3 and conv4 for ROI pooling, In our driver distraction detection framework, we first make use of the proposed MS-FRCNN to detect individual objects, namely a hand a cell-phone, and a steering wheel Then, the geometric information is extracted to determine if a cell-phone is being used or how many hands are on the wheel. The proposed approach is demonstrated and evaluated on the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge database and the challenging Strategic Highway Research Program ( SHRP-2 ) face view videos that was acquired to monitor drivers under naturalistic driving conditions The experimental Compare to the state-of-the-art cell-phone usage detection.
2K_test_1073	What is a fair way to assign rooms to several housemates, and divide the rent between them ? This is not just a theoretical question : many people have used the Spliddit website to obtain envy-free solutions to rent division instances, But envy freeness in and of itself, is insufficient to guarantee outcomes that people view as intuitive and acceptable, Based on these results, the maximin solution has been deployed on Spliddit since April 2015. We therefore focus on solutions that optimize a criterion of social justice, subject to the envy freeness constraint, in order to pinpoint the `` fairest '' solutions, that enables the computation of such solutions in polynomial time. And identify the maximin solution, which maximizes the minimum utility subject to envy freeness, as the most attractive that the maximin solution gives rise to significant gains in terms of our optimization objectives demonstrates that people find the maximin solution to be significantly fairer than arbitrary envy-free solutions ; this user study is unprecedented in that it asks people about their real-world rent division instances. We develop a general algorithmic framework. We then study the relations between natural optimization objectives, We demonstrate in theory and using experiments on real data from Spliddit, Finally a user study with Spliddit users as subjects.
2K_test_1074	Weakly supervised methods have recently become one of the most popular machine learning methods since they are able to be used on large-scale datasets without the critical requirement of richly annotated data. Our uniform method is able to achieve competitive results in various face analysis applications, such as occlusion detection, face recognition gender classification, twins verification and facial attractiveness analysis. In this paper we present a novel, self-taught discriminative approach in the weakly supervised framework, Our method can find regions which are discriminative across classes yet consistent within a class and can solve many face related problems The proposed method first trains a deep face model with high discriminative capability to extract facial features The hypercolumn features are then used to give pixel level representation for better classification performance along with discriminative region detection, In addition calibration approaches are proposed to enable the system to deal with multi-class and mixed-class problems The system is also able to detect multiple discriminative regions from one image.
2K_test_1075	Component-based modeling can be used to split large models into partial models to reduce modeling complexity. We study a component-based approach to simplify the challenges of verifying large-scale hybrid systems, Yet verification results also need to transfer from components to composites, to define the structure and behavior of components how to compose components. In this paper we propose a component-based hybrid system verification approach that combines the advantages of component-based modeling e, reduced model complexity with the advantages of formal verification e, guaranteed contract compliance Our strategy is to decompose the system into components, verify their local safety individually and compose them to form an overall system that provably satisfies a global contract, without proving the whole system We introduce the necessary formalism and a technique such that safety properties provably emerge from component safety.
2K_test_1076	Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web are associated with rich but noisy contextual information, such as the title, which provides weak annotations or labels about the video content, To leverage the big noisy web labels. The efficacy and the scalability of WELL have been extensively demonstrated The comprehensive results demonstrate that WELL outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data, In addition the results also verify that WELL is robust to the level of noisiness in the video data, Notably WELL trained on sufficient noisy web labels is able to achieve a comparable accuracy to supervised learning methods trained on the clean manually-labeled data. This paper proposes a novel method called WEbly-Labeled Learning ( WELL ), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human WELL introduces a number of novel multi-modal approaches to incorporate meaningful prior knowledge called curriculum from the noisy web videos. To investigate this problem, we empirically study the curriculum constructed from the multi-modal features of the videos collected from YouTube and Flickr, on two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set experimental.
2K_test_1077	Brain-computer interfaces ( BCIs ) have the potential to restore motor abilities to paralyzed individuals, These systems act by reading motor intent signals directly from the brain and using them to control, for example the movement of a cursor on a computer screen or the motion of a robotic limb, To construct a BCI, a mapping must be specified that dictates how neural activity will actuate the device. How should these mappings be constructed to maximize user performance ? Most approaches have focused on this problem from an estimation standpoint, mappings are designed to implement the best estimate of motor intent possible, under various sets of assumptions about how the recorded neural signals represent motor intent. Here we forward an alternate approach to the BCI design problem, using ideas from optimal control theory, We first argue that the brain can be considered as an optimal controller, We then introduce a mathematical definition of BCI usability, and formulate the BCI design problem as a constrained optimization problem that maximizes this usability.
2K_test_1078	Teaching chess to students with learning disabilities has been shown to benefit their school performance in unrelated domains At the same time, chess involves skills that are highly correlated with dyslexia, such as visuospatial and calculation abilities Therefore, dyslexia might have an impact on how people learn and play chess using a computer, suggesting that chess may be useful as a fun way to help people with dyslexia improve their abilities. Designed for people with dyslexia and seek to understand whether people with dyslexia learn and play chess online in ways that differ from other students and whether such differences may be leveraged to improve classroom performance To test how people with dyslexia learn to play chess. We could not find significant differences on four dependent measures out of the twelve measures we collected. In this paper we created a online chess game. We carried out a within-subject experiment with 62 participants, 31 of them with diagnosed dyslexia, Participants used an instrumented web-based chess learning platform that we developed to ( i ) complete lessons on how to play chess and about chess theory, ( ii ) work through exercises designed to test and reaffirm their skills, and ( iii ) play chess against a computer opponent.
2K_test_1080	The attacker performs an integrity attack in order to move the system to a target state while evading detection over a finite time window. This paper studies attackers with specific objectives against a cyber physical system, that captures the attacker 's objective - the solution gives the optimal sequence of attacks. Finally we demonstrate our proposed attack strategy. We formulate and solve an optimal control problem We provide a sufficient condition for the existence of an optimal attack sequence. In a numerical example.
2K_test_1081	We study the problem of variable selection in convex nonparametric regression to select a subset of variables that contains the relevant variables, for efficiently carrying out the required quadratic programs. We prove that the procedure is faithful in the population setting, yielding no false negatives The approach leads to computational and statistical advantages over fitting a full model, and provides an effective, practical approach to variable screening in convex regression. Under the assumption that the true regression function is convex and sparse, we develop a screening procedure Our approach is a two-stage quadratic programming method that estimates a sum of one-dimensional convex functions, followed by one-dimensional concave regression fits on the residuals, In contrast to previous methods for sparse additive models, the optimization is finite dimensional and requires no tuning parameters for smoothness. Under appropriate assumptions We give a finite sample statistical analysis.
2K_test_1082	Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in imperfect-information games. However imperfect-recall abstractions are poorly understood, and only weak algorithm-specific guarantees on solution quality are known. They show that running counterfactual regret minimization on such abstractions leads to good strategies in the original games. We develop the first general, algorithm-agnostic solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions, when implemented in the original ( perfect-recall ) game, Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions for which any such results have been obtained Further, our analysis is tighter in two ways, each of which can lead to an exponential reduction in the solution quality error bound, We then show that for extensive-form games that satisfy certain properties, the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem, where the increase in our bound is the distance function This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds, We proceed to show a divide in the class of abstraction problems, If payoffs are at the same scale at all information sets considered for abstraction, the input forms a metric space, and this immediately yields a $ 2 $ -approximation algorithm for abstraction Conversely, if this condition is not satisfied, we show that the input does not form a metric space. Finally we provide computational experiments to evaluate the practical usefulness of the abstraction techniques.
2K_test_1083	With the availability of high resolution digital technology, there has been increased interest in developing statistical and image processing techniques that can enhance the existing capabilities of analyzing works of art for authenticity This method is also valuable in determining whether an original painting has undergone any modifications, given that a representation of the initial version is available. In identifying forgeries among disputed paintings. We are not only able to distinguish between a low-quality digitized representation of a painting and its forgery, but also specifically indicate where the differences occur and where the replica is particularly faithful to the original. This work explores the merits of using advanced correlation filters in supplementing art experts efforts We show that by training the optimal trade-off synthetic discriminant function ( OTSDF ) filter on each section of a coarsely parceled image of an original painting.
2K_test_1084	Abstract SummaryWith the rapid advances in technologies of microarray and massively parallel sequencing, data of multiple omics sources from a large patient cohort are now frequently seen in many consortium studies Effective multi-level omics data integration has brought new statistical challenges, One important biological objective of such integrative analysis is to cluster patients in order to identify clinically relevant disease subtypes, which will form basis for tailored treatment and personalized medicine, Several methods have been proposed in the literature for this purpose, including the popular iCluster method used in many cancer applications, When clustering high-dimensional omics data, effective feature selection is critical for better clustering accuracy and biological interpretation, It is also common that a portion of `` scattered samples '' has patterns distinct from all major clusters and should not be assigned into any cluster as they may represent a rare disease subcategory or be in transition between disease subtypes. In this paper we firstly propose to improve feature selection. Of the iCluster factor model by an overlapping sparse group lasso penalty on the omics features using prior knowledge of inter-omics regulatory flows, We then perform regularization over samples to allow clustering with scattered samples and generate tight clusters. The proposed group structured tight iCluster method will be evaluated by two real breast cancer examples and simulations to demonstrate its improved clustering accuracy, biological interpretation and ability to generate coherent tight clusters.
2K_test_1085	Computational offloading services at the edge of the Internet for mobile devices are becoming a reality. We explore how such infrastructure improves latency and energy consumption relative to the cloud. We present experimental results that confirm substantial wins from edge computing for highly interactive mobile applications. Using a wide range of mobile applications. From WiFi and 4G LTE networks.
2K_test_1086	Demand response is seeing increased popularity worldwide and industrial loads are actively taking part in this trend, As a host of energy-intensive industrial processes, steel plants have both the motivation and potential to provide demand response. However the scheduling of steel plants is very complex and the involved computations are intense, In this paper we focus on these difficulties to make the computations more tractable. And propose methods such as adding cuts and implementing an application-specific branch and bound algorithm.
2K_test_1087	Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society, ranging from online personalization to insurance and credit decisions to predictive policing. But their decision-making processes are often opaque -- it is difficult to explain why a certain decision was made to improve the transparency of such decision-making systems. Demonstrates that QII measures are a useful transparency mechanism when black box access to the learning system is available, In particular they provide better explanations than standard associative measures for a host of scenarios that we consider Further, we show that in the situations we consider, QII is efficiently approximable and can be made differentially private while preserving accuracy. We develop a formal foundation Specifically, we introduce a family of Quantitative Input Influence ( QII ) measures that capture the degree of influence of inputs on outputs of systems, These measures provide a foundation for the design of transparency reports that accompany system decisions ( e, explaining a specific credit decision ) and for testing tools useful for internal and external oversight ( e, to detect algorithmic discrimination ) Distinctively, our causal QII measures carefully account for correlated inputs while measuring influence They support a general class of transparency queries and can, in particular explain decisions about individuals ( e, a loan decision ) and groups ( e, disparate impact based on gender ) Finally, since single inputs may not always have high influence, the QII measures also quantify the joint influence of a set of inputs ( e, age and income ) on outcomes ( e, loan decisions ) and the marginal influence of individual inputs within such a set ( e, Since a single input may be part of multiple influential sets, the average marginal influence of the input is computed using principled aggregation measures, such as the Shapley value, previously applied to measure influence in voting Further, since transparency reports could compromise privacy, we explore the transparency-privacy tradeoff and prove that a number of useful transparency reports can be made differentially private with very little addition of noise. Our empirical validation with standard machine learning algorithms.
2K_test_1088	Cities are increasingly equipped with low-resolution cameras, Video from some of these cameras is publicly accessible in real time. In this project the authors addressed the problem of building a traffic model for parts of the roads visible from publicly accessible cameras capable of detecting different types of vehicles in images in various weather conditions and times of the day except night. In particular the end goal is to build a model Models learn different appearance of vehicles as seen from different viewpoints, A major difficulty with any type of analysis like this is the need for large amounts of training data, In our case it is easy to collect unlabeled data from publicly available low-resolution low-framerate cameras in Pittsburgh or NYC.
2K_test_1089	Multimedia event detection has been receiving increasing attention in recent years, Besides recognizing an event, the discovery of evidences ( which is refered to as `` recounting '' ) is also crucial for user to better understand the searching result. Due to the difficulty of evidence annotation, only limited supervision of event labels are available for training a recounting model, To deal with the problem. And demonstrate the promising results obtained by our method. We propose a weakly supervised evidence discovery method based on self-paced learning framework, which follows a learning process from easy `` evidences '' to gradually more complex ones, and simultaneously exploit more and more positive evidence samples from numerous weakly annotated video segments. Moreover to evaluate our method quantitatively, we also propose two metrics, \textit { PctOverlap } and \textit { F1-score }, for measuring the performance of evidence localization specifically, The experiments are conducted on a subset of TRECVID MED dataset.
2K_test_1090	Given a large-scale and high-order tensor, how can we find dense blocks in it ? Can we find them in near-linear time but with a quality guarantee ? Extensive previous work has shown that dense blocks in tensors as well as graphs indicate anomalous or fraudulent behavior e, lockstep behavior in social networks. However available methods for detecting such dense blocks are not satisfactory in terms of speed, accuracy or flexibility for finding dense blocks in tensors. Upito 114 $ $ \times $ $ faster than state-of-the-art methods with similar accuracy 4 Effective : M-Zoom successfully detected edit wars and bot activities and spotted network attacks with near-perfect accuracy AUCi=i0, The data and software related to this paper are available at http : //www. In this work we propose M-Zoom, a flexible framework which works with a broad class of density measures, M-Zoom has the following properties : 1 Scalable : M-Zoom scales linearly with all aspects of tensors and is 2 Provably accurate : M-Zoom provides a guarantee on the lowest density of the blocks it finds, 3 Flexible : M-Zoom supports multi-block detection and size bounds as well as diverse density measures. In Wikipedia from a TCP dump.
2K_test_1091	An efficient alternative to convolutional layers in standard convolutional neural networks ( CNN ). The LBC layer affords significant parameter savings, 9x to 169x in the number of learnable parameters compared to a standard convolutional layer results in up to 9x to 169x savings in model size compared to a standard convolutional layer, that our local binary convolution layer is a good approximation of a standard convolutional layer, CNNs with LBC layers, called local binary convolutional neural networks ( LBCNN ), reach state-of-the-art performance on a range of visual datasets ( MNIST, SVHN CIFAR-10 and a subset of ImageNet ) while enjoying significant computational savings. We propose local binary convolution ( LBC ), The design principles of LBC are motivated by local binary patterns ( LBP ), The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights, The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer, Furthermore due to lower model complexity and sparse and binary nature of the weights also. We demonstrate both theoretically and experimentally Empirically.
2K_test_1093	Machine comprehension tests the systems ability to understand a piece of text through a reading comprehension task. We show that this approach leads to state of the art results on the task. We propose an approach using the Abstract Meaning Representation ( AMR ) formalism, We construct meaning representation graphs for the given text and for each question-answer pair by merging the AMRs of comprising sentences using cross-sentential phenomena such as coreference and rhetorical structures, Then we reduce machine comprehension to a graph containment problem We posit that there is a latent mapping of the question-answer meaning representation graph onto the text meaning representation graph that explains the answer, We present a unified max-margin framework that learns to find this mapping ( given a corpus of texts and question-answer pairs ), and uses what it learns to answer questions on novel texts.
2K_test_1094	Rapid advances in biology demand new tools for more active research dissemination and engaged teaching. To let college students explore genome evolution of mammalian species. While existing views communicate the same information, study participants found the interactive, karyogram-based views much easier and likable to use We additionally discuss feedback from biology and genomics faculty. This paper presents Synteny Explorer, an interactive visualization application designed The tool visualizes synteny blocks : segments of homologous DNA shared between various extant species that can be traced back or reconstructed in extinct, We take a karyogram-based approach to create an interactive synteny visualization, leading to a more appealing and engaging design for undergraduate-level genome evolution education. For validation we conduct three user studies : two focused studies on color and animation design choices and a larger study that performs overall system usability testing while comparing our karyogram-based designs with two more common genome mapping representations in an educational context who judge Synteny Explorer 's fitness for use in classrooms.
2K_test_1095	Online learning is used in a wide range of real applications, predicting ad click-through rates ( CTR ) and personalized recommendations. We discover that the most recent users ' actions can better reflect users ' current intentions and preferences, TDAP achieves good accuracy : it improves at least 5, 6 % in terms of prediction accuracy, and TDAP scales well : it runs 4 times faster when the number of machines increases from 2 to 10. Under this observation we thereby propose a novel time-decaying online learning algorithm derived from the state-of-the-art FTRL-proximal algorithm, called Time-Decaying Adaptive Prediction ( TDAP ) algorithm, To scale Big Data, we further parallelize our algorithm following the data parallel scheme under both BSP and SSP consistency model. Based on the analysis of users ' behaviors in Video-On-Demand ( VoD ) recommender systems, We experimentally evaluate our TDAP algorithm on real IPTV VoD datasets using two state-of-the-art distributed computing platforms, compared to FTRL-proximal algorithm ;.
2K_test_1096	An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer. For a setting in which the human user can only receive assistance from a semi-trusted computer. For these schemes we prove that forging passwords is equivalent to recovering the secret mapping, Thus our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts. We propose several candidate authentication protocols -- - a computer that stores information and performs computations correctly but does not provide confidentiality Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $, The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate, We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $, for a security parameter $ s ( f ) $ that depends on two key properties of $ f $, To obtain our results, we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $, Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate ), and generalize recent results of Feldman et al.
2K_test_1097	Vast quantities of videos are now being captured at astonishing rates, but the majority of these are not labelled, To cope with such data, we consider the task of content-based activity recognition in videos without any manually labelled examples, also known as zero-shot video recognition. Show the superiority of our approach. To achieve this videos are represented in terms of detected visual concepts, which are then scored as relevant or irrelevant according to their similarity with a given textual query, In this paper we propose a more robust approach in order to alleviate many of the brittleness and low precision problems of previous work, Not only do we jointly consider semantic relatedness, visual reliability and discriminative power, To handle noise and non-linearities in the ranking scores of the selected concepts, we propose a novel pairwise order matrix approach for score aggregation. Extensive experiments on the large-scale TRECVID Multimedia Event Detection data.
2K_test_1098	Counterfactual Regret Minimization ( CFR ) is the most popular iterative algorithm for solving zero-sum imperfect-information games, Regret-Based Pruning ( RBP ) is an improvement that allows poorly-performing actions to be temporarily pruned, thus speeding up CFR. That reduces the space requirements of CFR. We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium, This leads to provably faster convergence and lower space requirements, show that Total RBP results in an order of magnitude reduction in space, and the reduction factor increases with game size. We introduce Total RBP, a new form of RBP as actions are pruned.
2K_test_1099	Many applications collect a large number of time series, for example the financial data of companies quoted in a stock exchange, the health care data of all patients that visit the emergency room of a hospital, or the temperature sequences continuously measured by weather stations across the US, These data are often referred to as un structured The first task in its analytics is to derive a low dimensional representation, a graph or discrete manifold, that describes well the inter relations among the time series and their intra relations across time. For estimating this graph that structures the data. The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested. This paper presents a computationally tractable algorithm The resulting graph is directed and weighted, possibly capturing causal relations, not just reciprocal correlations as in many existing approaches in the literature. A convergence analysis is carried out, The algorithm is demonstrated on random graph datasets and real network time series datasets, and its performance is compared to that of related methods.
2K_test_1100	For ensuring that verification results about models apply to cyber-physical systems ( CPS ) implementations. A method is presented The invention provides correctness guarantees for CPS executions at runtime Offline verification of CPS models are combined with runtime validation of system executions for compliance with the model, The invention ensures that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model, assuming the system dynamics deviation is bounded If, at some point the observed behavior no longer complies with the model, such that offline verification results no longer apply, provably safe fallback actions are initiated, The invention includes a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic.
2K_test_1101	Pooling plays an important role in generating a discriminative video representation. For challenging event analysis tasks ( e, event detection recognition and recounting ) in long untrimmed Internet videos. And we prove new and closed-form proximal steps, and achieve promising improvements. In this paper we propose a new semantic pooling approach especially when only a few shots/segments are relevant to the event of interest while many other shots are irrelevant or even misleading, The commonly adopted pooling strategies aggregate the shots indifferently in one way or another, resulting in a great loss of information, Instead in this work we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event analysis, Next we propose a new isotonic regularizer that is able to exploit the constructed semantic ordering information The resulting nearly-isotonic support vector machine classifier exhibits higher discriminative power in event analysis tasks. Computationally we develop an efficient implementation using the proximal gradient algorithm, We conduct extensive experiments on three real-world video datasets.
2K_test_1103	Recently there has been a surge of interest in using spectral methods for estimating latent variable models. However it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as \emph { continuous matrices }, We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. In this paper we study the estimation of an $ m $ -state hidden Markov model ( HMM ) with only smoothness assumptions, such as H\ '' olderian conditions, on the emission densities, We implement our method using Chebyshev polynomial approximations.
2K_test_1104	Meeting tail latency Service Level Objectives ( SLOs ) in shared cloud networks is both important and challenging, One primary challenge is determining limits on the multi-tenancy such that SLOs are met, Doing so involves estimating latency, which is difficult especially when tenants exhibit bursty behavior as is common in production environments, Nevertheless recent papers in the past two years ( Silo, QJump and PriorityMeister ) show techniques for calculating latency based on a branch of mathematical modeling called Deterministic Network Calculus ( DNC ), The DNC theory is designed for adversarial worst-case conditions, which is sometimes necessary, but is often overly conservative. Typical tenants do not require strict worst-case guarantees, but are only looking for SLOs at lower percentiles ( e, 9th ) for tail latency SLOs. SNC-Meister supports 75 % more tenants than the state-of-the-art. This paper describes SNC-Meister, a new admission control system SNC-Meister improves upon the state-of-the-art DNC-based systems by using a new theory, Stochastic Network Calculus ( SNC ), which is designed for tail latency percentiles Focusing on tail latency percentiles, rather than the adversarial worst-case DNC latency, allows SNC-Meister to pack together many more tenants. : in experiments with production traces.
2K_test_1105	To detect and segment facial beard/moustache simultaneously in challenging facial images. The robustness and effectiveness of the proposed system. This paper presents a robust, fully automatic and semi self-training system Based on the observation that some certain facial areas, cheeks do not typically contain any facial hair whereas the others, brows often contain facial hair, a self-trained model is first built using a testing image itself, To overcome the limitation of that facial hairs in brows regions and beard/moustache regions are different in length, a pre-trained model is also constructed using training data The pre-trained model is only pursued when the self-trained model produces low confident classification results, In the proposed system, we employ the superpixel together a combination of two classifiers, Random Ferns ( rFerns ) and Support Vector Machines ( SVM ) to obtain good classification performance as well as improve time efficiency, A feature vector consisting of Histogram of Gabor ( HoG ) and Histogram of Oriented Gradient of Gabor ( HOGG ) at different directions and frequencies, is generated from both the bounding box of the superpixel and the super pixel foreground, The segmentation result is then refined by our proposed aggregately searching strategy in order to deal with inaccurate landmarking points, Detect and segment beard/moustache simultaneouslyUse advantages of both pre-trained model and self-trained modelWork on superpixelPropose an aggregate searching strategy to overcome the limits of landmarkerPropose a new feature that is able to emphasize high frequency information of facial hair. Experimental results have demonstrated It is evaluated in images drawn from three entire databases i, the Multiple Biometric Grand Challenge ( MBGC ) still face database, the NIST color Facial Recognition Technology FERET database and a large subset from Pinellas County database.
2K_test_1106	The best prior complete algorithm has significantly worse complexity and has, to our knowledge never been implemented. For finding an epsilon-Nash equilibrium for arbitrarily small epsilon. We present a complete algorithm in games with more than two players, The main components of our tree-search-based method are a node-selection strategy, an exclusion oracle and a subdivision scheme, The node-selection strategy determines the next region to be explored -- -based on the region 's size and an estimate of whether the region contains an equilibrium The exclusion oracle provides a provably correct sufficient condition for there not to exist an equilibrium in the region, The subdivision scheme determines how the region is split if it can not be excluded, Unlike well-known incomplete methods, our method does not need to proceed locally, which avoids it getting stuck in a local minimum that may be far from any actual equilibrium, The run time grows rapidly with the game size, and this suggests a hybrid scheme where one of the relatively fast prior incomplete algorithms is run, and if it fails to find an equilibrium, then our method is used.
2K_test_1107	Multimodal sentiment analysis is drawing an increasing amount of attention these days, It enables mining of opinions in video reviews and surveys which are now available aplenty on online platforms like YouTube. However the limited number of high-quality multimodal sentiment data samples may introduce the problem of the sentiment being dependent on the individual specific features in the dataset, This results in a lack of generalizability of the trained models for classification on larger online platforms that improves the generalizability of trained discriminative neural networks. We show how SAL improves the generalizability of state-of-the-art models, We increase prediction accuracy significantly in all three modalities ( text, audio video ) as well as in their fusion, We show how SAL, achieves good accuracy across test datasets. Then we propose a Select-Additive Learning ( SAL ) procedure SAL is a two-phase learning method, In Selection phase it selects the confounding learned representation, In Addition phase it forces the classifier to discard confounded representations by adding Gaussian noise. In this paper we first examine the data and verify the existence of this dependence problem, In our experiments even when trained on one dataset.
2K_test_1108	We discuss the implications of our results for market design in general, and kidney exchange in particular. We revisit the problem of designing optimal, individually rational matching mechanisms ( in a general sense, allowing for cycles in directed graphs ), where each player -- -who is associated with a subset of vertices -- -matches as many of his own vertices when he opts into the matching mechanism as when he opts out We offer a new perspective on this problem. Our main result asserts that, any fixed optimal matching is likely to be individually rational up to lower-order terms We also show that a simple and practical mechanism is ( fully ) individually rational, and likely to be optimal up to lower-order terms. By considering an arbitrary graph, but assuming that vertices are associated with players at random.
2K_test_1109	A descending clock auction ( DCA ) is for buying items from multiple sellers, The literature has focused on the case where each bidder has two options : to accept or reject the offered price. However in many settings -- such as the FCC 's imminent incentive auction -- each bidder may be able to sell one from a set of options, for the dynamics of each bidder 's state to optimize the trajectory of price offers to different bidders for different options. Show that the optimization-based approach dramatically outperforms the percentile-based approach -- because it takes feasibility into account in pricing, Both pricing techniques scale to the large. We present a multi-option DCA ( MDCA ) framework where at each round, the auctioneer offers each bidder different prices for different options, and a bidder may find multiple options still acceptable Setting prices during a MDCA is trickier than in a DCA, We develop a Markov chain model ( which options are still acceptable ), We leverage it This is unlike most auctions which only compute the next price vector, Computing the trajectory enables better planning, We reoptimize the trajectory after each round, Each optimization minimizes total payment while ensuring feasibility in a stochastic sense We also introduce percentile-based approaches to decrementing prices. Experiments with real FCC incentive auction interference constraint data.
2K_test_1110	Provides a scalable efficient solution to the straggler problem for iterative machine learning ( ML ), The frequent ( e, per iteration ) barriers used in traditional BSP-based distributed ML implementations cause every transient slowdown of any worker thread to delay all others, to address straggler threads. Confirm the significance of the problem and the effectiveness of FlexRR 's solution, Using FlexRR we consistently observe near-ideal run-times ( relative to no performance jitter ) across all real and injected straggler behaviors tested. FlexRR FlexRR combines a more flexible synchronization model with dynamic peer-to-peer re-assignment of work among workers. Experiments with real straggler behavior observed on Amazon EC2 and Microsoft Azure, as well as injected straggler behavior stress tests.
2K_test_1111	For determining pitch and yaw of an elongated interface object as it interacts with a touchscreen surface. A method and apparatus A touch image is received, and this touch image has at least a first area that corresponds to an area of the touchscreen that has an elongated interface object positioned at least proximate to it, The elongated interface object has a pitch and a yaw with respect to the touchscreen surface, A first transformation is performed to obtain a first transformation image of the touch image, and a second transformation is performed to obtain a second transformation image of the touch image, The first transformation differs from the second transformation The yaw is determined for the elongated interface object based on both the first and second transformation images, The pitch is determined based on at least one of the first and second transformation images.
2K_test_1112	Which team is the best in the league ? How does my team fare with respect to the rest of the league ? These are questions that every sports fan is interested in knowing the answers to, In other cases such as in college sports, knowing the answer to these questions is crucial for shaping the picture of spe- cific contests, In professional sports sports networks provide power rankings regularly - typically every week or month de- pending on the season length of the league - based on their experts opinion, We finally propose an ad- vanced ranking technique based on tensor decomposition. Way of ranking sports teams. We show that the cycles in the network are significantly correlated with the performance. In this work we propose an alternative, ob- jective and network-based In brief, our method is based on analyzing a directed network formed between the teams of the corresponding leagues that captures their win-lose relationships Using data from the National Football League and the National Basketball As- sociation, we show that even simple network theory metrics ( e, Page Rank ) can provide a ranking that has the same ac- curacy in predicting winners of upcoming match-ups as more complicated systems ( e. We further explore the impact of the network structure on the prediction accuracy and.
2K_test_1114	Describing videos with natural language is one of the ultimate goals of video understanding, Video records multi-modal information including image, motion aural speech and so on. MSR Video to Language Challenge provides a good chance to study multi-modality fusion in caption task. Results show the effectiveness of multi-modal fusion encoder trained in the end-to-end framework, which achieved top performance in both common metrics evaluation and human evaluation. In this paper we propose the multi-modal fusion encoder and integrate it with text sequence decoder into an end-to-end video caption framework, Features from visual aural, speech and meta modalities are fused together to represent the video contents, Long Short-Term Memory Recurrent Neural Networks ( LSTM-RNNs ) are then used as the decoder to generate natural language sentences.
2K_test_1115	We envision that by casting accountability theories in computing and control systems in terms of causal information flow, we can provide a common foundation to develop a theory for CPS that compose elements from both domains. Our position is that a key component of securing cyber-physical systems ( CPS ) is to develop a theory of accountability that encompasses both control and computing systems. We summarize our results. We envision that a unified theory of accountability in CPS can be built on a foundation of causal information flow analysis, This theory will support design and analysis of mechanisms at various stages of the accountability regime : attack detection, attack identification or localization ), and corrective measures ( e, via resilient control ) As an initial step in this direction, We use the Kullback-Liebler ( KL ) divergence as a causal information flow measure, We then recover using information flow analyses, a set of existing results in the literature that were previously proved using different techniques, These results cover passive detection, stealthy attack characterization and active detection, This research direction is related to recent work on accountability in computational systems [ 1 ], [ 2 ] [ 3 ]. On attack detection in control systems.
2K_test_1116	Electric vehicles ( EVs ), specifically Battery EVs ( BEVs ), can offer significant energy and emission savings over internal combustion engine based vehicles, Norway has a long history of research and government incentives for BEVs. Allow us to fully examine consumers ' BEV choices influenced by car specifications, prices and government incentives ( public bus lanes access, toll waivers and charging stations ), To capture the choices of heterogeneous personal consumers and business buyers. The results suggest significant positive effects of BEV technology improvement, toll waivers and charging station density on BEV sales for both personal consumers and business buyers, except that bus lanes access may have a negative impact for personal consumers, possibly due to consumers ' concern regarding bus lane congestion The effects on business buyers are generally less pronounced than on personal consumers, In addition we find significant heterogeneity in consumer preferences over BEV price and car specifications In particular, a 9 500 NOK increases in consumer income can lead to approximately 10 % decrease in price sensitivity on average, In other words individual consumers with higher income would be less price-sensitive than those with lower income, Significant heterogeneity in incentive policy impacts on different brands are also found, especially for Renault Ford, Nissan ( all three being a good compromise of prices and ranges ) and Tesla ( with an exceptionally long range ). We use Random-Coefficient Discrete Choice Model ( referred to BLP model ). The BEV market and ample data sets in Norway Our study is instantiated on the entire BEV sales data in Norway from 2011 to 2013, as well as demographics information at municipality level.
2K_test_1117	Green and Laffonti ? [ 13 ] proved that one can not generically achieve both. We study efficiency and budget balance for designing mechanisms in general quasi-linear domains. Show that the inefficiency for a simple randomized mechanism is 5 -- -100 times smaller than the worst case, This relative difference increases with the number ofi ? agents. We consider strategyproof budget-balanced mechanisms that are approximately efficient For deterministic mechanisms, we show that a strategyproof and budget-balanced mechanism must have a sink agent whose valuation function is ignored in selecting an alternative, and she is compensated with the payments made by the other agents, We assume the valuations of the agents come from a bounded open interval, This result strengthens Green and Laffont 's impossibility result by showing that even in a restricted domain of valuations, there does not exist a mechanism that is strategyproof, budget balanced and takes every agent 's valuation into consideration -- a corollary of which is that it can not be efficient, Using this result we find a tight lower bound on the inefficiencies of strategyproof, budget-balanced mechanisms in this domain, The bound shows that the inefficiency asymptotically disappears when the number of agents is large -- a result close in spirit to Green and Laffonti ? [ 13, However our results provide worst-case bounds and the best possible rate of convergence Next, we consider minimizing any convex combination of inefficiency and budget imbalance, We show that if the valuations are unrestricted, no deterministic mechanism can do asymptotically better than minimizing inefficiency alone, Finally we investigate randomized mechanisms and provide improved lower bounds on expected inefficiency We give a tight lower bound for an interesting class of strategyproof, We also use an optimization-based approach -- in the spirit of automated mechanism design -- to provide a lower bound on the minimum achievable inefficiency of any randomized mechanism. Experiments with real data from two applications.
2K_test_1119	The Next-Generation Airborne Collision Avoidance System ( ACAS X ) is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft, It is currently being developed by the Federal Aviation Administration ( FAA ) Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety. Under which the advice given by ACAS X is safe formally verify these configurations. In this paper we determine the geometric configurations under a precise set of assumptions and using hybrid systems theorem proving techniques We consider subsequent advisories and show how to adapt our formal verification to take them into account, We examine the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version, demonstrating how formal hybrid systems proving approaches are helping to ensure the safety of ACAS X.
2K_test_1120	Low engagement rates and high attrition rates have been formidable challenges for mobile apps and their long-term success, especially for those whose revenues come mainly from in-app purchases, To date still little is known about how companies can comprehensively identify user engagement stages so as to improve business revenues, Our structural-model- and field-experimentation-based findings are nontrivial and suggest, with respect to the crucial role of modeling user engagement, potential overall welfare improvements in the mobile app market. For modeling of consumer latent engagement stages. Interestingly we found that such an engagement-specific pricing strategy leads, simultaneously to lower average prices for consumers and higher overall business revenues for the app, Our experimental results provide more causal evidence that a personalized promotion strategy targeting user engagement stages can both decrease costs to app users and enhance overall business performance. This paper proposes a structural econometric framework that accounts for both the time-varying nature of engagement and consumer forward-looking consumption behavior Our policy simulation enabled us to tailor, based on the model-detected engagement stages, an optimal pricing strategy to each consumer. The present study analyzed a fine-grained mobile tapstream dataset on mobile users ' continuous content consumption behavior in a popular mobile reading app, To further evaluate the effectiveness of our method, we conducted a randomized field experiment on a mobile app platform.
2K_test_1121	An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy, In a system with redundant requests, each job that arrives to the system is copied and dispatched to multiple servers, As soon as the first copy completes service, the job is considered complete, and all remaining copies are deleted. A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists. We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity. We propose a theoretical model of redundancy, the Redundancy- d system, in which each job sends redundant copies to d servers chosen uniformly at random, We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers.
2K_test_1123	Practical implications As many marketers are interested in hoarding consumers personal information, privacy advocates call for methods that would ensure careful and well-informed disclosure Offering reversibility to a decision to disclose personal information, or merely pointing out the irreversibility of that decision, can make consumers reevaluate the sensitivity of the situation, leading to more careful disclosures Originality/value Although previous research on reversibility in consumer behavior focused on product return policies and showed that reversibility increases purchases, none have studied how reversibility affects self-disclosure and how it can decrease it. Purpose This paper aims to examine how reversibility in disclosing personal information that is, having ( vs not having ) to option to later revise or retract personal information can impact consumers willingness to divulge personal information. Findings showed that consumers disclose less in both the reversible and irreversible conditions, compared to the control condition showed that this is because consumers treat reversibility as a cue to the sensitivity of the information they are asked to divulge, and that leads them to disclose less when reversibility or irreversibility is made explicitly salient beforehand. Design/methodology/approach Three studies examined how informing consumers they may ( reversible condition ) or may not ( irreversible condition ) revise their personal information in the future affected their propensity to disclose personal information, compared to a control condition Study 1 ( which included three experiments with different time intervals between initial and revised disclosure Studies 2 and 3.
2K_test_1124	The approach finds a dual flow solution to this linear system through a sequence of flow adjustments along cycles. We study the performance of linear solvers for graph Laplacians based on the combinatorial cycle adjustment methodology proposed by [ Kelner-Orecchia-Sidford-Zhu STOC-13 ], for handling these adjustments. Our methods demonstrate significant speedups over previous implementations, and are competitive with standard numerical routines. We study both data structure oriented and recursive methods The primary difficulty faced by this approach, updating and querying long cycles, motivated us to study an important special case : instances where all cycles are formed by fundamental cycles on a length $ n $ path.
2K_test_1125	Whose goal is to move the state of a CPS to a target state while ensuring that his or her probability of being detected does not exceed a given bound, to find suitable attack sequences. This paper studies an attacker against a cyber-physical system ( CPS ) The attacker 's probability of being detected is related to the nonnegative bias induced by his or her attack on the CPS ' detection statistic We formulate a linear quadratic cost function that captures the attacker 's control goal and establish constraints on the induced bias that reflect the attacker 's detection-avoidance objectives When the attacker is constrained to be detected at the false-alarm rate of the detector, we show that the optimal attack strategy reduces to a linear feedback of the attacker 's state estimate In the case that the attacker 's bias is upper bounded by a positive constant, we provide two algorithms -- an optimal algorithm and a sub-optimal, less computationally intensive algorithm --. Finally we illustrate our attack strategies in numerical examples based on a remotely-controlled helicopter under attack.
2K_test_1127	Real-time virtualization techniques have been investigated with the primary goal of consolidating multiple real-time systems onto a single hardware platform while ensuring timing predictability However, a shared last-level cache ( LLC ) on recent multi-core platforms can easily hamper timing predictability due to the resulting temporal interference among consolidated workloads. Since such interference caused by the LLC is highly variable and may have not even existed in legacy systems to be consolidated, it poses a significant challenge for real-time virtualization. Results show that our techniques can effectively control the cache allocation of tasks in VMs, Our cache management scheme yields a significant utilization benefit compared to other approaches. In this paper we propose a real-time cache management framework Our framework introduces two hypervisor-level techniques, vLLC and vColoring that enable the cache allocation of individual tasks running in a virtual machine ( VM ), which is not achievable by the current state of the art Our framework also provides a cache management scheme that determines cache allocation to tasks, designs VMs in a cache-aware manner, and minimizes the aggregated utilization of VMs to be consolidated. As a proof of concept, we implemented vLLC and vColoring in the KVM hypervisor running on x86 and ARM multi-core platforms, Experimental with three different guest OSs, namely Linux/RK vanilla Linux and MS Windows Embedded.
2K_test_1128	When navigating indoors blind people are often unaware of key visual information, such as posters signs, With VizMap we move towards integrating the strengths of the end user, on-site crowd online crowd, and computer vision to solve a long-standing challenge in indoor blind exploration. To collect this information and make it available non-visually. Our VizMap system uses computer vision and crowdsourcing VizMap starts with videos taken by on-site sighted volunteers and uses these to create a 3D spatial model, These video frames are semantically labeled by remote crowd workers with key visual information, These semantic labels are located within and embedded into the reconstructed 3D model, forming a query-able spatial representation of the environment, VizMap can then localize the user with a photo from their smartphone, and enable them to explore the visual elements that are nearby. We explore a range of example applications enabled by our reconstructed spatial representation.
2K_test_1129	Electrical Impedance Tomography ( EIT ) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch, This prior work demonstrated great promise for non-invasive, high accuracy recognition of gestures for interactive control shed light on the future feasibility of EIT for sensing human input. That offers improved sampling speed and resolution. We introduce a new system In turn, this enables superior interior reconstruction and gesture recognition. More importantly we use our new system as a vehicle for experimentation ' we compare two EIT sensing methods and three different electrode resolutions, Results from in-depth empirical evaluations and a user study.
2K_test_1130	A novel data classifier. Demonstrate that our approach achieves high accuracy in multiclass classification and outperforms other classification approaches. We present that is based on the regularization of graph signals, Our approach is based on the theory of discrete signal processing on graphs where the graph represents similarities between data and we interpret labels for the dataset elements as a signal indexed by the nodes of the graph, We postulate that true labels form a low-frequency graph signal and the classifier finds the smoothest graph signal that satisfies constraints given by known data labels.
2K_test_1131	Existing smartwatches rely on touchscreens for display and input, which inevitably leads to finger occlusion and confines interactivity to a small area, which enables rich around-device. Suggesting that AuraSense can be low latency and robust across users and environments. In this work we introduce AuraSense, using electric field sensing as an adapted device, We identified four configurations that can support six well-known modalities of particular interest and utility, including gestures above or in close proximity to watches, and touchscreen-like finger tracking on the skin. To explore how this sensing approach could enhance smartwatch interactions, we considered different antenna configurations and how they could enable useful interaction modalities We quantify the feasibility of these input modalities.
2K_test_1132	That can be used to quantify the shape progression patterns of the bilateral hippocampi, amygdalas and lateral ventricles in healthy control ( HC ), mild cognitive impairment ( MCI ), and Alzheimer 's disease ( AD ). Longitudinally the geodesic distance was found to be proportional to the elapsed time separating the two scans in question we found that each structures annualized rate of change in the geodesic distance followed the order of AD > MCI > HC, with statistical significance being reached in every case In addition, for each of the six structures of interest, within the same time interval ( e, from baseline to the 6th month ), we observed significant correlations between the geodesic distance and the cognitive deterioration as quantified by the ADAS-cog increase and the MMSE decrease Furthermore, as the disease progresses over time, this linkage between the inter-shape geodesic distance and the cognitive decline becomes considerably stronger and more significant. We propose a geodesic distance on a Grassmannian manifold Cross-sectionally, utilizing a linear mixed-effects statistical model. Longitudinal magnetic resonance imaging ( MRI ) scans of 754 subjects ( 3092 scans in total ) were used in this study.
2K_test_1133	The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals, Blind people can not independently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone. Help blind people use nearly any interface they encounter. We show that VizLens provides accurate and usable real-time feedback and our crowdsourcing labeling workflow was fast ( 8 minutes ), 7 % ) and cheap ( $ 1. We introduce VizLens - an accessible mobile application and supporting backend that can robustly and interactively VizLens users capture a photo of an inaccessible interface and send it to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface to make subsequent computer vision easier, The VizLens application helps users recapture the interface in the field of the camera, and uses computer vision to interactively describe the part of the interface beneath their finger ( updating 8 times per second ), We then explore extensions of VizLens that allow it to ( i ) adapt to state changes in dynamic interfaces, ( ii ) combine crowd labeling with OCR technology to handle dynamic displays, and ( iii ) benefit from head-mounted cameras VizLens robustly solves a long-standing challenge in accessibility by deeply integrating crowdsourcing and computer vision. In a study with 10 blind participants.
2K_test_1134	That learns the optimal parameters of the neural population model. The empirical results show that the patterns of parameters as a seizure approach and the method is efficient in analyzing nonlinear epilepsy electroencephalogram data, The accuracy of estimating the optimal parameters is improved by using the nonlinear dynamic model. We propose a nonlinear dynamic model for an invasive electroencephalogram analysis via the LevenbergMarquardt algorithm, We introduce the crucial windows where the estimated parameters present patterns before seizure onset The optimal parameters minimizes the error between the observed signal and the generated signal by the model, The proposed approach effectively discriminates between healthy signals and epileptic seizure signals. We evaluate the proposed method using an electroencephalogram dataset with normal and epileptic seizure sequences.
2K_test_1135	Smartwatches and wearables are unique in that they reside on the body, presenting great potential for always-available input and interaction, Their position on the wrist makes them ideal for capturing bio-acoustic signals, Overall our contributions unlock user interface techniques that previously relied on special-purpose and/or cumbersome instrumentation, making such interactions considerably more feasible for inclusion in future consumer devices. That boosts the sampling rate of a smartwatch 's existing accelerometer to 4 kHz. We developed a custom smartwatch kernel For example, we can use bio-acoustic data to classify hand gestures such as flicks, claps scratches and taps, which combine with on-device motion tracking to create a wide range of expressive input modalities, Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects, enabling passive object recognition that can augment everyday experiences with context-aware functionality Finally, we can generate structured vibrations using a transducer, and show that data can be transmitted through the human body. Using this new source of high-fidelity data, we uncovered a wide range of applications.
2K_test_1136	Given `` who-trusts/distrusts-whom '' information, how can we propagate the trust and distrust ? With the appearance of fraudsters in social network sites, the importance of trust prediction has increased. Most such methods use only explicit and implicit trust information ( e, if Smith likes several of Johnson 's reviews, then Smith implicitly trusts Johnson ), but they do not consider distrust to handle all three types of interaction information : explicit trust, implicit trust and explicit distrust. Confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy, achieving up to 50, 4 percentage relative improvement. In this paper we propose PIN -TRUST, a novel method The novelties of our method are the following : ( a ) it is carefully designed, to take into account positive, implicit and negative information, ( b ) it is scalable ( i, linear on the input size ), ( c ) most importantly, it is effective and accurate. Our extensive experiments with a real dataset, com data of 100K nodes and 1M edges.
2K_test_1137	Many applications in speech, robotics finance and biology deal with sequential data, where ordering matters and recurrent structures are common. However this structure can not be easily captured by standard kernel functions, To model such structure. Where the predictive uncertainties provided by GP-LSTM are uniquely valuable. We propose expressive closed-form kernel functions for Gaussian processes, The resulting model GP-LSTM, fully encapsulates the inductive biases of long short-term memory ( LSTM ) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic procedure and exploit the structure of these kernels for fast and scalable training and prediction. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application.
2K_test_1139	As mobile computing and cloud computing converge, the sensing and interaction capabilities of mobile devices can be seamlessly fused with compute-intensive and data-intensive processing in the cloud, Cloudlets are important architectural components in this convergence, representing the middle tier of a mobile device cloudlet cloud hierarchy. We show how cloudlets enable a new genre of applications called cognitive assistance applications that augment human perception and cognition for cognitive assistance. We describe a plug-and-play architecture. And a proof of concept using Google Glass.
2K_test_1140	In the human genome, distal enhancers are involved in regulating target genes through proximal promoters by forming enhancer-promoter interactions This work shows for the first time that sequence-based features alone can reliably predict enhancer-promoter interactions genome-wide, which provides important insights into the sequence determinants for long-range gene regulation. However although recently developed high-throughput experimental approaches have allowed us to recognize potential enhancer-promoter interactions genome-wide, it is still largely unknown whether there are sequence-level instructions encoded in our genome that help govern such interactions, to predict enhancer-promoter interactions. Demonstrate that SPEID is effective in predicting enhancer-promoter interactions as compared to state-of-the-art methods that use non-sequence features from functional genomic signals. Here we report a new computational method ( named `` SPEID '' ) using deep learning models based on sequence-based features only, when the locations of putative enhancers and promoters in a particular cell type are given. Our results across six different cell types.
2K_test_1141	Sensorized commercial buildings are a rich target for building a new class of applications that improve operational and energy efficiency of building operations that take into account human activities The attendees would be able to create example buildings and write their own queries. Such applications however rarely experience widespread adoption due to the lack of a common descriptive schema that would enable porting these applications and systems to different buildings, for representing metadata in buildings. Show application queries that extracts relevant metadata from these buildings. Our demo presents Brick [ 4 ], a uniform schema Our schema defines a concrete ontology for sensors, subsystems and relationships among them, which enables portable applications. Using a web application, we will demonstrate real buildings that have been mapped to the Brick schema.
2K_test_1142	Designed for low-power real-time sensing of the number of occupants in indoor spaces. Demonstrate that our algorithm can continuously execute using energy harvested from indoor solar panels we see that we can detect motions with an average of 85 % recall rate and perform occupancy counting with an average error of 10 % in terms of maximum occupancy. In this paper we present a platform The system transmits a wide-band ultrasonic signal into a room and then processes the superposition of the reflections recorded by a microphone The system has two modes of operation, one for presence detection and one for estimating the number of occupants in a region, The presence detection uses the difference between multiple transmissions in succession with a set of general classifiers that make a binary decision about if the room contains occupants, We then use a semi-supervised learning approach based on Weighted Principal Component Analysis ( WPCA ) that requires minimal training data to estimate the number of occupants We also present the design of an energy harvesting embedded platform and The platform has a dual Bluetooth Low-Energy and 802, 4 interface to communicate with a gateway or nearby mobile phone that runs an interface that aids in collecting training data. We evaluate our algorithm on a wide-variety of indoor spaces as well as benchmark the hardware in terms of sampling rate given an energy budget On more than three weeks of data.
2K_test_1143	Commercial buildings have long since been a primary target for applications from a number of areas : from cyber-physical systems to building energy use to improved human interactions in built environments, While technological advances have been made in these areas, such solutions rarely experience widespread adoption due to the lack of a common descriptive schema which would reduce the now-prohibitive cost of porting these applications and systems to different buildings. Recent attempts have sought to address this issue through data standards and metadata schemes, but fail to capture the set of relationships and entities required by real applications, Building upon these works, for representing metadata in buildings. We demonstrate the completeness and effectiveness of Brick. This paper describes Brick, a uniform schema Our schema defines a concrete ontology for sensors, subsystems and relationships among them, which enables portable applications. By using it to represent the entire vendor-specific sensor metadata of six diverse buildings across different campuses, comprising 17 700 data points, and running eight complex unmodified applications on these buildings.
2K_test_1144	Several generations of inexpensive depth cameras have opened the possibility for new kinds of interaction on everyday surfaces, A number of research systems have demonstrated that depth cameras, combined with projectors for output, can turn nearly any reasonably flat surface into a touch-sensitive display. However even with the latest generation of depth cameras, it has been difficult to obtain sufficient sensing fidelity across a table-sized surface to get much beyond a proof-of-concept demonstration a novel touch-tracking algorithm. Results show that our technique boosts touch detection accuracy by 15 % and reduces positional error by 55 % compared to the next best-performing technique. In this paper we present DIRECT, that merges depth and infrared imagery captured by a commodity sensor This yields significantly better touch tracking than from depth data alone, as well as any prior system, Further extending prior work, DIRECT supports arbitrary user orientation and requires no prior calibration or background capture, We describe the implementation of our system and. Quantify its accuracy through a comparison study of previously published.
2K_test_1145	To provide accuracy on the order of 10 's of nanoseconds for indoor applications. Generating a tightly synchronized PPS output that is able to adjust for the distance between the nodes, Even without communication the devices maintain synchronization over multiple seconds. In this demonstration we present Pulsar, a speed-of-light propagation-aware time synchronization platform, Pulsar uses ultra-wideband ( UWB ) radios for time transfer with each node backed by a chip scale atomic clock ( CSAC ) that in combination are able due to a stable CSAC clocking the system. The demonstration will show two Pulsar boards.
2K_test_1146	Cohesion and structural equivalence are two competing network models to explain diffusion of innovation. The dispute of which model plays a more influential role has not been resolved This paper attempts to reconcile this problem in a large network setting adoption of caller ringback tone ( CRBT ) in a cellular telephone conversation network to extract multiple densely connected and self-contained subpopulations from the network. We found subpopulation size in such million-node network only falls in two levels, 200 and 500 in the extraction step The results show CRBT adoption is affected by both cohesion and structural equivalence The size and direction of network influence both change with the size of group, Structural equivalence has a negative effect on adoption when group size is at about 200, and has a positive effect when group size is at about 500, The effect of cohesion, on the other hand. Since this societal scale network is very large, we use a novel technique Using a new auto-probit model with network terms. We then compare the competing influences of cohesion and structural equivalence on each of the subpopulation extracted, Finally we use meta-analysis to summarize the estimated parameters from all subpopulations.
2K_test_1147	To respond to environmental changes, such as drought plants must regulate numerous cellular processes, The work provides a framework for understanding and modulating plant responses to stress. Mapped the complex gene regulatory networks involved in the response to the plant hormone abscisic acid. Working in the model plant Arabidopsis, profiled the binding of 21 transcription factors to chromatin and.
2K_test_1148	For the problem of energy disaggregation. Furthermore we show that, once the model is trained, the algorithm can perform inference. In this paper we introduce BOLT, a novel approach that performs online binary matrix factorization on a sequence of high frequency current cycles collected in a building to infer additive subcomponents of the current signal, The system learns these constituent current waveforms in an unsupervised fashion and, in a subsequent step, seeks to find combinations of these subcomponents that constitute appliances By doing so, points in time when appliances are active and, to some degree their power consumption can be estimated by BOLT, Our system treats energy disaggregation as a binary matrix factorization problem and uses a neural network, with binary activations in the one but last layer and a linear output layer, which allows leveraging high-frequency information without having to explicitly transmit and store large amounts of data to a centralized repository. The algorithmic performance of the proposed method is evaluated on a publicly available dataset in real-time on inexpensive off-the-shelf and general purpose hardware.
2K_test_1149	Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. To enable classification multi-task learning, additive covariance structures and stochastic gradient training. We show improved performance. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective, Within this framework we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points and structure exploiting algebra. Over stand alone deep networks, SVMs and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points.
2K_test_1150	In graph signal processing, the graph adjacency matrix or the graph Laplacian commonly define the shift operator, The spectral decomposition of the shift operator plays an important role in that the eigenvalues represent frequencies and the eigenvectors provide a spectral basis, This is useful for example, in the design of filters. However the graph or network may be uncertain due to stochastic influences in construction and maintenance, and under such conditions, the eigenvalues of the shift matrix become random variables This paper examines the spectral distribution of the eigenvalues of random networks. The main results characterize the form of the solution to an important system of equations that leads to this deterministic distribution function and significantly reduce the number of equations that must be solved to find the solution for a given set of model parameters are provided for sample parameters. Formed by including each link of a D-dimensional lattice supergraph independently with identical probability, Using the stochastic canonical equation methods developed by Girko for symmetric matrices with independent upper triangular entries, a deterministic distribution is found that asymptotically approximates the empirical spectral distribution of the scaled adjacency matrix for a model with arbitrary parameters. Simulations comparing the expected empirical spectral distributions and the computed deterministic distributions.
2K_test_1151	Existing approaches for trilateration require three or more beacons to determine a unique position solution. In this paper we address the problem of range-based beacon placement given a floor plan to support indoor localization systems to uniquely localize with only two beacons, to account for indoor spaces. Our approach is able to reduce the number of beacons between 22 % and 60 % ( 33 % on an average ) as compared to standard trilateration. We show that with prior knowledge of the map and a model of beacon coverage, it is possible This not only reduces installation cost by requiring fewer nodes, but can also improve robustness One of the main challenges with respect to beacon placement algorithms is defining a metric for estimating performance We propose augmenting the commonly used Geometric Dilution of Precision ( GDOP ) metric. We then use this enhanced GDOP metric as part of a toolchain to compare various beacon placement algorithms in terms of coverage and expected accuracy, When applied to a set of real floor plans.
2K_test_1152	With the potential to enhance the power system 's operational flexibility in a cost-effective way, demand response is gaining increased attention worldwide, Industrial loads such as cement crushing plants consume large amounts of electric energy and therefore are prime candidates for the provision of significant amounts of demand response, They have the capability to turn on/off an arbitrary number of their crushers thereby adjusting their electric power consumption. However the change in power consumption by cement crushing plants and also other industrial loads are often not granular enough to provide valuable ancillary services such as regulation and load following, to overcome the granularity restriction with the help of an energy storage. In this paper we propose a coordination method based on model predictive control.
2K_test_1154	That enables smartphones ( and similar devices ) to establish quick, ad-hoc connections with a host touchscreen device. And demonstrate data transmission rates up to four times faster than prior camera-based techniques, highlighting different interaction techniques CapCam enables. We present CapCam a novel technique, simply by pressing a device to the screen 's surface Pairing data, used to bootstrap a conventional wireless connection, is transmitted optically to the phone 's rear camera, This approach utilizes the near-ubiquitous rear camera on smart devices, making it applicable to a wide range of devices, both new and old, CapCam also tracks phones ' physical positions on the host capacitive touchscreen without any instrumentation, enabling a wide range of targeted interactions. We quantify the communication performance of our pairing approach To demonstrate the unique capability and utility of our system, we built a series of example applications.
2K_test_1156	Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications, These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods. However the high variance inherent in these noisy gradients degrades performance and leads to slower mixing for reducing variance in stochastic gradient Langevin dynamics, yielding novel stochastic Monte Carlo methods that improve performance. We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics This is complemented by impressive empirical results obtained. In this paper we present techniques by reducing the variance in the stochastic gradient. On a variety of real world datasets, and on four different machine learning tasks ( regression, classification independent component analysis and mixture modeling ).
2K_test_1157	The core number of a node is the highest k-core in which the node participates, Core numbers are useful in many graph mining tasks, especially ones that involve finding communities of nodes, influential spreaders and dense subgraphs, Large graphs often do not fit on the memory of a single machine. We address the problem of estimating core numbers of nodes by reading edges of a large graph stored in external memory Existing external memory solutions do not give bounds on the required space, In practice existing solutions also do not scale with the size of the graph which estimates core numbers of nodes. Demonstrate that NimbleCore gives space savings up to 60X, while accurately estimating core numbers with average relative error less than 2. We propose NimbleCore an iterative external-memory algorithm, using O ( n log d max ) space, where n is the number of nodes and d max is the maximum node-degree in the graph, We also show that NimbleCore requires O ( n ) space for graphs with power-law degree distributions. Experiments on forty-eight large graphs from various domains.
2K_test_1158	This paper studies recursive nonlinear least squares parameter estimation in inference networks with observations distributed across multiple agents and sensed sequentially over time. It is shown that the proposed algorithms lead to consistent parameter estimates at each agent, the distributed estimators are shown to yield order-optimal convergence rates, as far as the order of pathwise convergence is concerned, the local agent estimates are as good as the optimal centralized nonlinear least squares estimator having access to the entire network observation data at all times. Conforming to a given inter-agent communication or interaction topology, distributed recursive estimators of the consensus + innovations type are presented in which at every observation sampling epoch the network agents exchange a single round of messages with their communication neighbors and recursively update their local parameter estimates by simultaneously processing the received neighborhood data and the new information ( innovation ) embedded in the observation sample. Under rather weak conditions on the connectivity of the inter-agent communication and a global observability criterion, Furthermore under standard smoothness assumptions on the sensing nonlinearities.
2K_test_1159	How can we design a product or movie that will attract, for example the interest of Pennsylvania adolescents or liberal newspaper critics ? What should be the genre of that movie and who should be in the cast. In this work we seek to identify how we can design new movies with features tailored to a specific user population. And show that it is highly scalable and effectively provides movie designs oriented towards different groups of users, including men women and adolescents. We formulate the movie design as an optimization problem over the inference of user-feature scores and selection of the features that maximize the number of attracted users Our approach, PNP is based on a heterogeneous, tripartite graph of users, movies and features ( e, actors directors genres ), where users rate movies and features contribute to movies, We learn the preferences by leveraging user similarities defined through different types of relations, and show that our method outperforms state-of-the-art approaches, including matrix factorization and other heterogeneous graph-based analysis. We evaluate PNP on publicly available real-world data.
2K_test_1160	The World Wide Web ( WWW ) has become a rapidly growing platform consisting of numerous sources which provide supporting or contradictory information about claims ( e, `` Chicken meat is healthy '' ), In order to decide whether a claim is true or false, one needs to analyze content of different sources of information on the Web, measure credibility of information sources, and aggregate all these information. This is a tedious process and the Web search engines address only part of the overall problem, producing only a list of relevant sources, estimates credibility of sources and correctness of claims. We demonstrate ClaimEval 's capability in determining validity of a set of claims, resulting in improved accuracy compared to state-of-the-art baselines. In this paper we present ClaimEval, a novel and integrated approach which given a set of claims to validate, extracts a set of pro and con arguments from the Web information sources, and jointly ClaimEval uses Probabilistic Soft Logic ( PSL ), resulting in a flexible and principled framework which makes it easy to state and incorporate different forms of prior-knowledge. Through extensive experiments on real-world datasets.
2K_test_1161	Human-Computer Music Performance for popular music - where musical structure is important, but where musicians often decide on the spur of the moment exactly what the musical form will be - presents many challenges to make computer systems that are flexible and adaptable to human musicians. One particular challenge is that humans easily follow scores and chord charts, adapt these to new performance plans, and understand media locations in musical terms ( beats and measures ), while computer music systems often use rigid and even numerical representations that are difficult to work with, where musical material in various media is synchronized, where musicians can quickly alter the performance order by specifying ( re- ) arrangements of the material, and where interfaces are supported in a natural way by music notation. We present new formalisms and representations, and a corresponding implementation.
2K_test_1162	In networks such as the smart grid, communication networks and social networks, local measurements/observations are scattered over a wide geographical area, Centralized inference algorithm are based on gathering all the observations at a central processing unit. However with data explosion and ever-increasing network sizes, centralized inference suffers from large communication overhead, heavy computation burden at the center, and susceptibility to central node failure This paper considers inference over networks. We discover and show that the message information matrix converges exponentially fast to a unique positive definite limit matrix for arbitrary positive semidefinite initialization. Using factor graphs and a distributed inference algorithm based on Gaussian belief propagation The distributed inference involves only local computation of the information matrix and of the mean vector and message passing between neighbors We provide the necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator, An easily verifiable sufficient convergence condition on the topology of a factor graph is further provided.
2K_test_1164	High performance dense linear algebra ( DLA ) libraries often rely on a general matrix multiply ( Gemm ) kernel that is implemented using assembly or with vector intrinsics, In particular the real-valued Gemm kernels provide the overwhelming fraction of performance for the complex-valued Gemm kernels, along with the entire level-3 BLAS and many of the real and complex LAPACK routines, Thus achieving high performance for the Gemm kernel translates into a high performance linear algebra stack above this kernel, However it is a monumental task for a domain expert to manually implement the kernel for every library-supported architecture, This leads to the belief that the craft of a Gemm kernel is more dark art than science, It is this premise that drives the popularity of autotuning with code generation in the domain of DLA. In order to shed light on the details or voo-doo required for implementing a high performance Gemm kernel. Results demonstrate that our approach yields generated kernels with performance that is competitive with kernels implemented manually or using empirical search. This paper instead focuses on an analytical approach to code generation of the Gemm kernel for different architecture, We distill the implementation of the kernel into an even smaller kernel, an outer-product and analytically determine how available SIMD instructions can be used to compute the outer-product efficiently We codify this approach into a system to automatically generate a high performance SIMD implementation of the Gemm kernel.
2K_test_1165	Many real-world graphs such as those that arise from the web, biology and transportation appear random and without a structure that can be exploited for performance on modern computer architectures, They focus primarily on reducing storage requirements and improving the cost of certain matrix operations for these large data sets. However these graphs have a scale-free graph topology that can be leveraged for locality, Existing sparse data formats are not designed to take advantage of this structure, for storing real-world scale-free graphs. We outperform the state of the art for graphs with up to 10 7 non-zero edges. Therefore we propose a data structure in a sparse and hierarchical fashion By maintaining the structure of the graph, we preserve locality in the graph and in the cache. For synthetic scale-free graph data.
2K_test_1166	We also suggest several useful extensions of this method for increasing interpretability of predictive models and prediction performance. To detect if a probabilistic binary classifier has statistically significant bias -- over or under predicting the risk -- for some subgroup, and identify the characteristics of this subgroup. We present a novel subset scan method This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias and poor classifier fit, not just in one or two dimensions of features of a priori interest, but in the space of all possible feature subgroups, We use subset scan and parametric bootstrap methods to efficiently address the difficulty of assessing the exponentially many possible subgroups.
2K_test_1167	Understanding how brain functions has been an intriguing topic for years With the recent progress on collecting massive data and developing advanced technology, people have become interested in addressing the challenge of decoding brain wave data into meaningful mind states, with many machine learning models and algorithms being revisited and developed, especially the ones that handle time series data because of the nature of brain waves. However many of these time series models, like HMM with hidden state in discrete space or State Space Model with hidden state in continuous space, only work with one source of data and can not handle different sources of information simultaneously. And reach a significant better results compared to traditional methods. In this paper we propose an extension of State Space Model to work with different sources of information together with its learning and inference algorithms. We apply this model to decode the mind state of students during lectures based on their brain waves.
2K_test_1168	To provide a novel driver behavior situational awareness system ( DB-SAW ). Abstract This paper presents a Grammar-aware Driver Parsing ( GDP ) algorithm, with deep features A deep model is first trained to extract highly discriminative features of the driver, Then a grammatical structure on the deep features is defined to be used as prior knowledge for a semi-supervised proposal candidate generation The Region with Convolutional Neural Networks ( R-CNN ) method is ultimately utilized to precisely segment parts of the driver, The proposed method not only aims to automatically find parts of the driver in challenging drivers in the wild databases, the standardized Strategic Highway Research Program ( SHRP-2 ) and the challenging Vision for Intelligent Vehicles and Application ( VIVA ), but is also able to investigate seat belt usage and the position of the driver 's hands ( on a phone vs on a steering wheel ). We conduct experiments on various applications and compare our GDP method against other state-of-the-art detection and segmentation approaches, SDS [ 1 ], CRF-RNN [ 2 ], DJTL [ 3 ], and R-CNN [ 4 ] on SHRP-2 and VIVA databases.
2K_test_1169	Mobile botnets have proliferated with the popularization of mobile and portable devices, being a simple and powerful method to launch Distributed Denial of Service ( DDoS ) attacks. For mobile botnets dynamics and their self-organized and self-adaptive behavior. This letter presents a stochastic adaptive model to generate DDoS attacks, The bots collaborations combine reinforcement and fading rules based upon the level of servers activity and map to a time-varying weighted directed graph This model can explain the natural emergence of two distinct time-scales when bots massively attack a server.
2K_test_1170	Recent computer systems research has proposed using redundant requests to reduce latency, The idea is to replicate a request so that it joins the queue at multiple servers, The request is considered complete as soon as any one copy of the request completes, Redundancy is beneficial because it allows us to overcome server-side variability the fact that the server we choose might be temporarily slow due to factors such as background load, network interrupts and garbage collection, When there is significant server-side variability, replicating requests can greatly reduce response times, In the past few years, queueing theorists have begun to study redundancy, first via approximations and, more recently via exact analysis, Unfortunately for analytical tractability, most existing theoretical analysis has assumed an Independent Runtimes ( IR ) model, wherein the replicas of a job each experience independent runtimes ( service times ) at different servers. The IR model is unrealistic and has led to theoretical results which can be at odds with computer systems implementation results to decouple the inherent job size ( X ) from the server-side slowdown ( S ). And has provably excellent performance. This paper introduces a much more realistic model of redundancy, Our model allows us where we track both S and X for each job, Analysis within the S & X model is, of course much more difficult, Nevertheless we design a policy, Redundant-to-Idle-Queue ( RIQ ) which is both analytically tractable within the S & X model.
2K_test_1171	The kernel trick becomes a burden for some machine learning tasks such as dictionary learning, where a huge amount of training samples are needed, making the kernel matrix gigantic and infeasible to store or process. In this work we propose to alleviate this problem and achieve Gaussian RBF kernel expansion explicitly for dictionary learning using Fastfood transform, which is an approximation of full kernel expansion. Yields much better results than its image space counterparts. We have shown in the context of missing data recovery through joint dictionary learning i, periocular-based full face hallucination, that the approximated kernel expansion using Fastfood transform for joint dictionary learning Also, explicit kernel expansion through Fastfood allows us to de-kernelize the reconstructed image in the feature space back to the image space, enabling applications that require reconstructive dictionaries such as cross-domain reconstruction, image super-resolution missing data recovery.
2K_test_1172	Policy approaches for addressing emerging consumer privacy concerns increasingly rely on providing consumers with more information and control over the usage of their personal data, Our results suggest that choice mechanisms alone may not reliably serve policy maker goals of protecting consumers privacy in the face of emerging data practices by firms. We evaluate the efficacy of such mechanisms in the face of subtle but common variation in the presentation of privacy choices to consumers. We find that consumers decision frames and thus, their propensity to select privacy protective alternatives can be subtly but powerfully influenced by commonplace heterogeneity in the presentation of privacy choices.
2K_test_1173	Counterfactual Regret Minimization ( CFR ) is a popular iterative algorithm for approximating Nash equilibria in imperfect-information multi-step two-player zero-sum games. For warm starting CFR. Demonstrate that one can improve overall convergence in a game by first running CFR on a smaller, coarser abstraction of the game and then using the strategy in the abstract game to warm start CFR in the full game. We introduce the first general, principled method Our approach requires only a strategy for each player, and accomplishes the warm start at the cost of a single traversal of the game tree, The method provably warm starts CFR to as many iterations as it would have taken to reach a strategy profile of the same quality as the input strategies, and does not alter the convergence bounds of the algorithms, Unlike prior approaches to warm starting, ours can be applied in all cases Our method is agnostic to the origins of the input strategies, For example they can be based on human domain knowledge, the observed strategy of a strong agent, the solution of a coarser abstraction, or the output of some algorithm that converges rapidly at first but slowly as it gets closer to an equilibrium.
2K_test_1174	Long-standing policy approaches to privacy protection are centered on consumer notice and control and assume that privacy decision making is a deliberative process of comparison between costs and benefits from information disclosure, An emerging body of work, however documents the powerful effects of factors unrelated to objective trade-offs in privacy settings, Our results confirm that understanding how differences in privacy choice emerge can help harmonize disparate perspectives on privacy decision making. In this paper we investigate how focusing on the process by which individuals make privacy choices can help explain the impact of rational and behavioral factors on privacy decision making. We find that effects of rational and behavioral factors are associated with differences in the order and valence of queries considered in privacy settings. In an online experiment, we borrow from query-theory literature and measure individuals ' considerations ( that is, queries ) across manipulations of rational and behavioral factors.
2K_test_1176	Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled a myriad of new applications many of which provide aerial vision-based sensing In intrusion detection or target tracking applications, it is important to reach a given area of interest in the shortest time, and create an online data streaming connection to a monitoring station for immediate delivery of content to the operator. However if the area of interest ( AOI ) is not contained in the field of view of a single UAV, it is necessary to move the sensor-UAV to sweep the region in order to provide the most fresh information as possible, In order to improve the collection time of the AOI to sweep the AOI as well as a decentralised formation control algorithm that maintains UAVs equally separated along the optimal path that covers the whole AOI, even with external disturbances such as wind gusts. We validate our approach We show a seven-fold increase in the refresh rate of the AOI coverage. We can cooperatively use multiple UAVs creating an array of moving cameras, that always remain connected without breaks in communication, We propose an optimal solution. In this work with a simulation that captures physical models and the application layer of each UAV, as well as the wireless network when comparing to a solution without the optimal sweeping and decentralised formation control.
2K_test_1179	Robust face detection is one of the most important preprocessing steps to support facial expression analysis, facial landmarking face recognition, pose estimation building of 3D facial models. Although this topic has been intensely studied for decades, it is still challenging due to numerous variants of face images in real-world scenarios, to robustly detect human facial regions from images collected under various challenging conditions, g large occlusions extremely low resolutions, facial expressions strong illumination variations. Results show that our proposed approach consistently achieves highly competitive results with the state-of-the-art performance against other recent face detection methods. In this paper we present a novel approach named Multiple Scale Faster Region-based Convolutional Neural Network ( MS-FRCNN ). The proposed approach is benchmarked on two challenging face detection databases, the Wider Face database and the Face Detection Dataset and Benchmark ( FDDB ), and compared against recent other face detection methods, Two-stage CNN Multi-scale Cascade CNN, Faceness Aggregate Chanel Features, HeadHunter Multi-view Face Detection.
2K_test_1183	5aural speech by converting it into visual text with less than a five second delay, Keeping the delay short 6 allows end-users to follow and participate in conversations, These results show the potential to 17 reliably capture speech even during sudden bursts of speed, as well as for generating enhanced captions, 18 unlike other human-powered captioning approaches. This article focuses on the fundamental prob7 lem that makes real-time captioning difficult : sequential keyboard typing is much slower than speaking. We show that both hearing and DHH participants preferred 15 and followed collaborative captions better than those generated by automatic speech recognition ( ASR ) or 16 professionals due to the more consistent flow of the resulting captions. We 8 first surveyed the audio characteristics of 240 one-hour-long captioned lectures on YouTube, such as speed 9 and duration of speaking bursts We then analyzed how these characteristics impact caption generation and 10 readability, considering specifically our human-powered collaborative captioning approach, We note that 11 most of these characteristics are also present in more general domains, For our caption comparison evalu12 ation, we transcribed a classroom lecture in real-time using all three captioning approaches, We recruited 13 48 participants ( 24 DHH ) to watch these classroom transcripts in an eye-tracking laboratory, We presented 14 these captions in a randomized.
2K_test_1184	Kidney exchange is a type of barter market where patients exchange willing but incompatible donors, These exchanges are conducted via cycleswhere each incompatible patient-donor pair in the cycle both gives and receives a kidneyand chains, which are started by an altruist donor who does not need a kidney in return. Finding the best combination of cycles and chains is hard The leading algorithms for this optimization problem use either branch and pricea combination of branch and bound and column generationor constraint generation, We show a correctness error in the leading prior branch-and-price-based approach [ Glorie et al, 2014 ] fix to it. Algorithms from our group autonomously make the transplant plans for that exchange, our new solver scales significantly better than the prior leading approaches. We develop a provably correct which also necessarily changes the algorithm 's complexity, as well as other improvements to the search algorithm A cap is desirable in practice since if even one edge in the chain fails, the rest of the chain fails : the cap precludes very long chains that are extremely unlikely to execute and instead causes the solution to have more parallel chains and cycles that are more likely to succeed We work with the UNOS nationwide kidney exchange, which uses a chain cap. Next we compare our solver to the leading constraint-generation-based solver and to the best prior correct branch-and-price-based solver, We focus on the setting where chains have a length cap, On that real data and demographically-accurate generated data.
2K_test_1185	We consider the problem of computing a maximal independent set ( MIS ). That shows that in this model, it is not possible to locally converge to an MIS in sub-polynomial time, which allow us to circumvent the lower bound and find an MIS in polylogarithmic time, it is possible to find an MIS in \ ( \mathcal O ( \log ^3 n ) \ ) time, then we can also find an MIS in \ ( \mathcal O ( \log ^3 n ) \ ) time, we can find an MIS in \ ( \mathcal O ( \log ^2 n ) \ ) time, it is also possible to find an MIS in \ ( \mathcal O ( \log ^2 n ) \ ) time. In an extremely harsh broadcast model that relies only on carrier sensing The model consists of an anonymous broadcast network in which nodes have no knowledge about the topology of the network or even an upper bound on its size, Furthermore it is assumed that an adversary chooses at which time slot each node wakes up, At each time slot a node can either beep, that is emit a signal, At a particular time slot, beeping nodes receive no feedback, while silent nodes can only differentiate between none of its neighbors beeping, or at least one of its neighbors beeping. We start by proving a lower bound We then study four different relaxations of the model First, we show that if a polynomial upper bound on the network size is known, Second if we assume sleeping nodes are awoken by neighboring beeps Third, if in addition to this wakeup assumption we allow sender-side collision detection, that is beeping nodes can distinguish whether at least one neighboring node is beeping concurrently or not, Finally if instead we endow nodes with synchronous clocks.
2K_test_1187	This paper studies attackers with control objectives and explicit detection constraints against cyber-physical systems, that gives the optimal sequence of attacks. And demonstrate our attack strategy. The cyber-physical system is equipped with a Kalman filter and an attack detector that uses the innovations process of the Kalman filter, The attacker performs an integrity attack on the actuators and sensors of the system with the aim of moving the system to a target state under the constraint that the probability of him or her being detected is equal to the false alarm probability of the attack detector, We formulate and solve a constrained optimization problem. In a numerical example.
2K_test_1188	Having a shared and accurate sense of time is critical to distributed Cyber-Physical Systems ( CPS ) and the Internet of Things ( IoT ), Thanks to decades of research in clock technologies and synchronization protocols, it is now possible to measure and synchronize time across distributed systems with unprecedented accuracy However, applications have not benefited to the same extent due to limitations of the system services that help manage time, and hardware-OS and OS-application interfaces through which timing information flows to the application. Due to the importance of time awareness in a broad range of emerging applications, running on commodity platforms and operating systems, it is imperative to rethink how time is handled across the system stack, to easily write applications whose activities are choreographed across time and space. Results from its evaluation are also presented. We advocate the adoption of a holistic notion of Quality of Time ( QoT ) that captures metrics such as resolution, Building on this notion we propose an architecture in which the local perception of time is a controllable operating system primitive with observable uncertainty, and where time synchronization balances applications ' timing demands with system resources such as energy and bandwidth Our architecture features an expressive application programming interface that is centered around the abstraction of a timeline a virtual temporal coordinate frame that is defined by an application to provide its components with a shared sense of time, with a desired accuracy and resolution, The timeline abstraction enables developers. Leveraging open source hardware and software components, we have implemented an initial Linux realization of the proposed timeline-driven QoT stack on a standard embedded computing platform.
2K_test_1189	Indicating potentials of the grounded modeling for semantic extraction and language understanding applications. Topic models represent latent topics as probability distributions over words which can be hard to interpret due to the lack of grounded semantics, to infer both hidden topics and entities from text corpora. Show significant superiority of our approach in topic perplexity and key entity identification. In this paper we propose a structured topic representation based on an entity taxonomy from a knowledge base A probabilistic model is developed Each topic is equipped with a random walk over the entity hierarchy to extract semantically grounded and coherent themes, Accurate entity modeling is achieved by leveraging rich textual features from the knowledge base.
2K_test_1190	Formative assessments allow learners to quickly identify knowledge gaps Our results suggest Questimator may be useful for assessing learning in topics for which there is not an existing quiz. In traditional educational settings, expert instructors can create assessments, but in informal learning environment, it is difficult for novice learners to self assess because they do n't know what they do n't know. We found that participants ' scores on Questimator-generated quizzes correlated well with their scores on existing online quizzes on topics ranging from philosophy to economics, Also Questimator generates questions with comparable discriminatory power as existing online quizzes. This paper introduces Questimator, an automated system that generates multiple-choice assessment questions for any topic contained within Wikipedia, Given a topic Questimator traverses the Wikipedia graph to find and rank related topics, and uses article text to form questions, answers and distractor options. In a study with 833 participants from Mechanical Turk.
2K_test_1191	Concurrent C0 is an imperative programming language in the C family with session-typed message-passing concurrency. The previously proposed semantics implements asynchronous ( non-blocking ) output ; we extend it here with non-blocking input. And show the results obtained, While the abstract measure of span always decreases ( or remains unchanged ), only a few of the examples reap a practical benefit. A key idea is to postpone message reception as much as possible by interpreting receive commands as a request for a message, We implemented our ideas as a translation from a blocking intermediate language to a non-blocking language. Finally we evaluated our techniques with several benchmark programs.
2K_test_1192	A type-safe C-like language. That outperforms traditional message passing techniques. We describe Concurrent C0 with contracts and session-typed communication over channels, Concurrent C0 supports an operation called forwarding which allows channels to be combined in a well-defined way The language 's type system enables elegant expression of session types and message-passing concurrent programs. We provide a Go-based implementation with language based optimizations.
2K_test_1193	Matrix-parametrized models ( MPMs ) are widely used in machine learning ( ML ) applications. In large-scale ML problems, the parameter matrix of a MPM can grow at an unexpected rate, resulting in high communication and parameter synchronization costs, To address this issue. To show that SFB guarantees convergence of algorithms ( under full broadcasting ) without requiring a centralized synchronization mechanism, corroborate SFB 's efficiency. We offer two contributions : first, we develop a computation model for a large family of MPMs, which share the following property : the parameter update computed on each data sample is a rank-1 matrix, the outer product of two `` sufficient factors '' ( SFs ) Second, we implement a decentralized, peer-to-peer system Sufficient Factor Broadcasting ( SFB ), which broadcasts the SFs among worker machines, and reconstructs the update matrices locally at each worker, SFB takes advantage of small rank-1 matrix updates and efficient partial broadcasting strategies to dramatically improve communication efficiency We propose a graph optimization based partial broadcasting scheme, which minimizes the delay of information dissemination under the constraint that each machine only communicates with a subset rather than all of machines. Furthermore we provide theoretical analysis Experiments on four MPMs.
2K_test_1194	We explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational Bayes ( SGVB ) with global model parameters inference of more flexibly parameterized posteriors often leading to better generalization. Regular SGVB estimators rely on sampling of parameters once per minibatch of data, and have variance that is constant w, The efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch, Such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence, We find an important connection with regularization by dropout : the original Gaussian dropout objective corresponds to SGVB with local noise, a scale-invariant prior and proportionally fixed posterior variance, Our method allows ; specifically, we propose \emph { variational dropout }, a generalization of Gaussian dropout, but with a more flexibly parameterized posterior. The method is demonstrated through several experiments.
2K_test_1195	A well-established approach -- which we refer to as implicit utilitarian voting -- assumes that voters have latent utility functions that induce the reported rankings, and seeks voting rules that approximately maximize utilitarian social welfare, Our methods underlie the design and implementation of an upcoming social choice website. How should one aggregate ordinal preferences expressed by voters into a measurably superior social choice ?. Results show that regret-based rules are more compelling than distortion-based rules, leading us to focus on developing a scalable implementation for the optimal ( deterministic ) regret-based rule. We extend this approach to the design of rules that select a subset of alternatives, We derive analytical bounds on the performance of optimal ( deterministic as well as randomized ) rules in terms of two measures.
2K_test_1196	The GFT is the mapping from the signal set into its representation by a direct sum of irreducible shift invariant subspaces : 1 ) this decomposition may not be unique ; and 2 ) there is freedom in the choice of basis for each component subspace, These issues are particularly relevant when the graph shift has repeated eigenvalues as is the case in many real-world applications ; by ignoring them, there is no way of knowing if different researchers are using the same definition of the GFT and whether their results are comparable or not. This paper considers the definition of the graph Fourier transform ( GFT ) and of the spectral decomposition of graph signals, Current literature does not address the lack of unicity of the GFT, The paper presents how to resolve the above degrees of freedom. We develop a quasi -coordinate free definition of the GFT and graph spectral decomposition of graph signals that we implement through oblique spectral projectors, We present properties of the GFT and of the spectral projectors and discuss a generalized Parseval 's inequality. For a large real-world urban traffic dataset is provided.
2K_test_1197	Optical music recognition ( OMR ) is the task of recognizing images of musical scores. For the first steps of optical music recognition. In this paper improved algorithms were developed, which facilitated bulk annotation of scanned scores for use in an interactive score display system, Creating an initial annotation by OMR and verifying by hand substantially reduced the manual effort required to process scanned scores to be used in a live performance setting.
2K_test_1198	Computer music systems can interact with humans at different levels, including scores phrases notes, However most current systems lack basic musicianship skills. As a consequence the results of human-computer interaction are often far less musical than the interaction between human musicians, In this paper we explore the possibility of learning some basic music performance skills from rehearsal data, In particular we consider the piano duet scenario where two musicians expressively interact with each other, Our work extends previous automatic accompaniment systems. And claim that a more human-like interaction is achieved. We have built an artificial pianist that can automatically improve its ability to sense and coordinate with a human pianist, learning from rehearsal experience, We describe different machine learning algorithms to learn musicianship for duet interaction. Explore the properties of the learned models, such as dominant features, limits of validity and minimal training size.
2K_test_1199	Processes such as disease propagation and information diffusion often spread over some latent network structure which must be learned from observation. Given a set of unlabeled training examples representing occurrences of an event type of interest ( e, a disease outbreak ), our goal is to learn a graph structure that can be used to accurately detect future events of that type, for learning graph structure from unlabeled data. We show that our method learns a structure similar to the true underlying graph, but enables faster and more accurate detection. Motivated by new theoretical results on the consistency of constrained and unconstrained subset scans, we propose a novel framework by comparing the most anomalous subsets detected with and without the graph constraints, Our framework uses the mean normalized log-likelihood ratio score to measure the quality of a graph structure, and efficiently searches for the highest-scoring graph structure. Using simulated disease outbreaks injected into real-world Emergency Department data from Allegheny County.
2K_test_1200	For the graph Fourier transform of a graph signal. Our results show that identical highly expressed geolocations can be identified with the inexact method and the method based on eigenvector projections, while reducing computation time by a factor of 26, 000 and reducing energy dispersal among the spectral components corresponding to the multiple zero eigenvalue. We propose an inexact method as defined by the signal decomposition over the Jordan subspaces of the graph adjacency matrix, This method projects the signal over the generalized eigenspaces of the adjacency matrix, which accelerates the transform computation over large, sparse and directed adjacency matrices The trade-off between execution time and fidelity to the original graph structure is discussed, In addition properties such as a generalized Parseval 's identity and total variation ordering of the generalized eigenspaces are discussed. The method is applied to 2010-2013 NYC taxi trip data to identify traffic hotspots on the Manhattan grid.
2K_test_1201	Design of filters for graph signal processing benefits from knowledge of the spectral decomposition of matrices that encode graphs, such as the adjacency matrix and the Laplacian matrix, used to define the shift operator, For shift matrices with real eigenvalues, which arise for symmetric graphs, the empirical spectral distribution captures the eigenvalue locations, Under realistic circumstances stochastic influences often affect the network structure and, consequently the shift matrix empirical spectral distribution, Nevertheless deterministic functions may often be found to approximate the asymptotic behavior of empirical spectral distributions of random matrices. To derive such deterministic equivalent distributions for the empirical spectral distributions of random graphs formed by structured, non-uniform percolation of a D-dimensional lattice supergraph. Demonstrate the results for sample parameters. This paper uses stochastic canonical equation methods developed by Girko.
2K_test_1203	Robust principal component analysis PCA is one of the most important dimension-reduction techniques for handling high-dimensional data with outliers. However most of the existing robust PCA presupposes that the mean of the data is zero and incorrectly utilizes the average of data as the optimal mean of robust PCA In fact, this assumption holds only for the squared -norm-based traditional PCA. Illustrate the effectiveness and superiority of the proposed method. In this letter we equivalently reformulate the objective of conventional PCA and learn the optimal projection directions by maximizing the sum of projected difference between each pair of instances based on -norm, The proposed method is robust to outliers and also invariant to rotation, More important the reformulated objective not only automatically avoids the calculation of optimal mean and makes the assumption of centered data unnecessary, but also theoretically connects to the minimization of reconstruction error, To solve the proposed nonsmooth problem, we exploit an efficient optimization algorithm to soften the contributions from outliers by reweighting each data point iteratively, We theoretically analyze the convergence and computational complexity of the proposed algorithm. Extensive experimental results on several benchmark data sets.
2K_test_1204	Biological adaptation is a powerful mechanism that makes many disorders hard to combat. In this paper we study steering such adaptation through sequential planning to compute a treatment plan. We show that for the development of regulatory cells, sequential plans yield significantly higher utility than the best static therapy In contrast, for developing effector cells, we find that ( at least for the given simulator, objective function action possibilities, and measurement possibilities ) single-step plans suffice for optimal treatment. We propose a general approach where we leverage Monte Carlo tree search and the biological entity is modeled by a black-box simulator that the planner calls during planning We show that the framework can be used to steer a biological entity modeled via a complex signaling pathway network that has numerous feedback loops that operate at different rates and have hard-to-understand aggregate behavior We apply the framework to steering the adaptation of a patient 's immune system, In particular we apply it to a leading T cell simulator ( available in the biological modeling package BioNetGen. We run experiments with two alternate goals : developing regulatory T cells or developing effector T cells, The former is important for preventing autoimmune diseases while the latter is associated with better survival rates in cancer patients We are especially interested in the effect of sequential plans, an approach that has not been explored extensively in the biological literature.
2K_test_1205	As publishers gather more information about their users, they can use that information to enable advertisers to create increasingly targeted campaigns, This enables better usage of advertising inventory. However it also dramatically increases the complexity that the publisher faces when optimizing campaign admission decisions and inventory allocation to campaigns, for abstracting fine-grained audience segments into coarser abstract segments that are not too numerous for use in such optimization. It yields two orders of magnitude improvement in run time and significant improvement in abstraction quality These benefits hold both for guaranteed and non-guaranteed campaigns. We develop an optimal anytime algorithm The performance stems from three improvements : 1 ) a quadratic-time ( as opposed to doubly exponential or heuristic ) algorithm for finding an optimal split of an abstract segment, 2 ) a better scoring function for evaluating splits, and 3 ) splitting time lossily like any other targeting attribute ( instead of losslessly segmenting time first ). Compared to the segment abstraction algorithm by Walsh et al, [ 2010 ] for the same problem.
2K_test_1206	Learning detectors that can recognize concepts, such as people actions, in video content is an interesting but challenging problem, To the best of our knowledge, WELL achieves by far the best reported performance on these two webly-labeled big video datasets. In this paper we study the problem of automatically learning detectors from the big video data on the web without any additional manual annotations, The contextual information available on the web provides noisy labels to the video content, To leverage the noisy web labels. The efficacy and the scalability of WELL have been extensively demonstrated Experimental results show that WELL significantly outperforms the state-of-the-art methods. We propose a novel method called WEbly-Labeled Learning ( WELL ), It is established on two theories called curriculum learning and self-paced learning and exhibits useful properties that can be theoretically verified, We provide compelling insights on the latent non-convex robust loss that is being minimized on the noisy data, In addition we propose two novel techniques that not only enable WELL to be applied to big data but also lead to more accurate results. On two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set.
2K_test_1207	State-of-the-art applications of Stackelberg security games -- including wildlife protection -- offer a wealth of data, which can be used to learn the behavior of the adversary. But existing approaches either make strong assumptions about the structure of the data, or gather new data through online algorithms that are likely to play severely suboptimal strategies, to learning the parameters of the behavioral model of a bounded rational attacker ( thereby pinpointing a near optimal strategy ). We also validate our approach. We develop a new approach, by observing how the attacker responds to only three defender strategies. Using experiments on real and synthetic data.
2K_test_1208	Robust principal component analysis ( PCA ) is one of the most important dimension reduction techniques to handle high-dimensional data with outliers. However the existing robust PCA presupposes that the mean of the data is zero and incorrectly utilizes the Euclidean distance based optimal mean for robust PCA with l1-norm, Some studies consider this issue and integrate the estimation of the optimal mean into the dimension reduction objective, which leads to expensive computation. Some experimental results demonstrate the effectiveness and superiority of the proposed approaches on image reconstruction and recognition. In this paper we equivalently reformulate the maximization of variances for robust PCA, such that the optimal projection directions are learned by maximizing the sum of the projected difference between each pair of instances, rather than the difference between each instance and the mean of the Based on this reformulation, we propose a novel robust PCA to automatically avoid the calculation of the optimal mean based on l1-norm distance This strategy also makes the assumption of centered data unnecessary, Additionally we intuitively extend the proposed robust PCA to its 2D version for image recognition Efficient non-greedy algorithms are exploited to solve the proposed robust PCA and 2D robust PCA with fast convergence and low computational complexity. On benchmark data sets.
2K_test_1211	End-to-end learning of CNN/RNNs is currently not possible for whole videos due to GPU memory limitations and so a common practice is to use sampled frames as inputs along with the video labels as supervision. We investigate the problem of representing an entire video using CNN features for human action recognition, However the global video labels might not be suitable for all of the temporally local samples as the videos often contain content besides the action of interest. Show that a simple maximum pooling on the sparsely sampled local features leads to significant performance improvement. We therefore propose to instead treat the deep networks trained on local inputs as local feature extractors, The local features are then aggregated to form global features which are used to assign video-level labels through a second classification stage, We investigate a number of design choices for this local feature approach. Experimental results on the HMDB51 and UCF101 datasets.
2K_test_1212	Hybrid systems verification is quite important for developing correct controllers for physical systems, but is also challenging, Verification engineers thus need to be empowered with ways of guiding hybrid systems verification while receiving as much help from automation as possible, We also share thoughts how the success of such a user interface design could be evaluated and anecdotal observations about it. Due to undecidability verification tools need sufficient means for intervening during the verification and need to allow verification engineers to provide system design insights, We discuss how they make it easier to prove hybrid systems as well as help learn how to conduct proofs in the first place. Unsurprisingly the most difficult user interface challenges come from the desire to integrate automation and human guidance. This paper presents the design ideas behind the user interface for the hybrid systems theorem prover KeYmaera X.
2K_test_1213	In human-robot teams humans often start with an inaccurate model of the robot capabilities, As they interact with the robot, they infer the robot 's capabilities and partially adapt to the robot, they might change their actions based on the observed outcomes and the robot 's actions, without replicating the robot 's policy. Of human partial adaptation to the robot capturing the evolution of their expectations of the robot 's capabilities. We prove that the optimal policy can be computed efficiently, that the proposed model significantly improves human-robot team performance. We present a game-theoretic model where the human responds to the robot 's actions by maximizing a reward function that changes stochastically over time, The robot can then use this model to decide optimally between taking actions that reveal its capabilities to the human and taking the best action given the information that the human currently has. Under certain observability assumptions We demonstrate through a human subject experiment compared to policies that assume complete adaptation of the human to the robot.
2K_test_1214	Biological systems are increasingly being studied by high throughput profiling of molecular data over time, TPS can thus serve as a key design strategy for high throughput time series experiments. Determining the set of time points to sample in studies that profile several different types of molecular data is still challenging, that solves this combinatorial problem. The points selected by TPS can be used to reconstruct an accurate representation for the expression values of the non selected points. Here we present the Time Point Selection ( TPS ) method in a principled and practical way TPS utilizes expression data from a small set of genes sampled at a high rate, Further even though the selection is only based on gene expression, these points are also appropriate for representing a much larger set of protein, miRNA and DNA methylation changes over time. As we show by applying TPS to study mouse lung development.
2K_test_1215	Identifying a masked suspect is one of the toughest challenges in biometrics that exist, This is an important problem faced in many law-enforcement applications on almost a daily basis. In such situations investigators often only have access to the periocular region of a suspect 's face and, unfortunately conventional commercial matchers are unable to process these images in such a way that the suspect can be identified, to hallucinate a full frontal face given only a periocular region of a face. Herein a practical method is presented, This approach reconstructs the entire frontal face based on an image of an individual 's periocular region, By using an approach based on a modified sparsifying dictionary learning algorithm, faces can be effectively reconstructed more accurately than with conventional methods, Further various methods presented herein are open set, and thus can reconstruct faces even if the algorithms are not specifically trained using those faces.
2K_test_1216	Complex event detection has been progressively researched in recent years for the broad interest of video indexing and retrieval, To fulfill the purpose of event detection, one needs to train a classifier using both positive and negative examples, Current classifier training treats the negative videos as equally negative. However we notice that many negative videos resemble the positive videos in different degrees, Intuitively we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning. Have validated the efficacy of our proposed approach. We use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event, Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation, The resulting fine-grained labels may be not optimal to capture the discriminative cues from the negative videos, Hence we propose to jointly optimize the fine-grained labels with the classifier learning, which brings mutual reciprocality, Meanwhile the labels of positive examples are supposed to remain unchanged, We thus additionally introduce a constraint for this purpose On the other hand, the state-of-the-art deep convolutional neural network features are leveraged in our approach for event detection to further boost the performance. Extensive experiments on the challenging TRECVID MED 2014 dataset.
2K_test_1217	Generalized canonical correlation analysis ( GCCA ) aims at extracting common structure from multiple 'views ', high-dimensional matrices representing the same objects in different feature domains an extension of classical two-view CCA. Existing ( G ) CCA algorithms have serious scalability issues, since they involve square root factorization of the correlation matrices of the views, The memory and computational complexity associated with this step grow as a quadratic and cubic function of the problem dimension ( the number of samples / features ), To circumvent such difficulties. Further reduce the runtime significantly ( by 30 % ) if multiple cores are available, to showcase the effectiveness of the proposed algorithms. We propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements, respectively Consequently the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100, 000 while the current approaches can only handle thousands of features / samples, Our second contribution is a distributed algorithm for GCCA, which computes the canonical components of different views in parallel and thus can. In experiments Judiciously designed synthetic and real-data experiments using a multilingual dataset are employed.
2K_test_1218	Display appropriation provides a means by which mobile users can cyber-forage local display hardware to provide them with access to a high-quality output device. However displays are of little use without applications to drive them and yet the nature of application support has been largely ignored in the field with the prevailing assumption being that applications will be cloud-based and Web-centric, to execute high-performance applications that would not be possible using purely Web-centric technologies. In this demonstration we show a system that presents an alternative vision in which users are able to cyber-forage for both display and compute resources in their local area enabling them The demonstration leverages a cohesive suite of existing systems, cloudlets Internet Suspend/Resume ( ISR ), Yarely and Tacita to deliver this vision.
2K_test_1219	We seek to extract and explore statistics that characterize New York City traffic flows based on 700 million taxi trips in the 20102013 New York City taxi data, for intensive computation : for estimating taxi trajectories and job parallelization and scheduling. That reduces execution time from 3, 000 days to less than a day. This paper presents a two-part solution space and time design considerations with Dijkstra 's algorithm, with HTCondor Our contribution is to present a solution. With detailed analysis of the necessary design decisions.
2K_test_1220	In social voting Web sites, how do the user actions up-votes, down-votes and comments evolve over time ? Are there relationships between votes and comments ? What is normal and what is suspicious ? These are the questions we focus on, that models the coevolution of user activities. Our first contribution is two discoveries : ( i ) the number of comments grows as a power-law on the number of votes and ( ii ) the time between a submission creation and a user 's reaction obeys a log-logistic distribution, VnC outperformed state-of-the-art baselines on accuracy Additionally, we illustrate VnC usefulness for forecasting and outlier detection. Based on these patterns, we propose VnC ( Vote-and-Comment ), a parsimonious but accurate and scalable model. We analyzed over 20, 000 submissions corresponding to more than 100 million user interactions from three social voting Web sites : Reddit, Imgur and Digg In our experiments on real data.
2K_test_1221	Given a heterogeneous network, with nodes of different types - e, products users and sellers from an online recommendation site like Amazon - and labels for a few nodes ( 'honest ', 'suspicious ' etc ), can we find a closed formula for Belief Propagation ( BP ), exact or approximate ? Can we say whether it will converge ?. BP traditionally an inference algorithm for graphical models, exploits so-called `` network effects '' to perform graph classification tasks when labels for a subset of nodes are provided ; and it has been successful in numerous settings like fraudulent entity detection in online retailers and classification in social networks, However it does not have a closed-form nor does it provide convergence guarantees in general, to perform fast BP on undirected heterogeneous graphs. ( 4 ) Effectiveness ZooBP identifies fraudulent users with a near-perfect precision of 92, 3 % over the top 300 results. We propose ZooBP a method with provable convergence guarantees ZooBP has the following advantages : ( 1 ) Generality : It works on heterogeneous graphs with multiple types of nodes and edges ; ( 2 ) Closed-form solution : ZooBP gives a closed-form solution as well as convergence guarantees ; ( 3 ) Scalability : ZooBP is linear on the graph size and is up to 600 faster than BP, running on graphs with 3, 3 million edges in a few seconds. Applied on real data ( a Flipkart e-commerce network with users, products and sellers ).
2K_test_1222	Many theories have emerged which investigate how in- variance is generated in hierarchical networks through sim- ple schemes such as max and mean pooling, The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations. We con- jecture that hierarchically building selective invariance ( i, carefully choosing the range of the transformation to be in- variant to at each layer of a hierarchical network ) is im- portant for pattern recognition to find linear pooling weights within networks. We utilize a novel pooling layer called adaptive pooling These networks with the learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks In- terestingly, adaptive pooling can converge to mean pooling ( when initialized with random pooling weights ), find more general linear pooling schemes or even decide not to pool at all. We illustrate the general notion of selective invari- ance through object categorization experiments on large- scale datasets such as SVHN and ILSVRC 2012.
2K_test_1223	A k-core is the maximal subgraph where all vertices have degree at least k, This concept has been applied to such diverse areas as hierarchical structure analysis, graph visualization and graph clustering. How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we use them for algorithm design and applications ? Here, we explore pervasive patterns that are related to k-cores and emerging in graphs from several diverse domains. Our discoveries are as follows : ( 1 ) Mirror Pattern : coreness of vertices ( i, maximum k such that each vertex belongs to the k-core ) is strongly correlated to their degree, ( 2 ) Core-Triangle Pattern : degeneracy of a graph ( i, maximum k such that the k-core exists in the graph ) obeys a 3-to-1 power law with respect to the count of triangles, ( 3 ) Structured Core Pattern : degeneracy-cores are not cliques but have non-trivial structures such as core-periphery and communities. Our algorithmic contributions show the usefulness of these patterns, ( 1 ) Core-A, which measures the deviation from Mirror Pattern, successfully finds anomalies in real-world graphs complementing densest-subgraph based anomaly detection methods, ( 2 ) Core-D, a single-pass streaming algorithm based on Core-Triangle Pattern, accurately estimates the degeneracy of billion-scale graphs up to 7 faster than a recent multipass algorithm, ( 3 ) Core-S, inspired by Structured Core Pattern, identifies influential spreaders up to 17 faster than top competitors with comparable accuracy.
2K_test_1224	For determining a pitch and yaw of an elongated interface object relative to a proximity sensitive surface. Methods and apparatuses are provided In one aspect, a proximity image is received having proximity image data from which it can be determined which areas of the proximity sensitive surface sensed the elongated interface object during a period of time, A proximity blob is identified in the proximity image and the proximity image is transformed using a plurality of different transformations to obtain a plurality of differently transformed proximity images, A plurality of features is determined for the identified blob in the transformed proximity images and the pitch of the elongated interface object relative to the proximity sensitive surface is determined based upon the determined features and a multi dimensional heuristic regression model of the proximity sensitive surface ; and a yaw is determined based upon the pitch.
2K_test_1225	There are many cases where collections of subgraphs may be contrasted against each other, For example they may be as- signed ground truth labels ( spam/not-spam ), or it may be desired to directly compare the biological networks of different species or compound networks of different chemicals. Given a set of attributed subgraphs known to be from different classes, how can we discover their differences ? In this work we introduce the problem of characterizing the differences between attributed subgraphs that belong to different classes. Show findings that agree with human intuition on datasets from Amazon co-purchases, Congressional bill sponsorships and DBLP co-authorships, We also show that our approach of characterizing subgraphs is better suited for sense-making than discriminating classification approaches. We define this characterization problem as one of partitioning the attributes into as many groups as the number of classes, while maximizing the total attributed quality score of all the given subgraphs We show that our attribute-to-class assignment problem is NP-hard and an optimal ( 1 -- 1/e ) -approximation algorithm exists, We also propose two different faster heuristics that are linear-time in the number of attributes and subgraphs Unlike previous work where only attributes were taken into account for characterization, here we exploit both attributes and social ties ( i. Through extensive experiments we compare our proposed algorithms.
2K_test_1226	The ubiquity of mobile devices and cloud services has led to an unprecedented growth of online personal photo and video collections, Due to the scarcity of personal media search log data, research to date has mainly focused on searching images and videos on the web, To the best of our knowledge, this paper is the first The insightful observations will not only be instrumental in guiding future personal media search methods, but also benefit related tasks such as personal photo browsing and recommendation. However in order to manage the exploding amount of personal photos and videos, we raise a fundamental question : what are the differences and similarities when users search their own photos versus the photos on the web ? to study personal media search To bridge the gap. Our findings suggest there is a significant gap between personal queries and automatically detected concepts, which is responsible for the low accuracy of many personal media search queries verify the efficacy of the proposed method in improving personal media search, where the proposed method consistently outperforms baseline methods. We propose the deep query understanding model to learn a mapping from the personal queries to the concepts in the clicked photos. Using large-scale real-world search logs, We analyze different types of search sessions mined from Flickr search logs and discover a number of interesting characteristics of personal media search in terms of information needs and click behaviors.
2K_test_1229	With the ubiquitous development of mobile technologies, many cities today have installed mobile-enabled bike sharing systems - both publicly and privately owned - in an effort to nudge dwellers towards a more sustainable mode of transportation However, there is little evidence - apart from anecdote stories - for the success of these systems, This can have significant implications that shared bike systems can shift transportation modes, which consequently can have rippling effects for the economy and environment. In this work we are focusing on analyzing the impact of a shared bike system on the parking demand we quantify the impact of the bike stations on the parking demand around them. Our findings provide evidence that even when controlling for the lost parking space ( used to build the parking stations ) the parking demand in the nearby areas was reduced by approximately 2 %, In particular our follow-up analyses indicate that the new bike share system could lead to a monthly reduction of 0, 82 metric tones CO2 emissions per square mile, or approximately 4 381 metric tones of CO2 in the metro area of Pittsburgh. And using the difference-in-differences framework. The latter can be thought of as a lower bound for the car trips generated towards a specific area and has implications towards potential substitution effects between driving and biking, In particular we use data from Healthy Ride, the newly installed shared bike system in the city of Pittsburgh, combined with data we obtained from the Pittsburgh Parking Authority.
2K_test_1230	Multi-aspect data appear frequently in many web-related applications, For example product reviews are quadruplets of ( user, product keyword timestamp ), How can we analyze such web-scale multi-aspect data ? Can we analyze them on an off-the-shelf workstation with limited amount of memory ? Tucker decomposition has been widely used for discovering patterns in relationships among entities in multi-aspect data, naturally expressed as high-order tensors. However existing algorithms for Tucker decomposition have limited scalability, and especially fail to decompose high-order tensors since they explicitly materialize intermediate data, whose size rapidly grows as the order increases ( 4 ), We call this problem M-Bottleneck ( `` Materialization Bottleneck '' ), To avoid M-Bottleneck to minimize the materialized intermediate data. S-HOT showed better scalability not only with the order but also with the dimensionality and the rank than baseline methods In particular, S-HOT decomposed tensors 1000 larger than baseline methods in terms dimensionality S- HOT also successfully analyzed real-world tensors that are both large-scale and high-order on an off-the-shelf workstation with limited amount of memory, while baseline methods failed The source code of S-HOT is publicly available at http : //dm, kr/shot to encourage reproducibility. We propose S-HOT a scalable high-order tucker decomposition method that employs the on-the-fly computation Moreover, S-HOT is designed for handling disk-resident tensors, too large to fit in memory, without loading them all in memory at once. We provide theoretical analysis on the amount of memory space and the number of scans of data required by S-HOT.
2K_test_1231	How can we detect fraudulent lockstep behavior in large-scale multi-aspect data ( i, tensors ) ? Can we detect it when data are too large to fit in memory or even on a disk ? Past studies have shown that dense blocks in real-world tensors ( e, social media Wikipedia TCP dumps, ) signal anomalous or fraudulent behavior such as retweet boosting, bot activities and network attacks, Thus various approaches including tensor decomposition and search, have been used for rapid and accurate dense-block detection in tensors. However all such methods have low accuracy, or assume that tensors are small enough to fit in main memory, which is not true in many real-world applications such as social media and web, To overcome these limitations. D-Cube is ( 1 ) Memory Efficient : requires up to 1, 600 times less memory and handles 1, 000 times larger data ( 2, 6TB ) ( 2 ) Fast : up to 5 times faster due to its near-linear scalability with all aspects of data, ( 3 ) Provably Accurate : gives a guarantee on the densities of the blocks it finds, and ( 4 ) Effective : successfully spotted network attacks from TCP dumps and synchronized behavior in rating data with the highest accuracy. We propose D-Cube a disk-based dense-block detection method, which also can be run in a distributed manner across multiple machines. Compared with state-of-the-art methods.
2K_test_1232	Supervised CNNs due to their immense learning capacity, have shown superior performance on a range of computer vision problems including optical flow prediction. We study the unsupervised learning of CNNs for optical flow estimation using proxy ground truth data, They however require the ground truth flow which is usually not accessible except on limited synthetic data, Without the guidance of ground truth optical flow, unsupervised CNNs often perform worse as they are naturally ill-conditioned to guide the CNN learning. Our guided learning approach is competitive with or superior to state-of-the-art approaches. We therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used The models are further refined in an unsupervised fashion using an image reconstruction loss yet is completely unsupervised and can run in real time. On three standard benchmark datasets.
2K_test_1233	This generalizes a prior decomposition result for an M/M/k/staggeredsetup. We consider the M/G/k/staggered-setup, where idle servers are turned off to save cost, necessitating a setup time for turning a server back on ; however, at most one server may be in setup mode at any time. We show the response time of an M/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/k and the setup time, where the approximation is nearly exact. That for exponentially distributed setup times.
2K_test_1234	A group of agents makes linear measurements of the unknown parameter, The agent measurements are locally unobservable, and the agents exchange information over a communication network in order to compute an estimate, A subset of the agents is adversarial and exchanges false information in order to prevent the remaining, normally-behaving agents from correctly estimating the parameter. This paper studies the resilient distributed estimation of an unknown vector parameter belonging to a compact set, that allows the normally-behaving agents to perform parameter estimation and adversary detection. Finally we provide examples of the performance of the FRDE algorithm. We present Flag Raising Distributed Estimation ( FRDE ) algorithm The FRDE algorithm is a consensus+innovations type estimator in which agents combine estimates of neighboring agents ( consensus ) with local sensing information ( innovations ), Under the FRDE algorithm, global observability for connected normally-behaving agents is a necessary and sufficient condition to either correctly estimate the parameter or correctly detect the presence of an adversary, If FRDE detects an adversary, we show how existing methods for attack identification in cyber-physical systems can be used to identify the adversarial agents.
2K_test_1235	Unmanned aerial vehicles ( UAVs ) recently enabled a myriad of new applications spanning domains from personal entertainment to surveillance. To provide extended reach to an online video monitoring system for inspection of industrial installations. We show that this platform is not omnidirectional in the horizontal plane and that UAV-to-UAV communication ceases around 75m the paper derives the optimal number of hops that maximize the end-to-end throughput, as well as the corresponding hop lengths, transmitting payloads up to 200m ( over 802, 11g at 54MBps ). In this paper we focus on using several small UAVs collaboratively We make use of 802, 11 radios on low-cost commercial-off-the-shelf UAVs, set up a time-division multiple access overlay protocol to avoid mutual interference, and enable high channel utilization in multihop networks In particular, we provide a model for the quality of the UAV-to-UAV link, in terms of packet delivery ratio as a function of distance, packet size and orientation, based on an extensive measurement campaign. Concerning the operation in a multihop mode to allow extending the network We validate our mathematical model with extensive experimental measurements.
2K_test_1236	Summary Successful application of two-photon imaging withgenetic tools in awake macaque monkeys will enable fundamental advances in our understanding of higher cognitive function at the level of molecular and neuronal circuits, By providing two-photon imaging access to cortical neuronal populations at single-cell or single dendritic spine resolution in awake monkeys, the techniques reported can help bridge the use of modern genetic and molecular tools and the study of higher cognitive function. For long-term two-photon imaging in awake macaque monkeys. Confirm that fluorescence activity is linearly proportional to neuronal spiking activity across a wide range of firing rates ( 10Hz to 150Hz ). Here we report techniques Using genetically encoded indicators including GCaMP5 and GCaMP6s delivered by AAV2/1 into the visual cortex, we demonstrate that high-quality two-photon imaging of large neuronal populations can be achieved and maintained in awake monkeys for months. Simultaneous intracellular recording and two-photon calcium imaging.
2K_test_1237	Our work provides a solid step toward a systematic and quantitative wall-centric profiling of Facebook user activity. How do people interact with their Facebook wall ? At a high level, this question captures the essence of our work, While most prior efforts focus on Twitter, the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study. Our key results can be summarized in the following points, First we find that many wall activities, including number of posts, number of likes number of posts of type photo, can be described by the PowerWall distribution, What is more surprising is that most of these distributions have similar slope, with a value close to 1 ! Second, we show how our patterns and metrics can help us spot surprising behaviors and anomalies, For example we find a user posting every two days, exactly the same count of posts ; another user posting at midnight, with no other activity before or after. In this work we model Facebook user behavior : We propose PowerWall, a lesser known heavy-tailed distribution to fit our data. We analyze the wall activities of users focusing on identifying common patterns and surprising phenomena, We conduct an extensive study of roughly 7k users over 3 years during 4-month intervals each year.
2K_test_1238	To infer the histories of population admixture, one important challenge with methods based on the admixture linkage disequilibrium ( ALD ) is to get rid of the effect of source LD ( SLD ) which is directly inherited from source populations, In previous methods only the decay curve of weighted LD between pairs of sites whose genetic distance were larger than a certain starting distance was fitted by single or multiple exponential functions, for the inference of recent single- or multiple-wave of admixture. However the effect of SLD has not been well defined and no tool has been developed to estimate the effect of SLD on weighted LD decay, to study the weighted SLD and weighted LD, to infer Multiple-wave Admixture. We showed that iMAAPs is a considerable improvement over other current methods and further facilitates the inference of the histories of complex population admixtures. We further developed a method, iMAAPs by fitting ALD using Polynomial spectrum. In this study we defined the SLD in the formularized weighted LD statistic under the two-way admixture model, and proposed polynomial spectrum ( p-spectrum ) We also found reference populations could be used to reduce the SLD in weighted LD statistic, We evaluated the performance of iMAAPs under various admixture models in simulated data and applied iMAAPs into analysis of genome-wide single nucleotide polymorphism data from the Human Genome Diversity Project ( HGDP ) and the HapMap Project.
2K_test_1239	To model the evolution of tie strength among interacting agents. Moreover the family of models adapts well to capture the phenomenon of emergence and downfall of leaders in social networks. We formulate a set of time-varying stochastic networked dynamical systems The dynamics of the strength of connections abide by local laws of reinforcement and penalization due to interactions among the agents, The proposed stochastic dynamical systems exhibit a strong-attractor as a certain subset of the set of binary matrices. As it will be illustrated via numerical simulations.
2K_test_1240	Abstract Rapid advances in high-throughput sequencing and a growing realization of the importance of evolutionary theory to cancer genomics have led to a proliferation of phylogenetic studies of tumour progression These studies have yielded not only new insights but also a plethora of experimental approaches, sometimes reaching conflicting or poorly supported conclusions, closing with a perspective on the prospects and broader implications of this field. Here we consider this body of work in light of the key computational principles underpinning phylogenetic inference, with the goal of providing practical guidance on the design and analysis of scientifically rigorous tumour phylogeny studies. We survey the range of methods and tools available to the researcher, their key applications and the various unsolved problems.
2K_test_1241	Sparse iterative methods in particular first-order methods, are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games, The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on. For solving extensive-form games for the strategy spaces of sequential games. We show that for the first time, the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm. We investigate the acceleration of first-order methods through better design of the dilated entropy function -- -a class of distance-generating functions related to the domains associated with the extensive-form games, By introducing a new weighting scheme for the dilated entropy function, we develop the first distance-generating function that only a logarithmic dependence on the branching factor of the player, This result improves the convergence rate of several first-order methods by a factor of ( b dd ), where b is the branching factor of the player, and d is the depth of the game tree, Thus far counterfactual regret minimization methods have been faster in practice, and more popular than first-order methods despite their theoretically inferior convergence rates. Using our new weighting scheme and practical tuning.
2K_test_1242	These graph-based problems are related to many real-world applications, such as localizing stimulus in brain connectivity networks, and mining traffic events in city street networks, where the key issue is to find the supports of localized activated patterns, Counterparts of these problems in classical signal/image processing, such as impulse detection and foreground detection, have been studied over the past few decades. Motivated by the need to extract meaning from large amounts of complex structured data, we consider three critical problems on graphs : localization, decomposition and dictionary learning of piecewise-constant signals, to model localized patterns For each of the three problems, localization decomposition and dictionary learning. The analysis validates the effectiveness of the approach and suggests that graph signal processing tools may aid in urban planning and traffic forecasting. We use piecewise-constant graph signals where each piece indicates a localized pattern that exhibits homogeneous internal behavior and the number of pieces indicates the number of localized patterns For such signals, we show that decomposition and dictionary learning are natural extensions of localization, the goal of which is not only to efficiently approximate graph signals, but also to accurately find supports of localized patterns, we propose a specific graph signal model, an optimization problem and a computationally efficient solver The proposed solvers directly find the supports of arbitrary localized activated patterns without. We then conduct an extensive empirical study to validate the proposed methods on both simulated and real data including the analysis of a large volume of spatio-temporal Manhattan urban data.
2K_test_1243	Video semantic recognition usually suffers from the curse of dimensionality and the absence of enough high-quality labeled instances, thus semisupervised feature selection gains increasing attentions for its efficiency and comprehensibility, Most of the previous methods assume that videos with close distance ( neighbors ) have similar labels and characterize the intrinsic local structure through a predetermined graph of both labeled and unlabeled data. However besides the parameter tuning problem underlying the construction of the graph, the affinity measurement in the original feature space usually suffers from the curse of dimensionality, Additionally the predetermined graph separates itself from the procedure of feature selection, which might lead to downgraded performance for video semantic recognition. Illustrate the effectiveness and superiority of the proposed approach on video semantic recognition related tasks. In this paper we exploit a novel semisupervised feature selection method from a new perspective, The primary assumption underlying our model is that the instances with similar labels should have a larger probability of being neighbors, Instead of using a predetermined similarity graph, we incorporate the exploration of the local structure into the procedure of joint feature selection so as to learn the optimal graph simultaneously, Moreover an adaptive loss function is exploited to measure the label fitness, which significantly enhances model 's robustness to videos with a small or substantial loss, We propose an efficient alternating optimization algorithm to solve the proposed challenging problem, together with analyses on its convergence and computational complexity in theory. Finally extensive experimental results on benchmark datasets.
2K_test_1244	: Cellular Electron CryoTomography ( CECT ) enables 3D visualization of cellular organization at near-native state and in sub-molecular resolution, making it a powerful tool for analyzing structures of macromolecular complexes and their spatial organizations inside single cells, However high degree of structural complexity together with practical imaging limitations make the systematic de novo discovery of structures within cells challenging, It would likely require averaging and classifying millions of subtomograms potentially containing hundreds of highly heterogeneous structural classes. Motivation Although it is no longer difficult to acquire CECT data containing such amount of subtomograms due to advances in data acquisition automation, existing computational approaches have very limited scalability or discrimination ability, making them incapable of processing such amount of data, To complement existing approaches. Results show that our new approach achieves significant improvements in both discrimination ability and scalability, More importantly our new approach is able to discover new structural classes and recover structures that do not exist in training data. In this paper we propose a new approach for subdividing subtomograms into smaller but relatively homogeneous subsets, The structures in these subsets can then be separately recovered using existing computation intensive methods, Our approach is based on supervised structural feature extraction using deep learning, in combination with unsupervised clustering and reference-free classification. Our experiments compared to existing unsupervised rotation invariant feature and pose-normalization based approaches.
2K_test_1245	The techniques developed in the paper for establishing weak convergence might be of independent interest. For observable macroscopic state variables of interacting particle systems ( e, voter and contact processes ) over fast time-varying sparse random networks of interactions.
2K_test_1246	For locating a mobile device. A method is disclosed Initially, a set of modulated ultrasound signals and a set of radio signals are separately broadcast from a group of transmitters, The ultrasound signals include at least one symbol configured for pulse compression, After the receipt of a demodulated ultrasound signal from a mobile device, wherein the demodulated ultrasound signal is derived from the modulated ultrasound signals, transmitter identifier and timing information are extracted from the demodulated ultrasound signal Timing information include, for example the arrival time of the demodulated ultrasound signal in relation to the start time of its transmission, After the locations of the transmitters have been ascertained from the transmitter identifier information, the location of the mobile device can be determined based on the timing information and the locations of the transmitters.
2K_test_1247	Researchers and educators have designed curricula and resources for introductory programming environments such as Scratch, App Inventor and Kodu to foster computational thinking in K-12. This paper is an empirical study of the effectiveness and usefulness of tiles and flashcards developed for Microsoft Kodu Game Lab to support students in learning how to program and develop games. We found that the students who used physical manipulatives performed well in rule construction, whereas the students who engaged more with the rule editor of the programming environment had better mental simulation of the rules and understanding of the concepts. In particular we investigated the impact of physical manipulatives on 3rd -- 5th grade students ' ability to understand, recognize construct and use game programming design patterns.
2K_test_1248	Our result essentially states that under an appropriate dynamics of the underlying network of contacts, the macroprocess ( Y N ( t ) ) becomes asymptotically ( in N ) Markov.
2K_test_1249	Abstract The heterogeneity-gap between different modalities brings a significant challenge to multimedia information retrieval, Some studies formalize the cross-modal retrieval tasks as a ranking problem and learn a shared multi-modal embedding space to measure the cross-modality similarity. However previous methods often establish the shared embedding space based on linear mapping functions which might not be sophisticated enough to reveal more complicated inter-modal correspondences, Additionally current studies assume that the rankings are of equal importance, and thus all rankings are used simultaneously, or a small number of rankings are selected randomly to train the embedding space at each iteration, Such strategies however always suffer from outliers as well as reduced generalization capability due to their lack of insightful understanding of procedure of human cognition, to rank and learn an optimal multi-modal embedding space based on non-linear mapping functions. Indicate that the proposed method achieves significant improvements over the state-of-the-arts in this literature. In this paper we involve the self-paced learning theory with diversity into the cross-modal learning This strategy enhances the models robustness to outliers and achieves better generalization via training the model gradually from easy rankings by diverse queries to more complex ones, An efficient alternative algorithm is exploited to solve the proposed challenging problem with fast convergence in practice. Extensive experimental results on several benchmark datasets.
2K_test_1250	We study the estimation of the latent variable Gaussian graphical model ( LVGGM ), where the precision matrix is the superposition of a sparse matrix and a low-rank matrix In order to speed up the estimation of the sparse plus low-rank components. In addition we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision, demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory. We propose a sparsity constrained maximum likelihood estimator based on matrix factorization, and an efficient alternating gradient descent algorithm with hard thresholding to solve it, Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM. Experiments on both synthetic and genomic data.
2K_test_1251	Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships, To capture such global interdependency. Validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types, We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes. We propose a deep Variation-structured Reinforcement Learning ( VRL ) framework to sequentially discover object relationships and attributes in the whole image, First a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, Next we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions, In particular an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish, We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection ( VRD ) dataset and the large-scale Visual Genome dataset.
2K_test_1253	For automatically detecting and segmenting blood cells including normal red blood cells ( RBCs ), connected RBCs abnormal RBCs ( i, tear drop burr cell, ) and white blood cells ( WBCs ). 14 The precision and recall of RBCs detection are 98, 43 % and 94, 99 % respectively whereas those of WBCs detection are 99, 12 % and 99, The F-measure of our proposed WBCs segmentation gets up to 95. This paper presents an end-to-end framework Our proposed system contains several components to solve different problems regarding RBCs and WBCs We first design a novel blood cell color representation which is able to emphasize the RBCs and WBCs in separate channels, Template matching technique is then employed to individually detect RBCs and WBCs in our proposed representation, In order to automatically segment the RBCs and nuclei from WBCs, we develop an adaptive level set-based segmentation method which makes use of both local and global information The detected and segmented RBCs, however can be a single RBC, a connected RBC or an abnormal RBC Therefore, we first separate and reconstruct RBCs from the connected RBCs by our suggested modified template matching, Shape matching by inner distance is later used to classify the abnormal RBCs from the normal RBCs. Our proposed method has been tested and evaluated on different images from ALL-IDB, 10 WebPath 24 UPMC, 23 Flicker datasets and the one used by Mohamed et al.
2K_test_1254	As smartphones and tablets have been widely adopted and mobile banking apps have come into ubiquitous use, mobile devices have increasingly become new tools that customers use for banking, payments budgeting and shopping, This study has implications for banks managers related to the design and management of service delivery channels, and for financial regulators related to the inclusiveness of financial system. This paper examines the impact of the mobile channel on customer service demand across banking digital channels, and investigates how the use of the mobile channel influences customer financial decision-making. Our findings suggest that : ( 1 ) the use of the mobile channel increases customer demand for digital services ; ( 2 ) lower ATM density and higher branch channel density in the customers vicinity is associated with higher digital service demand ; ( 3 ) the mobile phone channel serves as a complement to the PC channel, the tablet channel substitutes for the PC channel, and the mobile phone channel and the tablet channel complement one another ; ( 4 ) customers acquire more information for financial decision-making following the use of the mobile channel, and mobile phone and tablet users are less likely to incur overdraft and credit card penalty fees, Net benefit of the mobile channel to the bank is $ 0, 07 USD per month per ( average ) customer. Based on a novel large-scale dataset that contains 43 million individual transactions from 190, 000 customers during April to June 2013 from a financial institution in the United States. Our analysis is validated.
2K_test_1255	Intelligent conversational assistants such as Apple 's Siri, Microsoft 's Cortana and Amazon 's Echo, have quickly become a part of our digital life, Our observations could assist the deployment of crowd-powered conversation systems and crowd-powered systems in general. However these assistants have major limitations, which prevents users from conversing with them as they would with human dialog partners This limits our ability to observe how users really want to interact with the underlying system, To address this problem In this paper, we present an account of Chorus ' deployment, with a focus on four challenges : ( i ) identifying when conversations are over, ( ii ) malicious users and workers, ( iii ) on-demand recruiting, and ( iv ) settings in which consensus is not enough. Up to the first month of our deployment, 59 users have held conversations with Chorus during 320 conversational sessions. We developed a crowd-powered conversational assistant, Chorus and deployed it to see how users and workers would interact together when mediated by the system Chorus sophisticatedly converses with end users over time by recruiting workers on demand, which in turn decide what might be the best response for each user sentence.
2K_test_1256	Voting systems typically treat all voters equally. We argue that perhaps they should not : Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones, To develop a formal framework for desirable weighting schemes. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule. We draw on no-regret learning, Specifically given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight.
2K_test_1257	Understanding traffic density from large-scale web camera ( webcam ) videos is a challenging problem because such videos have low spatial and temporal resolution, high occlusion and large perspective. To deeply understand traffic density To avoid individual vehicle detection and tracking. And get insights from optimization based method to improve deep model FCN based method significantly reduces the mean absolute error from 10, 31 on the public dataset TRANCOS compared with the state-of-the-art baseline. We explore both deep learning based and optimization based methods, both methods map the image into vehicle density map, one based on rank constrained regression and the other one based on fully convolution networks ( FCN ) The regression based method learns different weights for different blocks in the image to increase freedom degrees of weights and embed perspective information, The FCN based method jointly estimates vehicle density map and vehicle count with a residual learning framework to perform end-to-end dense prediction, allowing arbitrary image resolution, and adapting to different vehicle scales and perspectives. We analyze and compare both methods Since existing datasets do not cover all the challenges in our work, we collected and labelled a large-scale traffic video dataset, containing 60 million frames from 212 webcams Both methods are extensively evaluated and compared on different counting tasks and datasets.
2K_test_1258	For a component-based modeling and verification approach for hybrid systems. In this paper we present reasoning techniques comprising discrete dynamics as well as continuous dynamics, in which the components have local responsibilities, Our approach supports component contracts i, input assumptions and output guarantees of interfaces that are more general than previous component-based hybrid systems verification techniques in the following ways : We introduce change contracts, which characterize how current values exchanged between components along ports relate to previous values, We also introduce delay contracts, which describe the change relative to the time that has passed since the last value was exchanged Together, these contracts can take into account what has changed between two components in a given amount of time since the last exchange of information, Most crucially we prove that the safety of compatible components implies safety of the composite. The proof steps of the theorem are also implemented as a tactic in KeYmaerai ? X, allowing automatic generation of a KeYmaerai ? X proof for the composite system from proofs of the concrete components.
2K_test_1259	Processes such as disease propagation and information diffusion often spread over some latent network structure that must be learned from observation. Given a set of unlabeled training examples representing occurrences of an event type of interest ( such as a disease outbreak ), the authors aim to learn a graph structure that can be used to accurately detect future events of that type for learning graph structure from unlabeled data. The authors show that their method learns a structure similar to the true underlying graph, but enables faster and more accurate detection. They propose a novel framework by comparing the most anomalous subsets detected with and without the graph constraints, Their framework uses the mean normalized log-likelihood ratio score to measure the quality of a graph structure, and it efficiently searches for the highest-scoring graph structure. Using simulated disease outbreaks injected into real-world Emergency Department data from Allegheny County.
2K_test_1260	For visual question answering and dialog agents. We show results on a synthetic world, where the agents communicate in ungrounded vocabulary, symbols with no pre-specified meanings ( X, Y Z ) We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes ( shape/color/style ) Thus, we demonstrate the emergence of grounded language and communication among 'visual ' dialog agents with no human supervision, and show that the RL 'fine-tuned ' agents significantly outperform SL agents, Interestingly the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team. We introduce the first goal-driven training Specifically, we pose a cooperative 'image guessing ' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images, We use deep reinforcement learning ( RL ) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward. We demonstrate two experimental results First, as a 'sanity check ' demonstration of pure RL ( from scratch ), Second we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data.
2K_test_1261	Chest X-ray ( CXR ) is one of the most commonly prescribed medical imaging procedures, often with over 2-10x more scans than other imaging modalities such as MRI, CT scan and PET scans, These voluminous CXR scans place significant workloads on radiologists and medical practitioners, surpassing the current state-of-the-art. Organ segmentation is a crucial step to obtain effective computer-aided detection on CXR, to segment lung fields and the heart in CXR images. Show that our method produces highly accurate and natural segmentation our model reaches human-level performance without relying on any existing trained model or dataset, Our method also generalizes well to CXR images from a different patient population and disease profiles. In this work we propose Structure Correcting Adversarial Network ( SCAN ) SCAN incorporates a critic network to impose on the convolutional segmentation network the structural regularities emerging from human physiology, During training the critic network learns to discriminate between the ground truth organ annotations from the masks synthesized by the segmentation network Through this adversarial process the critic network learns the higher order structures and guides the segmentation model to achieve realistic segmentation outcomes. Extensive experiments Using only very limited training data available.
2K_test_1262	That provides end users with intelligent sensing capabilities. The disclosure describes a sensor system, and embodies both crowd sourcing and machine learning together Further, a sporadic crowd assessment is used to ensure continued sensor accuracy when the system is relying on machine learning analysis This sensor approach requires minimal and non-permanent sensor installation by utilizing any device with a camera as a sensor host, and provides human- centered and actionable sensor output.
2K_test_1263	Modeling the long-term facial aging process is extremely challenging due to the presence of large and non-linear variations during the face development stages. In order to efficiently address the problem to model the facial aging process at each stage. To further show the advantages of our proposed approach. This work first decomposes the aging process into multiple short-term stages, Then a novel generative probabilistic model, named Temporal Non-Volume Preserving ( TNVP ) transformation, is presented Unlike Generative Adversarial Networks ( GANs ), which requires an empirical balance threshold, and Restricted Boltzmann Machines ( RBM ), an intractable model our proposed TNVP approach guarantees a tractable density function, exact inference and evaluation for embedding the feature transformations between faces in consecutive stages, Our model shows its advantages not only in capturing the non-linear age related variance in each stage but also producing a smooth synthesis in age progression across faces, Our approach can model any face in the wild provided with only four basic landmark points, Moreover the structure can be transformed into a deep convolutional network while keeping the advantages of probabilistic models with tractable log-likelihood density estimation. Our method is evaluated in both terms of synthesizing age-progressed faces and cross-age face verification and consistently shows the state-of-the-art results in various face aging databases, FG-NET MORPH AginG Faces in the Wild ( AGFW ), and Cross-Age Celebrity Dataset ( CACD ) A large-scale face verification on Megaface challenge 1 is also performed.
2K_test_1264	Analyzing videos of human actions involves understanding the temporal relationships among video frames, CNNs are the current state-of-the-art methods for action recognition in videos. However the CNN architectures currently being used have difficulty in capturing these relationships, State-of-the-art action recognition approaches rely on traditional local optical flow estimation methods to pre-compute the motion information for CNNs, Such a two-stage approach is computationally expensive, storage demanding and not end-to-end trainable. Show that it achieves competitive accuracy with the two-stage approaches. In this paper we present a novel CNN architecture that implicitly captures motion information, Our method is 10x faster than a two-stage approach, does not need to cache flow information, and is end-to-end trainable. Experimental results on UCF101 and HMDB51.
2K_test_1266	This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks. To capture the notion of object persistence and continuity in our visual experience. Interestingly the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet. We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network ( DCNN ) that learns to associate different views of each 3D object The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations, It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories, This untangling enables the model to discriminate and recognize objects within the same category, We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects.
2K_test_1267	Given a collection of seasonal time-series, how can we find regular ( cyclic ) patterns and outliers ( i, rare events ) ? These two types of patterns are hidden and mixed in the time-varying activities. How can we robustly separate regular patterns and outliers, without requiring any prior information ? to capture both cyclic patterns and outliers, which solves the above problem. Demonstrate the benefits of the proposed model and algorithm, in that the model can capture latent cyclic patterns, trends and rare events, and the algorithm outperforms the existing state-of-the-art approaches, CycloneFact was up to 5 times more accurate and 20 times faster than top competitors. We present CycloneM a unifying model and CycloneFact, a novel algorithm We also present an automatic mining framework AutoCyclone, based on CycloneM and CycloneFact Our method has the following properties ; ( a ) effective : it captures important cyclic features such as trend and seasonality, and distinguishes regular patterns and rare events clearly ; ( b ) robust and accurate : it detects the above features and patterns accurately against outliers ; ( c ) fast : CycloneFact takes linear time in the data size and typically converges in a few iterations ; ( d ) parameter free : our modeling framework frees the user from having to provide parameter values. Extensive experiments on 4 real datasets.
2K_test_1268	The recently developed variational autoencoders ( VAEs ) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena, to enable infinite flexibility of the latent representation space. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations. In this work we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference, The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning.
2K_test_1270	Many problems in image processing and computer vision ( e, colorization style transfer ) can be posed as 'manipulating ' an input image into a corresponding output image given a user-specified guiding signal, A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals ( even signals unseen during training ), such as diverse paintings and arbitrary descriptive attributes. However existing methods are either inefficient to simultaneously process multiple signals ( let alone generalize to unseen signals ), or unable to handle signals from other modalities In this paper, we make the first attempt to address the zero-shot image manipulation task to promote further research. Show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals ( e, style images and attributes ) in real-time ( tens of milliseconds per image ) even for unseen signals. We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal ( even unseen ones ) To this end, we propose the Zero-shot Manipulation Net ( ZM-Net ), a fully-differentiable architecture that jointly optimizes an image-transformation network ( TNet ) and a parameter network ( PNet ) The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself Moreover, a large-scale style dataset with over 20, 000 style images is also constructed.
2K_test_1271	Reading tracing and explaining the behavior of code are strongly correlated with the ability to write code effectively, Kodu reasoning problems appear to be a promising tool for assessing computational thinking in young programmers. To investigate program understanding in young children. Explicitly teaching semantics proved helpful with one type of misconception but not with others We found different styles of student reasoning ( analytical and analogical ) that may correspond to distinct neo-Piagetian stages of development as described by Teague and Lister ( 2014 ). We introduced two groups of third graders to Microsoft 's Kodu Game Lab ; the second group was also given four semantic `` Laws of Kodu '' to better scaffold their reasoning and discourage some common misconceptions During each session, students were asked to predict the behavior of short Kodu programs.
2K_test_1272	We analyze how alternative consumer data handling regimes affect the welfare of consumers, advertising firms and an intermediary Ad exchange in the context of targeted advertising. We find that the collection and use of consumer data for targeting purposes affect consumer welfare through three distinct, and possibly countervailing effects : match improvement, offer discrimination and supply expansion, Furthermore we find that the economic interests of the three agents can be misaligned, depending on the degree of heterogeneity in consumer preferences, Finally we find that a strategic intermediary may choose to share with advertising firms only a subset of consumer data, maximizing its profits at their cost, overlooking the other agents interests, regulation of data collection and sharing may increase consumers welfare. In situations where the intermediary has an incentive to reveal the information that maximizes its payoff.
2K_test_1273	When tasked to find fraudulent social network users, what is a practitioner to do ?. Traditional classification can lead to poor generalization and high misclassification given few and possibly biased labels We tackle this problem to handle new and multimodal fraud types. We report the signs of such behaviors, including oddities in local network connectivity, account attributes and similarities and differences across fraud providers We discover several types of fraud behaviors, with the possibility of even more, which give exceptionally strong ( > 0, 95 precision/recall ) discriminative power on ground-truth data, and which reduces misclassification rate by > 18 % over baselines and routes practitioner attention to samples at high-risk of misclassification. And building algorithms First, we set up honeypots, or `` dummy '' social network accounts on which we solicit fake followers ( after careful IRB approval ), We discuss how to leverage these insights in practice, build strongly performing entropy-based features, and propose OEC ( Open-ended Classification ), an approach for `` future-proofing '' existing algorithms to account for the complexities of link fraud, Our contributions are ( b ) features : we engineer features ( c ) algorithm : we motivate and discuss OEC. By analyzing fraudulent behavioral patterns, featurizing users to yield strong discriminative performance, ( a ) observations : we analyze our honeypot fraudster ecosystem and give insights regarding various fraud behaviors.
2K_test_1274	Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled countless new applications in the domain of aerial sensing, In scenarios such as intrusion detection, target tracking and facility monitoring it is important to reach a given area of interest ( AOI ), and create an online data streaming connection to a monitoring ground station ( GS ) for immediate delivery of content to the operator, In previous work we showed that a multi-hop line network can increase the range of the mission by finding the optimal number of relay UAVs, and their optimal placement. In this demo we show that CSMA ( typical 802, 11 's medium access protocol ) behaves poorly in this type of networks due to mutual interference, and that TDMA is a better alternative. We will also discuss how changing slot width online can overcome typical and less known TDMA in-efficiencies, and therefore reach maximum end-to-end throughput and low delay.
2K_test_1275	Most cameras are equipped with an auto-contrast feature that enables them to take high quality pictures in a wide range of lighting conditions, Auto-contrast works by increasing the sensitivity of the camera to light in dimly lit surroundings, but reducing it in bright conditions to ensure that images do not become saturated, Our visual system is equipped with a similar feature, Neurons in the visual system increase or decrease their sensitivity to light as appropriate to enable us to see in both dimly lit rooms and dazzling sunshine, This process which is known as dynamic range adaptation, also occurs in neurons that are sensitive to sound or touch, This makes sense because in a 3D task, which also features depth, the neurons have a greater range of possible movement directions to encode, These results presented by Rasmussen et al, raise several additional questions, Are the mechanisms that support dynamic range adaptation the same in sensory and motor neurons ? If these neurons also encode other aspects of movement, such as speed would these also be included in the same range as direction or is the adaptation process segregated by specific parameter categories ? And how do these changes in sensitivity affect the movements that animals produce. Therefore wondered whether the same might hold true for neurons that encode non-sensory stimuli such as the direction of movement, Would these neurons change their sensitivity to direction if presented with a wide range of possible directions instead of a narrow range ? If so, this would suggest that dynamic range adaptation occurs throughout the nervous system. Showed that neurons became less sensitive to the cursors direction of movement when the task switched from 2D to 3D, Conversely the neurons became more sensitive to the direction of movement when the task switched from 3D to 2D, Under these circumstances the neurons can use activity that was previously dedicated to encoding depth to instead represent the 2D space in finer detail. Trained two rhesus macaque monkeys to use their brain activity to move a cursor on a virtual reality screen in either 2D or 3D Studying this brain activity.
2K_test_1276	With ever growing data volume and model size, an error-tolerant communication efficient, yet versatile distributed algorithm has become vital for the success of many large-scale machine learning applications. We prove that every limit point of the sequence generated by m-PAPG is a critical point of the objective function we prove that the function value decays linearly for every $ s $ steps ; we prove that the sequences generated by m-PAPG converge to the same critical point, provided that a proximal Lipschitz condition is satisfied. In this work we propose m-PAPG, an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol The worker machines communicate asynchronously with a controlled staleness bound $ s $ and operate at different frequencies We characterize various convergence properties of m-PAPG :. 1 ) Under a general non-smooth and non-convex setting, ; 2 ) Under an error bound condition, 3 ) Under the Kurdyka- $ { \L } $ ojasiewicz inequality.
2K_test_1277	Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation, One of the main challenges for self-driving vehicles on public roads is the safe cooperation and collaboration among multiple vehicles using sensor-based perception and inter-vehicle communications, When self-driving vehicles try to occupy the same spatial area simultaneously, they might collide with one another, might become deadlocked or might slam on the brakes making it uncomfortable or unsafe for passengers in a self-driving vehicle. In this paper we study how a self-driving vehicle can safely navigate merge points, where two lanes with different priorities meet, for cooperating with other self-driving and/or human-driven vehicles. Results show that our traffic protocol has higher traffic throughput, compared to simple traffic protocols. We present a safe protocol for merge points named Autonomous Vehicle Protocol for Merge Points, where self-driving vehicles use both vehicular communications and their own perception systems.
2K_test_1279	Gaussian belief propagation ( BP ) has been widely used for distributed estimation in large-scale networks such as the smart grid, communication networks and social networks, where local meansurements/observations are scattered over a wide geographical area. However the convergence of Gaussian BP is still an open issue In this paper, we consider the convergence of Gaussian BP. That the exchanged message information matrix converges for arbitrary positive semidefinite initial value, and its distance to the unique positive definite limit matrix decreases exponentially fast. Focusing in particular on the convergence of the information matrix.
2K_test_1280	Intelligent personalization systems are becoming increasingly reliant on contextually-relevant devices and services, such as those available within modern IoT deployments, An IoT context may emerge -- -or become pervasive -- -when the intelligent system generates knowledge from dialogue-based interactions with the end-user ; the context is strengthened even further by incorporating state representations about the environment ( e, generated from wireless sensor data ) into the knowledge graph, This is crucial for pervasive applications like digital assistance in IoT, where context-aware systems need to adapt quickly : activities like leaving work home-bound, driving to the grocery store, arriving at home and walking the dog, for example can occur in a relatively short period of time -- - during which an intelligent assistant must be able to support user requests in a consistent and coherent manner, Given that computational ontologies can serve as semantic models for heterogeneous data, they are becoming increasingly viable for reasoning across different IoT contexts, This involves : ( a ) federation and dynamic pruning of multiple modular ontologies, ideally to comprehensively capture only the knowledge that will facilitate execution of a multi-context task ; ( b ) fast consistency-checking and ontology-based inferences, aided by rules-based execution environments that can evaluate/transform ambient wireless sensor network ( WSN ) data, in real-time ; and ( c ) run-time execution of ontology-based control procedures, through rule-engine actuation commands sent across the WSN, Only by realizing these functionalities may intelligent systems be capable of reasoning over device properties, system states and user activities, while appropriately delegating commands to other intelligent agents or other relevant IoT services. In order to enable scalable contextual reasoning for intelligent assistance. Preliminary results are also discussed. In this poster we illustrate how a multi-context knowledge base can be structured on the basis of modular ontologies and integrated with a distributed rules-based inference engine in multiple smart-building environments, The approach we describe is also partially based on the Ubiquitous Personal Assistant ( UPA ) project, Bosch Research 's largest research initiative worldwide. This work is conducted through the partnership of Bosch Research Pittsburgh and Carnegie Mellon University ( CMU ), and is in partial satisfaction of CMU 's Bosch Energy Research Network ( BERN ) grant, awarded for developments in intelligent building solutions.
2K_test_1283	Traditional generative adversarial networks ( GAN ) and many of its variants are trained by minimizing the KL or JS-divergence loss that measures how close the generated data distribution is from the true data distribution A recent advance called the WGAN based on Wasserstein distance can improve on the KL and JS-divergence based GANs, and alleviate the gradient vanishing, instability and mode collapse issues that are common in the GAN training. In this work we aim at improving on the WGAN. That the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN, and have seen both visual and quantitative improvement over baseline WGAN. By first generalizing its discriminator loss to a margin-based one, which leads to a better discriminator, and in turn a better generator, and then carrying out a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later stages will improve upon early stages We call this method Gang of GANs ( GoGAN ) We have also proposed a new way of measuring GAN quality which is based on image completion tasks. We have shown theoretically We have evaluated our method on four visual datasets : CelebA, LSUN Bedroom CIFAR-10 and 50K-SSFF.
2K_test_1284	The problems of hand detection have been widely addressed in many areas, human computer interaction environment, driver behaviors monitoring etc. However the detection accuracy in recent hand detection systems are still far away from the demands in practice due to a number of challenges, hand variations highly occlusions, low-resolution and strong lighting conditions, to handle the problems of hand detection in given digital images collected under challenging conditions. Our proposed method achieves the state-of-the-art results with 20 % of the detection accuracy higher than the second best one in the VIVA challenge. This paper presents the Multiple Scale Faster Region-based Convolutional Neural Network ( MS-FRCNN ) Our proposed method introduces a multiple scale deep feature extraction approach in order to handle the challenging factors to provide a robust hand detection algorithm. The method is evaluated on the challenging hand database, the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge, and compared against various recent hand detection methods.
2K_test_1285	While only recently developed, the ability to profile expression data in single cells ( scRNA-Seq ) has already led to several important studies and findings, Such database queries ( which can be performed using our web server ) will enable researchers to better characterize cells when analyzing heterogeneous scRNA-Seq samples. However this technology has also raised several new computational challenges including questions related to handling the noisy and sometimes incomplete data, how to identify unique group of cells in such experiments and how to determine the state or function of specific cells based on their expression profile, To address these issues for the analysis and retrieval of single cell RNA-Seq data. We show that the NN method improves upon prior methods in both, the ability to correctly group cells in experiments not used in the training and the ability to correctly infer cell type or state by querying a database of tens of thousands of single cell profiles. We develop and test a method based on neural networks ( NN ) We tested various NN architectures, some biologically motivated and used these to obtain a reduced dimension representation of the single cell expression data.
2K_test_1286	Synthesizes a faithful and high performance implementation from the mathematical specification of a given controller or monitor, In this paper we discuss the use of HA-SPIRAL in generating provably-correct and high-performance implementations for different controllers and monitors for autonomous land and air vehicles. High Assurance SPIRAL ( HA-SPIRAL ) is a tool that At the heart of HA-SPIRAL is a mathematical identity rewrite engine based on a computer algebra system, The rewrite engine refines the mathematical expression provided by a control engineer, through mathematical identities into an equivalent mathematical expression that can be implemented in code.
2K_test_1287	The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors. How quickly does a piece of news spread over these media ? How does its popularity diminish over time ? Does the rising and falling pattern follow a simple universal law ? the rise and fall patterns of information diffusion for the real-time monitoring of information diffusion. Demonstrate that S pike M accurately and succinctly describes all patterns of the rise and fall spikes in social networks. In this article we propose S pike M, a concise yet flexible analytical model of Our model has the following advantages First, unification power : it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models We provide the threshold of the take-off versus die-out conditions for S pike M and discuss the generality of our model by applying it to an arbitrary graph topology Second, practicality : it matches the observed behavior of diverse sets of real data Third, parsimony : it requires only a handful of parameters, Fourth usefulness : it makes it possible to perform analytic tasks such as forecasting, spotting anomalies and interpretation by reverse engineering the system parameters of interest ( quality of news, number of interested bloggers, ) We also introduce an efficient and effective algorithm namely S pike S tream, which identifies multiple diffusion patterns in a large collection of online event streams. Extensive experiments on real datasets.
2K_test_1288	In face recognition tasks, the changing pose of the face can cause enough information to be lost to cause the recognition to fail so being able to determine the pose of the face beforehand can allow for some better recognition performance. Many methods used for pose estimation tasks rely on finding some underlying structure of the data given to create a classifier. We show this method can perform pose estimation with a high accuracy of 85, 21 % and an accuracy of 98, 42 % when allowing a 15 tolerance on the pose estimate on the CUbiC FacePix dataset, with our methods achieving 77, 01 % accuracy on yaw estimation. We propose method in which the training data itself is the underlying structure of a classifier, This is accomplished through the use of matrix decomposition equations However, instead of decomposing a matrix, one is created by carefully selecting the terms in the decomposition equation such that the resulting matrix has the desired properties for classification, We show two recomposition methods using the Spectral Decomposition and Singular Value Decomposition equations. We also show results on both yaw and pitch estimation on the Pointing'04 dataset.
2K_test_1289	The mechanism classes we study are significantly different from well-understood function classes typically found in machine learning, so bounding their complexity requires a sharp understanding of the interplay between mechanism parameters and buyer valuations. We study the design of pricing mechanisms and auctions when the mechanism designer does not know the distribution of buyers ' values, to measure the intrinsic complexity of a variety of widely-studied single- and multi-item auction classes We demonstrate how to determine the precise level in a hierarchy with the optimal tradeoff between profit and generalization using structural profit maximization. We present a single, overarching theorem that uses empirical Rademacher complexity, including affine maximizer auctions, mixed-bundling auctions and second-price item auctions Despite the extensive applicability of our main theorem, we match and improve over the best-known generalization guarantees for many auction classes, This all-encompassing theorem also applies to multi- and single-item pricing mechanisms in both multi- and single-unit settings, such as linear and non-linear pricing mechanisms, Finally our central theorem allows us to easily derive generalization guarantees for every class in several finely grained hierarchies of auction and pricing mechanism classes. Instead the mechanism designer receives a set of samples from this distribution and his goal is to use the sample to design a pricing mechanism or auction with high expected profit, We provide generalization guarantees which bound the difference between average profit on the sample and expected profit over the distribution, These bounds are directly proportional to the intrinsic complexity of the mechanism class the designer is optimizing over.
2K_test_1290	Rapid improvements in the precision of mobile technologies make it possible for advertisers to go beyond using the real-time static location and contextual information about consumers Our finding suggests that highly targeted mobile promotions can have the inadvertent impact of reducing impulse purchase behavior by customers who are in an exploratory shopping stage, On a broader note, our work can be viewed as a first step towards studying the large-scale, fine-grained digital trace of individual physical behavior, and how it can be used to predict and market to individual anticipated future behavior. We find that trajectory-based mobile targeting can lead to higher redemption probability, faster redemption behavior and higher transaction amount from customers compared to other baselines, It also facilitates higher revenues for the focal store as well as the overall shopping mall, Moreover the effect of trajectory-based targeting comes not only from improvements in the efficiency of customers current shopping process, but also from its ability to nudge customers towards changing their future shopping patterns and generate additional revenues, Finally we find significant heterogeneity in the impact of trajectory-based targeting, It is especially effective in influencing high-income consumers Interestingly, it becomes less effective in boosting the revenues of the shopping mall during the weekends and for those shoppers who like to explore across products categories. In this study we propose a novel trajectory-based targeting strategy that leverages full information on consumers physical movement trajectories using granular behavioral information from different mobility dimensions. To analyze the effectiveness of this new strategy, we design a large-scale randomized field experiment in a large shopping mall that involved 83, 370 unique user responses for a 14-day period in June 2014.
2K_test_1291	Common appliances have shifted toward flat interface panels, making them inaccessible to blind people, Although blind people can label appliances with Braille stickers, doing so generally requires sighted assistance to identify the original functions and apply the labels. To help blind people independently make physical interfaces accessible. We demonstrate the viability of Facade. We introduce Facade - a crowdsourced fabrication pipeline by adding a 3D printed augmentation of tactile buttons overlaying the original panel Facade users capture a photo of the appliance with a readily available fiducial marker ( a dollar bill ) for recovering size information This image is sent to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface, Facade then generates a 3D model for a layer of tactile and pressable buttons that fits over the original controls Finally, a home 3D printer or commercial service fabricates the layer, which is then aligned and attached to the interface by the blind person. In a study with 11 blind participants.
2K_test_1293	The same techniques can be applied to monitor other types of traffic data. Is it possible to monitor the entire traffic in Manhattan at a few intersections ? to handle complex. We are able to approximately recover the taxi-pick activities in Manhattan by sampling at only 5 selected intersections. This paper proposes a series of sampling, recovery and representation techniques based on graph signal processing. We validate our proposed techniques on Manhattan 's taxi pickups during the years of 2014 and 2015.
2K_test_1294	Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition, to extract similarly salient features for accurate classification, to enable an adaptive imitation scheme. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged We develop an adversarial model through competition between the implicit network and a rival feature discriminator.
2K_test_1295	Hearing-impaired people and non-native speakers rely on captions for access to video content Based on our results, we outline opportunities for future research and provide design suggestions to deliver cost-efficient captioning solutions. Yet most videos remain uncaptioned or have machine-generated captions with high error rates, to provide a cost-efficient captioning solution for accessible online videos. Our findings show that BandCaption enables crowd workers who have different needs and strengths to accomplish micro-tasks and make complementary contributions. In this paper we present the design, implementation and evaluation of BandCaption, a system that combines automatic speech recognition with input from crowd workers Each group has different abilities and incentives, which our workflow leverages. We consider four stakeholder groups as our source of crowd workers : ( i ) individuals with hearing impairments, ( ii ) second-language speakers with low proficiency, ( iii ) second-language speakers with high proficiency, and ( iv ) native speakers.
2K_test_1296	The promise of smart environments and the Internet of Things ( IoT ) relies on robust sensing of diverse environmental facets. Traditional approaches rely on direct and distributed sensing, most often by measuring one particular aspect of an environment with a special purpose sensor, This approach can be costly to deploy, hard to maintain and aesthetically and socially obtrusive In this work, we explore the notion of general purpose sensing, wherein a single enhanced sensor can indirectly monitor a large context, without direct instrumentation of objects. The results of which show the versatility, accuracy and potential utility of our approach. Further through what we call Synthetic Sensors, we can virtualize raw sensor data into actionable feeds, whilst simultaneously mitigating immediate privacy issues. A series of structured, formative studies informed the development of our new sensor hardware and accompanying information architecture, We deployed our system across many months and environments.
2K_test_1297	Introduction Drug overdoses are an increasingly serious problem in the United States and worldwide, The CDC estimates that 47, 055 drug overdose deaths occurred in the United States in 2014, 61 % of which involved opioids ( including heroin, pain relievers such as oxycodone, 1 Overdose deaths involving opioids increased 3-fold from 2000 to 2014, 1 These statistics motivate public health to identify emerging trends in overdoses, including geographic demographic and behavioral patterns ( e, which combinations of drugs are involved ), Early detection can inform prevention and response efforts, as well as quantifying the effects of drug legislation and other policy changes, The fast subset scan 2 detects significant spatial patterns of disease by efficiently maximizing a log-likelihood ratio statistic over subsets of data points, and has recently been extended to multidimensional data ( MD-Scan ), Conclusions Retrospective analysis of Allegheny County overdose data suggests high potential utility for a prospective overdose surveillance system, which would enable public health users to identify emerging patterns of overdoses in their early stages and facilitate targeted and effective health interventions, The MDTS approach can also be used for other multidimensional public health surveillance tasks, such as STI surveillance, where the patterns or outbreaks of interest may have demographic, geographic and behavioral components. Objective for identifying emerging patterns in multidimensional spatio-temporal data 3 While MD-Scan is a potentially useful tool for drug overdose surveillance, the high dimensionality and sparsity of the data requires a new approach to estimate and represent baselines ( expected counts ), maintaining both accuracy and efficient computation when searching over subsets, to subset scanning in multidimensional data. And demonstrate the utility of this approach for discovering emerging geographic, demographic and behavioral trends in fatal drug overdoses, Results The highest-scoring clusters discovered by MDTS were shared with Allegheny Countys Dept, of Human Services and their feedback obtained, One set of potentially relevant findings from our analysis involved fentanyl, a dangerous and potent opioid which has been a serious problem in western PA, In addition to identifying two well- known, large clusters of overdoses14 deaths in January 2014 and 26 deaths in March-April 2015MDTS was able to provide additional information about each cluster, For example the first cluster was likely due to fentanyl-laced heroin, while the second was more likely due to fentanyl disguised as heroin ( only 11 victims had heroin in their system ), Moreover the second cluster was initially confined to the Pittsburgh suburb of McKeesport and a typical demographic ( white males ages 20-49 ), before spreading across the county, Our analysis demonstrated that prospective surveillance using MDTS would have identified the cluster as early as March 29th, enabling targeted prevention efforts, MDTS also discovered a previously unidentified, highly localized cluster of fentanyl-related overdoses affecting an unusual and underserved demographic ( elderly black males near downtown Pittsburgh ), This cluster occurred in January- February 2015, and may have been related to the larger cluster of fentanyl-related overdoses that occurred two months later Finally, we identified multiple overdose clusters involving combinations of methadone and Xanax between 2008 and 2012, and observed dramatic reductions in these clusters corresponding to the passage of the Methadone Death and Incident Review Act ( October 2012 ), which increased state oversight of methadone clinics and prescribing physicians. We present the multidimensional tensor scan ( MDTS ), a new method Methods The multidimensional tensor scan ( MDTS ) is a new approach In addition to detecting the spatial area ( subset of locations ) and time window affected by an emerging outbreak, MDTS can also identify the affected subset of values for each observed attribute, For example given the drug overdose surveillance data described below, MDTS can identify the affected genders, races age ranges and which drugs were involved, MDTS finds subsets of the attribute space with higher than expected case counts, first using a novel tensor decomposition approach to estimate the expected counts MDTS then iteratively applies a conditional optimization step, optimizing over all subsets of values for each attribute conditional on the current subsets of values for all other attributes 3, and using the linear-time subset scanning property 2 to make each conditional optimization step computationally efficient The resulting approach has high power to detect and characterize emerging trends which may only affect a subset of the monitored population ( e, specific ages genders neighborhoods, or users of particular combinations of drugs. We used MDTS to analyze publicly available data from the Allegheny County, PA medical examiners office and to detect emerging overdose patterns and trends The dataset consists of ~2000 fatal accidental drug overdoses between 2008 and 2015, For each overdose victim, we have date location ( zip code ), age decile gender race, and the presence/absence of 27 commonly abused drugs in their system.
2K_test_1298	Homes offices and many other environments will be increasingly saturated with connected, computational appliances forming the `` Internet of Things '' ( IoT ), At present most of these devices rely on mechanical inputs, webpages or smartphone apps for control. However as IoT devices proliferate, these existing interaction methods will become increasingly cumbersome, Will future smart-home owners have to scroll though pages of apps to select and dim their lights to discover and rapidly utilize contextual functionality. Suggests high accuracy 98, 8 % recognition accuracy among 17 appliances. We propose an approach where users simply tap a smartphone to an appliance To achieve this, our prototype smartphone recognizes physical contact with uninstrumented appliances, and summons appliance-specific interfaces. Our user study Finally, to underscore the immediate feasibility and utility of our system, we built twelve example applications, including six fully functional end-to-end demonstrations.
2K_test_1299	Small local groups who share protected resources ( e, families work teams student organizations ) have unmet authentication needs. For these groups existing authentication strategies either create unnecessary social divisions ( e, biometrics ) do not identify individuals ( e, shared passwords ) do not equitably distribute security responsibility ( e, individual passwords ) or make it difficult to share or revoke access ( e, To explore an alternative. Our results suggest that ( 1 ) individuals who enter the same shared thumprint are distinguishable from one another, ( 2 ) that people can enter thumprints consistently over time, and ( 3 ) that thumprints are resilient to casual adversaries. We designed Thumprint : inclusive group authentication with a shared secret knock, All group members share one secret knock, but individual expressions of the secret are discernible. We evaluated the usability and security of our concept through two user studies with 30 participants.
2K_test_1300	Infrastructure monitoring applications currently lack a cost-effective and reliable solution for supporting the last communication hop for low-power devices, The use of cellular infrastructure requires contracts and complex radios that are often too power hungry and cost prohibitive for sensing applications that require just a few bits of data each day, New low-power sub-GHz long-range radios are an ideal technology to help fill this communication void by providing access points that are able to cover multiple kilometers of urban space with thousands of end-point devices, These new Low-Power Wide-Area Networking ( LPWAN ) platforms provide a cost-effective and highly deployable option that could piggyback off of existing public and private wireless networks ( WiFi. With the goal of simplifying the design and deployment of Internet-of-Things ( IoT ) devices across wide areas like campuses and cities allowing users to register devices, describe transducer properties transfer data and retrieve historical values, to provide basic encoding and syntax to raw data streams, to help provision LoRa clients that can be extended with custom transducers. In this paper we present OpenChirp, a prototype end-to-end LPWAN architecture built using LoRa Wide-Area Network ( LoRaWAN ) We present a software architecture that exposes an application layer We define a service model on top of LoRaWAN that acts as a session layer At the device-level, we introduce and benchmark an open-source hardware platform that uses Bluetooth Low-Energy ( BLE ). We evaluate the system in terms of end-node energy consumption, radio penetration into buildings as well as coverage provided by a network currently deployed at Carnegie Mellon University.
2K_test_1301	Whereas previous models assuming no exogenous infection, showed dependency only on the infection and healing rates. The process we study is the scaled SIS network process, a continuous-time Markov process on a static network. We show that the sufficient condition for infection to become extinct not only depends on the ratio of infection and healing rates but also on N, the size of the network. We relate the time-limiting behavior of a network epidemics process to the spectral radius of the underlying network Our analysis differs from previous work in that the scaled SIS process accounts for the possibility that a healthy individual has a nonzero probability of becoming infected even when all of its neighbors are healthy, For example the source of infection may be outside a human only contact network for diseases with animal to human transmissions such as Ebola.
2K_test_1302	The proliferation of mobile and sensor technologies has contributed to the rise of location-based mobile targeting, Beyond the location time and spatial context of individuals, the social context wherein they are embedded can reveal rich information about their behavior, Such real-time social dynamics can help mobile advertisers to more fully understand consumer contextual preferences and, thereby provide better digital experiences Overall, our study demonstrates the potential of inferring individuals social contexts in real time from their movement trajectories as well as the value of leveraging such real-time social dynamics for improved mobile-targeting effectiveness. In this study the real-time social contexts of customers. Our analyses indicated significant heterogeneity in consumer behavior under different real-time social contexts, We found for example, that a customer in a group with others is on average 1, 97 times more responsive to mobile promotions than is a solo shopper, and that this impact increases with increased group size ( from dyad to triad ), Interestingly we also found that couples seemed to have an attention deficit with respect to mobile promotions and were the least responsive compared with the other social groups, Meanwhile high-income customers and male customers were more likely to respond to mobile promotions when shopping alone than when shopping with social groups, Our analyses also revealed significant heterogeneity in the interaction effect between mobile promotion design and real-time social contexts. We automatically detected based on their detailed GPS trajectories using state-of-the-art machine-learning methods. To evaluate the effectiveness of mobile targeting under different social contexts, we designed a randomized field experiment for a large shopping mall in Asia based on 52, 500 unique user responses for 252 stores over the course of a 21-day period in April 2015.
2K_test_1303	To the study of taxi movement in New York City. We show that PSNR=29, 90 dB is recovered for graph signals reconstructed from 70 % of the graph frequency components We illustrate that graph frequency components reveal taxi behaviors that are not obvious from the raw signal. We apply graph signal processing based on 20102013 New York City taxi data, Such analysis requires a signal extraction method that involves computing shortest paths between the start and end locations for each of the 700 million trip records. We perform spectral analysis on these graph signals, for which it is necessary to address the challenge of finding the eigendecomposition of the 6K-node directed Manhattan road network.
2K_test_1304	Current touch input technologies are best suited for small and flat applications, such as smartphones tablets and kiosks. In general they are too expensive to scale to large surfaces, such as walls and furniture, and can not provide input on objects having irregular and complex geometries, such as tools and toys that enables touch input on a wide variety of objects and surfaces, whether small or large. We show that Electrick can enable new interactive opportunities on a diverse set of objects and surfaces that were previously static. We introduce Electrick a low-cost and versatile sensing technique This is achieved by using electric field tomography in concert with an electrically conductive material, which can be easily and cheaply added to objects and surfaces We show that our technique is compatible with commonplace manufacturing methods, such as spray/brush coating, vacuum forming and casting/molding enabling a wide range of possible uses and outputs Our technique can also bring touch interactivity to rapidly fabricated objects, including those that are laser cut or 3D printed. Through a series of studies and illustrative example uses.
2K_test_1305	Blind people often need to identify objects around them, from packages of food to items of clothing, Automatic object recognition continues to provide limited assistance in such tasks because models tend to be trained on images taken by sighted people with different background clutter, scale viewpoints occlusion and image quality than in photos taken by blind users. We explore personal object recognizers, where visually impaired people train a mobile application with a few snapshots of objects of interest and provide custom labels. Demonstrate the feasibility of our approach, which reaches accuracies over 90 % for some participants. We adopt transfer learning with a deep learning system for user-defined multi-label k-instance classification. Experiments with blind participants We analyze user data and feedback to explore effects of sample size, photo-quality variance and object shape ; and contrast models trained on photos by blind participants to those by sighted participants and generic recognizers.
2K_test_1306	Current tools for screening dyslexia use linguistic elements, since most dyslexia manifestations are related to difficulties in reading and writing. These tools can only be used with children that have already acquired some reading skills and ; sometimes, this detection comes too late to apply proper remediation which aims to predict risk of having dyslexia before acquiring reading skills. In this paper we propose a method and present DysMusic, a prototype The advantages of DysMusic are that the approach is language independent and could be used with younger children. The prototype was designed with the help of five children and five parents who tested the game using the think aloud protocol and being observed while playing.
2K_test_1307	While a number of schemes exist for mixed-criticality scheduling in a single processor setting, no solution exists to cover the industry need for end-to-end scheduling across multiple processors in a pipeline that addresses this need, for computing the zero-slack instants of tasks scheduled across a pipeline. In this paper we present an end-to-end zero-slack rate-monotonic scheme ( ZSRM ) based on real-time pipelines, called the ZSRM pipeline scheduler, Under ZSRM each task is associated with a parameter called zero-slack instant, and whenever a higher-criticality job has not finished at its zero-slack instant relative to its arrival time, all jobs of lower criticality are suspended to meet the deadline of the higher-criticality job, We develop a new schedulability test and algorithm.
2K_test_1309	Information cascades are ubiquitous in both physical society and online social media, taking on large variations in structures, dynamics and semantics potentially providing insights into intrinsic mechanisms governing information spreading in nature and new models to forecast as well as to impose good control over information cascades in real applications. Although there has been much progress on understanding the dynamics and semantics of information cascades, little is known about their structural patterns to quantify the structural characteristics of millions of information cascades. We find that the structural complexity of information cascades is far beyond the previous conjectures, finding some brand new structure patterns of information cascades. In this paper we explore a large-scale dataset including 432 million information cascades with explicit records of spreading traces We first propose seven-dimensional metrics, which reflect size and spreading orientation aspects. Further we analyze the correlations of these metrics.
2K_test_1310	This paper continues the program initiated in [ 5 ]. For composing security protocols from given security components. The applicability of the method is demonstrated. Towards a derivation system The general idea is that complex protocols can be formally derived, starting from basic security components, using a sequence of refinements and transformations, just like logical proofs are derived starting from axioms, using proof rules and transformations, The claim is that in practice, many protocols are already derived in such a way, but informally Capturing this practice in a suitable formalism turns out to be a considerable task, The present paper proposes rules In general, security protocols are of course, not compositional : information revealed by one may interfere with the security of the other, However annotating protocol steps by pre- and post-conditions, allows secure sequential composition, Establishing that protocol components satisfy each other 's invariants allows more general forms of composition, ensuring that the individually secure sub-protocols will not interact insecurely in the composite protocol. On modular derivations of two standard protocols, together with their simple security properties.
2K_test_1312	Feature extraction and encoding represent two of the most crucial steps in an action recognition system. For building a powerful action recognition pipeline it is important that both steps are efficient and in the same time provide reliable performance for feature extraction and encoding. This work proposes a new approach that allows us to obtain real-time frame rate processing for an action recognition system, The motion information represents an important source of information within the video, The common approach to extract the motion information is to compute the optical flow, However the estimation of optical flow is very demanding in terms of computational cost, in many cases being the most significant processing step within the overall pipeline of the target video analysis application, In this work we propose an efficient approach to capture the motion information within the video, Our proposed descriptor Histograms of Motion Gradients ( HMG ), is based on a simple temporal and spatial derivation, which captures the changes between two consecutive frames For the encoding step a widely adopted method is the Vector of Locally Aggregated Descriptors ( VLAD ), which is an efficient encoding method, however it considers only the difference between local descriptors and their centroids, In this work we propose Shape Difference VLAD ( SD-VLAD ), an encoding method which brings complementary information by using the shape information within the encoding process, and we propose also a real-time framework for action recognition. We validated our proposed pipeline for action recognition on three challenging datasets UCF50.
2K_test_1313	As online fraudsters invest more resources, including purchasing large pools of fake user accounts and dedicated IPs, fraudulent attacks become less obvious and their detection becomes increasingly challenging. Existing approaches such as average degree maximization suffer from the bias of including more nodes than necessary, resulting in lower accuracy and increased need for manual verification, to more accurately detect groups of fraudulent users. Showed that HoloScope achieved significant accuracy improvements on synthetic and real data, compared with state-of-the-art fraud detection methods. Hence we propose HoloScope, which uses information from graph topology and temporal spikes In terms of graph topology, we introduce `` contrast suspiciousness, '' a dynamic weighting approach, which allows us to more accurately detect fraudulent blocks, In terms of temporal spikes, HoloScope takes into account the sudden bursts and drops of fraudsters ' attacking patterns In addition, we provide theoretical bounds for how much this increases the time cost needed for fraudsters to conduct adversarial attacks Additionally, from the perspective of ratings, HoloScope incorporates the deviation of rating scores in order to catch fraudsters more accurately, Moreover HoloScope has a concise framework and sub-quadratic time complexity, making the algorithm reproducible and scalable.
2K_test_1314	In comparison-shopping services ( CSS ), there exist frauds who perform excessive clicks on a target item in order to boost the popularity of it. In this paper we introduce the problem of detecting frauds in CSS and. Propose three anomaly scores designed based on click behaviors of users in CSS.
2K_test_1315	As one of the featured initiatives in smart grids, demand response is enabling active participation of electricity consumers in the supply/demand balancing process, thereby enhancing the power systems operational flexibility in a costeffective way, Industrial load plays an important role in demand response because of its intense power consumption, already existing advanced monitoring and control infrastructure, and its strong economic incentive due to the high energy costs, As typical industrial loads, cement plants are able to quickly adjust their power consumption rate by switching on/off the crushers. However in the cement plant as well as other industrial loads, switching on/off the loading units only achieves discrete power changes, which restricts the load from offering valuable ancillary services such as regulation and load following, as continuous power changes are required for these services, In this paper we overcome this restriction of poor granularity. By proposing methods that enable these loads to provide regulation or load following with the support of an on-site energy storage system.
2K_test_1316	Large graph datasets have caused renewed interest for graph partitioning. However existing well-studied graph partitioners often assume that vertices of the graph are always active during the computation, which may lead to time-varying skewness for traversal-style graph workloads, like Breadth First Search, since they only explore part of the graph in each superstep, Additionally existing solutions do not consider what vertices each partition will have, as a result high-degree vertices may be concentrated into a few partitions, causing imbalance the objective is to create an initial partitioning that will `` hold well '' over time without suffering from skewness. Towards this we introduce the idea of skew-resistant graph partitioning, where Skewresistant graph partitioning tries to mitigate skewness by taking the characteristics of both the target workload and the graph structure into consideration.
2K_test_1317	To formalizing and enforcing a class of use privacy properties in data-driven systems. Our evaluation shows that these algorithms are able to detect proxy use instances that would be difficult to find using existing techniques, and subsequently remove them while maintaining acceptable classification performance. This paper presents an approach In contrast to prior work, we focus on use restrictions on proxies ( i, strong predictors ) of protected information types Our definition relates proxy use to intermediate computations that occur in a program, and identify two essential properties that characterize this behavior : 1 ) its result is strongly associated with the protected information type in question, and 2 ) it is likely to causally affect the final output of the program, For a specific instantiation of this definition, we present a program analysis technique that detects instances of proxy use in a model, and provides a witness that identifies which parts of the corresponding program exhibit the behavior Recognizing that not all instances of proxy use of a protected information type are inappropriate, we make use of a normative judgment oracle that makes this inappropriateness determination for a given witness, Our repair algorithm uses the witness of an inappropriate proxy use to transform the model into one that provably does not exhibit proxy use, while avoiding changes that unduly affect classification accuracy. Using a corpus of social datasets.
2K_test_1319	Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content, To tackle the problem of large-scale noisy learning. Results demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data, In addition the results also verify that WELL-MM is robust to the level of noisiness in the video data, Notably WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data. We propose a novel method called Multi-modal WEbly-Labeled Learning ( WELL-MM ), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human, WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images The comprehensive experimental on FCVID and YFCC100M.
2K_test_1320	Summary Cryo-electron tomography ( cryo-ET ) captures the 3Delectron density distribution of macromolecular complexes in close to native state, With the rapid advance of cryo-ET acquisition technologies, it is possible to generate large numbers ( > 100, 000 ) of subtomograms, each containing a macromolecular complex, Often these subtomograms represent a heterogeneous sample due to variations in the structure and composition of a complex insitu form or because particles are a mixture of different complexes, In this case subtomograms must be classified. However classification of large numbers of subtomograms is a time-intensive task and often a limiting bottleneck for large-scale subtomogram classification, template matching subtomogram averaging. This paper introduces an open source software platform, TomoMiner Its scalable and robust parallel processing allows efficient classification of tens to hundreds of thousands of subtomograms, In addition TomoMiner provides a pre-configured TomoMinerCloud computing service permitting users without sufficient computing resources instant access to TomoMiners high-performance features.
2K_test_1321	The Kinect sensing devices have been widely used in current Human-Computer Interaction entertainment. A fundamental issue involved is to detect users motions accurately and quickly. The results show that our method has its advantage for motion detection in a real-time Kinect entertaining environment. In this paper we tackle it by proposing a linear algorithm, which is augmented by feature interaction, The linear property guarantees its speed whereas feature interaction captures the higher order effect from the data to enhance its accuracy The Schatten-p norm is leveraged to integrate the main linear effect and the higher order nonlinear effect by mining the correlation between them, The resulted classification model is a desirable combination of speed and accuracy We propose a novel solution to solve our objective function. Experiments are performed on three public Kinect-based entertainment data sets related to fitness and gaming.
2K_test_1322	Different from general image localization task through matching, the appearance of an environment during significant events varies greatly from its daily appearance, since there are usually crowds, decorations or even destruction when a major event happens. In this paper we study automatic geo-localization of online event videos This introduces a major challenge : matching the event environment to the daily environment, as recorded by Google Street View, We observe that some regions in the image, as part of the environment, still preserve the daily appearance even though the whole image ( environment ) looks quite different. Experimental results show that our solution significantly improves over matching on whole images and the automatically learned saliency is a strong predictor of distinctive building areas. Based on this observation, we formulate the problem as joint saliency estimation and matching at the image region level, as opposed to the key point or whole-image level, As image-level labels of daily environment are easily generated with GPS information, we treat region based saliency estimation and matching as a weakly labeled learning problem over the training data, Our solution is to iteratively optimize saliency and the region-matching model, For saliency optimization we derive a closed form solution, which has an intuitive explanation, For region matching model optimization, we use self-paced learning to learn from the pseudo labels generated by ( sub-optimal ) saliency values. We conduct extensive experiments on two challenging public datasets : Boston Marathon 2013 and Tokyo Time Machine.
2K_test_1323	Utility maximization under a budget constraint is a classical problem in economics and management science, It is commonly assumed that the utility is a `` nice '' known analytic function, for example continuous and concave, In many domains such as marketing, increased availability of computational resources and data has enabled the development of sophisticated simulations to evaluate the impact of allocating a fixed budget among alternatives ( e, marketing channels ) on outcomes, While simulations enable high resolution evaluation of alternative budget allocation strategies, they significantly complicate the associated budget optimization problem. In particular simulation runs are time consuming, significantly limiting the space of options that can be explored, An important second challenge is the common presence of budget complementarities, where non-negligible budget increments are required for an appreciable marginal impact from a channel, This introduces a combinatorial structure on the decision space We propose to address these challenges for achieving this approximation in an online fashion. Demonstrates the effectiveness of our approach. By first converting the problem into a multi-choice knapsack optimization problem with unknown weights We show that if weights ( corresponding to marginal impact thresholds for each channel ) are well approximated, we can achieve a solution within a factor of 2 of optimal, and this bound is tight, We then develop several parsimonious query algorithms.
2K_test_1324	Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks ( GANs ) and Variational Autoencoders ( VAEs ), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent study respectively, This paper establishes formal connections between deep generative modeling approaches. Show generality and effectiveness of the imported extensions. Through a new formulation of GANs and VAEs, We show that GANs and VAEs are essentially minimizing KL divergences with opposite directions and reversed latent/visible treatments, extending the two learning phases of classic wake-sleep algorithm, The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to exchange ideas across research lines in a principled way For example, we transfer the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism.
2K_test_1325	In total this work substantially expands the scope and scale of problems that can be solved using semidefinite programming methods. To low-rank structured semidefinite programming. We show that for certain problems, the method is strictly decreasing and guaranteed to converge to a critical point, In all settings we demonstrate improvement over the existing state of the art along various dimensions. In this paper we propose a coordinate descent approach The approach, which we call the Mixing method, is extremely simple to implement, has no free parameters, and typically attains an order of magnitude or better improvement in optimization performance over the current state of the art. We then apply the algorithm to three separate domains : solving the maximum cut semidefinite relaxation, solving a ( novel ) maximum satisfiability relaxation, and solving the GloVe word embedding optimization problem.
2K_test_1327	In this paper we address the challenge of recovering a time sequence of counts from aggregated historical data, For example given a mixture of the monthly and weekly sums, how can we find the daily counts of people infected with flu ? In general, what is the best way to recover historical counts from aggregated, possibly overlapping historical reports, in the presence of missing values ? Equally importantly, how much should we trust this reconstruction ?. Demonstrates that H-FUSE reconstructs the original data 30 81 % better than the least squares method. We propose H-FUSE a novel method that solves above problems by allowing injection of domain knowl- edge in a principled way, and turning the task into a well- defined optimization problem H-FUSE has the following desirable properties : ( a ) Effectiveness, recovering histori- cal data from aggregated reports with high accuracy ; ( b ) Self-awareness, providing an assessment of when the re- covery is not reliable ; ( c ) Scalability, computationally lin- ear on the size of the input data. Experiments on the real data ( epidemiology counts from the Tycho project [ 13 ].
2K_test_1328	Occupancy estimation is an important primitive for a wide range of applications including building energy efficiency. In this paper we explore the potential of using depth sensors to detect, estimate identify and track occupants in buildings While depth sensors have been widely used for human detection and gesture recognition, computer vision algorithms are typically run on a powerful computer like XBOX or Intel R CoreTM i7 processor. We observe that FORK achieves over 99 % accuracy in real-time ( 4-9 FPS ) in occupancy estimation. In this work we develop a prototype system called FORK using off-the-shelf components that performs the entire depth data processing on a cheaper and low power ARM processor in real-time, As ARM processors are extremely weak in running computer vision algorithms, FORK is designed to detect humans and track them in a very efficient way by leveraging a novel lightweight model based approach instead of traditional approaches based on histogram of oriented gradients ( HOG ) features Unlike other camera based approaches, FORK is much less privacy invasive ( even if the sensor is compromised ). Based on a complete implementation, real-world deployment and extensive evaluation at realistic scenarios.
2K_test_1329	Open-source face recognition system for denaturing video streams for large camera networks using RTFace. We present OpenFace our new that approaches state-of-the-art accuracy, Integrating OpenFace with inter-frame tracking, we build RTFace a mechanism that selectively blurs faces according to specified policies at full frame rates, This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions, Finally we present a scalable.
2K_test_1330	As video cameras proliferate, the ability to scalably capture and search their data becomes important. Scalability is improved by performing video analytics on cloudlets at the edge of the Internet, and only shipping extracted index information and meta-data to the cloud, to human-in-the-loop content-based retrospective search. In this setting we describe interactive data exploration ( IDE ), which refers using predicates that may not have been part of any prior indexing We also describe a new technique called just-in-time indexing ( JITI ) that improves response times in IDE.
2K_test_1331	Due to the advent of active safety features and automated driving capabilities, the complexity of embedded computing systems within automobiles continues to increase, Such advanced driver assistance systems ( ADAS ) are inherently safetycritical and must tolerate failures in any subsystem, Recent work has studied the use of software-based faulttolerance techniques that utilize task-level hot and cold standbys to tolerate fail-stop processor and task failures, The benefit of using standbys is maximal when a task and any of its standbys obey the placement constraint of not being co-located on the same processor. However fault-tolerance in safety-critical systems has been traditionally supported by hardware replication, which is prohibitively expensive in terms of cost, weight and size for the automotive market. That saves at least one processor up to 40 % of the time relative to the best known heuristic to date, finds that our heuristic uses no more than one additional processor in most cases relative to an optimal allocation that we construct for evaluation purposes using a creative technique. We propose based on a `` tiered '' placement constraint, and show that our heuristic produces a better task assignment We then introduce a task allocation algorithm that, for the first time to our knowledge, leverages the run-time attributes of cold standbys We have designed and implemented our software fault-tolerance framework in AUTOSAR, an automotive industry standard. Our empirical study We use this implementation to provide an experimental evaluation of our task-level fault-tolerance features, Finally we present an analysis of the worst-case behavior of our task recovery features.
2K_test_1332	Audience Participation Games challenge traditional assumptions about gameplay by blurring the line between audience and player, allowing audience members to impact gameplay in a meaningful way, Their recent rise in popularity has created new opportunities for game research and development Our results show the breadth of opportunities and challenges that designers face in creating engaging Audience Participation Games. To better understand this design space, to develop a framework for audience motivations and participation styles, to explore ways in which mechanics can affect audience members ' sense of agency, and to identify promising design spaces. We developed several versions of two prototype games as design probes We livestreamed them to an online audience in order.
2K_test_1334	Nanosecond-level clock synchronization is a missing capability for many real-time applications like next-generation wireless systems that leverage spatial multiplexing to improve channel capacity and provide services like time-of-flight localization, With finegrained synchronization both clock stability and propagation delays introduce significant sources of error. That can achieve clock synchronization. To show a clock synchronization of better than five nanoseconds per hop with an average of 2, 12 ns and a standard deviation of 0, The platform is able to identify and avoid clock error in cases where there is heavy multi-path or non-Line-of-Sight signals. In this paper we introduce Pulsar, a wireless time transfer platform to better than five nanosecond between indoor or GPS-denied devices, Pulsar leverages a stable clock source derived from a Chip-Scale Atomic Clock ( CSAC ) along with an Ultra-WideBand ( UWB ) radio able to perform sub-nanosecond packet timestamping to estimate and correct for clock offsets, We design and evaluate a proof-ofconcept network-wide synchronization protocol for Pulsar that selects low-jitter links to both estimate the location of nodes and reduce cumulative synchronization error across multiple hops, The Pulsar platform and protocol together provide a phase synchronized one pulse per second ( 1PPS ) signal and 10 MHz reference clock that can be easily integrated with typical enduser applications like software-defined radios and communication systems. We experimentally evaluate the Pulsar platform in terms of clock synchronization accuracy, Allan deviation between pairwise clocks and ranging accuracy.
2K_test_1335	Despite their growing prominence, optimization in generative adversarial networks ( GANs ) is still a poorly-understood topic which is able to guarantee local stability for both the WGAN and for the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse. We show that even though GAN optimization does not correspond to a convex-concave game, even for simple parameterizations, under proper conditions equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation, On the other hand, we show that the recently-proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates. In this paper we analyze the `` gradient descent '' form of GAN optimization ( i, the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters ).
2K_test_1336	Consider a stream of retweet events - how can we spot fraudulent lock-step behavior in such multi-aspect data ( i, tensors ) evolving over time ? Can we detect it in real time, with an accuracy guarantee ? Past studies have shown that dense subtensors tend to indicate anomalous or even fraudulent behavior in many tensor data, including social media Wikipedia. Thus several algorithms have been proposed for detecting dense subtensors rapidly and accurately, However existing algorithms assume that tensors are static, while many real-world tensors, including those mentioned above. Updates by our algorithms are up to a million times faster than the fastest batch algorithms Effective : our DENSESALERT successfully spots anomalies especially those overlooked by existing algorithms. We propose DENSESTREAM an incremental algorithm that maintains and updates a dense subtensor in a tensor stream ( i, a sequence of changes in a tensor ), and DENSESALERT an incremental algorithm spotting the sudden appearances of dense subtensors, Our algorithms are : ( 1 ) Fast and `` any time '' :, ( 2 ) Provably accurate : our algorithms guarantee a lower bound on the density of the subtensor they maintain, and ( 3 ).
2K_test_1337	Abstract : The multidisciplinary goal was to develop an integrated conceptualization of the mid-level encoding of 3D object structure from multiple surface cues, The unprecedented dips of performance reduction in the component psychometric functions was captured. Showed that depth continuity is a prerequisite for facilitation of Gabor target detection in the context of flanking Gabors, and that similarly surface continuity in purely disparity-defined slanted surfaces was strongly enhanced in distributed patch detections as a function of stimulus duration in this dual discrimination task, showing that the perceptual processing of disparity and integration of 3D surface information across depth cues has time courses of several seconds, attesting to complexity of the neural processing hardware showed how surface reconstruction could be accomplished across the typically sparse depth information available, and integrated among sparse. In a computational model based on a novel Leaky Drift Diffusion Theory that we developed for the underlying neural signals, which can serve as an analytic basis for the time course of all neural decision processes, Three complementary computational modeling projects from three collaborating laboratories. Psychophysical studies The time course of depth surface perception was studied in a coordinated trio of psychophysical, neurophysiological and functional imaging studies.
2K_test_1338	Deep learning models can take weeks to train on a single GPU-equipped machine, necessitating scaling out DL training to a GPU-cluster. However current distributed DL implementations can scale poorly due to substantial parameter synchronization over the network, because the high throughput of GPUs allows more data batches to be processed per unit time than CPUs, leading to more frequent network synchronization for distributed DL on GPUs. We show that Poseidon is applicable to different DL frameworks We show that Poseidon enables Caffe and TensorFlow to achieve 15, 5x speed-up on 16 single-GPU machines, even with limited bandwidth ( 10GbE ) and the challenging VGG19-22K network for image classification Moreover, Poseidon-enabled TensorFlow achieves 31, 5x speed-up with 32 single-GPU machines on Inception-V3, a 50 % improvement over the open-source TensorFlow ( 20x speed-up. We present Poseidon an efficient communication architecture Poseidon exploits the layered model structures in DL programs to overlap communication and computation, reducing bursty network communication Moreover, Poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer, according to layer properties and the number of machines. By plugging Poseidon into Caffe and TensorFlow.
2K_test_1339	Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled countless new applications in the domain of aerial sensing, In scenarios such as intrusion detection, target tracking and facility monitoring it is important to reach a given area of interest ( AOI ), and create an online data streaming connection to a monitoring ground station ( GS ) for immediate delivery of content to the operator, In previous work we showed that a multi-hop line network can increase the range of the mission by finding the optimal number of relay UAVs, and their optimal placement. Behaves poorly in this type of networks due to mutual interference. In this demo we show that CSMA ( typical 802, 11 's medium access protocol ) and that TDMA is a better alternative, We will also discuss how changing slot width online can overcome typical and less known TDMA inefficiencies, and therefore reach maximum end-to-end throughput and low delay.
2K_test_1340	Gaussian belief propagation ( BP ) has been widely used for distributed inference in large-scale networks such as the smart grid, sensor networks and social networks, where local measurements/observations are scattered over a wide geographical area One particular case is when two neighboring agents share a common observation, For example to estimate voltage in the direct current ( DC ) power flow model, the current measurement over a power line is proportional to the voltage difference between two neighboring buses. When applying the Gaussian BP algorithm to this type of problem, the convergence condition remains an open issue, In this paper we analyze the convergence properties. That the updating information matrix converges at a geometric rate to a unique positive definite matrix with arbitrary positive semidefinite initial value and further provide the necessary and sufficient convergence condition for the belief mean vector to the optimal estimate. Of Gaussian BP for this pairwise linear Gaussian model.
2K_test_1341	Developing a remote exploit is not easy, It requires a comprehensive understanding of a vulnerability and delicate techniques to bypass defense mechanisms, As a result attackers may prefer to reuse an existing exploit and make necessary changes over developing a new exploit from scratch, One such adaptation is the replacement of the original shellcode ( i, the attacker-injected code that is executed as the final step of the exploit ) in the original exploit with a replacement shellcode, resulting in a modified exploit that carries out the actions desired by the attacker as opposed to the original exploit author, We call this a shellcode transplant. Current automated shellcode placement methods are insufficient because they over-constrain the replacement shellcode, and so can not be used to achieve shellcode transplant, For example these systems consider the shellcode as an integrated memory chunk and require that the execution path of the modified exploit must be same as the original one, To resolve these issues, to achieve shellcode transplant. Among the 100 test cases, our system successfully generated 88 % of the exploits. We present ShellSwap a system that uses symbolic tracing, with a combination of shellcode layout remediation and path kneading. We evaluated the ShellSwap system on a combination of 20 exploits and 5 pieces of shellcode that are independently developed and different from the original exploit.
2K_test_1342	The availability of large idea repositories ( e, patent database ) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems Our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations. However finding useful analogies in these large, messy real-world repositories remains a persistent challenge for either human or automated methods, Previous approaches include costly hand-created databases that have high relational structure ( e, predicate calculus representations ) but are very sparse, Simpler machine-learning/information-retrieval similarity metrics can scale to large, natural-language datasets but struggle to account for structural similarity, which is central to analogy, In this paper we explore the viability and value of learning simpler structural representations. We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods, analogies retrieved by our models significantly increased people 's likelihood of generating creative ideas compared to analogies retrieved by traditional methods. Specifically `` problem schemas '', which specify the purpose of a product and the mechanisms by which it achieves that purpose, Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions. In an ideation experiment.
2K_test_1343	Our results significantly expand knowledge of eutherian genome evolution and will facilitate greater understanding of the role of chromosome rearrangements in adaptation, speciation and the etiology of inherited and spontaneously occurring diseases. Abstract Whole-genome assemblies of 19 placental mammals and two outgroup species were used to reconstruct the order and orientation of syntenic fragments in chromosomes of the eutherian ancestor and six other descendant ancestors leading to human, For ancestral chromosome reconstructions. Orangutan was found to have eight chromosomes that were completely conserved in homologous sequence order and orientation with the eutherian ancestor, the largest number for any species, Ruminant artiodactyls had the highest frequency of intrachromosomal rearrangements, and interchromosomal rearrangements dominated in murid rodents, A total of 162 chromosomal breakpoints in evolution of the eutherian ancestral genome to the human genome were identified ; however, the rate of rearrangements was significantly lower ( 0, 80/My ) during the first 60 My of eutherian evolution, then increased to greater than 2, 0/My along the five primate lineages studied. We developed an algorithm ( DESCHRAMBLER ) that probabilistically determines the adjacencies of syntenic fragments using chromosome-scale and fragmented genome assemblies, The reconstructed chromosomes of the eutherian, boreoeutherian and euarchontoglires ancestor each included > 80 % of the entire length of the human genome, whereas reconstructed chromosomes of the most recent common ancestor of simians, catarrhini great apes and humans and chimpanzees included > 90 % of human genome sequence These high-coverage reconstructions permitted reliable identification of chromosomal rearrangements over 105 My of eutherian evolution.
2K_test_1344	With the explosion in the availability of user-generated videos documenting any conflicts and human rights abuses around the world, analysts and researchers increasingly find themselves overwhelmed with massive amounts of video data to acquire and analyze useful information. For intense audio events in videos which addresses the problem. We show our framework 's efficacy on localizing intense audio event like gunshot, and further experiments also indicate that our methods can be generalized to localizing other audio events in noisy videos. In this paper we develop a temporal localization framework The proposed method utilizes Localized Self-Paced Reranking ( LSPaR ) to refine the localization results LSPaR utilizes samples from easy to noisier ones so that it can overcome the noisiness of the initial retrieval results from user-generated videos.
2K_test_1345	Previous work has replaced structural assumptions on the noise with a worst-case approach that aims to choose an outcome that minimizes the maximum error with respect to any feasible true ranking, This approach underlies algorithms that have recently been deployed on the social choice website RoboVote. We revisit the classic problem of designing voting rules that aggregate objective opinions, in a setting where voters have noisy estimates of a true ranking of the alternatives. We derive ( mostly sharp ) analytical bounds on the expected error and establish the practical benefits of our approach. We take a less conservative viewpoint by minimizing the average error with respect to the set of feasible ground truth rankings.
2K_test_1346	Low-income families pay substantial portions of their total expenditure on household energy bills, making them vulnerable to rising energy costs Habitat for Humanity houses are built for low-income families and made affordable with volunteer work and construction material donations While specific enclosure suggestions apply to this case-study, the utilized approach on exploring different options to identify opportunities to save energy can be used to understand impact on the lives of low-income families. Hence the trade-off between the homesO initial construction costs and their life-time energy costs must be evaluated carefully, This paper targets to support better-informed decisions that balance the affordability of certain construction materials with their potential for energy efficiency. The results show that it is possible to reduce the energy cost of these houses without significantly increasing the construction costs through exploration of different wall and window options. In collaboration with Habitat for Humanity of Westchester, we created an energy simulation model of an existing low-income house and calculated the homeOs annual energy usage with different design alternatives for windows and walls. The resulting estimated annual energy savings are then evaluated alongside their initial investment costs, which were retrieved from RS Means standard construction cost data and quotations from industry.
2K_test_1347	It is known that such allocations can be computed using O ( n ln ( 1/e ) ) operations in the standard Robertson-Webb Model, implies that allocations that are exactly equitable can not be computed. We are interested in the problem of dividing a cake -- a heterogeneous divisible good -- among n players, in a way that is e- equitable : every pair of players must have the same value for their own allocated pieces, up to a difference of at most e. We establish a lower bound of ( ln ( 1/e ) /lnln ( 1/e ) ) on the complexity of this problem, which is almost tight for a constant number of players.
2K_test_1348	In the era of social media, a large number of user-generated videos are uploaded to the Internet every day, capturing events all over the world. Reconstructing the event truth based on information mined from these videos has been an emerging challenging task Temporal alignment of videos in the wild which capture different moments at different positions with different perspectives is the critical step to synchronize videos. Show that the proposed method achieves excellent precision and robustness. In this paper we propose a hierarchical approach Our system utilizes clustered audio-signatures to align video pairs, Global alignment for all videos is then achieved via forming alignable video groups with self-paced learning. Experiments on the Boston Marathon dataset.
2K_test_1349	Mainstream crowdwork platforms treat microtasks as indivisible units We reflect on the implications of these findings for the design of future crowd work platforms that effectively harness the potential of subcontracting workflows. ; however in this article, we propose that there is value in re-examining this assumption We argue that crowdwork platforms can improve their value proposition for all stakeholders by supporting subcontracting within microtasks. Finally we describe the outcome of two tasks on Mechanical Turk meant to simulate aspects of subcontracting. : real-time assistance task management, and task improvement and reflect on potential use cases and implementation considerations associated with each. After describing the value proposition of subcontracting, we then define three models for microtask subcontracting.
2K_test_1351	The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals, Blind people can not independently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance. To address this problem. We introduce VizLens -- -a robust and interactive screen reader for interfaces in the real world, VizLens users take a picture of an interface they would like to use, it is interpreted quickly and robustly by multiple crowd workers in parallel, and then computer vision is able to give interactive feedback and guidance to users to help them use the interface in real time Built on top of VizLens, we developed automatically generating tactile overlays to physical interfaces to provide blind people with a permanent static solution. We introduce Facade -- -a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel.
2K_test_1352	Crowd work is an increasingly prevalent and important kind of work, Because of its flexible nature, crowd work may offer benefits for people with disabilities Given ongoing and upcoming changes to the world economy and technological progress, we believe it is important to find a way to make sure people with disabilities are able to equally participate in this kind of work. Unfortunately people with disabilities currently lack access to much of this work because the tasks that are posted are often inaccessible. In this paper we first characterize the accessibility of the tasks posted to a popular crowd marketplace, Amazon Mechanical Turk by performing manual and automatic checks on 120 tasks from several common types, We then outline research directions that could have positive impact on this problem.
2K_test_1353	Providing navigation assistance to people with visual impairments often requires augmenting the environment with after-market technology. However installing navigation infrastructure in large environments requires a critical mass of trained personnel, Recruiting training and managing participants for such a task is difficult, to recruit instruct and orchestrate volunteers to perform physical crowdsourcing tasks. LuzDeploy is a computational method We use LuzDeploy to orchestrate volunteers to install physical infrastructure for the navigation assistance of people with visual impairments Our system provides on-the-go enrollment so that volunteers can participate to the collective action whenever they have time, coming and leaving as needed, Providing automated instructions also allows to avoid instructing participants directly, so experts do not need to be available on-site.
2K_test_1354	What can humans compute in their heads ? We are thinking of a variety of Crypto Protocols, games like Sudoku Crossword Puzzles, Speed Chess and so on. The intent of this paper is to apply the ideas and methods of theoretical computer science to better understand what humans can compute in their heads, For example can a person compute a function in their head so that an eavesdropper with a powerful computer -- - who sees the responses to random input -- - still can not infer responses to new inputs ? To address such questions. We propose a rigorous model of human computation and associated measures of complexity We apply the model and measures first and foremost to the problem of ( 1 ) humanly computable password generation, and then consider related problems of ( 2 ) humanly computable `` one-way functions '' and ( 3 ) humanly computable `` pseudorandom generators '' The theory of Human Computability developed here plays by different rules than standard computability, and this takes some getting used to For reasons to be made clear, the polynomial versus exponential time divide of modern computability theory is irrelevant to human computation In human computability, the step-counts for both humans and computers must be more concrete, Specifically we restrict the adversary to at most 10^24 ( Avogadro number of ) steps, An alternate view of this work is that it deals with the analysis of algorithms and counting steps for the case that inputs are small as opposed to the usual case of inputs large-in-the-limit.
2K_test_1355	How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we exploit them for applications ? A k-core is the maximal subgraph in which all vertices have degree at least k, This concept has been applied to such diverse areas as hierarchical structure analysis, graph visualization and graph clustering. Here we explore pervasive patterns related to k-cores and emerging in graphs from diverse domains. Our discoveries are : ( 1 ) Mirror Pattern : coreness ( i, maximum k such that each vertex belongs to the k-core ) is strongly correlated with degree, ( 2 ) Core-Triangle Pattern : degeneracy ( i, maximum k such that the k-core exists ) obeys a 3-to-1 power-law with respect to the count of triangles, ( 3 ) Structured Core Pattern : degeneracycores are not cliques but have non-trivial structures such as coreperiphery and communities. Our algorithmic contributions show the usefulness of these patterns, ( 1 ) Core-A, which measures the deviation from Mirror Pattern, successfully spots anomalies in real-world graphs, ( 2 ) Core-D, a single-pass streaming algorithm based on Core-Triangle Pattern, accurately estimates degeneracy up to 12\ ( \times \ ) faster than its competitor ( 3 ) Core-S, inspired by Structured Core Pattern, identifies influential spreaders up to 17\ ( \times \ ) faster than its competitors with comparable accuracy.
2K_test_1356	Systems for providing mixed physical-virtual interaction on desktop surfaces have been proposed for decades, though no such systems have achieved widespread use. One major factor contributing to this lack of acceptance may be that these systems are not designed for the variety and complexity of actual work surfaces, which are often in flux and cluttered with physical objects. Demonstrating their imminent feasibility. In this paper we use an elicitation study and interviews to synthesize a list of ten interactive behaviors that desk-bound, digital interfaces should implement to support responsive cohabitation with physical objects As a proof of concept, we implemented these interactive behaviors in a working augmented desk system.
2K_test_1357	Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral, ) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph, Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look normal, Even worse some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. Show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud FRAUDAR successfully detected a subgraph of more than 4, 000 detected accounts of which a majority had tweets showing that they used follower-buying services. We propose FRAUDAR an algorithm that ( a ) is camouflage resistant, ( b ) provides upper bounds on the effectiveness of fraudsters, and ( c ) is effective in real-world data. Experimental results under various attacks Additionally, in real-world experiments with a Twitter follower -- followee graph of 1.
2K_test_1358	Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling which learns compact topic embeddings and captures topic correlations. Show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval. In this paper we propose a new model through the closeness between the topic vectors, Our method enables efficient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w, t the topic size, We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence.
2K_test_1360	Intelligent personalization systems are becoming increasingly reliant on contextually-relevant devices and services, such as those available within modern IoT deployments, An IoT context may emerge -- -or become pervasive -- -when the intelligent system generates knowledge from dialogue-based interactions with the end-user ; the context is strengthened even further by incorporating state representations about the environment ( e, generated from wireless sensor data ) into the knowledge graph, This is crucial for pervasive applications like digital assistance in IoT, where context-aware systems need to adapt quickly : activities like leaving work home-bound, driving to the grocery store, arriving at home and walking the dog, for example can occur in a relatively short period of time -- -during which an intelligent assistant must be able to support user requests in a consistent and coherent manner, Given that computational ontologies can serve as semantic models for heterogeneous data, they are becoming increasingly viable for reasoning across different IoT contexts, This involves : ( a ) federation and dynamic pruning of multiple modular ontologies, ideally to comprehensively capture only the knowledge that will facilitate execution of a multi-context task ; ( b ) fast consistency-checking and ontology-based inferences, aided by rules-based execution environments that can evaluate/transform ambient wireless sensor network ( WSN ) data, in real-time ; and ( c ) run-time execution of ontology-based control procedures, through rule-engine actuation commands sent across the WSN The approach we describe is also partially based on the Ubiquitous Personal Assistant ( UPA ) project, Bosch Research 's largest research initiative worldwide. Only by realizing these functionalities may intelligent systems be capable of reasoning over device properties, system states and user activities, while appropriately delegating commands to other intelligent agents or other relevant IoT services in order to enable scalable contextual reasoning for intelligent assistance. Preliminary results are also discussed. In this poster we illustrate how a multi-context knowledge base can be structured on the basis of modular ontologies and integrated with a distributed rules-based inference engine in multiple smart-building environments. This work is conducted through the partnership of Bosch Research Pittsburgh and Carnegie Mellon University ( CMU ), and is in partial satisfaction of CMU 's Bosch Energy Research Network ( BERN ) grant, awarded for developments in intelligent building solutions.
2K_test_1361	Audio transcription is an important task for making content accessible to people who are deaf or hard of hearing, Much of the transcription work is increasingly done by crowd workers, people online who pick up the work as it becomes available often in small bits at a time Whereas work typically provides a ladder for skill development -- a series of opportunities to acquire new skills that lead to advancement -- crowd transcription work generally does not, Our research demonstrates a new way for workers on crowd platforms to align their work and skill development with the accessibility domain while they work. To demonstrate how crowd work might create a skill ladder. We show that Scopist can distinguish touch-typing from stenotyping with 94 % accuracy. We created Scopist a JavaScript application for learning an efficient text-entry method known as stenotype while doing audio transcription tasks, Scopist facilitates on-the-job learning to prepare crowd workers for remote, real-time captioning by supporting both touch-typing and chording Real-time captioning is a difficult skill to master but is important for making live events accessible. We conducted 3 crowd studies of Scopist focusing on Scopist 's performance and support for learning.
2K_test_1362	Situational awareness involves the timely acquisition of knowledge about real-world events, distillation of those events into higher-level conceptual constructs, and their synthesis into a coherent context-sensitive view. For situational awareness in vehicular systems that span driverless and drivered vehicles. We explore how convergent trends in video sensing, crowd sourcing and edge computing can be harnessed to create a shared real-time information system.
2K_test_1364	Social media has become a popular and important tool for human communication, However due to this popularity, spam and the distribution of malicious content by computer-controlled users, known as bots has become a widespread problem, At the same time, when users use social media, they generate valuable data that can be used to understand the patterns of human communication. In this article we focus on the following important question : Can we identify and use patterns of human communication to decide whether a human or a bot controls a user ? fit the distribution of IATs that detects if users are bots based only on the timing of their postings. Is characterized by following four patterns : ( i ) heavy-tails, ( ii ) periodic-spikes, ( iii ) correlation between consecutive values, and ( iv ) bimodallity, Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics, Additionally when detecting bots, Act-M provided a precision higher than 93 % and 77 % with a sensitivity of 70 % for the Twitter and Reddit datasets. As our second contribution, we propose a mathematical model named Act-M ( Activity Model ) We show that Act-M can accurately from social media users Finally, we use Act-M to develop a method. The first contribution of this article is showing that the distribution of inter-arrival times ( IATs ) between postings We validate Act-M using data from over 55 million postings from four social media services : Reddit, Twitter Stack-Overflow and Hacker-News.
2K_test_1365	To infer the past, describe the present and predict the future. Demonstrate that our approach significantly outperforms the compared baselines. In this work we introduce Video Question Answering in the temporal domain We present an encoderdecoder approach using Recurrent Neural Networks to learn the temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions, In addition 390 744 corresponding questions are generated from annotations. We explore approaches for finer understanding of video content using the question form of fill-in-the-blank, and collect our Video Context QA dataset consisting of 109, 895 video clips with a total duration of more than 1000 h from existing TACoS, MPII-MD and MEDTest 14 datasets.
2K_test_1366	Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain, This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. Validates the accuracy of sentence and attribute generation. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes.
2K_test_1367	Nuclear organization has an important role in determining genome function ; however, it is not clear how spatiotemporal organization of the genome relates to functionality, We anticipate the general applicability and scalability of our method will enhance causative analyses between gene function and compartmentalization in a high-throughput manner. To elucidate this relationship, a high-throughput method for tracking any locus of interest is desirable, for live cell imaging of specific chromosomal regions. Was successful at labeling 9 different loci in HCT116 cells with up to 20 % efficiency These loci included both nuclear speckle-associated, euchromatin regions and nuclear lamina-associated. Here we report an efficient and scalable method named SHACKTeR ( Short Homology and CRISPR/Cas9-mediated Knock-in of a TetO Repeat Compared to alternatives, our method does not require a nearby repetitive sequence and it requires only two modifications to the genome : CRISPR/Cas9-mediated knock-in of an optimized TetO repeat and its visualization by TetR-EGFP expression, Our simplified knock-in protocol, utilizing short homology arms integrated by PCR.
2K_test_1368	Robust hand detection and classification is one of the most crucial pre-processing steps to support human computer interaction, driver behavior monitoring virtual reality. This problem however is very challenging due to numerous variations of hand images in real-world scenarios, to robustly detect and classify human hand regions under various challenging conditions. The experimental results show that our proposed MS-FRCN approach consistently achieves the state-of-the-art hand detection results, Average Precision ( AP ) / Average Recall ( AR ) of 95, 1 % / 94, 5 % at level 1 and 86, 0 % / 83, 4 % at level 2, on the VIVA challenge In addition, the proposed method achieves the state-of-the-art results for left/right hand and driver/passenger classification tasks on the VIVA database with a significant improvement on AP/AR of ~7 % and ~13 % for both classification tasks, respectively The hand detection performance of MS-RFCN reaches to 75, 1 % of AP and 77, 8 % of AR on Oxford database. This work presents a novel approach named Multiple Scale Region-based Fully Convolutional Networks ( MSRFCN ) In this approach, the whole image is passed through the proposed fully convolutional network to compute score maps Those score maps with their position-sensitive properties can help to efficiently address a dilemma between translation-invariance in classification and detection. The method is evaluated on the challenging hand databases, the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge, Oxford hand dataset and compared against various recent hand detection methods.
2K_test_1369	Abstract Server-side variability the idea that the same job can take longer to run on one server than another due to server-dependent factors isan increasingly important concern in many queueing systems, One strategy for overcoming server-side variability to achieve low response time is redundancy, under which jobs create copies of themselves and send these copies to multiple different servers, waiting for only one copy to complete service, Most of the existing theoretical work on redundancy has focused on developing bounds, approximations and exact analysis to study the response time gains offered by redundancy. However response time is not the only important metric in redundancy systems : in addition to providing low overall response time, the system should also be fair in the sense that no job class should have a worse mean response time in the system with redundancy than it did in the system before redundancy is allowed In this paper we use scheduling to address the simultaneous goals of ( 1 ) achieving low response time and ( 2 ) maintaining fairness across job classes, for per-class response time. Shows that FCFS can be unfair in that it can hurt non-redundant jobs, which is provably fair and also achieves excellent overall mean response time. We develop new exact analysis under First-Come First-Served ( FCFS ) scheduling for a general type of system structure ; We then introduce the Least Redundant First ( LRF ) scheduling policy, which we prove is optimal with respect to overall system response time, but which can be unfair in that it can hurt the jobs that become redundant, Finally we introduce the Primaries First ( PF ) scheduling policy.
2K_test_1371	Generative Adversarial Networks ( GANs ) have recently achieved significant improvement on paired/unpaired image-to-image translation, such as photo $ \rightarrow $ sketch and artist painting style transfer. However existing models can only be capable of transferring the low-level information ( e, color or texture changes ), but fail to edit high-level semantic meanings ( e, geometric structure or content ) of objects, On the other hand, while some researches can synthesize compelling real-world images given a class label or caption, they can not condition on arbitrary shapes or structures, which largely limits their application scenarios and interpretive capability of model results In this work, we focus on a more challenging semantic manipulation task, which aims to modify the semantic meaning of an object while preserving its own characteristics ( e, viewpoints and shapes ), such as cow $ \rightarrow $ sheep, motor $ \rightarrow $ bicycle, cat $ \rightarrow $ dog, To tackle such large semantic changes. Show considerable performance gain by our contrast-GAN over other conditional GANs Quantitative results further demonstrate the superiority of our model on generating manipulated results with high visual fidelity and reasonable object semantics. We introduce a contrasting GAN ( contrast-GAN ) with a novel adversarial contrasting objective, Instead of directly making the synthesized samples close to target data as previous GANs did, our adversarial contrasting objective optimizes over the distance comparisons between samples, that is enforcing the manipulated data be semantically closer to the real data with target category than the input data Equipped with the new contrasting objective, a novel mask-conditional contrast-GAN architecture is proposed to enable disentangle image background with object semantic changes. Experiments on several semantic manipulation tasks on ImageNet and MSCOCO dataset.
2K_test_1372	The influence of motor cortex on muscles during different behaviors is incompletely understood. In this issue of Neuron, ( 2017 ) show that the population activity patterns produced by motor cortex during different behaviors determine the selective routing of signals along different pathways between motor cortex and muscles.
2K_test_1373	Future frame prediction in videos is a promising avenue for unsupervised video representation learning, Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. Demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows, Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning. In this paper we develop a dual motion Generative Adversarial Net ( GAN ) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction, To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational.
2K_test_1374	How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time ? How do people join the social groups, uniformly or with burst ? What is the pattern of people quitting from groups ? Is there a simple universal model to depict the come-and-go patterns of various groups ? for group evolution. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR, For example we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore we propose a new model come N go, a concise yet flexible dynamic model Our model has the following advantages : ( a ) Unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation, ( b ) Succinctness and interpretability : it contains only six parameters with clear physical meanings ( c ) Accuracy : it can capture various kinds of group evolution patterns preciously, and the goodness of fit increases by 58 % over baseline, ( d ) Usefulness : it can be used in multiple application scenarios, such as forecasting and pattern discovery Furthermore, our model can provide insights about different evolution patterns of social groups, and we also find that group structure and its evolution has notable relations with temporal patterns of group evolution. In this article we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users.
2K_test_1375	Computer vision based technologies have seen widespread adoption over the recent years, This use is not limited to the rapid adoption of facial recognition technology but extends to facial expression recognition, scene recognition and more. These developments raise privacy concerns and call for novel solutions to ensure adequate user awareness, and ideally control over the resulting collection and use of potentially sensitive data, While cameras have become ubiquitous, most of the time users are not even aware of their presence, enhance user 's awareness of and control over the collection and use of video data about them. In this paper we introduce a novel distributed privacy infrastructure for the Internet-of-Things and discuss in particular how it can help The infrastructure, supports the automated discovery of IoT resources and the selective notification of users This includes the presence of computer vision applications that collect data about users, In particular we describe an implementation of functionality that helps users discover nearby cameras and choose whether or not they want their faces to be denatured in the video streams. Which has undergone early deployment and evaluation on two campuses.
2K_test_1376	Information cascades are ubiquitous in both physical society and online social media, taking on large variations in structures, Our discoveries also provide a foundation for the microscopic mechanisms for information spreading, potentially leading to implications for cascade prediction and outlier detection. Although the dynamics and semantics of information cascades have been studied, the structural patterns and their correlations with dynamics and semantics are largely unknown to quantify the structural characteristics of information cascades. We find that the structural complexity of information cascades is far beyond the previous conjectures We find that bimodal law governs majority of the metrics, information flows in cascades have four directions, and the self-loop number and average activity of cascades follows power law and finally uncover some notable implications of structural patterns in information cascades. We first propose a ten-dimensional metric, reflecting cascade size silhouette, direction and activity aspects. Here we explore a large-scale dataset including $ 432 $ million information cascades with explicit records of spreading traces, spreading behaviors information content as well as user profiles We then analyze the high-order structural patterns of information cascades Finally, we evaluate to what extent the structural features of information cascades can explain its dynamic patterns and semantics.
2K_test_1377	Extreme Classification comprises multi-class or multi-label prediction where there is a large number of classes, and is increasingly relevant to many real-world applications such as text and image tagging. In this setting standard classification methods, with complexity linear in the number of classes, become intractable while enforcing structural constraints among classes ( such as low-rank or tree-structure ) to reduce complexity often sacrifices accuracy for efficiency, The recent PD-Sparse method addresses this via an algorithm that is sub-linear in the number of variables, by exploiting primal-dual sparsity inherent in a specific loss function, namely the max-margin loss to be efficiently parallelized in large-scale distributed settings. Our proposed method achieves accuracy competitive to the state-of-the-art while reducing the training time from days to tens of minutes compared with existing parallel or sparse methods on a cluster of 100 cores. In this work we extend PD-Sparse, By introducing separable loss functions, we can scale out the training, with network communication and space efficiency comparable to those in one-versus-all approaches while maintaining an overall complexity sub-linear in the number of classes. On several large-scale benchmarks.
2K_test_1378	Time-limited promotions that exploit consumers ' sense of urgency to boost sales account for billions of dollars in consumer spending each year. However it is challenging to discover the right timing and duration of a promotion to increase its chances of being redeemed, In this work we consider the problem of delivering time-limited discount coupons, where we partner with a large national bank functioning as a commission-based third-party coupon provider to model consumer spending and forecast future purchases. And show that RUSH ! provides higher expected value than various baselines that do not jointly model time and category information. Specifically we use large-scale anonymized transaction records, based on which we generate data-driven, Our proposed model RUSH ( 1 ) predicts { both the time and category } of the next event ; ( 2 ) captures correlations between purchases in different categories ( such as shopping triggering dining purchases ) ; ( 3 ) incorporates temporal dynamics of purchase behavior ( such as increased spending on weekends ) ; ( 4 ) is composed of additive factors that are easily interpretable ; and finally ( 5 ) scales linearly to millions of transactions. We design a cost-benefit framework that facilitates systematic evaluation in terms of our application.
2K_test_1379	How do people make friends dynamically in social networks ? What are the temporal patterns for an individual increasing its social connectivity ? What are the basic mechanisms governing the formation of these temporal patterns ? No matter cyber or physical social systems, their structure and dynamics are mainly driven by the connectivity dynamics of each individual, Our model and discoveries provide a foundation for the microscopic mechanisms of network growth dynamics, potentially leading to implications for prediction, clustering and outlier detection on human dynamics. However due to the lack of empirical data, little is known about the empirical dynamic patterns of social connectivity at microscopic level, let alone the regularities or models governing these microscopic dynamics. We uncover a wide range of long-term power law growth and short-term bursty growth for the social connectivity of different users, We propose three key ingredients, namely average-effect multiscale-effect and correlation-effect, which govern the observed growth patterns at microscopic level, we discover statistical regularities underlying the empirical growth dynamics. As a result we propose the long short memory process incorporating these ingredients, demonstrating that it successfully reproduces the complex growth patterns observed in the empirical data. We examine the detailed growth process of `` WeChat '', the largest online social network in China, with 300 million users and 4, 75 billion links spanning two years, By analyzing modeling parameters.
2K_test_1380	With the pervasiveness of mobile technology and location-based computing, new forms of smart urban transportation, such as Uber & Lyft, have become increasingly popular, These new forms of urban infrastructure can influence individuals ' movement frictions and patterns, in turn influencing local consumption patterns and the economic performance of local businesses. To gain insights about future impact of urban transportation changes to present of how the emerging growth of peer-to-peer car sharing services may have affected local consumer mobility and consumption patterns. In this paper we utilize a novel dataset and econometric analysis methods.
2K_test_1381	This generalizes a prior decomposition result for an M/M/k/staggeredM/M/k/staggered-setup. We consider the M/G/k/staggeredM/G/k/staggered-setup, where idle servers are turned off to save cost, necessitating a setup time for turning a server back on ; however, at most one server may be in setup mode at any time. We show that the response time of an M/G/k/staggeredM/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/kM/G/k and the setup time, where the approximation is nearly exact. For exponentially distributed setup times.
2K_test_1382	Quickly converting speech to text allows deaf and hard of hearing people to interactively follow along with live speech Doing so reliably requires a combination of perception, understanding and speed that neither humans nor machines possess alone Scribe illustrates the broad potential for deeply interleaving human labor and machine intelligence to provide intelligent interactive services that neither can currently achieve alone. To reliably convert speech to text with less than 4s latency, To achieve this speed while maintaining high accuracy. In this article we discuss how our Scribe system combines human labor and machine intelligence in real time, Scribe integrates automated assistance in two ways, First its user interface directs workers to different portions of the audio stream, slows down the portion they are asked to type, and adaptively determines segment length based on typing speed, Second it automatically merges the partial input of multiple workers into a single transcript using a custom version of multiple-sequence alignment.
2K_test_1383	The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore makes the video captioning task even more challenging. The results demonstrate the effectiveness of our proposed model, M & M TGM not only outperforms prior state-of-the-art methods but also achieves better generalization ability. In this paper we propose an unified caption framework, M & M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics, Compared to pre-defined topics, the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better, We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction in addition to the caption task, For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos, The topic prediction provides intermediate supervision to the learning process, As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics, The entire learning procedure is end-to-end and it optimizes both tasks simultaneously. From extensive experiments conducted on the MSR-VTT and Youtube2Text datasets on multiple evaluation metrics and on both benchmark datasets.
2K_test_1384	Data association which could be categorized into offline approaches and the online counterparts, is a crucial part of a multi-object tracker in the tracking-by-detection framework. On the one hand, classical offline data association methods exploit all the video data and have high computation cost, which makes them unscalable to long-term offline video data, On the other hand, online approaches have much lower computation cost, but they suffer from ID-switches and tracklet drifting problem when directly applied to offline data as they are only aware of past observations. Results show that our approach achieves the state-of-the-art performance on challenging datasets. In this paper we propose a mixed style tracker, which is not only as efficient as the online tracker but also aware of future observations in offline setting, We start from a Markov Decision Process ( MDP ) online tracker and design a parallelized apprenticeship learning algorithm to learn both the reward function and transition policy in MDP, By proposing a rewind to track strategy to generate backward tracklets, future detections in offline data are efficiently utilized to obtain a more stable similarity measurement for association.
2K_test_1385	As an important branch of multimedia content analysis, Surveillance Event Detection ( SED ) is still a quite challenging task due to high abstraction and complexity such as occlusions, cluttered backgrounds and viewpoint changes etc, To address the problem. Demonstrate the effectiveness of proposed framework. We propose a unified SED detection framework which divides events into two categories, short-term events and long-duration events The former can be represented as a kind of snapshots of static key-poses and embodies an inner-dependencies, while the latter contains complex interactions between pedestrians, and shows obvious inter-dependencies and temporal context, For short-term event a novel cascade Convolutional Neural Network ( CNN ) -HsNet is first constructed to detect the pedestrian, and then the corresponding events are classified, For long-duration event Dense Trajectory ( DT ) and Improved Dense Trajectory ( IDT ) are first applied to explore the temporal features of the events respectively, and subsequently Fisher Vector ( FV ) coding is adopted to encode raw features and linear SVM classifiers are learned to predict. Finally a heuristic fusion scheme is used to obtain the results In addition, a new large-scale pedestrian dataset, named SED-PD is built for evaluation, Comprehensive experiments on TRECVID SEDtest datasets.
2K_test_1386	Genome-wide association studies have discovered a large number of genetic variants associated with complex diseases such as Alzheimers disease. However the genetic background of such diseases is largely unknown due to the complex mechanisms underlying genetic effects on traits, as well as a small sample size ( e, 1000 ) and a large number of genetic variants ( e, 1 million ) Fortunately, datasets that contain genotypes, transcripts and phenotypes are becoming more readily available, creating new opportunities for detecting disease-associated genetic variants for detecting three-way associations among genotypes. We demonstrate that BTAM significantly improves the statistical power over forward three-way association mapping that finds genotypes associated with both transcripts and phenotypes and genotype-phenotype association mapping and report top 10 genotype-transcript-phenotype associations. In this paper we present a novel approach called Backward Three-way Association Mapping ( BTAM ) Assuming that genotypes affect transcript levels, which in turn affect phenotypes, we first find transcripts associated with the phenotypes, and then find genotypes associated with the chosen transcripts, The backward ordering of association mappings allows us to avoid a large number of association testings between all genotypes and all transcripts, making it possible to identify three-way associations with a small computational cost. In our simulation study, Furthermore we apply BTAM on an Alzheimers disease dataset.
2K_test_1387	The Next-Generation Airborne Collision Avoidance System ACASi ? X is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft, It is currently being developed by the Federal Aviation Administration FAA, Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety. Under which the advice given by ACAS X is safe. In this paper we determine the geometric configurations under a precise set of assumptions and formally verify these configurations using hybrid systems theorem proving techniques, We conduct an initial examination of the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version, demonstrating how formal hybrid approaches are helping ensure the safety of ACAS X.
2K_test_1388	Recently to solve large-scale lasso and group lasso problems, screening rules have been developed, the goal of which is to reduce the problem size by efficiently discarding zero coefficients using simple rules independently of the others. However screening for overlapping group lasso remains an open challenge because the overlaps between groups make it infeasible to test each group independently To address the challenge arising from groups with overlaps. We demonstrate the efficiency of our screening rules. In this paper we develop screening rules for overlapping group lasso we take into account overlapping groups only if they are inclusive of the group being tested, and then we derive screening rules, adopting the dual polytope projection approach This strategy allows us to screen each group independently of each other. In our experiments on various datasets.
2K_test_1389	Facility location and committee selection are classic embodiments of this problem. We consider the mechanism design problem for agents with single-peaked preferences over multi-dimensional domains when multiple alternatives can be chosen derive worst-case approximation ratios for social cost and maximum load for optimizing the choice of percentiles relative to any prior distribution over preferences. Demonstrate the viability of this approach and the value of such optimized mechanisms vis-a-vis mechanisms derived through worst-case analysis. We propose a class of percentile mechanisms, a form of generalized median mechanisms, that are strategy-proof and for L1 and L2 cost models More importantly, we propose a sample-based framework. Our empirical investigations using social cost and maximum load as objectives.
2K_test_1390	Controlling a registered-user session of a registered user on a device. Using first and second authentication processes and a handoff from the first process to the second process, In one embodiment the first authentication process is a stronger process performed at the outset of a session, and the second authentication process is a weaker process iteratively performed during the session, The stronger authentication process may require cooperation from the user, while the weaker authentication process is preferably one that requires little or no user cooperation, In other embodiments a strong authentication process may be iteratively performed during the session.
2K_test_1391	Social choice theory provides insights into a variety of collective decision making settings. But nowadays some of its tenets are challenged by internet environments, which call for dynamic decision making under constantly changing preferences. In this paper we model the problem via Markov decision processes ( MDP ), where the states of the MDP coincide with preference profiles and a ( deterministic, stationary ) policy corresponds to a social choice function We can therefore employ the axioms studied in the social choice literature as guidelines in the design of socially desirable policies, We present tractable algorithms that compute optimal policies under different prominent social choice constraints, Our machinery relies on techniques for exploiting symmetries and isomorphisms between MDPs.
2K_test_1392	While the analysis of unlabeled networks has been studied extensively in the past, finding patterns in different kinds of labeled graphs is still an open challenge. Given a large edge-labeled network, a time-evolving network how can we find interesting patterns ? which can discover communities appearing over subsets of the labels. We show that Com $ $ ^2 $ $ 2 spots intuitive patterns regarding edge labels that carry temporal or other discrete information, Our findings include large `` star '' -like patterns, near-bipartite cores as well as tiny groups ( five users ), calling each other hundreds of times within a few days, We also show that we are able to automatically identify competing airline companies. We propose Com $ $ ^2 $ $ 2, a novel fast and incremental tensor analysis approach The method is ( a ) scalable, being linear on the input size, ( b ) general, ( c ) needs no user-defined parameters and ( d ) effective, returning results that agree with intuition. We apply our method to real datasets, including a phone call network, a computer-traffic network and a flight information network, The phone call network consists of 4 million mobile users, with 51 million edges ( phone calls ), over 14 days while the flights dataset consists of 7733 airports and 5995 airline companies flying 67.
2K_test_1393	The Pitman-Yor process provides an elegant way to cluster data that exhibit power law behavior, where the number of clusters is unknown or unbounded. Unfortunately inference in Pitman-Yor process-based models is typically slow and does not scale well with dataset size, to develop parallel inference algorithms that distribute inference both on the data space and the model space. We show that our method scales well with increasing data while avoiding any degradation in estimate quality. In this paper we present new auxiliary-variable representations for the Pitman-Yor process and a special case of the hierarchical Pitman-Yor process that allows us.
2K_test_1394	In this paper we investigate information validation tasks that are initiated as queries from either automated agents or humans new online information validation technique, to automatically evaluate the truth of queries. We show that OpenEval is able to respond to the queries within a limited amount of time while also achieving high F1 score In addition, we show that the accuracy of responses provided by OpenEval is increased as more time is given for evaluation, that illustrate the effectiveness of our approach compared to related techniques. We introduce OpenEval a which uses information on the web that are stated as multiargument predicate instances ( e, DrugHasSideEffect ( Aspirin GI Bleeding ) ) ), OpenEval gets a small number of instances of a predicate as seed positive examples and automatically learns how to evaluate the truth of a new predicate instance by querying the web and processing the retrieved unstructured web pages. We have extensively tested our model and shown empirical results.
2K_test_1395	How can we correlate the neural activity in the human brain as it responds to typed words, with properties of these terms ( like edible, fits in hand ) ? In short, we want to find latent variables, that jointly explain both the brain activity, as well as the behavioral responses, This is one of many settings of the Coupled Matrix-Tensor Factorization ( CMTF ) problem. Can we accelerate any CMTF solver, so that it runs within a few minutes instead of tens of hours to a day, while maintaining good accuracy ? capable of doing exactly that. By up to 200, along with an up to 65 fold increase in sparsity, with comparable accuracy to the baseline, TURBO-SMT is able to find meaningful latent variables, as well as to predict brain activity with competitive accuracy. We introduce TURBO-SMT a meta-method : it boosts the performance of any CMTF algorithm. We apply TURBO-SMT to BRAINQ, a dataset consisting of a ( nouns, brain voxels human subjects ) tensor and a ( nouns, properties ) matrix with coupling along the nouns dimension.
2K_test_1396	The typical approach thus far is to use tensors or dynamical systems. Given electroencephalogram time series data from patients with epilepsy, can we find patterns and regularities ?. EEG-MINE ( a ) can successfully reconstruct the signals with high accuracy ; ( b ) can spot surprising patterns within seizure EEG signals ; and ( c ) may provide early warning of epileptic seizures. Here we present EEG-MINE, a nonlinear chaos-based `` gray box model '', that blends domain knowledge with data observations. When applied to numerous.
2K_test_1398	Live music performance with computers has motivated many research projects in science, engineering and the arts, We conclude with directions for future work. In spite of decades of work, it is surprising that there is not more technology for, and a better understanding of the computer as music performer, Our goal is to enable musicians to ncorporate computers into performances easily and effectively through a better understanding of requirements, new techniques and practical. Outline our efforts to establish a new direction, Human-Computer Music Performance ( HCMP ), as a framework for a variety of coordinated studies, Our work in this area spans performance analysis, synchronization techniques and interactive performance systems. We review the development of techniques for live music performance and.
2K_test_1399	Matching function binaries -- the process of identifying similar functions among binary executables -- is a challenge that underlies many security applications such as malware analysis and patch-based exploit generation, Recent work tries to establish semantic similarity based on static analysis methods. Unfortunately these methods do not perform well if the compared binaries are produced by different compiler toolchains or optimization levels, dynamic equivalence testing primitive that identifies similar functions across optimization boundaries. BLEX outperforms BinDiff by up to 3, 5 times in correctly identifying similar functions BLEX also outperforms BinDiff if the binaries have been compiled by different compilers Averaged over all indexed functions, our search engine ranks the correct matches among the top ten results 77 % of the time. In this work we propose blanket execution, a novel that achieves complete coverage by overriding the intended program logic, Blanket execution collects the side effects of functions during execution under a controlled randomized environment, Two functions are deemed similar, if their corresponding side effects, as observed under the same environment, We implement our blanket execution technique in a system called BLEX, Using the functionality in BLEX, we have also built a binary search engine. We evaluate BLEX rigorously against the state of the art binary comparison tool BinDiff, When comparing optimized and un-optimized executables from the popular GNU coreutils package.
2K_test_1400	Complex systems are designed using the model-based design paradigm in which mathematical models of systems are created and checked against specifications, Cyber-physical systems ( CPS ) are complex systems in which the physical environment is sensed and controlled by computational or cyber elements possibly distributed over communication networks, Various aspects of CPS design such as physical dynamics, software control and communication networking must interoperate correctly for correct functioning of the systems, Modeling formalisms analysis techniques and tools for designing these different aspects have evolved ind ependently, and remain dissimilar and disparate, There is no unifying formalism in which one can model all these aspects equally well, In current practice there is no principled approach that deals with this modeling heterogeneity within a formal framework. Therefore model-based design of CPS must make use of a collection of models in several different formalisms and use respective analysis methods and tools together to ensure correct system design, To enable doing this in a formal manner for multi-model verification of cyber-physical systems. Composition of analysis results. This thesis develops a framework based on behavioral semantics Heterogeneity arising from the different interacting aspects of CPS design must be addressed in order to enable system-level verification We develop behavioral semantics to address heterogeneity in a general yet formal manner, Our framework makes no assumptions about the specifics of any particular formalism, therefore it readily supports various formalisms, Models can be analyzed independently in isolation, supporting separation of concerns, Mappings across heterogeneous semantic domains enable associations between analysis results, Interdependencies across different models and specifications can be formally represented as constraints over parameters and verification can be carried out in a semantically consistent manner. Is supported both hierarchically across different levels of abstraction and structurally into interacting component models at a given level of abstraction, The theoretical concepts developed in the thesis are illustrated using a case study on the hierarchical heterogeneous verification of an automotive intersection collision avoidance system.
2K_test_1402	Randomly mutating well-formed program inputs or simply fuzzing, is a highly effective and widely used strategy to find bugs in software, Other than showing fuzzers find bugs, there has been little systematic effort in understanding the science of how to fuzz properly. In this paper we focus on how to mathematically formulate and reason about one critical aspect in fuzzing : how best to pick seed files to maximize the total number of bugs found during a fuzz campaign. Overall we find 240 bugs in 8 applications and show that the choice of algorithm can greatly increase the number of bugs found We also show that current seed selection strategies as found in Peach may fare no better than picking seeds at random, We make our data set and code publicly available. We design six different algorithms using over 650 CPU days on Amazon Elastic Compute Cloud ( EC2 ). And evaluate to provide ground truth data.
2K_test_1403	A reliable and accurate biometric identification system must be able to distinguish individuals even in situations where their biometric signatures are very similar. However the strong similarity in the facial appearance of twins has complicated facial feature based recognition and has even compromised commercial face recognition systems, This paper addresses the above problem and to distinguish identical twins. Indicate that both our proposed approaches achieve high identification rates and are hence quite promising at distinguishing twins. Proposes two novel methods using ( 1 ) facial aging features and ( 2 ) asymmetry decomposition features, Facial aging features are extracted using Gabor filters from regions of the face that typically exhibit wrinkles and laugh lines, while Facial asymmetry decomposition based features are obtained by projecting the difference between the two left sides ( consisting of the left half of the face and its mirror ) and two right sides ( consisting of the right half of the face and its mirror ) of a face onto a subspace, Feature vectors obtained using these methods were used for classification HighlightsThe proposal of two novel approaches to distinguishing identical twins, Facial aging and intrinsic facial symmetry features are sued, A thorough evaluation on a challenging database, The summarizing of existing techniques and the results obtained by them. Experiments conducted on images of five types of twins from the University of Notre Dame ND-Twins database.
2K_test_1404	Function identification is a fundamental challenge in reverse engineering and binary program analysis, For instance binary rewriting and control flow integrity rely on accurate function detection and identification in binaries. Although many binary program analyses assume functions can be identified a priori, identifying functions in stripped binaries remains a challenge. We found that BYTE-WEIGHT missed 44, 621 functions in comparison with the 266, 672 functions missed by the industry-leading tool IDA, Furthermore while IDA misidentified 459, 247 functions BYTEWEIGHT misidentified only 43. In this paper we propose BYTEWEIGHT, a new algorithm Our approach automatically learns key features for recognizing functions and can therefore easily be adapted to different platforms, new compilers and new optimizations. We evaluated our tool against three well-known tools that feature function identification : IDA, Our data set consists of 2, 200 binaries created with three different compilers, with four different optimization levels, and across two different operating systems, In our experiments with 2.
2K_test_1405	For single-channel source separation to estimate the correlations between these features and the unobserved signal decomposition, to provide itemized energy usage. We demonstrate that contextual supervision improves significantly over a reasonable baseline and existing unsupervised methods for source separation and show that recovery of the signal components depends only on cross-correlation between features for different signals, not on correlations between features for the same signal. We propose a new framework that lies between the fully supervised and unsupervised setting, Instead of supervision we provide input features for each source signal and use convex methods Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision ; our motivating application is energy disaggregation of hourly smart meter data ( the separation of whole-home power signals into different energy uses ), Here contextual supervision allows us for thousands homes, a task previously impossible due to the need for specialized data collection hardware. On smaller datasets which include labels, Finally we analyze the case of l2 loss theoretically.
2K_test_1406	Computing equilibria of games is a central task in computer science, A large number of results are known for Nash equilibrium ( NE ). However these can be adopted only when coalitions are not an issue, When instead agents can form coalitions, NE is inadequate and an appropriate solution concept is strong Nash equilibrium ( SNE ), Few computational results are known about SNE, In this paper we first study the problem of verifying whether a strategy profile is an SNE. Showing that the problem is in P, We then design a spatial branch -- and -- bound algorithm to find an SNE. And we experimentally evaluate the algorithm.
2K_test_1407	This paper explores how to find, track and learn models of arbitrary objects in a video without a predefined method for object detection. Where we show performance comparable to state of the art detector-based methods. We present a model that localizes objects via unsupervised tracking while learning a representation of each object, avoiding the need for pre-built detectors, Our model uses a dependent Dirichlet process mixture to capture the uncertainty in the number and appearance of objects and requires only spatial and color video data that can be efficiently extracted via frame differencing, We give two inference algorithms for use in both online and offline settings, and use them to perform accurate detection-free tracking on multiple real videos. We demonstrate our method in difficult detection scenarios involving occlusions and appearance shifts, on videos containing a large number of objects, and on a recent human-tracking benchmark.
2K_test_1408	Given the re-broadcasts ( i, retweets ) of posts in Twitter, how can we spot fake from genuine user reactions ? What will be the tell-tale sign the connectivity of retweeters, their relative timing or something else ? High retweet activity indicates influential users, and can be monetized, Hence there are strong incentives for fraudulent users to artificially boost their retweets ' volume. Here we explore the identifi- cation of fraudulent and genuine retweet threads. Our main contribu- tions are : ( a ) the discovery of patterns that fraudulent activity seems to follow ( the `` triangles `` a nd `` homogeneity '' patterns, the formation of micro-clusters in appropriate feature spaces ) ; and. ( b ) `` RTGen '', a realistic generator that mimics the behaviors of both honest and fraud- ulent users. We present experiments on a dataset of more than 6 million retweets crawled from Twitter.
2K_test_1409	Safety critical systems often have shutdown mechanisms to bring the system to a safe state in the event of a malfunction. To reduce the frequency of safety shutdowns. Shows that using a rate-limited ride-through bound permits a tighter safety limit on speed than a xed threshold without creating false alarm shutdowns resulted in im- proved detection of speed limit violations and shorter shutdown stopping distances without needing to increase the false alarm shutdown rate. We ex- amine the use of ride-through, a technique by allowing small transient violations of safety rules Adding state machines to select speci c safety bounds based on vehicle state accommodates expected control system transients. An illustrative example of enforcing a speed limit for an autonomous vehicle Testing these principles on an autonomous utility vehicle.
2K_test_1411	Many vision tasks require a multi-class classifier to discriminate multiple categories, on the order of hundreds or thousands for large-scale multi-class classification. Empirical results demonstrate the effectiveness of our proposed approach. In this paper we propose sparse output coding, a principled way by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem, Specifically sparse output coding is composed of two steps : efficient coding matrix learning with scalability to thousands of classes. On object recognition and scene classification.
2K_test_1413	Given a multimillion-node social network, how can we sum- marize connectivity pattern from the data, and how can we find unex- pected user behavior ?. In this paper we study a complete graph from a large who-follows-whom network and spot lockstep behavior that large groups of followers connect to the same groups of followees, to detect users who offer the lockstep behavior. We discover that ( a ) the lockstep behavior on the graph shapes dense `` block '' in its adjacency matrix and creates `` ray '' in spectral subspaces, and ( b ) partially overlapping of the behavior shapes `` staircase '' in the matrix and creates `` pearl '' in the subspaces, We demonstrate that our approach is effective. The second contribution is that we provide a fast algorithm, using the discovery as a guide for practi- tioners. Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep, on both synthetic and real data.
2K_test_1414	Out of the many potential factors that determine which links form in a document citation network, two in particular are of high importance : first, a document may be cited based on its subject matterthis can be modeled by analyzing document content ; second, a document may be cited based on which other documents have previously cited itthis can be modeled by analyzing citation structure, Both factors are important for users to make informed decisions and choose appropriate citations as the network grows. Our model can be used to effectively explore a citation network and provide meaningful explanations for links while still maintaining competitive citation prediction performance. In this paper we present a novel model that integrates the merits of content and citation analyses into a single probabilistic framework. We demonstrate our model on three real-world citation networks, Compared with existing baselines.
2K_test_1416	Automatically recognizing a large number of action categories from videos is of significant importance for video understanding. Most existing works focused on the design of more discriminative feature representation, and have achieved promising results when the positive samples are enough, However very limited efforts were spent on recognizing a novel action without any positive exemplars, which is often the case in the real settings due to the large amount of action classes and the users ' queries dramatic variations, To address this issue. Validate the superiority of our method over fully-supervised approaches using few positive exemplars. We propose to perform action recognition when no positive exemplars of that class are provided, which is often known as the zero-shot learning, Different from other zero-shot learning approaches, which exploit attributes as the intermediate layer for the knowledge transfer, our main contribution is SIR, which directly leverages the semantic inter-class relationships between the known and unknown actions followed by label transfer learning, The inter-class semantic relationships are automatically measured by continuous word vectors, which learned by the skip-gram model using the large-scale text corpus. Extensive experiments on the UCF101 dataset.
2K_test_1418	With the goal of improving the quality of life for people suffering from various motor control disorders, brain-machine interfaces provide direct neural control of prosthetic devices by translating neural signals into control signals, These systems act by reading motor intent signals directly from the brain and using them to control, for example the movement of a cursor on a computer screen, Over the past two decades, much attention has been devoted to the decoding problem : how should recorded neural activity be translated into the movement of the cursor ? These results have implications for understanding how motor neurons are recruited to perform various tasks, and may lend insight into the brain 's ability to conceptualize artificial systems. Most approaches have focused on this problem from an estimation standpoint, decoders are designed to return the best estimate of motor intent possible, under various sets of assumptions about how the recorded neural signals represent motor intent, investigate how various classes of decoders lead to different types of physical systems for the subject to control. This framework leads to new interpretations of why certain types of decoders have been shown to perform better than others. Here we recast the decoder design problem from a physical control system perspective.
2K_test_1419	To predict accurately trust relationships of a target user even if he/she does not have much interaction information. We propose a novel method The proposed method considers positive, implicit and negative information of all users in a network based on belief propagation to predict trust relationships of a target user.
2K_test_1421	Given the retweeting activity for the posts of several Twitter users, how can we distinguish organic activity from spammy retweets by paid followers to boost a post 's appearance of popularity ? More gen- erally, given groups of observations, can we spot strange groups ?. Our main intuition is that organic behavior has more variability, while fraud- ulent behavior, like retweets by botnet members, We refer to the detection of such synchronized observations as the Syn- chonization Fraud problem, and we study a specific instance of it, Retweet Fraud Detection manifested in Twitter, for detecting group fraud for characterizing retweet threads. Our method achieves a 97 % accuracy on a real dataset of 12 million retweets crawled from Twitter. Here we propose : ( A ) ND-Sync, an efficient method and ( B ) a set of carefully designed features ND-Sync is effec- tive in spotting retweet fraudsters, robust to different types of abnormal activity, and adaptable as it can easily incorporate additional features.
2K_test_1422	Age-estimation of a face of an individual in order to improve the accuracy of age-estimation over the current techniques. Is represented in image data In one embodiment, age-estimation techniques involves combining a Contourlet Appearance Model ( CAM ) for facial-age feature extraction and Support Vector Regression ( SVR ) for learning aging rules. In a particular example, characteristics of input facial images are converted to feature vectors by CAM, then these feature vectors are analyzed by an aging-mechanism-based classifier to estimate whether the images represent faces of younger or older people prior to age-estimation, the aging-mechanism-based classifier being generated in one embodiment by running Support Vector Machines ( SVM ) on training images, In an exemplary binary youth/adult classifier, faces classified as adults are passed to an adult age-estimation function and the others are passed to a youth age-estimation function.
2K_test_1423	Refactoring of code is a common device in software engineering, As cyber-physical systems CPS become ever more complex, similar engineering practices become more common in CPS development, Proper safe developments of CPS designs are accompanied by a proof of correctness. Since the inherent complexities of CPS practically mandate iterative development, frequent changes of models are standard practice, but require reverification of the resulting models after every change, To overcome this issue. For some of these we can give strong results that they are correct. We develop proof-aware refactorings for CPS, That is we study model transformations on CPS and show how they correspond to relations on correctness proofs, As the main technical device, we show how the impact of model transformations on correctness can be characterized by different notions of refinement in differential dynamic logic Furthermore, we demonstrate the application of refinements on a series of safety-preserving and liveness-preserving refactorings. By proving on a meta-level Where this is impossible, we construct proof obligations for showing that the refactoring respects the refinement relation.
2K_test_1424	Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender 's randomized assignment of resources to targets, implies that in some realistic situations, limited surveillance may not need to be explicitly addressed. This assumption has been challenged by recent papers, which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance. That in zero-sum security games, lazy defenders who simply keep optimizing against perfectly informed attackers, are almost optimal against diligent attackers, who go to the effort of gathering a reasonable number of observations.
2K_test_1425	Motivated by the success of CNNs in object recognition on images, researchers are striving to develop CNN equivalents for learning video features. However learning video features globally has proven to be quite a challenge due to the difficulty of getting enough labels, processing large-scale video data, and representing motion information. Results show competitive performance. Therefore we propose to leverage effective techniques from both data-driven and data-independent approaches to improve action recognition system, Our contribution is three-fold, First we explicitly show that local handcrafted features and CNNs share the same convolution-pooling network structure, Second we propose to use independent subspace analysis ( ISA ) to learn descriptors for state-of-the-art handcrafted features, Third we enhance ISA with two new improvements, which make our learned descriptors significantly outperform the handcrafted ones. Experimental on standard action recognition benchmarks.
2K_test_1427	For managing electric power networks in a distributed manner. Methods and software A supply and demand balancing scheme is used across a network of agents to facilitate agreement on the optimal incremental price for energy provision in an electric power network subject to the constraint that the total generation in the electric power network matches the total network demand A multi-step optimization approach is provided that incorporates inter-temporal constraints, allowing for optimal integration of flexible power loads and power storage entities and taking into account power generation ramp rate constraints at individual generation entities The approach is extended to cope with line flow constraints imposed by the physical system topology and transmission line limits/capacities.
2K_test_1429	Designers of human computation systms often face the need to aggregate noisy information provided by multiple people, Our short-term goal is to motivate the design of better human computation systems ; our long-term goal is to spark an interaction between researchers in ( computational ) social choice and human computation. While voting is often used for this purpose, the choice of voting method is typically not principled to better understand how different voting rules perform in practice. Our empirical conclusions show that noisy human voting can differ from what popular theoretical models would predict. We conduct extensive experiments on Amazon Mechanical Turk.
2K_test_1430	The Poisson distribution has been widely studied and used for modeling univariate count-valued data, Multivariate generalizations of the Poisson distribution that permit dependencies, however have been far less popular, Finally we suggest new research directions as explored in the subsequent discussion section. These empirical experiments develop intuition about the comparative advantages and disadvantages of each class of multivariate distribution that was derived from the Poisson. Compare the models in terms of interpretability and theory Then, we empirically compare multiple models from each class on three real-world datasets that have varying data characteristics from different domains, namely traffic accident data, biological next generation sequencing data.
2K_test_1432	More generally the approach could be used to identify malleable components of cognitive functions, such as spatial reasoning or executive functions. To explore a long-standing question of the scope of transfer of learning To test these theories to represent mental functions that are changed while learning to cause a reduction in error rates for new tasks. We find that the component models provide both better predictions and better explanations than the faculty models, Weak model variations tend to improve generalization across students, but hurt generalization across items and make a sacrifice to explanatory power. We develop statistical models of them These models use latent variables Strong versions of these models provide a common explanation for the variance in task difficulty and transfer, Weak versions decouple difficulty and transfer explanations by describing task difficulty with parameters for each unique task. We analyze naturally occurring datasets from student use of educational technologies We contrast a faculty theory of broad transfer with a component theory of more constrained transfer We evaluate these models in terms of both their prediction accuracy on held-out data and their power in explaining task difficulty and learning transfer, In comparisons across eight datasets.
2K_test_1433	Current methods for depression assessment depend almost entirely on clinical interview or self-report ratings, These findings suggest that automatic detection of depression from behavioral indicators is feasible and that multimodal measures afford most powerful detection. Such measures lack systematic and efficient ways of incorporating behavioral observations that are strong indicators of psychological disorder.
2K_test_1434	Results on two-particle angular correlations for charged particles produced in pp collisions at a center-of-mass energy of 13TeV are presented.
2K_test_1438	If a coherent field of inquiry in human conflict research emerged to highlight the main contributions in conflict research and to test if research on conflict has in fact evolved to represent a coherent field of inquiry, show how the CP was topically linked ( i, through democracy modeling resources.
2K_test_1440	The core premise of evidence-based medicine is that clinical decisions are informed by the peer-reviewed literature and discuss the implication of this form of bias as it pertains to evidence-based medicine. To extract meaningful conclusions from this literature, one must first understand the various forms of biases inherent within the process of peer review, and analysed this literature for patterns of publication. We performed an exhaustive search that identified articles exploring the question of whether survival benefit was associated with maximal high-grade glioma ( HGG ) resection.
2K_test_1441	Fluorescence microscopy is one of the most important tools in cell biology research because it provides spatial and temporal information to investigate regulatory systems inside cells This technique can generate data in the form of signal intensities at thousands of positions resolved inside individual live cells, However given extensive cell-to-cell variation, these data can not be readily assembled into three- or four-dimensional maps of protein concentration that can be compared across different cells and conditions. To enable comparison of imaging data from many cells to investigate actin dynamics in T cell activation. We have developed a method and applied it Antigen recognition in T cells by the T cell receptor ( TCR ) is amplified by engagement of the costimulatory receptor CD28, We imaged actin and eight core actin regulators to generate over a thousand movies of T cells under conditions in which CD28 was either engaged or blocked in the context of a strong TCR signal.
2K_test_1442	One goal of human genetics is to understand the genetic basis of disease, a challenge for diseases of complex inheritance because risk alleles are few relative to the vast set of benign variants, Risk variants are often sought by association studies in which allele frequencies in case subjects are contrasted with those from population-based samples used as control subjects, If such a resource were to exist, it would yield ample savings and would facilitate the effective use of data repositories by removing administrative and technical barriers, These results highlight how UNICORN can enable reliable, powerful and convenient genetic association analyses without access to the individual-level data. In an ideal world we would know population-level allele frequencies, releasing researchers to focus on case subjects, We argue this ideal is possible, at least theoretically and we outline a path to achieving it in reality, to perform association analyses without necessitating direct access to individual-level control data. We call this concept the Universal Control Repository Network ( UNICORN ), a means Our approach to UNICORN uses existing genetic resources and various statistical tools to analyze these data, including hierarchical clustering with spectral analysis of ancestry ; and empirical Bayesian analysis along with Gaussian spatial processes to estimate ancestry-specific allele frequencies. We demonstrate our approach using tens of thousands of control subjects from studies of Crohn disease.
2K_test_1443	Indicate that SMs reveal more PTSD symptoms to the VH than they report on the Post Deployment Health Assessment Pre/Post deployment facial expression analysis indicated more sad expressions and few happy expressions at post deployment. SimSensei is a Virtual Human ( VH ) interviewing platform that uses off-the-shelf sensors ( i, webcams Microsoft Kinect and a microphone ) to capture and interpret real-time audiovisual behavioral signals from users interacting with the VH system, The system was specifically designed for clinical interviewing and health care support by providing a face-to-face interaction between a user and a VH that can automatically react to the inferred state of the user through analysis of behavioral signals gleaned from the user 's facial expressions, body gestures and vocal parameters, Akin to how non-verbal behavioral signals have an impact on human-to-human interaction and communication. Results from of sample of service members ( SMs ) who were interviewed before and after a deployment to Afghanistan.
2K_test_1444	Multifunctional polymer-based composites have been widely used in various research and industrial applications, such as flexible and stretchable electronics and sensors and sensor-integrated smart structures This work may provide rational methods for the fabrication of aligned composites. This study investigates the influence of particle coalescence on the mechanical and electrical properties of spherical nickel powder ( SNP ) /polydimethylsiloxane ( PDMS ) composites in which SNP was aligned using an external magnetic field.
2K_test_1445	An all-soft-matter composite with exceptional electro-elasto properties is demonstrated. By embedding liquid-metal inclusions in an elastomer matrix. The elasticity electrostatics and electromechanical coupling of the composite are investigated.
2K_test_1446	Maximum-a-Posteriori ( MAP ) inference lies at the heart of Graphical Models and Structured Prediction, Despite the intractability of exact MAP inference, approximate methods based on LP relaxations have exhibited superior performance across a wide range of applications. Yet for problems involving large output domains ( i, the state space for each variable is large ), standard LP relaxations can easily give rise to a large number of variables and constraints which are beyond the limit of existing optimization algorithms. In this paper we introduce an effective MAP inference method for problems with large output domains, The method builds upon alternating minimization of an Augmented Lagrangian that exploits the sparsity of messages through greedy optimization techniques, A key feature of our greedy approach is to introduce variables in an on-demand manner with a pre-built data structure over local factors, In addition we introduce a variant of GDMM for binary MAP inference problems with a large number of factors.
2K_test_1447	Recent studies have shown that brain-machine interfaces ( BMIs ) offer great potential for restoring upper limb function. However grasping objects is a complicated task and the signals extracted from the brain may not always be capable of driving these movements reliably, Vision-guided robotic assistance is one possible way to improve BMI performance. We describe a method of shared control where the user controls a prosthetic arm using a BMI and receives assistance with positioning the hand when it approaches an object.
2K_test_1448	Complex networks have been shown to exhibit universal properties, with one of the most consistent patterns being the scale-free degree distribution. But are there regularities obeyed by the r-hop neighborhood in real networks ? We answer this question. By identifying another power-law pattern that describes the relationship between the fractions of node pairs C ( r ) within r hops and the hop count r, This scale-free distribution is pervasive and describes a large variety of networks, ranging from social and urban to technological and biological networks, In particular inspired by the definition of the fractal correlation dimension D2 on a point-set, we consider the hop-count r to be the underlying distance metric between two vertices of the network, and we examine the scaling of C ( r ) with r, We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. Under successful existing network models, while we analyze a large set of real and synthetic network datasets and.
2K_test_1449	Eumelanins are extended heterogeneous biopolymers composed of molecular subunits with ambiguous macromolecular topology. Which suggests that natural eumelanin pigments contain indole-based tetramers that are arranged into porphyrin-like domains, suggest that sodium ions undergo occupancy-dependent stepwise insertion into the core of porphyrin-like tetramers in natural eumelanins at discrete potentials. Here an electrochemical fingerprinting technique is described. Spectroscopy and density functional theory calculations.
2K_test_1450	High throughput screening determines the effects of many conditions on a given biological target, Currently to estimate the effects of those conditions on other targets requires either strong modeling assumptions ( e, similarities among targets ) or separate screens, The results represent the first practical demonstration of the utility of active learning-driven biological experimentation in which the set of possible phenotypes is unknown in advance. Ideally data-driven experimentation could be used to learn accurate models for many conditions and targets without doing all possible experiments. We have previously described an active machine learning algorithm that can iteratively choose small sets of experiments to learn models of multiple effects. We now show that, with no prior knowledge and with liquid handling robotics and automated microscopy under its control.
2K_test_1451	Despite the enormous medical impact of cancers and intensive study of their biology, detailed characterization of tumor growth and development remains elusive, This difficulty occurs in large part because of enormous heterogeneity in the molecular mechanisms of cancer progression, both tumor-to-tumor and cell-to-cell in single tumors Advances in genomic technologies, especially at the single-cell level, are improving the situation, but these approaches are held back by limitations of the biotechnologies for gathering genomic data from heterogeneous cell populations and the computational methods for making sense of those data, One popular way to gain the advantages of whole-genome methods without the cost of single-cell genomics has been the use of computational deconvolution ( unmixing ) methods to reconstruct clonal heterogeneity from bulk genomic data, a key step in the process of accurately deconvolving tumor genomic data and inferring clonal heterogeneity from bulk data. These methods too are limited by the difficulty of inferring genomic profiles of rare or subtly varying clonal subpopulations from bulk data, a problem that can be computationally reduced to that of reconstructing the geometry of point clouds of tumor samples in a genome space to improve that reconstruction. That this new method substantially improves our ability to resolve discrete tumor subgroups. Here we present a new method by better identifying subspaces corresponding to tumors produced from mixtures of distinct combinations of clonal subpopulations We develop a nonparametric clustering method based on medoidshift clustering for identifying subgroups of tumors expected to correspond to distinct trajectories of evolutionary progression. We show on synthetic and real tumor copy-number data.
2K_test_1453	In eukaryotic cells mitochondria form a dynamic interconnected network to respond to changing needs at different subcellular locations. A fundamental yet unanswered question regarding this network is whether, and if so how, local fusion and fission of individual mitochondria affect their global distribution, To address this question. We developed high-resolution computational image analysis techniques. To examine the relations between mitochondrial fusion/fission and spatial distribution within the axon of Drosophila larval neurons.
2K_test_1454	Bayesian theory has provided a compelling conceptualization for perceptual inference in the brain, Central to Bayesian inference is the notion of statistical priors, To understand the neural mechanisms of Bayesian inference, we need to understand the neural representation of statistical regularities in the natural environment, They also suggest that the Boltzmann machine can be a viable model for conceptualizing computations in the visual cortex and, as such can be used to predict neural circuits in the visual cortex from natural scene statistics. In this paper we investigated empirically how statistical regularities in natural 3D scenes are represented in the functional connectivity of disparity-tuned neurons in the primary visual cortex of primates, to learn from 3D natural scenes. And found that the units in the model exhibited cooperative and competitive interactions, forming a `` disparity association field '', analogous to the contour association field, The cooperative and competitive interactions in the disparity association field are consistent with constraints of computational models for stereo matching, and found the results to be consistent with neurophysiological data in terms of the functional connectivity measurements between disparity-tuned neurons in the macaque primary visual cortex, These findings demonstrate that there is a relationship between the functional connectivity observed in the visual cortex and the statistics of natural scenes. We applied a Boltzmann machine model. In addition we simulated neurophysiological experiments on the model.
2K_test_1455	Without this comprehensive approach, a full understanding of how cortical circuits adapt during learning or altered sensory input will be impossible to perform an unbiased analysis of developmental and experience-dependent changes in synaptic properties across an entire cortical column in mice. Here we adapt an electron microscopy technique that selectively labels synapses, in combination with a machine-learning algorithm for semiautomated synapse detection. Synapse density and length were compared across development and during whisker-evoked plasticity, Targeted electrophysiological analysis of changes in miniature EPSC and IPSC properties in L2 pyramidal neurons.
2K_test_1456	We investigated the dynamics of head movement in mothers and infants during an age-appropriate, well-validated emotion induction the Still Face paradigm. In male but not female infants, angular displacement increased from Play to Still-Face and decreased from Still Face to Reunion, Infant angular velocity was higher during Still-Face than Reunion with no differences between male and female infants Windowed cross-correlation suggested changes in how infant and mother head movements are associated, revealing dramatic changes in direction of association, Coordination between mother and infant head movement velocity was greater during Play compared with Reunion.
2K_test_1458	Characterizing the spatial distribution of proteins directly from microscopy images is a difficult problem with numerous applications in cell biology ( e, identifying motor-related proteins ) and clinical research ( e, identification of cancer biomarkers ), Such models are expected to be valuable for representing and summarizing each pattern and for constructing systems biology simulations of cell behaviors. That provides automated analysis of punctate protein patterns in microscope images, including quantification of their relationships to microtubules, modeling of punctate distributions that captures the essential characteristics of the distinct patterns. We were able to show that these patterns could be distinguished from each other with high accuracy, and we were able to assign to one of these subclasses hundreds of proteins whose subcellular localization had not previously been well defined.
2K_test_1459	We introduce a method for sealing liquid metal ( LM ) circuits with soft anisotropic conductors that prevent leaking, while simultaneously allowing for electrical contact with skin and surface mounted electronics. These films are composed of polydimethylsiloxane ( PDMS ) embedded with vertically aligned columns of ferromagnetic Ag-Ni microparticles, The microparticles are magnetically aligned and support electrical conductivity only through the thickness ( z-axis ) of the elastomer film.
2K_test_1460	This lack of consistency will affect future research on the clinical significance of snoring. The objective of this study was to compare to each other the methods currently recommended by the American Academy of Sleep Medicine ( AASM ) to measure snoring : an acoustic sensor, a piezoelectric sensor and a nasal pressure transducer ( cannula ). Ten subjects reporting habitual snoring were included in the study, performed at Landspitali-University Hospital, Snoring was assessed by listening to the air medium microphone located on a patient 's chest, compared to listening to two overhead air medium microphones ( stereo ) and manual scoring of a piezoelectric sensor and nasal cannula vibrations The sensitivity and positive predictive value of scoring snore events from the different sensors was compared to the chest audio.
2K_test_1462	Games for health ( G4H ) aim to improve health outcomes and encourage behavior change While existing theoretical frameworks describe features of both games and health interventions, We discuss how this work can be applied to provide conceptual tools, improve the G4H design process, and guide approaches to encoding G4H-related data for large-scale empirical analysis. There has been limited systematic investigation into how disciplinary and interdisciplinary stakeholders understand design features in G4H. We found evidence of conceptual differences suggesting that a G4H perspective is not simply the sum of game and health perspectives At the same time, we found evidence of convergence in stakeholder views, including areas where game experts provided insights about health and vice versa.
2K_test_1463	Both the occurrence and intensity of facial expressions are critical to what the face reveals, While much progress has been made towards the automatic detection of facial expression occurrence, controversy exists about how to estimate expression intensity, The most straight-forward approach is to train multiclass or regression models using intensity ground truth, However collecting intensity ground truth is even more time consuming and expensive than collecting binary ground truth. As a shortcut some researchers have proposed using the decision values of binary-trained maximum margin classifiers as a proxy for expression intensity, We provide empirical evidence that this heuristic is flawed in practice as well as in theory Unfortunately, there are no shortcuts when it comes to estimating smile intensity : researchers must take the time to collect and train on intensity ground truth. However if they do so, high reliability with expert human coders can be achieved, Intensity-trained multiclass and regression models outperformed binary-trained classifier decision values on smile intensity estimation Multiclass models even outperformed binary-trained classifiers. Across multiple databases and methods for feature extraction and dimensionality reduction, on smile occurrence detection.
2K_test_1466	Laser photocoagulation is a mainstay or adjuvant treatment for a variety of common retinal diseases. Automated laser photocoagulation during intraocular surgery has not yet been established, The goals of the system are to enhance accuracy and efficiency and improve safety. The authors introduce an automated laser photocoagulation system for intraocular surgery, based on a novel handheld instrument.
2K_test_1467	As sampling-based motion planners become faster, they can be re-executed more frequently by a robot during task execution to react to uncertainty in robot motion, obstacle motion sensing noise, and uncertainty in the robot 's kinematic model. We investigate and analyze high-frequency replanning ( HFR ) The objective is to maximize the probability of success ( i, avoid collision with obstacles and reach the goal ) or to minimize path length subject to a lower bound on the probability of success. We show that as parallel computation power increases, HFR offers asymptotic optimality for these objectives during each period for goal-oriented problems, We then demonstrate the effectiveness of HFR. Where during each period, fast sampling-based motion planners are executed in parallel as the robot simultaneously executes the first action of the best motion plan from the previous period, We consider discrete-time systems with stochastic nonlinear ( but linearizable ) dynamics and observation models with noise drawn from zero mean Gaussian distributions. For holonomic and nonholonomic robots including car-like vehicles and steerable medical needles.
2K_test_1468	Nationally sponsored cancer-care quality-improvement efforts have been deployed in community health centers to increase breast, cervical and colorectal cancer-screening rates among vulnerable populations Despite several immediate and short-term gains, screening rates remain below national benchmark objectives, Overall improvement has been both difficult to sustain over time in some organizational settings and/or challenging to diffuse to other settings as repeatable best practices, Reasons for this include facility-level changes, which typically occur in dynamic organizational environments that are complex. This study seeks to understand the factors that shape community health center facility-level cancer-screening performance over time. This study applies a computational-modeling approach, combining principles of health-services research, health informatics network theory.
2K_test_1469	Management decisions underpinning availability of ecosystem services and the organisms that provide them in agroecosystems, such as pollinators and pollination services, have emerged as a foremost consideration for both conservation and crop production goals, There is growing evidence that innovative management practices can support diverse pollinators and increase crop pollination. However there is also considerable debate regarding factors that support adoption of these innovative practices, The goals of this quantitative, social survey were to investigate grower experience with concerns and benefits associated with each practice, as well as the influence of grower networks, which are comprised of contacts that reflect potential pathways for social and technical learning.
2K_test_1471	Robust efficient and low-cost networks are advantageous in both biological and engineered systems, During neural network development in the brain, synapses are massively over-produced and then pruned-back over time, This strategy is not commonly used when designing engineered networks, since adding connections that will soon be removed is considered wasteful. Here we show that for large distributed routing networks, network function is markedly enhanced by hyper-connectivity followed by aggressive pruning and that the global rate of pruning, a developmental parameter not previously studied by experimentalists, plays a critical role in optimizing network structure, to improve the distributed design of airline networks. And found that the rate is decreasing over time, and show that decreasing rates lead to more robust and efficient networks compared to other rates, Thus inspiration from neural network formation suggests effective ways to design distributed networks across several domains. We also present an application of this strategy. We first used high-throughput image analysis techniques to quantify the rate of pruning in the mammalian neocortex across a broad developmental time window Based on these results, we analyzed a model of computational routing networks using both theoretical analysis and simulations.
2K_test_1472	There are likely marked differences in endotracheal intubation ( ETI ) techniques between novice and experienced providers. To determine if portable motion technology could identify the motion components of ETI between novice and experienced providers. We performed a proof of concept study.
2K_test_1473	For home-based stroke rehabilitation The objective of the game approach is to enrich the training experience and establish a higher level of compliance to prescribed exercises, while maintaining a supportive training environment as found in common therapy sessions. Offered positive indications towards this concept. In our research we propose a portfolio of serious games Our system provides a collection of mini games based on rehabilitation exercises used in conventional physical therapy, monitors the patient 's performance while exercising and provides clinicians with an interface to personalize the training, The clinician can set the current state of rehabilitation and change the playable games over time to drive diversification. While the system still has to be evaluated, an early stage case study with one patient.
2K_test_1474	Analogous to genomic sequence alignment, biological network alignment identifies conserved regions between networks of different species, Then function can be transferred from well- to poorly-annotated species between aligned network regions, Network alignment typically encompasses two algorithmic components : node cost function ( NCF ), which measures similarities between nodes in different networks, and alignment strategy ( AS ), which uses these similarities to rapidly identify high-scoring alignments, Different methods use both different NCFs and different ASs, Thus it is unclear whether the superiority of a method comes from its NCF, its AS or both, We already showed on state-of-the-art methods, MI-GRAAL and IsoRankN that combining NCF of one method and AS of another method can give a new superior method, Existing methods determine this parameter more-less arbitrarily, which could affect alignment quality, Existing methods assume that the larger the neighborhood size. Here we evaluate MI-GRAAL against a newer approach to potentially further improve alignment quality, While doing so we approach important questions that have not been asked systematically thus far, First we ask how much of the NCF information should come from protein sequence data compared to network topology data, Second when topological information is used in NCF, we ask how large the size of the neighborhoods of the compared nodes should be. GHOST by mixing-and-matching the methods ' NCFs and ASs.
2K_test_1475	Platinum ( Pt ) drugs are the most potent and commonly used anti-cancer chemotherapeutics, Nanoformulation of Pt drugs has the potential to improve the delivery to tumors and reduce toxic side effects. A major challenge for translating nanodrugs to clinical settings is their rapid clearance by the reticuloendothelial system ( RES ), hence increasing toxicities on off-target organs and reducing efficacy.
2K_test_1476	In studying the strength and specificity of interaction between members of two protein families, key questions center on which pairs of possible partners actually interact, how well they interact, and why they interact while others do not. The advent of large-scale experimental studies of interactions between members of a target family and a diverse set of possible interaction partners offers the opportunity to address these questions, for learning and using graphical models that explicitly represent the amino acid basis for interaction specificity ( why ) and extend earlier classification-oriented approaches ( which ) to predict the _G of binding ( how well ). We develop here a method, DgSpi ( data-driven graphical models of specificity in protein : protein interactions ). Based on data from MacBeath and colleagues.
2K_test_1478	It is often assumed that central pattern generators, which generate rhythmic patterns without rhythmic inputs, play a key role in the spinal control of human locomotion suggest feedback integration to be functionally more important than central pattern generation in human locomotion across behaviours, In addition the proposed control architecture may serve as a guide in the search for the neurophysiological origin and circuitry of spinal control in humans. To compose steady and transitional 3-D locomotion behaviours. We propose a neural control model in which the spinal control generates muscle stimulations mainly through integrated reflex pathways with no central pattern generator Using a physics-based neuromuscular human model, we show that this control network is sufficient including walking and running, acceleration and deceleration slope and stair negotiation, turning and deliberate obstacle avoidance.
2K_test_1479	For automated intraocular laser surgery. We compared the performance of the automated scanning using various control thresholds, in order to find the most effective threshold in terms of accuracy and speed we conducted the handheld operation above a fixed target surface.
2K_test_1480	Less attention has been paid to multiplane undulations, which are particularly important in terrestrial environments where vertical undulations can regulate substrate contact, We demonstrate that the high maneuverability displayed by sidewinder rattlesnakes ( Crotalus cerastes ) emerges from the animal 's ability to independently modulate these waves. We tested these mechanisms using a multimodule snake robot as a physical model, successfully generating differential and reversal turning with performance comparable to that of the organisms.
2K_test_1481	The assessment of jaundice in outpatient neonates is problematic, Visual assessment is inaccurate, and more exact methodologies are cumbersome and/or expensive. Our goal in this study was to assess the accuracy of a technology based on the analysis of digital images of newborns obtained. Using a smartphone application called BiliCam.
2K_test_1482	Gene therapies have emerged as a promising treatment for congestive heart failure Prior work on controlling the movement of Cerberus required accurate knowledge of device geometry. Yet they lack a method for minimally invasive, To address this need for cardiac interventions, In order to determine the geometry of the device in vivo to measure the geometry of the robot. We developed Cerberus a minimally invasive parallel wire robot, this paper presents work on developing an auto-calibration procedure using force sensors to move injector.
2K_test_1483	Recent studies implicate chromatin modifiers in autism spectrum disorder ( ASD ) through the identification of recurrent de novo loss of function mutations in affected individuals. ASD risk genes are co-expressed in human midfetal cortex, suggesting that ASD risk genes converge in specific regulatory networks during neurodevelopment, To elucidate such networks. CHD8 targets are strongly enriched for other ASD risk genes in both human and mouse neurodevelopment, and converge in ASD-associated co-expression networks in human midfetal cortex CHD8 knockdown in hNSCs results in dysregulation of ASD risk genes directly targeted by CHD8, Integration of CHD8-binding data into ASD risk models improves detection of risk genes These results suggest loss of CHD8 contributes to ASD by perturbing an ancient gene regulatory network during human brain development. We identify genes targeted by CHD8, a chromodomain helicase strongly associated with ASD, in human midfetal brain, human neural stem cells ( hNSCs ) and embryonic mouse cortex.
2K_test_1485	The overall purpose of this study was to learn how community-dwelling older adults would interact with our prototype multi-user telehealth kiosk and their views about its usability. Seven subjects participated in laboratory-based usability sessions to evaluate the physical design, appearance functionality and perceived ease of use of a multi-user telehealth kiosk prototype.
2K_test_1486	The quantitative relationship between presynaptic calcium influx and transmitter release critically depends on the spatial coupling of presynaptic calcium channels to synaptic vesicles, When there is a close association between calcium channels and synaptic vesicles, the flux through a single open calcium channel may be sufficient to trigger transmitter release. With increasing spatial distance, however a larger number of open calcium channels might be required to contribute sufficient calcium ions to trigger vesicle fusion, to show that release of individual synaptic vesicles is predominately triggered by calcium ions entering the nerve terminal through the nearest open calcium channel. Furthermore calcium ion flux through this channel has a low probability of triggering synaptic vesicle fusion ( _6 % ), even when multiple channels open in a single active zone, These mechanisms work to control the rare triggering of vesicle fusion in the frog neuromuscular junction from each of the tens of thousands of individual release sites at this large model synapse. Here we used a combination of pharmacological calcium channel block, high-resolution calcium imaging postsynaptic recording, and 3D Monte Carlo reaction-diffusion simulations in the adult frog neuromuscular junction.
2K_test_1487	Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data, For example they are often used to explore complex systems where connections between entities are not well understood, such as in functional brain networks or genetic networks. Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable, such as a binary neural activation state or a continuous mRNA abundance measurement, even though in many real world problems, nodes can represent multivariate variables with much richer meanings, such as whole images, text documents or multi-view feature vectors, for estimating the structure of undirected graphical models from such multivariate ( or multi-attribute ) nodal data. Extensive simulation studies from gene and protein profiles positron emission tomography data.
2K_test_1489	Individuals who exhibit large-magnitude blood pressure ( BP ) reactions to acute psychological stressors are at risk for hypertension and premature death by cardiovascular disease. This study tested whether a multivariate pattern of stressor-evoked brain activity could reliably predict individual differences in BP reactivity. Providing novel evidence for a candidate neurophysiological source of stress-related cardiovascular risk.
2K_test_1490	Of course for testing the significance of an additional variable between two nested linear models, one typically uses the chi-squared test, comparing the drop in residual sum of squares ( RSS ) to a [ Formula : see text ] distribution. In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path, But when this additional variable is not fixed, and has been chosen adaptively or greedily, this test is no longer appropriate : adaptivity makes the drop in RSS stochastically much larger than [ Formula : see text ] under the null hypothesis, Our analysis explicitly accounts for adaptivity.
2K_test_1491	Recently there has been substantial interest in spectral methods for learning dynamical systems, These methods are popular since they often offer a good tradeoff between computational and statistical efficiency. Unfortunately they can be difficult to use and extend in practice : e, they can make it difficult to incorporate prior information such as sparsity or structure, To address this problem. The correctness of these instances follows directly from our general analysis. We present a new view of dynamical system learning : we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems, thereby allowing users to incorporate prior knowledge via standard techniques such as L1 regularization Many existing spectral methods are special cases of this new framework, using linear regression as the supervised learner. We demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does ;.
2K_test_1492	While studies show that autism is highly heritable, the nature of the genetic basis of this disorder remains illusive, The proposed modeling framework can be naturally extended to incorporate additional structural information concerning the dependence between genes. To find more potentially autism risk genes. Based on the idea that highly correlated genes are functionally interrelated and more likely to affect risk, we develop a novel statistical tool by combining the genetic association scores with gene co-expression in specific brain regions and periods of development, The gene dependence network is estimated using a novel partial neighborhood selection ( PNS ) algorithm, where node specific properties are incorporated into network estimation for improved statistical and computational efficiency Then we adopt a hidden Markov random field ( HMRF ) model to combine the estimated network and the genetic association scores in a systematic manner. Using currently available genetic association data from whole exome sequencing studies and brain gene expression levels.
2K_test_1494	Reconstructing regulatory and signaling response networks is one of the major goals of systems biology. While several successful methods have been suggested for this task, some integrating large and diverse datasets, these methods have so far been applied to reconstruct a single response network at a time, even when studying and modeling related conditions, To improve network reconstruction We formulate the multi-task learning problem and discuss methods for optimizing the joint target function. Our multi-task learning method was able to identify known and novel factors and genes, improving upon prior methods that model each condition independently, The MT-SDREM networks were also better at identifying proteins whose removal affects viral load indicating that joint learning can still lead to accurate, Supporting website with MT-SDREM implementation : http : //sb. We developed MT-SDREM a multi-task learning method which jointly models networks for several related conditions, In MT-SDREM parameters are jointly constrained across the networks while still allowing for condition-specific pathways and regulation. We applied MT-SDREM to reconstruct dynamic human response networks for three flu strains : H1N1.
2K_test_1495	Methods to assess individual facial actions have potential to shed light on important behavioral phenomena ranging from emotion and social interaction to psychological disorders and health, However manual coding of such actions is labor intensive and requires extensive training, To date establishing reliable automated coding of unscripted facial actions has been a daunting challenge impeding development of psychological theories and applications requiring facial expression assessment These findings suggest automated FACS coding has progressed sufficiently to be applied to observational research in emotion and related areas of study. It is therefore essential that automated coding systems be developed with enough precision and robustness to ease the burden of manual coding in challenging data involving variation in participant gender, ethnicity head pose speech.
2K_test_1497	Autism is a psychiatric/neurological condition in which alterations in social interaction ( among other symptoms ) are diagnosed by behavioral psychiatric methods, The approach is based on previous advances in fMRI analysis methods that permit ( a ) the identification of a concept, such as the thought of a physical object, from its fMRI pattern, and ( b ) the ability to assess the semantic content of a concept from its fMRI pattern The findings suggest that psychiatric alterations of thought can begin to be biologically understood by assessing the form and content of the altered thought 's underlying brain activation patterns. The main goal of this study was to determine how the neural representations and meanings of social concepts ( such as to insult ) are altered in autism, A second goal was to determine whether these alterations can serve as neurocognitive markers of autism. And machine learning methods.
2K_test_1498	Given a simple noun such as apple, and a question such as `` Is it edible ?, '' what processes take place in the human brain ?, even though originating from the field of neuroscience to provide neuroscientific insights toward a better understanding of the way that neurons interact with each other. More specifically given the stimulus, what are the interactions between ( groups of ) neurons ( also known as functional connectivity ) and how can we automatically infer those interactions, given measurements of the brain activity ? Furthermore, how does this connectivity differ across different human subjects ? we show that this problem, can benefit from big data techniques ; able to effectively model the dynamics of the neuron interactions and infer the functional connectivity. GeBM produces brain activity patterns that are strikingly similar to the real ones, where the inferred functional connectivity is able as well as detect regularities and outliers in multisubject brain activity measurements. In this work we present a simple, novel good-enough brain model, or GeBM in short, and a novel algorithm Sparse-SysId, which are Moreover GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text ). We evaluate GeBM by using real brain data.
2K_test_1499	Story understanding involves many perceptual and cognitive subprocesses, from perceiving individual words, to parsing sentences to understanding the relationships among the story characters Additionally, this approach is promising for studying individual differences : it can be used to create single subject maps that may potentially be used to measure reading comprehension and diagnose reading disorders. To distinguish which of two story segments is being read simultaneously track diverse reading subprocesses during complex story processing and predict the detailed neural representation of diverse story features. We present an integrated computational model of reading that incorporates these and additional subprocesses, simultaneously discovering their fMRI signatures Our model predicts the fMRI activity associated with reading arbitrary text passages, well enough This approach is the first to ranging from visual word properties to the mention of different story characters and different actions they perform We construct brain representation maps that replicate many results from a wide range of classical studies that focus each on one aspect of language processing and offer new insights on which type of information is processed by different areas involved in language processing.
2K_test_1500	Which performs active tremor compensation during microsurgery. The manipulation performance was investigated in both clamped and handheld conditions In positioning experiments with varying side loads.
2K_test_1501	In studying the strength and specificity of interaction between members of two protein families, The advent of large-scale experimental studies of interactions between members of a target family and a diverse set of possible interaction partners offers the opportunity to address these questions. Key questions center on which pairs of possible partners actually interact, how well they interact, and why they interact while others do not, for learning and using graphical models that explicitly represent the amino acid basis for interaction specificity ( why ) and extend earlier classification-oriented approaches ( which ) to predict the _G of binding ( how well. We develop here a method, DgSpi ( Data-driven Graphical models of Specificity in Protein : protein Interactions ). Based on data from MacBeath and colleagues.
2K_test_1502	Computer-mediated communication is driving fundamental changes in the nature of written language. We investigate these changes we identify high-level patterns in diffusion of linguistic change over the United States. The results of this analysis offer support for prior arguments that focus on geographical proximity and population size, However demographic similarity - especially with regard to race - plays an even more central role, as cities with similar racial demographics are far more likely to share linguistic influence Rather than moving towards a single unified `` netspeak '' dialect, language evolution in computer-mediated communication reproduces existing fault lines in spoken American English. Using a latent vector autoregressive model to aggregate across thousands of words Our model is robust to unpredictable changes in Twitter 's sampling rate, and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors.
2K_test_1503	Many statistical methods gain robustness and flexibility by sacrificing convenient computational structures, Though this paper focuses on the problem of graph estimation, the proposed methodology is widely applicable to other problems with similar structures. In this paper we illustrate this fundamental tradeoff by studying a semi-parametric graph estimation problem in high dimensions, help to solve this type of problem to estimate high dimensional semiparametric graphical models to analyze the tradeoff between computational efficiency and statistical error. We also report results. We explain how novel computational techniques In particular, we propose a nonparanormal neighborhood pursuit algorithm with theoretical guarantees, Moreover we provide an alternative view under a smoothing optimization framework. Thorough experimental on text, stock and genomic datasets.
2K_test_1504	While only recently developed, the ability to profile expression data in single cells ( scRNA-Seq ) has already led to several important studies and findings, However this technology has also raised several new computational challenges, Such database queries ( which can be performed using our web server ) will enable researchers to better characterize cells when analyzing heterogeneous scRNA-Seq samples. These include questions about the best methods for clustering scRNA-Seq data, how to identify unique group of cells in such experiments, and how to determine the state or function of specific cells based on their expression profile, To address these issues for the analysis and retrieval of single cell RNA-Seq data. We show that the NN method improves upon prior methods in both, the ability to correctly group cells in experiments not used in the training and the ability to correctly infer cell type or state by querying a database of tens of thousands of single cell profiles. We develop and test a method based on neural networks ( NN ), and used these to obtain a reduced dimension representation of the single cell expression data. We tested various NN architectures, some of which incorporate prior biological knowledge.
2K_test_1505	Limbless organisms such as snakes can navigate nearly all terrain, In particular desert-dwelling sidewinder rattlesnakes ( Crotalus cerastes ) operate effectively on inclined granular media ( such as sand dunes ) that induce failure in field-tested limbless robots through slipping and pitching, Together these three approaches demonstrate how sidewinding with contact-length control mitigates failure on granular media. Enables the device to ascend sandy slopes close to the angle of maximum slope stability. Reveal that as granular incline angle increases, sidewinder rattlesnakes increase the length of their body in contact with the sand demonstrate that granular yield stresses decrease with increasing incline angle. Implementing this strategy in a physical robot model of the snake. Our laboratory experiments Plate drag experiments.
2K_test_1506	Whole-exome sequencing ( WES ) studies have demonstrated the contribution of de novo loss-of-function single-nucleotide variants ( SNVs ) to autism spectrum disorder ( ASD ). However challenges in the reliable detection of de novo insertions and deletions ( indels ) have limited inclusion of these variants in prior analyses.
2K_test_1507	Fluctuations in the growth rate of a bacterial culture during unbalanced growth are generally considered undesirable in quantitative studies of bacterial Our method has implications for both basic understanding of bacterial physiology and for the classification of bacterial strains. Under well-controlled experimental conditions, however these fluctuations are not random but instead reflect the interplay between intra-cellular networks underlying bacterial growth and the growth environment, Therefore these fluctuations could be considered quantitative phenotypes of the bacteria under a specific growth condition to identify `` phenotypic signatures. Here we present a method '' by time-frequency analysis of unbalanced growth curves measured with high temporal resolution, The signatures are then applied to differentiate amongst different bacterial strains or the same strain under different growth conditions, and to identify the essential architecture of the gene network underlying the observed growth dynamics.
2K_test_1508	Spontaneously arising ( de novo ) mutations have an important role in medical genetics, For diseases with extensive locus heterogeneity, such as autism spectrum disorders ( ASDs ), the signal from de novo mutations is distributed across many genes, suggesting that the role of de novo mutations in ASDs might reside in fundamental neurodevelopmental processes. Making it difficult to distinguish disease-relevant mutations from background variation. Here we provide a statistical framework for the analysis of excesses in de novo mutation per gene and gene set by calibrating a model of de novo mutation.
2K_test_1509	Influence maximization in social networks has been widely studied motivated by applications like spread of ideas or innovations in a network and viral marketing of products, Current studies focus almost exclusively on unsigned social networks containing only positive relationships ( e, friend or trust ) between users. Influence maximization in signed social networks containing both positive relationships and negative relationships ( e, foe or distrust ) between users is still a challenging problem that has not been studied, which aims to find the seed node set with maximum positive influence or maximum negative influence in signed social networks, To address the PRIM problem. We prove that the influence function of the PRIM problem under the IC-P model is monotonic and submodular Thus, a greedy algorithm can be used to achieve an approximation ratio of 1-1/e for solving the PRIM problem in signed social networks, validate that our approximation algorithm for solving the PRIM problem outperforms state-of-the-art methods. Thus in this paper, we propose the polarity-related influence maximization ( PRIM ) problem, we first extend the standard Independent Cascade ( IC ) model to the signed social networks and propose a Polarity-related Independent Cascade ( named IC-P ) diffusion model. Experimental results on two signed social network datasets.
2K_test_1510	The HMT3522 progression series of human breast cells have been used to discover how tissue architecture, microenvironment and signaling molecules affect breast cell growth and behaviors. However much remains to be elucidated about malignant and phenotypic reversion behaviors of the HMT3522-T4-2 cells of this series. We found that different breast cell states contain distinct gene networks, The network specific to non-malignant HMT3522-S1 cells is dominated by genes involved in normal processes, whereas the T4-2-specific network is enriched with cancer-related genes The networks specific to various conditions of the reverted T4-2 cells are enriched with pathways suggestive of compensatory effects, showed that aberrant expression values of certain hubs in the identified networks are associated with poor clinical outcomes, Thus analysis of various reversion conditions ( including non-reverted ) of HMT3522 cells using Treegl can be a good model system to study drug effects on breast cancer. We employed a `` pan-cell-state '' strategy, and analyzed jointly microarray profiles obtained from different state-specific cell populations from this progression and reversion model of the breast cells using a tree-lineage multi-network inference algorithm. Consistent with clinical data showing patient resistance to anticancer drugs, We validated the findings using an external dataset.
2K_test_1512	A key component of genetic architecture is the allelic spectrum influencing trait variability, For autism spectrum disorder ( herein termed autism ), the nature of the allelic spectrum is uncertain, Individual risk-associated genes have been identified from rare variation, especially de novo mutations From this evidence, one might conclude that rare variation dominates the allelic spectrum in autism. Yet recent studies show that common variation, individually of small effect, has substantial impact en masse, At issue is how much of an impact relative to rare variation this common variation has. New methods that distinguish total narrow-sense heritability from that due to common variation. Using a unique epidemiological sample from Sweden and synthesis of results from other studies.
2K_test_1513	In effort to improve thermal control in minimally invasive cryosurgery, the concept of a miniature, wireless implantable sensing unit has been developed recently The sensing unit integrates a wireless power delivery mechanism, wireless communication means and a sensing core-the subject matter of the current study. Focuses on design principles, fabrication of a proof-of-concept, and characterization in a cryogenic environment.
2K_test_1514	Stochastic models are increasingly used to study the behaviour of biochemical systems, While the structure of such models is often readily available from first principles, unknown quantitative features of the model are incorporated into the model as parameters. Algorithmic discovery of parameter values from experimentally observed facts remains a challenge for the computational systems biology community, to learn the parameters in a stochastic model. We present a new parameter discovery algorithm that uses simulated annealing, sequential hypothesis testing and statistical model checking We apply our technique to a model of glucose and insulin metabolism used for in-silico validation of artificial pancreata and. By developing parallel CUDA-based implementation for parameter synthesis in this model.
2K_test_1515	Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism, The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks, around which transcriptional regulation is focused owing to a disease or a genetic perturbation, Using a z-score-based scoring scheme, RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network, The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes. With the goal of eliminating the following problems in detecting reporter metabolites : ( i ) conventional statistical methods suffer from small sample sizes, ( ii ) as z-score ranges from minus to plus infinity, calculating average scores can lead to canceling out opposite effects and ( iii ) analyzing genes one by one, then aggregating results can lead to information loss. We show that MIRA 's results are biologically sound, empirically significant and more reliable than RA. In this article we propose a mutual information-based multivariate reporter algorithm MIRA MIRA is a multivariate and combinatorial algorithm that calculates the aggregate transcriptional response around a metabolite using mutual information.
2K_test_1516	Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism, The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks, around which transcriptional regulation is focused owing to a disease or a genetic perturbation Using a z-score-based scoring scheme, RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network, The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes. I ) conventional statistical methods suffer from small sample sizes, ( ii ) as z-score ranges from minus to plus infinity, calculating average scores can lead to canceling out opposite effects and ( iii ) analyzing genes one by one, then aggregating results can lead to information loss. We show that MIRA 's results are biologically sound, empirically significant and more reliable than RA. In this article we propose a mutual information-based multivariate reporter algorithm ( MIRA ) with the goal of eliminating the following problems in detecting reporter metabolites : MIRA is a multivariate and combinatorial algorithm that calculates the aggregate transcriptional response around a metabolite using mutual information.
2K_test_1517	These phages include representatives of all three virion morphologies, The phages also span considerable sequence diversity.
2K_test_1518	With the continuous improvement in genotyping and molecular phenotyping technology and the decreasing typing cost, it is expected that in a few years, more and more clinical studies of complex diseases will recruit thousands of individuals for pan-omic genetic association analyses. Hence there is a great need for algorithms and software tools that could scale up to the whole omic level, integrate different omic data, leverage rich structure information, and be easily accessible to non-technical users detect genome- and phenome-wide associations among genotypes, gene expression data and clinical or other macroscopic traits to aid in the exploration of association mapping results. Report some interesting findings, GenAMap is available from http : //sailing. We demonstrate the function of GenAMap via a case study of the Brem and Kruglyak yeast dataset, and then apply it on a comprehensive eQTL analysis of the NIH heterogeneous stock mice dataset and.
2K_test_1521	The level of force required during these manipulations is often below the human tactile threshold, requiring the surgeon to rely on subtle visual cues or to apply larger forces above the tactile threshold for feedback, However both of these methods can lead to tissue damage, Excursions can be made into tissues which are not felt by the surgeon, while larger forces have a higher chance of damaging tissue within the eye, To prevent damage to the retina and other anatomy. We present the implementation of hybrid position/force control operating in the sub-tactile force range for a handheld robotic system.
2K_test_1523	Compared to the general population and patients with other chronic diseases reviewed here, patients with CLD reported significantly lower health quality status, more bad mental and physical health days, a significant symptom disease burden, and greater activity limitations, They also reported impairment in their ability to work, increased utilization of healthcare services, and greater out of pocket medical costs, CLD patients have significantly impaired HRQoL and greater healthcare utilization compared to the general population and patients with other chronic diseases.
2K_test_1524	Performing micromanipulation and delicate operations in submillimeter workspaces is difficult because of destabilizing tremor and imprecise targeting, Accurate micromanipulation is especially important for microsurgical procedures, such as vitreoretinal surgery, to maximize successful outcomes and minimize collateral damage. Robotic aid combined with filtering techniques that suppress tremor frequency bands increases performance ; however, if knowledge of the operator 's goals is available, virtual fixtures have been shown to further improve performance. In this paper we derive a virtual fixture framework for active handheld micromanipulators that is based on high-bandwidth position measurements rather than forces applied to a robot handle, For applicability in surgical environments, the fixtures are generated in real-time from microscope video during the procedure, Additionally we develop motion scaling behavior around virtual fixtures as a simple and direct extension to the proposed framework. In more medically relevant experiments of vein tracing and membrane peeling in eye phantoms.
2K_test_1525	Population stratification is an important task in genetic analyses, It provides information about the ancestry of individuals and can be an important confounder in genome-wide association studies, Public genotyping projects have made a large number of datasets available for study, However practical constraints dictate that of a geographical/ethnic population, only a small number of individuals are genotyped, The resulting data are a sample from the entire population. If the distribution of sample sizes is not representative of the populations being sampled, the accuracy of population stratification analyses of the data could be affected, We attempt to understand the effect of biased sampling on the accuracy of population structure analysis and individual ancestry recovery, for sample selection bias. Found that the accuracy of recovery of population structure is affected to a large extent by the sample used for analysis and how representative it is of the underlying populations, we show that sample selection bias can affect the results of population structure analyses, We demonstrate that such a correction is effective in practice. We develop a mathematical framework in models for population structure and also proposed a correction for sample selection bias using auxiliary information about the sample. We examined two commonly used methods for analyses of such datasets, ADMIXTURE and EIGENSOFT and Using simulated data and real genotype data from cattle using simulated and real data.
2K_test_1526	Other identified temporal structures that were not highlighted in this paper may also be used to gain insights to possible novel mechanisms, Importantly the Temp-O workflow is generic ; while we applied it on NSCLC, it can be applied in other cancers and diseases, Importantly the identified temporal structures are meaningful in the tumor progression of NSCLC as. Can we use graph mining algorithms to find patterns in tumor molecular mechanisms ? Can we model disease progression with multiple time-specific graph comparison algorithms ? In this paper, we will focus on this area to model tumor progression in non-small cell lung cancer ( NSCLC ). Validating our findings in independent datasets Further, on a large independent dataset.
2K_test_1528	Matched field processing is a model-based framework for localizing targets in complex propagation environments, In underwater acoustics it has been extensively studied for improving localization performance in multimodal and multipath media, For guided wave structural health monitoring problems, matched field processing has not been widely applied but is an attractive option for damage localization due to equally complex propagation environments. Although effective matched field processing is often challenging to implement because it requires accurate models of the propagation environment, and the optimization methods used to generate these models are often unreliable and computationally expensive, To address these obstacles to build models of multimodal propagation environments directly from measured data for localization. And demonstrates its localization performance by distinguishing two nearby scatterers the data-driven matched field processing framework is shown to successfully localize two nearby scatterers with significantly smaller localization errors and finer resolutions. This paper introduces data-driven matched field processing, a framework and then use these models This paper presents the data-driven framework. Analyzes its behavior under unmodeled multipath interference from experimental measurements of an aluminum plate, Compared with delay-based models that are commonly used in structural health monitoring.
2K_test_1530	Recent technological advances coupled with large sample sets have uncovered many factors underlying the genetic basis of traits and the predisposition to complex disease, but much is left to discover, A common thread to most genetic investigations is familial relationships, Close relatives can be identified from family records, and more distant relatives can be inferred from large panels of genetic markers Finally, while our methods have been developed for refining genetic relationship matrices and improving estimates of heritability, they have much broader potential application in statistics Most notably, for error-in-variables random effects models and settings that require regularization of matrices with block or hierarchical structure. Unfortunately these empirical estimates can be noisy, especially regarding distant relatives, for denoising genetically-inferred relationship matrices. We show that smoothing leads to better estimates of the relatedness amongst distantly related individuals, We illustrate our method We show that by using smoothed relationship matrices we can estimate heritability using population-based samples. We propose a new method by exploiting the underlying structure due to hierarchical groupings of correlated individuals The approach, which we call Treelet Covariance Smoothing, employs a multiscale decomposition of covariance matrices to improve estimates of pairwise relationships. On both simulated and real data with a large genome-wide association study and estimate the `` heritability '' of body mass index quite accurately, Traditionally heritability defined as the fraction of the total trait variance attributable to additive genetic effects, is estimated from samples of closely related individuals using random effects models.
2K_test_1531	Good feature design is important to achieve effective image classification, The feature design is applicable to different application domains. We propose to transform the images to obtain more representative feature descriptors, to transform the computed descriptors to further improve the classification accuracy. Both experiments show promising performance improvements over the state-of-the-art. This paper presents a novel feature design with two main contributions, First prior to computing the feature descriptors, with learning-based filters Second, we propose with another set of learning-based filters In this way, while generic feature descriptors are used, data-adaptive information is integrated into the feature extraction process based on the optimization objective to enhance the discriminative power of feature descriptors. And is evaluated on both lung tissue classification in high-resolution computed tomography ( HRCT ) images and apoptosis detection in time-lapse phase contrast microscopy image sequences.
2K_test_1533	This paper presents a fast and efficient computational approach to higher order spectral graph matching. The experimental results show that the proposed method is faster and requires smaller memory than the existing methods with little or no loss of accuracy. Exploiting the redundancy in a tensor representing the affinity between feature points, we approximate the affinity tensor with the linear combination of Kronecker products between bases and index tensors, The bases and index tensors are highly compressed representations of the approximated affinity tensor, requiring much smaller memory than in previous methods, which store the full affinity tensor, We compute the principal eigenvector of the approximated affinity tensor using the small bases and index tensors without explicitly storing the approximated tensor, To compensate for the loss of matching accuracy by the approximation, we also adopt and incorporate a marginalization scheme that maps a higher order tensor to matrix as well as a one-to-one mapping constraint into the eigenvector computation process.
2K_test_1535	For use in ankle-foot rehabilitation. We describe the design and control of a wearable robotic device powered by pneumatic artificial muscle actuators The design is inspired by the biological musculoskeletal system of the human foot and lower leg, mimicking the morphology and the functionality of the biological muscle-tendon-ligament structure A key feature of the device is its soft structure that provides active assistance without restricting natural degrees of freedom at the ankle joint, Four pneumatic artificial muscles assist dorsiflexion and plantarflexion as well as inversion and eversion, The prototype is also equipped with various embedded sensors for gait pattern analysis. Experimentally demonstrated using a linear time-invariant ( LTI ) controller, The controller is found using an identified LTI model of the system, resulting from the interaction of the soft orthotic device with a human leg, and model-based classical control design techniques demonstrated with several angle-reference following experiments.
2K_test_1536	Virus capsid assembly has been widely studied as a biophysical system, both for its biological and medical significance and as an important model for complex self-assembly processes No current technology can monitor assembly in detail and what information we have on assembly kinetics comes exclusively from in vitro studies, There are many differences between the intracellular environment and that of an in vitro assembly assay, however that might be expected to alter assembly pathways These models may help us understand how complicated assembly systems may have evolved to function with high efficiency and fidelity in the densely crowded environment of the cell. Here we explore one specific feature characteristic of the intracellular environment and known to have large effects on macromolecular assembly processes : molecular crowding, to examine possible effects of crowding on assembly pathways. Suggest a striking difference depending on whether or not a system uses nucleation-limited assembly, with crowding tending to promote off-pathway growth in a nonnucleation-limited model but often enhancing assembly efficiency at high crowding levels even while impeding it at lower crowding levels in a nucleation-limited model. We combine prior particle simulation methods for estimating crowding effects with coarse-grained stochastic models of capsid assembly, using the crowding models to adjust kinetics of capsid simulations.
2K_test_1537	Computational cancer phylogenetics seeks to enumerate the temporal sequences of aberrations in tumor evolution, thereby delineating the evolution of possible tumor progression pathways, molecular subtypes and mechanisms of action, We previously developed a pipeline for constructing phylogenies describing evolution between major recurring cell types computationally inferred from whole-genome tumor profiles. The accuracy and detail of the phylogenies, however depend on the identification of accurate, high-resolution molecular markers of progression, reproducible regions of aberration that robustly differentiate different subtypes and stages of progression, for the problem of inferring such phylogenetically significant markers. We show which confirms its effectiveness for tumor phylogeny inference and suggests avenues for future advances. Here we present a novel hidden Markov model ( HMM ) scheme through joint segmentation and calling of multisample tumor data, Our method classifies sets of genome-wide DNA copy number measurements into a partitioning of samples into normal ( diploid ) or amplified at each probe It differs from other similar HMM methods in its design specifically for the needs of tumor phylogenetics, by seeking to identify robust markers of progression conserved across a set of copy number profiles. An analysis of our method in comparison to other methods on both synthetic and real tumor data.
2K_test_1538	In many behavioral domains, such as facial expression and gesture, sparse structure is prevalent As a consequence, high-dimensional representations such as SIFT and Gabor features have been favored despite their much greater computational cost and potential loss of information. This sparsity would be well suited for event detection but for one problem, Features typically are confounded by alignment error in space and time that can handle both the temporal alignment problem and the structured sparse reconstruction within a common framework, and it can rely on simple features. We propose a Kernel Structured Sparsity ( KSS ) method We characterize spatio-temporal events as time-series of motion patterns and by utilizing time-series kernels we apply standard structured-sparse coding techniques to tackle this important problem. We evaluated the KSS method using both gesture and facial expression datasets that include spontaneous behavior and differ in degree of difficulty and type of ground truth coding.
2K_test_1539	In contrast to previous systems in which the user interrogates an intermediate representation of visual information, such as a tactile display representing a camera generated image, a potentially useful feature for higher levels analysis of the visual scene. For acquiring and transmitting visual information through haptic channels. Have quantified the user 's ability to discriminate the angle of the edge. We present a novel device mounted on the fingertip our device uses a fingertip-mounted camera and haptic stimulator to allow the user to feel visual features directly from the environment, Visual features ranging from simple intensity or oriented edges to more complex information identified automatically about objects in the environment may be translated in this manner into haptic stimulation of the finger. Experiments using an initial prototype to trace a continuous straight edge.
2K_test_1540	How can we correlate the neural activity in the human brain as it responds to typed words, with properties of these terms ( like 'edible ', 'fits in hand ' ) ? In short, we want to find latent variables, that jointly explain both the brain activity, as well as the behavioral responses, This is one of many settings of the Coupled Matrix-Tensor Factorization ( CMTF ) problem. Can we accelerate any CMTF solver, so that it runs within a few minutes instead of tens of hours to a day, while maintaining good accuracy ? capable of doing exactly that :. TURBO-SMT is able to find meaningful latent variables, as well as to predict brain activity with competitive accuracy. We apply TURBO-SMT to BRAINQ, a dataset consisting of a ( nouns, brain voxels human subjects ) tensor and a ( nouns, properties ) matrix with coupling along the nouns dimension.
2K_test_1541	Our results significantly expand knowledge of eutherian genome evolution and will facilitate greater understanding of the role of chromosome rearrangements in adaptation, speciation and the etiology of inherited and spontaneously occurring diseases. To reconstruct the order and orientation of syntenic fragments in chromosomes of the eutherian ancestor and six other descendant ancestors leading to human, For ancestral chromosome reconstructions.
2K_test_1542	Chemical transformations of silver nanoparticles ( Ag NPs ) and zinc oxide nanoparticles ( ZnO NPs ) during wastewater treatment and sludge treatment must be characterized to accurately assess the risks that these nanomaterials pose from land application of biosolids.
2K_test_1545	With the introduction of next-generation sequencing ( NGS ) technologies, we are facing an exponential increase in the amount of genomic sequence data, The success of all medical and genetic applications of next-generation sequencing critically depends on the existence of computational techniques that can process and analyze the enormous amount of sequence data quickly and accurately. Unfortunately the current read mapping algorithms have difficulties in coping with the massive amounts of data generated by. We propose a new algorithm, FastHASH which drastically improves the performance of the seed-and-extend type hash table based read mapping algorithms, while maintaining the high sensitivity and comprehensiveness of such methods FastHASH is a generic algorithm compatible with all seed-and-extend class read mapping algorithms, It introduces two main techniques, namely Adjacency Filtering and Cheap K-mer Selection, We implemented FastHASH and merged it into the codebase of the popular read mapping program. Depending on the edit distance cutoffs.
2K_test_1546	Most of the relevant prior works involve supervised learning frameworks ( e, Support Vector Machines ), However in-home monitoring provides only coarse ground truth information about symptom occurrences, making it very hard to adapt and train supervised learning classifiers for symptom detection. In this paper we propose for automatic detection of Parkinson 's Disease motor symptoms in daily living environments, Our primary goal is to develop a monitoring system capable of being used outside of controlled laboratory settings, Such a system would enable us to track medication cycles at home and provide valuable clinical feedback. We were able to detect subject specific symptoms ( e, dyskinesia ) that conformed with a daily log maintained by the patients. To use a weakly supervised machine learning framework We address this challenge by formulating symptom detection under incomplete ground truth information as a multiple instance learning ( MIL ) problem, MIL is a weakly supervised learning framework that does not require exact instances of symptom occurrences for training ; rather, it learns from approximate time intervals within which a symptom might or might not have occurred on a given day Once trained, the MIL detector was able to spot symptom-prone time windows on other days and approximately localize the symptom instances. We monitored two Parkinson 's disease ( PD ) patients, each for four days with a set of five triaxial accelerometers and utilized a MIL algorithm based on axis parallel rectangle ( APR ) fitting in the feature space.
2K_test_1547	The proposed soft capsules could be used as minimally invasive tetherless medical devices with therapeutic capability for the next generation capsule endoscopy. For the treatment of gastric disease. Show that the drug release can be controlled by the frequency of the external magnetic pulse, to evaluate the magnetically actuated multimodal drug release capability. The experimental results This paper presents simulations and various experiments.
2K_test_1548	Recognizing facial action units ( AUs ) is important for situation analysis and automated video annotation, Previous work has emphasized face tracking and registration and the choice of features classifiers, Relatively neglected is the effect of imbalanced data for action unit detection, Our findings suggest that skew is a critical factor in evaluating performance metrics. While the machine learning community has become aware of the problem of skewed data for training classifiers, little attention has been paid to how skew may bias performance metrics, To address this question. With exception of area under the ROC curve, all were attenuated by skewed distributions, in many cases dramatically so, While ROC was unaffected by skew, precision-recall curves suggest that ROC may mask poor performance, To avoid or minimize skew-biased estimates of performance, we recommend reporting skew-normalized scores along with the obtained ones. We conducted experiments using both simulated classifiers and three major databases that differ in size, type of FACS coding, and degree of skew We evaluated influence of skew on both threshold metrics ( Accuracy, F-score Cohen 's kappa, and Krippendorf 's alpha ) and rank metrics ( area under the receiver operating characteristic ( ROC ) curve and precision-recall curve ).
2K_test_1549	Maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. Demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes. We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel ( SSP ) model of computation that The parameter server provides an easy-to-use shared interface for read/write access to an ML model 's values ( parameters and variables ), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage, This significantly increases the proportion of time workers spend computing, as opposed to waiting, Furthermore the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results.
2K_test_1550	For making inference about latent spaces of large networks. Our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction. We propose a scalable approach With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efficient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches.
2K_test_1551	This paper introduces a new approach to prediction Specifically, we consider the problem of constructing nonparametric tolerance/prediction sets. The performance of our method is investigated and illustrated. By bringing together two different nonparametric ideas : distribution free inference and nonparametric smoothing We start from the general conformal prediction approach and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution, The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cut-off values, Under standard smoothness conditions, we get an asymptotic efficiency result that is near optimal for a wide range of function classes But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size. Through simulation studies in a real data example.
2K_test_1552	Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D. Manual needle insertion allows the clinician to control the insertion speed, ensuring patient safety for automatic 3D steering of manually inserted flexible needles. Demonstrate the performance of the proposed controller results also show the feasibility of this technique in 2D and 3D environments. This paper presents a control law A look-ahead proportional controller for position and orientation is presented, The look-ahead distance is a linear function of insertion speed. Simulations in a 3D brain-like environment Experimental.
2K_test_1553	Previous studies have examined the characteristics of physiological tremor under laboratory settings as well as different operating conditions, However different test methods make the comparison of results across trials and conditions difficult. This paper presents the characterization and comparison of physiological tremor for pointing tasks in multiple environments, as a baseline for performance evaluation of microsurgical robotics. Two vitroretinal microsurgeons were evaluated while performing a pointing task with no entry-point constraint, constrained by an artificial eye model, and constrained by a rabbit eye in vivo, A spectral analysis was also performed.
2K_test_1554	According to this hypothesis, when symptoms are severe, depressed participants withdraw from other people in order to protect themselves from anticipated rejection, scorn and social exclusion, As their symptoms fade, participants send more signals indicating a willingness to affiliate, suggests that automatic facial expression analysis may be ready for use in behavioral and clinical science. Investigated the relationship between change over time in severity of depression symptoms and facial expression. Automatic and manual coding were highly consistent for FACS action units, and showed similar effects for change over time in depression severity, For both systems when symptom severity was high, participants made more facial expressions associated with contempt, smiled less and those smiles that occurred were more likely to be accompanied by facial actions associated with contempt, These results are consistent with the `` social risk hypothesis '' of depression, The finding that automatic facial expression analysis was both consistent with manual coding and produced the same pattern of depression effects. Depressed participants were followed over the course of treatment and video recorded during a series of clinical interviews, Facial expressions were analyzed from the video using both manual and automatic systems.
2K_test_1556	G protein coupled receptors ( GPCRs ) are seven helical transmembrane proteins that function as signal transducers, They bind ligands in their extracellular and transmembrane regions and activate cognate G proteins at their intracellular surface at the other side of the membrane, The relay of allosteric communication between the ligand binding site and the distant G protein binding site is poorly understood. To identify those that may be involved in communicating the activation signal across the membrane. The GREMLIN-predicted long-range interactions between amino acids were analyzed with respect to the seven GPCR structures that have been crystallized at the time this study was undertaken.
2K_test_1557	For estimating high dimensional undirected graphs from data.
2K_test_1558	Previous works either study the group sparsity in the parametric setting ( e, group lasso ) or address the problem in the nonparametric setting without exploiting the structural information ( e, sparse additive models ). We consider the problem of sparse variable selection in nonparametric additive models, with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly. That GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models, and also conduct a comparative experiment on a real breast cancer dataset. In this paper we present a new method, called group sparse additive models ( GroupSpAM ), which can handle group sparsity in additive models, We generalize the _1/_2 norm to Hilbert spaces as the sparsity-inducing penalty in GroupSpAM, Moreover we derive a novel thresholding condition for identifying the functional sparsity at the group level, and propose an efficient block coordinate descent algorithm for constructing the estimate. We demonstrate by simulation.
2K_test_1559	This paper describes for a surgical microscope, to track the movement of the tip of Micron, show the desired position, and indicate the position error. An inexpensive pico-projector-based augmented reality ( AR ) display The system is designed for use with Micron, an active handheld surgical tool that cancels hand tremor of surgeons to improve microsurgical accuracy, Using the AR display, virtual cues can be injected into the microscope view Cues can be used to maintain high performance by helping the surgeon to avoid drifting out of the workspace of the instrument, Also boundary information such as the view range of the cameras that record surgical procedures can be displayed to tell surgeons the operation area, Furthermore numerical textual or graphical information can be displayed, showing such things as tool tip depth in the work space and on/off status of the canceling function of Micron.
2K_test_1560	We study the problem of estimating a temporally varying coefficient and varying structure ( VCVS ) graphical model underlying data collected over a period of time, such as social states of interacting individuals or microarray expression profiles of gene networks, as opposed to i, data from an invariant model widely considered in current literature of structural estimation, In particular we consider the scenario in which the model evolves in a piece-wise constant fashion. We propose a procedure that estimates the structure of a graphical model by minimizing the temporally smoothed L1 penalized regression, which allows jointly estimating the partition boundaries of the VCVS model and the coefficient of the sparse precision matrix on each block of the partition A highly scalable proximal gradient method is proposed to solve the resultant convex optimization problem ; and the conditions for sparsistent estimation and the convergence rate of both the partition boundaries and the network structure are established for the first time for such estimators.
2K_test_1561	Semantic grounding is the process of relating meaning to symbols ( e, It is the foundation for creating a representational symbolic system such as language. Semantic grounding for verb meaning is hypothesized to be achieved through two mechanisms : sensorimotor mapping, directly encoding the sensorimotor experiences the verb describes, and verb-category mapping i, encoding the abstract category a verb belongs to, This study is the first step towards understanding how words are processed by neurons. Motor and a portion of somatosensory neurons were found to be involved in primarily sensorimotor mapping, while parietal and some somatosensory neurons were found to be involved in both sensorimotor and verb-category mapping, The time course of the spike activities and the selective tuning pattern of these neurons indicate that they belong to a large neural network used for semantic processing. These two mechanisms were investigated by examining neuronal-level spike ( i, neuronal action potential ) activities from the motor, somatosensory and parietal areas in two human participants.
2K_test_1562	Present treatments for ventricular tachycardia have significant drawbacks. That performs mapping and ablation, This paper examines the feasibility of such a system. To ameliorate these drawbacks, it may be advantageous to employ an epicardial robotic walker with precise control of needle insertion depth.
2K_test_1563	However classification of large numbers of subtomograms is a time-intensive task and often a limiting bottleneck, for large-scale subtomogram classification, template matching subtomogram averaging. This paper introduces an open source software platform, TomoMiner Its scalable and robust parallel processing allows efficient classification of tens to hundreds of thousands of subtomograms, In addition TomoMiner provides a pre-configured TomoMinerCloud computing service permitting users without sufficient computing resources instant access to TomoMiners high-performance features.
2K_test_1565	To ensure that the final program works even with a preemptive scheduler. Our experiments demonstrate that our synthesis method is precise and efficient The implicit specification helped us find one concurrency bug previously missed when model-checking using an explicit, observed that different synchronization placements are produced for our experiments, favoring a minimal number of synchronization operations or maximum concurrency. We present a computer-aided programming approach The approach allows programmers to program assuming a friendly, non-preemptive scheduler and our synthesis procedure inserts synchronization The correctness specification is implicit, inferred from the non-preemptive behavior, Let us consider sequences of calls that the program makes to an external interface, The specification requires that any such sequence produced under a preemptive scheduler should be included in the set of sequences produced under a non-preemptive scheduler, We guarantee that our synthesis does not introduce deadlocks and that the synchronization inserted is optimal w, a given objective function, The solution is based on a finitary abstraction, an algorithm for bounded language inclusion moduloan independence relation, and generation of a set of global constraints over synchronization placements, Each model of the global constraints set corresponds to a correctness-ensuring synchronization placement, The placement that is optimal w, the given objective function is chosen as the synchronization solution. We apply the approach to device-driver programming, where the driver threads call the software interface of the device and the API provided by the operating system, We implemented objective functions for coarse-grained and fine-grained locking and.
2K_test_1566	Macroautophagy is regarded as a nonspecific bulk degradation process of cytoplasmic material within the lysosome, indicating stimulus-specific pathways in stress-induced macroautophagy. However the process has mainly been studied by nonspecific bulk degradation assays using radiolabeling. Protein dynamics are linked to image-based models of autophagosome turnover, Depending on the inducing stimulus, protein as well as organelle turnover differ, Amino acid starvation-induced macroautophagy leads to selective degradation of proteins important for protein translation, Thus protein dynamics reflect cellular conditions in the respective treatment. In the present study we monitor protein turnover and degradation by global, unbiased approaches relying on quantitative mass spectrometry-based proteomics, Macroautophagy is induced by rapamycin treatment, and by amino acid and glucose starvation in differentially.
2K_test_1569	In this paper we describe work towards retinal vessel cannulation. To show the higher accuracy of the surface reconstruction as compared to standard stereo reconstruction, to show the increased surgical accuracy due to motion scaling are also carried out. Using an actively stabilized handheld robot, guided by monocular vision We employ a previously developed monocular camera based surface reconstruction method using automated laser beam scanning over the retina, We use the reconstructed plane to find a coordinate transform between the 2D image plane coordinate system and the global 3D frame Within a hemispherical region around the target, we use motion scaling for higher precision, The contribution of this work is the homography matrix estimation using monocular vision and application of the previously developed laser surface reconstruction to Micron guided vein cannulation. Experiments are conducted in a wet eye phantom Further.
2K_test_1570	Neuromechanical simulations have been used to study the spinal control of human locomotion which involves complex mechanical dynamics So far, most neuromechanical simulation studies have focused on demonstrating the capability of a proposed control model in generating normal walking, As many of these models with competing control hypotheses can generate human-like normal walking behaviors, a more in-depth evaluation is required, A model that captures these selective amplifications would be able to explain both steady and reactive spinal control of human locomotion, Neuromechanical simulations that investigate hypothesized control models are complementary to gait experiments in better understanding the control of human locomotion. Here we conduct the more in-depth evaluation on a spinal-reflex-based control model using five representative gait disturbances, ranging from electrical stimulation to mechanical perturbation at individual leg joints and at the whole body. Remarkably similar response trends for the majority of investigated muscles and experimental conditions reinforce the plausibility of the reflex circuits of the model, However the model 's responses lack in amplitude suggesting that in these cases the proposed reflex circuits need to be amplified by additional control structures such as location-specific cutaneous reflexes. The immediate changes in muscle activations of the model are compared to those of humans across different gait phases and disturbance magnitudes, for two experiments with whole body disturbances.
2K_test_1571	Palau has unique features advantageous for this study : due to its population history, Palauans are substantially interrelated ; affected individuals often, but not always cluster in families ; and we have essentially complete ascertainment of affected individuals. To localize genetic variation affecting risk for psychotic disorders in the population of Palau To localize risk variants to genomic regions. This extensive sharing typically identical by descent, was significantly greater in cases than population controls, even after controlling for relatedness, Several regions of the genome exhibited substantial excess of shared haplotypes for affected individuals, including 3p21 3p12 4q28, Two of these regions, 4q28 and 5q23-q31 showed significant linkage by traditional LOD score analysis and could harbor variants of more sizeable risk for psychosis or a multiplicity of risk variants, The pattern of haplotype sharing in 4q28 highlights PCDH10, encoding a cadherin-related neuronal receptor, as possibly involved in risk.
2K_test_1572	We investigate the problem of learning an evolution equation directly from some given data, to identify the terms in the underlying partial differential equations and to approximate the coefficients of the terms in order to perform feature selection and parameter estimation. This work develops a learning algorithm only using data, The algorithm uses sparse optimization The features are data driven in the sense that they are constructed using nonlinear algebraic equations on the spatial derivatives of the data.
2K_test_1573	Three-dimensional live cell imaging of the interaction of T cells with antigen-presenting cells ( APCs ) visualizes the subcellular distributions of signaling intermediates during T cell activation at thousands of resolved positions within a cell, These information-rich maps of local protein concentrations are a valuable resource in understanding T cell signaling. For the efficient acquisition of such imaging data and their computational processing to create four-dimensional maps of local concentrations. Here we describe a protocol This protocol allows quantitative analysis of T cell signaling as it occurs inside live cells with resolution in time and space across thousands of cells.
2K_test_1574	Quantitative image analysis procedures are necessary for the automated discovery of effects of drug treatment in large collections of fluorescent micrographs, These results can function as a baseline for comparison to other protein organization modeling approaches in plant cells. When compared to their mammalian counterparts, the effects of drug conditions on protein localization in plant species are poorly understood and underexplored, To investigate this relationship. We report the dose dependent drug effects in the first high-content Arabidopsis thaliana drug screen of its kind. We generated a large collection of images of single plant cells after various drug treatments, For this protoplasts were isolated from six transgenic lines of A, thaliana expressing fluorescently tagged proteins, Eight drugs at three concentrations were applied to protoplast cultures followed by automated image acquisition For image analysis, we developed a cell segmentation protocol for detecting drug effects using a Hough transform-based region of interest detector and a novel cross-channel texture feature descriptor, In order to determine treatment effects, we summarized differences between treated and untreated experiments with an L1 Cramr-von Mises statistic. The distribution of these statistics across all pairs of treated and untreated replicates was compared to the variation within control replicates to determine the statistical significance of observed effects.
2K_test_1575	Event discovery aims to discover a temporal segment of interest, such as human behavior, Most approaches to event discovery within or between time series use supervised learning, A potential solution to CED is searching over all possible pairs of segments, which would incur a prohibitive quartic cost, We consider extensions to video search and supervised event detection. This becomes problematic when some relevant event labels are unknown, are difficult to detect, or not all possible combinations of events have been anticipated To overcome these problems, this paper explores Common Event Discovery ( CED ), a new problem that aims to discover common events of variable-length segments in an unsupervised manner. In this paper we propose an efficient branch-and-bound ( B & B ) framework that avoids exhaustive search while guaranteeing a globally optimal solution, To this end we derive novel bounding functions for various commonality measures and provide extensions to multiple commonality discovery and accelerated search The B & B framework takes as input any multidimensional signal that can be quantified into histograms A generalization of the framework can be readily applied to discover events at the same or different times ( synchrony and event commonality. The effectiveness of the B & B framework is evaluated in motion capture of deliberate behavior and in video of spontaneous facial behavior in diverse interpersonal contexts : interviews, small groups of young adults, and parent-infant face-to-face interaction.
2K_test_1576	Recent research has uncovered an important These findings illustrate the importance of population-based reference cohorts for the interpretation of candidate pathogenic variants, even for analyses of complex diseases and de novo variation. Role for de novo variation in neurodevelopmental disorders, to identify a subset of LoF-intolerant genes containing the observed signal of associated de novo protein-truncating variants ( PTVs ) in neurodevelopmental disorders.
2K_test_1579	Robust principal component analysis ( PCA ) is one of the most important dimension-reduction techniques for handling high-dimensional data with outliers. Illustrate the effectiveness and superiority of the proposed method. Extensive experimental results on several benchmark data sets.
2K_test_1581	A state estimation technique fuses both absolute state measurements ( GPS, barometer ) and the relative state measurements ( IMU, visual odometry ) for high-altitude MAV odometry calculation. Experimental results show the effectiveness of the proposed state estimation system, especially for the aggressive, intermittent GPS and high-altitude MAV flight. In this paper we present by fusing long-range stereo visual odometry, GPS barometric and IMU ( Inertial Measurement Unit ) measurements The new estimation system has two main parts, a stochastic cloning EKF ( Extended Kalman Filter ) estimator that loosely, and is derived and discussed in detail, A long-range stereo visual odometry is proposed by using both multi-view stereo triangulation and a multi-view stereo inverse depth filter, The odometry takes the EKF information ( IMU integral ) for robust camera pose tracking and image feature matching, and the stereo odometry output serves as the relative measurements for the update of the state estimation. On a benchmark dataset and our real flight dataset.
2K_test_1582	Autonomous robots often rely on models of their sensing and actions for intelligent decision making. However when operating in unconstrained environments, the complexity of the world makes it infeasible to create models that are accurate in every situation This article addresses the problem of using potentially large and high-dimensional sets of robot execution data to detect situations in which a robot model is inaccurate-that is, detecting context-dependent model inaccuracies in a high-dimensional context space. Shows that this approach significantly enhances the detection power of existing RIM-detection algorithms in high-dimensional spaces. To find inaccuracies tractably, the robot conducts an informed search through low-dimensional projections of execution data to find parametric Regions of Inaccurate Modeling ( RIMs ). Empirical evidence from two robot domains.
2K_test_1584	Quantifying differences or similarities in connectomes has been a challenge due to the immense complexity of global brain networks, This novel approach opens a new door for probing the influence of pathological, genetic social or environmental factors on the unique configuration of the human connectome. That allows for a direct comparison between structural connectomes. Here we introduce a noninvasive method that uses diffusion MRI to characterize whole-brain white matter architecture as a single local connectome fingerprint.
2K_test_1585	Clinical decision support tools ( DSTs ) are computational systems that aid healthcare decision-making, While effective in labs, almost all these systems failed when they moved into clinical practice, Healthcare researchers speculated it is most likely due to a lack of user-centered HCI considerations in the design of these systems These findings suggest an alternative perspective to the traditional use models, in which clinicians engage with DSTs at the point of making a decision, We identify situations across patients ' healthcare trajectories when decision supports would help, and we discuss new forms it might take in these situations. This paper describes with a focus on how to best integrate an intelligent DST into their work process. Our findings reveal a lack of perceived need for and trust of machine intelligence, as well as many barriers to computer use at the point of clinical decision-making. A field study investigating how clinicians make a heart pump implant decision.
2K_test_1586	Death by suicide demonstrates profound personal suffering and societal failure, The results provide insight into how advanced technology can be used for suicide assessment and prevention. While basic sciences provide the opportunity to understand biological markers related to suicide, computer science provides opportunities to understand suicide thought markers, to measure and fuse two classes of suicidal thought markers : verbal and nonverbal. In this novel prospective, multimodal multicenter mixed demographic study, we used machine learning.
2K_test_1587	How neural stem cells generate the correct number and type of differentiated neurons in appropriate places remains an important question Although nervous systems are diverse across phyla, in many taxa the larva forms an anterior concentration of serotonergic neurons, which are observed in a great diversity of metazoans This work explains how spatial patterning in the ectoderm controls progression of neurogenesis in addition to providing spatial cues for neuron location. The sea star embryo initially has a pan-neurogenic ectoderm, but the genetic mechanism that directs a subset of these cells to generate serotonergic neurons in a particular location is unresolved. We show that neurogenesis in sea star larvae begins with soxc-expressing multipotent progenitors, These give rise to restricted progenitors that express lhx2/9 soxc- and lhx2/9-expressing cells can undergo both asymmetric divisions, allowing for progression towards a particular neural fate, and symmetric proliferative divisions We show that nested concentric domains of gene expression along the anterior-posterior ( AP ) axis, control neurogenesis in the sea star larva by promoting particular division modes and progression towards becoming a neuron, Modification to the sizes of these AP territories provides a simple mechanism to explain the diversity of neuron number among apical organs.
2K_test_1588	A fundamental problem in comparative genomics is to compute the distance between two genomes in terms of its higher level organization ( given by genes or syntenic blocks ), For two genomes without duplicate genes, we can easily define ( and almost always efficiently compute ) a variety of distance measures, Of the many distance measures, the breakpoint distance ( the number of nonconserved adjacencies ) was the first one to be studied and remains of significant interest because of its simplicity and model-free property, The three breakpoint distance problems corresponding to the three formulations have been widely studied Although we provided last year a solution for the exemplar problem that runs very fast on full genomes. But the problem is NP-hard under most models when genomes contain duplicate genes, To tackle duplicate genes, all of which aim to build a matching between homologous genes so as to minimize some distance measure, computing optimal solutions for the other two problems has remained challenging. We show that our algorithms run very fast ( in seconds ) on mammalian genomes and scale well beyond, We find that our algorithm for the `` any matching '' formulation significantly outperforms other methods in terms of accuracy while achieving nearly maximum coverage. Three formulations ( exemplar, maximum matching and any matching ) have been proposed, In this article we describe very fast, exact algorithms for these two problems Our algorithms rely on a compact integer-linear program that we further simplify by developing an algorithm to remove variables, based on new results on the structure of adjacencies and matchings. Through extensive experiments using both simulations and biological data sets, We also apply these algorithms ( as well as the classic orthology tool MSOAR ) to create orthology assignment, then compare their quality in terms of both accuracy and coverage.
2K_test_1589	Yet how these variants confer liability is uncertain, a resource of gene expression and its genetic regulation. Using this resource Altering expression of FURIN, TSNARE1 or CNTN4 knockdown of FURIN in human neural progenitor cells.
2K_test_1590	Fortunately datasets that contain genotypes, transcripts and phenotypes are becoming more readily available, creating new opportunities for detecting disease-associated genetic variants ) for detecting three-way associations among genotypes, to identify three-way associations with a small computational cost. In this paper we present a novel approach called `` Backward Three-way Association Mapping '' ( BTAM Assuming that genotypes affect transcript levels, which in turn affect phenotypes, we first find transcripts associated with the phenotypes, and then find genotypes associated with the chosen transcripts, The backward ordering of association mappings allows us to avoid a large number of association testings between all genotypes and all transcripts. In our simulation study, Furthermore we apply BTAM on an Alzheimer 's disease dataset.
2K_test_1591	Stable chronic functionality of intracortical probes is of utmost importance toward realizing clinical application of brain-machine interfaces, Sustained immune response from the brain tissue to the neural probes is one of the major challenges that hinder stable chronic functionality, There is a growing body of evidence in the literature that highly compliant neural probes with sub-cellular dimensions may significantly reduce the foreign-body response, thereby enhancing long term stability of intracortical recordings, thereby showing promise toward chronic applications. Since the prevailing commercial probes are considerably larger than neurons and of high stiffness, new approaches are needed for developing miniature probes with high compliance. To demonstrate the versatility of the process to show the co-delivery potential of the needles, Insertion of the needles without mechanical failure, and their subsequent dissolution are demonstrated, It is concluded that ultra-miniature, ultra-compliant probes and associated biodissolvable delivery needles can be successfully fabricated, and the use of the ultra-compliant meandered probes results in drastic reduction in strains imposed in the tissue as compared to stiff probes. Needles from different biodissolvable materials, as well as two-dimensional needle arrays with different geometries and dimensions, Further needles incorporating anti-inflammatory drugs are created.
2K_test_1592	Automated machine-reading biocuration systems typically use sentence-by-sentence information extraction to construct meaning representations for use by curators, This does not directly reflect the typical discourse structure used by scientists to construct an argument from the experimental data available within a article, and is therefore less likely to correspond to representations typically used in biomedical informatics systems ( let alone to the mental models that scientists have ) Although preliminary, these results support the notion that targeting information extraction methods to experimental results could provide accurate, automated methods for biocuration, We also suggest the need for finer-grained curation of experimental methods used when constructing molecular biology databases. To locate extract and classify the individual passages of text from articles ' Results sections that refer to experimental data. We evaluate our system on text passages from articles that were curated in molecular biology databases ( the Pathway Logic Datum repository, the Molecular Interaction MINT and INTACT databases ) linking individual experiments in articles to the type of assay used ( coprecipitation, We use supervised machine learning techniques on text passages containing unambiguous references to experiments.
2K_test_1593	To examine three-junctions in mixed lipid bilayer membranes. We show that the stable phase is the one with the lower defect line tension, The strong and opposite monolayer curvatures present in junctions and edges enhance the mole fraction of negatively curved lipids in junctions and deplete it in edges, This lipid sorting affects the two line tensions and in turn the relative stability of the two phases, It also leads to a subtle entropic barrier for the transition between junction and edge that is absent in uniform membranes. We use a combination of coarse-grained molecular dynamics simulations and theoretical modeling These junctions are localized defect lines in which three bilayers merge in such a way that each bilayer shares one monolayer with one of the other two bilayers, The resulting local morphology is non-lamellar, resembling the threefold symmetric defect lines in inverse hexagonal phases, but it regularly occurs during membrane fission and fusion events, We realize a system of junctions by setting up a honeycomb lattice, which in its primitive cell contains two hexagons and four three-line junctions, permitting us to study their stability as well as their line tension We specifically consider the effects of lipid composition and intrinsic curvature in binary mixtures, which contain a fraction of negatively curved lipids in a curvature-neutral background phase, Three-junction stability results from a competition between the junction and an open edge, which arises if one of the three bilayers detaches from the other two.
2K_test_1595	The paradigm of evidence-based medicine dictates that clinical practice should reflect the shifting landscape of the peer-reviewed literature. Here we examined the extent to which this premise is fulfilled as it pertains to the surgical resection of high-grade gliomas ( HGGs ).
2K_test_1597	Aneuploidy and structural variations ( SVs ) generate cancer genomes containing a mixture of rearranged genomic segments with extensive somatic copy number alterations. However existing methods can identify either SVs or allele-specific copy number alterations but not both simultaneously, which provides a limited view of cancer genome structure for the quantification and analysis of allele-specific copy numbers of SVs. We demonstrate the accuracy of Weaver findingsfrom Our approach provides a more complete assessment of the complex genomic architectures inherent to many cancer genomes. Here we introduce Weaver, an algorithm Weaver uses a Markov random field to estimate joint probabilities of allele-specific copy numbers of SVs and their inter-connectivity based on paired-end whole-genome sequencing data, Weaver also predicts the timing of SVs relative to chromosome amplifications.
2K_test_1598	Wearable activity trackers have become a viable business opportunity, Nevertheless research has raised concerns over their potentially detrimental effects on wellbeing, For example a recent study found that while counting steps with a pedometer increased steps taken throughout the day, at the same time it decreased the enjoyment people derived from walking, This poses a serious threat to the incorporation of healthy routines into everyday life. Most studies aim at proving the effectiveness of activity trackers, In contrast a wellbeing-oriented perspective calls for a deeper understanding of how trackers create and mediate meaningful experiences in everyday life.
2K_test_1599	Advances in fluorescence in situ hybridization ( FISH ) make it feasible to detect multiple copy-number changes in hundreds of cells of solid tumors, Studies using FISH sequencing, and other technologies have revealed substantial intra-tumor heterogeneity, The evolution of subclones in tumors may be modeled by phylogenies Tumors often harbor aneuploid or polyploid cell populations. Using a FISH probe to estimate changes in ploidy can guide the creation of trees that model changes in ploidy and individual gene copy-number variations. Tests on simulated data show improved accuracy of the ploidy-based approach relative to prior ploidyless methods Tests on real data further demonstrate novel insights these methods offer into tumor progression processes Trees for DCIS samples are significantly less complex than trees for paired IDC samples Consensus graphs show substantial divergence among most paired samples from both sets, Low consensus between DCIS and IDC trees may help explain the difficulty in finding biomarkers that predict which DCIS cases are at most risk to progress to IDC, The FISHtrees software is available at ftp : //ftp.
2K_test_1600	The recent proliferation of cryptomarkets and the associated emergence of a sub-field of research on the anonymous web have outpaced the development of an ethical consensus regarding research methods and dissemination amongst scholars working in this unique online space, The peculiar characteristics of cryptomarket research, which often involves encryption, illegal activity large-scale data collection, and geographical separation from research participants, challenge conventional ethical frameworks, A further complicating factor for reaching ethical consensus is the confluence of scholars drawn from a variety of academic disciplines, each with their own particular norms. This paper is intended to stimulate awareness and debate, and to prompt further reflection amongst scholars studying these fascinating online phenomena, The paper explores tensions and addresses some of the more prominent and pressing ethical questions, private online spaces anonymity, data sharing and ownership, risks and threats to research subjects and researchers, Also discussed is how best to balance the potential harms of cryptomarket research against benefits to the public.
2K_test_1601	Efforts to model how signaling and regulatory networks work in cells have largely either not considered spatial organization or have used compartmental models with minimal spatial resolution Fluorescence microscopy provides the ability to monitor the spatiotemporal distribution of many molecules during signaling events. But as of yet no methods have been described for large scale image analysis to learn a complex protein regulatory network, for identifying how changes in concentration in one cell region influence concentration of other proteins in other regions. Here we present and methods.
2K_test_1602	However free and easy-to-use software that incorporates all these functionalities is unavailable, for automated facial feature tracking, head pose estimation facial attribute recognition, and facial expression analysis from video, to discover correlated facial behavior between two or more persons. IF achieved state-of-the-art results for emotion expression and action unit detection in three databases, FERA CK+ and RU-FACS ; measured audience reaction to a talk given by one of the authors ; and discovered synchrony for smiling in videos of parent-infant interaction IF is free of charge for academic use at http : //www. This paper presents IntraFace ( IF ), a publicly-available software package In addition, IFincludes a newly develop technique for unsupervised synchrony detection a.
2K_test_1603	The generativity and complexity of human thought stem in large part from the ability to represent relations among concepts and form propositions. The current study reveals how a given object such as rabbit is neurally encoded differently and identifiably depending on whether it is an agent ( `` the rabbit punches the monkey '' ) or a patient ( `` the monkey punches the rabbit '' ). The classifiers were able to reliably identify the thematic role of an object from its associated fMRI activation pattern, classifiers reliably identified the thematic roles in the data of a left-out participant ( mean accuracy_=_, 66 ) indicating that the neural representations of thematic roles were common across individuals. Machine-learning classifiers were trained on functional magnetic resonance imaging ( fMRI ) data evoked by a set of short videos that conveyed agent-verb-patient propositions, When tested on a held-out video Moreover, when trained on one subset of the study participants.
2K_test_1604	Despite the widespread popularity of genome-wide association studies ( GWAS ) for genetic mapping of complex traits. Most existing GWAS methodologies are still limited to the use of static phenotypes measured at a single time point. In this work we propose a new method that considers dynamic phenotypes measured at a sequence of time points, Our approach relies on the use of Time-Varying Group Sparse Additive Models ( TV-GroupSpAM ) for high-dimensional.
2K_test_1605	Because no assumptions are required about illumination or surface properties, the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video, The software is available online at http : //zface. To enable real-time person-independent 3D registration from 2D video. The method has been validated Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video. In a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction.
2K_test_1606	The environment of a living cell is vastly different from that of an in vitro reaction system, an issue that presents great challenges to the use of in vitro models, or computer simulations based on them, for understanding biochemistry in vivo Virus capsids make an excellent model system for such questions because they typically have few distinct components, making them amenable to in vitro and modeling studies, yet their assembly can involve complex networks of possible reactions that can not be resolved in detail by any current experimental technology, We previously fit kinetic simulation parameters to bulk in vitro assembly data to yield a close match between simulated and real data, and then used the simulations to study features of assembly that can not be monitored experimentally, The work demonstrates how computer simulations can help us understand how assembly might differ between the in vitro and in vivo environments and what features of the cellular environment account for these differences. The present work seeks to project how assembly in these simulations fit to in vitro data would be altered by. The resulting simulations exhibit surprising behavioral complexity, with distinct effects often acting synergistically to drive efficient assembly and alter pathways relative to the in vitro model. Computationally adding features of the cellular environment to the system, specifically the presence of nucleic acid about which many capsids assemble, The major challenge of such work is computational : simulating fine-scale assembly pathways on the scale and in the parameter domains of real viruses is far too computationally costly to allow for explicit models of nucleic acid interaction, We bypass that limitation by applying analytical models of nucleic acid effects to adjust kinetic rate parameters learned from in vitro data to see how these adjustments, singly or in combination, might affect fine-scale assembly progress.
2K_test_1607	Establishing quantitative bounds on the execution cost of programs is essential in many areas of computer science such as complexity analysis, compiler optimizations security and privacy, Techniques based on program analysis, type systems and abstract interpretation are well-studied. But methods for analyzing how the execution costs of two programs compare to each other have not received attention, Naively combining the worst and best case execution costs of the two programs does not work well in many cases because such analysis forgets the similarities between the programs or the inputs, that is capable of establishing precise bounds on the difference in the execution cost of two programs for a higher-order functional language with recursion and subtyping. We prove our type system sound We demonstrate the precision and generality of our technique. In this work we propose a relational cost analysis technique by making use of relational properties of programs and inputs, We develop Rel Cost, a refinement type and effect system The key novelty of our technique is the combination of relational refinements with two modes of typing-relational typing for reasoning about similar computations/inputs and unary typing for reasoning about unrelated computations/inputs, This combination allows us to analyze the execution cost difference of two programs more precisely than a naive non-relational approach. Using a semantic model based on step-indexed unary and binary logical relations accounting for non-relational and relational reasoning principles with their respective costs.
2K_test_1608	During human-robot collaboration a robot and a user must often complete a disjoint set of tasks that use an overlapping set of objects, without using the same object simultaneously, A key challenge is deciding what task the robot should perform next in order to facilitate fluent and efficient collaboration, Most prior work does so by first predicting the human 's intended goal, and then selecting actions given that goal, However it is often difficult, and sometimes impossible to infer the human 's exact goal in real time, and this serial predict-then-act method is not adaptive to changes in human goals. Our human-robot collaboration research aims to improve the fluency and efficiency of interactions between humans and robots when executing a set of tasks in a shared workspace, for inferring a probability distribution over human goals, and producing assistance actions given that distribution in real time, The aim is to minimize the disruption caused by the nature of human-robot shared workspace, to provide assistance without knowing the exact goal. Show that our POMDP model outperforms state of the art predict-then-act models by producing fewer human-robot collisions and less human idling time. In this paper we present a system We extend recent work utilizing Partially Observable Markov Decision Processes ( POMDPs ) for shared autonomy in order.
2K_test_1609	Which recovers 3D scene structure from multiple 2D sonar images, while at the same time localizing the sonar. Demonstrate successful data association results. We evaluate our algorithm in simulation and on real sonar images.
2K_test_1610	Showing significantly longer flight distances over the current state of the art. We demonstrate this method on two flight data sets from a full-sized helicopter.
2K_test_1611	As robots aspire for long-term autonomous operations in complex dynamic environments, the ability to reliably take mission-critical decisions in ambiguous situations becomes critical. This motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision, We call this self-evaluating capability as introspection In this paper, we take a small step in this direction and for introspective behavior in perception systems, Our goal is to learn a model to reliably predict failures in a given system, with respect to a task, directly from input sensor data. And show that it effectively handles uncertain situations. Propose a generic framework. We present this in the context of vision-based autonomous MAV flight in outdoor natural environments.
2K_test_1612	The design of legged robots is often inspired by animals evolved to excel at different tasks, However while mimicking morphological features seen in nature can be very powerful, robots may need to perform motor tasks that their living counterparts do not, In the absence of designs that can be mimicked, an alternative is to resort to mathematical models that allow the relationship between a robot 's form and function to be explored. To co-design the motion and leg configurations of a robot. Our model was successfully used to find optimized designs for legged robots performing tasks that include jumping, walking and climbing up a step, Although our results are preliminary and our analysis makes a number of simplifying assumptions, our findings indicate that the cost function, the sum of squared joint torques over the duration of a task, varies substantially as the design parameters change. In this paper we propose such a model such that a measure of performance is optimized The framework begins by planning trajectories for a simplified model consisting of the center of mass and feet, The framework then optimizes the length of each leg link while solving for associated full-body motions.
2K_test_1613	We relax parametric inference to a non parametric representation towards more general solutions on factor graphs, to maximally exploit structure in the joint posterior to represent a wider class of constraint beliefs. We use the Bayes tree factorization thereby minimizing computation We use kernel density estimation which naturally encapsulates multi-hypothesis and non-Gaussian inference, A variety of new uncertainty models can now be directly applied in the factor graph, and have the solver recover a potentially multi modal posterior, For example data association for loop closure proposals can be incorporated at inference time without further modifications to the factor graph, Our implementation of the presented algorithm is written entirely in the Julia language, exploiting high performance parallel computing. We show a larger scale use case with the well known Victoria park mapping and localization data set inferring over uncertain loop closures.
2K_test_1614	This is the first amortized analysis, that automatically derives polynomial bounds for higher-order functions and polynomial bounds that depend on user-defined inductive types. This article presents a resource analysis system for OCaml programs. The practicality of the analysis system is the system infers bounds on the number of queries that are sent by OCaml programs to DynamoDB, a commercial NoSQL cloud database service. The system automatically derives worst-case resource bounds for higher-order polymorphic programs with user-defined inductive types, The technique is parametric in the resource and can derive bounds for time, memory allocations and energy usage The derived bounds are multivariate resource polynomials which are functions of different size parameters that depend on the standard OCaml types, Bound inference is fully automatic and reduced to a linear optimization problem that is passed to an off-the-shelf LP solver Technically, the analysis system is based on a novel multivariate automatic amortized resource analysis ( AARA ) It builds on existing work on linear AARA for higher-order programs with user-defined inductive types and on multivariate AARA for first-order programs with built-in lists and binary trees Moreover, the analysis handles a limited form of side effects and even outperforms the linear bound inference of previous systems At the same time, it preserves the expressivity and efficiency of existing AARA techniques.
2K_test_1615	For robots with several degrees of freedom, collision checks are computationally expensive and often dominate planning time. For geometric path planning on roadmaps Our goal is to minimize the number of collision checks for obtaining the first feasible path and successively shorter feasible paths. Demonstrate that POMP performs comparably with RRTConnect and LazyPRM for the first feasible path, and BIT { * } for anytime performance, both in terms of collision checks and total planning time. We present POMP ( Pareto Optimal Motion Planner ), an anytime algorithm We assume that the roadmaps we search over are embedded in a continuous ambient space, where nearby points tend to share the same collision state, This enables us to formulate a probabilistic model that computes the probability of unevaluated configurations being collision-free, We update the model over time as more checks are performed This model lets us define a weighting function for roadmap edges that is related to the probability of the edge being in collision Our approach is to trade off between these two weights, gradually prioritizing edge length over collision likelihood, We also show that this tradeoff is approximately equivalent to minimizing the expected path length, with a penalty of being in collision.
2K_test_1616	For mapping of underwater structures with complex geometries. We propose a submap-based technique Our approach relies on the use of probabilistic volumetric techniques to create submaps from multibeam sonar scans, as these offer increased outlier robustness, Special attention is paid to the problem of denoising/enhancing sonar data, Pairwise submap alignment constraints are used in a factor graph framework to correct for navigation drift and improve map accuracy. We provide experimental from the inspection of the running gear and bulbous bow of a 600-foot.
2K_test_1617	Inertial reorientation of airborne articulated bodies has been an active area of research in the robotics community, as this behavior can help guide dynamic robots to a safe landing with minimal damage. The main objective of this work is emulating the aggressive and large angle correction maneuvers, like somersaults that are performed by human divers. Show that the DiverBot can execute one somersault without drift and multiple somersaults with minimal drift. To this end a planar three link robot, called DiverBot is proposed By considering a gravity-free scenario, a local connection is obtained between joint angles and the body orientation, resulting in a reduction in the system dynamics, An optimal control policy applied on this reduced configuration space yielded diving maneuvers that are dynamically feasible.
2K_test_1618	The challenge here is to have a signal and detection system that works from long range ( > 1000m ) amongst ground clutter during various seasonal conditions on passive imagery. That is suitable for an unmanned aerial vehicle to find an optical signal released at a desired landing site for the purposes of cargo delivery or rescue situations where radio signals or other communication systems are not available or the wind conditions at the landing site need to be signaled, to estimate wind orientation and approximate wind strength. We present a long-range visual signal detection system We use a smoke-grenade as a ground signal, which has the advantageous properties of being easy to carry by ground crews because of its light weight and small size, but when released has a long visual signaling range, We employ a camera system on the UAV with a visual texture feature extraction approach in a machine learning framework to classify image patches as `signal ' or `background ', We study conventional approaches and develop a visual feature descriptor that can better differentiate the appearance of the visual signal under varying conditions and, when used to train a random-forest classifier, Further we develop a method by assessing the shape of the smoke signal. The system was rigorously and quantitatively evaluated on data collected from a camera mounted on a helicopter and flown towards a plume of signal smoke over a variety of seasons, ground conditions weather conditions, and environments We present a preliminary evaluation of the wind estimation in conditions with different wind intensities and orientations relative to the approach direction.
2K_test_1619	In a hierarchical motion planning system for urban autonomous driving, it is a common practice to separate tactical reasoning from the lower-level trajectory planning. This separation makes it difficult to achieve robust maneuver-based tactical reasoning, which is intrinsically linked to trajectory planning. The results demonstrate enhanced planning feasibility. We therefore propose a planning method that automatically discovers tactical maneuver patterns, and fuses pattern reasoning and sampling-based trajectory planning.
2K_test_1620	We consider the problem of generating dynamically feasible and safe plans for teams of aerial robots ( quadrotors ) while holding a fixed relative formation as well as transitioning between a sequence of formations. We validate the performance of the proposed approach. We extend the existing assignment and planning approaches for quadrotor teams to find minimal-time trajectories to enable team transition between non-rest initial and ending states while ensuring dynamic feasibility with respect to predefined kinematic, dynamic and collision constraints This work also presents a method for safe splitting and merging of robot formations according to input specification, The proposed methodology is capable of generating dynamically feasible and safe plans for teams of quadrotors in real time. Through various trials and scenarios conducted in simulation.
2K_test_1621	In many multi-robot applications such as target search, environmental monitoring and reconnaissance, the multi-robot system operates semi-autonomously, but under the supervision of a remote human who monitors task progress, In these applications each robot collects a large amount of task-specific data that must be sent to the human periodically to keep the human aware of task progress It is often the case that the human-robot communication links are extremely bandwidth constrained and/or have significantly higher latency than inter-robot communication links, so it is impossible for all robots to send their task-specific data together Thus, only a subset of robots, which we call the knowledge leaders, can send their data at a time. In this paper we study the knowledge leader selection problem, where the goal is to select a subset of robots with a given cardinality that transmits the most informative task-specific data for the human. We prove that the knowledge leader selection is a submodular function maximization problem under explicit conditions The effectiveness of our approach is demonstrated. And present a novel distributed submodular optimization algorithm that has the same approximation guarantees as the centralized greedy algorithm.
2K_test_1622	Formal constructive type theory has proved to be an effective language for mechanized proof, By avoiding non-constructive principles, such as the law of the excluded middle, type theory admits sharper proofs and broader interpretations of results, From a computer science perspective, interest in type theory arises from its applications to programming languages, Standard constructive type theories used in mechanization admit computational interpretations based on meta-mathematical normalization theorems, These proofs are notoriously brittle ; any change to the theory potentially invalidates its computational meaning, As a case in point, Voevodsky 's univalence axiom raises questions about the computational meaning of proofs. We consider the question : Can higher-dimensional type theory be construed as a programming language ? We answer this question affirmatively. By providing a direct, deterministic operational interpretation for a representative higher-dimensional dependent type theory with higher inductive types and an instance of univalence, Rather than being a formal type theory defined by rules, it is instead a computational type theory in the sense of Martin-Lof 's meaning explanations and of the NuPRL semantics, The definition of the type theory starts with programs ; types are specifications of program behavior.
2K_test_1624	Planning in CPSs requires temporal reasoning to handle the dynamics of the environment, including human behavior as well as temporal constraints on system goals and durations of actions that systems and human actors may take, The discrete abstraction of time in a state space planning should have a time sampling parameter value that satisfies some relation to achieve a certain precision, In particular the sampling period should be small enough to allow the dynamics of the problem domain to be modeled with sufficient precision, Meanwhile in many cases, events in the far future ( relative to the sampling period ) may be relevant to the decision making earlier in the planning timeline ; therefore, a longer planning look-ahead horizon can yield a closer-to optimal plan. Unfortunately planning with a uniform fine-grained discrete abstraction of time and a long look-ahead horizon is typically computationally infeasible, to preserve the required time fidelity of the problem domain and at the same time approximate a globally optimal plan. We illustrate our approach. In this paper we propose a multiscale temporal planning approach formulated as MDP planning. In a middleware used to monitor large sensor networks.
2K_test_1625	Modern frameworks are required to be extendable as well as secure. However these two qualities are often at odds, that can improve security while maintaining framework extendability. In this poster we describe an approach that uses a combination of static analysis and run-time management, based on software architecture models, Static analysis identifies the architecture and communication patterns among the collection of apps on an Android device and which communications might be vulnerable to attack, Run-time mechanisms monitor these potentially vulnerable communication patterns, and adapt the system to either deny them, request explicit approval from the user. We implement a prototype of the approach for the Android platform.
2K_test_1626	The Android platform is designed to support mutually un-trusted third-party apps, which run as isolated processes but may interact via platform-controlled mechanisms, Interactions among third-party apps are intended and can contribute to a rich user experience, for example the ability to share pictures from one app with another, The Android platform presents an interesting point in a design space of module systems that is biased toward isolation, extensibility and untrusted contributions, The Intent mechanism essentially provides message channels among modules, in which the set of message types is extensible However, the module system has design limitations including the lack of consistent mechanisms to document message types, very limited checking that a message conforms to its specifications, the inability to explicitly declare dependencies on other modules, and the lack of checks for backward compatibility as message types evolve over time, Based on our results, we outline further research questions and propose possible mitigation strategies. In order to understand the degree to which these design limitations result in real issues. Our findings suggest that design limitations do indeed cause development problems. We studied a broad corpus of apps and cross-validated our results against app documentation and Android support forums.
2K_test_1627	Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed, Some of these issues might be addressed by future tools which autocomplete more complete handlers. A major complaint is that programmers often write minimal and low quality handlers, to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions. We found that programmers handle exceptions locally in catch blocks much of the time, rather than propagating by throwing an Exception, Programmers make heavy use of actions like Log, Print Return or Throw in catch blocks, and also frequently copy code between handlers We found bad practices like empty catch blocks or catching Exception are indeed widespread, We discuss evidence that programmers may misjudge risk when catching Exception, and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception. We used the Boa tool.
2K_test_1628	The theorems are not specific to sequences and can be applied to other data types with different costs for operating on interior and leaf versions. The goal of this paper is ) that are as efficient as imperative arrays, can be used in parallel, and have well defined cost-semantics. The key advantages of the present approach compared to current approaches is that our implementation requires no changes to existing programming languages, supports nested parallelism and has well defined cost semantics, At the same time, it allows for functional implementations of algorithms such as depth-first search with the same asymptotic complexity as imperative implementations. To develop a form of functional arrays ( sequences The key idea is to consider sequences with functional value semantics but nonfunctional cost semantics, Because the value semantics is functional, `` updating { '' } a sequence returns a new sequence, We allow operations on `` older { '' } sequences ( called interior sequences ) to be more expensive than operations on the `` most recent { '' } sequences ( called leaf sequences ), We embed sequences in a language supporting fork-join parallelism, Due to the parallelism, operations can be interleaved non-deterministically, and in conjunction with the different cost for interior and leaf sequences, this can lead to non-deterministic costs for a program, Consequently the costs of programs can be difficult to analyze, The main result is the derivation of a deterministic cost dynamics which makes analyzing the costs easier, We present a wait-free concurrent implementation of sequences that requires constant work for accessing and updating leaf sequences, and logarithmic work for accessing and linear work for updating interior sequences. We sketch a proof of correctness for the sequence implementation.
2K_test_1629	Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation. We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility, then reconstructs face shape by fitting a part-based 3D model, Next the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation, The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze. To evaluate the algorithm in comparison with alternative approaches, three publicly-available databases were used, Boston University Head Tracking, Multi-View Gaze and CAVE Gaze datasets.
2K_test_1630	Massive Open Online Courses ( MOOCs ) have been promoted as a means to revolutionize access to education. In this paper we describe experiences with MOOCs for Liberian students who are connected to the iLab technology hub. We describe their motivations for participating as well as the challenges they encountered We also describe the importance of the face-to-face learning environment provided by the iLab as a source of community support.
2K_test_1631	Massive online classes can benefit from peer interactions such as discussion. However to scaffold productive peer interactions, systems must be able to detect student behavior in interactions at scale, which is challenging when interactions occur over rich media like video, We show how this turn detector can find dominance in video-based conversations, we show how detected conversational turn behavior correlates with participants ' subjective experience in discussions and their final course grade. This paper introduces an imprecise yet simple browser-based conversational turn detector for video conversations, Turns are detected without accessing video or audio data.
2K_test_1632	Although xMOOCs are not designed to directly engage students via social media platforms, some students in these courses join MOOC-associated Facebook groups, These findings have implications for how MOOCs and social media platforms can support learners from non-English speaking contexts. This study explores the prevalence of Facebook groups associated with courses from MITx and HarvardX. Results suggests that a non-trivial number of MOOC students engage in Facebook groups, that learners from a number of non-U, locations are disproportionately likely to participate in such groups, and that the groups display both location and language homophily. The geographic distribution of students in such groups as compared to the courses at large, and the extent to which such groups are location and/or language homophilous.
2K_test_1633	Massive Open Online Courses ( MOOCs ) provide an effective learning platform with various high-quality educational materials accessible to learners from all over the However, current MOOCs lack personalized learning guidance and intelligent assessment for individuals. Though a few recent attempts have been made to trace students ' knowledge states by adapting the popular Bayesian Knowledge Tracing ( BKT ) model, they have largely ignored the rich structures and correlations among knowledge components ( KCs ) within a course. Show our approach significantly improves over previous vanilla BKT models on predicting students ' quiz performance. This paper proposes to model both the hierarchical and the temporal properties of the knowledge states in order to improve the modeling accuracy, Based on the content organization characteristics on the Coursera MOOC platform, we provide a well-defined KC model, and develop Multi-Grained-BKT and Historical-BKT to capture the above features effectively. Experiments on a Coursera course dataset.
2K_test_1634	For covering and packing problems with ( non-linear ) convex objectives.
2K_test_1635	We consider the robust curve fitting problem, for both algebraic and Fourier ( trigonometric ) polynomials, in the presence of outliers.
2K_test_1636	Determine whether a ( T ) X and b ( T ) Y are uncorrelated for every a is an element of R-p, b is an element of R-q or not, Linear independence testing is a fundamental information-theoretic and statistical problem that can be posed as follows : given n points \ { ( X ( i ) ; Y-i ) \ } ( n ) ( i=1 ) from a p + q dimensional multivariate distribution where X-i is an element of R-p and Y-i is an element of R-q. We give minimax lower bound for this problem.
2K_test_1637	With the appearance of fraudsters in social network sites, the importance of trust prediction has increased, Most such methods use only explicit and implicit trust information ( e, if Smith likes several of Johnson 's reviews, then Smith implicitly trusts Johnson ), but they do not consider distrust. Given `` who-trusts/distrusts-whom { '' } information, how can we propagate the trust and distrust ? to handle all three types of interaction information : explicit trust, implicit trust and explicit distrust. In this paper we propose PIN-TRUST, a novel method The novelties of our method are the following : ( a ) it is carefully designed, to take into account positive, implicit and negative information, ( b ) it is scalable ( i, linear on the input size ), ( c ) most importantly, it is effective and accurate. Our extensive experiments with a real dataset, corn data of 100K nodes and 1M edges.
2K_test_1638	Recent work in dense monocular 3D reconstruction relies on dense pixel correspondences and assumes brightness constancy and saliency, and thus are fundamentally unable to reconstruct low-textured or non-lambertian objects such as glass or metal, Occlusion boundaries differ from texture in that each unique view generates a unique set of occlusions. We show how dense reconstructions of challenging objects can be integrated with existing monocular reconstruction algorithms. By detecting and solving for the depths of occlusion boundaries by compensating with an increasing number of unique views.
2K_test_1639	Change introduces conflict into software ecosystems : breaking changes may ripple through the ecosystem and trigger rework for users of a package, but often developers can invest additional effort or accept opportunity costs to alleviate or delay downstream costs Our results illustrate that there is a large design space in how to build an ecosystem, its policies and its supporting infrastructure ; and there is value in making community values and accepted tradeoffs explicit and transparent in order to resolve conflicts and negotiate change-related costs. To understand how developers make decisions about change and change-related costs and what practices, tooling and policies are used. We found that all three ecosystems differ substantially in their practices and expectations toward change and that those differences can be explained largely by different community values in each ecosystem. We performed a multiple case study of three software ecosystems with different tooling and philosophies toward change, Eclipse R/CRAN and Node.
2K_test_1640	We explore online reinforcement learning techniques to find good policies to control the orientation of a mobile robot during social group conversations can be used to find good policies for the robot. Our results show that These policies can generalize across interactions with different numbers of people and can handle various levels of sensing noise. A new state representation that we designed for this problem. In this scenario we assume that the correct behavior for the robot should convey attentiveness to the focus of attention of the conversation, Thus the robot should turn towards the speaker, from tests in a simulated environment.
2K_test_1641	For low-cost awareness of characteristics of dense, moving crowds such as group formation, personal space approximation and occlusion compensation for use in navigating through crowds. Showed good performance in comparison to an existing people detection approach, The projected polygon step captures significantly more people in the scene ( 77\ % vs, 80\ % ) and supports group clustering in dense, complex scenarios Examples are provided for group splitting and merging, dense crowds with obstructions, and cases where other approaches typically encounter difficulty. We describe a method It incorporates social expectations and is inspired by human perceptual processes, The approach uses a single Kinect to cluster all moving objects into groups, applies a 2D polygon projection in obscured regions, and a group personal space modeled using asymmetric Gaussians in order to inhibit certain socially inappropriate robot paths, This approach trades off detection of individual people for higher coverage and lower cost, while preserving high speed processing. A real-world evaluation of this approach.
2K_test_1642	Trajectory planning methods for on-road autonomous driving are commonly formulated to optimize a Single Objective calculated by accumulating Multiple Weighted Feature terms ( SOMWF ). Such formulation typically suffers from the lack of planning tunability, Two main causes are the lack of physical intuition and relative feature prioritization due to the complexity of SOMWF, especially when the number of features is big. This paper addresses this issue by proposing a framework with multiple tunable phases of planning, along with two novel techniques : Optimization-free trajectory smoothing/nudging, Sampling-based trajectory search with cascaded ranking.
2K_test_1643	Proactive latency-aware adaptation is an approach for self-adaptive systems that improves over reactive adaptation by considering both the current and anticipated adaptation needs of the system, and taking into account the latency of adaptation tactics so that they can be started with the necessary lead time, Making an adaptation decision with these characteristics requires solving an optimization problem to select the adaptation path that maximizes an objective function over a finite look-ahead horizon Since this is a problem of selecting adaptation actions in the context of the probabilistic behavior of the environment, Markov decision processes ( MDP ) are a suitable approach, However given all the possible interactions between the different and possibly concurrent adaptation tactics, the system and the environment, constructing the MDP is a complex task, Probabilistic model checking can be used to deal with this problem since it takes as input a formal specification of the stochastic system, which is internally translated into an MDP. One drawback of this solution is that the MDP has to be constructed every time an adaptation decision has to be made to incorporate the latest predictions of the environment behavior, that eliminates that run-time overhead. Results show that this approach reduces the adaptation decision time by an order of magnitude while producing the same results. In this paper we present an approach by constructing most of the MDP offline, also using formal specification, At run time the adaptation decision is made by solving the MDP through stochastic dynamic programming, weaving in the stochastic environment model as the solution is computed. Our experimental compared to the probabilistic model checking approach.
2K_test_1644	Such incentives are likely aligned with benefits to utilities and grid operators, which might take the form of peak-shaving or ancillary services However, private cost savings are not strictly aligned with public benefits related to the avoidance of health and environmental damages from power plant emissions. This paper assesses the potential cost-saving incentives for content distribution networks to shift traffic load among geographically distributed data centers in response to hourly variation in electricity prices. We find that feasible strategies exist to simultaneously realize public and private benefits and that load shifting can result in substantial cost savings and avoided damages in some circumstances, Concerns over increased latency and bandwidth costs can be mitigated with modifications to the model, However the level of realized savings is dependent upon the specifics of a particular network operator and electricity rate schedule. So we compare private cost minimization with a strategy that minimizes these externalities.
2K_test_1645	Personal informatics systems are becoming increasing prevalent as their price, form and ease of use improves, Though these systems offer great potential value to users, many systems are hampered by issues that limit their ability to foster engagement, and people often abandon use of these systems without garnering meaningful outcomes, While continued use of these systems is not necessary for all people, there is an opportunity to better support people working towards achievement-based goals and discuss how these strategies could be used to foster engagement with PI systems. In this paper we draw from the literature and our own prior Work to identify a number of problems that hinder engagement with achievement-based personal informatics systems-problems related to inadequate support for goal setting, misalignment of user and system goals, and the burden of system maintenance, for mitigating these problems. We then propose seven strategies for the design community to explore.
2K_test_1646	Machine learning improves mobile user experience, Interestingly envisioning apps with adaptive interfaces that reduce navigation and selection effort is not standard UX practice, When implementing an adaptive UI for our mobile transit app, we encountered a number of problems. Our original design did not log necessary information nor did it induce users to provide good labels, On reflection we realized UX designers should identify and refine UI adaptions when sketching wireframes, To advance on this insight to communicate planned adaptation and note the information ( logs and labels ) needed to make the desired inferences. Extracted six design patterns where UI adaptation can improve in-app navigation. Next we designed an exemplar set of wireframes, illustrating how UX designers might annotate their interaction flows. We reviewed the interfaces of popular apps and.
2K_test_1647	With the aim of better scaffolding discussion to improve learning in a MOOC context, this work investigates what kinds of discussion behaviors contribute to learning, We explored whether engaging in higher-order thinking behaviors results in more learning than paying general or focused attention to course materials, In order to evaluate whether to attribute the effect to engagement in the associated behaviors versus persistent characteristics of the students. The results of both analyses support the attribution of the effect to the behavioral interpretation, suggests that more social oriented topics triggered richer discussion than more biopsychology oriented topics. We adopted two approaches, First we used propensity score matching to pair students who exhibit a similar level of involvement in other course activities, Second we explored individual variation in engagement in higher-order thinking behaviors across weeks, A further analysis using LDA applied to course materials.
2K_test_1648	We explored players ' gameplay patterns to help us understand player dropout in Quantum Spectre. We found that students ' progress through the first zone of the game seemed to encounter a `` roadblock { '' } during gameplay, dropping out when they can not ( or do not want to ) progress further, These results demonstrate that modeling player behavior can be useful for both assessing learning and for designing complex problem solving content for learning environments. In this study of a Science learning game called Quantum Spectre, Using this prior analysis, alongside Survival Analysis techniques for analyzing time-series data and drop-out rates.
2K_test_1649	Compressed sensing is a simple and efficient technique that has a number of applications in signal processing and machine learning, In machine learning it provides answers to questions such as : `` under what conditions is the sparse representation of data efficient ? { '' } ; `` when is learning a large margin classifier directly on the compressed domain possible ? { '' } ; and `` why does a large margin classifier learn more effectively if the data is sparse ? { '' }. This work tackles the problem of feature representation from the context of sparsity and affine rank minimization in order to provide answers to the aforementioned questions. And show for the high dimensional sparse signals, when the bounds are tight, directly learning in the compressed domain is possible. By leveraging compressed sensing from the learning perspective We show, for a full-rank signal, the high dimensional sparse representation of data is efficient because from the classifiers viewpoint such a representation is in fact a low dimensional problem. We provide practical bounds on the linear classifier to investigate the relationship between the SVM classifier in the high dimensional and compressed domains.
2K_test_1650	An important research problem in learning analytics is. To expedite the cycle of data leading to the analysis of student progress and the improvement of student support, For this goal in the context of social learning. Which suggests ways in which we might foster these social benefits through intervention. We propose a pipeline that includes data infrastructure, learning analytics and intervention, along with computational models for individual components. Next we describe an example of applying this pipeline to real data in a case study, whose goal is to investigate the positive effects that goal-setting students have on their peers.
2K_test_1651	Various trends are reshaping content delivery on the Internet : the explosive growth of traffic due to video, users ' increasing expectations for higher quality of experience ( QoE ), and the proliferation of server capacity from a variety of sources ( e, cloud computing services content provider-owned datacenters, CDNs and ISP-owned CDNs ), In order to meet the scale and quality demands imposed by users, content providers have started to spread demand across multiple CDNs using a broker, Brokers break many traditional CDN assumptions ( e, unexpected traffic skew and significant variance in demand over short timescales ). We take the first steps towards improvement. We show the potential challenges and opportunities that brokers impart on content delivery. Through a redesigned broker-CDN interface. Through an analysis of data from a leading broker and a leading CDN.
2K_test_1652	The core number of a node is the highest k-core in which the node participates, Core numbers are useful in many graph mining tasks, especially ones that involve finding communities of nodes, influential spreaders and dense subgraphs, Large graphs often do not fit on the memory of a single machine, Existing external memory solutions do not give bounds on the required space. We address the problem of estimating core numbers of nodes by reading edges of a large graph stored in external memory, In practice existing solutions also do not scale with the size of the graph, estimates core numbers of nodes. Demonstrate that Nimble Core gives space savings up to 60X, while accurately estimating core numbers with average relative error less than 2. We propose Nimble Core, an iterative external-memory algorithm, which using O ( n log d ( max ) ) space, where n is the number of nodes and d ( max ) is the maximum node-degree in the graph, We also show that Core requires O ( n ) space for graphs with power-law degree distributions. Nimble Experiments on forty-eight large graphs from various domains.
2K_test_1653	Making effective problem selection decisions is a challenging Self-Regulated Learning skill Students need to learn effective problem-selection strategies but also develop the motivation to use them, A mastery-approach orientation is generally associated with positive problem selection behaviors such as willingness to work on new materials Our experiment contributes to prior literature by demonstrating that with tutor features to foster a mastery-approach orientation, shared control over problem selection can lead to significantly better learning outcomes than full system control. To investigate the effectiveness of shared control over problem selection with mastery-oriented features ( i, features that aim at fostering a mastery-approach orientation that simulates effective problem-selection behaviors ) on students ' domain-level learning outcomes, problem-selection skills enjoyment future learning and future problem selection. The results show that shared control over problem selection accompanied by mastery-oriented features leads to significantly better learning outcomes, as compared to fully system-controlled problem selection, as well as better declarative knowledge of a key problem-selection strategy Nevertheless, there was no effect on future problem selection and future learning.
2K_test_1654	The results show that the metacognitive scaffolding facilitated tutor learning ( regardless of the presence of the cognitive scaffolding ), whereas cognitive scaffolding had virtually no effect The same pattern was confirmed. We conducted a classroom study to test these hypotheses in the context of learning to solve equations by teaching a synthetic peer, by two additional datasets collected from two previous school studies we conducted.
2K_test_1655	We hypothesize that when cognitive tutors are integrated into online courseware, the online courseware can provide a new type of adaptive instructions, such as impasse-driven adaptive remediation and need-based assessments, As a proof of concept. The results show that the proposed adaptive online course technology is robust enough to be used in actual classroom with mixed effect for learning. We have developed an adaptive online course on the Open Learning Initiative ( OLI ) platform by integrating four new instances of cognitive tutors into an existing OLI course, Cognitive tutors were created with an innovative cognitive tutor authoring system called WATSON. To evaluate the effectiveness of the adaptive online course, a quasi-experiment was conducted in a gateway course at Carnegie Mellon University.
2K_test_1656	Ambiguity arises in requirements when a statement is unintentionally or otherwise incomplete, missing information or when a word or phrase has more than one possible meaning. For web-based and mobile information systems, ambiguity and vagueness in particular, undermines the ability of organizations to align their privacy policies with their data practices, which can confuse or mislead users thus leading to an increase in privacy risk, The theory predicts how vague modifiers to information actions and information types can be composed to increase or decrease overall vagueness. To yield a rank order of vague terms in both isolation and composition, to show how increases in vagueness will decrease users ' acceptance of privacy risk and thus decrease users ' willingness to share personal information. The taxonomy was evaluated in a paired comparison experiment and results were analyzed using the Bradley-Terry model We further provide empirical evidence based on factorial vignette surveys.
2K_test_1657	Confidentiality of training data induced by releasing machine-learning models, and has recently received increasing attention, which to the best of our knowledge, were not previously known. This paper initiates a formal study of MI attacks. Interestingly we also discovered an intriguing phenomenon, which we call `` invertibility interference, { '' } where a highly invertible model quickly becomes highly non-invertible by adding little noise, We show that even very restricted communication between layers could leak a significant amount of information Perhaps more importantly, our study also unveils unexpected computational power of these restricted communication channels. Motivated by existing MI attacks and other previous attacks that turn out to be MI `` in disguise, { '' } by presenting a game-based methodology Our methodology uncovers a number of subtle issues, and devising a rigorous game-based definition, analogous to those in cryptography, is an interesting avenue for future work, We describe methodologies for two types of attacks The first is for black-box attacks, which consider an adversary who infers sensitive values with only oracle access to a model, The second methodology targets the white-box scenario where an adversary has some additional knowledge about the structure of a model, For the restricted class of Boolean models and black-box attacks, we characterize model invertibility using the concept of influence from Boolean analysis in the noiseless case, and connect model invertibility with stable influence in the noisy case, For the white-box case, we consider a common phenomenon in machine-learning models where the model is a sequential composition of several sub-models.
2K_test_1658	Social networking sites ( SNSs ) offer users a platform to build and maintain social connections. Understanding when people feel comfortable sharing information about themselves on SNSs is critical to a good user experience, because self-disclosure helps maintain friendships and increase relationship closeness to measure self-disclosure in SNSs to understand the contexts where it is higher or lower. Results show that women self-disclose more than men, People with a stronger desire to manage impressions self-disclose less Network size is negatively associated with self-disclosure, while tie strength and network density are positively associated. This observational research develops a machine learning model and uses it Features include emotional valence, social distance between the poster and people mentioned in the post, the language similarity between the post and the community and post topic. To validate the model and advance our understanding about online self-disclosure, we applied it to de-identified, aggregated status updates from Facebook users.
2K_test_1659	Anonymity online is important to people at times in their lives, Anonymous communication applications such as Whisper and YikYak enable people to communicate with strangers anonymously through their smartphones Our results provide implications for how anonymity in mobile apps can encourage expressiveness and interaction among users. The goal of our study was to identify why and how people use anonymous apps, their perceptions of their audience and interactions on the apps, and how these apps compare with other online social communities. People share various types of content that range from deep confessions and secrets to lighthearted jokes and momentary feelings, An important driver for participation and posting is to get social validation from others, even though they are anonymous strangers We also find that participants believe these anonymous apps allow more honesty, openness and diversity of opinion than they can find elsewhere. We present a typology of the content people share, and their motivations for participation in anonymous apps.
2K_test_1660	When health services involve long-term treatment over months or years, providers have the ability, not present in acute emergency care, to collaboratively reflect on clients ' changing health data and adjust interventions, Current literature shows a bias toward standardized records and routines in the implementation of health information technology, a policy that may not be appropriate for long-term health services, We discuss how the design of information systems should vary based on temporal factors. In this paper we discuss temporality as a factor in the design of health information technology. Our fieldwork in this context complements and provides contrasts to previous CSCW studies performed in time-critical hospital settings. We define a temporal spectrum ranging from time-critical services that benefit from standardization to long-term services that require more flexibility, We provide empirical evidence from fieldwork that we performed in organizations providing long-term behavioral and mental health services for children.
2K_test_1661	Hackathons are events where people who are not normally collocated converge for a few days to write code together, Hackathons it seems are everywhere Our findings have implications for technology support that needs to be in place for hackathons and for understanding the role of brief interludes of collocation in loosely-coupled. We know that long-term collocation helps advance technical work and facilitate enduring interpersonal relationships, but can similar benefits come from brief, hackathon-style collocation ? How do participants spend their time preparing, working face-to-face and following through these brief encounters ? Do the activities participants select suggest a tradeoff between the social and technical benefits of collocation ?. We present results that suggest the way that hackathon-style collocation advances technical work varies across technical domain, community structure and expertise of participants, Building social ties in contrast, seems relatively constant across hackathons, Results from different hackathon team formation strategies suggest a tradeoff between advancing technical work and building social ties. From a multiple-case study.
2K_test_1662	People are more creative at solving difficult design problems when they use relevant examples from outside of the problem 's domain as inspirations. However finding such `` outside-the-box { '' } inspirations is difficult, particularly in large idea repositories such as the web, because without guidance people select domains to search based on surface similarity to the problem 's domain. Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own, or drew inspiration from domains closer to or not sharing structural correspondence with the original problem. In this paper we demonstrate an approach in which non-experts identify domains that have the potential to yield useful and non-obvious inspirations for solutions. We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains.
2K_test_1663	Large graph datasets have caused renewed interest for graph partitioning. However existing well-studied graph partitioners often assume that vertices of the graph are always active during the computation, which may lead to time-varying skewness for traversal-style graph workloads, like Breadth First Search, since they only explore part of the graph in each superstep, Additionally existing solutions do not consider what vertices each partition will have ; as a result, high-degree vertices may be concentrated into a few partitions, Towards this where the objective is to create an initial partitioning that will `` hold well { '' } over time without suffering from skewness. We introduce the idea of skew-resistant graph partitioning Skewresistant graph partitioning tries to mitigate skewness by taking the characteristics of both the target workload and the graph structure into consideration.
2K_test_1664	Current dialogue systems typically lack a variation of audio-visual feedback tokens, Either they do not encompass feedback tokens at all, or only support a limited set of stereotypical functions, However this does not mirror the subtleties of spontaneous conversations. If we want to be able to build an artificial listener, as a first step towards building an empathetic artificial agent, we also need to be able to synthesize more subtle audio-visual feedback tokens, to understand how different realisations of verbal and visual feedback tokens influence third-party perception of the degree of attentiveness. In this study we devised an array of monomodal and multimodal binary comparison perception tests and experiments. This allowed us to investigate i ) which features ( amplitude, ) of the visual feedback influences attentiveness perception ; ii ) whether visual or verbal backchannels are perceived to be more attentive iii ) whether the fusion of unimodal tokens with low perceived attentiveness increases the degree of perceived attentiveness compared to unimodal tokens with high perceived attentiveness taken alone ; iv ) the automatic ranking of audio-visual feedback token in terms of conveyed degree of attentiveness.
2K_test_1665	A previous approach utilizes gaze duration and word rarity features to perform this detection, However while this system can be used by trained users, its performance is not sufficient during natural reading by untrained users. To detect unknown words during natural reading of non-native language text. The experimental results demonstrate that learning using SVMs and proposed eye movement features improves detection performance and that personalization further improves results. ) examine the effect of personalization, as measured by F-measure.
2K_test_1666	Automatic emotion recognition plays a central role in the technologies underlying social robots, affect-sensitive human computer interaction design and affect-aware tutors. Although there has been a considerable amount of research on automatic emotion recognition in adults, emotion recognition in children has been understudied, This problem is more challenging as children tend to fidget and move around more than adults, leading to more self-occlusions and non-frontal head poses, Also the lack of publicly available datasets for children with annotated emotion labels leads most researchers to focus on adults. Finally we present a detailed analysis of the most indicative behavioral cues for emotion recognition in children. Our experiments compare unimodal and multimodal emotion recognition baseline models to enable future research on this topic.
2K_test_1667	Cyber-attacks aimed at breaking into networks and bringing websites down appear to have become an every-day phenomenon. But there is less clarity on where the attacks come from and who are the top targets, to understand the cyber-attacks network. Using which we summarize the major players and trends in DDoS cyber-attacks. We take a high-level view of attacks mostly considering aggregate country-to-country attacks.
2K_test_1668	Cyber-attacks are cheap easy to conduct and often pose little risk in terms of attribution, but their impact could be lasting, The low attribution is because tracing cyber-attacks is primitive in the current network architecture, Moreover even when attribution is known, the absence of enforcement provisions in international law makes cyber attacks tough to litigate, and hence attribution is hardly a deterrent, Rather than attributing attacks, we can re-look at cyber-attacks as societal events associated with social, political economic and cultural ( SPEC ) motivations, Because it is possible to observe SPEC motives on the internet, social media data could be valuable in understanding cyber attacks. To observe country-to-country perceptions to build ground truth of country-to-country DDoS cyber-attacks, Using this dataset this research makes three important contributions : a ) We evaluate the impact of heightened sentiments towards a country on the trend of cyber-attacks received by the country. We find that for some countries, the probability of attacks increases by up to 27\ % while experiencing negative sentiments from other nations, To verify our model. B ) Using cyber-attacks trend and sentiments trend, we build a decision tree model to find attacks that could be related to extreme sentiments. In this research we use sentiment in Twitter posts, and Arbor Networks data we describe three examples in which cyber-attacks follow increased tension between nations, as perceived in social media.
2K_test_1669	Increasing proliferation of mobile and online social networking platforms have given us unprecedented opportunity to observe and study social interactions at a fine temporal scale, A collection of all such social interactions among a group of individuals ( or agents ) observed over an interval of time is referred to as a temporally-detailed ( TD ) social network, A TD social network opens up the opportunity to explore TD questions on the underlying social system, `` How is the betweenness centrality of an individual changing with time ? { '' } To this end, related work has proposed temporal extensions of centrality metrics ( e, betweenness and closeness ). However scalable computation of these metrics for long time-intervals is challenging, This is due to the non-stationary ranking of shortest paths ( the underlying structure of betweenness and closeness ) between a pair of nodes which violates the assumptions of classical dynamic programming based techniques, for addressing the non-stationarity challenge of TD social networks. We prove the correctness and completeness of our algorithm, shows that the proposed algorithm out performs the alternatives by a wide margin. To this end we propose a novel computational paradigm called epoch-point based techniques Using the concept of epoch-points, we develop a novel algorithm for computing shortest path based centrality metric such as betweenness on a TD social network.
2K_test_1670	Software architects inhabit a complex, rapidly evolving technological landscape, These must be overcome by future research in order to make our vision of curated knowledge bases a reality. An ever growing collection of competing architecturally significant technologies, ranging from distributed databases to middleware and cloud platforms, makes rigorously comparing alternatives and selecting appropriate solutions a daunting engineering task, To address this problem that enable straightforward and streamlined technical comparisons of related products. We report in this paper on the initial results of using supervised machine learning to assist with knowledge base curation, Our results show immense promise in recommending Web pages that are highly relevant to curators, We also describe the major obstacles, both practical and scientific, that our work has uncovered. We envisage an ecosystem of curated, automatically updated knowledge bases These knowledge bases would emulate engineering handbooks that are commonly found in other engineering disciplines As a first step towards this vision, we have built a curated knowledge base for comparing distributed databases based on a semantically defined feature taxonomy.
2K_test_1671	Large-scale deep learning requires huge computational resources to train a multi-layer neural network, Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections. While the computation involved can be done more efficiently on GPUs than on more traditional CPU cores, training such networks on a single GPU is too slow and training on distributed GPUs can be inefficient, due to data movement overheads, GPU stalls and limited GPU memory, that supports scalable deep learning across GPUs distributed among multiple machines. This paper describes a new parameter server, called GeePS overcoming these obstacles.
2K_test_1672	That improves ML algorithm convergence speed. We show that SchMP programs running on STRADS outperform non-model-parallel ML implementations : for example, SchMP LDA and SchMP Lasso respectively achieve 10x and 5x faster convergence than recent. We propose scheduled model parallelism ( SchMP ), a programming approach by efficiently scheduling parameter updates, taking into account parameter dependencies and uneven convergence, To support SchMP at scale, we develop a distributed framework STRADS which optimizes the throughput of SchMP programs, and benchmark four common ML applications written as SchMP programs : LDA topic modeling, matrix factorization sparse least-squares ( Lasso ) regression and sparse logistic regression By improving ML progress per iteration through SchMP programming whilst improving iteration throughput through STRADS.
2K_test_1673	Today 's cellular core, which connects the radio access network to the Internet, relies on fixed hardware appliances placed at a few dedicated locations and uses relatively static routing policies, As such today 's core design has key limitations-it induces inefficient provisioning tradeoffs and is poorly equipped to handle overload, failure scenarios and diverse application requirements, To address these limitations, ongoing efforts envision `` clean slate { '' } solutions that depart from cellular standards and routing protocols ; e, via programmable switches at base stations and per-flow SDN-like orchestration. The driving question of this work is to ask if a clean-slate redesign is necessary and if not, how can we design a flexible cellular core that is minimally disruptive. Show that KLEIN can scale to billions of devices and is close to optimal for wide variety of traffic and deployment parameters. We propose KLEIN a design that stays within the confines of current cellular standards and addresses the above limitations by combining network functions virtualization with smart resource management, We address key challenges w, scalability and responsiveness in realizing KLEIN via backwards-compatible orchestration mechanisms. Our evaluations through data-driven simulations and real prototype experiments using OpenAirInterface.
2K_test_1674	Mutual adaptation is critical for effective team collaboration. For human-robot mutual adaptation in collaborative tasks, which captures human adaptive behaviors which enables robot adaptation to the human. Indicate that the proposed formalism can significantly improve the effectiveness of human-robot teams, while human subject ratings on the robot performance and trust are comparable to those achieved by cross training, a state-ofthe-art human-robot team training practice. This paper presents a formalism We propose the bounded-memory adaptation model ( BAM ), based on a bounded memory assumption, We integrate BAM into a partially observable stochastic model, When the human is adaptive, the robot will guide the human towards a new, optimal collaborative strategy unknown to the human in advance, When the human is not willing to change their strategy, the robot adapts to the human in order to retain human trust.
2K_test_1675	This is the first demonstration that robot appearance affects people 's moral judgments about robots. That people blame robots more for inaction than action in a moral dilemma but blame humans more for action than inaction in the identical dilemma ( where inaction allows four persons to die and action sacrifices one to save the four ). We found further evidence for a previously discovered Human-Robot ( HR ) asymmetry in moral judgments : Importantly, we found that people 's representation of the `` robot { '' } making these moral decisions appears to be one of a mechanical robot, people showed the HR asymmetry only when making judgments about a mechanical-looking robot, not a humanoid robot. In three studies For when we manipulated the pictorial display of a verbally described robot.
2K_test_1676	When interacting with robots deployed in the open world, people may often attempt to engage with them in a playful manner or test their competencies, and frame research directions and design implications for robots deployed in the wild. Such engagements are often associated with language and behaviors that fall outside of designed task capabilities and can lead to interaction failures, Detecting when users are driven by play and curiosity can help a robot to understand why some interactions are breaking down, respond more appropriately by conveying its capabilities to its users, and enhance perceptions of its situational awareness and social intelligence. We report on a pilot field-study We discuss early results from this initial study. We have been studying the intentions of everyday users in their engagement with a long-lived robot system that provides directions within an office building, exploring the use of direct queries to elicit the sincerity of user requests, in terms of their actual need for directions.
2K_test_1677	2D alignment of face images works well provided images are frontal or nearly so and pitch and yaw remain modest, In spontaneous facial behavior, these constraints often are violated by moderate to large head rotation, 3D alignment from 2D video has been proposed as a solution, The results suggest that 3D alignment from 2D video is feasible on a wide range of face orientations, Differences among methods are considered and suggest directions for further research. A number of approaches have been explored, but comparisons among them have been hampered by the lack of common test data, To enable comparisons among alternative methods. We report results for four that provided necessary technical descriptions of their methods, The leading approach achieved prediction consistency error of 3, Corresponding result for the lowest ranked approach was 5. Made training and validation sets available to investigators, and invited them to test their algorithms on an independent test-set, Eight teams accepted the challenge and submitted test results.
2K_test_1678	For each participating tracker, a short description is provided in the Appendix. The Visual Object Tracking challenge VOT2016 aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. The VOT2016 goes beyond its predecessors by ( i ) introducing a new semi-automatic ground truth bounding box annotation methodology and ( ii ) extending the evaluation system with the no-reset experiment.
2K_test_1679	Much robotics research has focused on intent-expressive ( legible ) motion. However algorithms that can autonomously generate legible motion have implicitly made the strong assumption of an omniscient observer, with access to the robot 's configuration as it changes across time, In reality human observers have a particular viewpoint, which biases the way they perceive the motion. That the produced motions are significantly more legible compared to those generated assuming an omniscient observer. Show through large-scale user studies.
2K_test_1680	Nonverbal behaviors play an important role in communication for both humans and social robots, However adding contextually appropriate animations by hand is time consuming and does not scale well, Previous researchers have developed automated systems for inserting animations based on utterance text, yet these systems lack human understanding of social context and are still being improved. This work proposes a middle ground where untrained human workers label semantic information, which is input to an automatic system to produce appropriate gestures. Results showed untrained workers are capable of providing reasonable labeling of semantic information and that emotional expressions derived from the labels were rated more highly than control videos, More study is needed to determine the effects of emphasis labels. To test this approach, untrained workers from Mechanical Turk labeled semantic information, specifically emotion and emphasis, for each utterance which was used to automatically add animations, Videos of a robot performing the animated dialogue were rated by a second set of participants.
2K_test_1681	Morphable face models are a powerful tool. But have previously failed to model the eye accurately due to complexities in its material and motion captures eye region shape to allow independent eyeball movement. We present a new multi-part model of the eye that includes a morphable model of the facial eye region, as well as an anatomy-based eyeball model, It is the first morphable model that accurately, since it was built from high-quality head scans, It is also the first, since we treat it as a separate part, To showcase our model we present a new method for illumination-and head-pose invariant gaze estimation from a single RGB image, We fit our model to an image through analysis-by-synthesis, solving for eye region shape, texture eyeball pose and illumination simultaneously, The fitted eyeball pose parameters are then used to estimate gaze direction. Through evaluation on two standard datasets.
2K_test_1682	Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes, While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts We believe that the realism, diversity and casual nature of this dataset will present unique challenges and new opportunities for computer vision community. So how do we collect sufficiently many diverse but boring samples representing our lives ? to collect such data. Provide baseline results for several tasks including action recognition and automatic description generation. Using this rich data.
2K_test_1683	We consider codes over fixed alphabets against worst case symbol deletions.
2K_test_1684	Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets, For example babies push objects, poke them put them in their mouth and throw them to learn representations. What is the right supervisory signal to train visual representations ? However, in case of biological agents, visual representation learning does not require millions of semantic labels, We argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations ( images and videos downloaded from web ). Towards this goal we build one of the first systems on a Baxter platform that pushes, pokes grasps and observes objects in a tabletop environment It uses four different types of physical interactions to collect more than 130K datapoints, with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Quantitatively we evaluate our learned ConvNet on image classification tasks.
2K_test_1685	Discrete energy minimization is widely-used in computer vision and machine learning for problems such as MAP inference in graphical models The problem, in general is notoriously intractable, and finding the global optimal solution is known to be NP-hard This paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms. However is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time ? We show in this paper that the answer is no. Specifically we show that general energy minimization, even in the 2-label pairwise case, and planar energy minimization with three or more labels are exp-APX-complete, This finding rules out the existence of any approximation algorithm with a sub-exponential approximation ratio in the input size for these two problems, including constant factor approximations. Moreover we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes - PO, APX and exp-APX corresponding to problems that are solvable, approximable and inapproximable in polynomial time, Problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones.
2K_test_1686	What happens if one pushes a cup sitting on a table toward the edge of the table ? How about pushing a desk against a wall ?. In this paper we study the problem of understanding the movements of objects as a result of applying external forces to them For a given force vector applied to a specific location in an image, our goal is to predict long-term sequential movements caused by that force. Show that the challenging task of predicting long-term movements of objects as their reaction to external forces is possible from a single image The code and dataset are available at : http : //allenai.
2K_test_1687	New IT functions have greatly increased the amount of in-car information delivered to drivers, Although valuable that information can distract drivers when delivered during vehicle operation, By inferring driver state from sensor data, prior research has shown that it can accurately identify opportune moments to deliver information, With these results researchers can then build information delivery systems that can deliver information to drivers both when they are interruptible and when they find the information valuable. Now that we know when to best deliver information, it raises the question : what information should we deliver at those interruptible moments ?. We identified driving situations when each of the in-car information items is highly valuable, and verified these situations Results from our study offer important insights for understanding the diversity of drivers ' experiences about the value of in-car information and the ability to determine situations in which this information is valuable to drivers. To answer this question, we conducted a series of surveys and interviews and compiled a list of representative in-car information items and context factors that affect the importance of these items By combining and exploring those context factors through a large online survey of drivers, Lastly we examined what technology is available for detecting these driving situations, and which situations require further advanced technologies for detection.
2K_test_1688	In a given scene, humans can easily predict a set of immediate future events that might happen, However pixel-level anticipation in computer vision is difficult because machine learning struggles with the ambiguity in predicting the future. In this paper we focus on predicting the dense trajectory of pixels in a scene-what will move in the scene, where it will travel, and how it will deform over the course of one second. We show that our method predicts events in a variety of scenes and can produce multiple different predictions for an ambiguous future, We also find that our method learns a representation that is applicable to semantic vision tasks. We propose a conditional variational autoencoder as a solution to this problem In this framework, direct inference from the image shapes the distribution of possible trajectories while latent variables encode information that is not available in the image.
2K_test_1689	There is a growing interest in behavior based biometrics, Although biometric data has considerable variations for an individual and may be faked, yet the combination of such `weak experts ' can be rather strong, A remotely detectable component is gaze direction estimation and thus. For gaze estimation systems. We show that it improves the precision of gaze direction estimation algorithms considerably. Here we present a novel personalization method which does not require a precise calibration setup, can be non-obtrusive is fast and easy to use The method is convenient ; we exploit 3D face model reconstruction for the enrichment of a small number of collected data artificially.
2K_test_1690	Software architecture modeling is important for analyzing system quality attributes, However such analyses often assume that the architecture is completely known in advance, In many modern domains, especially those that use plugin-based frameworks, it is not possible to have such a complete model because the software system continuously changes, The Android mobile operating system is one such framework, where users can install and uninstall apps at run time. We need ways to model and analyze such architectures that strike a balance between supporting the dynamism of the underlying platforms and enabling analysis, particularly throughout a system 's lifetime, that captures the modifiable architectures of Android systems, and that supports security analysis as a system evolves. That indicates that the architecture can be amenable for use throughout the system 's lifetime. In this paper we describe a formal architecture style We illustrate the use of the style with two security analyses : a predicate-based approach defined over architectural structure that can detect some common security vulnerabilities, and inter-app permission leakage determined by model checking We also show how the evolving architecture of an Android device can be obtained by analysis of the apps on a device. Provide some performance evaluation.
2K_test_1691	Language usage behavior of users evolves over time, as they interact on social media such as Twitter. We study the evolution of language usage behavior of individuals, across topics on microblogs, to model such evolution. Our work is applicable in predicting activity and influence, interest evolution job change and community change expected to happen to a user. We propose Man-O-Meter a framework We model the evolution using a combination of three dimensions : ( a ) time, ( b ) content ( topics ) and ( c ) influence flow over social relationships We assert the goodness of our approach, by predicting ranks of experts, with respect to their influence in their respective expertise category, using the change in language used in time.
2K_test_1692	Conducted a number of ablation experiments on a population of worms in which one or more of the neurons in the TW circuit are surgically ablated ( removed ). We present The neural circuit underlying this response is the subject of this investigation. Of a ( nonlinear ODE ) model of a neural circuit in Caeorhabditis elegans ( C, elegans ) the common roundworm, Our approach to this problem rests on encoding each TW response as a hybrid automaton with parametric uncertainty, In contrast our technique allow us to more thoroughly explore the models parameter space using statistical sampling theory, identifying in the process the distribution of TW responses.
2K_test_1693	Problems of this nature arise in formal verification of continuous and hybrid dynamical systems, where there is an increasing need for methods to expedite formal proofs, we discuss and illustrate certain classes of problems where this relationship is interesting. This paper studies sound proof rules for checking positive invariance of algebraic and semi-algebraic sets, that is sets satisfying polynomial equalities and those satisfying finite boolean combinations of polynomial equalities and inequalities, under the flow of polynomial ordinary differential equations. The relationship between increased deductive power and running time performance of the proof rules is far from obvious. We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of benchmarks.
2K_test_1694	Suppose you are a teacher, and have to convey a set of object-property pairs ( 'lions eat meat ' ), A good teacher will convey a lot of information, with little effort on the student side, What is the best and most intuitive way to convey this information to the student, without the student being overwhelmed ?. A related harder problem is : how can we assign a numerical score to each lesson plan ( i, way of conveying information ? Here, we give a formal definition of this problem of forming learning units and we provide a metric for comparing different approaches based on information theory. It is effective achieving excellent results both with respect to our proposed metric, but also with respect to encoding length demonstrate the effectiveness of GROUPNTEACH. We also design an algorithm, GROUPNTEACH for this problem, Our proposed GROUPNTEACH is scalable ( near-linear in the dataset size ) ; and it is intuitive, conforming to wellknown educational principles. On real data Experiments on real and synthetic datasets.
2K_test_1695	Recent research has improved our understanding of how to create strong, and suggest ways to ease password entry for mobile users. However this research has generally been in the context of desktops and laptops, while users are increasingly creating and entering passwords on mobile devices, In this paper we study whether recent password guidance carries over to the mobile setting. We compare the strength and usability of passwords created and used on mobile devices with those created and used on desktops and laptops, while varying password policy requirements and input methods.
2K_test_1696	Friendsourcing consists of broadcasting questions and help requests to friends on social networking sites, Despite its potential value, friendsourcing requests often fall on deaf ears, One way to improve response rates and motivate friends to undertake more effortful tasks may be to offer extrinsic rewards, such as money or a gift, for responding to friendsourcing requests, However past research suggests that these extrinsic rewards can have unintended consequences, including undermining intrinsic motivations and undercutting the relationship between people. To explore the effects of extrinsic reward on friends ' response rate and perceived relationship. Results indicate that large extrinsic rewards increase friends ' response rates without reducing the relationship strength between friends Additionally, the extrinsic rewards allow requesters to explain away the failure of friendsourcing requests and thus preserve their perceptions of relationship ties with friends. We conducted an experiment on a new friendsourcing platform - Mobilyzr.
2K_test_1697	People accumulate huge assortments of a possessions, but it is not yet clear how systems and system designers can help people make meaning from these large archives, Early research in HCl has suggested that people generally appear to value their virtual things less than their material things, but theory on material possessions does not entirely explain this difference, We conclude with implication and strategies for aimed at supporting people in having more meaningful interactions and experiences with their virtual possessions. To investigate if changes to the form and behavior of virtual things may surface valued elements of a virtual archive. Our study revealed insights about how materializing virtual possessions influences factors shaping how people draw on, understand and value those possessions. We designed a technology probe that selected snippets from old emails and mailed them as physical postcards to participating households The probe uncovered features of emails that trigger meaningful reflection, and how contextual information can help people engage in reminiscence.
2K_test_1698	Timebanking is a growing type of peer-to-peer service exchange, but is hampered by the effort of finding good transaction partners. We seek to reduce this effort. And shows that such an algorithm can retrieve matches that are subjectively better than matches based on matching the category of people 's historical offers or requests to the category of a current transaction request. By using a Matching Algorithm for Service Transactions ( MAST ), MAST matches transaction partners in terms of similarity of interests and complementarity of abilities and needs. We present an experiment involving data and participants from a real timebanking network, that evaluates the acceptability of MAST.
2K_test_1699	From these findings we argue for the importance of extensions in supporting modularity, community engagement and relatable prototyping materials in the iterative design of prosthetics. This paper presents to design prosthetic devices for specific tasks : playing the cello, operating a hand-cycle and using a table knife, Our goal was to identify requirements for a design process that can engage the assistive technology user in rapidly prototyping assistive devices that fill needs not easily met by traditional assistive technology. We discuss materials that support on-the-spot design and iteration, dimensions along which in-person iteration is most important ( such as length and angle ) and the value of a supportive social network for users who prototype their own assistive technology. A case study of three participants with upper-limb amputations working with researchers Our study made use of 3D printing and other playful and practical prototyping materials.
2K_test_1700	Crowdsourcing offers a powerful new paradigm for online work However, real world tasks are often interdependent, requiring a big picture view of the difference pieces involved, Existing crowdsourcing approaches that support such tasks - ranging fromWikipedia to flash teams-are bottlenecked by relying on a small number of individuals to maintain the big picture, that may be informative for other systems aimed at supporting big picture thinking in small pieces. In this paper we explore the idea that a computational system can scaffold an emerging interdependent, big picture view entirely through the small contributions of individuals, each of whom sees only a part of the whole, To investigate the viability, strengths and weaknesses of this approach. We instantiate the idea in a prototype system for accomplishing distributed information synthesis and We also contribute a set of design patterns. Evaluate its output across a variety of topics.
2K_test_1701	More and more data nowadays exist in hierarchical formats such as JSON due to the increasing popularity of web applications and web services. While many end-user systems support getting hierarchical data from databases without programming, they provide very little support for using hierarchical data beyond turning the data into a flat string or table for using and exploring hierarchical datasets to manipulate and visualize hierarchical data in a spreadsheet to support selecting, grouping joining sorting and filtering hierarchical data in spreadsheets. Showed that our tool helped spreadsheet users complete data exploration tasks nearly two times faster than using Excel and even outperform programmers in most tasks. In this paper we present a spreadsheet tool We introduce novel interaction techniques and algorithms using the data 's relative hierarchical relationships with the data in its adjacent columns Our tool leverages the data 's structural information.
2K_test_1702	The present research investigated whether digital and non-digital platforms activate differing default levels of cognitive construal. Two initial randomized experiments A pair of final studies compared to ( equivalent ) performance levels exhibited by participants who had either completed no prior activity or completed an activity activating a concrete mindset.
2K_test_1703	Crowdsourced clustering approaches present a promising way to harness deep semantic knowledge for clustering complex information. However existing approaches have difficulties supporting the global context needed for workers to generate meaningful categories, and are costly because all items require human judgments supports greater global context improves efficiency to help a machine cluster the head of the distribution, then classify low-confidence examples in the tail, to stitch together different types of judgment tasks. We introduce Alloy a hybrid approach that combines the richness of human judgments with the power of machine algorithms, Alloy through a new `` sample and search { '' } crowd pattern which changes the crowd 's task from classifying a fixed subset of items to actively sampling and querying the entire dataset, It also through a two phase process in which crowds provide examples To accomplish this, Alloy introduces a modular `` cast and gather { '' } approach which leverages a machine learning backbone.
2K_test_1704	Although many users create predictable passwords, the extent to which users realize these passwords are predictable is not well understood, We conclude with design directions for helping users make better passwords. We investigate the relationship between users ' perceptions of the strength of specific passwords and their actual strength. Participants had serious misconceptions about the impact of basing passwords on common phrases and including digits and keyboard patterns in passwords, However in most other cases, participants ' perceptions of what characteristics make a password secure were consistent with the performance of current password-cracking tools, We find large variance in participants ' understanding of how passwords may be attacked, potentially explaining why users nonetheless make predictable passwords. In this 165-participant online study, we ask participants to rate the comparative security of carefully juxtaposed pairs of passwords, as well as the security and memorability of both existing passwords and common password-creation strategies.
2K_test_1705	Stateless model checking is a powerful technique for testing concurrent programs, but suffers from exponential state space explosion when the test input parameters are too large, Several reduction techniques can mitigate this explosion, but even after pruning equivalent interleavings, the state space size is often intractable Most prior tools are limited to pre-empting only on synchronization APIs, which reduces the space further, but can miss unsynchronized thread communication bugs. Data race detection another concurrency testing approach, focuses on suspicious memory access pairs during a single test execution, It avoids concerns of state space size, but may report races that do not lead to observable failures, which jeopardizes a user 's willingness to use the analysis, which manages the exploration of many state spaces to prioritize jobs to add new preemption points on the fly. 25x as many bugs and verified 4, 3x as many tests compared to prior model checking approaches. We present QUICKSAND a new stateless model checking framework using different preemption points It uses state space estimation most likely to complete in a fixed CPU budget, and it incorporates data-race analysis Preempting threads during a data race 's instructions can automatically classify the race as buggy or benign, and uncovers new bugs not reachable by prior model checkers, It also enables full verification of all possible schedules when every data race is verified as benign within the CPU budget.
2K_test_1706	Previous work on muscle activity sensing has leveraged specialized sensors such as electromyography and force sensitive resistors Our work is the first to explore the feasibility of using solely motion sensors on everyday wearable devices to detect fine-grained gestures, This promising technology can be deployed today on current smartwatches and has the potential to be applied to cross-device interactions, or as a tool for research in fields involving finger and hand motion. While these sensors show great potential for detecting finger/hand gestures, they require additional hardware that adds to the cost and user discomfort, Past research has utilized sensors on commercial devices, focusing on recognizing gross hand gestures, In this work we present Serendipity for recognizing unremarkable and fine-motor finger gestures. A new technique using integrated motion sensors ( accelerometer and gyroscope ) in off-the-shelf smartwatches.
2K_test_1707	Clinical decision support tools ( DSTs ) are computational systems that aid healthcare decision-making While effective in labs, almost all these systems failed when they moved into clinical practice Healthcare researchers speculated it is most likely due to a lack of user-centered HCI considerations in the design of these systems, and we discuss new forms it might take in these situations. This paper describes investigating how clinicians make a heart pump implant decision with a focus on how to best integrate an intelligent DST into their work process. Our findings reveal a lack of perceived need for and trust of machine intelligence, as well as many barriers to computer use at the point of clinical decision-making, These findings suggest an alternative perspective to the traditional use models, in which clinicians engage with DSTs at the point of making a decision, We identify situations across patients ' healthcare trajectories when decision supports would help.
2K_test_1708	There is a significant gap in the body of research on cross-device interfaces. Research has largely focused on enabling them technically, but when and how users want to use cross-device interfaces is not well understood, This paper presents to enable non-technical users to adapt existing single-device web interfaces for cross-device use while viewing them in the browser, In particular we identify the need to easily switch between different interface distributions depending on the task and to have more fine-grained control over synchronization. With XDBrowser a cross-device web browser we are developing We describe the design space in this context, the usage scenarios targeted by users the strategies used for designing cross-device interfaces, and seven concrete mobile multi-device design patterns that emerged, We discuss the method.
2K_test_1709	Social media is an increasingly important part of modern life While Twitter has traditionally been thought of as the most accessible social media platform for blind users, Twitter 's increasing integration of image content and users ' diverse uses for images have presented emergent accessibility challenges. We investigate the use of and usability of Twitter by blind users, We propose changes that Twitter and other social platforms should make to promote fuller access to users with visual impairments. Our findings illuminate the importance of the ability to use social media for people who are blind, while also highlighting the many challenges such media currently present this user base, including difficulty in creating profiles, in awareness of available features and settings, in controlling revelations of one 's disability status, and in dealing with the increasing pervasiveness of image based content. Via a combination of surveys of blind Twitter users, large-scale analysis of tweets from and Twitter profiles of blind and sighted users, and analysis of tweets containing embedded imagery.
2K_test_1710	Due to the rapid deployability and low cost of the tags used, we can create a new class of interactive paper devices that are drawn on demand for simple tasks, These capabilities allow new interactive possibilities for pop-up books and other papercraft objects. That allow inexpensive ultra-thin, battery-free Radio Frequency Identification ( RFID ) tags to be turned into simple paper input devices. We describe techniques We use sensing and signal processing techniques that determine how a tag is being manipulated by the user via an RFID reader and show how tags may be enhanced with a simple set of conductive traces that can be printed on paper, stencil-traced or even hand-drawn, These traces modify the behavior of contiguous tags to serve as input devices Our techniques provide the capability to use off-the-shelf RFID tags to sense touch, cover overlap of tags by conductive or dielectric ( insulating ) materials, and tag movement trajectories, Paper prototypes can be made functional in seconds.
2K_test_1711	Which has been successfully used for this purpose, AutoSLEX finds the best basis in a library of smoothed localized exponentials ( SLEX ) basis functions that are orthogonal and localized in both time and frequency. We address the problem of segmenting a multi-dimensional time series into stationary blocks. We demonstrate the utility of the proposed improvements. On synthetic and real data.
2K_test_1712	Current symbol-based dictionaries providing vocabulary support for persons with the language disorder, aphasia are housed on smartphones or other portable devices, To employ the support on these external devices requires the user to divert their attention away from their conversation partner, to the neglect of conversation dynamics like eye contact or verbal inflection, A prior study investigated head-worn displays ( HWDs ) as an alternative form factor for supporting glanceable, unobtrusive and always-available conversation support, Our findings should motivate further work on head-worn conversation support for persons with aphasia. But it did not directly compare the HWD to a control condition, To address this limitation. We compared vocabulary support on a HWD to equivalent support on a smartphone in terms of overall experience, perceived focus and conversational success, Lastly we elicited critical discussion of how each device might be better designed for conversation support.
2K_test_1713	When navigating indoors blind people are often unaware of key visual information, such as posters signs, With VizMap we move towards integrating the strengths of the end user, on-site crowd online crowd, and computer vision to solve a long-standing challenge in indoor blind exploration. To collect this information and make it available non-visually. Our VizMap system uses computer vision and crowdsourcing VizMap starts with videos taken by on-site sighted volunteers and uses these to create a 3D spatial model, These video frames are semantically labeled by remote crowd workers with key visual information, These semantic labels are located within and embedded into the reconstructed 3D model, forming a query-able spatial representation of the environment, VizMap can then localize the user with a photo from their smartphone, and enable them to explore the visual elements that are nearby. We explore a range of example applications enabled by our reconstructed spatial representation.
2K_test_1714	Playtesting or using play to guide game design, gives designers feedback about whether their game is meeting their goals and the player 's expectations We conclude with lessons learned and next steps in our research on playtesting. We report a of designing, deploying and iterating on a series of playtesting workshops for novice game designers. Novice game designers leveraged playtest methods and tools, employed playtesting and data collection methods appropriate for their goals, and effectively applied playtest data in iterative design. Case study We identify common missteps made by novice designers and address these missteps through the concept of purposefulness, understanding why you are playtesting as well as how to playtest, We ground our workshops in the development of rich player experience goals, which inform playtest design, data collection and iteration, We show that by applying methods taught in our workshops.
2K_test_1715	Unease over data privacy will retard consumer acceptance of IoT deployments. The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud, This is a direct consequence of the over-centralization of today 's cloud-based IoT hub designs. We propose a solution that interposes a locally-controlled software component called a privacy mediator on every raw sensor stream, Each mediator is in the same administrative domain as the sensors whose data is being collected, and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain, This solution necessitates a logical point of presence for mediators within the administrative boundaries of each organization, Such points of presence are provided by cloudlets, which are small locally-administered data centers at the edge of the Internet that can support code mobility, The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility.
2K_test_1716	For refinement relations on hybrid systems for verifying such relations This paper gives a syntax, semantics and proof calculus for dRL. We demonstrate its usefulness results in easier and better-structured proofs. We introduce differential refinement logic ( dRL ), a logic with first-class support and a proof calculus dRL simultaneously solves several seemingly different challenges common in theorem proving for hybrid systems : By using a refinement relation to arrange proofs hierarchically according to the structure of natural subsystems, we can increase the readability and modularity of the resulting proof, dRL extends an existing specification and verification language for hybrid systems ( differential dynamic logic, dL ) by adding a refinement relation to directly compare hybrid systems. With examples where using refinement.
2K_test_1717	Self-adaptive systems have the ability to adapt their behavior to dynamic operating conditions, In reaction to changes in the environment, these systems determine the appropriate corrective actions based in part on information about which action will have the best impact on the system. Existing models used to describe the impact of adaptations are either unable to capture the underlying uncertainty and variability of such dynamic environments, or are not compositional and described at a level of abstraction too low to scale in terms of specification effort required for non-trivial systems, In this paper we address these shortcomings to the specification of impact models allows us to represent both variability and uncertainty in the outcome of adaptations. Can improve the accuracy of predictions used for decision-making. By describing an approach based on architectural system descriptions, which at the same time, hence improving the selection of the best corrective action, The core of our approach is a language equipped with a formal semantics defined in terms of Discrete Time Markov Chains that enables us to describe both the impact of adaptation tactics, as well as the assumptions about the environment. To validate our approach, we show how employing our language in the Rainbow framework for architecture-based self-adaptation.
2K_test_1718	Everyday tools and objects often need to be customized for an unplanned use or adapted for specific user, such as adding a bigger pull to a zipper or a larger grip for a pen The advent of low-cost 3D printing offers the possibility to rapidly construct a wide range of such adaptations, We believe this work would benefit makers and designers for prototyping lifehacking solutions and assistive technologies. However while 3D printers are now affordable enough for even home use, the tools needed to design custom adaptations normally require skills that are beyond users with limited 3D modeling experience. In this paper we describe Reprise-a design tool for specifying, generating customizing and fitting adaptations onto existing household objects, Reprise allows users to express at a high level what type of action is applied to an object Based on this high level specification, Reprise automatically generates adaptations, Users can use simple sliders to customize the adaptations to better suit their particular needs and preferences, such as increasing the tightness for gripping, enhancing torque for rotation, or making a larger base for stability, Finally Reprise provides a toolkit of fastening methods and support structures for fitting the adaptations onto existing objects.
2K_test_1719	Patients researching medical diagnoses, scientist exploring new fields of literature, and students learning about new domains are all faced with the challenge of capturing information they find for later use, However saving information is challenging on mobile devices, where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use Furthermore, beyond the challenge of simply selecting a region of bounded text on a mobile device, in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user, In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster. We introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration, to support identifying and saving information in an intentionally uncertain way on mobile devices. We find that this approach reduced selection time and was preferred by participants over the default system text selection method. We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context. In a two part user study.
2K_test_1720	The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals, Blind people can not independently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance, and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone. That can robustly and interactively help blind people use nearly any interface they encounter.
2K_test_1721	The recent advances in image captioning stimulate the research in generating natural language description for visual content, which can be widely applied in many applications such as assisting blind people, Video description generation is a more complex task than image caption, Most works of video description generation focus on visual information in the video. However audio provides rich information for describing video contents as well, to generate video descriptions in natural sentences. Prove that fusing audio information greatly improves the video description performance. In this paper we propose using both audio and visual cues, We use unified deep neural networks with both convolutional and recurrent structure. Experimental results on the Microsoft Research Video Description ( MSVD ) corpus.
2K_test_1723	An important feature of functional programs is that they are parallel by default. Implementing an efficient parallel functional language, however is a major challenge, in part because the high rate of allocation and freeing associated with functional programs requires an efficient and scalable memory manager for parallel memory management for strict functional languages with nested parallelism. We prove the safety of this collector In addition, we describe how the proposed techniques can be implemented on modern shared-memory machines. In this paper we present a technique At the highest level of abstraction, the approach consists of a technique to organize memory as a hierarchy of heaps, and an algorithm for performing automatic memory reclamation by taking advantage of a disentanglement property of parallel functional programs, More specifically the idea is to assign to each parallel task its own heap in memory and organize the heaps in a hierarchy/tree that mirrors the hierarchy of tasks, We present a nested-parallel calculus that specifies hierarchical heaps and prove in this calculus a disentanglement property, which prohibits a task from accessing objects allocated by another task that might execute in parallel, Leveraging the disentanglement property, we present a garbage collection technique that can operate on any subtree in the memory hierarchy concurrently as other tasks ( and/or other collections ) proceed in parallel, and present a prototype implementation as an extension to MLton, a high-performance compiler for the Standard ML language. By formalizing it in the context of our parallel calculus, Finally we evaluate the performance of this implementation on a number of parallel benchmarks.
2K_test_1724	Micro-clones are small pieces of redundant code, such as repeated subexpressions or statements, Our results suggest that the detection and removal of micro-clones is valued by developers, can be automated at scale, and may be fixed with rapid turnaround times. In this paper we establish the considerations and value toward automated detection and removal of micro-clones at scale.
2K_test_1725	Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in imperfect-information games. However imperfect-recall abstractions are poorly understood, and only weak algorithm-specific guarantees on solution quality are known. They show that running counterfactual regret minimization on such abstractions leads to good strategies in the original games. We develop the first general, algorithm-agnostic solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions, when implemented in the original ( perfect-recall ) game, Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions for which any such results have been obtained, Further our analysis is tighter in two ways, each of which can lead to an exponential reduction in the solution quality error bound, We then show that for extensive-form games that satisfy certain properties, the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem, where the increase in our bound is the distance function, This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds, We proceed to show a divide in the class of abstraction problems, If payoffs are at the same scale at all information sets considered for abstraction, the input forms a metric space, and this immediately yields a 2-approximation algorithm for abstraction, Conversely if this condition is not satisfied, we show that the input does not form a metric space. Finally we provide computational experiments to evaluate the practical usefulness of the abstraction techniques.
2K_test_1726	To provide load balancing in the face of unpredictable workload skew, to enable efficient content-based routing. Results demonstrate that SwitchKV can achieve up to 5x throughput and 3x latency improvements over traditional system designs. SwitchKV is a new key-value store system design that combines high-performance cache nodes with resource-constrained backend nodes The cache nodes absorb the hottest queries so that no individual backend node is over-burdened, Compared with previous designs, SwitchKV exploits SDN techniques and deeply optimized switch hardware Programmable network switches keep track of cached keys and route requests to the appropriate nodes at line speed, based on keys encoded in packet headers, A new hybrid caching strategy keeps cache and switch forwarding rules updated with low overhead and ensures that system load is always well-balanced under rapidly changing workloads.
2K_test_1727	Multi-stage log-structured ( MSLS ) designs, such as LevelDB RocksDB, HBase and Cassandra are a family of storage system designs that exploit the high sequential write speeds of hard disks and flash drives by using multiple append-only data structures. Towards accurate and fast evaluation of MSLS that quickly give accurate performance estimates. Find optimized system parameters that decrease LevelDB 's insert cost by up to 9, 2\ % ; our analytic primitives and model also suggest changes to RocksDB that reduce its insert cost by up to 32, 0\ % without reducing query performance or requiring extra memory. As a first step, we propose new analytic primitives and MSLS design models Our model can almost perfectly estimate the cost of inserts in LevelDB, whereas the conventional worst-case analysis gives 1, 5X higher estimates than the actual cost. A few minutes of offline analysis using our model can.
2K_test_1728	Motivation : Reconstructing regulatory networks from expression and interaction data is a major goal of systems biology, While much work has focused on trying to experimentally and computationally determine the set of transcription-factors ( TFs ) and microRNAs ( miRNAs ) that regulate genes in these networks, relatively little work has focused on inferring the regulation of miRNAs by TFs, Such regulation can play an important role in several biological processes including development and disease, The main challenge for predicting such interactions is the very small positive training set currently available, Another challenge is the fact that a large fraction of miRNAs are encoded within genes making it hard to determine the specific way in which they are regulated, and can be used by any method that combines miRNAs. To enable genome wide predictions of TF-miRNA interactions. Results As we show, the methods we develop achieve good performance demonstrating the advantage of using the predicted set of interactions for identifying more coherent and relevant modules, genes and miRNAs The complete set of predictions is available on the supporting website. We extended semisupervised machine-learning approaches to integrate a large set of different types of data including sequence, expression ChIP-seq and epigenetic data. On both a labeled test set, and when analyzing general co-expression networks, We next analyze mRNA and miRNA cancer expression data.
2K_test_1729	For compositional verification of security properties of extensible hypervisors written in C and Assembly. We validate uSpark and demonstrating only minor performance overhead with low verification costs. We present uberSpark ( uSpark ), an innovative architecture uSpark comprises two key ideas : ( i ) endowing low-level system software with abstractions found in higher-level languages ( e, objects interfaces function-call semantics for implementations of interfaces, access control on interfaces, concurrency and serialization ), enforced using a combination of commodity hardware mechanisms and lightweight static analysis ; and ( ii ) interfacing with platform hardware by programming in Assembly using an idiomatic style ( called CASM ) that is verifiable via tools aimed at C, while retaining its performance and low-level access to hardware After verification, the C code is compiled using a certified compiler while the CASM code is translated into its corresponding Assembly instructions, Collectively these innovations enable compositional verification of security invariants without sacrificing performance. By building and verifying security invariants of an existing open-source commodity x86 micro-hypervisor and several of its extensions.
2K_test_1730	Human-chosen text passwords today 's dominant form of authentication, are vulnerable to guessing attacks. Unfortunately existing approaches for evaluating password strength by modeling adversarial password guessing are either inaccurate or orders of magnitude too large and too slow for real-time. We show that neural networks can often guess passwords more effectively than state-of-the-art approaches, such as probabilistic context-free grammars and Markov models We also show that our neural networks can be highly compressed-to as little as hundreds of kilobytes-without substantially worsening guessing effectiveness, Together our contributions enable more accurate and practical password checking than was previously possible. We propose using artificial neural networks to model text passwords ' resistance to guessing attacks and explore how different architectures and training methods impact neural networks ' guessing effectiveness, Building on these results, we implement in JavaScript the first principled client-side model of password guessing, which analyzes a password 's resistance to a guessing attack of arbitrary duration with sub-second latency.
2K_test_1731	Modern RDMA hardware offers the potential for exceptional performance. But design choices including which RDMA operations to use and how to use them significantly affect observed performance, to navigate the RDMA design space, We also present and evaluate several new RDMA optimizations and pitfalls, and discuss how they affect the design of RDMA systems. That outperforms an existing design by 50x, and improve the CPU efficiency of a prior high-performance key-value store by 83\ %. This paper lays out guidelines that can be used by system designers Our guidelines emphasize paying attention to low-level details such as individual PCIe transactions and NIC architecture. We empirically demonstrate how these guidelines can be used to improve the performance of RDMA-based systems we design a networked sequencer.
2K_test_1732	In this paper we empirically explore how the latency of transcriptions created by participants recruited on Amazon Mechanical Turk vary based on the accuracy of speech recognition output. We present results which indicate that starting with the ASR output is worse unless it is sufficiently accurate ( Word Error Rate of under 30\ % ).
2K_test_1733	Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines, which could be of interest to studies on website profiling, Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models. Yet there has been relatively little work in modeling the behaviors and temporal properties of websites, as most research focuses on detecting whether a website distributes malware, In this paper we ask : How does web-based malware spread ?. In order to conduct this study, we develop a classifier to distinguish between compromised vs. We conduct an extensive study and follow a website-centric and user-centric point of view, We collect data from four online databases, including Symantec 's WINE Project, for a total of more than 600K malicious URLs and over 500K users.
2K_test_1734	Modern Internet applications are being disaggregated into a microservice-based architecture, with services being updated and deployed hundreds of times a day, The accelerated software life cycle and heterogeneity of language runtimes in a single application necessitates a new approach for testing the resiliency of these applications in production infrastructures. For systematically testing the failure-handling capabilities of microservices. We present Gremlin a framework Gremlin is based on the observation that microservices are loosely coupled and thus rely on standard message-exchange patterns over the network, Gremlin allows the operator to easily design tests and executes them by manipulating inter-service messages at the network layer, We show how to use Gremlin to express common failure scenarios and how developers of an enterprise application were able to discover previously unknown bugs in their failure-handling code without modifying the application.
2K_test_1735	We consider the task of multiparty computation performed over networks in the presence of random noise the goal is to find a coding scheme that takes R ' rounds and computes the same function with high probability even when the communication is noisy, while maintaining a constant asymptotic rate, while keeping inf ( n, R - > infinity ) R/R ' positive. We revisit this question and provide an efficient coding scheme with a constant rate for the interesting case of fully connected networks. That if a ( d-regular ) network has mixing time m.
2K_test_1736	Concurrency bugs that stem from schedule-dependent branches are hard to understand and debug. Their root causes imply not only different event orderings, but also changes in the control-flow between failing and non-failing executions that helps exposing and understanding concurrency bugs that result from schedule-dependent branches. Shows that Cortex is able to expose failing schedules with only a few perturbations to non-failing executions, and takes a practical amount of time. We present Cortex : a system without relying on information from failing executions Cortex preemptively exposes failing executions by perturbing the order of events and control-flow behavior in non-failing schedules from production runs of a program, By leveraging this information from production runs, Cortex synthesizes executions to guide the search for failing schedules, Production-guided search helps cope with the large execution search space by targeting failing executions that are similar to observed non-failing. Evaluation on popular benchmarks.
2K_test_1738	To obtain a speedup from this.
2K_test_1739	The widespread availability of high-quality motion capture data and the maturity of solutions to animate virtual characters has paved the way for the next generation of interactive virtual worlds exhibiting intricate interactions between characters and the environments they inhabit. However current motion synthesis techniques have not been designed to scale with complex environments and contact-rich motions, requiring environment designers to manually embed motion semantics in the environment geometry in order to address online motion synthesis in order to represent the different ways in which an environment can afford a character to move. This paper presents an automated approach for analyzing both motions and environments We extract the salient features that characterize the contact-rich motion repertoire of a character and detect valid transitions in the environment where each of these motions may be possible, along with additional semantics that inform which surfaces of the environment the character may use for support during the motion, The precomputed motion semantics can be easily integrated into standard navigation and animation pipelines in order to greatly enhance the motion capabilities of virtual characters, The computational efficiency of our approach enables two additional applications, Environment designers can interactively design new environments and get instant feedback on how characters may potentially interact, which can be used for iterative modeling and refinement, End users can dynamically edit virtual worlds and characters will automatically accommodate the changes in the environment in their movement strategies.
2K_test_1740	Data compression can be an effective method to achieve higher system performance and energy efficiency in modern data-intensive applications by exploiting redundancy and data similarity Prior works have studied a variety of data compression techniques to improve both capacity ( e, of caches and main memory ) and bandwidth utilization ( e, of the on-chip and off-chip interconnects ). In this paper we make a new observation about the energy-efficiency of communication when compression is applied, To mitigate the problem. We propose two new toggle-aware compression techniques : Energy Control and Metadata Consolidation.
2K_test_1741	Factorization Machines offer good performance and useful embeddings of data. However they are costly to scale to large amounts of data and large numbers of features. In this paper we describe DiFacto, which uses a refined Factorization Machine model with sparse memory adaptive constraints and frequency adaptive regularization, We show how to distribute DiFacto over multiple machines using the Parameter Server framework by computing distributed sub gradients on minibatches asynchronously. We analyze its convergence and in computational advertising datasets with billions examples and features.
2K_test_1742	Social community detection is a growing field of interest in the area of social network applications, and many approaches have been developed, including graph partitioning latent space model, block model and spectral clustering. Most existing work purely focuses on network structure information which is, however often sparse noisy and lack of interpretability, To improve the accuracy and interpretability of community discovery to infer users ' social communities to simulate the generative process of communities as a result of network proximities, spatiotemporal co-occurrences and semantic similarity. Show that UCGT achieves better performance than existing state-of-the-art comparison methods. We propose by incorporating their spatiotemporal data and semantic information, Technically we propose a unified probabilistic generative model, User-Community-Geo-Topic ( UCGT ) With a well-designed multi-component model structure and a parallel inference implementation to leverage the power of multicores and clusters, our UCGT model is expressive while remaining efficient and scalable to growing large-scale geo-social networking data. We deploy UCGT to two application scenarios of user behavior predictions : check-in prediction and social interaction prediction, Extensive experiments on two large-scale geo-social networking datasets.
2K_test_1743	Given such geometric alignments, the natural approach for recognition might extract pose-normalized appearance features from a canonically-aligned coordinate frame, Though such approaches are extraordinarily common. This paper introduces and analyzes the novel task of categorical classification of cuboidal objects - e, distinguishing washing machines versus filing cabinets, To aid our empirical analysis. That synthesis is a surprisingly simple but effective strategy that allows for state-of-the-art categorization and automatic 3D alignment. To do so it makes use of recent methods for automatic alignment of cuboidal objects in images we demonstrate that they are not optimal, both theoretically and empirically, One reason is that such approaches require accurate shape alignment, However even with ground-truth alignment, posenormalized representations may still be sub-optimal, Instead we introduce methods based on pose-synthesis, a somewhat simple approach of augmenting training data with geometrically perturbed training samples we introduce a novel dataset for cuboidal object categorization. We demonstrate both theoretically and empirically.
2K_test_1744	And enable a diverse set of shader optimizations to be described by a single mechanism. We demonstrate use of Spire to author complex shaders that are portable across different rendering pipelines and to rapidly explore shader optimization decisions that span multiple compute and graphics passes and even offline asset preprocessing, and demonstrate rapid automatic re-optimization of shaders for different target hardware platforms. We present Spire a shading language and compiler framework that facilitates rapid exploration of shader optimization choices ( such as frequency reduction and algorithmic approximation ) afforded by modern real-time graphics engines, Our design combines ideas from rate-based shader programming with new language features that expand the scope of shader execution beyond traditional GPU hardware pipelines, overloading shader terms at various spatio-temporal computation rates provided by the pipeline, In contrast to prior work, neither the shading language 's design, nor our compiler framework 's implementation, is specific to the capabilities of any one rendering pipeline, thus Spire establishes architectural separation between the shading system and the implementation of modern rendering engines ( allowing different rendering pipelines to utilize its services. We further demonstrate the utility of Spire by developing a shader level-of-detail library and shader auto-tuning system on top of its abstractions.
2K_test_1745	To support lightweight process assessments while promoting at the same time collaboration among assessment participants. Were it got previously discording participants, talking to each other and agreeing on the issues. This paper describes a group interview technique designed The technique borrows from agile software development the concept of user stories to cast CMMI 's specific practices in concrete terms and the Planning Poker technique, instead of document reviews and audit like interviews, for fact finding and corroboration. The method was successfully used in one consulting assignment.
2K_test_1746	In order to improve a topic segmentation task. Initial results show that the proposed method does afford better segmentation compared to one of the present state of the art algorithms, a Bayesian baseline approach that segments the documents individually. This paper proposes the use of lexical similarity across different documents Given a set of topically related documents, the segmentation process is carried out using a Bayesian framework By using similar sentences from different documents more accurate segment likelihood estimations are obtained. The proposed approach was tested in an educational domain where a set of learning materials from different media sources needed to be segmented so that students could browse through them more efficiently.
2K_test_1747	Which improves the propagation of reservoir constraints on cumulative resources in schedules with optional tasks, to solve a Single-Commodity Pickup and Delivery Problem : the Bicycle Rebalancing Problem with Time-Windows and heterogeneous fleet. The resulting CP approach outperforms a Branch-and-Bound approach derived from two closely related problems In addition, the CP approach presented in this paper resulted in a first place position in the competition.
2K_test_1748	In order to achieve smooth autonomous driving in real-life urban and highway environments, a motion planner must generate trajectories that are locally smooth and responsive ( reactive ), and at the same time, far-sighted and intelligent ( deliberative ), Prior approaches achieved both planning qualities for full-speed-range operations at a high computational cost. Moreover the planning formulations were mostly a trajectory search problem based on a single weighted cost, which became hard to tune and highly scenario-constrained due to overfitting, for general on-road motion planning to reduce the computational overhead and improve the tunability of the planner. In this paper a pipelined ( phased ) framework with tunable planning modules is proposed.
2K_test_1749	The topic of kinematics of laser rangefinders has received little attention in the robotics literature, even though such sensors have been the perception sensors of choice on commercial AGVs, field robots and aerial robots for some time. In recognition of the uniqueness of optical reflection mechanisms, this paper presents a formulation based on a matrix reflection operator.
2K_test_1750	Many applications for robotic systems require the systems to traverse diverse, unstructured environments State estimation with Visual Odometry ( VO ) in these applications is challenging because there is no single algorithm that performs well across all environments and situations, The unique trade-offs inherent to each algorithm mean different algorithms excel in different environments. To increase robustness in state estimation. Our method reduces the mean translational relative pose error by 3, 5\ % and the angular error by 4, 3\ % compared to the single best odometry algorithm, Compared to the poorest performing odometry algorithm, our method reduces the mean translational error by 39, 4\ % and the angular error by 20. We develop a method by using an ensemble of VO algorithms The method combines the estimates by dynamically switching to the best algorithm for the current context, according to a statistical model of VO estimate errors The model is a Random Forest regressor that is trained to predict the accuracy of each algorithm as a function of different features extracted from the sensory input. We evaluate our method in a dataset of consisting of four unique environments and eight runs, totaling over 25min of data.
2K_test_1751	One of the challenges of field testing planetary rovers on Earth is the difference in gravity between the test and the intended operating conditions, This not only changes the weight exerted by the robot on the surface but also affects the behaviour of the granular surface itself, and unfortunatly no field test can fully address this shortcoming, Excavating with gravity offload underestimates the detrimental effects of gravity on traction, but overestimates the detrimental effects on excavation resistance ; though not ideal, this is a more balanced test than excavating in Earth gravity, which underestimates detrimental effects on both traction and resistance. Experiments demonstrate that continuous excavation ( e, bucket-wheel ) fares better than discrete excavation ( e, front-loader ) when subjected to gravity offload, and is better suited for planetary excavation, Lessons learned from the prototype development also address ways to mitigate suspension lift-off for lightweight skid-steer robots, a problem encountered during mobility field testing. This key result is incorporated into the development of a novel planetary excavator prototype.
2K_test_1752	This paper presents a method for generating semi-algebraic invariants for systems governed by non-linear polynomial ordinary differential equations under semi-algebraic evolution constraints. The resulting invariant generation method is observed to be much more scalable and efficient than the na `` ive approach, exhibiting orders of magnitude performance improvement on many of the problems. Based on the notion of discrete abstraction, our method eliminates unsoundness and unnecessary coarseness found in existing approaches for computing abstractions for non-linear continuous systems and is able to construct invariants with intricate boolean structure, in contrast to invariants typically generated using template-based methods, In order to tackle the state explosion problem associated with discrete abstraction, we present invariant generation algorithms that exploit sound proof rules for safety verification, such as differential cut ( DC ), and a new proof rule that we call differential divide-and-conquer ( DDC ), which splits the verification problem into smaller sub-problems.
2K_test_1753	Reduced frequency range in vowel production is a well documented speech characteristic of individuals with psychological and neurological disorders, Affective disorders such as depression and post-traumatic stress disorder ( PTSD ) are known to influence motor control and in particular speech production, These findings could potentially support treatment of affective disorders, like depression and PTSD in the future. The assessment and documentation of reduced vowel space and reduced expressivity often either rely on subjective assessments or on analysis of speech under constrained laboratory conditions ( e, sustained vowel production reading tasks ), These constraints render the analysis of such measures expensive and impractical to assess a speaker 's vowel space. The experiments show a significantly reduced vowel space in subjects that scored positively on the questionnaires, We show the measure 's statistical robustness against varying demographics of individuals and articulation rate, The reduced vowel space for subjects with symptoms of depression can be explained by the common condition of psychomotor retardation influencing articulation and motor control. Within this work we investigate an automatic unsupervised machine learning based approach.
2K_test_1754	Level-of-detail ( LOD ) rendering is a key optimization used by modern video game engines to achieve high-quality rendering with fast performance, These LOD systems require simplified shaders, but generating simplified shaders remains largely a manual optimization task for game developers. Prior efforts to automate this process have taken hours to generate simplified shader candidates, making them impractical for use in modern shader authoring workflows for complex scenes, for automatically generating a LOD policy for an input shader. We present an end-to-end system The system operates on shaders used in both forward and deferred rendering pipelines, requires no additional semantic information beyond input shader source code, and in only seconds to minutes generates LOD policies ( consisting of simplified shader, the desired LOD distance set, and transition generation ) with performance and quality characteristics comparable to custom hand-authored solutions, Our design contributes new shader simplification transforms such as approximate common subexpression elimination and movement of GPU logic to parameter bind-time processing on the CPU, and it uses a greedy search algorithm that employs extensive caching and up-front collection of input shader statistics to rapidly identify simplified shaders with desirable performance-quality trade-offs.
2K_test_1755	For designing ornamental curve networks-structurally-sound physical surfaces to identify potentially large deformations between geodesically-close curves and guide the user in strengthening the corresponding regions. We present a computational tool with user-controlled aesthetics In contrast to approaches that leverage texture synthesis for creating decorative surface patterns, our method relies on user-defined spline curves as central design primitives More specifically, we build on the physically-inspired metaphor of an embedded elastic curve that can move on a smooth surface, deform and connect with other curves We formalize this idea as a globally coupled energy-minimization problem, discretized with piece-wise linear curves that are optimized in the parametric space of a smooth surface, Building on this technical core, we propose a set of interactive design and editing tools that we demonstrate on manually-created layouts and semi-automated deformable packings In order to prevent excessive compliance, we furthermore propose a structural analysis tool that uses eigenanalysis. We used our approach to create a variety of designs in simulation with a set of 3D-printed physical prototypes.
2K_test_1756	For designing physical surfaces In order to address this challenging problem of combinatorial geometry allows the user to interactively explore the space of feasible designs, In order to assist the user in building the designs, to automatically generate assembly instructions. We demonstrate the versatility of our method. We present an interactive tool made from flexible interlocking quadrilateral elements of a single size and shape, With the element shape fixed, the design task becomes one of finding a discrete structure-i, element connectivity and binary orientations-that leads to a desired geometry, we propose a forward modeling tool that Paralleling principles from conventional modeling software, our approach leverages a library of base shapes that can be instantiated, combined and extended using two fundamental operations : merging and extrusion, we furthermore propose a method. By creating a diverse set of digital and physical examples that can serve as personalized lamps or decorative items.
2K_test_1757	The proposed framework generalizes moderately well from textbook graphics to hand-drawn sketches, and user effort ratio results demonstrate the potential power of an interactive system in which simple user interactions complement computer recognition for fast kinematic modeling. For automatically generating kinematic models of planar mechanical linkages from raw images. Current state-of-the-art performance is achieved. A rigorous set of experiments was conducted to systematically evaluate the performance of each phase in our framework, comparing various combinations of joint and body detection schemes and feasibility constraints, Precision-recall curves are used to assess object detection performance.
2K_test_1758	Data races complicate programming language semantics, and a data race is often a bug, Existing techniques detect data races and define their semantics by detecting conflicts between synchronization-free regions ( SFRs ), Valor is the first region conflict detector to provide strong semantic guarantees for racy program executions with under 2X slowdown, Overall Valor advances the state of the art in always-on support for strong behavioral guarantees for data races. However such techniques either modify hardware or slow programs dramatically, preventing always-on use today, a sound precise software-only region conflict detection analysis. Showing that Valor dramatically outperforms FastRCD and FastTrack. This paper describes Valor that achieves high performance by eliminating the costly analysis on each read operation that prior approaches require Valor instead logs a region 's reads and lazily detects conflicts for logged reads when the region ends, we have also developed FastRCD that leverages the epoch optimization strategy of the FastTrack data race detector. As a comparison We evaluate Valor.
2K_test_1759	And the effects of iterated deception. Moving to a mathematical model. And ending with a studies on the implications of deceptive motion for human-robot interactions.
2K_test_1760	Communication constraints dictated by hardware often require a multi-robot system to make decisions and take actions locally Unfortunately, local knowledge may impose limits that ultimately impede global optimality in a decentralized optimization problem. To address task allocation problems in a decentralized fashion. To show that the convergence of local searching processes is related to a shortest path routing problem on a graph subject to the network topology, results show that this fully decentralized method converges quickly while sacrificing little optimality. This paper enhances a recent anytime optimal assignment method based on a task-swap mechanism, redesigning the algorithm We propose a fully decentralized approach that allows local search processes to execute concurrently while minimizing interactions amongst the processes, needing neither global broadcast nor a multi-hop communication protocol. The formulation is analyzed in a novel way using tools from group theory and optimization duality theory Simulation.
2K_test_1761	Autonomous landing is an essential function for micro air vehicles ( MAVs ) for many scenarios. That enables MAVs with limited onboard sensing and processing capabilities to concurrently assess feasible rooftop landing sites that balance continued landing site assessment and the requirement to provide visual monitoring of an interest point. In order to establish the efficacy and robustness of the proposed approach. Simulation and experimental evaluation of the performance of the perception and trajectory generation methodologies are analyzed independently and jointly.
2K_test_1762	Consider networks in harsh environments, where nodes may be lost due to failure, attack or infection-how is the topology affected by such events ? Can we mimic and measure the effect ? to evaluate robustness to construct secure networks operating within malicious environments. We propose a new generative model of network evolution in dynamic and harsh environments, Our model can reproduce the range of topologies observed across known robust and fragile biological networks, as well as several additional transport, communication and social networks, We also develop a new optimization measure based on preserving high connectivity following random or adversarial bursty node loss, propose a new distributed algorithm. Using this measure we evaluate the robustness of several real-world networks and.
2K_test_1763	For interactive editing of planar linkages. Illustrating the potential to adapt and personalize the structure and motion of existing linkages. We present a method Given a working linkage as input, the user can make targeted edits to the shape or motion of selected parts while preserving other, In order to make this process intuitive and efficient, we provide a number of editing tools at different levels of abstraction, For instance the user can directly change the structure of a linkage by displacing joints, edit the motion of selected points on the linkage, or impose limits on the size of its enclosure, Our method safeguards against degenerate configurations during these edits, thus ensuring the correct functioning of the mechanism at all times, Linkage editing poses strict requirements on performance that standard approaches fail to provide In order to enable interactive and robust editing, we build on a symbolic kinematics approach that uses closed-form expressions instead of numerical methods to compute the motion of a linkage and its derivatives. We demonstrate our system on a diverse set of examples To validate the feasibility of our edited designs, we fabricated two physical prototypes.
2K_test_1764	Motivation : It remains a challenge to detect associations between genotypes and phenotypes because of insufficient sample sizes and complex underlying mechanisms involved in associations, Fortunately it is becoming more feasible to obtain gene expression data in addition to genotypes and phenotypes, giving us new opportunities. In this article we propose a novel method, NETAM We take a network-driven approach : NETAM first constructs an association network, where nodes represent SNPs, gene traits or phenotypes, and edges represent the strength of association between two nodes NETAM assigns a score to each path from an SNP to a phenotype, and then identifies significant paths based on the scores. In our simulation study Furthermore, we applied NETAM on late-onset Alzheimer 's disease data.
2K_test_1765	For realtime generation of stylistic human motion to capture the complex relationships between styles of motion, to predict the timings of synthesized poses in the output style. We demonstrate the power of our approach Our method achieves superior performance. This paper presents a novel solution that automatically transforms unlabeled, heterogeneous motion data into new styles, The key idea of our approach is an online learning algorithm that automatically constructs a series of local mixtures of autoregressive models ( MAR ) We construct local MAR models on the fly by searching for the closest examples of each input pose in the database, Once the model parameters are estimated from the training data, the model adapts the current pose with simple linear transformations, In addition we introduce an efficient local regression model. By transferring stylistic human motion for a wide variety of actions, including walking running punching, kicking jumping and transitions between those behaviors in a comparison against alternative methods, We have also performed experiments to evaluate the generalization ability of our data-driven model as well as the key components of our system.
2K_test_1766	A concurrency debugging technique. Shows that in practical time, Symbiosis generates DSPs that both isolate the small fraction of event orders and data-flows responsible for the failure, and show which event reorderings prevent failing DSPs contain 81\ % fewer events and 96\ % fewer data-flows than the full failure-inducing schedules, Moreover by allowing developers to focus on only a few events, DSPs reduce the amount of time required to find a valid fix. We present Symbiosis based on novel differential schedule projections ( DSPs ), A DSP shows the small set of memory operations and data-flows responsible for a failure, as well as a reordering of those elements that avoids the failure, To build a DSP, Symbiosis first generates a full, failing multithreaded schedule via thread path profiling and symbolic constraint solving, Symbiosis selectively reorders events in the failing schedule to produce a non-failing, A DSP reports the ordering and data-flow differences between the failing and non-failing schedules. Our evaluation on buggy real-world software and benchmarks In our experiments.
2K_test_1767	Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval. Existing methods index a video by the raw concept detection score that is dense and inconsistent, and thus can not scale to `` big data { '' } that are readily available on the Internet, This paper proposes a scalable solution. The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index, The proposed adjustment model relies on a concise optimization framework with interpretations, The proposed index leverages the text-based inverted index for video retrieval.
2K_test_1768	Multimedia event detection ( MED ) and multimedia event recounting ( MER ) are fundamental tasks in managing large amounts of unconstrained web videos, and have attracted a lot of attention in recent years. Most existing systems perform MER as a post-processing step on top of the MED results, In order to leverage the mutual benefits of the two tasks Our premise is that a good recounting algorithm should not only explain the detection result, but should also be able to assist detection in the first place. And obtain very promising results for both MED and MER. We propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events Coupled in a joint optimization framework, recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences To better utilize the powerful and interpretable semantic video representation, we segment each video into several shots and exploit the rich temporal structures at shot level, The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm, which after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps, enables us to efficiently process extremely large video corpora.
2K_test_1770	Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features. In this paper we explore the bi-directional mapping between images and their sentence-based descriptions. State-of-the-art results are shown for the task of generating novel image descriptions our automatically generated captions are equal to or preferred by humans 21, 0\ % of the time. Critical to our approach is a recurrent neural network that attempts to dynamically build a visual representation of the scene as a caption is being generated or read, The representation automatically learns to remember long-term visual concepts, Our model is capable of both generating novel captions given an image, and reconstructing visual features given an image description. We evaluate our approach on several tasks, These include sentence generation, sentence retrieval and image retrieval When compared to human generated captions.
2K_test_1771	Convolutional Neural Networks ( CNNs ) have achieved promising performance in image classification and action recognition tasks. In this paper we focus on complex event detection in internet videos while also providing the key evidences of the detection results, However it remains an open problem how to use CNNs for video event detection and recounting, mainly due to the complexity and diversity of video events, that simultaneously detects pre-defined events and provides key spatial-temporal evidences. Demonstrate the promising performance of our method, both for event detection and evidence recounting. In this work we propose a flexible deep CNN infrastructure, namely Deep Event Network ( DevNet ), Taking key frames of videos as input, we first detect the event of interest at the video level by aggregating the CNN features of the key frames, The pieces of evidences which recount the detection results, are also automatically localized, both temporally and spatially, The challenge is that we only have video level labels, while the key evidences usually take place at the frame levels, Based on the intrinsic property of CNNs, we first generate a spatial-temporal saliency map by back passing through DevNet, which then can be used to find the key frames which are most indicative to the event, as well as to localize the specific spatial position, usually an object in the frame of the highly indicative area.
2K_test_1772	Reconstructs building facades in 3D space. Show that our method compares competitively with the state of the art on both 2D and 3D measures, while yielding a richer interpretation of the 3D scene behind the image. In this paper we propose a novel algorithm that infers the 3D layout of building facades from a single 2D image of an urban scene, Different from existing methods that only yield coarse orientation labels or qualitative block approximations, our algorithm quantitatively using a set of planes mutually related by 3D geometric constraints, Each plane is characterized by a continuous orientation vector and a depth distribution, An optimal solution is reached through inter-planar interactions Due to the quantitative and plane-based nature of our geometric reasoning, our model is more expressive and informative than existing approaches.
2K_test_1773	Two of the most prevalent sources of data on the Web, Blogs consist of sequences of images and associated text : they portray events and experiences with concise sentences and representative images, In the opposite direction, blog posts can be enhanced with sets of photo streams by showing interpolations between consecutive images in the blogs. For joint story-based summarization and exploration. We demonstrate that blog posts and photo streams are mutually beneficial for summarization, exploration semantic knowledge transfer. We propose an approach that utilize large collections of photo streams and blog posts, We leverage Hogs to help achieve story-based semantic summarization of collections of photo streams, We formulate the problem of joint alignment from blogs to photo streams and photo stream summarization in a unified latent ranking SVM framework, We alternate between solving the two coupled latent SVM problems, by first fixing the summarization and solving for the alignment from blog images to photo streams and vice versa. On a newly collected large-scale Disneyland dataset of 10K blogs ( 120K associated images ) and OK photo streams ( 540K images ).
2K_test_1774	In contrast existing approaches either do not consider multiple object instances per video, or rely heavily on the motion of the objects present. Localizes multiple unknown object instances in long videos, reliable object detection and tracking. Demonstrate the effectiveness of our approach. We present a semi-supervised approach that We start with a handful of labeled boxes and iteratively learn and label hundreds of thousands of object instances, We propose criteria for for constraining the semi-supervised learning process and minimizing semantic drift, Our approach does not assume exhaustive labeling of each object instance in any single frame, or any explicit annotation of negative data, Working in such a generic setting allow us to tackle multiple object instances in video, many of which are static. The experiments by evaluating the automatically labeled data on a variety of metrics like quality, coverage ( recall ), diversity and relevance to training an object detector.
2K_test_1775	Contact behaviors in physics simulations are important for real-time interactive applications, especially in virtual reality applications where user 's body parts are tracked and interact with the environment via contact, For these contact simulations, it is ideal to have small changes in initial condition yield predictable changes in the output, Predictable simulation is key for success in iterative learning processes as well, such as learning controllers for manipulations or locomotion tasks, Our results confirmed that parameter settings do matter a great deal and suggest that there may be a trade-off between accuracy and predictability. Here we present an extensive comparison of contact simulations using Bullet Physics, Dynamic Animation and Robotics Toolkit ( DART ), MuJoCo and Open Dynamics Engine, with a focus on predictability of behavior. We found that in the commonly available physics engines, small changes in initial condition can sometimes induce different sequences of contact events to occur and ultimately lead to a vastly different result. We first tune each engine to match an analytical solution as closely as possible and then compare the results for a more complex simulation.
2K_test_1776	That can be used to discover multiple semantic and visual senses of a given Noun Phrase ( NP ), extracts the multiple senses in both semantic and visual feature space, but also discovers the mapping between the senses. We present a co-clustering framework Unlike traditional clustering approaches which assume a one-to-one mapping between the clusters in the text-based feature space and the visual space, we adopt a one-to-many mapping between the two spaces, This is primarily because each semantic sense ( concept ) can correspond to different visual senses due to viewpoint and appearance variations Our structure-EM style optimization not only.
2K_test_1777	We envision a future time when wearable cameras ( e, small cameras in glasses or pinned on a shirt collar ) are worn by the masses and record first-person point-of-view ( POV ) videos of everyday life, Furthermore we show how our approach can enable several practical applications such as privacy filtering, automated video collection and social group discovery. While these cameras can enable new assistive technologies and novel research challenges, they also raise serious privacy concerns, For example first-person videos passively recorded by wearable cameras will necessarily include anyone who comes into the view of a camera - with or without consent. Our proposed approach significantly improves self-search performance over several well-known face detectors and recognizers. Motivated by these benefits and risks, we develop a self-search technique tailored to first-person POV videos, The key observation of our work is that the egocentric head motions of a target person ( i, the self ) are observed both in the POV video of the target and observer The motion correlation between the target person 's video and the observer 's video can then be used to uniquely identify instances of the self, We incorporate this feature into our proposed approach that computes the motion correlation over supervoxel hierarchies to localize target instances in observer videos.
2K_test_1778	Semantic search in video is a novel and challenging problem in information and multimedia retrieval Existing solutions are mainly limited to text matching, in which the query words are matched against the textual metadata generated by users, We share our observations and lessons in building such a state-of-the-art system, which may be instrumental in guiding the design of the future system for semantic search in video. For event search without any textual metadata or example videos. The novelty and practicality is demonstrated where the proposed system achieves the best performance. This paper presents a state-of-the-art system The system relies on substantial video content understanding and allows for semantic search over a large collection of videos.
2K_test_1779	Many recent works propose mechanisms demonstrating the potential advantages of managing memory at a fine ( e, cache line ) granularity-e, fine-grained deduplication and fine-grained memory protection. We show that our framework can enable simple and efficient implementations of seven memory management techniques, each of which has a wide variety of applications, Our evaluations show that overlay-on-write, when applied to fork, can improve performance by 15\ % and reduce memory capacity requirements by 53\ % on average compared to traditional copy-on-write, For sparse data computation, our framework can outperform a state-of-the-art software-based sparse representation on a number of real-world sparse matrices, Our framework is general, powerful and effective in enabling fine-grained memory management at low cost. We propose a new virtual memory framework In our framework, each virtual page can be mapped to a structure called a page overlay, in addition to a regular physical page, An overlay contains a subset of cache lines from the virtual page, Cache lines that are present in the overlay are accessed from there and all other cache lines are accessed from the regular physical page, Our page-overlay framework enables cache-line-granularity memory management without significantly altering the existing virtual memory framework or introducing high overheads. We quantitatively evaluate the potential benefits of two of these techniques : overlay-on-write and sparse-data-structure computation.
2K_test_1780	Distributed in-memory key-value stores ( KVSs ), such as memcached have become a critical data serving layer in modern Internet-oriented datacenter infrastructure, Their performance and efficiency directly affect the QoS of web services and the efficiency of datacenters Traditionally, these systems have had significant overheads from inefficient network processing, OS kernel involvement and concurrency control. Two recent research thrusts have focused upon improving key-value performance, Hardware-centric research has started to explore specialized platforms including FPGA5 for KVSs ; results demonstrated an order of magnitude increase in throughput and energy efficiency over stock memcached, Software-centric research revisited the KVS application to address fundamental software bottlenecks and to exploit the full potential of modern commodity hardware ; these efforts too showed orders of magnitude improvement over stock memcached, We aim at architecting high performance and efficient KVS platforms. And start with a rigorous architectural characterization across system stacks over a collection of representative KVS implementations, Our detailed full-system characterization not only identifies the critical hardware/software ingredients for high-petformance KVS systems, but also leads to guided optimizations atop a recent design We craft a set of design principles for future platform architectures. And via detailed simulations.
2K_test_1781	Recovering the motion of a non-rigid body from a set of monocular images permits the analysis of dynamic scenes in uncontrolled environments However, the extension of factorisation algorithms for rigid structure from motion to the low-rank non-rigid case has proved challenging, We therefore make the recommendation that 3D reconstruction error always be measured relative to a trivial reconstruction such as a planar one. This stems from the comparatively hard problem of finding a linear `` corrective transform { '' } which recovers the projection and structure matrices from an ambiguous factorisation While it has previously been recognised that finding a single solution to this problem is sufficient to estimate cameras, we show that it is possible to bootstrap this partial solution to find the complete transform in closed-form. We elucidate that this greater difficulty is due to the need to find multiple solutions to a non-trivial problem, casting a number of previous approaches as alleviating this issue by either a ) introducing constraints on the basis, making the problems nonidentical, or b ) incorporating heuristics to encourage a diverse set of solutions, making the problems inter-dependent, However we acknowledge that our method minimises an algebraic error and is thus inherently sensitive to deviation from the low-rank model. We compare our closed-form solution for non-rigid structure with known cameras to the closed-form solution of Dai et al.
2K_test_1782	These two types of images are captured from orthogonal viewpoints and have different resolutions, thus conveying very different types of information that can be used in a complementary way Moreover, their integration is necessary to enable an accurate understanding of changes in natural phenomena over massive city-scale landscapes. We address the task of estimating large-scale land surface conditions using overhead aerial ( macro-level ) images and street view ( micro-level ) images The key technical challenge is devising a method to integrate these two disparate types of image data in an effective manner, to leverage the wide coverage capabilities of macro-level images and detailed resolution of micro-level images. Our proposed method is capable of generating detailed estimates of land surface conditions over an entire city. The strategy proposed in this work uses macro-level imaging to learn the extent to which the land condition corresponds between land regions that share similar visual characteristics ( e, mountains streets buildings rivers ), whereas micro-level images are used to acquire high resolution statistics of land conditions ( e, the amount of debris on the ground ), By combining macro- and micro-level information about regional correspondences and surface conditions.
2K_test_1783	A popular approach in this regard is to represent a sequence using a bag of words ( BOW ) representation due to its : ( i ) fixed dimensionality irrespective of the sequence length, and ( ii ) its ability to compactly model the statistics in A drawback to the BOW representation, however is the intrinsic destruction of the temporal ordering information. In this paper we tackle the problem of efficient video event detection, We argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation. Show significant performance improvements across both isolated and continuous event detection tasks. In this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering Our representation, like BOW is of a fixed dimensionality making it easily integrated with a linear detection function. Extensive experiments on CK+, 6DMG and UvA-NEMO databases.
2K_test_1784	Occupancy count in rooms is valuable for applications such as room utilization, opportunistic meeting support and efficient heating-cooling operations. Few buildings however have the means of knowing occupancy beyond simple binary presence-absence of estimating person count. In this paper we present the PerCCS algorithm that explores the possibility from CO2 sensors already integrated in everyday room airconditioning infrastructure, PerCSS uses task-driven Sparse Non-negative Matrix Factorization ( SNMF ) to learn a nonnegative low-dimensional representation of the CO2 data in the preprocessing stage This denoised CO2 acts as the predictor variable for estimating occupancy count using Ensemble Least Square Regression.
2K_test_1785	Understanding the purpose of why sensitive data is used could help improve privacy as well as enable new kinds of access control. For inferring the purpose of sensitive data usage in the context of Android smartphone apps. And achieved an accuracy of about 85\ % and 94\ % respectively in inferring purposes, We have also found that text-based features alone are highly effective in inferring purposes. In this paper we introduce a new technique We extract multiple kinds of features from decompiled code, focusing on app-specific features and text-based features These features are then used to train a machine learning classifier. We have evaluated our approach in the context of two sensitive permissions, namely ACCESS FINE LOCATION and READ CONTACT LIST.
2K_test_1786	Some languages have very consistent mappings between graphemes and phonemes, while in other languages, this mapping is more ambiguous, Consonantal writing systems prove to be a challenge for Text to Speech Systems ( TTS ) because they do not indicate short vowels, which creates an ambiguity in pronunciation Special letter-to-sound rules may be needed for some cases in languages that otherwise have a good correspondence between graphemes and phonemes Our methods can be generalized to other languages that exhibit similar phenomena. In the low-resource scenario, we may not have linguistic resources such as diacritizers or hand-written rules for the language, to automatically learn pronunciations iteratively from acoustics. And show significant improvements for dialects of Arabic. We propose a technique during ITS training and predict pronunciations from text during synthesis time, We conduct experiments on dialects of Arabic for disambiguating homographs and Hindi for discovering the schwa-deletion rules. We evaluate our systems using objective and subjective metrics of TTS.
2K_test_1787	Distant speech recognition ( DSR ) remains to be an open challenge, even for the state-of-the-art deep neural network ( DNN ) models, Previous work has attempted to improve DNNs under constantly distant speech. However in real applications, the speaker-microphone distance ( SMD ) can be quite dynamic, varying even within a single utterance, This paper investigates how to alleviate the impact of dynamic SMD on DNN models. Our experiments show that in the simplest case, incorporating the SMD descriptors improves word error rates of DNNs by 5, Further optimizing SMD extraction and integration results in more gains. Our solution is to incorporate the frame-level SMD information into DNN training, Generation of the SMD information relies on a universal extractor that is learned on a meeting corpus. We study the utility of different architectures in instantiating the SMD extractor, On our target acoustic modeling task, two approaches are proposed to build distance-aware DNN models using the SMD information : simple concatenation and distance adaptive training ( DAT ).
2K_test_1788	Spoken Term Detection ( STD ) or Keyword Search ( KWS ) techniques can locate keyword instances but do not differentiate between meanings. Spoken Word Sense Induction ( SWSI ) differentiates target instances by clustering according to context, providing a more useful result. We show that the distributed representation approach outperforms all other approaches, regardless of the WER Although LDA-based approaches do well on clean data, they degrade significantly with WER, Paradoxically lower WER does not guarantee better SWSI performance, due to the influence of common locutions. In this paper we present a fully unsupervised SWSI approach based on distributed representations of spoken utterances To determine how ASR performance affects SWSI, we used three different levels of Word Error Rate ( WER ), 40\ % 20\ % and 0\ % ; 40\ % WER is representative of online video, 0\ % of text. We compare this approach to several others, including the state-of-the-art Hierarchical Dirichlet Process ( HDP ).
2K_test_1789	Ensuring language coverage in dialog systems can be a challenge, since the language in a domain may drift over time, creating a mismatch between the original training data and current input, This in turn degrades performance by increasing misunderstanding and eventually leading to task failure, Without the capability of adapting the vocabulary and the language model based on certain domains or users, recognition errors may degrade the understanding performance, and even lead to a task failure, which incurs more time and effort to recover. This paper investigates how coverage can be maintained by automatically acquiring potential out-of-vocabulary ( OOV ) words. Show that both recognition and semantic parsing accuracy can thereby be improved. By leveraging different types of relatedness between vocabulary items and words retrieved from web-based resources.
2K_test_1790	We study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems, in particular learning the hierarchical semantic structure from the data. The experiments show that high-level semantic information can accurately estimate the prominence of slots, significantly improving the slot induction performance ; furthermore, a semantic decoder trained on the data with automatically extracted slots achieves about 68\ % F-measure, which is close to the one from hand-crafted grammars. Given unlabelled conversations we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots ( e, both slots `` direction { '' } and `` locale { '' } convey location-related information ) for building a coherent semantic hierarchy, and then estimate the slot importance at different levels, The high-level semantic estimation involves not only within-slot but also cross slot relations.
2K_test_1791	Self-adaptive systems overcome many of the limitations of human supervision in complex software-intensive systems by endowing them with the ability to automatically adapt their structure and behavior in the presence of runtime changes, However adaptation in some classes of systems ( e, safety-critical ) can benefit by receiving information from humans ( e, acting as sophisticated sensors, decision-makers ) or by involving them as system-level effectors to execute adaptations ( e, when automation is not possible, or as a fallback mechanism ) However, human participants are influenced by factors external to the system ( e, training level fatigue ) that affect the likelihood of success when they perform a task, its duration or even if they are willing to perform it in the first place. Without careful consideration of these factors, it is unclear how to decide when to involve humans in adaptation, and in which way, In this paper we investigate how the explicit modeling of human participants can provide a better insight into the trade-offs of involving humans in adaptation, to reason about human involvement in self-adaptation. We illustrate our approach. We contribute a formal framework, focusing on the role of human participants as actors ( i, effectors ) during the execution stage of adaptation, The approach consists of : ( i ) a language to express adaptation models that capture factors affecting human behavior and its interactions with the system, and ( ii ) a formalization of these adaptation models as stochastic multiplayer games ( SMGs ) that can be used to analyze human-system-environment interactions. In an adaptive industrial middleware used to monitor and manage sensor networks in renewable energy production plants.
2K_test_1793	The approach allows programmers to program assuming a friendly, non-preemptive scheduler to ensure that the final program works even with a preemptive scheduler. Our experiments demonstrate that our synthesis method is precise and efficient, The implicit specification helped us find one concurrency bug previously missed when model-checking using an explicit, user-provided specification and observed that different synchronization placements are produced for our experiments, favoring a minimal number of synchronization operations or maximum concurrency. We present a computer-aided programming approach to concurrency, and our synthesis procedure inserts synchronization The correctness specification is implicit, inferred from the non-preemptive behavior, Let us consider sequences of calls that the program makes to an external interface, The specification requires that any such sequence produced under a preemptive scheduler should be included in the set of sequences produced under a non-preemptive scheduler, We guarantee that our synthesis does not introduce deadlocks and that the synchronization inserted is optimal w, a given objective function, The solution is based on a finitary abstraction, an algorithm for bounded language inclusion modulo an independence relation, and generation of a set of global constraints over synchronization placements, Each model of the global constraints set corresponds to a correctness-ensuring synchronization placement, The placement that is optimal w, the given objective function is chosen as the synchronization solution. We apply the approach to device-driver programming, where the driver threads call the software interface of the device and the API provided by the operating system, We implemented objective functions for coarse-grained and fine-grained locking.
2K_test_1794	Self-adaptive systems tend to be reactive and myopic, adapting in response to changes without anticipating what the subsequent adaptation needs will be, Adapting reactively can result in inefficiencies due to the system performing a suboptimal sequence of adaptations, Furthermore when adaptations have latency, and take some time to produce their effect, they have to be started with sufficient lead time so that they complete by the time their effect is needed, Proactive latency-aware adaptation addresses these issues by making adaptation decisions with a look-ahead horizon and taking adaptation latency into account. For proactive latency-aware adaptation under uncertainty. Our results show that the decision based on a look-ahead horizon, and the factoring of both tactic latency and environment uncertainty, considerably improve the effectiveness of adaptation decisions. In this paper we present an approach that uses probabilistic model checking for adaptation decisions, The key idea is to use a formal model of the adaptive system in which the adaptation decision is left underspecified through nondeterminism, and have the model checker resolve the nondeterministic choices so that the accumulated utility over the horizon is maximized, The adaptation decision is optimal over the horizon, and takes into account the inherent uncertainty of the environment predictions needed for looking ahead.
2K_test_1795	Almost every complex software system today is configurable, While configurability has many benefits, it challenges performance prediction, Worse configuration options may interact, giving rise to a configuration space of possibly exponential size. Often the influences of individual configuration options on performance are unknown, Addressing this challenge describing all relevant influences of configuration options and their interactions. Demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them. A series of experiments.
2K_test_1796	Column subset selection ( CSS ) is the problem of selecting a small portion of columns from a large data matrix as one form of interpretable data summarization, Leverage score sampling which enjoys both sound theoretical guarantee and superior empirical performance, is widely recognized as the state-of-the-art algorithm for column subset selection. In this paper we revisit iterative norm sampling, another sampling based CSS algorithm proposed even before leverage score sampling. And demonstrate its competitive performance show its superior performance in terms of both approximation accuracy and computational efficiency, We conclude that further theoretical investigation and practical consideration should be devoted to iterative norm sampling in column subset selection. Under a wide range of experimental settings, We also compare iterative norm sampling with several of its other competitors and.
2K_test_1797	In geometrically complex situations, where many surfaces intersect a pixel, current rendering systems shade each contributing surface at least once per pixel, As the sample density and geometric complexity increase, the shading cost becomes prohibitive for real-time rendering, Under deferred shading so does the required framebuffer memory. For efficient anti-aliased deferred rendering of complex geometry. We present Aggregate G-Buffer Anti-Aliasing ( AGAA ), a new technique using modern graphics hardware AGAA uses the rasterization pipeline to generate a compact, pre-filtered geometric representation inside each pixel, We then shade this at a fixed rate, independent of geometric complexity, By decoupling shading rate from geometric sampling rate, the algorithm reduces the storage and bandwidth costs of a geometry buffer, and allows scaling to high visibility sampling rates for anti-aliasing.
2K_test_1799	Although substantial progress has been achieved in speech-to-speech translation systems over the last few years. Such systems still require that the speech be written in some appropriate orthography, As speech may differ greatly from the standardized written form of a language, it can be non-trivial to collect written data when there is no standard way for it to be represented This project addresses the problem to produce a phonetically-related symbolic representation. From the other end and expects that speech alone is available in the target language, and that no ( standard or nonstandard ) orthography exists It, therefore treats the acoustic representation of the language as primary and uses language-independent methods that is then used in the translation system Thus, the speech translation system is created for the target language as defined by the recording of that language rather than some body of orthographic transcripts In this work, we are creating an application called APT ( Acoustic Patient Translator ) which uses a novel scheme of speech recognition and translation within a targeted domain, By working with a set of predefined sentences appropriately chosen to fit a scenario, we use utterance classification as a speech recognition algorithm The utterance classification is achieved using cross-lingual, Since we are working with a set of select phrases, the translation part is trivial. We are concentrating on communication with hospital staff, such as scheduling a doctor 's appointment, In addition to English, we also run experiments on Tamil.
2K_test_1800	Traditional automated response grading approaches use manually engineered time-aggregated features ( such as mean length of pauses. To grade non-native spoken language tests automatically. We find such models reach the best performance in terms of correlation with human raters We also find that when there are limited time-aggregated features available, our model that incorporates time-sequence features improves performance drastically. We introduce a new method We propose to incorporate general time-sequence features ( such as pitch ) which preserve more information than time-aggregated features and do not require human effort to design, We use a type of recurrent neural network to jointly optimize the learning of high level abstractions from time-sequence features with the time-aggregated features, We first automatically learn high level abstractions from time-sequence features with a Bidirectional Long Short Term Memory ( BLSTM ) and then combine the high level abstractions with time-aggregated features in a Multilayer Perceptron ( MLP ) /Linear Regression ( LR ), We optimize the BLSTM and the MLP/LR jointly.
2K_test_1801	Programmers often need to revert some code to an earlier state, or restore a block of code that was deleted a while ago, However support for this backtracking in modern programming environments is limited. Many of the backtracking tasks can be accomplished by having a selective undo feature in code editors, but this has major challenges : there can be conflicts among edit operations, and it is difficult to provide usable interfaces for selective undo, that allows programmers to selectively undo fine-grained code changes made in the code editor. Showed that programmers can successfully use AZURITE, and were twice as fast as when limited to conventional features. In this paper we present AZURITE, an Eclipse plug-in With AZURITE, programmers can easily perform backtracking tasks, even when the desired code is not in the undo stack or a version control system, AZURITE also provides novel user interfaces specifically designed for selective undo, which were iteratively improved through user feedback gathered from actual users in a preliminary field trial. A formal lab study.
2K_test_1802	We prove the conjecture. We suggest attempting the conjecture in the case that X-1, X-n are the leaves of an information flow tree. In the case that the information flow tree is a caterpillar graph ( similar to a two-state hidden Markov model ).
2K_test_1803	Vehicular networks are inherently unstable networks with high mobility and intermittent connectivity, These networks can greatly benefit from Delay Tolerant Networking ( DTN ) solutions for opportunistic connectivity in the transmission of delaytolerant data. In this paper we evaluate the performance of different DTN routing protocols in real world vehicular networks with different degrees of connectivity in order to understand their feasibility in real vehicular environments. Experimental insight and extract lessons for DTN routing protocol design.
2K_test_1804	Syntax extension mechanisms are powerful, but reasoning about syntax extensions can be difficult, Recent work on type-specific languages ( TSLs ) addressed reasoning about composition, hygiene and typing for extensions introducing new literal forms. To give meaning to delimited segments of arbitrary syntax, To maintain a typing discipline, that generates a type of a specified kind along with its TSL. And show interesting where the two mechanisms operate in concert. We supplement TSLs with typed syntax macros ( TSMs ), which unlike TSLs are explicitly invoked we describe two flavors of term-level TSMs : synthetic TSMs specify the type of term that they generate, while analytic TSMs can generate terms of arbitrary type, but can only be used in positions where the type is otherwise known, At the level of types, we describe a third flavor of TSM.
2K_test_1805	Multimodal analysis has long been an integral part of studying learning, Historically multimodal analyses of learning have been extremely laborious and time intensive and the implications that this work may have in non-education-related contexts. However researchers have recently been exploring ways to use multimodal computational analysis in the service of studying how people learn in complex learning environments, In an effort to advance this research agenda, In this paper we discuss the algorithms used. We find that affect-and pose-based segmentation are more effective, than traditional approaches for drawing correlations between learning-relevant constructs, and multimodal behaviors We also find that pose-based segmentation outperforms the two more traditional segmentation strategies for predicting student success on the hands-on task. In particular we propose affect-and pose-based data segmentation, as alternatives to human-based segmentation. We present a comparative analysis of four different data segmentation techniques, In a study of ten dyads working on an open-ended engineering design task.
2K_test_1806	One powerful aspect of 3D printing is its ability to extend, repair or more generally modify everyday objects. However nearly all existing work implicitly assumes that whole objects are to be printed from scratch, Designing objects as extensions or enhancements of existing ones is a laborious process in most of today 's 3D authoring tools, for 3D printing to augment existing objects that covers a wide range of attachment options. Our validation helps to illustrate the strengths and weaknesses of each technique. For example we characterize how surface curvature and roughness affect print-over 's strength compared to the conventional print-in-one-piece.
2K_test_1808	Do we really need 3D labels in order to learn how to predict 3D ?. In this paper we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label to learn the mapping in a completely unsupervised manner. Despite never seeing a 3D label, our method produces competitive results. Rather than use explicit supervision, we use the regularity of indoor scenes. We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D is unavailable.
2K_test_1809	Contemporary approaches extract features from a single output layer. We explore multi-scale convolutional neural nets ( CNNs ) for image classification, to learn a set of multi-scale features that can be effectively shared between coarse and fine-grained classification tasks. While finetuning such models helps performance, we show that even `` off-the-self { '' } multi-scale features perform quite well, and demonstrate state-of-the-art classification performance our results reduce the lowest previously-reported error by 23, 9\ % and 9. By extracting features from multiple layers, one can simultaneously reason about high, mid and low-level features during classification, The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph ( DAG-CNNs ). We present extensive analysis on three standard scene benchmarks ( SUN397, MIT67 and Scene15 ) In terms of the heavily benchmarked MIT67 and Scene15 datasets.
2K_test_1810	This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation, We argue that doing well on this task requires the model to learn to recognize objects and their parts. Given only a large, unlabeled image collection we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first.
2K_test_1811	Mobile applications frequently access sensitive personal information to meet user or business requirements, Because such information is sensitive in general, regulators increasingly require mobile-app developers to publish privacy policies that describe what information is collected, Furthermore regulators have fined companies when these policies are inconsistent with the actual data practices of mobile apps. To help mobile-app developers check their privacy policies against their apps ' code for consistency. We propose a semi-automated framework that consists of a policy terminology-API method map that links policy phrases to API methods that produce sensitive information, and information flow analysis to detect misalignments. We present an implementation of our framework based on a privacy-policy-phrase ontology and a collection of mappings from API methods to policy phrases.
2K_test_1812	We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training, First we use easy images to train an initial visual representation, We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories.
2K_test_1813	While feedforward deep convolutional neural networks ( CNNs ) have been a great success in computer vision, it is important to note that the human visual cortex generally contains more feedback than feedforward connections. In this paper we will briefly introduce the background of feedbacks in the human visual cortex, which motivates us to develop to infer the activation status of hidden layer neurons. Demonstrate its effectiveness in solving tasks such as image classification and object localization. A computational feedback mechanism in deep neural networks In addition to the feedforward inference in traditional neural networks, a feedback loop is introduced according to the `` goal { '' } of the network, We analogize this mechanism as `` Look and Think Twice, { '' } The feedback networks help better visualize and understand how deep neural networks work, and capture visual attention on expected objects, even in images with cluttered background and multiple. Experiments on ImageNet dataset.
2K_test_1814	To capture the 3D structure and motion of a group of people engaged in a social interaction.
2K_test_1815	Varied sources of error contribute to the challenge of facial action unit detection, Previous approaches address specific and known sources, However many sources are unknown. To address the ubiquity of error. With few exceptions CPM outperformed baseline and state-of-the art methods. We propose a Confident Preserving Machine ( CPM that follows an easy-to-hard classification strategy, During training CPM learns two confident classifiers, A confident positive classifier separates easily identified positive samples from all else ; a confident negative classifier does same for negative samples, During testing CPM then learns a person-specific classifier using `` virtual labels { '' } provided by confident classifiers, This step is achieved using a quasi-semi-supervised ( QSS ) approach, Hard samples are typically close to the decision boundary, and the QSS approach disambiguates them using spatio-temporal constraints. To evaluate CPM we compared it with a baseline single-margin classifier and state-of-the-art semi-supervised learning, transfer learning and boosting methods in three datasets of spontaneous facial behavior.
2K_test_1816	Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent, semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ. Motivated by object recognition literature and recent work on rapidly estimating linear classifiers, we treat semantic correspondence as a constrained detection problem, where an exemplar LDA classifier is learned for each pixel. LDA classifiers have two distinct benefits : ( i ) they exhibit higher average precision than similarity metrics typically used in correspondence problems, and ( ii ) unlike exemplar SVM, can output globally interpretable posterior probabilities without calibration, whilst also being significantly faster to train, We pose the correspondence problem as a graphical model, where the unary potentials are computed via convolution with the set of exemplar classifiers, and the joint potentials enforce smoothly varying correspondence assignment.
2K_test_1817	Starting with the seminal work by Kempe et al, a broad variety of problems, such as targeted marketing and the spread of viruses and malware, have been modeled as selecting a subset of nodes to maximize diffusion through a network, In cyber-security applications however, a key consideration largely ignored in this literature is stealth, In particular an attacker often has a specific target in mind, but succeeds only if the target is reached ( e, by malware ) before the malicious payload is detected and corresponding countermeasures deployed, The dual side of this problem is deployment of a limited number of monitoring units, such as cyber-forensics specialists, so as to limit the likelihood of such targeted and stealthy diffusion processes reaching their intended targets. We investigate the problem of optimal monitoring of targeted stealthy diffusion processes, and for the setting in which an attacker optimally responds to the placement of monitoring nodes. Show that a number of natural variants of this problem are NP-hard to approximate, On the positive side, we show that if stealthy diffusion starts from randomly selected nodes, the defender 's objective is submodular, and a fast greedy algorithm has provable approximation guarantees, show that the proposed algorithms are highly effective and scalable. In addition we present approximation algorithms by adaptively selecting the starting nodes for the diffusion process.
2K_test_1818	Poor spelling is a challenge faced by people with dyslexia throughout their lives, Spellcheckers are therefore a crucial tool for people with dyslexia, but current spellcheckers do not detect real-word errors, which are a common type of errors made by people with dyslexia, Real-word errors are spelling mistakes that result in an unintended but real word, for instance form instead of from, Nearly 20\ % of the errors that people with dyslexia make are real-word. To detect real-world errors. And showed that it detects more of these errors than widely used spellcheckers, people with dyslexia corrected sentences more accurately and in less time with Real Check. In this paper we introduce a system called Real Check that uses a probabilistic language model, a statistical dependency parser and Google n-grams.
2K_test_1819	Today many data-driven web pages present information in a way that is difficult for blind and low vision users to navigate and to understand. It re-writes confusing and complicated template-based data sets. EnTable as accessible tables, EnTable allows blind and low vision users to submit requests for pages they wish to access, The system then employs sighted informants to markup the desired page with semantic information, allowing the page to be re-written using straightforward < table > tags, Screen reader users who browse the web using the EnTable browser extension can report data sets that are confusing, and utilize data sets re-written with the < table > tag based on their own requests or on the requests of other users.
2K_test_1820	Large scale deployment of sensors is essential to practical applications in cyber physical systems, For instance instrumenting a commercial building for `smart energy ' management requires deployment and operation of thousands of measurement and metering sensors and actuators that direct operation of the HVAC system, Each of these sensors need to be named consistently and constantly calibrated. Doing this process manually is not only time consuming but also error prone given the scale, heterogeneity and complexity of buildings as well as lack of uniform naming schemas, To address this challenge for automatically classifying, naming and managing sensors. We show that Zodiac can successfully classify sensors with an average accuracy of 98\ % with 28\ % fewer training examples when compared to a regular expression based method. We propose Zodiac-a framework based on active learning from sensor metadata, In contrast to prior work, Zodiac requires minimal user input in terms of labelling examples while being more accurate. To evaluate Zodiac we deploy it across four real buildings on our campus and label the ground truth metadata for all the sensors in these buildings manually, Using a combination of hierarchical clustering and random forest classifiers.
2K_test_1821	Modern cyber-physical systems interact closely with continuous physical processes like kinematic movement, Software component frameworks do not provide an explicit way to represent or reason about these processes, Meanwhile hybrid program models have been successful in proving critical properties of discrete-continuous systems, These programs deal with diverse aspects of a cyber-physical system such as controller decisions, component communication protocols and mechanical dynamics, requiring several programs to address the variation, However currently these aspects are often intertwined in mostly monolithic hybrid programs, which are difficult to understand. These issues can be addressed by component-based engineering, making hybrid modeling more practical, This paper lays the foundation for using to provide component-based benefits to developing hybrid programs. Architectural models We build formal architectural abstractions of hybrid programs and formulas, enabling analysis of hybrid programs at the component level, reusing parts of hybrid programs, and automatic transformation from views into hybrid programs and formulas. Our approach is evaluated in the context of a robotic collision avoidance case study.
2K_test_1822	Dependencies among software projects and libraries are an indicator of the often implicit collaboration among many developers in software ecosystems, Negotiating change can be tricky : changes to one module may cause ripple effects to many other modules that depend on it, yet insisting on only backward-compatible changes may incur significant opportunity cost and stifle change. We argue that awareness mechanisms based on various notions of stability can enable developers to make decisions that are independent yet wise and provide stewardship rather than disruption to the ecosystem. We are finding that developers in fact struggle with change, that they often use adhoc mechanisms to negotiate change, and that existing awareness mechanisms like Github notification feeds are rarely used due to information overload. Outline a vision toward a change-based awareness system. In ongoing interviews with developers in two software ecosystems ( CRAN and Node, js ) We study the state of the art and current information needs and.
2K_test_1823	Research has shown that understanding conversational structure between students is paramount to evaluating the productivity of the collaboration and estimating outcomes, However previous methods often rely on human supplied dialogue act labels or discourse parsing algorithms requiring large labeled datasets. For understanding discussions between students in MOOC forums, for discovering instances in which a response relation exists between a pair of posts in a forum thread. In this paper we present a new method In particular, we introduce a machine learning method, for example when one student provides the answer to a question or comments on something another student previously said Our method, which utilizes a fast, exact optimization process known as spectral optimization, does not require manually annotated training data and is highly scalable and generalizable. Empirical using real world datasets consisting of conversations between students participating in Coursera courses.
2K_test_1825	Mobile and web applications increasingly leverage service-oriented architectures in which developers integrate third-party services into end user applications, This includes identity management, mapping and navigation cloud storage, and advertising services among others. While service reuse reduces development time, it introduces new privacy and security risks due to data repurposing and over-collection as data is shared among multiple parties who lack transparency into third-party data practices, To address this challenge for modeling multiparty data flow requirements and verifying the purpose specification and collection and use limitation principles. The study results include detected conflicts and violations of the principles as well as two patterns for balancing privacy and data use flexibility in requirements specifications, show that reasoning over complex compositions of multi-party systems is feasible within exponential asymptotic timeframes proportional to the policy size, the number of expressed data, and orthogonal to the number of conflicts found. We propose new techniques based on Description Logic ( DL ) which are prominent privacy properties found in international standards and guidelines. We evaluate our techniques in an empirical case study that examines the data practices of the Waze mobile application and three of their service providers : Facebook Login, Amazon Web Services ( a cloud storage provider ), com ( a popular mobile analytics and advertising platform ), Analysis of automation reasoning over the DL models.
2K_test_1826	In software development IDE services such as syntax highlighting, code completion and `` jump to declaration { '' } are used to assist developers in programming. In dynamic web applications, however since the client-side code is dynamically generated from the server-side code and is embedded in the server-side program as string literals, providing IDE services for such embedded code is challenging a tool that provides editor services on the client-side code of a PHP-based web application, while it is still embedded within server-side code. Is available at http : //www. In this work we introduce Varis Technically, we first perform symbolic execution on a PHP program to approximate all possible variations of the generated client-side code and subsequently parse this client code into a VarDOM that compactly represents all its variations, Finally using the VarDOM, we implement various types of IDE services for embedded client code including syntax highlighting, code completion and `` jump to declaration { '' }. The video demonstration for Varis.
2K_test_1827	To ensure quality and trustworthiness of mobile apps, Google Play store imposes various developer policies, Once an app is reported for exhibiting policy-violating behaviors, it is removed from the store to protect users, Currently Google Play store relies on mobile users ' feedbacks to identify policy violations. Our paper takes the first step towards understanding these policy-violating apps.
2K_test_1828	The distance-keeping target can either be used for lane following for a standalone ACC system or an autonomous vehicle, Our object tracking algorithm can also be extended to find the target of interest for lane changing or ramp merging for an autonomous vehicle. We demonstrate that the overall performance of the proposed algorithm is better than that of a commercial ACC system. We propose a robust object tracking algorithm Taking advantage of a context-based region of interest, we are able to maximize the performance of each sensor, and reduce the computation time since we only focus on the targets inside the region Tracking targets in road coordinates enables finding the distance-keeping target on any curved road, while a commercial Adaptive Cruise Control ( ACC ) system works best on straight roads.
2K_test_1829	Suppose you are a teacher, and have to convey a set of object-property pairs ( 'lions eat meat ' ; or `aspirin is a blood-thinner ' ), A good teacher will convey a lot of information, with little effort on the student side, Specifically given a list of objects ( like animals or medical drugs ) and their associated properties, what is the best and most intuitive way to convey this information to the student, without the student being overwhelmed. A related harder problem is : how can we assign a numerical score to each lesson plan ( i, way of conveying information ) ? Here, we give a formal definition of this problem of forming learning units and we provide a metric for comparing different approaches based on information theory. It is effective achieving excellent results on real data, both with respect to our proposed metric, but also with respect to encoding length demonstrate the effectiveness of HYTRA. We also design a multi-pronged algorithm, HYTRA Our proposed HYTRA is scalable ( near-linear in the dataset size ) ; and it is intuitive, conforming to well-known educational principles, such as grouping related concepts, and `` comparing { '' } and `` contrasting { '' }. Experiments on real and synthetic datasets.
2K_test_1830	This approach is complementary to other efforts in the literature on speeding up computation through GPU implementation, fast matrix operations or quantization, in that any of these optimizations can be incorporated. In this paper we investigate the issue of evaluating efficiently a large set of models on an input image in detection and classification tasks. We are able to dramatically reduce the rate of growth of computation as the number of models increases, show that we are able to maintain, or even exceed the level of performance compared to the default approach of using all the models directly, in both detection and classification tasks. We show that by formulating the visual task as a large matrix multiplication problem, something that is possible for a broad set of modern detectors and classifiers The approach, based on a bilinear separation model, combines standard matrix factorization with a task-dependent term which ensures that the resulting smaller size problem maintains performance on the original task.
2K_test_1831	The rapid growth of cloud storage systems calls for fast and scalable namespace processing. While few commercial file systems offer anything better than federating individually non- scalable namespace servers, a recent academic file system, IndexFS demonstrates scalable namespace processing based on client caching of directory entries and permissions ( directory lookup state ) with no per-client state in servers, as an alternative to caching this information in all clients, to resolve hierarchical permission tests. In this paper we explore explicit replication of directory lookup state in all servers Both eliminate most repeated RPCs to different servers in order Our realization for server replicated directory lookup state, ShardFS employs a novel file system specific hybrid optimistic and pessimistic concurrency control favoring single object transactions over distributed transactions.
2K_test_1832	Humans play an active role in the execution of certain kinds of programs, such as spreadsheets workflows and interactive notebooks, Interacting closely with execution is especially useful when end-users are learning from examples while doing their to derive implications for the design of interactive and mixed-initiative programming languages. In order to better understand the language features needed to support this kind of use. These protocols present a linear, idealized process despite the complex contingencies of the lab work they describe, However they employ a variety of techniques for limiting or expanding the semantic interpretation of individual steps and for integrating outside protocols, We use these observations. We investigated a particularly rigid and formalized category of `` program { '' } people write for each other : lab protocols.
2K_test_1833	Requirements analysts can model regulated data practices to identify and reason about risks of noncompliance, If terminology is inconsistent or ambiguous, however these models and their conclusions will be unreliable, Tregex is a utility to match regular expressions against constituency parse trees, which are hierarchical expressions of natural language clauses, including noun and verb phrases. To study this problem, we investigated an approach to automatically construct an information type ontology by identifying information type hyponymy in privacy policies using Tregex patterns.
2K_test_1834	Cilk Plus and OpenMP are parallel language extensions for the C and C++ programming languages, The CPLEX Study Group of the ISO/IEC C Standards Committee is developing a proposal for a parallel programming extension to C that combines ideas from Cilk Plus and OpenMP. To evaluate the design tradeoffs in the usability and security of these two approaches The eventual goal is to inform decision-making within the committee. We found several usability problems worthy of further investigation based on student performance, including declaring and using reductions, multi-line compiler directives and the understandability of task assignment to threads. We conducted a preliminary comparison of Cilk Plus and OpenMP in a master 's level course on security.
2K_test_1835	The widespread presence of motion sensors on users ' personal mobile devices has spawned a growing research interest in human activity recognition ( HAR ). However when deployed at a large-scale, on multiple devices the performance of a HAR system is often significantly lower than in reported research results, This is due to variations in training and test device hardware and their operating system characteristics among others for large-scale deployment of HAR. Our results indicate that on-device sensor and sensor handling heterogeneities impair HAR performances significantly, Moreover the impairments vary significantly across devices and depends on the type of recognition technique used. Propose a novel clustering-based mitigation technique suitable, where heterogeneity of devices and their usage scenarios are intrinsic.
2K_test_1836	To enable real-time person-independent 3D registration from 2D video. The method has been validated Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video, The software is available online at http : //zface. In a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction.
2K_test_1837	Cultural events are kinds of typical events closely related to history and nationality, which play an important role in cultural heritage through generations. However automatically recognizing cultural events still remains a great challenge since it depends on understanding of complex image contents such as people, objects and scene context, Therefore it is intuitive to associate this task with other high-level vision problems, object detection recognition and scene understanding, In this paper we address this problem for object / scene contents mining for representation via CNN. By combining both ideas of object / scene contents mining and strong image representation via CNN into a whole framework, Specifically  we employ selective search to extract a batch of bottom-up region proposals, which are served as key object / scene candidates in each event image ; while, we investigate two state-of-the-art deep architectures, VGGNet and GoogLeNet and adapt them to our task by performing domain-specific ( i, event ) fine-tuning on both global image and hierarchical region proposals, These two models can complementarily exploit feature hierarchies spatially, which simultaneously capture the global context and local evidences within the image.
2K_test_1838	Thus it is necessary for designers to keep track of definitions in one or more regulations and to compare these definitions across jurisdictions, In this paper we report a study to analyze and compare natural language definitions across legal texts and how to analyze the legal statements with respect to definitions. To develop a method Our method helps reduce the number of comparison between definitions across multiple jurisdictions as well as allows software designers keep track of several inter-related definitions in a systematic way.
2K_test_1839	Automated program repair ( APR ) is a challenging process of detecting bugs, localizing buggy code generating fix candidates and validating the fixes, Effectiveness of program repair methods relies on the generated fix candidates, and the methods used to traverse the space of generated candidates to search for the best ones Existing approaches generate fix candidates based on either syntactic searches over source code or semantic analysis of specification. To enhance the search space of APR, and provide a function to effectively traverse the search space. In this paper we propose to combine both syntactic and semantic fix candidates We present an automated repair method based on structured specifications, deductive verification and genetic programming, Given a function with its specification, we utilize a modular verifier to detect bugs and localize both program statements and sub-formulas in the specification that relate to those bugs While the former are identified as buggy code, the latter are transformed as semantic fix candidates, We additionally generate syntactic fix candidates via various mutation operators, Best candidates which receives fewer warnings via a static verification, are selected for evolution though genetic programming until we find one satisfying the specification, Another interesting feature of our proposed approach is that we efficiently ensure the soundness of repaired code through modular ( or compositional ) verification. We implemented our proposal and tested it on C programs taken from the SIR benchmark that are seeded with bugs.
2K_test_1840	The idea that people interpret a robot 's non-verbal cues, as the robots ' costuming and baskets of candy seem to have communicated an implicit offer of candy. Our behavioral data supports In fact, one third of our detection instances occurred during robot transit, while the robots were making no verbal offer, We find that candy accessibility dominates any social influence of robot orientation and that robot speed influences both whether people will interrupt a robot in transit ( slow more interruptible ) and whether they will respond to its verbal offer ( fast more salient ).
2K_test_1841	The advent of multi-core systems set off a race to get concurrent programming to the masses. One of the challenging aspects of this type of system is how to deal with exceptional situations, since it is very difficult to assert the precise state of a concurrent program when an exception arises. And we show its application to the construction of concurrent software. In this paper we propose an exception-handling model for concurrent systems Its main quality attributes are simplicity and expressiveness, allowing programmers to deal with exceptional situations in a concurrent setting in a familiar way, The proposal is centered on a new kind of exception type that defines new paths for exception propagation among concurrent threads of execution In our model, beyond being able to control where exceptions are raised, the developer can define in which thread, and when during its execution, a particular exception will be handled. The proposed model has been implemented in Scala.
2K_test_1842	The noise model of deletions poses significant challenges in coding theory, with basic questions like the capacity of the binary deletion channel still being open, The abovementioned results bring our understanding of deletion code constructions in these regimes to a similar level as worst case errors. In this paper we study the harder model of worst case deletions, for the two extreme regimes of high-noise and high-rate.
2K_test_1843	Interface-confinement is a common mechanism that secures untrusted code by executing it inside a sandbox, The sandbox limits ( confines ) the code 's interaction with key system resources to a restricted set of interfaces, This practice is seen in web browsers, hypervisors and other security-critical systems, System M is the first program logic that allows proofs of safety for programs that execute adversary-supplied code without forcing the adversarial code to be available for deep static analysis, System M can be used to model and verify protocols as well as system designs. For modeling and proving safety properties of systems that execute adversary-supplied code via interface-confinement. And prove the soundness of System M relative to the model, We demonstrate the reasoning principles of System M. Motivated by these systems, we present a program logic, called System M In addition to using computation types to specify effects of computations, System M includes a novel invariant type to specify the properties of interface-confined code, The interpretation of invariant type includes terms whose effects satisfy an invariant. We construct a step-indexed model built over traces by verifying the state integrity property of the design of Memoir, a previously proposed trusted computing system.
2K_test_1844	Information flow analysis has largely focused on methods that require access to the program in question or total control over an analyzed system. We consider the case where the analyst has neither control over nor a white-box model of the analyzed system. We reduce these problems to ones of causal inference Our systematic study leads to practical advice for detecting web data usage, a previously unformalized area. Leveraging this connection we provide a systematic black-box methodology based on experimental science and statistical analysis. We formalize such limited information flow analyses and study an instance of it : detecting the usage of data by websites by proving a connection between noninterference and causation, We illustrate these concepts with a series of experiments collecting data on the use of information by websites.
2K_test_1845	In today 's ubiquitous computing environment where the number of devices, applications and web services are ever increasing, human attention is the new bottleneck in computing. To minimize user cognitive load. Proved the effectiveness of Attelia showed that notifications at detected breakpoint timing resulted in 46\ % lower cognitive load compared to randomly-timed notifications, further validated Attelia 's value, with a 33\ % decrease in cognitive load compared to randomly-timed notifications. We propose Attelia a novel middleware that identifies breakpoints in user interaction and delivers notifications at these moments, Attelia works in real-time and uses only the mobile devices that users naturally use and wear, without any modifications to applications, and without any dedicated psycho-physiological sensors.
2K_test_1846	Applications such as construction monitoring and planning for renovations, require the accurate recovery of existing conditions of structures. Many types of infrastructure are primarily comprised of arbitrarily-shaped thin structures ( e, truss bridges steel frame buildings under construction, and transmission towers ), which existing automatic modeling methods are incapable of handling, to automatically recognize and model beams, planes and joints to recover their topology. We demonstrate the capability and robustness of our approach. To address this issue, this paper presents an approach from a 3D point cloud containing a complex network of thin structures, and In our approach, each beam is evolved from a seed by matching and aligning the cross section images, This growing algorithm can model beams with arbitrary cross sections By performing the algorithm on a point connectivity graph, we distinguish beams from joints and improve the algorithm 's robustness to closely spaced objects, In parallel planes and joints are also extracted and modeled, The connectivity graph of these primitives allows for a compact, object-level understanding of the entire structure. On both synthetic and real datasets.
2K_test_1847	Many learning-based computer vision algorithms perform poorly when faced with examples that are dissimilar to those on which they were trained. Domain adaptation methods attempt to address this problem, but usually assume that the source domain is specified a priori for situations where more than one source domain available to choose the source domain most similar to the target domain to further adapt the chosen source domain to the target data. Show that the proposed approach outperforms existing single-step methods on a dataset of nine building styles. We propose a two-step approach The first step uses a small number of labeled examples, while the second step uses traditional domain adaptation methods.
2K_test_1848	Registration of Point Cloud Data ( PCD ) forms a core component of many 3D vision algorithms such as object matching and environment reconstruction. We show how this decoupling technique facilitates both faster and more robust registration. Showing better convergence for a wider range of initial conditions and higher speeds than previous state of the art methods. In this paper we introduce a PCD registration algorithm that utilizes Gaussian Mixture Models ( GMM ) and a novel dual-mode parameter optimization technique which we call mixture decoupling, by first optimizing over the mixture parameters ( decoupling the mixture weights, means and covariances from the points ) before optimizing over the 6DOF registration parameters, Furthermore we frame both the decoupling and registration process inside a unified, dual-mode Expectation Maximization ( EM ) framework, for which we derive a Maximum Likelihood Estimation ( MLE ) solution along with a parallel implementation on the GPU. We evaluate our MLE-based mixture decoupling ( MLMD ) registration method over both synthetic and real data.
2K_test_1850	Formal verification of industrial systems is very challenging, due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams, More importantly industrial systems are rarely designed for verification, but rather for operational needs, The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team. To formally verify ACAS X, an airborne collision avoidance system for airliners.
2K_test_1851	Human swarm interaction ( HSI ) involves operators gathering information about a swarm 's state as it evolves, and using it to make informed decisions on how to influence the collective behavior of the swarm, In order to determine the proper input, an operator must have an accurate representation and understanding of the current swarm state, including what emergent behavior is currently happening. In this paper we investigate how human operators perceive three types of common, emergent swarm behaviors : rendezvous, Particularly we investigate how recognition of these behaviors differ from each other in the presence of background noise. Our results show that, while participants were good at recognizing all behaviors, there are indeed differences between the three, with rendezvous being easier to recognize than flocking or dispersion Furthermore, differences in recognition are also affected by viewing time for flocking, was also especially insightful for understanding how participants went about recognizing behaviors-allowing for potential avenues of research in future studies.
2K_test_1852	A server has constantly received various reference time series Q of length X and seeks the exact kNN over a collection of time series distributed across a set of M local sites, When X and M are large, and when the amount of query increases, simply sending each Q to all M sites incurs high communication bandwidth costs, which we would like to avoid, Prior work has presented a communication-efficient kNN algorithm for the Euclidean distance similarity measure. We study the fundamental k-nearest neighbor ( kNN ) search problem on distributed time series communication-efficient for the dynamic time warping ( DTW ) similarity measure To handle the complexities of DTW for communication efficiency for computational efficiency. Show that our method reduces communication bandwidth by up to 92\ %. In this paper we present the first kNN algorithm, which is generally believed a better measure for time series, we design a new multi-resolution structure for the reference time series, and multi-resolution lower bounds that can effectively prune the search space, We present a new protocol between the server and the local sites that leverages multi-resolution pruning and cascading lower bounds. Empirical studies on both real-world and synthetic data sets.
2K_test_1853	A multi-faceted graph defines several facets on a set of nodes, Each facet is a set of edges that represent the relationships between the nodes in a specific context, Mining multi-faceted graphs have several applications, including finding fraudster rings that launch advertising traffic fraud attacks, tracking IP addresses of botnets over time, analyzing interactions on social networks and co-authorship of scientific papers. Soft clustering on individual facets, to discover communities across facets. Where NeSim is shown to be superior to MCL, JP and AP the well-established clustering algorithms, We also report the success stories of MuFace in finding advertisement click rings. We propose NeSim a distributed efficient clustering algorithm that does We also propose optimizations to further improve the scalability, the efficiency and the clusters quality, We employ generalpurpose graph-clustering algorithms in a novel way Due to the qualities of NeSim, we employ it as a backbone in the distributed MuFace algorithm, which discovers multi-faceted communities. We evaluate the proposed algorithms on several real and synthetic datasets.
2K_test_1854	Dataflow analysis-based dynamic parallel monitoring ( DADPM ) is a recent approach for identifying bugs in parallel software as it executes, based on the key insight of explicitly modeling a sliding window of uncertainty across parallel threads. First by explicitly tracking new `` uncertain { '' } states in the metadata lattice, Second as the analysis tool runs dynamically, it can use the existence ( or absence ) of observed uncertain states For example, we demonstrate how the epoch size parameter can be adjusted dynamically in response to uncertainty in order. This paper shows how to adapt a canonical dataflow analysis problem ( reaching definitions ) and a popular security monitoring tool ( TAINTCHECK ) to our new uncertainty-tracking framework.
2K_test_1855	Retinal vein cannulation is a demanding procedure proposed to treat retinal vein occlusion by direct therapeutic agent delivery methods. Challenges in identifying the moment of venous puncture, achieving cannulation and maintaining cannulation during drug delivery currently limit the feasibility of the procedure we respond to these problems. Demonstrates a significant improvement in the total time the needle could be maintained stably inside of the vein, This was especially evident in smaller veins and is attributed to decreased movement of the positioned cannula following venous cannulation. In this study with an assistive system combining a handheld micromanipulator, Micron with a force-sensing microneedle, The integrated system senses the instant of vein puncture based on measured forces and the position of the needle tip, The system actively holds the cannulation device securely in the vein following cannulation and during drug delivery. Preliminary testing of the system in a dry phantom.
2K_test_1856	Public speaking has become an integral part of many professions and is central to career building opportunities Yet, public speaking anxiety is often referred to as the most common fear in everyday life and can hinder one 's ability to speak in public severely, While virtual and real audiences have been successfully utilized to treat public speaking anxiety in the past, little work has been done on identifying behavioral characteristics of speakers suffering from anxiety Complementary to automatic measures of anxiety, we are also interested in speakers ' perceptual differences when interacting with a virtual audience based on their level of anxiety in order to improve and further the development of virtual audiences for the training of public speaking and the reduction of anxiety. In this work we focus on the characterization of behavioral indicators and the automatic assessment of public speaking anxiety. Achieves a high correlation between ground truth and our estimation ( r=0. We identify several indicators for public speaking anxiety, among them are less eye contact with the audience, reduced variability in the voice, We automatically assess the public speaking anxiety as reported by the speakers through a self-assessment questionnaire using a speaker independent paradigm, Our approach using ensemble trees.
2K_test_1857	Action Unit ( AU ) detection from facial images is an important classification task in affective computing, However most existing approaches use carefully engineered feature extractors along with off-the-shelf classifiers, There has also been less focus on how well classifiers generalize when tested on different datasets. To learn a shared representation between multiple AUs directly from the input image. Indicate that our approach obtains competitive results on all datasets, also indicate that the network generalizes well to other datasets, even when under different training and testing conditions. In our paper we propose a multi-label convolutional neural network approach. Experiments on three AU datasets-CK+, DISFA and BP4D Cross-dataset experiments.
2K_test_1858	Large teams of robots that operate collectively, whose behavior emerges from local interactions with neighbors, are known as swarms This research is necessary if real world swarms are to be deployed in the future, With these results and with participant feedback about the helpfulness of the four display types, we hope future studies can make more informed decision about interface design when it comes to the control of swarms. While significant progress has been made improving the hardware, communication capabilities and autonomous operation of these swarms, we still have much to learn about how human operators control and interact with them, The study presented here investigates different methods of displaying information about the swarm state to operators, and asks them to make predictions about the swarm 's future state. Results show that summarizing the swarm 's current state to just an average position and bounding ellipse allowed predictions as accurate as those made when full state information was shown However, such display methods were inferior for prediction than either the summary center and ellipse or full information methods. In the study participants are shown swarms performing one of three different behaviors, and are asked to use the information available from the display to make their predictions, Furthermore two leader-based methods were used, whereby the operators were shown only a small subset of the swarm.
2K_test_1859	Planning for multirobot manipulation in dense clutter becomes particularly challenging as the motion of the manipulated object causes the connectivity of the robots ' free space to change and discuss future adaptations to general environments. Solve such complex motion planning problems, to decouple the problems of composing feasible object motions and planning paths for individual robots. Finally we show how to construct the FTG. This paper introduces a data structure, the Feasible Transition Graph ( FTG ), and algorithms that We define an equivalence relation over object configurations based on the robots ' free space connectivity Within an equivalence class, the homogeneous multirobot motion planning problem is straightforward, which allows us The FTG captures transitions among the equivalence classes and encodes constraints that must be satisfied for the robots to manipulate the object, From this data structure, we readily derive a complete planner to coordinate such motion. In some sample environments.
2K_test_1860	Sharing scientific data software, and instruments is becoming increasingly common as science moves toward large-scale, Sharing these resources requires extra work to make them generally useful, Our results have important implications for future empirical studies as well as funding policy. Although we know much about the extra work associated with sharing data, we know little about the work associated with sharing contributions to software, even though software is of vital importance to nearly every scientific result, This paper presents of the extra work that developers and end users of scientific software undertake. Our findings indicate that they conduct a rich set of extra work around community management, code maintenance education and training, developer-user interaction and foreseeing user needs, We identify several conditions under which they are likely to do this work, as well as design principles that can facilitate it. A qualitative interview-based study.
2K_test_1861	Eye tracking is a compelling tool for revealing people 's spatial-temporal distribution of visual attention Such an approach will allow designers to evaluate and refine their visual design without requiring the use of limited/expensive eye trackers. But quality eye tracking hardware is expensive and can only be used with one person at a time, Further webcam eye tracking systems have significant limitations on head movement and lighting conditions that result in significant data loss and inaccuracies, To address these drawbacks, to understand allocation of visual attention. Which demonstrated good accuracy when compared to a real eye tracker, and showed that it accurately generated gaze heatmaps and trajectory maps. We introduce a new approach that harnesses the crowd In our approach, crowdsourcing participants use mouse clicks to self-report the positions and trajectory for the following valuable eye tracking measures : first gaze, last gaze and all gazes. We validate our crowdsourcing approach with a user study, We then deployed our prototype, GazeCrowd in a crowdsourcing setting.
2K_test_1862	In a variety of peer production settings, from Wikipedia to open source software development to crowdsourcing, individuals may encounter edit, or review the work of unknown others Typically this is done without much context to the person 's past behavior or performance, This work provides insight into the impact of activity history design factors on psychological and behavioral outcomes that can be of use in other related settings. To understand how exposure to an unknown individual 's activity history influences attitudes and behaviors. Surprisingly negative work history did not lead to negative outcomes, but in contrast a positive work history led to positive initial impressions that persisted in the face of contrary information. We conducted an online experiment on Mechanical Turk varying the content, quality and presentation of information about another Turker 's work history.
2K_test_1863	Telepresence means business people can make deals in other countries, doctors can give remote medical advice, and soldiers can rescue someone from thousands of miles away, When interaction is mediated, people are removed from and lack context about the person they are making decisions about, We discuss implications of our results for theory and future research. In this paper we explore the impact of technological mediation on risk and dehumanization in decision-making. The results suggest that technological mediation influences decision making, but its influence depends on an individual 's self-construal : participants who saw themselves as defined through their relationships ( interdependent self-construal ) recommended riskier and more painful treatments in video conferencing than when face-to-face. We conducted a laboratory experiment involving medical treatment decisions.
2K_test_1864	And discuss possible incentives and safeguards to context sharing from a user standpoint. That allows mobile devices to opportunistically share contextual information, way for developers to request contextual data for their applications. That show how GCF 's ability to form groups increases users ' access to relevant and timely information. In this paper we present the Group Context Framework ( GCF ), a general-purpose toolkit GCF provides a standardized The framework then intelligently groups with other devices to satisfy these requirements Through two prototypes, we demonstrate how GCF can be used to support a broad range of collaborative and cooperative tasks, We then show how our framework 's architecture allows devices to opportunistically detect and collaborate with one another, even when running different applications. Finally we present two real-world domains.
2K_test_1865	In many scenarios involving human interaction with a remote swarm, the human operator needs to be periodically updated with state information from the robotic swarm, A complete representation of swarm state is high dimensional and perceptually inaccessible to the human Thus, a summary representation is often required, In addition it is often the case that the human-swarm communication channel is extremely bandwidth constrained and may have high latency, This motivates the need for the swarm itself to compute a summary representation of its own state for transmission to the human operator, The summary representation may be generated by selecting a subset of robots, known as the information leaders, whose own states suffice to give a bounded approximation of the entire swarm, even in the presence of uncertainty. For information leader selection. And proof of convergence for the algorithms demonstrating the performance and effectiveness of the proposed algorithms. In this paper we propose two fully distributed asynchronous algorithms that only rely on inter-robot local communication, In particular by representing noisy robot states as error ellipsoids with tunable confidence level, the information leaders are selected such that the Minimum-Volume Covering Ellipsoid ( MVCE ) summarizes the noisy swarm state boundary. We provide bounded optimality analysis We present simulation results.
2K_test_1866	High-mobility walking robots offer unique capabilities in complex off-road environments where wheeled vehicles are not able to travel, Key steps in planning a safe path for the robot autonomously include estimating the height of the support ground surface - which is often occluded by vegetation - and classifying the terrain and obstacles above the ground surface. However these environments can also pose significant autonomous navigation challenges, to support autonomous navigation for a high-mobility walking robot. This paper describes the development and experimental evaluation of a terrain classification and ground surface height estimation system. We provide experimental evaluation on an extensive, manually-labeled dataset collected from geographically diverse sites over a 28-month period.
2K_test_1867	For multi purpose manipulation tasks. We present the design, fabrication and characterization of a fiber optically sensorized robotic hand The robotic hand has three fingers that enable both pinch and power grips, The main bone structure was made of a rigid plastic material and covered by soft skin Both bone and skin contain embedded fiber optics for force and tactile sensing, Eight fiber optic strain sensors were used for rigid bone force sensing, and six fiber optic strain sensors were used for soft skin tactile sensing, For characterization different loads were applied in two orthogonal axes at the fingertip and the sensor signals were measured from the bone structure The skin was also characterized by applying a light load on different places for contact localization, The actuation of the hand was achieved by a tendon-driven under-actuated system Gripping motions are implemented using an active tendon located on the volar side of each finger and connected to a motor, Opening motions of the hand were enabled by passive elastic tendons located on the dorsal side of each finger.
2K_test_1868	For generating open-loop trajectories that solve the problem of rearrangement planning under uncertainty. Our key insight is we can formalize the selection problem as the `` best arm { '' } variant of the multi-armed bandit problem, We show that the successive rejects algorithm identifies the best candidate using fewer rollouts than a baseline algorithm in simulation, We also show that selecting a good candidate increases the likelihood of successful execution on a real robot. We present an algorithm We frame this as a selection problem where the goal is to choose the most robust trajectory from a finite set of candidates, We generate each candidate using a kinodynamic state space planner and evaluate it using noisy rollouts, We use the successive rejects algorithm to efficiently allocate rollouts between candidate trajectories given a rollout budget.
2K_test_1869	Because Simple Hand gives limited space for links, current iteration of links is not obviously nonlinear. However nonlinearity should be more obvious if links are designed for larger grippers. This paper presents a novel nonlinear compliant link It has two major properties : bi-directionality and stiffening compliance, Bi-directionality means it can be stretched and compressed, and is realized by antagonistic arrangement of an extension spring and a compression spring, Stiffening compliance means it becomes stiffer as it is stretched, and is realized by asymmetric geometry, The links are parts of Simple Hand.
2K_test_1870	A key challenge of developing robots that work closely with people is creating a user interface that allows a user to communicate complex instructions to a robot quickly and easily. We consider a walking logistics support robot, which is designed to carry heavy loads to locations that are too difficult to reach with a wheeled or tracked vehicle to allow a particular user to designate himself as the robot 's leader, and guide the robot along a desired path. To show that the proposed system is able to detect and track a leader through unconstrained and cluttered off-road environments under a wide variety of illumination and motion conditions. This paper presents a marker tracking system that uses near infrared cameras, retro-reflective markers and LIDAR. In this application the robot is carrying equipment and supplies for a group of pedestrians, and the primary task for the user interface is to keep the robot traveling with the overall group in the right formation, We provide an extensive quantitative evaluation.
2K_test_1871	Exploration of unknown environments is an important aspect to fielding teams of robots Without the ability to determine on their own where to go in the environment, the full potential of robotic teams is limited to the abilities of human operators to deploy them for search and rescue, mapping or other tasks that are predicated on gaining knowledge from the environment, This is of particular importance in real-world 3-Dimensional ( 3-D ) environments where simple planar assumptions can lead to incomplete exploration, for example real-world environments have areas underneath overhangs or inside caves. As an additional challenge, when the teams of robots have vastly different capabilities, the planning system must take those into account to efficiently utilize the available assets, for conducting 3-D exploration in cluttered environments. In this paper we present a combined air-ground system We first describe the hardware and software components of the system We then present our algorithm for planning 3-D goal locations for a heterogeneous team of robots to efficiently explore a previously unknown environment and.
2K_test_1872	Navigating in a previously unknown environment and recognizing naturally occurring text in a scene are two important autonomous capabilities that are typically treated as distinct. However these two tasks are potentially complementary, ( i ) scene and pose priors can benefit text spotting, and ( ii ) the ability to identify and associate text features can benefit navigation accuracy through loop closures, Previous approaches to autonomous text spotting typically require significant training data and are too slow for real-time implementation, text representation fast to compute. We show that we are able to improve SLAM illustrating how location priors enable improved loop closure with text features. In this work we propose a novel high-level feature descriptor, the `` junction { '' }, which is particularly well-suited to and is also. Through text spotting on datasets collected with a Google Tango.
2K_test_1873	Robot perception is generally viewed as the interpretation of data from various types of sensors such as cameras suggesting further investigation on the use of non-visual perception in human-robot team operations. In this paper we study indirect perception where a robot can perceive new information by making inferences from non-visual observations of human teammates, to represent the inter-visibility and to infer the locations of the doors, potential locations of the opponents. We use a special type of the Noisy-OR model known as BN2O model of Bayesian inference network. As a proof-of-concept study, we specifically focus on a door detection problem in a stealth mission setting where a team operation must not be exposed to the visibility of the team 's opponents, on both synthetic data and real person tracking data.
2K_test_1874	This paper explores the design space of simple legged robots capable of leaping culminating in new behaviors for the Penn Jerboa, an underactuated dynamically dexterous robot. Using a combination of formal reasoning and physical intuition, we analyze and test successively more capable leaping behaviors through successively more complicated body mechanics, Theoretical contributions Conceptual contributions.
2K_test_1875	Robotic swarms are distributed systems whose members interact via local control laws to achieve a variety of behaviors, In many practical applications, human operators may need to change the current behavior of a swarm from the goal that the swarm was going towards into a new goal due to dynamic changes in mission objectives, There are two related but distinct capabilities needed to supervise a robotic swarm The first is comprehension of the swarm 's state and the second is prediction of the effects of human inputs on the swarm 's behavior, Both of them are very challenging, Prior work in the literature has shown that inserting the human input as soon as possible to divert the swarm from its original goal towards the new goal does not always result in optimal performance ( measured by some criterion such as the total time required by the swarm to reach the second goal ), This phenomenon has been called Neglect Benevolence, conveying the idea that in many cases it is preferable to neglect the swarm for some time before inserting human input. In this paper we study how humans can develop an understanding of swarm dynamics so they can predict the effects of the timing of their input on the state and performance of the swarm allowing comparison between human and optimal input timing performance in control of swarms. Our results show that humans can learn to approximate optimal timing and that displays which make consensus variables perceptually accessible can enhance performance. We developed the swarm configuration shape-changing Neglect Benevolence Task as a Human Swarm Interaction ( HSI ) reference task.
2K_test_1876	In the future we envision domestic robots to play a large role in our everyday lives, This requires robots able to anticipate our needs and preferences and adapt their behavior. Since current robotics research takes place primarily in laboratory settings, it fails to take into account real users, In this work we explore how organization occurs in the kitchen to the problem of object return. Qualitative insights towards robot behavior during kitchen organization. An open source dataset of real life kitchens. Through a home study, Our analysis includes and a proof-of-concept application of this dataset.
2K_test_1877	For designing fully streaming, area-efficient FPGA implementations of common building blocks for vision algorithm. We demonstrate that our design works in practice. We develop a new paradigm By focusing on avoiding redundant computation we achieve a reduction of one to two orders of magnitude reduction in design area utilization as compared to previous implementations.
2K_test_1878	Experience Graphs have been shown to accelerate motion planning using parts of previous paths in an A { * } framework, Experience Graphs work by computing a new heuristic for weighted A { * } search on top of the domain 's original heuristic and the edges in an Experience Graph, The new heuristic biases the search toward relevant prior experience and uses the original heuristic for guidance otherwise, In previous work Experience Graphs were always built on top of domain heuristics which were computed by dynamic programming ( a lower dimensional version of the original planning problem ), When the original heuristic is computed this way the Experience Graph heuristic can be computed very efficiently However, there are many commonly used heuristics in planning that are not computed in this fashion, such as euclidean distance. While the Experience Graph heuristic can be computed using these heuristics, it is not efficient, and in many cases the heuristic computation takes much of the planning time, In this work we present a more efficient way to use these heuristics for motion planning problems. By making use of popular nearest neighbor algorithms.
2K_test_1879	Many robot applications involve lifelong planning in relatively static environments e, assembling objects or sorting mail in an office building In these types of scenarios, the robot performs many tasks over a long period of time, Thus the time required for computing a motion plan becomes a significant concern, prompting the need for a fast and efficient motion planner, Since these environments remain similar in between planning requests, planning from scratch is wasteful. Recently Experience Graphs ( E-Graphs ) were proposed to accelerate the planning process by reusing parts of previously computed paths to solve new motion planning queries more efficiently, to improve planning times with E-Graphs. We show the improvements with our method. This work describes a method given changes in the environment by lazily evaluating the validity of past experiences during the planning process. In a single-arm manipulation domain with simulations on the PR2 robot.
2K_test_1880	To solve the Multi-Robot Persistent Coverage Problem ( MRPCP ). The results produced by our algorithm. In this paper we present an algorithm Here, we seek to compute a schedule that will allow a fleet of agents to visit all targets of a given set while maximizing the frequency of visitation and maintaining a sufficient fuel capacity by refueling at depots, We also present a heuristic method to allow us to compute bounded suboptimal results in real time will allow a team of robots to efficiently cover a given set of targets or tasks persistently over long periods of time, even when the cost to transition between tasks is dynamic.
2K_test_1881	This paper reports on a 3D photomosaicing pipeline to relax the need for a prior model. Our approach produces 3D models that are more structurally representative of the environment being surveyed, We show that the method produces reasonably accurate surface reconstruction and blending consistency, with and without the use of a prior mesh. Using data collected from an autonomous underwater vehicle performing simultaneous localization and mapping ( SLAM ) Compared to other methods that generate a 2D-only mosaic We experimentally evaluate our approach with a Hovering Autonomous Underwater Vehicle ( HAUV ) performing inspection of a large underwater ship hull.
2K_test_1882	Wearable cameras provide a first-person perspective which enables continuous visual hand grasp analysis of everyday activities, In contrast to previous work focused on manual analysis of first-person videos of hand grasps. Our goal is to automatically recognize hand grasps and to discover the visual structures ( relationships ) between hand grasps using wearable cameras. We propose a fully automatic vision-based approach for grasp analysis A set of grasp classifiers are trained for discriminating between different grasp types based on large margin visual predictors Building on the output of these grasp classifiers, visual structures among hand grasps are learned based on an iterative discriminative clustering procedure. We first evaluated our classifiers on a controlled indoor grasp dataset and then validated the analytic power of our approach on real-world data taken from a machinist Analysis of real-world video.
2K_test_1883	Assembly of large structures requires large fixtures, often referred to as monuments Their cost and massive size limit flexibility and scalability of the manufacturing process, Numerous small mobile robots can replace these large structures and, therefore replicate the efficiency of the assembly line with far more flexibility, An assembly line made up of mobile manipulators can easily and rapidly be reconfigured to support scalability and a varied product mix, while allowing for near optimal resource assignment. The challenge to using small robots in place of monuments is making their joint behavior precise enough to accomplish the task and efficient enough to execute subtasks in a reasonable period of time, to achieve the necessary precision and overall efficiency to build a large structure. In this paper we describe a set of techniques that we combine We describe and. In the context of a testbed we implemented for assembling a wing ladder.
2K_test_1884	Is mapping each noun in the command into a physical object in the environment. For commanding mobile robots in outdoor environments, aims at making interactions in human-robot teams natural, Grounding the main focus of this paper for interpreting the spatial relations. Our experiments clearly show that the proposed approach is efficient for commanding outdoor robots. We propose a language-driven navigation approach We consider unknown environments that contain previously unseen objects The proposed approach Robots receive from human teammates commands in natural language, such as `` Navigate around the building to the car left of the fire hydrant and near the tree { '' }, A robot needs first to classify its surrounding objects into categories, using images obtained from its sensors, The result of this classification is a map of the environment, where each object is given a list of semantic labels, such as `` tree { '' } and `` car { '' }, with varying degrees of confidence, Then the robot needs to ground the nouns in the command We use a probabilistic model, such as `` left of { '' } and `` near { '' } The model is learned from examples provided by humans, For each noun in the command, a distribution on the objects in the environment is computed by combining spatial constraints with a prior given as the semantic classifier 's confidence values The robot needs also to ground the navigation mode specified in the command, such as `` navigate quickly { '' } and `` navigate covertly { '' }, as a cost map, The cost map is also learned from examples, using Inverse Optimal Control ( IOC ), The cost map and the grounded goal are used to generate a path for the robot. This approach is evaluated on a robot in a real-world environment.
2K_test_1885	For combining visual odometry and lidar odometry. Here we present a general framework in a fundamental and first principle method The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity, Then scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously. With datasets collected in our own experiments as well as using the KITTI odometry benchmark In addition to comparison of the motion estimation accuracy, we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes.
2K_test_1886	The problem of adapting a demonstrated trajectory to a new start and goal configuration enables the robot to select a more appropriate norm for the task, as well as learn how to adapt the demonstration from the user. Show that this can significantly improve the robot 's ability to accurately generalize the demonstration. We formalize as an optimization problem over a Hilbert space of trajectories : minimize the distance between the demonstration and the new trajectory subject to the new end point constraints, We show that the commonly used version of Dynamic Movement Primitives ( DMPs ) implement this minimization in the way they adapt demonstrations, for a particular choice of the Hilbert space norm, The generalization to arbitrary norms.
2K_test_1887	Many motion planning problems in robotics are high dimensional planning problems, While sampling-based motion planning algorithms handle the high dimensionality very well, the solution qualities are often hard to control due to the inherent randomization, In addition they suffer severely when the configuration space has several `narrow passages ', Search-based planners on the other hand typically provide good solution qualities and are not affected by narrow passages, However in the absence of a good heuristic or when there are deep local minima in the heuristic, they suffer from the curse of dimensionality. For dynamically generating heuristics to guide the search out of local minima, On the theoretical side, we provide guarantees on completeness and bounds on suboptimality of the solution found. And show its benefits over these approaches. In this work our primary contribution is a method in addition to the original heuristic ( s ) used, With the ability to escape local minima easily, the effect of dimensionality becomes less pronounced. We compare our proposed method with the recently published Multi-Heuristic A { * } search, and the popular RRT-Connect in a full-body mobile manipulation domain for the PR2 robot.
2K_test_1888	Autonomous systems that navigate in unknown environments encounter a variety of planning problems, This work opens the door on the more general problem of adaptive motion planning. The success of any one particular planning strategy depends on the validity of assumptions it leverages about the structure of the problem, Is the cost map locally convex ? Does the feasible state space have good connectivity ? We address the problem of determining suitable motion planning strategies that can work on a diverse set of applications. We have developed a planning system that does this by running competing planners in parallel, In this paper we present an approach that constructs a planner ensemble - a set of complementary planners that leverage a diverse set of assumptions, Our approach optimizes the submodular selection criteria with a greedy approach and lazy evaluation, We seed our selection with learnt priors on planner performance, thus allowing us to solve new applications without evaluating every planner on that application. In simulation from an autonomous helicopter.
2K_test_1889	That solves rearrangement planning problems. We demonstrate the ability to solve more rearrangement by pushing tasks than existing primitive based solutions, Finally we show the plans we generate are feasible for execution on a real robot. We present a randomized kinodynamic planner We embed a physics model into the planner to allow reasoning about interaction with objects in the environment By carefully selecting this model, we are able to reduce our state and action space, gaining tractability in the search The result is a planner capable of generating trajectories for full arm manipulation and simultaneous object interaction.
2K_test_1890	To rearrange cluttered environments. And show that on a variety of environments we can achieve a higher planning success rate given a restricted time budget for planning. In this work we present a fast kinodynamic RRT-planner that uses dynamic nonprehensile actions In contrast to many previous works, the presented planner is not restricted to quasi-static interactions and monotonicity, Instead the results of dynamic robot actions are predicted using a black box physics model, Given a general set of primitive actions and a physics model, the planner randomly explores the configuration space of the environment to find a sequence of actions that transform the environment into some goal configuration, In contrast to a naive kinodynamic RRT-planner we show that we can exploit the physical fact that in an environment with friction any object eventually comes to rest, This allows a search on the configuration space rather than the state space, reducing the dimension of the search space by a factor of two without restricting us to non-dynamic interactions. We compare our algorithm against a naive kinodynamic RRT-planner.
2K_test_1891	Capable of finding statistically significant discrepancies, determining the situations in which they occur, and making simple corrections to the world model to improve performance. We present an execution monitoring framework In our approach, plans are initially based on a model of the world that is only as faithful as computational and algorithmic limitations allow Through experience, the monitor discovers previously unmodeled modes of the world, defined as regions of a feature space in which the experienced outcome of a plan deviates significantly from the predicted outcome, The monitor may then make suggestions to change the model to match the real world more accurately. We demonstrate this approach on the adversarial domain of robot soccer : we monitor pass interception performance of potentially unknown opponents to try to find unforeseen modes of behavior that affect their interception performance.
2K_test_1892	To perform statistical analysis with thousands of experiments and arbitrary mesh models. We demonstrate the advantages of our algorithm Our results show that spare work surfaces are beneficial to assembly, Tilted work surfaces are only sometimes beneficial, depending on the objects. The goal of this paper is to develop a regrasp planning algorithm general enough We focus on pick-and-place regrasp which reorients an object from one placement to another by using a sequence of pickups and place-downs, We improve the pick-and-place regrasp approach developed in 1990s and analyze its performance in robotic assembly with different work surfaces in the workcell Our algorithm will automatically compute the stable placements of an object, find several force-closure grasps, generate a graph of regrasp actions, and search for regrasp sequences. With various mesh models and use the algorithm to evaluate the completeness, the cost and the length of regrasp sequences with different mesh models and different assembly tasks in the presence of different work surfaces.
2K_test_1893	Simultaneous localization and mapping with infinite planes is attractive because of the reduced complexity with respect to both sparse point-based and dense volumetric methods. We show how to include infinite planes into a least-squares formulation for mapping that improves convergence. To show its advantages over alternative solutions results. Using a homogeneous plane parametrization with a corresponding minimal representation for the optimization, Because it is a minimal representation, it is suitable for use with Gauss-Newton, Powell 's Dog Leg and incremental solvers such as iSAM, We also introduce a relative plane formulation We also introduce a simple mapping system and present. We evaluate our proposed approach on simulated data experimental, showing real-time mapping of select indoor environments with a hand-held RGBD sensor.
2K_test_1894	To build 3D maps with ground and aerial robots The paper describes the basic models, the problem formulation and the algorithm. We develop a computationally efficient control policy for active perception that incorporates explicit models of sensing and mobility Like previous work, our policy maximizes an information-theoretic objective function between the discrete occupancy belief distribution ( e, voxel grid ) and future measurements that can be made by mobile sensors, However our work is unique in three ways, First we show that by using Cauchy-Schwarz Quadratic Mutual Information ( CSQMI ), we get significant gains in efficiency, Second while most previous methods adopt a myopic, gradient-following approach that yields poor convergence properties, our algorithm searches over a set of paths and is less susceptible to local minima, In doing so we explicitly incorporate models of sensors, and model the dependence ( and independence ) of measurements over multiple time steps in a path, Third because we consider models of sensing and mobility, our method naturally applies to both ground and aerial vehicles. Via simulation and experimentation.
2K_test_1895	With the prevalence of social media, such as Twitter short-length text like microblogs have become an important mode of text on the Internet, In contrast to other forms of media, such as newspaper the text in these social media posts usually contains fewer words, and is concentrated on a much narrower selection of topics, For these reasons traditional LDA-based sentiment and topic modeling techniques generally do not work well in case of social media data, Another characteristic feature of this data is the use of special meta tokens, such as hashtags which contain unique semantic meanings that are not captured by other ordinary words. In the recent years, many topic modeling techniques have been proposed for social media data, but the majority of this work does not take into account the specialty of tokens, such as hashtags and treats them as ordinary words, to address the problem of discovering latent topics and their sentiment from social media data, mainly microblogs like Twitter. In this paper we propose probabilistic graphical models We first propose MTM ( Microblog Topic Model ), a generative model that assumes each social media post generates from a single topic, and models both words and hashtags separately, We then propose MSTM ( Microblog Sentiment Topic Model ), an extension of MTM, which also embodies the sentiment associated with the topics. We evaluated our models using Twitter dataset.
2K_test_1896	There have been increasing interests in the robotics community in building smaller and more agile autonomous micro aerial vehicles ( MAVs ). In particular the monocular visual-inertial system ( VINS ) that consists of only a camera and an inertial measurement unit ( IMU ) forms a great minimum sensor suite due to its superior size, weight and power ( SWaP ) characteristics. We present extensive statistical analysis to verify the performance of our approach in different environments with varying flight speeds.
2K_test_1897	Learning from demonstration ( LfD ) is a common technique applied to many problems in robotics, such as populating grasp databases, training for reinforcement learning of high-level skill sets and bootstrapping motion planners, The data set collected has been made available to the robotics community. While such approaches are generally highly valued, they rely on the often time-consuming process of gathering user demonstrations, and hence it becomes difficult to attain a sizeable dataset, Furthermore we show how our tool can be used to gather a large set of demonstrations of a mobile manipulation task by leveraging existing crowdsource platforms. To teach a robot how to grasp, to teach a robot how to perform dexterous manipulation tasks such as scooping and to accelerate motion planning for full-body manipulation tasks. We also present experiments in which we apply demonstrations collected through our infrastructure.
2K_test_1898	We demonstrate distributed online, and real-time cooperative localization and mapping between multiple robots operating throughout an unknown environment using indirect measurements, to efficiently identify inlier multi-robot loop closures, which significantly improves the trajectory accuracy over long-term navigation. That our method can efficiently build maps of large indoor and outdoor environments in a distributed, online and real-time setting. We present a novel Expectation Maximization ( EM ) based approach by incorporating robot pose uncertainty An EM and hypothesis based method is used to determine a common reference frame, We detail a 2D laser scan correspondence method to form robust correspondences between laser scans shared amongst robots. The implementation is experimentally validated using teams of aerial vehicles, and analyzed to determine its accuracy, computational efficiency scalability to many robots, and robustness to varying environments, We demonstrate through multiple experiments.
2K_test_1899	Our work is motivated by the potential impact of realistic simulators on the development cycle of software for real robots, Unlike calibration where the goal is to identify and remove error from a signal. We study the problem of building a sensor model for the purpose of simulation our aim to reproduce the signal in its entirety, including its error properties. The case is made for building models from approximate state information, relieving the burden of ground truth, Instead of physically modeling sensor behavior, a data-driven approach is taken. The implementation of our approach to simulate a simple but noisy laser rangefinder is described, Finally approaches to validate the simulator are discussed, We compare not only raw sensor predictions, but also overall performance of algorithms on simulated versus real data.
2K_test_1900	In social voting Web sites, how do the user actions - up-votes, down-votes and comments - evolve over time ? Are there relationships between votes and comments ? What is normal and what is suspicious ? These are the questions we focus on, models the coevolution of user activities. Our first contribution is two discoveries : ( i ) the number of comments grows as a power-law on the number of votes and ( ii ) the time between a submission creation and a user 's reaction obeys a log-logistic distribution VNC outperformed state-of-the-art baselines on accuracy, Additionally we illustrate VNC usefulness for forecasting and outlier detection. Based on these patterns, we propose VNC ( VOTE-AND-COMMENT ), a parsimonious but accurate and scalable model that.
2K_test_1901	Autonomous mobile robots are required to operate in partially known and unstructured environments, It is imperative to guarantee safety of such systems for their successful deployment, Current state of the art does not fully exploit the sensor and dynamic capabilities of a robot, Also given the non-holonomic systems with non-linear dynamic constraints, it becomes computationally infeasible to find an optimal solution if the full dynamics are to be exploited online. To guarantee the safety of the robot. In this paper we present an online algorithm through an emergency maneuver library, The maneuvers in the emergency maneuver library are optimized such that the probability of finding an emergency maneuver that lies in the known obstacle free space is maximized, We prove that the related trajectory set diversity problem is monotonic and submodular which enables one to develop an efficient trajectory set generation algorithm with bounded sub-optimality, We generate an off-line computed trajectory set that exploits the full dynamics of the robot and the known obstacle-free region.
2K_test_1902	One example is server-side scheduling for video service, where clients request flows of content from a server with limited capacity, and any content not delivered by its deadline is lost State-of-the-art policies, like Discriminatory Processor Sharing and Weighted Fair Queueing, use a fixed static proportional allocation of service rate and fail to achieve both goals, The well-known Earliest Deadline First policy minimizes overall loss, but fails to provide proportional loss across flows, because it treats packets as independent jobs. We prove that all policies in this broad class minimize overall loss Furthermore, we demonstrate that many EPDF policies accurately differentiate loss fractions in proportion to class weights, satisfying the second goal. This paper introduces the Earliest Progressive Deadline First ( EPDF ) class of policies.
2K_test_1903	We consider the task of estimating the entropy of p to within +/- Delta ( with high probability ).
2K_test_1904	Theoretical and practical implications for collaboration across culture are discussed. We examined the effects gender and moral identity on collaborative behavior among Face ( Chinese ) and Dignity ( Canadian ) cultures. We predicted and found that overall, Chinese individuals were less helpful than Canadians, This effect was stronger for males than females Interestingly, more helping behavior was observed among Canadians with high levels of internal moral identity Yet, this effect was not observed among Chinese individuals.
2K_test_1905	Human activity recognition is an important and challenging task for video content analysis and understanding, Individual activity recognition has been well studied recently. However recognizing the activities of human group with more than three people having complex interactions is still a formidable challenge to deal with complex situation where there are multiple sub-groups, To characterize the inherent interactions of intra-subgroups and inter-subgroups with the varying number of participants. Demonstrate effectiveness of the proposed method. In this paper a novel human group activity recognition method is proposed this paper proposes three types of group-activity descriptor using motion trajectory and appearance information of people. Experimental results on a public human group activity dataset.
2K_test_1906	In many markets products are highly complex with an extremely large set of features, In advertising auctions for example, a viewer on a web page, has numerous features describing the viewer 's demographics, browsing history temporal aspects, In these markets an auctioneer must select a few key features to signal to bidders. These features should be selected such that the bidder with the highest value for the product can construct a bid so as to win the auction, solution for this problem. We present an efficient algorithmic in a setting where the product 's features are drawn independently from a known distribution, the bidders ' values for a product are additive over their known values for the features of the product, and the number of features is exponentially larger than the number of bidders and the number of signals, Our approach involves solving a novel optimization problem regarding the expectation of a sum of independent random vectors that may be of independent interest, We complement our positive result with a hardness result for the problem when features are arbitrarily correlated This result is based on the conjectured hardness of learning k-juntas, a central open problem in learning theory.
2K_test_1907	Standard approaches to reachability problems for linear hybrid systems require numerical solutions for large optimization problems, and become infeasible for systems involving both nonlinear dynamics over the reals and stochasticity. Which solves probabilistic bounded reachability problems for two classes of models of stochastic hybrid systems. We demonstrate SReach 's applicability. In this paper we present a new tool SReach The first one is ( nonlinear ) hybrid automata with parametric uncertainty, The second one is probabilistic hybrid automata with additional randomness for both transition probabilities and variable resets, SReach encodes stochastic information by using a set of introduced random variables, and combines delta-complete decision procedures and statistical tests to solve delta-reachability problems in a sound manner Compared to standard simulation-based methods, it supports non-deterministic branching, increases the coverage of simulation, and avoids the zero-crossing problem. By discussing three representative biological models and additional benchmarks for nonlinear hybrid systems with multiple probabilistic system parameters.
2K_test_1908	Weighted signed networks ( WSNs ) are networks in which edges are labeled with positive and negative weights, WSNs can capture like/dislike, trust/distrust and other social relationships between people. In this paper we consider the problem of predicting the weights of edges in such networks. We propose two novel measures of node behavior : the goodness of a node intuitively captures how much this node is liked/trusted by other nodes, while the fairness of a node captures how fair the node is in rating other nodes ' likeability or trust level We provide axioms that these two notions need to satisfy and show that past work does not meet these requirements for WSNs, We provide a mutually recursive definition of these two concepts and prove that they converge to a unique solution in linear time, We use the two measures to predict the edge weight in WSNs. That when compared against several individual algorithms from both the signed and unsigned social network literature We then use these as features in different multiple regression models.
2K_test_1909	The collective buys energy as a group through a central coordinator who also decides about the storage and usage of renewable energy produced by the collective, Minimizing the cost is not only of interest to the consumers but is also socially desirable because it reduces the consumption at times of peak demand. In this paper we focus on demand side management in consumer collectives with community owned renewable energy generation and storage facilities for effective integration of renewable energy with the existing fossil fuel-based power supply system, Our objective is to design coordination algorithms to minimize the cost of electricity consumption of the consumer collective while allowing the consumers to make their own consumption decisions based on their private consumption constraints and preferences. We prove that our algorithm converges, and it achieves the optimal solution We also present simulation results to quantify the performance of our algorithm. We develop an iterative coordination algorithm in which the coordinator makes the storage decision and shapes the demands of the consumers by designing a virtual price signal for the agents. Based on real world consumption data.
2K_test_1910	A key challenge in ITS research and development is to support tutoring at scale, for example by embedding tutors in MOOCs. An obstacle to at-scale deployment is that ITS architectures tend to be complex, not easily deployed in browsers without significant server-side processing, and not easily embedded in a learning management system ( LMS ), We present so that tutors can be embedded in MOOCs. The feasibility of this general approach to ITS/MOOC integration was demonstrated. A widely used ITS authoring tool suite, CTAT/TutorShop was modified Specifically, the inner loop ( the example-tracing tutor engine ) was moved to the client by reimplementing it in JavaScript, and the tutors were made compatible with the LTI e-learning standard. A case study in which with simple tutors in an edX MOOC `` Data Analytics and Learning.
2K_test_1911	To be able to provide better support for collaborative learning in Intelligent Tutoring Systems, it is important to understand how collaboration patterns change Although interactive talk is often held as a gold standard in collaboration, as students become more proficient, it may not be as important. Prior work has looked at the interdependencies between utterances and the change of dialogue over time, but it has not addressed how dialogue changes during a lesson, an analysis that allows us to investigate the adaptivity of student strategies as students gain domain knowledge. We found that over time, the frequency of interactive talk and errors both decrease in dyads working together on conceptual problems.
2K_test_1912	Better conversational alignment can lead to shared understanding, changed beliefs and increased rapport, provide guidelines for development of peer tutoring agents that can increase learning gains through subtle changes to improve tutor-tutee alignment. We investigate the relationship in peer tutoring of convergence, interpersonal rapport and student learning, for computational modeling of convergence. Our results which illustrate that rapport as well as convergence are significantly correlated with learning gains. We develop an approach by accounting for the horizontal richness and time-based dependencies that arise in non-stationary and noisy longitudinal interaction streams.
2K_test_1913	Collaborative and individual learning appear to have complementary strengths ; however, the best way to combine these learning methods is still unclear, While previous work has demonstrated the effectiveness of Intelligent Tutoring Systems ( ITSs ) for individual learning, collaborative learning with ITSs is much less frequent - especially for young students, In addition we propose future research to understand how to best combine individual and collaborative learning within an ITS. In this paper we discuss our prior and future work with elementary school students that aims to investigate how to best combine individual and collaborative learning using their complementary strengths within an ITS. Our previous findings demonstrate that ITSs are able to support collaboration, as well as individual learning.
2K_test_1914	Internet of Things ( IoT ) allows for cyber-physical applications to be created and composed to provide intelligent support or automation of end-user tasks For many of such tasks, human participation is crucial to the success and the quality of the tasks, The cyber systems should proactively request help from the humans to accomplish the tasks when needed. However the outcome of such system-human synergy may be affected by factors external to the systems, Failure to consider those factors when involving human participants in the tasks may result in suboptimal performance and negative experience on the humans for automated generation of control strategies of cyber-human systems, how explicit modeling of human participant can be used in automated planning to generate cooperative strategy of human and system to achieve a given task, by means of which best and appropriately utilize the human. We illustrate our approach. Through an example of indoor air quality control in smart homes.
2K_test_1915	For capturing subjective similarity measurements. We found MindMiner was easy to learn and use, and could capture users ' implicit knowledge about writing performance and cluster target entities into groups that match subjects ' mental models We also found that MindMiner 's constraint suggestions and uncertainty polling functions could improve both efficiency and the quality of clustering. We present MindMiner a mixed-initiative interface via a combination of new interaction techniques and machine learning algorithms, MindMiner collects qualitative hard to express similarity measurements from users via active polling with uncertainty and example based visual constraint creation, MindMiner also formulates human prior knowledge into a set of inequalities and learns a quantitative similarity distance metric via convex optimization. In a 12-subject peer-review understanding task.
2K_test_1916	Gestures during spoken dialog play a central role in human communication, As a consequence models of gesture generation are a key challenge in research on virtual humans, embodied agents capable of face-to-face interaction with people. Machine learning approaches to gesture generation must take into account the conceptual content in utterances, physical properties of speech signals and the physical properties of the gestures themselves, To address this challenge to facilitate supervised learning to jointly learn deep neural networks and second order linear chain temporal contingency. Shows significant improvement over previous work on gesture prediction, shows that DCNFs outperform the state-of-the-art approaches. We proposed a gestural sign scheme and presented the DCNF model, a model The approach we took realizes both the mapping relation between speech and gestures while taking account temporal relations among gestures. Our experiments on human co-verbal dataset A generalization experiment performed on handwriting recognition also.
2K_test_1918	Automated visual analysis is an effective method for understanding changes in natural phenomena over massive city-scale landscapes. However the view-point spectrum across which image data can be acquired is extremely wide, ranging from macro-level overhead ( aerial ) images spanning several kilometers to micro-level front-parallel ( street-view ) images that might only span a few meters, to generate large-scale estimates of land surface conditions. Our results show that our approach can efficiently integrate both micro and macro-level images, along with other forms of meta-data, to efficiently estimate city-scale phenomena to show the ability of our method to generalize to a diverse set of estimation tasks. This work presents a unified framework for robustly integrating image data taken at vastly different viewpoints.
2K_test_1919	Computer vision is increasingly becoming interested in the rapid estimation of object detectors, The canonical strategy of using Hard Negative Mining to train a Support Vector Machine is slow, since the large negative set must be traversed at least once per detector, Recent work has demonstrated that, with an assumption of signal stationarity, Linear Discriminant Analysis is able to learn comparable detectors without ever revisiting the negative set, Even with this insight, the time to learn a detector can still be on the order of minutes, Correlation filters on the other hand, can produce a detector in under a second However, this involves the unnatural assumption that the statistics are periodic, and requires the negative set to be re-sampled per detector size, These two methods differ chiefly in the structure which they impose on the covariance matrix of all examples. This paper is ( i ) to assume periodic statistics without needing to revisit the negative set and ( ii ) to accelerate the estimation of detectors with aperiodic statistics. Verified that periodicity is detrimental. A comparative study It is experimentally.
2K_test_1920	Given the re-broadcasts ( i, retweets ) of posts in Twitter, how can we spot fake from genuine user reactions ? What will be the tell-tale sign - the connectivity of retweeters, their relative timing or something else ? High retweet activity indicates influential users, and can be monetized Hence, there are strong incentives for fraudulent users to artificially boost their retweets ' volume. Here we explore the identification of fraudulent and genuine retweet threads. Our main contributions are : ( a ) the discovery of patterns that fraudulent activity seems to follow ( the `` triangles { '' } and `` homogeneity { '' } patterns, the formation of micro-clusters in appropriate feature spaces ) ; and. B ) `` RTGen { '' }, a realistic generator that mimics the behaviors of both honest and fraudulent users.
2K_test_1921	Given the retweeting activity for the posts of several Twitter users. How can we distinguish organic activity from spammy retweets by paid followers to boost a post 's appearance of popularity ? More generally, given groups of observations, can we spot strange groups ? Our main intuition is that organic behavior has more variability, while fraudulent behavior like retweets by botnet members. Here we propose : ( A ) ND-Sync, an efficient method for detecting group fraud, and ( B ) a set of carefully designed features for characterizing retweet threads, ND-Sync is effective in spotting retweet fraudsters, robust to different types of abnormal activity, and adaptable as it can easily incorporate additional features. We refer to the detection of such synchronized observations as the Synchonization Fraud problem, and we study a specific instance of it, Retweet Fraud Detection manifested in Twitter.
2K_test_1922	For representing objects in depth images. Reveal the superiority of the proposed method. Comparisons with the state-of-the-art methods.
2K_test_1923	Ensemble methods for classification have been effectively used for decades, while for outlier detection it has only been studied recently. For outlier detection in multi-dimensional point data. We show that CARE performs significantly better than or at least similar to the individual baselines as well as the existing state-of-the-art outlier ensembles. In this work we design a new ensemble approach which provides improved accuracy by reducing error through both bias and variance by considering outlier detection as a binary classification task with unobserved labels In this paper, we propose a sequential ensemble approach called CARE that employs a two-phase aggregation of the intermediate results in each iteration to reach the final outcome Unlike existing outlier ensembles, our ensemble incorporates both the parallel and sequential building blocks to reduce bias as well as variance by ( i ) successively eliminating outliers from the original dataset to build a better data model on which outlierness is estimated ( sequentially ), and ( ii ) combining the results from individual base detectors and across iterations ( parallelly ).
2K_test_1924	For identifying drug-target interactions The goal of the proposed method is not simply to predict such interactions from experiments that have already been conducted, but to iteratively choose as few new experiments as possible to improve the accuracy of the predictive model, for estimating the accuracy of the current model. That active learning driven experimentation using KBMF can result in highly accurate models while performing as few as 14\ % of the possible experiments, and more accurately than random sampling of an equivalent number, and show how it can be used in practice to decide when to stop an active learning process. An active learning method is presented which considers the interaction between multiple drugs and multiple targets at the same time Kernelized Bayesian matrix factorization ( KBMF ) is used to model the interactions We also provide a method based on the learning curve. We demonstrate on four previously characterized drug effect data sets.
2K_test_1925	To improve the automatic detection of events in short sentences when in the presence of a large number of event classes.
2K_test_1926	Problems of this nature arise in formal verification of continuous and hybrid dynamical systems, where there is an increasing need for methods to expedite formal proofs. This paper presents a theoretical and experimental comparison of sound proof rules for proving invariance of algebraic sets, that is sets satisfying polynomial equalities, under the flow of polynomial ordinary differential equations. The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting. We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of heterogeneous benchmarks.
2K_test_1927	However every object in the image is composed of several features such as color, texture depth and motion, That is why single-feature based segmentation method often fails Humans can segment the objects in video with ease because the human visual system enables to consider color, texture depth and motion at the same time. In this paper we propose the video segmentation algorithm which is motivated by the human visual system The algorithm performs the video segmentation task by simultaneously utilizing the color histogram of the color, the optical flow of the motion, and the homography of the structure.
2K_test_1928	Many data structures ( e, matrices ) are typically accessed with multiple access patterns, Depending on the layout of the data structure in physical address space, some access patterns result in non-unit strides, In existing systems which are optimized to store and access cache lines, non-unit strided accesses exhibit low spatial locality Our framework is general, and can benefit many modern data-intensive applications. Therefore they incur high latency, and waste memory bandwidth and cache space, to address this problem Our idea is to enable the memory controller to access multiple values that belong to a strided pattern from different chips using a single read/write command. We propose the Gather-Scatter DRAM ( GS-DRAM ) We observe that a commodity DRAM module contains many chips Each chip stores a part of every cache line mapped to the module, To realize this idea, GS-DRAM first maps the data of each cache line to different chips such that multiple values of a strided access pattern are mapped to different chips Second, instead of sending a separate address to each chip, GS-DRAM maps each strided pattern to a small pattern ID that is communicated to the module, Based on the pattern ID, each chip independently computes the address of the value to be accessed, The cache line returned by the module contains different values of the strided pattern gathered from different chips. We design an end-to-end system to exploit GS-DRAM.
2K_test_1929	Though most would agree that accountability and privacy are both valuable, today 's Internet provides little support for either, Previous efforts have explored ways to offer stronger guarantees for one of the two, typically at the expense of the other ; indeed, at first glance accountability and privacy appear mutually exclusive. At the center of the tussle is the source address : in an accountable Internet, source addresses undeniably link packets and senders so hosts can be punished for bad behavior, In a privacy-preserving Internet, source addresses are hidden as much as possible, In this paper we argue that a balance is possible. We introduce the Accountable and Private Internet Protocol ( APIP ), which splits source addresses into two separate fields - an accountability address and a return address - and introduces independent mechanisms for managing each, Accountability addresses rather than pointing to hosts, point to accountability delegates, which agree to vouch for packets on their clients ' behalves, taking appropriate action when misbehavior is reported, With accountability handled by delegates, senders are now free to mask their return addresses ; we discuss a few techniques for doing so.
2K_test_1930	The growing number of sensor-based interactive applications and services are pushing the limits of the on-board computing resources in vehicles, With vehicles increasingly being connected to the Internet, offloading the computation to cloud-computing infrastructures is an attractive solution. However the large sensory data inputs of interactive applications makes offloading challenging across dynamic network conditions, and different application requirements or policies To address this challenge to adaptively offload specific vehicular application components or modules. We show that our mechanism can help meet application response time constraints. We design a system to the cloud, We particularly develop heuristic mechanisms for the placement and scheduling of modules on the On-Board Unit ( OBU ) and a cloud server under dynamic networking conditions during driving. Through an experimental evaluation of the end-end application response time using our prototype vehicular cloud offloading system.
2K_test_1931	We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services. To make the best use of an RDMA network. This paper describes the design and implementation of HERD, a key-value system designed Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives ; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware HERD has two unconventional decisions : First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow A HERD client writes its request into the server 's memory ; the server computes the reply, This design uses a single round trip for all requests and.
2K_test_1932	TTL caching models have recently regained significant research interest due to their connection to popular caching policies such as LRU. This paper advances the state-of-the-art analysis of TTL-based cache networks by. Results highlight that existing Poisson approximations in binary-tree topologies are subject to relative errors as large as 30\ %, depending on the tree depth. Developing two exact methods with orthogonal generality and computational complexity The first method generalizes existing results for line networks under renewal requests to the broad class of caching policies whereby evictions are driven by stopping times ; in addition to classical policies used in DNS and web caching, our stopping time model captures an emerging new policy implemented in SON switches and Amazon web services, The second method further generalizes these results to feedforward networks with Markov arrival process ( MAP ) requests MAPs are particularly suitable for non-line networks because they are closed not only under superposition and splitting, as known but also under caching operations with phase-type ( PH ) TTL distributions, The crucial benefit of the two closure properties is that they jointly enable the first exact analysis of TTL feedforward cache networks in great generality.
2K_test_1933	To date the study of dispatching or load balancing in server farms has primarily focused on the minimization of response time, Server farms are typically modeled by a front-end router that employs a dispatching policy to route jobs to one of several servers, with each server scheduling all the jobs in its queue via Processor-Sharing. However the common assumption has been that all jobs are equally important or valuable, in that they are equally sensitive to delay, Our work departs from this assumption In this context, we ask `` what is a good dispatching policy to minimize the value-weighted response time metric ?. We are able to deduce many unexpected results regarding dispatching. : we model each arrival as having a randomly distributed value parameter, independent of the arrival 's service requirement ( job size ), Given such value heterogeneity, the correct metric is no longer the minimization or response time, but rather the minimization of value-weighted response time, { '' } We propose a number of new dispatching policies that are motivated by the goal of minimizing the value-weighted response time. Via a combination of exact analysis, asymptotic analysis and simulation.
2K_test_1934	For programming graph- based algorithms in a declarative fashion, In this paper we present the syntax and operational semantics of our language and. LM tends to be more expressive than other logic programming languages, LM programs are naturally concurrent illustrate its use. We have designed a new logic programming language called LM ( Linear Meld ) Our language is based on linear logic, an expressive logical system where logical facts can be consumed, Because LM integrates both classical and linear logic, because facts are partitioned by nodes of a graph data structure, Computation is performed at the node level while communication happens between connected nodes, through a number of examples.
2K_test_1935	It is hard to efficiently model the light transport in scenes with translucent objects for interactive applications, The inter-reflection between objects and their environments and the subsurface scattering through the materials intertwine to produce visual effects like color bleeding, light glows and soft shading. Monte-Carlo based approaches have demonstrated impressive results but are computationally expensive, and faster approaches model either only inter-reflection or only subsurface scattering. We demonstrate scene relighting and dynamically varying object translucencies at near interactive rates. In this paper we present a simple analytic model that combines diffuse inter-reflection and isotropic subsurface scattering, Our approach extends the classical work in radiosity by including a subsurface scattering matrix that operates in conjunction with the traditional form factor matrix, This subsurface scattering matrix can be constructed using analytic, measurement-based or simulation-based models and can capture both homogeneous and heterogeneous translucencies. Using a fast iterative solution to radiosity.
2K_test_1936	Motivation : Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism, The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks, around which transcriptional regulation is focused owing to a disease or a genetic perturbation, Using a z-score-based scoring scheme, RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network, The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes, Overall MIRA is a promising algorithm for detecting metabolic drug targets and understanding the relation between gene expression and metabolic activity. With the goal of eliminating the following problems in detecting reporter metabolites : ( i ) conventional statistical methods suffer from small sample sizes, ( ii ) as z-score ranges from minus to plus infinity, calculating average scores can lead to canceling out opposite effects and ( iii ) analyzing genes one by one, then aggregating results can lead to information loss, that calculates the aggregate transcriptional response around a metabolite. We show that MIRA 's results are biologically sound, empirically significant and more reliable than RA, Results and show that MIRA captures the underlying metabolic dynamics of the switch from aerobic to anaerobic respiration Results indicate that MIRA reports metabolites that highly overlap with recently found metabolic biomarkers in the autism literature. In this article we propose a mutual information-based multivariate reporter algorithm MIRA ) MIRA is a multivariate and combinatorial algorithm using mutual information. We apply MIRA to gene expression analysis of six knockout strains of Escherichia coli We also apply MIRA to an Autism Spectrum Disorder gene expression dataset.
2K_test_1937	Overall we hope that the work helps to advance concurrent programming in modern programming environments. The aim of AEMINIUM is to study the implications of having a concurrent-by-default programming language, This includes language design, runtime system performance and software engineering considerations. The AEMINIUM implementation and all case studies are publicly available under the General Public License, to demonstrate that AEMINIUM parallelized code has performance improvements compared to its sequential counterpart, to show that AEMINIUM is powerful enough to encode them AEMINIUM can achieve a 70\ % performance improvement over the sequential counterpart Our evaluation demonstrates that AEMINIUM can be used to express parallelism in such data-structures and that the performance benefits scale with the amount of annotation effort which is put into the implementation, Our experiments show that AEMINIUM is capable of extracting parallelism from functional code and achieving performance improvements up to the limits of Plaid 's inherent performance bounds. The design of the concurrent-by-default AEMINIUM programming language, AEMINIUM leverages the permission flow of object and group permissions through the program to validate the program 's correctness and to automatically infer a possible parallelization strategy via a dataflow graph, AEMINIUM supports not only fork-join parallelism but more general dataflow patterns of parallelism, In this paper we present a formal system, called mu AEMINIUM modeling the core concepts of AEMINIUM, mu AEMINIUM 's static type system is based on Featherweight Java with AEMINIUM-specific extensions, Besides checking for correctness AEMINIUM 's type system it also uses the permission flow to compute a potential parallel execution strategy for the program mu AEMINIUM 's dynamic semantics use a concurrent-by-default evaluation approach. We conduct our study through Along with the formal system we present its soundness proof, We provide a full description of the implementation along with the description of various optimization techniques we used, We implemented AEMINIUM as an extension of the Plaid programming language, which has first-class support for permissions built-in, We use various case studies to evaluate AEMINIUM 's applicability and We chose to use case studies from common domains or problems that are known to benefit from parallelization, We demonstrate through a webserver application, which evaluates AEMINIUM 's impact on latency-bound applications, that In another case study we chose to implement a dictionary function to evaluate AEMINIUM 's capabilities to express essential data structures We chose an integral computationally example to evaluate pure functional programming and computational intensive use cases.
2K_test_1938	Vehicular multi-hop protocols typically employ distance-based metrics. Which do not capture the complexity of vehicular connectivity. Where it achieved a 30\ % increase in packet delivery ratio over the benchmark GPSR protocol. In this work we present LASP, a geographic protocol that uses a more accurate spatial connectivity-based metric Spatial connectivity describes the historical probability of successfully delivering a packet from one geographic area to another, Analysis of data collected from a vehicular testbed showed that, unlike other metrics spatial connectivity indirectly captures all major factors affecting wireless connectivity, Moreover it is temporally stable, which makes it useful in estimating the quality of future co-located links, When forwarding LASP uses spatial connectivity information to pick a well-connected geographic forwarding zone, inside which multiple nodes cooperate in relaying through a distributed prioritization scheme, Compared with other techniques where the sender picks a specific next hop relay, cooperative forwarding improves resilience to losses through vehicle diversity. We evaluated LASP on a 30-node testbed.
2K_test_1939	Security requirements patterns represent reusable security practices that software engineers can apply to improve security in their system, Reusing best practices that others have employed could have a number of benefits, such as decreasing the time spent in the requirements elicitation process or improving the quality of the product by reducing product failure risk, Pattern selection can be difficult due to the diversity of applicable patterns from which an analyst has to choose. The challenge is that identifying the most appropriate pattern for a situation can be cumbersome and time-consuming, to review only relevant patterns and quickly select the most appropriate patterns for the situation, to relate patterns based on decisions made by the pattern user, to help the pattern user select the most appropriate patterns for their situation. We propose a new method that combines an inquiry-cycle based approach with the feature diagram notation Similar to patterns themselves, our approach captures expert knowledge The resulting pattern hierarchies allow users to be guided through these decisions by questions, which introduce related patterns in order, thus resulting in better requirement generation. We evaluate our approach using access control patterns in a pattern user study.
2K_test_1940	Government laws and regulations increasingly place requirements on software systems, Ideally experts trained in law will analyze and interpret legal texts to inform the software requirements process, However in small companies and development teams with short launch cycles, individuals with little or no legal training will be responsible for compliance, Two specific challenges commonly faced by non-experts are deciding if their system is covered by a law, and then deciding whether two legal requirements are similar or different. In this study we assess the ability of laypersons, technical professionals and legal experts to judge the similarity between legal coverage conditions and requirements. In so doing we discovered that legal experts achieved higher rates of consensus more frequently than technical professionals or laypersons and that all groups had slightly greater agreement when judging coverage conditions than requirements, we found that technical professionals and legal experts exhibited consistently greater agreement than that found between laypersons and legal experts, and that each group tended towards different justifications, such as laypersons and technical professionals tendency towards categorizing different coverage conditions or requirements as equivalent if they believed them to possess the same underlying intent. Measured by Fleiss ' K, When comparing judgments between groups using a consensus-based Cohen 's Kappa.
2K_test_1941	Many traditional challenges in reconstructing 3D motion, such as matching across wide baselines and handling occlusion, reduce in significance as the number of unique viewpoints increases. However to obtain this benefit, a new challenge arises : estimating precisely which cameras observe which points at each instant in time, to reconstruct the 3D motion of an event from a large number of cameras. We demonstrate that our method estimates visibility with greater accuracy, and increases tracking performance producing longer trajectories, at more locations and at higher accuracies than methods that ignore visibility or use photometric consistency alone. We present a maximum a posteriori ( MAP ) estimate of the time-varying visibility of the target points Our algorithm takes, as input camera poses and image sequences, and outputs the time-varying set of the cameras in which a target patch is visibile and its reconstructed trajectory, We model visibility estimation as a MAP estimate by incorporating various cues including photometric consistency, motion consistency and geometric consistency, in conjunction with a prior that rewards consistent visibilities in proximal cameras, An optimal estimate of visibility is obtained by finding the minimum cut of a capacitated graph over cameras.
2K_test_1942	Curse of dimensionality is a practical and challenging problem in image categorization, especially in cases with a large number of classes, Multi-class classification encounters severe computational and storage problems when dealing with these large scale tasks. To effectively reduce dimensionality of parameter space without sacrificing classification accuracy, and at the same time exploit information in semantic taxonomy among categories. Further demonstrate the effectiveness of hierarchical feature hashing. In this paper we propose hierarchical feature hashing. We provide detailed theoretical analysis on our proposed hashing method Moreover, experimental results on object recognition and scene classification.
2K_test_1943	With the widespread availability of video cameras, we are facing an ever-growing enormous collection of unedited and unstructured video data, Due to lack of an automatic way to generate summaries from this large collection of consumer videos, they can be tedious and time consuming to index or search. Of generating short video summarizing the most important and interesting contents of an unedited and unstructured video. Demonstrating the effectiveness of online video highlighting. In this work we propose online video highlighting, a principled way costly both time-wise and financially for manual processing, Specifically our method learns a dictionary from given video using group sparse coding, and updates atoms in the dictionary on-the-fly, A summary video is then generated by combining segments that can not be sparsely reconstructed using the learned dictionary, The online fashion of our proposed method enables it to process arbitrarily long videos and start generating summaries before seeing the end of the video Moreover, the processing time required by our proposed method is close to the original video length, achieving quasi real-time summarization speed.
2K_test_1944	This paper poses object category detection in images as a type of 2D-to-3D alignment problem, to establish part-based correspondences between 3D CAD models and real photographs. We demonstrate the ability of our system to align 3D models with 2D objects in the challenging PASCAL VOC images, which depict a wide variety of chairs in complex scenes. We propose an exemplar-based 3D category representation, which can explicitly model chairs of different styles as well as the large variation in viewpoint, We develop an approach This is achieved by ( i ) representing each 3D model using a set of view-dependent mid-level visual elements learned from synthesized views in a discriminative fashion, ( ii ) carefully calibrating the individual element detectors on a common dataset of negative images, and ( iii ) matching visual elements to the test image allowing for small mutual deformations but preserving the viewpoint and style constraints. Utilizing the large quantities of 3D CAD models that have been made publicly available online, Using the `` chair { '' } class as a running example.
2K_test_1945	The storyline graphs can be an effective summary that visualizes various branching narrative structure of events or activities recurring across the input photo sets of a topic class. For reconstructing storyline graphs In order to explore further the usefulness of the storyline graphs. We show that the proposed algorithm improves other candidate methods for both storyline reconstruction and image prediction tasks. In this paper we investigate an approach from large-scale collections of Internet images, and optionally other side information such as friendship graphs, we leverage them to perform the image sequential prediction tasks, from which photo recommendation applications can benefit, We formulate the storyline reconstruction problem as an inference of sparse time-varying directed graphs, and develop an optimization algorithm that successfully addresses a number of key challenges of Web-scale problems, including global optimality linear complexity.
2K_test_1946	In this paper we address the problem of jointly summarizing large sets of Flickr images and YouTube videos. We demonstrate that the proposed joint summarization approach outperforms other baselines and our own methods using videos or images only. Starting from the intuition that the characteristics of the two media types are different yet complementary, we develop a fast and easily-parallelizable approach for creating not only high-quality video summaries but also novel structural summaries of online images as storyline graphs The storyline graphs can illustrate various events or activities associated with the topic in a form of a branching network, The video summarization is achieved by diversity ranking on the similarity graphs between images and video frames, The reconstruction of storyline graphs is formulated as the inference of sparse time-varying directed graphs from a set of photo streams with assistance of videos.
2K_test_1948	Web services offer a more reliable and efficient way to access online data than scraping web pages. However web service data are often in complex hierarchical structures that make it difficult for people to extract the desired parts or to perform any further data manipulation without writing a significant amount of surprisingly intricate code, to support working with data returned from web services. To demonstrate our tool 's ability to create fast and reusable data extraction and manipulation programs that work with complex web service data. In this paper we present Gneiss, a tool that extends the familiar spreadsheet metaphor Gneiss allows users to extract the desired fields in web service data using drag-and-drop, and refine the results through spreadsheet formulas, along with sorting and filtering the data, Hierarchical data are stored as nested tables in the spreadsheet and can be flattened for future operations, Data flow is two-way between the spreadsheet and the web services, enabling people to easily make a new request by modifying spreadsheet cells, In addition using the dependency between spreadsheet cells, our tool is able to create parallel-running data extractions based on the user 's sequential demonstration. We use a set of examples.
2K_test_1949	Research shows that commonly accepted security requirements are not generally applied in practice Instead of relying on requirements checklists, security experts rely on their expertise and background knowledge to identify security vulnerabilities. To understand the gap between available checklists and practice, to evaluate their security requirements methods against how experts transition through different situation awareness levels in their decision-making process. We report our preliminary results of analyzing two interviews that reveal possible decision-making patterns that could characterize how analysts perceive, comprehend and project future threats which leads them to decide upon requirements and their specifications, in addition to how experts use assumptions to overcome ambiguity in specifications. Our goal is to build a model that researchers can use. We conducted a series of interviews to encode the decision-making process of security experts and novices during security requirements analysis, Participants were asked to analyze two types of artifacts : source code, and network diagrams for vulnerabilities and to apply a requirements checklist to mitigate some of those vulnerabilities, We framed our study using Situation Awareness-a cognitive theory from psychology-to elicit responses that we later analyzed using coding theory and grounded analysis.
2K_test_1950	The use of shared mutable state, commonly seen in object-oriented systems, is often problematic due to the potential conflicting interactions between aliases to the same state. That enables controlled aliasing of shared resources. We present a substructural type system outfitted with a novel lightweight interference control mechanism, rely-guarantee protocols By assigning each alias separate roles, encoded in a novel protocol abstraction in the spirit of rely-guarantee reasoning, our type system ensures that challenging uses of shared state will never interfere in an unsafe fashion, In particular rely-guarantee protocols ensure that each alias will never observe an unexpected value, or type when inspecting shared memory regardless of how the changes to that shared state ( originating from potentially unknown program contexts ) are interleaved at run-time.
2K_test_1951	Formal verification and validation play a crucial role in making cyberphysical systems ( CPS ) safe Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained, including models of the controller and of the physical dynamics. In CPS models are essential ; but any model we could possibly build necessarily deviates from the real world, If the real system fits to the model, its behavior is guaranteed to satisfy the correctness properties verified w, Otherwise all bets are off, ensuring that verification results about models apply to CPS implementations, to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic. This paper introduces ModelPlex, a method ModelPlex provides correctness guarantees for CPS executions at runtime : it combines offline verification of CPS models with runtime validation of system executions for compliance with the model, ModelPlex ensures that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model, assuming the system dynamics deviation is bounded If, at some point the observed behavior no longer complies with the model so that offline verification results no longer apply, ModelPlex initiates provably safe fallback actions This paper, furthermore develops a systematic technique.
2K_test_1952	That can search physical objects using a mobile or wearable device, to recognize the `` object of interest { '' } at high accuracy and at interactive speeds We present the system architecture, the probabilistic model that integrates the multi-modal information, and empirical results showing the benefits of multi-modal integration. We show that this multi-modal approach achieves superior recognition accuracy compared to using a vision system alone, especially in cluttered scenes where a vision system would be unable to distinguish which object is of interest to the user without additional input. We present Marvin a system It integrates HOG-based object recognition, SURF-based localization information automatic speech recognition, and user feedback information with a probabilistic model Once the object of interest is recognized, the information that the user is querying, is displayed on the user 's mobile or wearable device, It is computationally able to scale to large numbers of objects by focusing compute-intensive resources on the objects most likely to be of interest, inferred from user speech and implicit localization information. We tested this prototype in a real-world retail store during business hours, with varied degree of background noise and clutter.
2K_test_1953	Due to the diversity in appearance in real world objects, an object detector must capture variations in scale, The current approaches do this by using mixtures of models, where each mixture is designed to capture one ( or a few ) axis of variation, Current methods usually rely on heuristics to capture these variations ; however, it is unclear which axes of variation exist and are relevant to a particular task, Another issue is the requirement of a large set of training images to capture such variations, Current methods do not scale to large training sets either because of training time complexity I I or test time complexity I I. We consider the problem of discovering discriminative exemplars suitable for object detection, In this work we explore the idea of compactly capturing task-appropriate variation from the data itself for object detection. We propose a two stage data-driven process, which selects and learns a compact set of exemplar models These selected models have an inherent ranking, which can be used for anytime/budgeted detection scenarios, Another benefit of our approach ( beyond the computational speedup ) is that the selected set of exemplar models performs better than the entire set.
2K_test_1954	When people observe and interact with physical spaces, they are able to associate functionality to regions in the environment Additionally, we offer a preliminary glance of the applicability of Action Maps. Our goal is to automate dense functional understanding of large spaces enables functionality estimation in large scenes where people have behaved, as well as novel scenes where no behaviors are observed. We demonstrate that by capturing appearance-based attributes of the environment and associating these attributes with activity demonstrations, our proposed mathematical framework allows for the prediction of Action Maps in new environments. By leveraging sparse activity demonstrations recorded from an ego-centric viewpoint, The method we describe Our method learns and predicts `` Action Maps { '' }, which encode the ability for a user to perform activities at various locations, With the usage of an egocentric camera to observe human activities, our method scales with the size of the scene without the need for mounting multiple static surveillance cameras and is well-suited to the task of observing activities up-close. By demonstrating a proof-of-concept application in which they are used in concert with activity detections to perform localization.
2K_test_1955	Growing traffic volumes and the increasing complexity of attacks pose a constant scaling challenge for network intrusion prevention systems ( NIPS ), In this respect offloading NIPS processing to compute clusters offers an immediately deployable alternative to expensive hardware upgrades. Our evaluations show that SNIPS can reduce the maximum load by up to 10x while only increasing the latency by 2\ %. We present the SNIPS system We design a formal optimization framework that captures tradeoffs across scalability, network load and latency. We provide a practical implementation using recent advances in software-defined networking without requiring modifications to NIPS hardware.
2K_test_1956	This is a difficult problem due to the drastic change in perspective between the ground and aerial imagery and the lack of environmental features for image comparison, We do not rely on GPS, which may be jammed or uncertain. This paper studies the problem of matching images captured from an unmanned ground vehicle ( UGV ) to those from a satellite or high-flying vehicle, We focus on situations where the UGV navigates in remote areas with few man-made structures. The results show that vision-based UGV localization from satellite maps is not only possible, but often provides better position estimates than GPS estimates, enabling us to improve the location estimates of Google Street View. We analyze the performance of a variety of descriptors for different satellite map sizes and various terrain and environment.
2K_test_1957	State lattice-based planning has been used in navigation for ground, water aerial and space robots, State lattices are typically constructed of simple motion primitives connecting one state to another, For example if the robot has a camera it may be able to use simple visual servoing techniques to navigate through a GPS-denied region Likewise, a LIDAR may allow the robot to skirt along an environmental feature even if there is not enough information to generate an accurate pose estimate. There are situations where these metric motions may not be available, such as in GPS-denied areas, In many of these cases, however the robot may have some additional sensing capability that is not being fully utilized by the planner. Showing the practical application of this approach. In this paper we present an expansion of the state lattice framework that allows us to incorporate controller-based motion primitives and external perceptual triggers directly into the planning process, We provide a formal description of our method of constructing the search graph in these cases. As well as presenting real-world and simulated testing data.
2K_test_1958	A statically typed programming language typechecks the term assigns dynamic meaning to the term.
2K_test_1959	The field of object detection has made significant advances riding on the wave of region-based ConvNets. But their training procedure still includes many heuristics and hyperparameters that are costly to tune, for training region-based ConvNet detectors. We present a simple yet surprisingly effective online hard example mining ( OHEM ) algorithm Our motivation is the same as it has always been detection datasets contain an overwhelming number of easy examples and a small number of hard examples, Automatic selection of these hard examples can make training more effective and efficient, OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use.
2K_test_1960	For aligning coordinate frames in multi-robot systems. We also present preliminary results. In this paper we present both centralized and distributed algorithms based on inter-robot relative position measurements Robot orientations are not measured, but are computed by our algorithms, Our algorithms are robust to measurement error and are useful in applications where a group of robots need to establish a common coordinate frame based on relative sensing information, The problem of establishing a common coordinate frame is formulated in a least squares error framework minimizing the total inconsistency of the measurements We assume that robots that can sense each other can also communicate with each other, In this paper our key contribution is a novel asynchronous distributed algorithm for multi-robot coordinate frame alignment that does not make any assumptions about the sensor noise model After minimizing the least squares error ( LSE ) objective for coordinate frame alignment of two robots, we develop a novel algorithm that outperforms state-of-the-art centralized optimization algorithms for minimizing the LSE objective, Furthermore we prove that for multi-robot systems ( a ) with redundant noiseless relative sensing information, we will achieve the globally optimal solution ( this is non-trivial because the LSE objective is nonconvex for our problem ), ( b ) with noisy information but no redundant sensing ( e, sensing graph has a tree topology ), our algorithm will optimally minimize the LSE objective. Of the real-world performance of our algorithm on TurtleBots equipped with Kinect sensors.
2K_test_1961	When one uses a hand-held tool, the fingers often make the tool to be in contact with the palm in the form of multi-contact manipulation, Multi-contact manipulation is useful for object-environment interaction tasks because it can provide both powerful grasping of the object body and dexterous manipulation of the object end-effector. However dealing with the internal link contact with the object is not trivial so that the desired finger force can be reduced. In this paper we propose Reactional Internal Contact Hypothesis that regards the internal contact force as a reaction force By taking a handwriting task as an example, optimal configuration search and grasping force computation problems are addressed based on this hypothesis and. Validated via dynamic simulation.
2K_test_1962	This paper presents a technique for automated intraocular laser surgery. The average error and execution time are reduced by 63, 6\ % and 28, 5\ % respectively compared to the unaided trials Finally, the automated laser photocoagulation was demonstrated also in an eye phantom, including compensation for the eye movement. We compared the performance of the automated scanning using various control thresholds, in order to find the most effective threshold in terms of accuracy and speed Given the selected threshold, we conducted the handheld operation above a fixed target surface.
2K_test_1963	Transfemoral amputees often suffer from falls and a fear of falling that leads to a decreased quality of life, Existing control strategies for powered knee-ankle prostheses demonstrate only limited ability to react to disturbances that induce falls such as trips, In contrast prior work on neuromuscular modeling of human locomotion suggests that control strategies based on local reflexes exhibit robustness to unobserved terrain such as slopes and steps These results suggest that applying the proposed control to a powered knee-ankle prosthesis will substantially improve amputee gait stability. Therefore we propose that a powered knee-ankle prosthesis governed by reflexive local controls will more competently adapt to unperceived disturbances, To test this hypothesis. We show that the proposed control allows the amputee to walk farther over rough ground than does the state-of-the-art control, The proposed controller also more readily rejects deviations from nominal walking gaits such as those encountered during a trip. We simulate a neuromuscular model of a transfemoral amputee walking over rough ground with a powered knee-ankle prosthesis governed by the proposed reflexive controller.
2K_test_1964	Control design of running robots is often based on mapping the behavior of lower order models onto the robotic systems, and the robustness of running is largely determined by the robustness of these underlying models, However existing implementations do not take full advantage of the stability that the low order models can provide, In particular analysis of the theoretical spring mass model suggests leg placement policies that generate near deadbeat rejection of large, unobserved changes in ground height. Here we show in simulation that this blind robustness to rough terrain can be carried over to bipedal robots. And show that resulting system rejects ground disturbances of up to 25\ % leg length, adapts to persistent ground slopes, and tolerates sensor noise, signal delays and modeling errors, The results indicate that transferring control derived within the spring mass model is an effective technique for realizing highly robust running in robotic systems. We design a control that stably embeds the spring mass model 's behavior in a planar robot model.
2K_test_1965	Current GPS-based devices have difficulty localizing in cases where the GPS signal is unavailable or insufficiently accurate. For localizing a vehicle on an arbitrary road network. And show that it is effective when imagery is sparsely available. This paper presents an algorithm using vision, road curvature estimates or a combination of both The method uses an extension of topometric localization, which is a hybrid between topological and metric localization, The extension enables localization on a network of roads rather than just a single, The algorithm which does not rely on GPS, is able to localize reliably in situations where GPS-based devices fail, including `` urban canyons { '' } in downtown areas and along ambiguous routes with parallel roads. We demonstrate the algorithm experimentally on several road networks in urban, suburban and highway scenarios, We also evaluate the road curvature descriptor.
2K_test_1966	Active illumination based methods have a trade-off between acquisition time and resolution of the estimated 3D shapes, Multi-shot approaches can generate dense reconstructions but require stationary scenes. In contrast singleshot methods are applicable to dynamic objects but can only estimate sparse reconstructions and are sensitive to surface texture, to produce dense reconstructions of highly textured objects. We validate the approach. In this work we develop a single-shot approach The key to our approach is an image decomposition scheme that can recover the illumination and the texture images from their mixed appearance, Despite the complex appearances of the illuminated textured regions, our method can accurately compute per pixel warps from the illumination pattern and the texture template to the observed image The texture template is obtained by interleaving the projection sequence with an all-white pattern Our estimated warping functions are reliable even with infrequent interleaved projection, Thus we obtain detailed shape reconstruction and dense motion tracking of the textured surfaces. On synthetic and real data containing subtle non-rigid surface deformations.
2K_test_1967	In product design designers often generate a large number of concepts in the form of sketches and drawings to develop and communicate their ideas Concrete concepts typically evolve through a progressive refinement of initially coarse and ambiguous ideas. However a lack of suitable means to visualize the emerging form at these early stages forces the designer to constantly maintain and negotiate an elusive mental image, To assist this process that allows early incomplete 2D sketches to be transformed into suggestive complete models. We demonstrate and discuss preliminary results of our technique on 2D shape design problems. We describe a predictive modeling technique This helps designers take a sneak peek at the potential end result of a developing concept, without forcing them to commit to the suggestion.
2K_test_1968	In this paper we investigate 3D attributes as a means to understand the shape of an object in a single image. ( ii ) we show that such properties can be successfully inferred from a single image ( iv ) we show that the 3D attributes trained on this dataset generalize to images of other ( non-sculpture ) object classes ; and furthermore ( v ) we show that the CNN also provides a shape embedding that can be used to match previously unseen sculptures largely independent of viewpoint.
2K_test_1969	Active intracellular cargo transport is essential to survival and function of eukaryotic cells, The models and related computational analysis methods developed in this study are general and can be used for studying molecular machinery and spatiotemporal dynamics of other cellular processes. How this process is controlled spatially and temporally so that the right cargo is delivered to the right destination at the right time remains poorly understood, To address this question, it is essential to characterize and analyze the molecular machinery and spatiotemporal behavior of intracellular transport, Specifically to study the molecular machinery of intracellular transport, To study the spatiotemporal behavior of intracellular transport. We validated and benchmarked the image models. To this end we developed related computational image models, we developed anisotropic spatial density kernels for reconstruction and segmentation of related super-resolution STORM ( stochastic optical reconstruction microscopy ) images, we developed hidden Markov models and principal component analysis for representation and analysis of movement of individual transported cargoes. Using simulated and actual experimental images.
2K_test_1970	In addition the topologies predicted by the proposed method can be used as effective initial conditions for conventional topology optimization routines, resulting in substantial performance gains, We discuss the advantages and limitations of the presented approach and show its performance on a number of examples. We explore the feasibility and performance of a to topology optimization problems involving structural mechanics. The results indicate that when there is an underlying structure in the set of existing solutions, the proposed method can successfully predict the optimal topologies in novel loading configurations. Data-driven approach Our approach takes as input a set of images representing optimal 2-D topologies, each resulting from a random loading configuration applied to a common boundary support condition, These images represented in a high dimensional feature space are projected into a lower dimensional space using component analysis, Using the resulting components, a mapping between the loading configurations and the optimal topologies is learned. From this mapping we estimate the optimal topologies for novel loading configurations.
2K_test_1971	An increasing number of mobile devices are capable of automatically sensing and recording rich information about the surrounding environment, Spatial locations of such data can help to better learn about the environment. In this work we address the problem of identifying the locations visited by a mobile device as it moves within an indoor environment. We will show robustness the ability to propose coarse sensor noise errors. We focus on devices equipped with odometry sensors that capture changes in motion, Odometry suffers from cumulative errors of dead reckoning but it captures the relative shape of the traversed path well, Our approach will correct such errors by matching the shape of the trajectory from odometry to traversable paths of a known map, Our algorithm is inspired by prior vehicular GPS map matching techniques that snap global GPS measurements to known roads, We similarly wish to snap the trajectory from odometry to known hallways, Several modifications are required to ensure these techniques are robust when given relative measurements from odometry, If we assume an office-like environment with only straight hallways, then a significant rotation indicates a transition to another hallway, As a result we partition the trajectory into line segments based on significant turns, Each trajectory segment is snapped to a corresponding hallway that best maintains the shape of the original trajectory, These snapping decisions are made based on the similarity of the two curves as well as the rotation to transition between hallways. Under different types of noise in complex environments and.
2K_test_1972	That can greatly improve the level of intelligence and driving quality of autonomous vehicles. The proposed behavioral planning architecture improves the driving quality considerably, 3\ % reduction of required computation time in representative scenarios. In this paper we propose a novel planning framework A reference planning layer first generates kinematically and dynamically feasible paths assuming no obstacles on the road, then a behavioral planning layer takes static and dynamic obstacles into account, Instead of directly commanding a desired trajectory, it searches for the best directives for the controller, such as lateral bias and distance keeping aggressiveness, It also considers the social cooperation between the autonomous vehicle and surrounding cars. Based on experimental results from both simulation and a real autonomous vehicle platform.
2K_test_1973	Localization is a central problem for intelligent vehicles, Visual localization can supplement or replace GPS-based localization approaches in situations where GPS is unavailable or inaccurate Although visual localization has been demonstrated in a variety of algorithms and systems, the problem of how to best configure such a system remains largely an open question, Design choices such as `` where should the camera be placed ? { '' } and `` how should it be oriented ? { '' } can have substantial effect on the cost and robustness of a fielded intelligent vehicle. This paper analyzes how different sensor configuration parameters and environmental conditions affect visual localization performance with the goal of understanding what causes certain configurations to perform better than others and providing general principles for configuring systems for visual localization. Of a visual localization algorithm. We ground the investigation using extensive field testing, and the data sets used for the analysis are made available for comparative evaluation.
2K_test_1974	Our algorithms construct dual solutions using a regret-minimizing online learning algorithm in a black-box fashion, and use them to construct primal solutions, The adversarial guarantee that holds for the constructed duals help us to take care of most of the correlations that arise in the algorithm ; the remaining correlations are handled via martingale concentration and maximal inequalities, These ideas lead to conceptually simple and modular algorithms.
2K_test_1975	Illumination defocus and global illumination effects are major challenges for active illumination scene recovery algorithms. Illumination defocus limits the working volume of projector-camera systems and global illumination can induce large errors in shape estimates. We demonstrate the effectiveness of our approach. In this paper we develop an algorithm for scene recovery in the presence of both defocus and global light transport effects such as interreflections and sub-surface scattering, Our method extends the working volume by using structured light patterns at multiple projector focus settings, A careful characterization of projector blur allows us to decode even partially out-of-focus patterns This enables our algorithm to recover scene shape and the direct and global illumination components over a large depth of field while still using a relatively small number of images ( typically 25-30 ). By recovering high quality depth maps of scenes containing objects made of optically challenging materials such as wax, marble soap colored glass and translucent plastic.
2K_test_1976	Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D point triangulation to reconstruct a static scene. The triangulation constraint however is invalid for moving points captured in multiple unsynchronized videos and bundle adjustment is not purposed to estimate the temporal alignment between cameras that jointly optimizes four coupled sub-problems : estimating camera intrinsics and extrinsics, triangulating 3D static points, as well as subframe temporal alignment between cameras and estimating 3D trajectories of dynamic points. Because the videos are aligned with sub-frame precision, we reconstruct 3D trajectories of unconstrained outdoor activities at much higher temporal resolution than the input videos. In this paper we present a spatiotemporal bundle adjustment approach Key to our joint optimization is the careful integration of physics-based motion priors within the reconstruction pipeline, validated on a large motion capture corpus, We present an end-to-end pipeline that takes multiple uncalibrated and unsynchronized video streams and produces a dynamic reconstruction of the event.
2K_test_1977	Turbulence is studied extensively in remote sensing, astronomy meteorology aerodynamics and fluid dynamics, The strength of turbulence is a statistical measure of local variations in the turbulent medium, It influences engineering decisions made in these domains, Turbulence strength ( TS ) also affects safety of aircraft and tethered balloons, and reliability of free-space electromagnetic relays. We show that it is possible to estimate TS, without having to reconstruct instantaneous fluid flow fields, Instead the TS field can be directly recovered. Using videos captured from different viewpoints, We formulate this as a linear tomography problem with a structure unique to turbulence fields, No tight synchronization between cameras is needed Thus, realization is very simple to deploy using consumer-grade cameras. We experimentally demonstrate this both in a lab and in a large-scale uncontrolled complex outdoor environment, which includes industrial rural and urban areas.
2K_test_1978	In many behavioral domains, such as facial expression and gesture, sparse structure is prevalent. This sparsity would be well suited for event detection but for one problem, Features typically are confounded by alignment error in space and time, As a consequence high-dimensional representations such as SIFT and Gabor features have been favored despite their much greater computational cost and potential loss of information. We propose a Kernel Structured Sparsity ( KSS ) method that can handle both the temporal alignment problem and the structured sparse reconstruction within a common framework, and it can rely on simple features, We characterize spatio-temporal events as time-series of motion patterns and by utilizing time-series kernels we apply standard structured-sparse coding techniques to tackle this important problem. We evaluated the KSS method using both gesture and facial expression datasets that include spontaneous behavior and differ in degree of difficulty and type of ground truth coding, as measured by F-1 score.
2K_test_1979	The primary goal of an automotive headlight is to improve safety in low light and poor weather conditions, But despite decades of innovation on light sources, more than half of accidents occur at night even with less traffic on the road Recent developments in adaptive lighting have addressed some limitations of standard headlights. However they have limited flexibility - switching between high and low beams, turning off beams toward the opposing lane, or rotating the beam as the vehicle turns - and are not designed for all driving environments, that can sense react, and adapt quickly to any environment while moving at highway speeds. Anti-glare high beams improved driver visibility during snowstorms, increased contrast of lanes, markings and sidewalks and early visual warning of obstacles are demonstrated. This paper introduces an ultra-low latency reactive visual system Our single hardware design can be programmed to perform a variety of tasks.
2K_test_1980	To parse human motion in unconstrained Internet videos without labeling any videos for training. Experiments show that our method achieves good performance for parsing human motions Furthermore, we found that our method achieves better performance by using unlabeled video than adding more labeled pose images into the training set. In this paper we propose a method We use the training samples from a public image pose dataset to avoid the tediousness of labeling video streams There are two main problems confronted, First the distribution of images and videos are different, Second no temporal information is available in the training images, To smooth the inconsistency between the labeled images and unlabeled videos, our algorithm iteratively incorporates the pose knowledge harvested from the testing videos into the image pose detector via an adjust-and-refine method, During this process continuity and tracking constraints are imposed to leverage the spatio-temporal information only available in videos. For our experiments we have collected two datasets from YouTube and.
2K_test_1981	Forecasting human activities from visual evidence is an emerging area of research which aims to allow computational systems to make predictions about unseen human actions. We explore the task of activity forecasting in the context of dual-agent interactions to understand how the actions of one person can be used to predict the actions of another. Results show that our proposed method is able to properly model human interactions in a high dimensional space of human poses, results show that our method is able to generate highly plausible simulations of human interaction. We model dual-agent interactions as an optimal control problem, where the actions of the initiating agent induce a cost topology over the space of reactive poses - a space in which the reactive agent plans an optimal pose trajectory The technique developed in this work employs a kernel-based reinforcement learning approximation of the soft maximum value function to deal with the high-dimensional nature of human motion and applies a mean-shift procedure over a continuous cost function to infer a smooth reaction sequence. Experimental When compared to several baseline models.
2K_test_1982	We provide a semantics and a proof system and show its usefulness for nontrivial temporal properties of hybrid systems, We take particular care to handle the case of alternating universal dynamic and existential temporal modalities and its dual, solving an open problem formulated in previous work.
2K_test_1983	For single-view reasoning about 3D surfaces and their relationships. We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces. In this work we present a method We propose the use of midlevel constraints for 3D scene understanding in the form of convex and concave edges and introduce a generic framework capable of incorporating these and other constraints Our method takes a variety of cues and uses them to infer a consistent interpretation of the scene.
2K_test_1984	Cyber-physical systems ( CPS ), which are computerized systems directly interfacing their real-world surroundings, leverage the construction of increasingly autonomous systems, To meet the high safety demands of CPS, verification of their behavior is crucial, which has led to a wide range of tools for modeling and verification of hybrid systems, These tools are often used in combination, because they employ a wide range of different formalisms for modeling, and aim at distinct verification goals and techniques and propose future extension. To manage and exchange knowledge in the verification process and to overcome a lack of a common classification. Furthermore we illustrate how the CRM can support comparing models. We unify different terminologies and concepts of a variety of modeling and verification tools in a conceptual reference model ( CRM ).
2K_test_1985	Demographic information has a rich context from which to make decisions about how to filter or individualize computer users in forensic analysis. Although current explorations into technologies such as face and fingerprint analysis have seen varying rates of success, two main problems limit their applicability in the context of computer crimes : they can be intrusive, and they can require costly equipment, Our solution is to determine users ' demographic traits. By analyzing the interactions between users and computers, From user interaction data, we extracted keystroke timing and mouse movement features, and developed weighted random forest classifiers for five demographic traits : gender, age ethnicity handedness and language. We conducted a field study that gathered users ' keystroke and mouse data during interaction with a computer.
2K_test_1986	Refactoring of code is a common device in software engineering, As cyber-physical systems ( CPS ) become ever more complex, similar engineering practices become more common in CPS development, Proper safe developments of CPS designs are accompanied by a proof of correctness. Since the inherent complexities of CPS practically mandate iterative development, frequent changes of models are standard practice, but require reverification of the resulting models after every change, To overcome this issue. For some of these we can give strong results by proving on a meta-level that they are correct for showing that the refactoring respects the refinement relation. We develop proof-aware refactorings for CPS, That is we study model transformations on CPS and show how they correspond to relations on correctness proofs, As the main technical device, we show how the impact of model transformations on correctness can be characterized by different notions of refinement in differential dynamic logic. Furthermore we demonstrate the application of refinements on a series of safety-preserving and liveness-preserving refactorings, Where this is impossible, we construct proof obligations.
2K_test_1987	These results suggest that, when carefully designed learning by teaching can support students to not only learn cognitive skills but also employ meta-cognitive skills for effective tutoring. This paper investigates the effect of meta-cognitive help in the context of learning by teaching. The data showed that students with the meta-cognitive help showed better problem selection and scored higher on the post-test than those who tutored SimStudent without the meta-cognitive help.
2K_test_1988	Recent work has shown that features such as hand appearance, object attributes local hand motion and camera ego-motion are important for characterizing first-person actions, to highlight the importance of network design decisions. We bring together ideas from recent work on feature design for egocentric action recognition under one framework by exploring the use of deep convolutional neural networks ( CNN ), To integrate these ideas under one framework. We show that our proposed architecture naturally learns features that capture object attributes and hand-object configurations, show that our deep architecture enables recognition rates that significantly outperform state-of-the-art techniques - an average 6, 6\ % increase in accuracy over all datasets, the performance of individual recognition tasks also increase by 30\ % ( actions ) and 14\ % ( objects ), We also include the results. We propose a twin stream network architecture, where one stream analyzes appearance information and the other stream analyzes motion information Our appearance stream encodes prior knowledge of the egocentric paradigm by explicitly training the network to segment hands and localize objects, Furthermore by learning to recognize objects, actions and activities jointly. By visualizing certain neuron activation of our network, Our extensive experiments on benchmark egocentric action datasets of extensive ablative analysis.
2K_test_1989	Collaborative learning has been shown to be beneficial for older students, but there has not been much research to show if these results transfer to elementary school students, In addition collaborative and individual modes of instruction may be better for acquiring different types of knowledge, Collaborative Intelligent Tutoring Systems ( ITS ) provide a platform that may be able to provide both the cognitive and collaborative support that students need This work indicates that by embedding collaboration scripts in ITSs, collaborative learning can be an effective instructional method even with young children. This paper presents a study comparing collaborative and individual methods while receiving instruction on either procedural or conceptual knowledge. The collaborative groups had the same learning gains as the individual groups in both the procedural and conceptual learning conditions but were able to do so with fewer problems.
2K_test_1990	The amount of data available to build simulation models of schools is immense, but using these data effectively is difficult. Traditional methods of computer modeling of educational systems often either lack transparency in their implementation, are complex and often do not natively simulate nonlinear systems, towards modeling and data mining. In response we advocate a Complex Adaptive Systems approach By simulating agent-level attributes rather than system-level attributes, the modeling is inherently transparent, easily adjustable and facilitates analysis of the system due to the analogous nature of the simulated agents to real-world entities. We explore the design a CAS model of schools using multiple levels of data from varied data streams.
2K_test_1991	Heterogeneity of wireless networks has become an increasing problem in the wireless spectrum that breaks down spectrum sharing and exacerbates interference. Many coexistence techniques have been proposed to alleviate this interference, however they are difficult to deploy due to changes needed in the protocols, overhead and rapid changes in technology to provide a long-term solution. In this paper we focus on the potential of spectrum management We introduce novel components to a spectrum management system that overcomes limitations of current models that have remained relatively focused on homogeneous environments, Our approach is a centralized one, where we analyze information collected from heterogeneous monitors available today, structure the information in a hypergraph, and perform an analysis to detect heterogeneous conflicts Introducing a mixed integer program ( in addition to other novel components ), we reconfigure devices in the spectrum to avoid conflicts and improve performance.
2K_test_1992	When deployed in automated speech recognition ( ASR ), deep neural networks ( DNNs ) can be treated as a complex feature extractor plus a simple linear classifier, Previous work has investigated the utility of multilingual DNNs acting as language-universal feature extractors ( LUFEs ). To further improve LUFEs, to obtain more invariant feature space. Each of the proposed techniques results in word error rate reduction compared with the existing DNN-based LUFEs, Combining the two methods together brings additional improvement on the target language. In this paper we explore different strategies First, we replace the standard sigmoid nonlinearity with the recently proposed maxout units, The resulting maxout LUFEs have the nice property of generating sparse feature representations, Second the convolutional neural network ( CNN ) architecture is applied. We evaluate the performance of LUFEs on a cross-language ASR task.
2K_test_1993	Recent studies in computer vision have shown that, while practically invisible to a human observer, skin color changes due to blood flow can be captured on face videos and, surprisingly be used to estimate the heart rate ( HR ), While considerable progress has been made in the last few years, still many issues remain open. In particular stateof- the-art approaches are not robust enough to operate in natural conditions ( e, in case of spontaneous movements, facial expressions or illumination changes ), to dynamically select face regions useful for robust HR estimation. That the proposed approach significantly outperforms state-of-the-art HR estimation methods in naturalistic conditions. Opposite to previous approaches that estimate the HR by processing all the skin pixels inside a fixed region of interest, we introduce a strategy Our approach, inspired by recent advances on matrix completion theory, allows us to predict the HR while simultaneously discover the best regions of the face to be used for estimation. Thorough experimental evaluation conducted on public benchmarks suggests.
2K_test_1994	Multilingual deep neural networks ( DNNs ) can act as deep feature extractors and have been applied successfully to cross language acoustic modeling, Learning these feature extractors becomes an expensive task, because of the enlarged multilingual training data and the sequential nature of stochastic gradient descent ( SGD ). This paper investigates strategies to accelerate the learning process over multiple GPU cards. Better acceleration but worse recognition performance. When using DistLang we observe Further evaluations are conducted to scale DistModel to more languages and GPU cards.
2K_test_1995	Solving this problem is important for ensuring correctness of the decision procedures, At the same time, it is a new approach for automated theorem proving over real numbers. We show how to generate and validate logical proofs of unsatisfiability from delta-complete decision procedures that rely on error-prone numerical algorithms. We demonstrate how proofs generated from our solver can establish many nonlinear lemmas. We design a first-order calculus, and transform the computational steps of constraint solving into logic proofs, which are then validated using proof-checking algorithms. As an application in the the formal proof of the Kepler Conjecture.
2K_test_1996	Good communication is critical to seamless human-robot interaction, Among numerous communication channels. Here we focus on gestures, and in particular on spacial deixis : pointing at objects in the environment in order to reference them, that enables robots to generate pointing configurations that make the goal object as clear as possible - pointing configurations that are legible. Showing that the resulting pointing configurations make the goal object easier to infer for novice users. We propose a mathematical model. We study the implications of legibility on pointing, that the robot will sometimes need to trade off efficiency for the sake of clarity, Finally we test how well our model works in practice in a series of user studies.
2K_test_1997	For compelling human-robot interaction, social gestures are widely believed to be important, We conclude that social gesturing of a robot enhances physical interactions between humans and robots. This paper investigates the effects of adding gestures to a physical game between a human and a humanoid robot. For half of the cases in which the catch was unsuccessful, the robot made a physical gesture, such as shrugging its shoulders, shaking its head or throwing up its hands, In the other half of cases, no gestures were produced Participants smiled more and rated the robot as more engaging, responsive and humanlike when it gestured. Human participants repeatedly threw a ball to the robot, which attempted to catch it, If the catch was successful, the robot threw the ball back to the human, We used questionnaires and smile detection to compare participants ' feelings about the robot when it made gestures after failure versus when it did not.
2K_test_1998	There is a saying that 95\ % of communication is body language, but few robot systems today make effective use of that ubiquitous channel and could be used to help design more effectively expressive mobile robots. Motion is an essential area of social communication that will enable robots and people to collaborate naturally, develop rapport and seamlessly share environments, to analyze and generate expressive motion. Results indicate that the machine analysis ( 41, 7\ % match between intended and classified manner ) achieves similar accuracy overall compared to a human benchmark ( 41, 2\ % match ), We conclude that these motion features perform well for analyzing expression in low degree of freedom systems. The proposed work presents a principled set of motion features based on the Laban Effort system, a widespread and extensively tested acting ontology for the dynamics of `` how { '' } we enact motion, The features allow us, in future work using position ( x, y ) and orientation ( theta ), We formulate representative features for each Effort and parameterize them on expressive motion sample trajectories collected from experts in robotics and theater, We then produce classifiers for different `` manners { '' } of moving and. Assess the quality of results by comparing them to the humans labeling the same set of paths on Amazon Mechanical Turk.
2K_test_1999	For service robots operating in indoor environments, the crucial task of navigation is often complicated by the presence of people, Simply treating humans in the environment as additional ( often moving ) obstacles can violate the complex set of social rules by which people navigate around each other. In contrast emulating human behavior and navigating in a socially appropriate manner could positively affect people 's comfort with a robot 's presence and motion, generating social paths for a robot to approach a person based on a small amount of human data. We found that both approaches were rated comparably when the robot approached from the participant 's front or side, but the social approach was significantly preferred when the robot came from behind the participant. We present a method of. We also conducted a study in which a robot approached participants using both these social paths and straight-line.
2K_test_2000	Human operators in today 's control centers, such as air or road traffic control, need to monitor a plethora of information obtained from diverse sources, To support them in detecting critical situations within this information flood and taking timely actions, operators thus need adequate information fusion and decision support systems, Research efforts on such dedicated Situation Awareness ( SAW ) systems have concentrated on assisting the operator in managing the current situations, which encompasses the acquisition, representation validation maintenance and reuse of knowledge gathered for and during the use of these systems, such as configuring and maintaining suitable situation templates and exploiting already assessed situations, If operators and domain experts are not supported in these tasks, however this may discourage them from a successful adoption of such systems in real-world control center applications, as user studies revealed. However little focus has been so far on integratively supporting the different phases of knowledge management in SAW systems, fostering knowledge management in SAW systems. Based on these and the lessons learned from the application of our SAW system implementations BeAware ! and CSI to the domain of road traffic control, we therefore propose a first step towards a tool suite, which stretches from the configuration phase of the system to its runtime maintenance in the light of evolving environments and user needs.
2K_test_2001	Near-Infrared ( NIR ) images of most materials exhibit less texture or albedo variations making them beneficial for vision tasks such as intrinsic image decomposition and structured light depth estimation, Understanding the reflectance properties ( BRDF ) of materials in the NIR wavelength range can be further useful for many photometric methods including shape from shading and inverse rendering. However even with less albedo variation, exhibit complex fine-scale surface detail making it hard to accurately estimate BRDF, to simultaneously estimate NIR BRDF and fine-scale surface details. To demonstrate fine-scale reconstruction of objects from a single NIR image. The NIR BRDFs measured from material samples are used with a shape-from-shading algorithm.
2K_test_2002	Which is one of the major problems in HMM-based speech synthesis. To alleviate the over-smoothing effect However, this post-filter is not applicable to various lengths of speech parameter trajectories, such as phrases or segments, which are shorter than an utterance, To address this problem, to recover the phoneme-level MS of HMM-state duration. Results show that the modified post-filters also yield significant quality improvements in synthetic speech as yielded by the conventional post-filter.
2K_test_2003	We will never really understand learning until we can build machines that learn many different things, over years and become better learners over time. Learning to read the web.
2K_test_2004	Online popularity of a user or product ( via follows, ) can be monetized on the premise of higher ad click-through rates or increased sales, Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck, Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent ( but sometimes honest ) users. How can we detect suspicious users in large online networks ? However, small-scale stealthy attacks may go unnoticed due to the nature of low-rank eigenanalysis used in practice, to find and prove claims about the weaknesses of modern, state-of-the-art spectral methods to catch small-scale, stealth attacks that slip below the radar. It is shown to be highly effective ( c ) it is scalable ( linear on the input size ), with high precision identify many suspicious accounts which have persisted without suspension even to this day. In this work we take an adversarial approach and propose FBOX, an algorithm designed Our algorithm has the following desirable properties : ( a ) it has theoretical underpinnings.
2K_test_2005	For recognizing human actions ( e, waking riding ) in videos. Demonstrate superior computational cost ( real-time ), memory efficiency and very competitive performance of our approach compared to the state of the arts. In this work we propose to employ multi-channel correlation filters In our framework, each action sequence is represented as a multi-channel signal ( frames ) and the goal is to learn a multi-channel filter for each action class that produces a set of desired outputs when correlated with training examples. The experiments on the Weizmann and UCF sport datasets.
2K_test_2006	Dynamic assignment and re-assignment of large number of simple and cheap robots across multiple sites is relevant to applications like autonomous survey, environmental monitoring and reconnaissance, This problem can be posed as an optimal control problem ( which is hard to solve optimally ), and has been studied to a limited extent in the literature when the cost objective is time. Show that our method outperforms other proposed methods in the literature for the objective of time as well as more general objectives ( like total energy consumed ). In this paper we present supervisory control laws We consider the total energy consumed as the cost objective and present a linear programming based heuristic for computing a stochastic transition law for the robots to move between sites. We consider a robotic swarm consisting of tens to hundreds of simple robots with limited battery life and limited computation and communication capabilities, The robots have the capability to recognize the site that they are in and receive messages from a central supervisory controller, but they can not communicate with other robots, There is a cost ( e, energy time ) for the robots to move from one site to another, These limitations make the swarm hard to control to achieve the desired configurations, We evaluate our method for different objectives and through Monte Carlo simulations.
2K_test_2007	The study of human control of robotic swarms involves designing interfaces and algorithms for allowing a human operator to influence a swarm of robots. One of the main difficulties, however is determining how to most effectively influence the swarm after it has been deployed, Past work has focused on influencing the swarm via statically selected leaders-swarm members that the operator directly controls. Our results show that, while there was a large drop in the number of goals reached when moving from a 1-hop to a 2-hop guarantee, the difference between a 2-hop, 3-hop and 4-hop guarantee was not statistically significant, Furthermore we found that sensing error impacted the explicit information-propagation method more than the tacit method conditions, and caused participants more trouble the lower the density of leaders, although the explicit method performed better overall. This paper investigates the use of a small subset of the swarm as leaders that are dynamically selected during the scenario execution and are directly controlled by the human operator to guide the rest of the swarm, which is operating under a flocking-style algorithm, The goal of the operator in this study is to move the swarm to goal regions that arise dynamically in the environment. We experimentally investigated three different aspects of dynamic leader-based swarm control and their interactions : leader density ( in terms of guaranteed hops to a leader ), sensing error and method of information propagation from leaders to the rest of the swarm.
2K_test_2008	Our work will impact several first-person vision tasks that need the detailed understanding of social interactions, such as automatic video summarization of group events and assistive systems. We aim to understand the dynamics of social interactions between two people by recognizing their actions and reactions using a head-mounted camera, To recognize micro-level actions and reactions, such as slight shifts in attention, subtle nodding or small hand actions, where only subtle body motion is apparent to enable systematic evaluations on the task of micro-action and reaction recognition. We show that the first-person and second-person points-of-view features of two people, enabled by paired egocentric videos, are complementary and essential for reliably recognizing micro-actions and reactions.
2K_test_2009	Speaker adaptive training ( SAT ) is a well studied technique for Gaussian mixture acoustic models ( GMMs ), Recently we proposed to perform SAT for deep neural networks ( DNNs ), with speaker i-vectors applied in feature learning, The resulting SAT-DNN models significantly outperform DNNs on word error rates ( WERs ). To further improve and extend SAT-DNN, to investigate i-vector extractor training and flexible feature fusion, to improve tasks including bottleneck feature ( BNF ) generation, convolutional neural network ( CNN ) acoustic modeling and multilingual DNN-based feature extraction, for transcribing multimedia data. In this paper we present different methods First, we conduct detailed analysis Second, the SAT-DNN approach is extended Third, we enrich the i-vector representation with global speaker attributes ( age, ) obtained automatically from video signals, On a collection of instructional videos, incorporation of the additional visual features is observed to boost the recognition accuracy of SAT-DNN.
2K_test_2010	For gated recurrent neural networks that can be used in single-pass applications. Highlight the behavior and efficacy of such networks show that these networks, while simple are still more accurate. In this paper we introduce a simplified architecture where word-spotting needs to be done in real-time and phoneme-level information is not available for training The network operates as a self-contained block in a strictly forward-pass configuration to directly generate keyword labels, We call these simple networks causal networks, where the current output is only weighted by the the past inputs and outputs, Since the basic network has a simpler architecture as compared to traditional memory networks used in keyword spotting, it also requires less data to train. Experiments on a standard speech database Comparisons with a standard HMM-based keyword spotter.
2K_test_2011	Spoken language interfaces are being incorporated into various devices ( e, smart-phones smart TVs etc ). However current technology typically limits conversational interactions to a few narrow predefined domains/topics, For example dialogue systems for smart-phone operation fail to respond when users ask for functions not supported by currently installed applications. We propose to dynamically add application-based domains according to users ' requests by using descriptions of applications as a retrieval cue to find relevant applications The approach uses structured knowledge resources ( e, Freebase Wikipedia FrameNet ) to induce types of slots for generating semantic seeds, and enriches the semantics of spoken queries with neural word embeddings, where semantically related concepts can be additionally included for acquiring knowledge that does not exist in the predefined domains, The system can then retrieve relevant applications or dynamically suggest users install applications that support unexplored domains, We find that vendor descriptions provide a reliable source of information for this purpose.
2K_test_2012	For large-scale testing of commodity off-the-shelf ( COTS ) software, to amplify the effect of dynamic symbolic execution. We present MergePoint a new binary-only symbolic execution system MergePoint introduces veritesting, a new technique that employs static symbolic execution Veritesting allows MergePoint to find twice as many bugs, explore orders of magnitude more paths, and achieve higher code coverage than previous dynamic symbolic execution systems.
2K_test_2013	What defines an action like `` kicking ball { '' } ?. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51, More importantly our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset. In this paper we propose by modeling an action as a transformation which changes the state of the environment before the action happens ( precondition ) to the state after the action ( effect ), Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space.
2K_test_2014	Product architecture structures the coordination problem that the development organization must solve, The modularity strategy establishes design rules that fix module functionality and interfaces, and assigns development work for each module to a single team, The modules present relatively independent coordination problems that teams attempt to solve with all the traditional coordination mechanisms available to them, The applicability and effectiveness of this strategy is limited with increasing technical and organizational volatility. In the absence of theory explaining why and when modularity works, the technique is brittle, with very little firm basis for adjustment or for complementing it with other strategies, that generalizes the modularity strategy, and explore how this theoretical view can drive coordination research and provide a theoretical basis for practical techniques to assist architects. I present a theory of coordination, based on decision networks. I review evidence testing several hypotheses derived from the theory.
2K_test_2015	One of the pillars of the modern scientific method is model validation : comparing a scientific model 's predictions against empirical observations, Today a scientist demonstrates the validity of a model by making an argument in a paper and submitting it for peer review, a process comparable to code review in software engineering, While human review helps to ensure that contributions meet high-level goals, software engineers typically supplement it with unit testing to get a more complete picture of the status of a project, Scientific communities differ from software communities in several key ways. We argue that a similar test-driven methodology would be valuable to scientific communities as they seek to validate increasingly complex models against growing repositories of empirical data, and outline how supported by new and existing collaborative infrastructure, it could integrate into the modern scientific process. In this paper we introduce Sci Unit, a framework for test-driven scientific model validation.
2K_test_2016	Although different approaches to decision-making in self adaptive systems have shown their effectiveness in the past by factoring in predictions about the system and its environment ( e, resource availability ) no proposal considers the latency associated with the execution of tactics upon the target system, However different adaptation tactics can take different amounts of time until their effects can be observed, In reactive adaptation ignoring adaptation tactic latency can lead to suboptimal adaptation decisions ( e, activating a server that takes more time to boot than the transient spike in traffic that triggered its activation ), In proactive adaptation taking adaptation latency into account is necessary to get the system into the desired state to deal with an upcoming situation. That enables us to quantify the potential benefits of employing different types of algorithms for self-adaptation, to show the potential benefit of considering adaptation tactic latency in proactive adaptation algorithms, to do proactive adaptation. Our results show that factoring in tactic latency in decision making improves the outcome of adaptation and show that it achieves higher utility than an algorithm that under the assumption of no latency is optimal. In this paper we introduce a formal analysis technique based on model checking of stochastic multiplayer games ( SMGs ) In particular, we apply this technique We also present an algorithm that considers tactic latency.
2K_test_2017	Mobile devices have become powerful ultra-portable personal computers supporting not only communication but also running a variety of complex, interactive applications Because of the unique characteristics of mobile interaction Our findings underline the need for a more nuanced set of interactions that support short mobile device uses, in particular review sessions. A better understanding of the time duration and context of mobile device uses could help to improve and streamline the user experience, In this paper we first explore the anatomy of mobile device use We then focus our investigation on short review interactions and identify opportunities for streamlining these mobile device uses for proactively presenting tasks to the users. And propose a classification of use based on duration and interaction type : glance, through proactively suggesting short tasks to the user that go beyond simple application notifications, We use the findings from our study to create and explore the design space. We evaluate the concept through a user evaluation of an interactive lock screen prototype.
2K_test_2018	The growing size of modern storage systems is expected to exceed billions of objects, making metadata scalability critical to overall performance. Many existing distributed file systems only focus on providing highly parallel fast access to file data, and lack a scalable metadata service, for scalable high-performance operations on metadata and small files, for creation intensive workloads for hot spot mitigation. In this paper we introduce a middleware design called IndexFS that adds support to existing file systems such as PVFS, Lustre and HDFS IndexFS uses a table-based architecture that incrementally partitions the namespace on a per-directory basis, preserving server and disk locality for small directories, An optimized log-structured layout is used to store metadata and small files efficiently We also propose two client-based storm-free caching techniques : bulk namespace insertion such as N-N checkpointing ; and stateless consistent metadata caching. By combining these techniques.
2K_test_2019	However this result is existential and has thus attracted a great deal of subsequent research on explicit constructions of non-malleable codes against natural classes of adversaries. Constructions of coding schemes against two well-studied classes of tampering functions ; namely, bit-wise tampering functions ( where the adversary tampers each bit of the encoding independently ) and the much more general class of split-state adversaries ( where two independent adversaries arbitrarily tamper each half of the encoded sequence ).
2K_test_2020	Recent advances in rendering and data-driven animation have enabled the creation of compelling characters with impressive levels of realism, A better understanding of the factors that make human motion recognizable and appealing would be of great value in industries where creating a variety of appealing virtual characters with realistic motion is required Average faces are perceived to be less distinctive but more attractive. We found that dancing motions were most easily recognized and that distinctiveness in one gait does not predict how recognizable the same actor is when performing a different motion, As hypothesized average motions were always amongst the least distinctive and most attractive, Furthermore as 50\ % of participants in the experiment were Caucasian European and 50\ % were Asian Korean, we found that the latter were as good as or better at recognizing the motions of the Caucasian actors than their European counterparts, in particular for dancing males, whom they also rated more highly for attractiveness. We captured thirty actors walking, jogging and dancing and applied their motions to the same virtual character ( one each for the males and females ), We then conducted a series of perceptual experiments.
2K_test_2021	Transport protocols must accommodate diverse application and network requirements, As a result TCP has evolved over time with new congestion control algorithms such as support for generalized AIMD, background flows and multipath, On the other hand, explicit congestion control algorithms have been shown to be more efficient, However they are inherently more rigid because they rely on in-network components. Therefore it is not clear whether they can be made flexible enough to support diverse application requirements for network resource allocation that accommodates diversity. We show that FCP allows evolution by accommodating diversity and ensuring coexistence, while being as efficient as existing explicit congestion control algorithms. This paper presents a flexible framework called FCP by exposing a simple abstraction for resource allocation, FCP incorporates novel primitives for end-point flexibility ( aggregation and preloading ) into a single framework and makes economics-based congestion control practical by explicitly handling load variations and by decoupling it from actual billing.
2K_test_2022	Phase-contrast microscopy is one of the most common and convenient imaging modalities to observe long-term multi-cellular processes, which generates images by the interference of lights passing through transparent specimens and background medium with different retarded phases. Despite many years of study, computer-aided phase contrast microscopy analysis on cell behavior is challenged by image qualities and artifacts caused by phase contrast optics, Addressing the unsolved challenges. Demonstrate that the proposed approach produces quality segmentation of individual cells and outperforms previous approaches.
2K_test_2023	Motivation : Several types of studies, including genome-wide association studies and RNA interference screens, strive to link genes to diseases Although these approaches have had some success, genetic variants are often only present in a small subset of the population, and screens are noisy with low overlap between experiments in different labs, Neither provides a mechanistic model explaining how identified genes impact the disease of interest or the dynamics of the pathways those genes regulate, Such mechanistic models could be used to accurately predict downstream effects of knocking down pathway members and allow comprehensive exploration of the effects of targeting pairs or higher-order combinations of genes. To model the activation of signaling and dynamic regulatory networks involved in disease progression. Results The resulting networks correctly identified many of the known pathways and transcriptional regulators of this disease, Furthermore they accurately predict RNA interference effects and can be used to infer genetic interactions, greatly improving over other methods suggested for this task, allowed us to identify several strain-specific targets of this infection. We developed methods Our model, SDREM integrates static and time series data to link proteins and the pathways they regulate in these networks, SDREM uses prior information about proteins ' likelihood of involvement in a disease ( e, from screens ) to improve the quality of the predicted signaling pathways. We used our algorithms to study the human immune response to H1N1 influenza infection Applying our method to the more pathogenic H5N1 influenza.
2K_test_2024	Supporting students ' self-regulated learning ( SRL ) is an important topic in the learning sciences, Two critical processes involved in SRL are self-assessment and study choice, Intelligent tutoring systems ( ITSs ) have been shown to be effective in supporting students ' domain-level learning through guided problem-solving practice, but it is an open question how they can support SRL processes effectively, while maintaining or even enhancing their effectiveness at the domain level, This work informs the design of future ITS that supports SRL. To redesign and evaluate an ITS for linear equation solving so it supports self-assessment and study choice. The evaluations reveal that the new OLM with self-assessment support facilitates students ' learning processes, and enhances their learning outcomes significantly, However we did not find significant learning gains due to the problem selection feature. We used a combination of user-centered design techniques We added three features to the tutor ' Open Learner Model ( OLM ) that may scaffold students ' self-assessment ( self-assessment prompts, delaying the update of students ' progress bars, and providing progress information on the problem type level ), We also designed a problem selection screen with shared student/system control and game-like features.
2K_test_2025	When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention. We refer to these noisy `` human-centric { '' } annotations as exhibiting human reporting bias, Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions, In this paper we use these noisy annotations for learning visually correct image classifiers. Our results are highly interpretable for reporting `` what 's in the image { '' } versus `` what 's worth saying, We show significant improvements over traditional algorithms for both image classification and image captioning, doubling the performance of existing methods in some cases. Such annotations do not use consistent vocabulary, and miss a significant amount of the information present in an image ; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled, We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. { '' } We demonstrate the algorithm 's efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M.
2K_test_2026	Leading toward a rich family of potential extensions to CW algorithms. For robust cost-sensitive classification. Show that our robust, cost-sensitive extensions consistently reduce the cost incurred in both online and batch learning settings, We also demonstrate a correspondence between the VaR and CVaR constraints used for classification and uncertainty sets used in robust optimization. We introduce confidence-weighted ( CW ) online learning algorithms Our work extends the original confidence-weighted optimization framework in two important directions First, we show how the original value at risk ( VaR ) probabilistic constraint in CW algorithms can be generalized to a worst-case conditional value at risk ( CVaR ) constraint for more robust learning from cost-weighted examples, Second we show how to reduce adversarial feature noise, which can be useful in fraud detection scenarios, by reframing the optimization problem in terms of maximum a posteriori estimation, The resulting optimization problems can be solved efficiently. Experiments on real-world and synthetic datasets.
2K_test_2027	For the problems of minimizing the maximum label ( l ( infinity ) norm ) and minimizing l ( p ) and l ( q ) norms simultaneously.
2K_test_2028	Which are currently in clinical use for rescuing hematopoietic function during bone marrow transplants. Which is essential for non-perturbative monitoring of cell expansion. Our method achieved promising performance in the experiments with hematopoietic stem cell ( HSC ) populations. This paper proposes a vision-based method for detecting apoptosis ( programmed cell death ), Our method targets non-adherent cells, which float or are suspended freely in the culture medium-in contrast to adherent cells, which are attached to a petri dish, The method first detects cell regions and tracks them over time, resulting in the construction of cell tracklets, For each of the tracklets, visual properties of the cell are then examined to know whether and when the tracklet shows a transition from a live cell to a dead cell, in order to determine the occurrence and timing of a cell death event. For the validation a transductive learning framework is adopted to utilize unlabeled data in addition to labeled data.
2K_test_2029	Silhouettes provide rich information on three-dimensional shape, since the intersection of the associated visual cones generates the `` visual hull { '' }, which encloses and approximates the original shape However, not all silhouettes can actually be projections of the same object in space : this simple observation has implications in object recognition and multi-view segmentation, and has been ( often implicitly ) used as a basis for camera calibration, and point out some possible directions for future research. In this paper we investigate the conditions for multiple silhouettes, or more generally arbitrary closed image sets, to be geometrically `` consistent { '' }. After discussing some general results. We present a `` dual { '' } formulation for consistency, that gives conditions for a family of planar sets to be sections of the same object, Finally we introduce a more general notion of silhouette `` compatibility { '' } under partial knowledge of the camera projections. We present this notion as a natural generalization of traditional multi-view geometry, which deals with consistency for points.
2K_test_2030	Politeness is believed to facilitate communication in human interaction, as it can minimize the potential for conflict and confrontation, Regarding the role of politeness strategies for human-robot interaction, conflicting findings are presented in the literature. To gain a deeper understanding of how politeness on the one hand, and the type of interaction itself on the other hand, might affect and shape user experience and evaluation of HRI. Our findings suggest that the interaction context has a greater impact on participants ' perception of the robot in HRI than the use - or lack - of politeness strategies. Thus we conducted a between-participants experimental study with a receptionist robot.
2K_test_2031	Large-scale information processing systems are able to extract massive collections of interrelated facts. But unfortunately transforming these candidate facts into useful knowledge is a formidable challenge, In this paper we show how uncertain extractions about entities and their relations. We show that compared to existing methods, our approach is able to achieve improved AUC and F1 with significantly lower running time. Can be transformed into a knowledge graph The extractions form an extraction graph and we refer to the task of removing noise, inferring missing information and determining which candidate facts should be included into a knowledge graph as knowledge graph identification In order to perform this task, we must reason jointly about candidate facts and their associated extraction confidences, identify co-referent entities and incorporate ontological constraints, Our proposed approach uses probabilistic soft logic ( PSL ), a recently introduced probabilistic modeling framework which easily scales to millions of facts. We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL project containing over 1M extractions and 70K ontological relations.
2K_test_2032	Occlusions are common in real world scenes and are a major obstacle to robust object detection, Previous approaches primarily enforced local coherency or learned the occlusion structure from data, However local coherency ignores the occlusion structure in real world scenes and learning from data requires tediously labeling many examples of occlusions for every view of every object, Other approaches require binary classifications of matching scores. To coherently reason about occlusions on many types of detectors. Our method demonstrates significant improvement in estimating the mask of the occluding region and improves object instance detection on a challenging dataset of objects under severe occlusions. In this paper we present a method We address these limitations by formulating occlusion reasoning as an efficient search over occluding blocks which best explain a probabilistic matching pattern.
2K_test_2033	Binary codes that are binarizations of features represented by real numbers have recently been used in the object recognition field, in order to achieve reduced memory and robustness with respect to noise. However binarizing features represented by real numbers has a problem in that a great deal of the information within the features drops out, That is why we focus on quantization residual, which is information that drops out when features are binarized in order to take into consideration the possibility that a binary code which has been observed from an image will transition to another binary code. From the results of we confirmed that the proposed method enables an increase in detection performance while maintaining the same levels of memory and computing costs as those for previous methods of binarizing features. With this study we introduce a transition likelihood model into classifiers This enables classifications that consider transitions to the desired binary code, even if the observed binary code differs from the actually desired binary code for some reason.
2K_test_2034	`` Socially cooperative driving { '' } is an integral part of our everyday driving. Requiring special attention to imbue the autonomous driving with a more natural driving behavior, to enable an autonomous vehicle to perform cooperative social behavior to extract the probability of surrounding agents ' intentions in real time, to predict future scenarios to compute the cost for each scenario and select the decision corresponding to the lowest cost. Compared with approaches that do not take social behavior into account, the iPCB algorithm shows a 41, 7\ % performance improvement based on the chosen cost functions. In this paper an intention-integrated Prediction-and Cost function-Based algorithm iPCB ) framework is proposed An intention estimator is developed Then for each candidate strategy, a prediction engine considering the interaction between host and surrounding agents is used A cost function-based evaluation is applied.
2K_test_2035	On-road motion planning for autonomous vehicles is in general a challenging problem Past efforts have proposed solutions for urban and highway environments individually. We identify the key advantages/shortcomings of prior solutions, that addresses both urban and highway driving to generate an easy-to-tune and human-like reference trajectory accounting for road geometry, obstacles and high-level directives. And propose a novel two-step motion planning system in a single framework, Reference Trajectory Planning ( I ) makes use of dense lattice sampling and optimization techniques By focused sampling around the reference trajectory, Tracking Trajectory Planning ( II ) generates, evaluates and selects parametric trajectories that further satisfy kinodynamic constraints for execution The described method retains most of the performance advantages of an exhaustive spatiotemporal planner while significantly reducing computation.
2K_test_2036	Vehicular ad hoc networks ( VANETs ) are seen as an important enabling technology for improving both traffic safety and efficiency, Virtual Traffic Lights ( VTLs ) are a promising proposal for reducing travel time by efficiently controlling road intersections, VTLs use vehicle-to-vehicle communication to dynamically optimize traffic flow and they display traffic light information on the windshield. However research so far has assumed that all vehicles are equipped with VTL support and it has ignored the incremental deployment phase, which could last decades, This allows drivers in non-equipped vehicles, or even pedestrians to see the light color and respond accordingly. We show that the benefits of VTLs grow as a function of the penetration rate of equipped vehicles. In this paper we present a solution for a VTL partial deployment scenario that is based on the idea of having VTL equipped cars display traffic light information on the outside of the vehicle. In terms of intersection throughput and average delay reduction.
2K_test_2037	Which can be readily utilized in an active learning framework to improve identity-aware multi-object tracking. Produces identity-coherent trajectories but also has the ability to pinpoint potential tracking errors. Show that not only is our proposed tracker effective, but also the solution path enables automatic pinpointing of potential tracking failures.
2K_test_2038	Spoken dialogue systems typically use predefined semantic slots to parse users ' natural language inputs into unified semantic representations, Our slot filling evaluations also indicate the promising future of this proposed approach. To define the slots, domain experts and professional annotators are often involved, and the cost can be expensive, In this paper we ask the following question : given a collection of unlabeled raw audios, can we use the frame semantics theory to automatically induce and fill the semantic slots in an unsupervised fashion ?. Show that the automatically induced semantic slots are in line with the reference slots created by domain experts : we observe a mean averaged precision of 69, 36\ % using ASR-transcribed data. To do this we propose the use of a state-of-the-art frame-semantic parser, and a spectral clustering based slot ranking model that adapts the generic output of the parser to the target semantic space. Empirical experiments on a real-world spoken dialogue dataset.
2K_test_2040	Smartphones are now targets of malicious viruses Furthermore, the increasing `` connectedness { '' } of smartphones has resulted in new delivery vectors for malicious viruses, including proximity- social- and other technology-based methods, In fact Cabir and CommWarrior are two viruses-observed in the wild-that spread, at least in part, using proximity-based techniques ( line-of-sight bluetooth radio ). That describes the spread of two mutually exclusive viruses across heterogeneous composite networks, one static ( social connections ) and one dynamic ( mobility pattern ). Find that the first eigenvalue of the system matrices lambda ( S1 ), lambda ( S2 ) of the two networks ( static and dynamic networks ) appropriately captures the competitive interplay between two viruses and effectively predicts the competition 's `` winner { '' }, which provides a feasible way to defend against smartphone viruses. In this paper we propose and evaluate SI1I2S, a competition model To approximate dynamic network behavior, we use classic mobility models from ad hoc networking, Random Waypoint Random Walk and Levy Flight. We analyze our model using techniques from dynamic systems and.
2K_test_2041	Together these findings suggest that head motion is strongly related to age-appropriate emotion challenge, are consistent with the hypothesis that perturbations of normal responsiveness carry-over even after the parent resumes normal responsiveness in the reunion, and that there are frequent changes in direction of influence in the postural domain. We investigated the dynamics of head motion in parents and infants during an age-appropriate, well-validated emotion induction the Face-to-Face/Still-Face procedure. During infant gaze toward the parent, infant angular amplitude and velocity of pitch and yaw decreased from face-to-face ( FF ) to still-face ( SF ) episodes and remained lower in the following Reunion, During infant gaze away from the parent, angular velocity of pitch decreased from FF to SF and remained lower in the Reunion ( RE ) Windowed cross-correlation suggested strong bidirectional effects with frequent shifts in the direction of influence, The number of significant positive and negative peaks was higher during FF than RE, Gaze toward and away from the parent was modestly predicted by head orientation.
2K_test_2042	It is common to represent photos as vertices of a weighted graph, where edge weights measure similarity or distance between pairs of photos Ultimately, our system enables everyday people to take advantage of each others ' perspectives in order to create on-the-spot spatiotemporal visual experiences similar to the popular bullet-time sequence, We believe that this type of application will greatly enhance shared human experiences spanning from events as personal as parents watching their children 's football game to highly publicized red carpet galas. For interactively exploring a collectively captured moment without explicit 3D reconstruction, to organize collections of photos in a way that enables the construction of visually smooth paths. We present a near real-time algorithm Our system favors immediacy and local coherency to global consistency, We introduce Angled Graphs as a new data structure Weighted angled graphs extend weighted graphs with angles and angle weights which penalize turning along paths, As a result locally straight paths can be computed by specifying a photo and a direction, The weighted angled graphs of photos used in this paper can be regarded as the result of discretizing the Riemannian geometry of the high dimensional manifold of all possible photos.
2K_test_2043	They are important for formal verification of realistic hybrid systems and embedded software. We study SMT problems over the reals containing ordinary differential equations. We demonstrate scalability of the algorithms. We develop delta-complete algorithms for SMT formulas that are purely existentially quantified, as well as there exists for all-formulas whose universal quantification is restricted to the time variables. As implemented in our open-source solver dReal, on SMT benchmarks with several hundred nonlinear ODEs and variables.
2K_test_2044	A robotic swarm is a decentralized group of robots which overcome failure of individual robots with robust emergent behaviors based on local interactions, These behaviors are not well built for accomplishing complex tasks, however because of the changing assumptions required in various applications and environments, A new movement in the research field is to add human input to influence the swarm in order to help make the robots goal directed and overcome these problems Previous studies have all used visual feedback through a computer interface to give the user the swarm state information Researchers in multi-robot systems have shown benefits of haptic feedback in obstacle navigation before. This research in Human Swarm Interaction ( HSI ) focuses on different control laws and ways to integrate the human intent with local control laws of the robots, to give the operator haptic feedback as well as visual feedback. The study shows the benefits of the additional feedback in a target searching class, In most environments operators were able to cover significantly more area, increasing the chance of finding more targets, The other environment found no significant difference, showing that the haptic feedback does not degrade performance in any of the tested environments, This supports our hypothesis that haptic feedback is useful in HSI and requires further research to maximize its potential. This study adapted swarm control algorithms but this study is a novel method because of the decentralized formation of the robotic swarm.
2K_test_2045	Clustering is the task of grouping a set of objects so that objects in the same cluster are more similar to each other than to those in other clusters The crucial step in most clustering algorithms is to find an appropriate similarity metric, which is both challenging and problem-dependent, Supervised clustering approaches which can exploit labeled clustered training data that share a common metric with the test set, have thus been proposed. Unfortunately current metric learning approaches for supervised clustering do not scale to large or even medium-sized datasets for supervised clustering. Confirm several orders of magnitude speedup while still achieving state-of-the-art performance. In this paper we propose a new structured Mahalanobis Distance Metric Learning method We formulate our problem as an instance of large margin structured prediction and prove that it can be solved very efficiently in closed-form, The complexity of our method is ( in most cases ) linear in the size of the training dataset We further reveal a striking similarity between our approach and multivariate linear regression. Experiments on both synthetic and real datasets.
2K_test_2046	Previous studies have examined the characteristics of physiological tremor under laboratory settings as well as different operating conditions, However different test methods make the comparison of results across trials and conditions difficult. This paper presents the characterization and comparison of physiological tremor for pointing tasks in multiple environments, as a baseline for performance evaluation of microsurgical robotics. Two vitroretinal microsurgeons were evaluated while performing a pointing task with no entry-point constraint, constrained by an artificial eye model, and constrained by a rabbit eye in vivo For the three respective conditions A spectral analysis was also performed.
2K_test_2047	Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D, Manual needle insertion allows the clinician to control the insertion speed. For automatic 3D steering of manually inserted flexible needles. Demonstrate the performance of the proposed controller, show the feasibility of this technique in 2D and 3D environments. This paper presents a control law A look-ahead proportional controller for position and orientation is presented, The look-ahead distance is a linear function of insertion speed. Simulations in a 3D brain-like environment Experimental results also.
2K_test_2048	Remains a difficult task. Generating meaningful digests of videos by extracting interesting frames. We demonstrated competitive performance both in accuracy relative to human annotation and computation time. In this paper we define interesting events as unusual events which occur rarely in the entire video and we propose a novel interesting event summarization framework based on the technique of density ratio estimation recently introduced in machine learning Our proposed framework is unsupervised and it can be applied to general video sources, including videos from moving cameras. We evaluated the proposed approach on a publicly available dataset in the context of anomalous crowd behavior and with a challenging personal video dataset.
2K_test_2049	Existing semi-supervised approaches are typically unreliable and face semantic drift because the learning task is under-constrained, This is primarily because they ignore the strong interactions that often exist between scene categories, such as the common attributes shared across categories as well as the attributes which make one scene different from another For example, the knowledge that an image is an auditorium can improve labeling of amphitheaters by enforcing constraint that an amphitheater image should have more circular structures than an auditorium image. We consider the problem of semi-supervised bootstrap learning for scene categorization The goal of this paper is to exploit these relationships and constrain the semi-supervised learning problem to constrain the learning problem and avoid semantic drift. We demonstrate the effectiveness of our approach including results. We propose constraints based on mutual exclusion, binary attributes and comparative attributes and show that they help us. Through extensive experiments on a very large dataset of one million images.
2K_test_2050	We address the task of inferring the future actions of people from noisy visual input To achieve accurate activity forecasting. As proof-of-concept results demonstrate that our model accurately predicts distributions over future actions of individuals We show how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories. We denote this task activity forecasting, our approach models the effect of the physical environment on the choice of human actions, This is accomplished by the use of state-of-the-art semantic scene understanding combined with ideas from optimal control theory Our unified model also integrates several other key elements of activity analysis, namely destination forecasting sequence smoothing and transfer learning. We focus on the domain of trajectory-based activity analysis from visual input.
2K_test_2051	Reconstructing an arbitrary configuration of 3D points from their projection in an image is an ill-posed problem, When the points hold semantic meaning, such as anatomical landmarks on a body, human observers can often infer a plausible 3D configuration, drawing on extensive visual memory. To recover the 3D configuration of a human figure from 2D locations of anatomical landmarks in a single image. Show generalization to novel 3D configurations and robustness to missing data. We present an activity-independent method leveraging a large motion capture corpus as a proxy for visual memory Our method solves for anthropometrically regular body pose and explicitly estimates the camera via a matching pursuit algorithm operating on the image projections Anthropometric regularity ( i, that limbs obey known proportions ) is a highly informative prior, but directly applying such constraints is intractable, Instead we enforce a necessary condition on the sum of squared limb-lengths that can be solved for in closed form to discourage implausible configurations in 3D. We evaluate performance on a wide variety of human poses captured from different viewpoints and.
2K_test_2052	Human pose estimation requires a versatile yet well-constrained spatial model for grouping locally ambiguous parts together to produce a globally consistent hypothesis. Previous works either use local deformable models deviating from a certain template, or use a global mixture representation in the pose space. Showing its ability to capture high-order dependencies of parts Second, our model achieves accurate reconstruction of unseen poses Finally, our model achieves state-of-art performance, and substantially outperforms recent hierarchical models. In this paper we propose a new hierarchical spatial model that can capture an exponential number of poses with a compact mixture representation on each part, Using latent nodes it can represent high-order spatial relationship among parts with exact inference, Different from recent hierarchical models that associate each latent node to a mixture of appearance templates ( like HoG ), we use the hierarchical structure as a pure spatial prior avoiding the large and often confounding appearance space. We verify the effectiveness of this model in three ways, First samples representing human-like poses can be drawn from our model compared to a nearest neighbor pose representation on three challenging datasets.
2K_test_2053	Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition This success can be largely attributed to learning shared representations from multiple supervisory tasks. However existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize, to learn shared representations in ConvNets. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples. In this paper we propose a principled approach using multi-task learning, Specifically we propose a new sharing unit : `` cross-stitch { '' } unit, These units combine the activations from multiple networks and can be trained end-to-end, A network with cross-stitch units can learn an optimal combination of shared and task-specific representations.
2K_test_2054	This is an important scenario that frequently arises in practice not only when two different types of sensors are used, but also when the sensors are not co-located and have different sampling rates, Previous work has addressed this problem by restricting interpretation to a single representation in one of the domains, with augmented features that attempt to encode the information from the other modalities. We address the problem of understanding scenes from multiple sources of sensor data ( e, a camera and a laser scanner ) in the case where there is no one-to-one correspondence across modalities ( e, pixels and 3-D points ). We demonstrate that this co-inference approach also improves performance over the canonical approach. Instead we propose to analyze all modalities simultaneously while propagating information across domains during the inference procedure, In addition to the immediate benefit of generating a complete interpretation in all of the modalities.
2K_test_2055	Object discovery algorithms group together image regions that originate from the same object, This process is effective when the input collection of images contains a large number of densely sampled views of each object, thereby creating strong connections between nearby views. However existing approaches are less effective when the input data only provide sparse coverage of object views, that addresses this problem. { '' } Our approach can correctly discover links between regions of the same object even if they are captured from dramatically different viewpoints, With the help from these added links, our proposed approach can robustly discover object instances even with sparse coverage of the viewpoints.
2K_test_2056	The problem of training classifiers from limited data is one that particularly affects large-scale and social applications, and as a result, although carefully trained machine learning forms the backbone of many current techniques in research, it sees dramatically fewer applications for end-users Recently we demonstrated a technique for selecting or recommending a single good classifier from a large library even with highly impoverished training data. We consider alternatives for extending our recommendation technique to sets of classifiers, including for extending model recommendation to sets. A modification to the AdaBoost algorithm that incorporates recommendation, Evaluating on an action recognition problem, we present two viable methods.
2K_test_2057	We have shown in prior work how to give a Curry-Howard interpretation of the proofs in the linear sequent calculus as pi-calculus processes subject to a session type discipline. We study type-directed encodings of the simply-typed lambda-calculus in a session-typed pi-calculus. We show that the resulting translations induce sharing and copying parallel evaluation strategies for the original lambda-terms, thereby providing a new logically motivated explanation for these strategies. The translations proceed in two steps : standard embeddings of simply-typed lambda-calculus in a linear lambda-calculus, followed by a standard translation of linear natural deduction to linear sequent calculus.
2K_test_2058	The detection of apoptosis, or programmed cell death, is important to understand the underlying mechanism of cell development. At present apoptosis detection resorts to fluorescence or colorimetric assays, which may affect cell behavior and thus not allow long-term monitoring of intact cells, to detect apoptosis in time-lapse phase-contrast microscopy to determine if the candidate is indeed an apoptotic cell. The method achieved around 90\ % accuracy in terms of average precision and recall. In this work we present an image analysis method which is non-destructive imaging, The method first detects candidates for apoptotic cells based on the optical principle of phase-contrast microscopy in connection with the properties of apoptotic cells, The temporal behavior of each candidate is then examined in its neighboring frames in order.
2K_test_2059	Cache compression is a promising technique to increase on-chip cache capacity and to decrease on-chip and off-chip bandwidth usage, Unfortunately directly applying well-known compression algorithms ( usually implemented in software ) leads to high hardware complexity and unacceptable decompression/compression latencies, which in turn can negatively affect performance. Hence there is a need for a simple yet efficient compression technique that can effectively compress common in-cache data patterns, and has minimal effect on cache access latency, a practical technique for compressing data in on-chip caches. Our studies show that B Delta I strikes a sweet-spot in the tradeoff between compression ratio, decompression/compression latencies and hardware complexity Our results show that B Delta I compression improves performance for both single-core ( 8, 1\ % improvement ) and multi-core workloads ( 9, 2\ % improvement for two/four cores ), For many applications B Delta I provides the performance benefit of doubling the cache size of the baseline system, effectively increasing average cache capacity by 1. In this paper we introduce a new compression algorithm called Base-Delta-Immediate ( B Delta I ) compression, The key idea is that, for many cache lines, the values within the cache line have a low dynamic range - i, the differences between values stored within the cache line are small, As a result a cache line can be represented using a base value and an array of differences whose combined size is much smaller than the original cache line ( we call this the base+ delta encoding Moreover, many cache lines intersperse such base+ delta values with small values - our B Delta I technique efficiently incorporates such immediate values into its encoding. Compared to prior cache compression approaches.
2K_test_2060	Most contemporary object detection approaches assume each object instance in the training data to be uniquely represented by a single bounding box. In this paper we go beyond this conventional view by allowing an object instance to be described by multiple bounding boxes. The new bounding box annotations are determined based on the alignment of an object instance with the other training instances in the dataset, Our proposal enables the training data to be reused multiple times for training richer multi-component category models We operationalize this idea by two complementary operations : bounding box shrinking, which finds subregions of an object instance that could be shared ; and bounding box enlarging, which enlarges object instances to include local contextual cues. We empirically validate our approach on the PASCAL VOC detection dataset.
2K_test_2061	In practical applications of robot swarms with bio-inspired behaviors, a human operator will need to exert control over the swarm to fulfill the mission objectives, In many operational settings, human operators are remotely located and the communication environment is harsh Hence, there exists some latency in information ( or control command ) transfer between the human and the swarm. In this paper to investigate the effects of communication latency on the performance of a human-swarm system in a swarm foraging task, We develop and investigate the concept of neglect benevolence, where a human operator allows the swarm to evolve on its own and stabilize before giving new commands. Our experimental results indicate that operators exploited neglect benevolence in different ways to develop successful strategies in the foraging task, Furthermore we show that the use of a predictive display can help mitigate the adverse effects of communication latency. We conduct experiments of human-swarm interaction experimentally.
2K_test_2062	Human interaction with robot swarms ( HSI ) is a young field with very few user studies that explore operator behavior All these studies assume perfect communication between the operator and the swarm A key challenge in the use of swarm robotic systems in human supervised tasks is to understand human swarm interaction in the presence of limited communication bandwidth, which is a constraint arising in many practical scenarios. In this paper we present results of human-subject experiments designed to study the effect of bandwidth limitations in human swarm interaction. The lowest bandwidth condition performs poorly, but the medium and high bandwidth condition both perform well, In the medium bandwidth condition, we display useful aggregated swarm information ( like swarm centroid and spread ) to compress the swarm state information, We also observe interesting operator behavior and adaptation of operators ' swarm reaction. We consider three levels of bandwidth availability in a swarm foraging task.
2K_test_2063	Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. For the task of pose estimation. We demonstrate state-of-the-art performance and outperform competing methods. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation, We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference, Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. On standard benchmarks including the MPII, LSP and FLIC datasets.
2K_test_2064	Finding meaningful structured representations of 3D point cloud data ( PCD ) has become a core task for spatial perception applications. For constructing compact generative representations of PCD at multiple levels of detail. Our tests showing favorable performance when compared to octree and NDT-based methods. In this paper we introduce a method As opposed to deterministic structures such as voxel grids or octrees, we propose probabilistic subdivisions of the data through local mixture modeling, and show how these subdivisions can provide a maximum likelihood segmentation of the data, The final representation is hierarchical, compact parametric and statistically derived, facilitating run-time occupancy calculations through stochastic sampling, Unlike traditional deterministic spatial subdivision methods, our technique enables dynamic creation of voxel grids according the application 's best needs, In contrast to other generative models for PCD, we explicitly enforce sparsity among points and mixtures, a technique which we call expectation sparsification, This leads to a highly parallel hierarchical Expectation Maximization ( EM ) algorithm well-suited for the GPU and real-time execution. We explore the trade-offs between model fidelity and model size at various levels of detail.
2K_test_2065	The proposed framework provides a way to explore the interaction between climate change and policy factors at a global scale. How are the populations of the world likely to shift ? Which countries will be impacted by sea-level rise ? to examine shifts in population given network relations among countries, which influences overall population change. This paper uses a country-level agent-based dynamic network model Some of the networks considered include : alliance networks, shared language networks economic influence networks, Validation of model is done for migration probabilities between countries, as well as for country populations and distributions.
2K_test_2066	Sudden weight gain in patients living with Congestive Heart Failure ( CHF ) is often an indication that the individual is retaining fluid, which often means that patient 's heart has weakened leading to increased risk of kidney or cardiac failure, leading to the possibility of earlier clinical interventions, potentially preventing deadly medical emergencies. Clinical interventions can be made at this stage, leading to better outcomes, however it is essential that the interventions take place before the patient 's health declines too drastically, allowing us to predict weight values into the future. In this work we present a latent variable autoregression model that tracks patient weight and blood pressure over time We are also able to model continuous heart-rate signals and evaluate a subject 's response to physical activity, This allows us to detect signs of health decline days earlier than existing rule-based systems.
2K_test_2067	Many Android apps heavily depend on collecting and sharing sensitive privacy information, such as device ID, location and postal address, to provide service and value, To protect user privacy, apps are typically required by market places to provide privacy policies informing users about how their private information will be processed. To detect privacy-policy violations, inconsistencies between an app 's data collection code and the corresponding description in its privacy policy. In this paper we present PVDetector, an automatic tool that analyzes Android apps.
2K_test_2068	Implications for the understanding of human behavior and social agent design are discussed. This work focuses on data-driven discovery of the temporally co-occurring and contingent behavioral patterns that signal high and low interpersonal rapport. We validated the discovered behavioral patterns Our framework performs significantly better than a baseline linear regression method that does not encode temporal information among behavioral features. By predicting rapport against our ground truth via a forecasting model involving two-step fusion of learned temporal associated rules. We mined a reciprocal peer tutoring corpus reliably annotated for nonverbals like eye gaze and smiles, conversational strategies like self-disclosure and social norm violation, and for rapport ( in 30s thin slices ), We then performed a fine-grained investigation of how the temporal profiles of sequences of interlocutor behaviors predict increases and decreases of rapport, and how this rapport management manifests differently in friends and strangers.
2K_test_2069	Human communication literature states that people with different culture backgrounds act differently in conversations. Currently most virtual agents are designed for a single targeted popular culture. We found that users from different culture context express engagement differently. We implemented two versions of a virtual agent targeting American and Chinese cultures.
2K_test_2070	In this paper we inspect the performance of regularized linear mixed effect models, as an extension of linear mixed effect model, when multiple confounding factors coexist. Our results suggest that sequence multiple confounding factors corrections behave the best when different confounders contribute equally to response variables, On the other hand, when various confounders affect the response variable unevenly, results mainly rely on the degree of how the major confounder is corrected. We introduce three different methods for multiple confounding factors correction, namely concatenation sequence and interpolation. We first review its parameter estimation algorithms before Then we investigate the performance on variable selection task and predictive task on three different data sets, synthetic data set semi-empirical synthetic data set based on genome sequences and brain wave data set connecting to confused mental states.
