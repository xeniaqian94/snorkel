2K_dev_9	Social science researchers spend significant time annotating behavioral events in video data in order to quantitatively assess interactions [ 2 ], These behavioral events may be instantaneous changes, continuous actions that span unbounded periods of time, or behaviors that would be best described by severity or other scalar ratings. The complexity of these judgments, coupled with the time and effort required to meticulously assess video, results in a training and evaluation process that can take days or weeks, Computational analysis of video data is still limited due to the challenges introduced by objective interpretation and varied contexts. These new features allow analysts to acquire more specific information about events in video datasets. Glance [ 4 ] introduced a means of leveraging human intelligence by recruiting crowds of paid online workers to accurately analyze hours of video data in a matter of minutes, This approach has been shown to expedite work in human-centered fields, as well as generate training data for automated recognition systems, In this paper we describe an interactive demonstration of an improved, more expressive version of Glance that expands the initial set of supported annotation formats ( e, time range classification etc, ) from one to nine, Worker interfaces for each of these options are dynamically generated, along with tutorials based on the analyst 's question.
2K_dev_11	Social influence is key in technology adoption. But its role in security-feature adoption is unique and remains unclear. Our results suggest that social influence affects one 's likelihood to adopt a security feature, but its effect varies based on the observability of the feature, the current feature adoption rate among a potential adopter 's friends, and the number of distinct social circles from which those feature-adopting friends originate Curiously, there may be a threshold higher than which having more security feature adopting friends predicts for higher adoption likelihood, but below which having more feature-adopting friends predicts for lower adoption likelihood, Furthermore the magnitude of this threshold is modulated by the attributes of a feature-features that are more noticeable ( Login Approvals, Trusted Contacts ) have lower thresholds. Here we analyzed how three Facebook security features ' Login Approvals, Login Notifications and Trusted Contacts-diffused through the social networks of 1.
2K_dev_14	Online communities much like companies in the business world, often need to transfer best practices internally from one unit to another to improve their performance. Organizational scholars disagree about how much a recipient unit should modify a best practice when incorporating it Some evidence indicates that modifying a practice that has been successful in one environment will introduce problems, undercut its effectiveness and harm the performance of the recipient unit Other evidence, though suggests that recipients need to adapt the practice to fit their local environment. The current research introduces a contingency perspective on practice transfer, holding that the value of modifications depends on when they are introduced and who introduces them modifications are more helpful if they are introduced after the receiving project has had experience with the imported practice Furthermore, modifications are more effective if they are introduced by members who have experience in a variety of other projects. Empirical research on the transfer of a quality-improvement practice between projects within Wikipedia shows that.
2K_dev_28	Previous work has shown the promise of crowdsourcing analogical idea generation, where distributing the stages of analogical processing across many people can reduce fixation, identify inspirations from more diverse domains, and lead to more creative ideas. However prior work has only considered problems with a single constraint, while many real-world problems involve multiple constraints for eliciting multiple constraints inherent in a problem and using those constraints to find inspirations useful in solving it To do so we identify methods to elicit useful constraints at different levels of abstraction, and empirical results that identify how the level of abstraction influences creative idea generation. Our results show that crowds find the most useful inspirations when the problem domain is represented abstractly and constraints are represented more concretely. This paper contributes a systematic crowdsourcing approach.
2K_dev_38	Thus improving their ability to facilitate one-time or spontaneous exchanges of information. We present a new technique that allows mobile devices to opportunistically group with one another, we examine the limitations of using a single type of context to form groups. Show how leveraging multiple contexts improves our ability to detect and form relevant groupings Through two prototypes, we demonstrate how DIDJA enhances existing user experiences, and show how developers can use our toolkit to easily facilitate frictionless collaborations between users and their environment We then perform an extended experiment and show how DIDJA is able to accurately form groups under realistic conditions. In our approach devices share context with each other, and form groups when these readings are found to be similar to one another Through a formative study, We then present DIDJA, a robust software toolkit that automatically collects and analyzes contextual information in order to find and form groups.
2K_dev_54	Languages for music audio processing typically offer a large assortment of unit generators There is great duplication among different language implementations, as each language must implement many of the same ( or nearly the same ) unit generators, We suggest that these techniques might eliminate most of the effort of building unit generator libraries and could help with the implementation of embedded audio systems where unit generators are needed but a full embedded Csound engine is not required. Csound has a large library of unit generators and could be a useful source of reusable unit generators for other languages or for direct use in applications, In this study we consider how Csound unit generators can be exposed to direct access by other audio processing languages. Using Aura as an example, we modified Csound to allow efficient, dynamic allocation of individual unit generators without using the Csound compiler or writing Csound instruments, We then extended Aura using automatic code generation so that Csound unit generators can be accessed in the normal way from within Aura, In this scheme Csound details are completely hidden from Aura users.
2K_dev_56	Motivated by a radically new peer review system that the National Science Foundation recently experimented with. We study peer review systems in which proposals are reviewed by PIs who have submitted proposals themselves. An ( m ; k ) -selection mechanism asks each PI to review m proposals, and uses these reviews to select ( at most ) k proposals, We are interested in impartial mechanisms, which guarantee that the ratings given by a PI to others ' proposals do not affect the likelihood of the PI 's own proposal being selected We design an impartial mechanism that selects a k-subset of proposals that is nearly as highly rated as the one selected by the non-impartial ( abstract version of ) the NSF pilot mechanism, even when the latter mechanism has the `` unfair '' advantage of eliciting honest reviews.
2K_dev_57	The fairness notion of maximin share ( MMS ) guarantee underlies a deployed algorithm for allocating indivisible goods under additive valuations, Previous work has shown that such an MMS allocation may not exist, but the counterexample requires a number of goods that is exponential in the number of players ;. Our goal is to understand when we can expect to be able to give each player his MMS guarantee, that provably finds an MMS allocation with high probability. We give a new construction that uses only a linear number of goods, On the positive side, we formalize the intuition that these counterexamples are very delicate by designing an algorithm when valuations are drawn at random.
2K_dev_58	A paradigmatic problem in social choice theory deals with the aggregation of subjective preferences of individuals represented as rankings of alternatives into a social ranking. We are interested in settings where individuals are uncertain about their own preferences, and represent their uncertainty as distributions over rankings. We show that ignoring uncertainty altogether can lead to suboptimal outcomes. Under the classic objective of minimizing the ( expected ) sum of Kendall tau distances between the input rankings and the output ranking, we establish that preference elicitation is surprisingly straightforward and near-optimal solutions can be obtained in polynomial time. Both in theory and using real data.
2K_dev_61	We study the envy-free allocation of indivisible goods between two players To rigorously quantify the efficiency gain from selling. We show that envy-free allocations of sellable goods are significantly more efficient than their unsellable counterparts. Our novel setting includes an option to sell each good for a fraction of the minimum value any player has for the good, we reason about the price of envy-freeness of allocations of sellable goods -- the ratio between the maximum social welfare and the social welfare of the best envy-free allocation.
2K_dev_62	Some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives. It seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule, which simply counts the number of times each alternative was approved, We challenge this assertion. Results call attention to situations where approval voting is suboptimal. By proposing a probabilistic framework of noisy voting, and asking whether approval voting yields an alternative that is most likely to be the best alternative. While the answer is generally positive, our theoretical and empirical.
2K_dev_64	For cake cutting ( the fair allocation of a divisible good ), in which agents simultaneously send messages containing a sketch of their preferences over the cake. We introduce the simultaneous model We show that this model enables the computation of divisions that satisfy proportionality -- a popular fairness notion -- using a protocol that circumvents a standard lower bound via parallel information elicitation, Cake divisions satisfying another prominent fairness notion, envy-freeness are impossible to compute in the simultaneous model, but admit arbitrarily good approximations.
2K_dev_65	Motivated by applications to crowdsourcing. We study voting rules that output a correct ranking of alternatives by quality from a large collection of noisy input rankings We seek voting rules that are supremely robust to noise, in the sense of being correct in the face of any `` reasonable '' type of noise. We show that there is such a voting rule, which we call the modal ranking rule, Moreover we establish that the modal ranking rule is the unique rule with the preceding robustness property within a large family of voting rules, which includes a slew of well-studied rules.
2K_dev_66	Classic social choice theory assumes that votes are independent ( but possibly conditioned on an underlying objective ground truth ). This assumption is unrealistic in settings where the voters are connected via an underlying social network structure, as social interactions lead to correlated votes, for ranked voting to recover the ground truth. We establish a general framework -- based on random utility theory -- on a social network with arbitrarily many alternatives ( in contrast to previous work, which is restricted to two alternatives ), We identify a family of voting rules which, without knowledge of the social network structure, are guaranteed with high probability in large networks, with respect to a wide range of models of correlation among input votes.
2K_dev_67	Limited lookahead has been studied for decades in perfect-information games. This paper initiates a new direction via two simultaneous deviation points : generalization to imperfect-information games and a game-theoretic approach The question of how one should act when facing an opponent whose lookahead is limited is studied along multiple axes : lookahead depth, whether the opponent ( s ), too have imperfect information, and how they break ties for computing optimal commitment strategies. The limited-lookahead player often obtains the value of the game if she knows the expected values of nodes in the game tree for some equilibrium, but we prove this is not sufficient in general, This uncovers a lookahead pathology. We characterize the hardness of finding a Nash equilibrium or an optimal commitment strategy for either player, showing that in some of these variations the problem can be solved in polynomial time while in others it is PPAD-hard or NP-hard, We proceed to design algorithms for when the opponent breaks ties 1 ) favorably, 2 ) according to a fixed rule, or 3 ) adversarially. The impact of limited lookahead is then investigated experimentally Finally, we study the impact of noise in those estimates and different lookahead depths.
2K_dev_68	For solving a linear system arising from the 1-Laplacian corresponding to a collapsible simplicial complex with a known collapsing sequence. We present an efficient algorithm When combined with a result of Chillingworth, our algorithm is applicable to convex simplicial complexes embedded in R3 The running time of our algorithm is nearly-linear in the size of the complex and is logarithmic on its numerical properties, Our algorithm is based on projection operators and combinatorial steps for transferring between them The former relies on decomposing flows into circulations and potential flows using fast solvers for graph Laplacians, and the latter relates Gaussian elimination to topological properties of simplicial complexes.
2K_dev_69	Classic cake cutting protocols are susceptible to manipulation, Do their strategic outcomes still guarantee fairness ? To address this question. Specifically we show that each protocol in the class of generalized cut and choose ( GCC ) protocols which includes the most important discrete cake cutting protocols is guaranteed to have approximate subgame perfect Nash equilibria, or even exact equilibria if the protocol 's tie-breaking rule is flexible, We further observe that the ( approximate ) equilibria of proportional protocols which guarantee each of the n agents a 1/n-fraction of the cake must be ( approximately ) proportional, thereby answering the above question in the positive ( at least for one common notion of fairness ). We adopt a novel algorithmic approach, proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model. We study the paradigmatic fair division problem of fairly allocating a divisible good among agents with heterogeneous preferences, commonly known as cake cutting.
2K_dev_73	Abstract : Given a large network, changing over time how can we find patterns and anomalies ? which can discover both transient and periodic/ repeating communities. Com2 spots intuitive patterns, that is temporal communities ( comet communities ), We report our findings, which include large star-like patterns, near-bipartite-cores as well as tiny groups ( 5 users ), calling each other hundreds of times within a few days. We propose Com2 a novel and fast, incremental tensor analysis approach, The method is ( a ) scalable, being linear on the input size ( b ) general, ( c ) needs no user-defined parameters and ( d ) effective, returning results that agree with intuition. We apply our method on real datasets, including a phone-call network and a computer-traffic network The phone call network consists of 4 million mobile users, with 51 million edges ( phonecalls ).
2K_dev_77	A key challenge in solving extensive-form games is dealing with large, or even infinite action spaces, In games of imperfect information, the leading approach is to find a Nash equilibrium in a smaller abstract version of the game that includes only a few actions at each decision point, and then map the solution back to the original game. However it is difficult to know which actions should be included in the abstraction without first solving the game, and it is infeasible to solve the game without first abstracting it. Show it can outperform fixed abstractions at every stage of the run : early on it improves as quickly as equilibrium finding in coarse abstractions, and later it converges to a better solution than does equilibrium finding in fine-grained abstractions. We introduce a method that combines abstraction with equilibrium finding by enabling actions to be added to the abstraction at run time, This allows an agent to begin learning with a coarse abstraction, and then to strategically insert actions at points that the strategy computed in the current abstraction deems important The algorithm can quickly add actions to the abstraction while provably not having to restart the equilibrium finding, It enables anytime convergence to a Nash equilibrium of the full game even in infinite games.
2K_dev_88	We investigate a notion of behavioral genericity in the context of session type disciplines. Combined our results confer strong correctness guarantees for communicating systems. To this end we develop a logically motivated theory of parametric polymorphism, reminiscent of the Girard-Reynolds polymorphic -calculus, but casted in the setting of concurrent processes, In our theory polymorphism accounts for the exchange of abstract communication protocols and dynamic instantiation of heterogeneous interfaces, as opposed to the exchange of data types and dynamic instantiation of individual message types, Our polymorphic session-typed process language satisfies strong forms of type preservation and global progress, is strongly normalizing and enjoys a relational parametricity principle In particular, parametricity is key to derive non-trivial results about internal protocol independence, a concurrent analogous of representation independence, and non-interference properties of modular.
2K_dev_89	A dataset has been classified by some unknown classifier into two types of points, What were the most important factors in determining the classification outcome ?. In order to uniquely characterize an influence measure : a function that, given a set of classified points, outputs a value for each feature corresponding to its influence in determining the classification outcome. In this work we employ an axiomatic approach We show that our influence measure takes on an intuitive form when the unknown classifier is linear. Finally we employ our influence measure in order to analyze the effects of user profiling on Google 's online display advertising.
2K_dev_94	Given information about medical drugs and their properties, how can we automatically discover that Aspirin has blood-thinning properties, and thus prevents Expressed in more general terms, if we have a large in- formation network that integrates data from heterogeneous data sources, how can we extract semantic information that provides a better understanding of the in- tegrated data and also helps us to identify missing links ?. We propose to extract concepts that describe groups of objects and their common properties from the integrated data. We demonstrate the effectiveness and scalability of the proposed method. The discovered concepts provide semantic information as well as an abstract view on the integrated data and thus improve the understanding of complex systems, Our proposed method has the following desirable properties : ( a ) it is parameter-free and therefore requires no user-defined parameters ( b ) it is fault-tolerant, allowing for the detection of missing links and ( c ) it is scalable, being linear on the input size. On real publicly available graphs.
2K_dev_95	Multilinear analysis is pervasive in a wide variety of fields, ranging from Signal Processing to Chemometrics, and from Machine Vision to Data Mining, Determining the quality of a given tensor decomposition is a task of utmost importance that spans all fields of application of tensors, This task by itself is hard in its nature, since even determining the rank of a tensor is an NP-hard problem, Fortunately there exist heuristics in the literature that can be effectively used for this task ; one of these heuristics is the so-called Core Consistency Diagnostic ( CORCONDIA ) which is very intuitive and simple, However simple computation of this diagnostic proves to be a very daunting task even for data of medium scale, let alone big tensor data. With the increase of the size of the tensor data that need to be analyzed there grows the need for efficient and scalable algorithms to compute diagnostics such as CORCONDIA, in order to assess the modelling quality. In this work we derive a fast and exact algorithm for CORCONDIA which exploits data sparsity and scales very well as the tensor size increases.
2K_dev_99	In prior research we have developed a Curry-Howard interpretation of linear sequent calculus as session-typed processes. In this paper we uniformly integrate this computational interpretation in a functional language. Via a linear contextual monad that isolates session-based concurrency, Monadic values are open process expressions and are first class objects in the language, thus providing a logical foundation for higher-order session typed processes We illustrate how the combined use of the monad and recursive types allows us to cleanly write a rich variety of concurrent programs, including higher-order programs that communicate processes We show the standard metatheoretic result of type preservation, as well as a global progress theorem, which to the best of our knowledge, is new in the higher-order session typed setting.
2K_dev_101	Abstract : Suppose we are given a large graph in which, by some external process, a handful of nodes are marked, What can we say about these marked nodes ? Are they all close-by in the graph, or are they segregated into multiple groups ? How can we automatically determine how many, if any groups they form as well as find simple paths that connect the nodes in each group ? We formalize the problem for partitioning marked nodes as well as finding simple paths between nodes within parts. Shows DOT2DOT correctly groups nodes for which good connection paths can be constructed, while separating distant nodes. In terms of the Minimum Description Length principle : a set of paths is simple when we need few bits to describe each path from one node to another, For example we want to avoid high-degree nodes, unless we need to visit many of its spokes, As such the best partitioning requires the least number of bits to describe the paths that visit all marked nodes, We show that our formulation for finding simple paths between groups of nodes has connections to well-known other problems in graph theory, We propose fast effective solutions, and introduce DOT2DOT an efficient algorithm.
2K_dev_105	Summary form only given. What do graphs look like ? How do they evolve over time ? How does influence/news/viruses propagate. We show that fractals and self-similarity can explain several of the observed patterns, and we conclude and a surprising result on virus propagation and immunization. We present a long list of static and temporal laws. Some recent observations on real graphs.
2K_dev_107	We consider the task of designing sparse control laws for large-scale systems by directly minimizing an infinite horizon quadratic cost with an $ \ell_1 $ penalty on the feedback controller gains that allows us to scale to large systems ( i, those where sparsity is most useful ). We demonstrate the appeal of this approach on synthetic examples and real power networks significantly larger than those previously considered in the literature. Our focus is on an improved algorithm with convergence times that are several orders of magnitude faster than existing algorithms, In particular we develop an efficient proximal Newton method which minimizes per-iteration cost with a coordinate descent active set approach and fast numerical solutions to the Lyapunov equations.
2K_dev_117	How can we correlate neural activity in the human brain as it responds to words, with behavioral data expressed as answers to questions about these same words ?. In short we want to find latent variables, that explain both the brain activity, as well as the behavioral responses, that solves the CMTF problem. We show that this is an instance of the Coupled Matrix-Tensor Factorization ( CMTF ) problem, we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm for CMTF, along with a 5 fold increase in sparsity Scoup-SMT is able to find meaningful latent variables, as well as to predict brain activity with competitive accuracy Finally, we demonstrate the generality of Scoup-SMT there, Scoup-SMT spots spammer-like anomalies. We propose Scoup-SMT a novel, fast and parallel algorithm and produces a sparse latent low-rank subspace of the data Moreover, we extend Scoup-SMT to handle missing data without degradation of performance. In our experiments We apply Scoup-SMT to BrainQ, a dataset consisting of a ( nouns, brain voxels human subjects ) tensor and a ( nouns, properties ) matrix with coupling along the nouns dimension by applying it on a Facebook dataset ( users, friends wall-postings ) ;.
2K_dev_119	How can we find useful patterns and anomalies in large scale real-world data with multiple attributes ? For example, network intrusion logs with ( source-ip, target-ip port-number timestamp ) ? Tensors are suitable for modeling these multi-dimensional data, and widely used for the analysis of social networks, web data network traffic, and in many other settings. However current tensor decomposition methods do not scale for tensors with millions and billions of rows, columns and fibers that often appear in real datasets. And discover hidden concepts. In this paper we propose HaTen2, a scalable distributed suite of tensor decomposition algorithms running on the MapReduce platform By carefully reordering the operations, and exploiting the sparsity of real world tensors, HaTen2 dramatically reduces the intermediate data, and the number of jobs, As a result using HaTen2. We analyze big real-world tensors that can not be handled by the current state of the art.
2K_dev_122	Both SAT and # SAT can represent difficult problems in seemingly dissimilar areas such as planning, verification and probabilistic inference, # SAT problems require counting the number of satisfiable formulas in a concisely-describable set of existentially-quantified. Here we examine an expressive new language, # SAT that generalizes both of these languages. Our experiments show that, despite the formidable worst-case complexity of # PNP [ 1 ], many of the instances can be solved efficiently by noticing and exploiting a particular type of frequent structure. We characterize the expressiveness and worst-case difficulty of # SAT by proving it is complete for the complexity class # PNP [ 1 ], and relating this class to more familiar complexity classes. We also experiment with three new general-purpose # SAT solvers on a battery of problem distributions including a simple logistics domain.
2K_dev_123	Classic cake cutting protocols -- which fairly allocate a divisible good among agents with heterogeneous preferences -- are susceptible to manipulation. Do their strategic outcomes still guarantee fairness ? To answer this question. GCC protocols are guaranteed to have exact subgame perfect Nash equilibria. We adopt a novel algorithmic approach, proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model Specifically, we show that each protocol in the class of generalized cut and choose ( GCC ) protocols -- which includes the most important discrete cake cutting protocols -- is guaranteed to have approximate subgame perfect Nash equilibria Moreover, we observe that the ( approximate ) equilibria of proportional protocols -- which guarantee each of the n agents a 1/n-fraction of the cake -- must be ( approximately ) proportional, and design a GCC protocol where all Nash equilibrium outcomes satisfy the stronger fairness notion of envy-freeness. Finally we show that under an obliviousness restriction, which still allows the computation of approximately envy-free allocations.
2K_dev_125	The Bayesian paradigm has provided a useful conceptual theory for understanding perceptual computation in the brain, While the detailed neural mechanisms of Bayesian inference are not fully understood, recent computational and neurophysiological works have illuminated the underlying computational principles and representational architecture, The fundamental insights are that the visual system is organized as a modular hierarchy to encode an internal model of the world, and that perception is realized by statistical inference based on such internal model. In this paper we will discuss and analyze the varieties of representational schemes of these internal models and how they might be used to perform learning and inference for relating the internal models to the observed neural phenomena and mechanisms in the visual cortex. We will argue for a unified theoretical framework.
2K_dev_130	Differential game logic ( dG L ) is a logic for specifying and verifying properties of hybrid games, games that combine discrete, continuous and adversarial dynamics, Unlike hybrid systems hybrid games allow choices in the system dynamics to be resolved adversarially by different players with different objectives. To study the existence of winning strategies for such hybrid games. Finally dG L is proved to be strictly more expressive than the corresponding logic of hybrid systems. The logic dG L can be used i, ways of resolving the players choices in some way so that he wins by achieving his objective for all choices of the opponent, Hybrid games are determined, from each state one player has a winning strategy, yet computing their winning regions may take transfinitely many steps, The logic dG L, nevertheless has a sound and complete axiomatization relative to any expressive logic, Separating axioms are identified that distinguish hybrid games from hybrid systems. By characterizing the expressiveness of both.
2K_dev_137	Modern organizations ( e, hospitals social networks government agencies ) rely heavily on audit to detect and punish insiders who inappropriately access and disclose confidential information. Recent work on audit games models the strategic interaction between an auditor with a single audit resource and auditees as a Stackelberg game, augmenting associated well-studied security games with a configurable punishment parameter, to account for multiple audit resources enabling application to practical auditing scenarios. That this transformation significantly speeds up computation of solutions for a class of audit games and security games. We significantly generalize this audit game model where each resource is restricted to audit a subset of all potential violations, thus We provide an FPTAS that computes an approximately optimal solution to the resulting non-convex optimization problem The main technical novelty is in the design and correctness proof of an optimization transformation that enables the construction of this FPTAS. In addition we experimentally demonstrate.
2K_dev_142	A variety of problems in computing, service and manufacturing systems can be modeled via infinite repeating Markov chains with an infinite number of levels and a finite number of phases. Many such chains are quasi-birth-death processes with transitions that are skip-free in level, in that one can only transition between consecutive levels, and unidirectional in phase, in that one can only transition from lower-numbered phases to higher-numbered phases for determining the limiting probabilities of such Markov chains exactly. We present a procedure, which we call Clearing Analysis on Phases ( CAP ) The CAP method yields the limiting probability of each state in the repeating portion of the chain as a linear combination of scalar bases raised to a power corresponding to the level of the state, The weights in these linear combinations can be determined by solving a finite system of linear equations.
2K_dev_143	Any strong Nash equilibrium outcome is Pareto efficient for each coalition. In this paper we consider strong Nash equilibria, in mixed strategies for finite games, In order to get our result. Our main result in its simplest form, states that if a game has a strong Nash equilibrium with full support ( that is, both players randomize among all pure strategies ), then the game is strictly competitive. We use the indifference principle fulfilled by any Nash equilibrium, and the classical KKT conditions ( in the vector setting ), that are necessary conditions for Pareto efficiency, Our characterization enables us to design a strong-Nash-equilibrium-finding algorithm with complexity in Smoothed- $ \mathcal { P } $, So this problem -- -that Conitzer and Sandholm [ Conitzer, New complexity results about Nash equilibria, 63 621 -- 641 ] proved to be computationally hard in the worst case -- -is generically easy, Hence although the worst case complexity of finding a strong Nash equilibrium is harder than that of finding a Nash equilibrium, once small perturbations are applied, finding a strong Nash is easier than finding a Nash equilibrium Next we switch to the setting with more than two players, We demonstrate that a strong Nash equilibrium can exist in which an outcome that is strictly Pareto dominated by a Nash equilibrium occurs with positive probability Finally, we prove that games that have a strong Nash equilibrium where at least one player puts positive probability on at least two pure strategies are extremely rare : they are of zero measure. First we analyze the two -- player setting.
2K_dev_149	Which may be of independent interest. For approximate maximum flow in undirected graphs with good separator structures. We present faster algorithms such as bounded genus, minor free and geometric graphs Given such a graph with n vertices, m edges along with a recursive n-vertex separator structure, our algorithm finds an 1 -- e approximate maximum flow in time O ( m6/5poly ( e -- 1 ) ), ignoring poly-logarithmic terms Similar speedups are also achieved for separable graphs with larger size separators albeit with larger run times These bounds also apply to image problems in two and three dimensions, Key to our algorithm is an intermediate problem that we term grouped L2 flow, which exists between maximum flows and electrical flows, Our algorithm also makes use of spectral vertex sparsifiers in order to remove vertices while preserving the energy dissipation of electrical flows We also give faster spectral vertex sparsification algorithms on well separated graphs.
2K_dev_150	The cake cutting problem models the fair division of a heterogeneous good between multiple agents, Previous work assumes that each agent derives value only from its own piece. However agents may also care about the pieces assigned to other agents ; such externalities naturally arise in fair division settings, to capture externalities and generalize the classical fairness notions of proportionality and envyfreeness. We extend the classical model Our technical results characterize the relationship between these generalized properties, establish the existence or nonexistence of fair allocations, and explore the computational feasibility of fairness in the face of externalities.
2K_dev_151	If Alice has double the friends of Bob, will she also have dou- ble the phone-calls ( or wall-postings, or tweets ) ?. Our first contribution is the discovery that the relative frequencies obey a power-law ( sub-linear, or super-linear ) for a wide variety of diverse settings : tasks in a phone- call network, like count of friends, count of phone-calls total count of minutes ; tasks in a twitter-like network, like count of tweets, count of followees etc, We show how to use our observations to spot clusters and outliers, telemarketers in our phone-call network. The second contribution is that we further provide a full, digitized 2-d distribution which we call the Almond-DG model, thanks to the shape of its iso-surfaces, The Almond-DG model matches all our empirical observations : super-linear relationships among variables, and ( provably ) log-logistic marginals. We illustrate our observations on two large, real network datasets spanning 2, 1M individu- als with 5 features each.
2K_dev_153	Kidney exchange provides a life-saving alternative to long waiting lists for patients in need of a new kidney, Fielded exchanges typically match under utilitarian or near-utilitarian rules ; this approach marginalizes certain classes of patients. In this paper we focus on improving access to kidneys for highly-sensitized, or hard-to-match patients Toward this end. We formally adapt a recently introduced measure of the tradeoff between fairness and efficiency -- -the price of fairness -- -to the standard kidney exchange model, We show that the price of fairness in the standard theoretical model is small, We then introduce two natural definitions of fairness. And empirically explore the tradeoff between matching more hard-to-match patients and the overall utility of a utilitarian matching, on real data from the UNOS nationwide kidney exchange and simulated data from each of the standard kidney exchange distributions.
2K_dev_160	An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer. For a setting in which the human user can only receive assistance from a semi-trusted computer. For these schemes we prove that forging passwords is equivalent to recovering the secret mapping Thus, our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts. We propose several candidate authentication protocols -- - a computer that stores information and performs computations correctly but does not provide confidentiality, Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $, The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate, We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $, for a security parameter $ s ( f ) $ that depends on two key properties To obtain our results, we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $, Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate ), and generalize recent results of Feldman et al.
2K_dev_161	To achieve maximum security, defenders must perfectly synchronize their randomized allocations of resources. We study security games with multiple defenders However, in real-life scenarios ( such as protection of the port of Boston ) this is not the case, Our goal is to quantify the loss incurred by miscoordination between defenders, both theoretically and empirically, that capture this loss. Indicate that the loss may be extremely high in the worst case, establish a smaller yet significant loss in practice. We introduce two notions under different assumptions : the price of miscoordination, and the price of sequential commitment. Generally speaking our theoretical bounds while our simulations.
2K_dev_163	The proliferation of mobile devices that are capable of estimating their position, has lead to the emergence of a new class of social networks, namely location-based social networks ( LBSNs for short ), The main interaction between users in an LBSN is location sharing, To the best of our knowledge, this is the first attempt to model this problem using tensor analysis. While the latter can be realized through continuous tracking of a user 's whereabouts from the service provider, the majority of LBSNs allow users to voluntarily share their location, LBSNs provide incentives to users to perform check-ins, However these incentives can also lead to people faking their location, thus generating false information, for spotting anomalies in the check-in behavior of users. In this work we propose the use of tensor decomposition.
2K_dev_164	The computational characterization of game-theoretic solution concepts is a central topic in artificial intelligence, with the aim of developing computationally efficient tools for finding optimal ways to behave in strategic interactions, The central solution concept in game theory is Nash equilibrium ( NE ), However it fails to capture the possibility that agents can form coalitions ( even in the 2-agent case ), Strong Nash equilibrium ( SNE ) refines NE to this setting, It is known that finding an SNE is NP-complete when the number of agents is constant, This hardness is solely due to the existence of mixed-strategy SNEs, given that the problem of enumerating all pure-strategy SNEs is trivially in P. Our central result is that, in order for a game to have at least one non-pure-strategy SNE, the agents ' payoffs restricted to the agents ' supports must, in the case of 2 agents, lie on the same line, and in the case of n agents, lie on an ( n - 1 ) -dimensional hyperplane, Leveraging this result we provide two contributions. First we develop worst-case instances for support-enumeration algorithms, These instances have only one SNE and the support size can be chosen to be of any size-in particular, Second we prove that, unlike NE finding an SNE is in smoothed polynomial time : generic game instances ( i, all instances except knife-edge cases ) have only pure-strategy SNEs.
2K_dev_165	If we know most of Smith 's friends are from Boston, what can we say about the rest of Smith 's friends ? which is one of the most important topics in AI and Web communities. In this paper we focus on the node classification problem on networks. We also prove the theoretical connections of our algorithm to the semi-supervised learning ( SSL ) algorithms and to random-walks demonstrate the benefits of the proposed algorithm, where OMNI-Prop outperforms the top competitors. Our proposed algorithm which is referred to as OMNI-Prop has the following properties : ( a ) seamless and accurate ; it works well on any label correlations ( i, homophily het-erophily and mixture of them ) ( b ) fast ; it is efficient and guaranteed to converge on arbitrary graphs ( c ) quasi-parameter free ; it has just one well-interpretable parameter with heuristic default value of 1. Experiments on four real.
2K_dev_170	It is unsolved even for two bidders and two items for sale. Designing optimalthat is revenue-maximizingcombinatorial auctions ( CAs ) is an important elusive problem. Show that our algorithms create mechanisms that yield significantly higher revenue than the VCG and scale dramatically better than prior automated mechanism design algorithms The algorithms yielded deterministic mechanisms with the highest known revenues for the settings tested, including the canonical setting with two bidders, two items and uniform additive valuations. Rather than pursuing the manual approach of attempting to characterize the optimal CA, we introduce a family of CAs and then seek a high-revenue auction within that family, The family is based on bidder weighting and allocation boosting ; we coin such CAs virtual valuations combinatorial auctions ( VVCAs ) VVCAs are the Vickrey-Clarke-Groves ( VCG ) mechanism executed on virtual valuations that are affine transformations of the bidders valuations, The auction family is parameterized by the coefficients in the transformations The problem of designing a CA is thereby reduced to search in the parameter space of VVCAor the more general space of affine maximizer auctions, We first construct VVCAs with logarithmic approximation guarantees in canonical special settings : ( 1 ) limited supply with additive valuations and ( 2 ) unlimited supply, In the main part of the paper, we develop algorithms that design high-revenue CAs for general valuations using samples from the prior distribution over bidders valuations, ( Priors turn out to be necessary for achieving high revenue, ) We prove properties of the problem that guide our design of algorithms, We then introduce a series of algorithms that use economic insights to guide the search and thus reduce the computational complexity.
2K_dev_172	How many listens will an artist receive on a online radio ? How about plays on a YouTube video ? How many of these visits are new or returning users ? Modeling and mining popularity dynamics of social activity has important implications for researchers, content creators and providers. We here investigate the effect of revisits ( successive visits from a single user ) on content popularity which captures the popularity dynamics of individual objects. We show the effect of revisits in the popularity evolution of such objects. Secondly we propose the Phoenix-R model Phoenix-R has the desired properties of being : ( 1 ) parsimonious, being based on the minimum description length principle, and achieving lower root mean squared error than state-of-the-art baselines ; ( 2 ) applicable, the model is effective for predicting future popularity values of objects. Using four datasets of social activity, with up to tens of millions media objects ( e, YouTube videos Twitter hashtags or LastFM artists ).
2K_dev_177	Given a large number of taxi trajectories, we would like to find interesting and unexpected patterns from the data, How can we summarize the major trends, and how can we spot anomalies ? The anal- ysis of trajectories has been an issue of considerable interest with many applications such as tracking trails of migrating animals and predicting the path of hurricanes. Several recent works propose methods on clus- tering and indexing trajectories data, However these approaches are not especially well suited to pattern discovery with respect to the dynamics of social and economic behavior To further analyze a huge collection of taxi trajectories, to find meaningful patterns and anomalies. In fact F-Trail does produce concise, informative and interesting patterns. We develop a novel method, called F-Trail w hich al- lows us Our approach has the following advantages : ( a ) it is fast, and scales linearly on the input size, ( b ) it is effective, leading to novel discoveries. We demonstrate the effectiveness of our approach, by performing exper- iments on real taxi trajectories.
2K_dev_182	Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in incomplete-information games. However imperfect-recall abstractions are poorly understood, and only weak algorithm-specific guarantees on solution quality are known for Nash equilibria. In this paper we show the first general, algorithm-agnostic solution quality guarantees and approximate self-trembling equilibria computed in imperfect-recall abstractions, when implemented in the original ( perfect-recall ) game, Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions where any results had been obtained, Further our analysis is tighter in two ways, each of which can lead to an exponential reduction in the solution quality error bound We then show that for extensive-form games that satisfy certain properties, the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem, where the increase in our bound is the distance function This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds We proceed to show a divide in the class of abstraction problems If payoffs are at the same scale at all information sets considered for abstraction, the input forms a metric space, Conversely if this condition is not satisfied, we show that the input does not form a metric space. Finally we use these results to experimentally investigate the quality of our bound for single-level abstraction.
2K_dev_184	Formal verification of industrial systems is very challenging, due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams, More importantly industrial systems are rarely designed for verification, but rather for operational needs The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team. To formally verify ACAS X. In this paper we present an overview of our experience using hybrid systems theorem proving an airborne collision avoidance system for airliners scheduled to be operational around 2020 The methods and proof techniques presented here are an overview of the work already presented in [ 8 ], while the evaluation of ACAS X has been significantly expanded and updated to the most recent version of the system.
2K_dev_187	How much did a network change since yesterday ? How different is the wiring between Bob 's brain ( a left-handed male ) and Alice 's brain ( a right-handed female ) ? Graph similarity with known node correspondence, the detection of changes in the connectivity of graphs, arises in numerous settings. In this work we formally state the axioms and desired properties of the graph similarity functions, and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs, that assesses the similarity between two graphs on the same nodes. Showcase the advantages of our method over existing similarity measures. We propose DeltaCon a principled, intuitive and scalable algorithm ( e, g employees of a company, customers of a mobile carrier ). Experiments on various synthetic and real graphs Finally, we employ DeltaCon to real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs, and ( b ) do temporal anomaly detection in the who-emails-whom Enron graph.
2K_dev_188	Why does Smith follow Johnson on Twitter ?. In most cases the reason why users follow other users is unavailable In this work, we answer this question. Results show that TagF uncovers different, but explainable reasons why users follow other users. By proposing TagF which analyzes the who-follows-whom network ( matrix ) and the who-tags-whom network ( tensor ) simultaneously Concretely, our method decomposes a coupled tensor constructed from these matrix and tensor. The experimental on million-scale Twitter networks.
2K_dev_190	How often do individuals perform a given communication activity in the Web, such as posting comments on blogs or news ? Could we have a generative model to create communication events with realistic inter-event time distributions ( IEDs ) ? Which properties should we strive to match ?. Current literature has seemingly contradictory results for IED : some studies claim good fits with power laws ; others with non-homogeneous Poisson processes, Given these two approaches, we ask : which is the correct one ? Can we reconcile them all ? We show here that, surprisingly both approaches are correct. Reveal that the SFP mimics their properties very well. Being corner cases of the proposed Self-Feeding Process ( SFP ), We show that the SFP ( a ) exhibits a unifying power, which generates power law tails ( including the so-called `` top-concavity '' that real data exhibits ), as well as short-term Poisson behavior ; ( b ) avoids the `` i, d fallacy '' which none of the prevailing models have studied before ; and ( c ) is extremely parsimonious, requiring usually only one, and in general at most two parameters. Experiments conducted on eight large, diverse real datasets ( e, Youtube and blog comments, e-mails SMSs etc ).
2K_dev_192	The difference is that hybrid games also provide all the features of hybrid systems and discrete games, but only deterministic differential equations, Differential games instead provide differential equations with continuous-time game input by both players, but not the luxury of hybrid games, such as mode switches and discrete-time or alternating adversarial interaction. For the combined dynamics of differential hybrid games It shows how hybrid games subsume differential games for proving properties of differential games inductively. This article introduces differential hybrid games, which combine differential games with hybrid games, In both kinds of games, two players interact with continuous dynamics This article augments differential game logic with modalities and introduces differential game invariants and differential game variants.
2K_dev_195	Generative score spaces provide a principled method to exploit generative information, data distribution and hidden variables, The underlying methodology is to derive measures or score functions from generative models, The derived score functions, spanning the so-called score space, provide features of a fixed dimension for discriminative classification. Which is essentially the sufficient statistics of the adopted generative models and does not involve the parameters of generative models, for the score space that seeks to utilize label information. Shows that performance of the score space approach coupled with the proposed discriminative learning method is competitive with state-of-the-art classification methods. In this paper we propose a simple yet effective score space We further propose a discriminative learning method by constraining the classification margin over the score space, The form of score function allows the formulation of simple learning rules, which are essentially the same learning rules for a generative model with an extra posterior imposed over its hidden variables. Experimental evaluation of this approach over two generative models.
2K_dev_200	For differential dynamic logic ( dL ) for differential dynamic logic to internalize differential invariants, differential substitutions and derivations as first-class axioms in dL. This paper introduces a new proof calculus that is entirely based on uniform substitution, a proof rule that substitutes a formula for a predicate symbol everywhere, Uniform substitutions make it possible to rely on axioms rather than axiom schemata, Instead of nontrivial schema variables and soundness-critical side conditions on the occurrence patterns of variables, the resulting calculus adopts only a finite number of ordinary dL formulas as axioms, The static semantics of differential dynamic logic is captured exclusively in uniform substitutions and bound variable renamings as opposed to being spread in delicate ways across the prover implementation In addition to sound uniform substitutions, this paper introduces differential forms that make it possible.
2K_dev_202	Hidden information derived from probabilistic generative models of data distributions can be used to construct features for discriminative classifiers, This observation has motivated the development of approaches that attempt to couple generative and discriminative models together for classification. However existing approaches typically feed features derived from generative models to discriminative classifiers, and do not refine the generative models or the feature mapping functions based on classification results, to improve the classifier 's performance. This new framework produces a general classification tool with state-of-the-art performance. In this paper we propose a coupling mechanism developed under the PAC-Bayes framework that can fine-tune the generative models and the feature mapping functions iteratively In our approach, a stochastic feature mapping, which is a function over the random variables of a generative model, is derived to generate feature vectors for a stochastic classifier, We construct a stochastic classifier over the feature mapping and derive the PAC-Bayes generalization bound for the classifier, for both supervised and semi-supervised learning This allows us to jointly learn the feature mapping and the classifier by minimizing the bound with an EM-like iterative algorithm using labeled and unlabeled data, The resulting framework integrates the learning of the discriminative classifier and the generative model and allows iterative fine-tuning of the generative models, and the feedforward feature mappings based on task performance feedback. Our experiments show in three distinct applications.
2K_dev_203	Community detection plays a key role in understanding the structure of real-life graphs with impact on recommendation systems, load balancing and routing, Previous community detection methods look for uniform blocks in adjacency matrices. What do real communities in social networks look like ? as a better representation of communities and the relationships between their members, to detect communities with hyperbolic structure. We provide empirical evidence that communities are best represented as having an hyperbolic structure, We show that our method is effective in finding communities with a similar structure to self-declared ones. We detail HyCoM - the Hyperbolic Community Model - and show improvements in compression compared to standard methods We also introduce HyCoM-FIT, a fast parameter free algorithm. However after studying four real networks with ground-truth communities in real social networks, including a community in a blogging platform with over 34 million edges in which more than 1000 users established over 300 000 relations.
2K_dev_205	To encode the global relationship context of visual events across time and space to use the contextual information to modulate the analysis. We show that our model can outperform state-of-art performances of gated Boltzmann machines ( GBM ) Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information We show it achieves state-of-art performances and possesses the ability to interpolate missing frames, a function that is lacking in GBM. We propose a new neurally-inspired model that can learn and by synthesis process in a predictive coding framework, The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes, In contrast to standard predictive coding models, the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer, and is thus more consistent with neurophysiological observations. We establish the computational feasibility of this model by demonstrating its ability in several aspects, in estimation of contextual information, in terms of prediction accuracy in a variety of tasks.
2K_dev_206	We address the problem how high-fidelity verification results about the hybrid systems dynamics of cyber-physical flow systems can be provided at the scale of large ( traffic ) networks without prohibitive analytic cost, for traffic flow components. We propose the use of contracts concisely capturing the conditions for a safe operation in the context of a traffic network This reduces the analysis of flows in the full traffic network to simple arithmetic checks of the local compatibility of the traffic component contracts, while retaining higher-fidelity correctness guarantees of the global hybrid systems models that inherits from correct contracts of the hybrid system components. We evaluate our approach in a case study of a modular traffic network and a prototypical implementation in a model-based analysis and design tool for traffic flow networks.
2K_dev_207	Background Computer-assisted diagnosis of dermoscopic images of skin lesions has the potential to improve melanoma early detection, Conclusions Our classifier may aid clinicians in deciding if a skin lesion should be biopsied and can easily be incorporated into a portable tool ( that uses no proprietary equipment ) that could aid clinicians in noninvasively evaluating cutaneous lesions. Objective to generate a lesion severity score. Results The classifier sensitivity for melanoma was 97, 4 % ; specificity was 44, 2 % in a test set of images, In the reader study, the classifier 's sensitivity to melanoma was higher ( P P Limitations This is a retrospective study using existing images primarily chosen for biopsy by a dermatologist, The size of the test set is small. We sought to evaluate the performance of a novel classifier that uses decision forest classification of dermoscopic images. Methods Severity scores were calculated for 173 dermoscopic images of skin lesions with known histologic diagnosis ( 39 melanomas, 14 nonmelanoma skin cancers, and 120 benign lesions ), A threshold score was used to measure classifier sensitivity and specificity A reader study was conducted to compare the sensitivity and specificity of the classifier with those of 30 dermatology clinicians.
2K_dev_212	The use of deductive techniques, such as theorem provers, has several advantages in safety verification of hybrid systems ; however, state-of-the-art theorem provers require manual intervention to handle complex systems. Furthermore there is often a gap between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide directly for differential dynamic logic allows local reasoning to discover useful forward invariants to complete verification tasks. This paper presents an extension to KeYmaera, a deductive verification tool ; the new technique using system designer intuition about performance within particular modes as part of a proof task, Our approach allows the theorem prover to leverage forward invariants, discovered using numerical techniques, as part of a proof of safety, We introduce a new inference rule into the proof calculus of KeYmaera, the forward invariant cut rule, and we present a methodology, which are then used with the new cut rule. We demonstrate how our new approach can be used to complete verification tasks that lie out of the reach of existing automatic verification approaches using several examples, including one involving an automotive powertrain control system.
2K_dev_216	Given a large cloud of multi-dimensional points, and an off-the shelf outlier detection method, why does it take a week to finish ? to eliminate the problem. We discovered that duplicate points create subtle issues, that the literature has ignored : if d max is the multiplicity of the most over-plotted point, typical algorithms are quadratic on d max, we report wall-clock times and our time savings ; and we show that our methods give either exact results, or highly accurate approximate ones. We propose several ways ;.
2K_dev_218	To quantify sharpness of tuning. We propose using the statistical measurement of the sample skewness of the distribution of mean firing rates of a tuning curve For some features, like binocular disparity tuning curves are best described by relatively complex and sometimes diverse functions, making it difficult to quantify sharpness with a single function and parameter, Skewness provides a robust nonparametric measure of tuning curve sharpness that is invariant with respect to the mean and variance of the tuning curve and is straightforward to apply to a wide range of tuning, including simple orientation tuning curves and complex object tuning curves that often can not even be described parametrically, Because skewness does not depend on a specific model or function of tuning, it is especially appealing to cases of sharpening where recurrent interactions among neurons produce sharper tuning curves that deviate in a complex manner from the feedforward function of tuning, Since tuning curves for all neurons are not typically well described by a single parametric function, this model independence additionally allows skewness to be applied to all recorded neurons, maximizing the statistical power of a set of data We also compare skewness with other nonparametric measures of tuning curve sharpness and selectivity Compared to these other nonparametric measures tested, skewness is best used for capturing the sharpness of multimodal tuning curves defined by narrow peaks maximum and broad valleys minima Finally, we provide a more formal definition of sharpness using a shape-based information gain measure and derive and show that skewness is correlated with this definition.
2K_dev_219	How can we describe a large, dynamic graph over time ? Is it random ? If not, what are the most apparent deviations from randomness -- a dense block of actors that persists over time, or perhaps a star with many satellite nodes that appears with some fixed periodicity ? In practice, these deviations indicate patterns -- for example, botnet attackers forming a bipartite core with their victims over the duration of an attack, family members bonding in a clique-like fashion over a difficult period of time, or research collaborations forming and fading away over the years. Which patterns exist in real-world dynamic graphs, and how can we find and rank them in terms of importance ? These are exactly the problems we focus on in this work, Our main contributions are. We show that TIMECRUNCH is able to compress these graphs. ( a ) formulation : we show how to formalize this problem as minimizing the encoding cost in a data compression paradigm, ( b ) algorithm : we propose TIMECRUNCH, an effective scalable and parameter-free method for finding coherent, temporal patterns in dynamic graphs and ( c ) practicality : by summarizing important temporal structures and finds patterns that agree with intuition. We apply our method to several large, diverse real-world datasets with up to 36 million edges and 6.
2K_dev_223	Modern offices are crowded with personal computers, While studies have shown these to be idle most of the time, they remain powered consuming up to 60p of their peak power Hardware-based solutions engendered by PC vendors ( e, low-power states Wake-on-LAN ) have proved unsuccessful because, in spite of user inactivity, these machines often need to remain network active in support of background applications that maintain network presence. Recent proposals have advocated the use of consolidation of idle desktop Virtual Machines ( VMs ), However desktop VMs are often large, requiring gigabytes of memory, Consolidating such VMs creates large network transfers lasting in the order of minutes and utilizes server memory inefficiently, When multiple VMs migrate concurrently, networks become congested and the resulting migration latencies are prohibitive, that transparently migrates only the working set of an idle VM. Can deliver 44 -- 91p energy savings during idle periods of at least 10 minutes, while providing low migration latencies of about 4 seconds and migrating minimal state that is under an order of magnitude of the VMs memory footprint. We present partial VM migration, an approach It creates a partial replica of the desktop VM on the consolidation server by copying only VM metadata, and it transfers pages to the server on-demand, as the VM accesses them, This approach places desktop PCs in low-power mode when inactive and switches them to running mode when pages are needed by the VM running on the consolidation server, To ensure that desktops save energy, we have developed sleep scheduling and prefetching algorithms, as well as the context-aware selective resume framework, a novel approach to reduce the latency of power mode transition operations in commodity PCs. Jettison our software prototype of partial VM migration for off-the-shelf PCs.
2K_dev_226	These problems are motivated by the LASSO framework and have applications in machine learning and computer vision. We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems, Our work shows a close connection between these problems and core questions in algorithmic graph theory. While this connection demonstrates the difficulties of obtaining runtime guarantees, it also suggests an approach of using techniques originally developed for graph algorithms We then show that most of these problems can be formulated as a grouped least squares problem, and give efficient algorithms for this formulation Our algorithms rely on routines for solving quadratic minimization problems, which in turn are equivalent to solving linear systems. Some preliminary experimental work on image processing tasks are also presented.
2K_dev_233	Consider networks in harsh environments, where nodes may be lost due to failure, attack or infection -- how is the topology affected by such events ?. Can we mimic and measure the effect ? to evaluate robustness to construct secure networks operating within malicious environments. We propose a new generative model of network evolution in dynamic and harsh environments, Our model can reproduce the range of topologies observed across known robust and fragile biological networks, as well as several additional transport, communication and social networks, We also develop a new optimization measure based on preserving high connectivity following random or adversarial bursty node loss, propose a new distributed algorithm. Using this measure we evaluate the robustness of several real-world networks and.
2K_dev_243	Modern robots like todays smartphones, are complex devices with intricate software systems. Introductory robot programming courses must evolve to reflect this reality, by teaching students to make use of the sophisticated tools their robots provide rather than reimplementing basic algorithms. This paper focuses on teaching with Tekkotsu, an open source robot application development framework designed specifically for education But, the curriculum described here can also be taught using ROS, the Robot Operating System that is now widely used for robotics research.
2K_dev_244	As airspace becomes ever more crowded, air traffic management must reduce both space and time between aircraft to increase throughput, making on-board collision avoidance systems ever more important, These safety-critical systems must be extremely reliable, and as such many resources are invested into ensuring that the protocols they implement are accurate, Still it is challenging to guarantee that such a controller works properly under every circumstance, This is an important step in formally verified, flyable and distributed air traffic control. In tough scenarios where a large number of aircraft must execute a collision avoidance maneuver, a human pilot under stress is not necessarily able to understand the complexity of the distributed system and may not take the right course, especially if actions must be taken quickly. We prove that the controllers never allow the aircraft to get too close to one another, even when new planes approach an in-progress avoidance maneuver that the new plane may not be aware of, Because these safety guarantees always hold, the aircraft are protected against unexpected emergent behavior which simulation and testing may miss. We consider a class of distributed collision avoidance controllers designed to work even in environments with arbitrarily many aircraft or UAVs.
2K_dev_247	The importance of studying the implications of sampling is twofold : First, sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second, because every such data collection can be perceived as a sample of the real world To the best of our knowledge, our work represents the largest study of propagation patterns of executables. How does malware propagate ? Does it form spikes over time ? Does it resemble the propagation pattern of benign files, such as software patches ? Does it spread uniformly over countries ? How long does it take for a URL that distributes malware to be detected and shut down ? In this work, we answer these questions to efficiently extrapolate crucial properties of the data from a small sample. We discover the SharkFin temporal propagation pattern of executable files, the GeoSplit pattern in the geographical spread of machines that report executables to Symantecs servers, the Periodic Power Law ( Ppl ) distribution of the lifetime of URLs, and we show how. By analyzing patterns from 22 million malicious ( and benign ) files, 6 million hosts worldwide during the month of June 2011 We conduct this study using the WINE database available at Symantec Research Labs Additionally, we explore the research questions raised by sampling on such large databases of executables ; We further investigate the propagation pattern of benign and malicious executables, unveiling latent structures in the way these files spread.
2K_dev_249	Our approach is not limited to images, but they provide a convenient query space to test search optimizations. To opportunistic near real-time search of untagged images on smartphones. We present a cloud-based approach that is sensitive to bandwidth and energy constraints Our approach is inspired by the long-established practice of photographers using contact sheets to rapidly visualize a new collection of photographs, and then selecting a subset on which to focus attention, On behalf of each smartphone, the cloud maintains a virtual contact sheet of images that have been captured but not yet uploaded, The virtual contact sheet consists of a set of low-fidelity images as well as full or partial meta-data associated with each image, If search processing on the cloud indicates that a particular low-fidelity object is relevant, then its full-fidelity image can be obtained just-in-time from the corresponding smartphone for further search processing or presentation to the user.
2K_dev_254	Although widely touted as a replacement for glass slides and microscopes in pathology, digital slides present major challenges in data storage, transmission processing and interoperability OpenSlide is in use today by many academic and industrial organizations world-wide, including many research sites in the United States that are funded by the National Institutes of Health. Since no universal data format is in widespread use for these images today, each vendor defines its own proprietary data formats, analysis tools viewers and software libraries, This creates issues not only for pathologists, but also for interoperability for reading and manipulating digital slides of diverse vendor formats. Can transparently handle multiple vendor formats. In this paper we present the design and implementation of OpenSlide, a vendor-neutral C library The library is extensible and easily interfaced to various programming languages. An application written to the OpenSlide interface.
2K_dev_262	As part of a collaboration with a major California school district. We study the problem of fairly allocating unused classrooms in public schools to charter schools. Show that a nontrivial implementation of the leximin mechanism scales gracefully in terms of running time ( even though the problem is intractable in theory ), and performs extremely well with respect to a number of efficiency objectives. Our approach revolves around the randomized leximin mechanism We extend previous work to the classroom allocation setting, showing that the leximin mechanism is proportional, envy-free efficient and group strategyproof, We also prove that the leximin mechanism provides a ( worst-case ) 4-approximation to the maximum number of classrooms that can possibly be allocated, We take great pains to establish the practicability of our approach, and discuss issues related to its deployment. Our experiments which are based on real data.
2K_dev_264	Given a set of k networks, possibly with different sizes and no overlaps in nodes or links, how can we quickly assess similarity between them ? Analogously, are there a set of social theories which, when represented by a small number of descriptive, numerical features effectively serve as a `` signature '' for the network ?. Having such signatures will enable a wealth of graph mining and social network analysis tasks, including clustering outlier detection, visualization etc for solving the above problem. NETSIMILE outperforms baseline competitors. We propose a novel, effective and scalable method, called NETSIMILE Our approach has the following desirable properties : ( a ) It is supported by a set of social theories, ( b ) It gives similarity scores that are size-invariant, ( c ) It is scalable, being linear on the number of links for graph signature extraction our approach enables several mining tasks such as clustering, visualization discontinuity detection network transfer learning, and re-identification across networks. In extensive experiments on numerous synthetic and real networks from disparate domains, We also demonstrate how.
2K_dev_265	Short-term forecasting is a ubiquitous practice in a wide range of energy systems, including forecasting demand renewable generation. Although it is known that probabilistic forecasts ( which give a distribution over possible future outcomes ) can improve planning and control, many forecasting systems in practice are just used as point forecast tools, as it is challenging to represent high-dimensional non-Gaussian distributions over multiple spatial and temporal points. We show that this probabilistic model greatly outperforms other methods on the task of accurately modeling potential distributions of power ( as would be necessary in a stochastic dispatch problem. In this paper we apply a recently-proposed algorithm for modeling high-dimensional conditional Gaussian distributions to forecasting wind power and extend it to the non-Gaussian case using the copula transform. On a wind power forecasting task.
2K_dev_268	Question answering ( Q & A ) communities have been gaining popularity in the past few years, The success of such sites depends mainly on the contribution of a small number of expert users who provide a significant portion of the helpful answers. And so identifying users that have the potential of becoming strong contributers is an important task for owners of such communities, for detecting influential and anomalous users in the underlying user interaction network. Interestingly we find that while the majority of questions on the site are asked by low reputation users, on average a high reputation user asks more questions than a user with low reputation and find they are effective in detecting extreme behaviors such as those of spam users we predict who will become influential long-term contributors. We consider a number of graph analysis methods. We present a study of the popular Q & A website StackOverflow ( SO ), in which users ask and answer questions about software development, algorithms math and other technical topics, The dataset includes information on 3, 5 million questions and 6, 9 million answers created by 1, 3 million users in the years 2008 -- 2012, Participation in activities on the site ( such as asking and answering questions ) earns users reputation, which is an indicator of the value of that user to the site, We describe an analysis of the SO reputation system, and the participation patterns of high and low reputation users, The contributions of very high reputation users to the site indicate that they are the primary source of answers, and especially of high quality answers, Lastly we show an application of our analysis : by considering user contributions over first months of activity on the site.
2K_dev_273	Kidney exchanges allow incompatible donor-patient pairs to swap kidneys, but each donation must pass three tests : blood, tissue and crossmatch In practice a matching is computed based on the first two tests, and then a single crossmatch test is performed for each matched patient. However if two crossmatches could be performed per patient, in principle significantly more successful exchanges could take place, In this paper we ask : If we were allowed to perform two crossmatches per patient, could we harness this additional power optimally and efficiently ? for this problem. Our main result is a polynomial time algorithm that almost surely computes optimal -- - up to lower order terms -- - solutions on random large kidney exchange instances.
2K_dev_275	Recent computer systems research has proposed using redundant requests to reduce latency, The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request ). However there is no exact analysis of systems with redundancy, This paper presents the first exact analysis of systems with redundancy. We find some surprising results, First the response time of a fully redundant class follows a simple Exponential distribution and that of the non-redundant class follows a Generalized Hyperexponential, Second fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant, We find that in many cases, redundancy outperforms JSQ and Opt-Split with respect to overall response time, making it an attractive solution. We allow for any number of classes of redundant requests, any number of classes of non-redundant requests, any degree of redundancy, and any number of heterogeneous servers, In all cases we derive the limiting distribution on the state of the system, In small ( two or three server ) systems, we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes, and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy We also compare redundancy with other approaches for reducing latency, such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and Join-the-Shortest-Queue ( JSQ ) routing of a class.
2K_dev_279	For example the Project Tycho provides open access to the count infections for U, states from 1888 to 2013, for 56 contagious diseases ( e, measles influenza ) which include missing values, possible recording errors sudden spikes ( or dives ) of infections. Given a large collection of epidemiological data consisting of the count of d contagious diseases for l locations of duration n, how can we find patterns, rules and outliers ? So how can we find a combined model, for all these diseases, locations and time-ticks ? for large scale epidemiological data which solves the above problem. Demonstrate that FUNNELFIT does indeed discover important properties of epidemics : ( P1 ) disease seasonality, influenza spikes in January, Lyme disease spikes in July and the absence of yearly periodicity for gonorrhea ; ( P2 ) disease reduction effect, the appearance of vaccines ; ( P3 ) local/state-level sensitivity, many measles cases in NY ; ( P4 ) external shock events, historical flu pandemics ; ( P5 ) detect incongruous values. In this paper we present FUNNEL, a unifying analytical model as well as a novel fitting algorithm, FUNNELFIT Our method has the following properties : ( a ) Sense-making : it detects important patterns of epidemics, such as periodicities the appearance of vaccines, external shock events and more ; ( b ) Parameter-free : our modeling framework frees the user from providing parameter values ; ( c ) Scalable : FUNNELFIT is carefully designed to be linear on the input size ; ( d ) General : our model is general and practical, which can be applied to various types of epidemics, including computer-virus propagation as well as human diseases. Extensive experiments on real data.
2K_dev_281	The M/M/k/setup model where there is a penalty for turning servers on, is common in data centers, call centers and manufacturing systems, Setup costs take the form of a time delay, and sometimes there is additionally a power penalty, as in the case of data centers. While the M/M/1/setup was exactly analyzed in 1964, no exact analysis exists to date for the M/M/k/setup with $ $ k > 1 $ $ k In this paper, we provide the first exact, closed-form analysis for the M/M/k/setup and some of its important variants including systems in which idle servers delay for a period of time before turning off or can be put to sleep. By a new way of combining renewal reward theory and recursive techniques to solve Markov chains with a repeating structure, Our renewal-based approach uses ideas from renewal reward theory and busy period analysis to obtain closed-form expressions for metrics of interest such as the transform of time in system and the transform of power consumed by the system, The simplicity intuitiveness and versatility of our renewal-based approach makes it useful for analyzing Markov chains far beyond the M/M/k/setup, In general our renewal-based approach should be used to reduce the analysis of any 2-dimensional Markov chain which is infinite in at most one dimension and repeating to the problem of solving a system of polynomial equations, In the case where all transitions in the repeating portion of the Markov chain are skip-free and all up/down arrows are unidirectional, the resulting system of equations will yield a closed-form solution. Our analysis is made possible.
2K_dev_286	If Lisa visits Dr, Brown and there is no record of the drug he prescribed her, can we find it ? Data sources, much to analysts ' dismay, are too often plagued with incompleteness, making business analytics over the data difficult. Data entries with incomplete values are ignored, making some analytic queries fail to accurately describe how an organization is performing to choose a correct value after viewing possible values and why they were inferred. We introduce a principled way of performing value imputation on missing values, allowing a user We achieve this by turning our data into a graph network and performing link prediction on nodes of interest using the belief propagation algorithm.
2K_dev_287	A cognitive assistance application combines a wearable device such as Google Glass with cloudlet processing to provide step-by-step guidance on a complex task, We then reflect on the difficulties we faced in building these applications, and suggest future research that could simplify the creation of similar applications. For narrow and well-defined tasks that require specialized knowledge and/or skills. In this paper we focus on user assistance We describe proof-of-concept implementations for four different tasks : assembling 2D Lego models, freehand sketching playing ping-pong, and recommending context-relevant YouTube tutorials.
2K_dev_289	Let us consider that someone is starting a research on a topic that is unfamiliar to them. Which seminal papers have influenced the topic the most ? What is the genealogy of the seminal papers in this topic ? These are the questions that they can raise, which we try to answer in this paper, that finds a set of seminal papers on a given topic, that constructs a genealogy of the seminal papers. We show the effectiveness and efficiency of our approach. First we propose an algorithm We also address the performance and scalability issues of this sophisticated algorithm Next, we discuss the measures to decide how much a paper is influenced by another paper, Then we propose an algorithm by using the influence measure and citation information. Finally through extensive experiments with a large volume of a real-world academic literature data.
2K_dev_291	As kidney exchange programs are growing, manipulation by hospitals becomes more of an issue. Assuming that hospitals wish to maximize the number of their own patients who receive a kidney, they may have an incentive to withhold some of their incompatible donorpatient pairs and match them internally, thus harming social welfare, for hospitals to report all their incompatible pairs. Suggest that in practice our mechanism performs much closer to optimal. We study mechanisms for two-way exchanges that are strategyproof, make it a dominant strategy We establish lower bounds on the welfare loss of strategyproof mechanisms, both deterministic and randomized, and propose a randomized mechanism that guarantees at least half of the maximum social welfare in the worst case. Simulations using realistic distributions for blood types and other parameters.
2K_dev_295	For monitoring virtual machines ( VMs ) in the cloud. We propose a non-intrusive approach At the core of this approach is a mechanism for selective real-time monitoring of guest file updates within VM instances, This mechanism is agentless, requiring no guest VM support, It has low virtual I/O overhead, low latency for emitting file updates, and a scalable design, Its central design principle is distributed streaming of file updates inferred from introspected disk sector writes, The mechanism called DS-VMI, enables many system administration tasks that involve monitoring files to be performed outside VMs.
2K_dev_299	Imply improved parallel randomized algorithms for several problems, including single-source shortest paths, maximum flow minimum-cost flow, and approximate maximum flow. For solving symmetric diagonally dominant ( SDD ) linear systems. We present the design and analysis of a nearly-linear work parallel algorithm On input an SDD n-by-n matrix A with m nonzero entries and a vector b, our algorithm computes a vector $ \tilde { x } $ such that $ \|\tilde { x } - A^ { + } b\|_ { A } \leq\varepsilon\cdot\| { A^ { + } b } \|_ { A } $ in $ O ( m\log^ { O ( 1 ) } { n } \log { \frac { 1 } { \varepsilon } } ) $ work and $ O ( m^ { 1/3+\theta } \log\frac { 1 } { \varepsilon } ) $ depth for any ? > 0, where A + denotes the Moore-Penrose pseudoinverse of A, The algorithm relies on a parallel algorithm for generating low-stretch spanning trees or spanning subgraphs To this end, we first develop a parallel decomposition algorithm that in O ( mlog O ( 1 ) n ) work and polylogarithmic depth, partitions a graph with n nodes and m edges into components with polylogarithmic diameter such that only a small fraction of the original edges are between the components, This can be used to generate low-stretch spanning trees with average stretch O ( n ? ) in O ( mlog O ( 1 ) n ) work and O ( n ? ) depth for any ? > 0, Alternatively it can be used to generate spanning subgraphs with polylogarithmic average stretch in O ( mlog O ( 1 ) n ) work and polylogarithmic depth, We apply this subgraph construction to derive a parallel linear solver. By using this solver in known applications.
2K_dev_301	A password composition policy restricts the space of allowable passwords to eliminate weak passwords that are vulnerable to statistical guessing attacks. Usability studies have demonstrated that existing password composition policies can sometimes result in weaker password distributions ; hence a more principled approach is needed, for optimizing password composition policies. We introduce the first theoretical model Our main positive result is an algorithm that -- with high probability -- - constructs almost optimal policies ( which are specified as a union of subsets of allowed passwords ), and requires only a small number of samples of users ' preferred passwords. We study the computational and sample complexity of this problem under different assumptions on the structure of policies and on users ' preferences over passwords, We complement our with simulations using a real-world dataset of 32 million passwords.
2K_dev_310	This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities. To develop integrate and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations ' information systems, to detect independently developed red team inserts of malicious insider activities. We defined over 100 data features in seven categories We have achieved area under the ROC curve values of up to 0, 979 and lift values of 65 on the top 50 user-days identified on two months of real data. Our system combines structural and semantic information from a real corporate database of monitored activity on their users ' computers We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns temporal sequences, and normal graph evolution Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams, We have also developed a visual language for specifying combinations of features, baselines peer groups time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. Based on approximately 5, 5 million actions per day from approximately 5.
2K_dev_312	For computing the Voronoi diagram of a set of n points in constant-dimensional Euclidean space. We describe a new algorithm The running time of our algorithm is O ( f log n log ) where f is the output complexity of the Voronoi diagram and is the spread of the input, the ratio of largest to smallest pairwise distances, Despite the simplicity of the algorithm and its analysis, it improves on the state of the art for all inputs with polynomial spread and near-linear output size, The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation Then, the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity, yielding the output sensitive bound, The removal only involves local flips and is inspired by kinetic data structures.
2K_dev_313	Color descriptors are one of the important features used in content-based image retrieval, The dominant color descriptor ( DCD ) represents a few perceptually dominant colors in an image through color quantization, For image retrieval based on DCD, the earth movers distance ( EMD ) and the optimal color composition distance were proposed to measure the dissimilarity between two images. Although providing good retrieval results, both methods are too time-consuming to be used in a large image database, To solve the problem. The results reveal that our approach achieves almost the same results with the EMD in linear time. We propose a new distance function that calculates an approximate earth movers distance in linear time To calculate the dissimilarity in linear time, the proposed approach employs the space-filling curve for multidimensional color space, To improve the accuracy, the proposed approach uses multiple curves and adjusts the color positions, As a result our approach achieves order-of-magnitude time improvement but incurs small errors. We have performed extensive experiments to show the effectiveness and efficiency of the proposed approach.
2K_dev_323	Current applications have produced graphs on the order of hundreds of thousands of nodes and millions of edges, To take advantage of such graphs, one must be able to find patterns, These tasks are better performed in an interactive environment, where human expertise can guide the process. For large graphs though, there are some challenges : the excessive processing requirements are prohibitive, and drawing hundred-thousand nodes results in cluttered images hard to comprehend, To cope with these problems. We propose an innovative framework suited for any kind of tree-like graph visual design GMine integrates 1 ) a representation for graphs organized as hierarchies of partitions-the concepts of SuperGraph and Graph-Tree ; and 2 ) a graph summarization methodology-CEPS Our graph representation deals with the problem of tracing the connection aspects of a graph hierarchy with sub linear complexity, allowing one to grasp the neighborhood of a single node or of a group of nodes in a single click. As a proof of concept, the visual environment of GMine is instantiated as a system in which large graphs can be investigated globally and locally.
2K_dev_333	Existing methods are typically superlinear in space or execution time. That looks for clusters in subspaces of multidimensional data. Effectiveness : it is accurate, providing results with equal or better quality compared to top related works Halite was in average at least 12 times faster than seven representative works, and always presented highly accurate results, Halite was at least 11 times faster than others, increasing their accuracy in up to 35 percent. This paper proposes Halite, a novel fast and scalable clustering method Halite 's strengths are that it is fast and scalable, while still giving highly accurate results, Specifically the main contributions of Halite are : 1 ) Scalability : it is linear or quasi linear in time and space regarding the data size and dimensionality, and the dimensionality of the clusters ' subspaces ; 2 ) Usability : it is deterministic, robust to noise does n't take the number of clusters as an input parameter, and detects clusters in subspaces generated by original axes or by their linear combinations, including space rotation ; 3 ) ; and 4 ) Generality : it includes a soft clustering approach. Experiments on synthetic data ranging from five to 30 axes and up to 1 \rm million points were performed On real data Finally, we report experiments in a real scenario where soft clustering is desirable.
2K_dev_341	Computers are often used in performance of popular music, but most often in very restricted ways, such as keyboard synthesizers where musicians are in complete control, or pre-recorded or sequenced music where musicians follow the computer 's drums or click track, An interesting and yet little-explored possibility is the computer as highly autonomous performer of popular music, capable of joining a mixed ensemble of computers and humans. Considering the skills and functional requirements of musicians leads to a number of predictions about future humancomputer music performance ( HCMP ) systems for popular music for such systems. And our experience with them. We describe a general architecture. And describe some early implementations.
2K_dev_343	How can we detect suspicious users in large online networks ? Online popularity of a user or product ( via follows, ) can be monetized on the premise of higher ad click-through rates or increased sales, Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck, Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent ( but sometimes honest ) users. However small-scale stealthy attacks may go unnoticed due to the nature of low-rank Eigen analysis used in practice, In this work we take an adversarial approach to find and prove claims about the weaknesses of modern, state-of-the-art spectral methods to catch small-scale, stealth attacks that slip below the radar. ( b ) it is shown to be highly effective on real data and and with high precision identify many suspicious accounts which have persisted without suspension even to this day. And propose fBox an algorithm designed Our algorithm has the following desirable properties : ( a ) it has theoretical underpinnings, ( c ) it is scalable ( linear on the input size ). We evaluate fBox on a large, 7 million node 1, 5 billion edge who-follows-whom social graph from Twitter in 2010.
2K_dev_345	A well-studied approach to the design of voting rules views them as maximum likelihood estimators ; given votes that are seen as noisy estimates of a true ranking of the alternatives, the rule must reconstruct the most likely true ranking. We argue that this is too stringent a requirement, and instead ask : How many votes does a voting rule need to reconstruct the true ranking ?. And show that for all rules in this family the number of samples required from the Mallows noise model is logarithmic in the number of alternatives, and that no rule can do asymptotically better ( while some rules like plurality do much worse ) and find voting rules that are accurate in the limit for all noise models in such general families. We define the family of pairwise-majority consistent rules Taking a more normative point of view, we consider voting rules that surely return the true ranking as the number of samples tends to infinity ( we call this property accuracy in the limit ) ; this allows us to move to a higher level of abstraction We characterize the distance functions that induce noise models for which pairwise-majority consistent rules are accurate in the limit, and provide a similar result for another novel family of position-dominance consistent rules These characterizations capture three well-known distance functions. We study families of noise models that are parametrized by distance functions.
2K_dev_346	The quality and effectiveness of the load following services provided by centralized control of thermostatically controlled loads depend highly on the communication requirements and the underlying cyberinfrastructure characteristics, Specifically ensuring end-user comfort while providing real-time demand response services depends on the availability of the information provided from the thermostatically controlled loads to the main controller regarding their operating statuses and internal temperatures. State estimation techniques can be used to infer the necessary information from the aggregate power consumption of these loads, replacing the need for an upstream communication platform carrying information from appliances to the main controller in real-time, as an alternative to a Kalman filter approach. The results show that some improvement is possible for scenarios when loads are expected to be toggled frequently. In this paper we introduce a moving horizon mean squared error state estimator with constraints which assumes a linear model without constraints.
2K_dev_349	Big graphs are everywhere, ranging from social networks and mobile call networks to biological networks and the World Wide Web Mining big graphs leads to many interesting applications including cyber security, fraud detection Web search, recommendation and many more. How do we find patterns and anomalies in very large graphs with billions of nodes and edges ? How to mine such big graphs efficiently ? a big graph mining system data processing platform. Our findings include anomalous spikes in the connected component size distribution, the 7 degrees of separation in a Web graph, and anomalous adult advertisers in the who-follows-whom Twitter social network. In this paper we describe Pegasus built on top of MapReduce, a modern distributed We introduce GIM-V, an important primitive that Pegasus uses for its algorithms to analyze structures of large graphs We also introduce HEigen, a large scale eigensolver which is also a part of Pegasus Both GIM-V and HEigen are highly optimized, achieving linear scale up on the number of machines and edges, 2x and 76x faster performance than their naive counterparts. Using Pegasus we analyze very large, real world graphs with billions of nodes and edges.
2K_dev_359	The convergence of mobile computing and cloud computing is predicated on a reliable, This basic requirement is hard to guarantee in hostile environments such as military operations and disaster recovery This article is part of a special issue on the edge of the cloud. In this article the authors examine can overcome this challenge. How VM-based cloudlets that are located in close proximity to associated mobile devices.
2K_dev_369	Offers provably fair solutions for the division of rent. Spliddit is a first-of-its-kind fair division website, which In this note, we discuss Spliddit 's goals.
2K_dev_370	Hybrid systems with both discrete and continuous dynamics are an important model for real-world cyber-physical systems, The key challenge is to ensure their correct functioning w, Promising techniques to ensure safety seem to be model-driven engineering to develop hybrid systems in a well-defined and traceable manner, and formal verification to prove their correctness, Their combination forms the vision of verification-driven engineering, Often hybrid systems are rather complex in that they require expertise from many domains ( e, robotics control systems computer science, software engineering and mechanical engineering ). Moreover despite the remarkable progress in automating formal verification of hybrid systems, the construction of proofs of complex systems often requires nontrivial human guidance, since hybrid systems verification tools solve undecidable problems, It is thus not uncommon for development and verification teams to consist of many players with diverse expertise, tools for ( 1 ) graphical ( UML ) and textual modeling of hybrid systems, ( 2 ) exchanging and comparing models and proofs, and ( 3 ) managing verification tasks to tackle large-scale verification tasks. This paper introduces a verification-driven engineering toolset that extends our previous work on hybrid and arithmetic verification with This toolset makes it easier.
2K_dev_372	Cake cutting is a common metaphor for the division of a heterogeneous divisible good, There are numerous papers that study the problem of fairly dividing a cake. ; a small number of them also take into account self-interested agents and consequent strategic issues, but these papers focus on fairness and consider a strikingly weak notion of truthfulness In this paper we investigate the problem of cutting a cake in a way that is truthful. Where for the first time our notion of dominant strategy truthfulness is the ubiquitous one in social choice and computer science, We design both deterministic and randomized cake cutting mechanisms that are truthful and fair under different assumptions with respect to the valuation functions of the agents.
2K_dev_373	Mobile crowdsensing is becoming a vital technique for environment monitoring, infrastructure management and social computing. However deploying mobile crowdsensing applications in large-scale environments is not a trivial task, It creates a tremendous burden on application developers as well as mobile users, In this paper we try to reveal the barriers hampering the scale-up of mobile crowdsensing applications. And to offer our initial thoughts on the potential solutions to lowering the barriers.
2K_dev_376	Rating data is ubiquitous on websites such as Amazon, Since ratings are not static but given at various points in time, a temporal analysis of rating data provides deeper insights into the evolution of a product 's quality. In this work we tackle the following question : Given the time stamped rating data for a product or service, how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous ? For learning our model. We show the effectiveness of our method and we present interesting discoveries on multiple real world datasets. We propose a Bayesian model that represents the rating data as sequence of categorical mixture models, In contrast to existing methods, our method does not require any aggregation of the input but it operates on the original time stamped data To capture the dynamic effects of the ratings, the categorical mixtures are temporally constrained : Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time, Our method automatically determines the intervals where anomalies occur, and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions we propose an efficient algorithm combining principles from variational inference and dynamic programming. In our experimental study.
2K_dev_378	Abstraction has emerged as a key component in solving extensive-form games of incomplete information, However lossless abstractions are typically too large to solve, so lossy abstraction is needed. All prior lossy abstraction algorithms for extensive-form games either 1 ) had no bounds on solution quality or 2 ) depended on specific equilibrium computation approaches, limited forms of abstraction, and only decreased the number of information sets rather than nodes in the game tree to give bounds on solution quality for any perfect-recall extensive-form game. Show that it finds a lossless abstraction when one is available and lossy abstractions when smaller abstractions are desired. We introduce a theoretical framework that can be used The framework uses a new notion for mapping abstract strategies to the original game, and it leverages a new equilibrium refinement for analysis, Using this framework we develop the first general lossy extensive-form game abstraction method with bounds While our framework can be used for lossy abstraction, it is also a powerful tool for lossless abstraction if we set the bound to zero, Prior abstraction algorithms typically operate level by level in the game tree, We introduce the extensive-form game tree isomorphism and action subset selection problems, both important problems for computing abstractions on a level-by-level basis We show that the former is graph isomorphism complete, and the latter NP-complete, We also prove that level-by-level abstraction can be too myopic and thus fail to find even obvious lossless abstractions.
2K_dev_387	The increase in the number of bloggers and the amount of information diffused in the blogosphere makes the blogosphere an important medium through which to communicate and exchange information, Accordingly the interest in understanding the nature of the information diffusion in the blogosphere has also been increased. Existing studies in social networks have mainly focused on the information diffusion through explicit relationships between members, In this paper we analyze the causes for the information diffusion without explicit relationships in the blogosphere. BlogCast a functionality provided by blog-service providers to expose a high quality post on the portal main page, is found to be one of the main causes of the information diffusion without explicit relationships. We analyze the characteristics of the information diffusion through the BlogCast and its halo effect on the bloggers whose post has been exposed on the portal main page, In addition we examine the sustainability of the halo effect of the BlogCast over time.
2K_dev_394	Autonomous agents that operate as components of dynamic spatial systems are becoming increasingly popular and mainstream, Applications can be found in consumer robotics, in road rail and air transportation, manufacturing and military operations. Unfortunately the approaches to modeling and analyzing the behavior of dynamic spatial systems are just as diverse as these application domains for the medium-term control of autonomous agents in dynamic spatial systems for integrating different approaches of dynamic spatial system analysis to achieve coverage of all required features. In this article we discuss reasoning approaches, which requires a sufficiently detailed description of the agents behavior and environment but may still be conducted in a qualitative manner, We introduce a conceptual reference model, which summarizes the current understanding of the characteristics of dynamic spatial systems based on a catalog of evaluation criteria derived from the model We provide a comparative summary of the modeling features, discuss lessons learned and introduce a research roadmap. We survey logic-based qualitative and hybrid modeling and commonsense reasoning approaches with respect to their features for describing and analyzing dynamic spatial systems in general, and the actions of autonomous agents operating therein in particular, We assess the modeling features provided by logic-based qualitative commonsense and hybrid approaches for projection, planning simulation and verification of dynamic spatial systems.
2K_dev_395	For decomposing an undirected unweighted graph into small diameter pieces. With the same asymptotic guarantees as the best sequential algorithm. We show an improved parallel algorithm with a small fraction of the edges in between These decompositions form critical subroutines in a number of graph algorithms, Our algorithm builds upon the shifted shortest path approach introduced in [ Blelloch, Gupta Koutis Miller Peng, Tangwongsan SPAA 2011 ], By combining various stages of the previous algorithm, we obtain a significantly simpler algorithm.
2K_dev_398	This can potentially lead to a deeper understanding of programming, bringing students closer to true computational thinking. We describe a three-stage model beginning with a simple, highly scaffolded programming environment ( Kodu ) and progressing to more challenging frameworks ( Alice and Lego NXT-G ), In moving between frameworks, students explore the similarities and differences in how concepts such as variables, conditionals and looping are realized Some novel strategies for teaching with Kodu are outlined. Finally we briefly report on our methodology and select preliminary results from a pilot study using this curriculum with students ages 10-17, including several with disabilities.
2K_dev_406	In a Stackelberg Security Game, a defender commits to a randomized deployment of security resources, and an attacker best-responds by attacking a target that maximizes his utility. While algorithms for computing an optimal strategy for the defender to commit to have had a striking real-world impact, deployed applications require significant information about potential attackers, We address this problem. Via an online learning approach, We are interested in algorithms that prescribe a randomized strategy for the defender at each step against an adversarially chosen sequence of attackers, and obtain feedback on their choices ( observing either the current attacker type or merely which target was attacked We design no-regret algorithms whose regret ( when compared to the best fixed strategy in hindsight ) is polynomial in the parameters of the game, and sublinear in the number of times steps.
2K_dev_422	For improving performance in VM-based mobile computing systems implemented as thick clients on host PCs, to speed up performance-critical operations. Results confirm that TransPart offers low overhead and startup cost, while improving user experience. This article investigates the transient use of free local storage We use the term TransientPC systems to refer to these types of systems, The solution we propose, called TransPart uses the higher-performing local storage of host hardware Our solution constructs a virtual storage device on demand ( which we call transient storage ) by borrowing free disk blocks from the hosts storage, In this article we present the design. And evaluation of a TransPart prototype, which requires no modifications to the software or hardware of a host computer Experimental.
2K_dev_425	Due to the recent penetration of distributed green energy, distributed intelligence and plug-in electric vehicles Finally, the proposed method can be implemented given recent advances in machine learning, which are becoming drivers and sources of data previously unavailable in the electric power industry. This paper is motivated by major needs for fast and accurate on-line data analysis tools in the emerging electric energy systems topology estimation approach for the smart grid. Results of the proposed method show that the new method produces a topology estimate excelling the current industrial approach. Instead of taking the traditional complex physical model based approach, this paper proposes a data-driven method, leading to an effective Specifically, we first introduce the data-driven topology estimation problem, Then a novel Logistic Kernel Regression is proposed in a Bayesian framework based on Nearest Neighbors search, Notably unlike many machine learning approaches that do not account for physical constraints, and distinctive from deterministic engineering modeling defined solely by physical laws, this paper for the first time combines the two into one single regression modeling for topology estimation.
2K_dev_427	Effortless one-touch capture of video is a unique capability of wearable devices such as Google Glass. To create a new type of crowd-sourced system. We use this capability in which users receive queries relevant to their current location and opt-in preferences, In response they can send back live video snippets of their surroundings, A system of result caching, geolocation and query similarity detection shields users from being overwhelmed by a flood of queries.
2K_dev_429	It is known that in this setting both revenue and social welfare can be maximized by a threshold policy, whereby customers are barred from entry once the queue length reaches a certain threshold, allowing for settings with multiple servers. We consider the social welfare model of Naor [ 20 ] and revenue-maximization model of Chen and Frank [ 7 ], where a single class of delay-sensitive customers seek service from a server with an observable queue, under state dependent pricing, However no explicit expression for this threshold has been found, for the ( maximum ) revenue under this optimal threshold. Finally we present a generalization of our results. This paper presents the first derivation of the optimal threshold in closed form, and a surprisingly simple formula Utilizing properties of the Lambert W function, we also provide explicit scaling results of the optimal threshold as the customer valuation grows.
2K_dev_434	Clustering is one of the fundamental data mining tasks. While traditional clustering techniques assign each object to a single cluster only, in many applications it has been observed that objects might belong to multiple clusters with different degrees, to tackle the challenge of mixed membership clustering for vector data For learning our model. We show the strengths of our novel clustering technique. In this work we present a Bayesian framework We exploit the ideas of subspace clustering where the relevance of dimensions might be different for each cluster, Combining the relevance of the dimensions with the cluster membership degree of the objects, we propose a novel type of mixture model able to represent data containing mixed membership subspace clusters, we develop an efficient algorithm based on variational inference allowing easy parallelization. In our empirical study on synthetic and real data.
2K_dev_443	To design faster parallel graph algorithms involving distances. We use exponential start time clustering Previous algorithms usually rely on graph decomposition routines with strict restrictions on the diameters of the decomposed pieces, We weaken these bounds in favor of stronger local probabilistic guarantees, This allows more direct analyses of the overall process, giving : Linear work parallel algorithms that construct spanners with O ( k ) stretch and size O ( n 1+1/ k ) in unweighted graphs, and size O ( n 1+1/ k log k ) in weighted graphs Hopsets that lead to the first parallel algorithm for approximating shortest paths in undirected graphs with O ( m poly log n ) work.
2K_dev_445	Disparity tuning measured in the primary visual cortex ( V1 ) is described well by the disparity energy model, but not all aspects of disparity tuning are fully explained by the model. Such deviations from the disparity energy model provide us with insight into how network interactions may play a role in disparity processing and help to solve the stereo correspondence problem, that provides a simple account of the observed deviations. Our model predicted sharper disparity tuning for larger stimuli, In this case our model predicted reduced sharpening and strength of inverted disparity tuning, the dynamics of disparity tuning observed from the neurophysiological recordings in macaque V1 matched model simulation predictions, Overall the results of this study support the notion that, while the disparity energy model provides a primary account of disparity tuning in V1 neurons, neural disparity processing in V1 neurons is refined by recurrent interactions among elements in the neural circuit. Here we propose a neuronal circuit model with recurrent connections The model is based on recurrent connections inferred from neurophysiological observations on spike timing correlations, and is in good accord with existing data on disparity tuning dynamics. We further performed two additional experiments to test predictions of the model, First we increased the size of stimuli to drive more neurons and provide a stronger recurrent input Second, we displayed anti-correlated stereograms, where dots of opposite luminance polarity are matched between the left- and right-eye images and result in inverted disparity tuning in the disparity energy model.
2K_dev_459	Machine-learning ( ML ) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses and facial recognition, In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al, adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs. Show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people 's faces given only their name and access to the ML model The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions, Our new attacks are applicable in a variety of settings, and we explore two in depth : decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition, In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values.
2K_dev_462	Given a large graph, like who-calls-whom or who-likes-whom, what behavior is normal and what should be surprising, possibly due to fraudulent activity ? How do graphs evolve over time ? How does influence/news/viruses propagate, over time ? We conclude with some open research questions for graph mining. We focus on three topics : ( a ) anomaly detection in large static graphs ( b ) patterns and anomalies in large time-evolving graphs and ( c ) cascades and immunization. As well as some discoveries such settings, For the third we show that for virus propagation, a single number is enough to characterize the connectivity of graph. For the first we present a list of static and temporal laws, including advances patterns like 'eigenspokes ' ; we show how to use them to spot suspicious activities, in on-line buyer-and-seller settings, in FaceBook in twitter-like networks, For the second we show how to handle time-evolving graphs as tensors, how to handle large tensors in map-reduce environments, thus we show how to do efficient immunization for almost any type of virus ( SIS - no immunity ; SIR - lifetime immunity ; etc ).
2K_dev_466	In order to reduce the offloading workload. We show that offload shaping can produce significant reduction in resource demand, with little loss of application-level fidelity. When offloading computation from a mobile device, we show that it can pay to perform additional on-device work We call this offload shaping. Demonstrate its application at many different levels of abstraction using a variety of techniques.
2K_dev_468	These interrelated issues underpin advanced correctness analysis in models of structured communications. We investigate strong normalization, confluence and behavioral equality in the realm of session-based concurrency, Strong normalization and confluence are established for session-typed processes. We prove that all proof conversions induced by the logic interpretation actually express observational equivalences, and explain how type isomorphisms resulting from linear logic equivalences are realized by coercions between interface types of session-based concurrent systems. By developing a theory of logical relations, Defined upon a linear type structure, our logical relations remain remarkably similar to those for functional languages, We also introduce a natural notion of observational equivalence Strong normalization and confluence come in handy in the associated coinductive reasoning :. The starting point for our study is an interpretation of linear logic propositions as session types for communicating processes, proposed in prior work.
2K_dev_469	Given a large social graph, what can we say about its robustness ? Broadly speaking, the property of robustness is crucial in real graphs, since it is related to the structural behavior of graphs to retain their connectivity properties after losing a portion of their edges/nodes, Can we estimate a robustness index for a graph quickly ? Additionally, if the graph evolves over time, how this property changes ?. In this work we are trying to answer the above questions studying the expansion properties of large social graphs. And we observe interesting properties for both static and time-evolving social graphs, we show how to spot outliers and anomalies in graphs over time, Finally we examine how graph generating models that mimic several properties of real-world graphs and behave in terms of robustness dynamics. First we present a measure that characterizes the robustness properties of a graph and also serves as global measure of the community structure ( or lack thereof ), We show how to compute this measure efficiently by exploiting the special spectral properties of real-world networks. We apply our method on several diverse real networks with millions of nodes, As an application example.
2K_dev_470	User provided rating data about products and services is one key feature of websites such as Amazon, Since these ratings are rather static but might change over time, a temporal analysis of rating distributions provides deeper insights into the evolution of a products ' quality. Given a time-series of rating distributions, in this work we answer the following questions : ( 1 ) How to detect the base behavior of users regarding a product 's evaluation over time ? ( 2 ) How to detect points in time where the rating distribution differs from this base behavior, due to attacks or spontaneous changes in the product 's quality ? To achieve these goals solving our objective. And we present interesting findings. We model the base behavior of users regarding a product as a latent multivariate autoregressive process, This latent behavior is mixed with a sparse anomaly signal finally leading to the observed data We propose an efficient algorithm. On various real world datasets.
2K_dev_476	Given a large image set, in which very few images have labels, how to guess labels for the remaining majority ? How to spot images that need brand new labels different from the predefined ones ? How to summarize these data to route the user 's attention to what really matters ? Here we answer all these questions, solution to two problems : ( i ) Low-labor labeling ( LLL ), find the most appropriate labels for the rest ; and ( ii ) Mining and attention routing - that best represent the data. QuMinS scales linearly on the data size, being up to 40 times faster than top competitors ( GCap ), still achieving better or equal accuracy, it spots images that potentially require unpredicted labels, and it works even with tiny initial label sets, nearly five examples to show that QuMinS is a viable tool for automatic coffee crop detection from remote sensing images. Specifically we propose QuMinS, a fast scalable - given an image set, very few images have labels in the same setting, find clusters the top-N '' O outlier images, and the N '' R images. Experiments on satellite images spanning up to 2, 25 GB show that, contrasting to the state-of-the-art labeling techniques, We also report a case study of our method 's practical usage.
2K_dev_477	We show how a disruptive force in mobile computing can be created. By extending todays unmodified cloud to a second level consisting of self-managed data centers with no hard state called cloudlets These are located at the edge of the Internet, just one wireless hop away from associated mobile devices By leveraging lowlatency offload, cloudlets enable a new class of real-time cognitive assistive applications on wearable devices By processing high data rate sensor inputs such as video close to the point of capture, cloudlets can reduce ingress bandwidth demand into the cloud By serving as proxies for distant cloud services that are unavailable due to failures or cyberattacks, cloudlets can improve robustness and availability, We caution that proprietary software ecosytems surrounding cloudlets will lead to a fragmented marketplace that fails to realize the full business potential of mobile-cloud convergence, Instead we urge that the software ecosystem surrounding cloudlets be based on the same principles of openness and end-to-end design that have made the Internet so successful.
2K_dev_487	A descending ( multi-item ) clock auction ( DCA ) is a mechanism for buying items from multiple potential sellers, In the DCA bidder-specific prices are decremented over the course of the auction, In each round each bidder might accept or decline his offer price, Accepting means the bidder is willing to sell at that price, Rejecting means the bidder will not sell at that price or a lower price DCAs have been proposed as the method for procuring spectrum from existing holders in the FCC 's imminent incentive auctions so spectrum can be repurposed to higher-value uses. However the DCA design has lacked a way to determine the prices to offer the bidders in each round, This is a recognized, important and timely problem, We present to our knowledge, the first techniques for this, to naturally reduce the offer prices to the bidders through the bidding rounds to minimize expected payment. An unexpected paradox about DCAs is that sometimes when the number of rounds allowed increases, the final payment increases, We provide an explanation for this. We develop a percentile-based approach which provides a means We also develop an optimization model for setting prices so as while stochastically satisfying the feasibility constraint, ( The DCA has a final adjustment round that obtains feasibility after feasibility has been lost in the final round of the main DCA, ) We prove attractive properties of this, such as symmetry and monotonicity, We develop computational methods for solving the model, ( We also develop optimization models with recourse, but they are not computationally practical. We present experiments both on the homogeneous items case and the case of FCC incentive auctions, where we use real interference constraint data to get a fully faithful model of feasibility.
2K_dev_489	For solving symmetric diagonally dominant ( SDD ) linear systems with m non-zero entries to a relative error of e in O ( m log 1/2 n log c n log ( 1/ e ) ) time, for constructing optimal embeddings in snowflake spaces that runs in O ( m log log n ) time. We show an algorithm Our approach follows the recursive preconditioning framework, which aims to reduce graphs to trees using iterative methods, We improve two key components of this framework : random sampling and tree embeddings, Both of these components are used in a variety of other algorithms, and our approach also extends to the dual problem of computing electrical flows, We show that preconditioners constructed by random sampling can perform well without meeting the standard requirements of iterative methods, In the graph setting, this leads to ultra-sparsifiers that have optimal behavior in expectation The improved running time makes previous low stretch embedding algorithms the running time bottleneck in this framework In our analysis, we relax the requirement of these embeddings to snowflake spaces, We then obtain a two-pass approach algorithm This algorithm is also readily parallelizable.
2K_dev_495	Given a real world graph, how should we lay-out its edges ? How can we compress it ? These questions are closely related, and the typical approach so far is to find clique-like communities, like the cavemen graph. We show that the block-diagonal mental image of the cavemen graph is the wrong paradigm, in full agreement with earlier results that real world graphs have no good cuts Instead, we propose to envision graphs as a collection of hubs connecting spokes, with super-hubs connecting the hubs, and so on recursively. We show that SlashBurn consistently outperforms other methods for all data sets, resulting in better compression and faster running time, Moreover we show that SlashBurn with the appropriate spokes ordering can further improve compression while hardly sacrificing the running time. Based on the idea, we propose the SlashBurn method to recursively split a graph into hubs and spokes connected only by the hubs, We also propose techniques to select the hubs and give an ordering to the spokes, in addition to the basic SlashBurn, We give theoretical analysis of the proposed hub selection methods, Our view point has several advantages : ( a ) it avoids the no good cuts problem, ( b ) it gives better compression, and ( c ) it leads to faster execution times for matrix-vector operations, which are the back-bone of most graph processing tools.
2K_dev_499	In this paper we study the intertwined propagation of two competing `` memes '' ( or data, ) in a composite network, Within the constraints of this scenario, we ask two key questions : ( a ) which meme will prevail ? and ( b ) can one influence the outcome of the propagations ?. Our model is underpinned by two key concepts, a structural graph model ( composite network ) and a viral propagation model ( SI 1 I 2 S ), Using this framework we formulate a non-linear dynamic system and perform an eigenvalue analysis to identify the tipping point of the epidemic behavior Based on insights gained from this analysis, we demonstrate an effective and accurate prediction method to determine viral dominance, which we call the EigenPredictor. Next using a combination of synthetic and real composite networks, we evaluate the effectiveness of various viral suppression techniques by either a ) concurrently suppressing both memes or b ) unilaterally suppressing a single meme while leaving the other relatively unaffected.
2K_dev_500	The Gates Hillman prediction market ( GHPM ) was an internet prediction market designed to predict the opening day of the Gates and Hillman Centers, the new computer science complex at Carnegie Mellon University Unlike a traditional continuous double auction format, the GHPM was mediated by an automated market maker, a central agent responsible for pricing transactions with traders over the possible opening days, The GHPMs event partition was, at the time the largest ever elicited in any prediction market by an order of magnitude, and dealing with the markets size required new advances, including a novel span-based elicitation interface that simplified interactions with the market maker. To examine issues of trader performance and market microstructure, including how the market both reacted to and anticipated official news releases about the buildings opening day. We use the large set of identity-linked trades generated by the GHPM.
2K_dev_503	For computing the Voronoi diagram of a set of $ $ n $ $ n points in constant-dimensional Euclidean space. We describe a new algorithm The running time of our algorithm is $ $ O ( f \log n \log \varDelta ) $ $ O ( flognlog ) where $ $ f $ $ f is the output complexity of the Voronoi diagram and $ $ \varDelta $ $ is the spread of the input, the ratio of largest to smallest pairwise distances Despite the simplicity of the algorithm and its analysis, it improves on the state of the art for all inputs with polynomial spread and near-linear output size, The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation, Then the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity, yielding the output sensitive bound, The removal only involves local flips and is inspired by kinetic data structures.
2K_dev_507	The game played by hospitals participating in pairwise kidney exchange programs. Has an approximation ratio of 3/2 to the maximum cardinality matching, This is an improvement over a recent upper bound of 2 ( Ashlagi et al, 2010 2 ] ) and, furthermore our mechanism beats for the first time the lower bound on the approximation ratio of deterministic truthful mechanisms We complement our positive result with new lower bounds, Among other statements we prove that the weaker incentive compatibility property of truthfulness in expectation in our mechanism is necessary ; universally truthful mechanisms that have an inclusion-maximality property have an approximation ratio of at least 2. We study a mechanism design version of matching computation in graphs that models We present a new randomized matching mechanism for two agents which is truthful in expectation and.
2K_dev_510	The openness of wireless communication and the recent development of software-defined radio technology, respectively provide a low barrier and a wide range of capabilities for misbehavior, attacks and defenses against attacks. That allows a jammer and sender to choose ( 1 ) whether to transmit or sleep, ( 2 ) a power level to transmit with, and ( 3 ) what channel to transmit on, to choose on how many channels it simultaneously attacks. Matching our intuition the aggressiveness of an attacker is related to how much of a discount is placed on data delay, This results in the defender often choosing to sleep despite the latency implication, because the threat of jamming is high, We also present several other findings. In this work we present finite-energy jamming games, a game model We also allow the jammer A major addition in finite-energy jamming games is that the jammer and sender both have a limited amount of energy which is drained according to the actions a player takes We develop a model of our system as a zero-sum finite-horizon stochastic game with deterministic transitions, We leverage the zero-sum and finite-horizon properties of our model to design a simple polynomial-time algorithm to compute optimal randomized strategies for both players, The utility function of our game model can be decoupled into a recursive equation, Our algorithm exploits this fact to use dynamic programming to construct solutions in a bottom-up fashion, For each state of energy levels, a linear program is solved to find Nash equilibrium strategies for the subgame, With these techniques our algorithm has only a linear dependence on the number of states, and quadratic dependence on the number of actions, allowing us to solve very large instances, By computing Nash equilibria for our game models, we explore what kind of performance guarantees can be achieved both for the sender and jammer, when playing against an optimal opponent, We also use the optimal strategies to simulate finite-energy jamming games and provide insights into robust communication among reconfigurable, yet energy-limited radio systems. To test the performance of the optimal strategies we compare their performance with a random and adaptive strategy from simulations where we vary the strategies for one or both of the players.
2K_dev_521	Traditional power system state estimation methods lack the ability to track and manage increasing uncertainties inherent in the new technologies, such as recent and ongoing massive penetration of renewable energy, distribution intelligence and plug-in electric vehicles, To deal with the inability, a recent work proposes to utilize the unused historical data for power system state estimation First, because the power systems are with periodic patterns, which create clustered measurement data. Although able to achieve much higher accuracy, the new approach is slow due to the burden by sequential similarity check over large volumes of high dimensional historical measurements, making it unsuitable for online services, This calls for a general approach to preprocess the historical data, In this paper we propose to achieve such a goal with three steps, to remove redundancy To further reduce the computational time to group the clustered power system data into a tree structure. Results show that the new method can dramatically reduce the necessary computational time for online data-driven state estimation, while producing a highly accurate state estimate. Dimension reduction is proposed, but still able to retrieve similar measurements the k-dimensional tree indexing approach is employed in step two resulting in a log-reduction over searching time. Finally we verify the obtained historical power system states via AC power system model and the current measurements to filter out bad historical data.
2K_dev_524	A key idea in object-oriented programming is that objects encapsulate state and interact with each other by message exchange This perspective suggests a model of computation that is inherently concurrent ( to facilitate simultaneous message exchange ) and that accounts for the effect of message exchange on an object 's state ( to express valid sequences of state transitions ). In this paper we show that such a model of computation arises naturally from session-based communication to express the protocols of message exchange and to reason about concurrency and state. We show that our language supports the typical patterns of object-oriented programming ( e, encapsulation dynamic dispatch and subtyping ) while guaranteeing session fidelity in a concurrent setting In addition, we show that our language facilitates new forms of expression ( e, type-directed reuse internal choice ), which are not available in current object-oriented languages. We introduce an object-oriented programming language that has processes as its only objects and employs linear session types. Based on various examples We have implemented our language in a prototype compiler.
2K_dev_525	The Pascaline was the first working mechanical calculator, created in 1642 by the French polymath Blaise Pascal, Over the next two decades Pascal built 40 of these machines, of which nine survive today, Several good web resources describe the Pascaline, but to properly appreciate the sautoir, Pascal 's kinetic energy solution to jam-free ripple carry, building a working replica is invaluable. Thanks to the growing availability of rapid prototyping tools, it has become relatively easy for CS educators to fabricate physical artifacts to help students explore computational ideas. The Pascaline kit designed in SolidWorks, is open source and available at http : //www. I 've created a Pascaline kit using laser-cut acrylic and standard fasteners that can be assembled with just a screwdriver, pliers and Loctite High school or college students with minimal skills can put it together in a few hours and have a functioning calculator, Exploring the Pascaline 's design is an engaging way to connect a milestone in the early history of computing with more modern theoretical concepts, Students can investigate questions such as : What makes a device `` digital '' ? ( Slide rules have numeric scales but are analog devices, ) How does nonlinearity produce discrete states in a continuous world ? How are nonlinearities induced in the Pascaline vs, in digital electronics ? How do the logic design concepts `` half adder '' and `` full adder '' map onto the components of the Pascaline ? Is the Pascaline really adding, or merely counting ? How does the Pascaline use nines complement arithmetic to perform subtraction, and why is n't it tens complement ?.
2K_dev_526	User review is a crucial component of open mobile app markets such as the Google Play Store, How do we automatically summarize millions of user reviews and make sense out of them ? We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users. Unfortunately beyond simple summaries such as histograms of user ratings, there are few analytic tools that can provide insights into user reviews. Results using our techniques are reported. In this paper we propose Wiscom, a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail, Our system is able to ( a ) discover inconsistencies in reviews ; ( b ) identify reasons why users like or dislike a given app, and provide an interactive, zoomable view of how users ' reviews evolve over time ; and ( c ) provide valuable insights into the entire app market, identifying users ' major concerns and preferences of different types of apps. On a 32GB dataset consisting of over 13 million user reviews of 171, 493 Android apps in the Google Play Store.
2K_dev_537	With the advancement of information systems, means of communications are becoming cheaper, faster and more available, Today millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want, They can access their e-mails, comment on weblogs watch and post videos and photos ( as well as comment on them ), and make phone calls or text messages almost ubiquitously, We also show three potential applications of the SFP : as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications, as a technique to detect anomalies, and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems. Given this scenario in this article, we tackle a fundamental aspect of this new era of communication : How the time intervals between communication events behave for different technologies and means of communications Are there universal patterns for the Inter-Event Time Distribution ( IED ) q How do inter-event times behave differently among particular technologiesq To answer these questions to generate inter-event times between communications. Moreover we propose the use of the Self-Feeding Process ( SFP ) The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data. We analyzed eight different datasets from real and modern communication data and found four well-defined patterns seen in all the eight datasets.
2K_dev_540	Given a large collection of co-evolving multiple time-series, which contains an unknown number of patterns of different durations, how can we efficiently and effectively find typical patterns and the points of variation ? How can we statistically summarize all the sequences, and achieve a meaningful segmentation ? for co-evolving time sequences. Demonstrate that AutoPlait does indeed detect meaningful patterns correctly, and it outperforms state-of-the-art competitors as regards accuracy and speed : AutoPlait achieves near-perfect, over 95 % precision and recall, and it is up to 472 times faster than its competitors. In this paper we present AutoPlait, a fully automatic mining algorithm Our method has the following properties : ( a ) effectiveness : it operates on large collections of time-series, and finds similar segment groups that agree with human intuition ; ( b ) scalability : it is linear with the input size, and thus scales up very well ; and ( c ) AutoPlait is parameter-free, and requires no user intervention, no prior training and no parameter tuning. Extensive experiments on 67GB of real datasets.
2K_dev_542	Can we identify patterns of temporal activities caused by human communications in social media ? Is it possible to model these patterns and tell if a user is a human or a bot based only on the timing of their postings ? Social media services allow users to make postings, generating large datasets of human activity time-stamps. In this paper we analyze time-stamp data from social media services that is able to match all four discovered patterns. And find that the distribution of postings inter-arrival times ( IAT ) is characterized by four patterns : ( i ) positive correlation between consecutive IATs, ( ii ) heavy tails, ( iii ) periodic spikes and ( iv ) bimodal distribution, by showing that it can accurately fit real time-stamp data from Reddit and Twitter, We also show that RSC can be used to spot outliers and detect users with non-human behavior, RSC consistently provides a better fit to real data and clearly outperform existing models for human dynamics, RSC was also able to detect bots with a precision higher than 94 %. Based on our findings, we propose Rest-Sleep-and-Comment ( RSC ). We demonstrate the utility of RSC We validate RSC using real data consisting of over 35 million postings from Twitter and Reddit.
2K_dev_548	These theoretical results have direct practical implications. We consider the problem of fairly allocating indivisible goods, focusing on a recently-introduced notion of fairness called maximin share guarantee : Each player 's value for his allocation should be at least as high as what he can guarantee by dividing the items into as many bundles as there are players and receiving his least desirable bundle. We show that such allocations may not exist, but allocations guaranteeing each player 2/3 of the above value always exist. And can be computed in polynomial time when the number of players is constant. Assuming additive valuation functions.
2K_dev_562	In 1876 Charles Lutwidge Dodgson suggested the intriguing voting rule that today bears his name Although Dodgsons rule is one of the most well-studied voting rules, it suffers from serious deficiencies, both from the computational point of viewit is NP-hard even to approximate the Dodgson score within sublogarithmic factorsand from the social choice point of viewit fails basic social choice desiderata such as monotonicity and homogeneity. However this does not preclude the existence of approximation algorithms for Dodgson that are monotonic or homogeneous, and indeed it is natural to ask whether such algorithms exist, In this article we give definitive answers to these questions. Furthermore we show that a slight variation on a known voting rule yields a monotonic, homogeneous polynomial-time O ( m log m ) -approximation algorithm and establish that it is impossible to achieve a better approximation ratio even if one just asks for homogeneity we prove that algorithms with an approximation ratio that depends only on m do not exist. We design a monotonic exponential-time algorithm that yields a 2-approximation to the Dodgson score, while matching this result with a tight lower bound We also present a monotonic polynomial-time O ( log m ) -approximation algorithm ( where m is the number of alternatives ) ; this result is tight as well due to a complexity-theoretic lower bound. We complete the picture by studying several additional social choice properties ; for these properties.
2K_dev_563	Given a snapshot of a large graph, in which an infection has been spreading for some time, can we identify those nodes from which the infection started to spread ? In other words, can we reliably tell who the culprits are ? In this paper, we answer this question affirmatively Essentially, we are after that set of seed nodes that best explain the given snapshot, to identify the best set of seed nodes and virus propagation ripple to identify likely sets of seed nodes. Shows high accuracy in the detection of seed nodes, in addition to the correct automatic identification of their number Moreover, NetSleuth scales linearly in the number of nodes of the graph. And give an efficient method called NetSleuth for the well-known susceptible-infected virus propagation model, We propose to employ the minimum description length principle as the one by which we can most succinctly describe the infected graph, We give an highly efficient algorithm given a snapshot, Then given these seed nodes, we show we can optimize the virus propagation ripple in a principled way by maximizing likelihood, With all three combined, NetSleuth can automatically identify the correct number of seed nodes, as well as which nodes are the culprits. Experimentation on our method.
2K_dev_568	A way of preventing automated offline dictionary attacks against user selected passwords. We introduce GOTCHAs ( Generating panOptic Turing Tests to Tell Computers and Humans Apart ) as A GOTCHA is a randomized puzzle generation protocol, which involves interaction between a computer and a human, Informally a GOTCHA should satisfy two key properties : ( 1 ) The puzzles are easy for the human to solve, ( 2 ) The puzzles are hard for a computer to solve even if it has the random bits used by the computer to generate the final puzzle -- - unlike a CAPTCHA [ 44 ], Our main theorem demonstrates that GOTCHAs can be used to mitigate the threat of offline dictionary attacks against passwords by ensuring that a password cracker must receive constant feedback from a human being while mounting an attack Finally, we provide a candidate construction of GOTCHAs based on Inkblot images, Our construction relies on the usability assumption that users can recognize the phrases that they originally used to describe each Inkblot image -- - a much weaker usability assumption than previous password systems based on Inkblots which required users to recall their phrase exactly. We conduct a user study to evaluate the usability of our GOTCHA construction, We also generate a GOTCHA challenge where we encourage artificial intelligence and security researchers to try to crack several passwords protected with our scheme.
2K_dev_572	Control decisions are made. We consider an adaptive cruise control system in which based on position and velocity information received from other vehicles via V2V wireless communication If the vehicles follow each other at a close distance, they have better wireless reception but collisions may occur when a follower car does not receive notice about the decelerations of the leader car fast enough to react before it is too late, If the vehicles are farther apart, they would have a bigger safety margin, but the wireless communication drops out more often, so that the follower car no longer receives what the leader car is doing, In order to guarantee safety, such a system must return control to the driver if it does not receive an update from a nearby vehicle within some timeout period, The value of this timeout parameter encodes a tradeoff between the likelihood that an update is received and the maximum safe acceleration Combining formal verification techniques for hybrid systems with a wireless communication model. We analyze how the expected efficiency of a provably-safe adaptive cruise control system is affected by the value of this timeout.
2K_dev_578	Given a table where rows correspond to records and columns correspond to attributes, we want to find a small number of patterns that succinctly summarize the dataset, For example given a set of patient records with several attributes each, how can we find ( a ) that the `` most representative '' pattern is, say ( male adult, * ) followed by ( *, child low-cholesterol ) etc ? that provides a sequence of patterns. Demonstrate the effectiveness and intuitiveness of our discovered patterns. We propose TSum a method ordered by their `` representativeness It can decide both which these patterns are, as well as how many are necessary to properly summarize the data, Our main contribution is formulating a general framework, TSum using compression principles, TSum can easily accommodate different optimization strategies for selecting and refining patterns, The discovered patterns can be used to both represent the data efficiently, as well as interpret it quickly.
2K_dev_584	High-data-rate sensors such as video cameras, are becoming ubiquitous in the Internet of Things, This article is part of a special issue on smart spaces. An Internet-scale repository of crowd-sourced video content. This article describes GigaSight, with strong enforcement of privacy preferences and access controls The GigaSight architecture is a federated system of VM-based cloudlets that perform video analytics at the edge of the Internet, thus reducing the demand for ingress bandwidth into the cloud, Denaturing which is an owner-specific reduction in fidelity of video content to preserve privacy, is one form of analytics on cloudlets, Content-based indexing for search is another form of cloudlet-based analytics.
2K_dev_587	Computers have the potential to significantly extend the practice of popular music based on steady tempo and mostly determined form. There are significant challenges to overcome, however due to constraints including accurate timing based on beats and adherence to a form or structure despite possible changes that may occur, possibly even during performance, takes into account latency due to communication delays and audio buffering, the problem of mapping from a conventional score. We describe an approach to synchronization across media that We also address with repeats and other structures to an actual performance, which can involve both flattening the score and rearranging it, as is common in popular music, Finally we illustrate the possibilities of the score as a bidirectional user interface in a real-time system for music performance, allowing the user to direct the computer through a digitally displayed score, and allowing the computer to indicate score position back to human performers.
2K_dev_589	There has been significant interest and progress recently in algorithms that solve regression problems involving tall and thin matrices in input sparsity time, Our results build upon the close connection between randomized matrix algorithms, iterative methods and graph sparsification. Given a n * d matrix where n g d, these algorithms find an approximation with fewer rows, allowing one to solve a poly ( d ) sized problem instead, In practice the best performances are often obtained by invoking these routines in an iterative fashion to give theoretical guarantees comparable to and better than the current state of the art. We show these iterative methods can be adapted Our approaches are based on computing the importances of the rows, known as leverage scores, in an iterative manner We show that alternating between computing a short matrix estimate and finding more accurate approximate leverage scores leads to a series of geometrically smaller instances, This gives an algorithm whose runtime is input sparsity plus an overhead comparable to the cost of solving a regression problem on the smaller approximation.
2K_dev_593	Several researchers proposed using non-Euclidean metrics on point sets in Euclidean space for clustering noisy data, Almost always a distance function is desired that recognizes the closeness of the points in the same cluster, even if the Euclidean cluster diameter is large. There- fore it is preferred to assign smaller costs to the paths that stay close to the input points, In this paper we consider a natural metric with this property. Which we call the nearest neighbor metric, Given a point set P and a path, t his metric is the integral of the distance to P along, W e describe a ( 3 +e ) - approximation algorithm and a more intricate ( 1 + e ) -approximation algorithm to compute the nearest neighbor metric Both approximation algorithms work in near-linear time, The former uses shortest paths on a sparse graph defined over the input points, The latter uses a sparse sample of the ambient space, to find good approximate geodesic paths.
2K_dev_594	On input of an $ n $ -vertex $ m $ -edge weighted graph $ G $ and a value $ k $ produces an incremental sparsifier $ \hat { G } $ with $ n-1 + m/k $ edges, such that the relative condition number of $ G $ with $ \hat { G } $ is bounded above by $ \tilde { O } ( k\log^2 n ) $, with probability $ 1-p $ ( we use the $ \tilde { O } ( ) $ notation to hide a factor of at most $ ( \log\log n ) ^4 $ ). We present an algorithm that The algorithm runs in time $ \tilde { O } ( ( m \log { n } + n\log^2 { n } ) \log ( 1/p ) ), $ As a result, we obtain an algorithm that on input of an $ n\times n $ symmetric diagonally dominant matrix $ A $ with $ m $ nonzero entries and a vector $ b $ computes a vector $ { x } $ satisfying $ || { x } -A^ { + } b||_A < \epsilon ||A^ { + } b||_A $, in expected time $ \tilde { O } ( m\log^2 { n } \log ( 1/\epsilon ) ), $ The solver is based on repeated applications of the incremental sparsifier that produces a chain of graphs which is then used as input to the recursive preconditioned Chebyshev iteration.
2K_dev_598	For continuous collection of crowd-sourced video. Reveal the bottlenecks for video upload, denaturing indexing and content-based search, They also provide insight on how parameters such as frame rate and resolution impact scalability. We propose a scalable Internet system from devices such as Google Glass, Our hybrid cloud architecture, GigaSight is effectively a Content Delivery Network ( CDN ) in reverse It achieves scalability by decentralizing the collection infrastructure using cloudlets based on virtual machines~ ( VMs ), Based on time location, and content privacy sensitive information is automatically removed from the video, This process which we refer to as denaturing, is executed in a user-specific VM on the cloudlet Users can perform content-based searches on the total catalog of denatured videos.
2K_dev_600	Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns ? to quantify both concepts ( `` synchronicity '' and `` normality '' ). CatchSync consistently outperforms existing competitors, both in detection accuracy by 36 % on Twitter and 20 % on Tencent Weibo, as well as in speed. We propose a fast and effective method, CatchSync which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior pattern, because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority, We introduce novel measures and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots Thanks to careful design, CatchSync has the following desirable properties : ( a ) it is scalable to large datasets, being linear on the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology, without needing labeled data, nor timing information etc, while still capable of using side information. We applied CatchSync on two large, real datasets 1-billion-edge Twitter social graph and 3-billion-edge Tencent Weibo social graph, and several synthetic ones.
2K_dev_602	We investigate the power of voting among diverse, randomized software agents allows us to reason about a collection of agents with different biases ( determined by the first-stage noise models ). That a uniform team, consisting of multiple instances of any single agent, must make a significant number of mistakes, whereas a diverse team converges to perfection as the number of agents grows provide evidence for the effectiveness of voting when agents are diverse. With teams of computer Go agents in mind, we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning, This model which furthermore, apply randomized algorithms to evaluate alternatives and produce votes ( captured by the second-stage noise models ). We analytically demonstrate Our experiments, which pit teams of computer Go agents against strong agents.
2K_dev_614	Game-theoretic algorithms for physical security have made an impressive real-world impact These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game, where the attacker observes the defender 's strategy and best-responds. In order to build the game model, though the payoffs of potential attackers for various outcomes must be estimated ; inaccurate estimates can lead to significant inefficiencies that optimizes the defender 's strategy with no prior information. We design an algorithm, by observing the attacker 's responses to randomized deployments of resources and learning his priorities In contrast to previous work, our algorithm requires a number of queries that is polynomial in the representation of the game.
2K_dev_617	Most algorithmic matches in fielded kidney exchanges do not result in an actual transplant. In this paper we address the problem of cycles and chains in a proposed match failing after the matching algorithm has committed to them, for the probabilistic exchange clearing problem. We show that failure-aware kidney exchange can significantly increase the expected number of lives saved and show that this new solver scales well. From the computational viewpoint, we design a branch-and-price-based optimal clearing algorithm specifically. ( i ) in theory, on random graph models ; ( ii ) on real data from kidney exchange match runs between 2010 and 2012 ; ( iii ) on synthetic data generated via a model of dynamic kidney exchange on large simulated data, unlike prior clearing algorithms.
2K_dev_618	Cloud offload is an important technique in mobile computing, VM-based cloudlets have been proposed as offload sites for the resource-intensive and latency-sensitive computations typically associated with mobile multimedia applications. Since cloud offload relies on precisely-configured back-end software, it is difficult to support at global scale across cloudlets in multiple domains To address this problem. We demonstrate a prototype system that is capable of provisioning a cloudlet with a non-trivial VM image in 10 seconds. We describe just-in-time ( JIT ) provisioning of cloudlets under the control of an associated mobile device This speed is achieved through dynamic VM synthesis and a series of optimizations to aggressively reduce transfer costs and startup latency. Using a suite of five representative mobile applications.
2K_dev_622	Meeting service level objectives ( SLOs ) for tail latency is an important and challenging open problem in cloud computing infrastructures, The challenges are exacerbated by burstiness in the workloads to provide tail latency QoS for shared networked storage. PriorityMeister outperforms most recent reactive request scheduling approaches, with more workloads satisfying latency SLOs at higher latency percentiles. This paper describes PriorityMeister -- a system that employs a combination of per-workload priorities and rate limits, even with bursty workloads, PriorityMeister automatically and proactively configures workload priorities and rate limits across multiple stages ( e, a shared storage stage followed by a shared network stage ) to meet end-to-end tail latency SLOs PriorityMeister is also robust to mis-estimation of underlying storage device performance and contains the effect of misbehaving workloads. In real system experiments and under production trace workloads.
2K_dev_624	Regret-based methods have largely been favored in practice, in spite of their theoretically inferior convergence rates. We study the problem of computing a Nash equilibrium in large-scale two-player zero-sum extensive-form games While this problem can be solved in polynomial time, first-order or regret-based methods are usually preferred for large games. We find that mirror prox and the excessive gap technique outperform the prior regret-based methods for finding medium accuracy solutions. In this paper we investigate the acceleration of first-order methods both theoretically and experimentally An important component of many first-order methods is a distance-generating function, Motivated by this we investigate a specific distance-generating function, namely the dilated entropy function, over treeplexes which are convex polytopes that encompass the strategy spaces of perfect-recall extensive-form games, We develop significantly stronger bounds on the associated strong convexity parameter, In terms of extensive-form game solving, this improves the convergence rate of several first-order methods by a factor of O ( ( # information sets depth M ) / ( 2 depth ) ) where M is the maximum value of the l 1 norm over the treeplex encoding the strategy spaces, In order to instantiate stochastic mirror prox, we develop a class of gradient sampling schemes for game trees, Equipped with our distance-generating function and sampling scheme. Experimentally we investigate the performance of three first-order methods ( the excessive gap technique, mirror prox and stochastic mirror prox ) and compare their performance to the regret-based algorithms.
2K_dev_625	How does a new startup drive the popularity of competing websites into oblivion like Facebook famously did to MySpace ? This question is of great interest to academics, technologists and financial investors alike. In this work we exploit the singular way in which Facebook wiped out the popularity of MySpace, Hi5 Friendster and Multiply to guide the design of a new popularity competition model. The resulting model not only accurately fits the observed Daily Active Users ( DAU ) of Facebook and its competitors but also predicts their fate four years into the future. Our model provides new insights into what Nobel Laure- ate Herbert A, Simon called the `` marketplace of attention, '' which we recast as the attention-activity marketplace. Our model design is further substantiated by user-level activity of 250, 000 MySpace users obtained between 2004 and 2009.
2K_dev_632	Cloud-sourced virtual appliances ( VAs ) have been touted as powerful solutions for many software maintenance, mobility backward compatibility and security challenges. In this paper we ask whether it is possible that supports fluid, interactive user experience even over mobile networks. Supports fluid interaction even in challenging network conditions, such as 4G LTE. To create a VA cloud service More specifically, we wish to support a YouTube-like streaming service for executable content, such as games interactive books, Users should be able to post, browse through and interact with executable content swiftly and without long interruptions Intuitively, this seems impossible ; the bandwidths, latencies and costs of last-mile networks would be prohibitive given the sheer sizes of virtual machines ! Yet, we show that a set of carefully crafted, novel prefetching and streaming techniques can bring this goal surprisingly close to reality. We show that vTube, a VA streaming system that incorporates our techniques.
2K_dev_633	To learn a high-dimensional conditional distribution of outputs given inputs. Improving upon past bounds with convergence rates that depend logarithmically on the data dimension, and demonstrating state-of-the-art performance on two real-world tasks. This paper considers the sparse Gaussian conditional random field, a discriminative extension of sparse inverse covariance estimation, where we use convex methods The model has been proposed by multiple researchers within the past year, yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems In this paper, we make three contributions : 1 ) we develop a second-order active-set method which is several orders of magnitude faster than previously proposed optimization approaches for this problem. 2 ) we analyze the model from a theoretical standpoint, 3 ) we apply the method to large-scale energy forecasting problems.
2K_dev_634	Previously it has been shown that, under some conditions on the distribution of votes, if the number of manipulators is o ( n ), where n is the number of voters, then the probability that a random profile is manipulable by the coalition goes to zero as the number of voters goes to infinity, whereas if the number of manipulators is ( n ), then the probability that a random profile is manipulable goes to one This result analytically validates recent empirical results, and suggests that deciding the coalitional manipulation problem may be of limited computational hardness in practice. We study the phase transition of the coalitional manipulation problem for generalized scoring rules. And we show that as c goes from zero to infinity, the limiting probability that a random profile is manipulable goes from zero to one in a smooth fashion, there is a smooth phase transition between the two regimes. Here we consider the critical window, where a coalition has size cn.
2K_dev_637	In a multimillion-node network of who-follows-whom like Twitter, since a high count of followers leads to higher profits, users have the incentive to boost their in-degree. Can we spot the suspicious following behavior, which may indicate zombie followers and suspicious followees ? To answer the above question. Moreover we show it is effective. We propose CatchSync which exploits two tell-tale signs of the suspicious behavior : ( a ) synchronized behavior : the zombie followers have extremely similar following behavior pattern, because say they are generated by a script ; and ( b ) abnormal behavior : their behavior pattern is very different from the majority, Our CatchSync introduces novel measures to quantify both concepts and catches the suspicious behavior. In a real-world social network.
2K_dev_640	Recently data with complex characteristics such as epilepsy electroencephalography ( EEG ) time series has emerged, Epilepsy EEG data has special characteristics including nonlinearity. Therefore it is important to find a suitable forecasting method that covers these special characteristics. Results show that when compared to previous methods, the proposed method can forecast faster and accurately. In this paper we propose a coercively adjusted autoregression ( CA-AR ) method that forecasts future values from a multivariable epilepsy EEG time series, We use the technique of random coefficients, which forcefully adjusts the coefficients with 1 and 1, The fractal dimension is used to determine the order of the CA-AR model, We applied the CA-AR method reflecting special characteristics of data to forecast the future value of epilepsy EEG data.
2K_dev_645	When a free catchy application shows up, how quickly will people notify their friends about it ? Will the enthusiasm drop exponentially with time, or oscillate ? What other patterns emerge ? Here we answer these questions which generate networks that mimic our discovered patterns. We report surprising patterns, the most striking of which are : ( a ) the FIZZLE pattern, excitement about Polly shows a power-law decay over time with ex- ponent of -1, 2 ; ( b ) the RENDEZVOUS pattern, that obeys a power law ( we explain RENDEZVOUS in the text ) ; ( c ) the DISPERSION pattern, we find that the more a person uses Polly, the fewer friends he will use it with, but in a reciprocal fashion. Finally we also propose a generator of influence networks. Using data from the Polly telephone-based application, a large influence network of 72, 000 people with about 173, 000 in- teractions spanning 500MB of log data and 200 GB of audio data.
2K_dev_649	For users in cognitive decline. We describe the architecture and prototype implementation of an assistive system based on Google Glass devices It combines the first-person image capture and sensing capabilities of Glass with remote processing to perform real-time scene interpretation, The system architecture is multi-tiered It offers tight end-to-end latency bounds on compute-intensive operations, while addressing concerns such as limited battery capacity and limited processing capability of wearable devices The system gracefully degrades services in the face of network failures and unavailability of distant architectural tiers.
2K_dev_652	Formal verification and validation play a crucial role in making cyber-physical systems ( CPS ) safe, Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained, including models of the controller and of the physical dynamics, In CPS models are essential ; but any model we could possibly build necessarily deviates from the real world. If the real system fits to the model, its behavior is guaranteed to satisfy the correctness properties verified with respect to the model, Otherwise all bets are off, ensuring that verification results about models apply to CPS implementations, to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic. Overall ModelPlex generates provably correct monitor conditions that, are provably guaranteed to imply that the offline safety verification results about the CPS model apply to the present run of the actual CPS implementation. This article introduces ModelPlex, a method ModelPlex provides correctness guarantees for CPS executions at runtime : it combines offline verification of CPS models with runtime validation of system executions for compliance with the model, ModelPlex ensures in a provably correct way that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model If, at some point the observed behavior no longer complies with the model so that offline verification results no longer apply, ModelPlex initiates provably safe fallback actions, assuming the system dynamics deviation is bounded This article, furthermore develops a systematic technique by a correct-by-construction approach, leading to verifiably correct runtime model validation. If checked to hold at runtime.
2K_dev_654	Demand response has gained significant attention in recent years as it demonstrates potentials to enhance the power system 's operational flexibility in a cost-effective way, Industrial loads such as steel manufacturing plants consume large amounts of electric energy, and their electricity bills account for a remarkable percentage of their total operation cost, Meanwhile lots of industrial loads are very flexible in terms of adjusting their power consumption rate, through switching the transformer tap position. Hence industrial loads such as the steel plants have both the motivation and the ability to support power system operation through demand response to maximize its profits. In this paper we focus on the steel plant and optimize its scheduling from both the energy and the spinning reserve markets.
2K_dev_660	Given a graph with billions of nodes and edges, how can we find patterns and anomalies ? Are there nodes that participate in too many or too few triangles ? Are there close-knit near-cliques ? These questions are expensive to answer unless we have the first several eigenvalues and eigenvectors of the graph adjacency matrix. However eigensolvers suffer from subtle problems ( e, convergence ) for large sparse matrices, let alone for billion-scale ones, We address this problem. We report important discoveries about nearcliques and triangles on several real-world graphs, including a snapshot of the Twitter social network ( 56 Gb, 2 billion edges ) and the YahooWeb data set, one of the largest publicly available graphs ( 120 Gb, 4 billion nodes 6, 6 billion edges ). With the proposed HEIGEN algorithm, which we carefully design to be accurate, efficient and able to run on the highly scalable MAPREDUCE ( HADOOP ) environment, This enables HEIGEN to handle matrices more than 1 ; 000 larger than those which can be analyzed by existing algorithms. We implement HEIGEN and run it on the M45 cluster, one of the top 50 supercomputers in the world.
2K_dev_667	How can web services that depend on user generated content discern fraudulent input by spammers from legitimate input ? as well as potential extensions to anomaly detection problems in other domains. In this paper we focus on the social network Facebook and the problem of discerning ill-gotten Page Likes, made by spammers hoping to turn a profit, from legitimate Page Likes detects lockstep Page Like patterns on Facebook to find such suspicious lockstep behavior. Finally we demonstrate and discuss the effectiveness of CopyCatch. Our method which we refer to as CopyCatch, by analyzing only the social graph between users and Pages and the times at which the edges in the graph ( the Likes ) were created We offer the following contributions : ( 1 ) We give a novel problem formulation, with a simple concrete definition of suspicious behavior in terms of graph structure and edge constraints ( 2 ) We offer two algorithms - one provably-convergent iterative algorithm and one approximate, scalable MapReduce implementation 3 ) We show that our method severely limits `` greedy attacks '' and analyze the bounds from the application of the Zarankiewicz problem to our setting CopyCatch is actively in use at Facebook, searching for attacks on Facebook 's social graph of over a billion users, many millions of Pages, and billions of Page Likes. At Facebook and on synthetic data.
2K_dev_669	Given a simple noun such as { \em apple }, and a question such as `` is it edible ? ``, what processes take place in the human brain ?. More specifically given the stimulus, what are the interactions between ( groups of ) neurons ( also known as functional connectivity ) and how can we automatically infer those interactions, given measurements of the brain activity ? Furthermore, how does this connectivity differ across different human subjects ? which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity. GeBM produces brain activity patterns that are strikingly similar to the real ones, and the inferred functional connectivity is able to provide neuroscientific insights towards a better understanding of the way that neurons interact with each other, as well as detect regularities and outliers in multi-subject brain activity measurements. In this work we present a simple, novel good-enough brain model, or GeBM in short, and a novel algorithm Sparse-SysId Moreover, GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text ). We evaluate GeBM by using both synthetic and real brain data, Using the real data.
2K_dev_670	Given a network with attributed edges, how can we identify anomalous behavior ? Networks with edge attributes are ubiquitous, and capture rich information about interactions between nodes. In this paper we aim to utilize exactly this information to discern suspicious from typical behavior in an unsupervised fashion, lending well to the traditional scarcity of ground-truth labels in practical anomaly detection scenarios, for detecting edge-attributed graph anomalies. : we show that EdgeCentric successfully spots numerous such anomalies where it achieved 0, 87 precision over the top 100 results. Our work has a number of notable contributions, including ( a ) formulation : while most other graph-based anomaly detection works use structural graph connectivity or node information, we focus on the new problem of leveraging edge information, ( b ) methodology : we introduce EdgeCentric, an intuitive and scalable compression-based approach and ( c ) practicality. In several large edge-attributed real-world graphs, including the Flipkart e-commerce graph with over 3 million product reviews between 1, 1 million users and 545 thousand products.
2K_dev_672	Can we predict the values of unseen coalitions ?. This paper explores a PAC ( probably approximately correct ) learning model in cooperative games, Specifically we are given m random samples of coalitions and their values, taken from some unknown cooperative game ; We also establish a novel connection between PAC learnability and core stability : for games that are efficiently learnable, it is possible to find payoff divisions that are likely to be stable using a polynomial number of samples. We study the PAC learnability of several well-known classes of cooperative games, such as network flow games, threshold task games and induced subgraph games.
2K_dev_675	There has been recent interest in applying Stackelberg games to infrastructure security, in which a defender must protect targets from attack by an adaptive adversary, In real-world security settings the adversaries are humans and are thus boundedly rational. Most existing approaches for computing defender strategies against boundedly rational adversaries try to optimize against specific behavioral models of adversaries, and provide no quality guarantee when the estimated model is inaccurate, which provides guarantees against all adversary behavior models satisfying monotonicity, including all in the family of Regular Quantal Response functions for computing monotonic maximin. We propose a new solution concept, monotonic maximin We propose a mixed-integer linear program formulation We also consider top-monotonic maximin, a related solution concept that is more conservative, and propose a polynomial-time algorithm for top-monotonic maximin.
2K_dev_682	A well-spaced superset of points conforming to a given input set on the output points. We present a new algorithm that produces in any dimension with guaranteed optimal output size We also provide an approximate Delaunay graph Our algorithm runs in expected time O ( 2 O ( d ) ( n log n + m ) ), where n is the input size, m is the output point set size, and d is the ambient dimension The constants only depend on the desired element quality bounds, To gain this new efficiency, the algorithm approximately maintains the Voronoi diagram of the current set of points by storing a superset of the Delaunay neighbors of each point By retaining quality of the Voronoi diagram and avoiding the storage of the full Voronoi diagram, a simple exponential dependence on d is obtained in the running time Thus, if one only wants the approximate neighbors structure of a refined Delaunay mesh conforming to a set of input points, the algorithm will return a size 2 O ( d ) m graph in 2 O ( d ) ( n log n + m ) expected time, If m is superlinear in n, then we can produce a hierarchically well-spaced superset of size 2 O ( d ) n in 2 O ( d ) n log n expected time.
2K_dev_720	It is typically expected that if a mechanism is truthful, then the agents would, indeed truthfully report their private information. But why would an agent believe that the mechanism is truthful ? truthfulness can be verified efficiently ( in the computational sense ). We wish to design truthful mechanisms that are simple, that is whose Our approach involves three steps : ( i ) specifying the structure of mechanisms, ( ii ) constructing a verification algorithm, and ( iii ) measuring the quality of verifiably truthful mechanisms. We demonstrate this approach using a case study : approximate mechanism design without money for facility location.
2K_dev_725	To analyze a control algorithm designed to provide directional force feedback for a surgical robot, that provides safe operation along with directional force feedback. That guarantees the new algorithm is safe for all possible inputs. We then applied QdL to guide the development of a new algorithm. We applied quantified differential-dynamic logic ( QdL ) We identified problems with the algorithm, proved that it was in general unsafe, and described exactly what could go wrong, Using \KeYmaeraD ( a tool that mechanizes QdL ), we created a machine-checked proof.
2K_dev_728	I010178 Complex software systems are becoming increasingly prevalent in aerospace applications : in particular, to accomplish critical tasks, Ensuring the safety of these systems is crucial, as they can have subtly different behaviors under slight variations in operating and proposals are given on how to address these issues. This paper advocates the use of formal verification techniques and in particulartheoremprovingfor hybridsoftware-intensivesystemsasawell-foundedcomplementaryapproachtothe classical aerospace verification and validation techniques, such as testing or simulation. The challenges that naturally arise when applying such technology to industrial-scale applications is then detailed. As an illustration of these techniques, a novel lateral midair collision-avoidance maneuver is studied in an ideal setting, without accounting for the uncertainties of the physical reality.
2K_dev_731	The convergence of mobile computing and cloud computing enables new multimedia applications that are both resource-intensive and interaction-intensive, For these applications end-to-end network bandwidth and latency matter greatly when cloud resources are used to augment the computational power and battery life of a mobile device. That this crucial design consideration to meet interactive performance criteria limits data center consolidation. We then describe an architectural solution that is a seamless extension of today 's cloud computing infrastructure. We first present quantitative evidence.
2K_dev_734	How can we tell when accounts are fake or real in a social network ? And how can we tell which accounts belong to liberal, conservative or centrist users ? Often, we can answer such questions and label nodes in a network based on the labels of their neighbors and appropriate assumptions of homophily ( `` birds of a feather flock together '' ) or heterophily ( `` opposites attract '' ), One of the most widely used methods for this kind of inference is Belief Propagation ( BP ) which iteratively propagates the information from a few nodes with explicit labels throughout a network until convergence. A well-known problem with BP, however is that there are no known exact guarantees of convergence in graphs with loops that allows a closed-form solution that propagates information across every edge at most once. Show that LinBP and SBP are orders of magnitude faster than standard BP, while leading to almost identical node labels. This paper introduces Linearized Belief Propagation ( LinBP ), a linearization of BP via intuitive matrix equations and, thus comes with exact convergence guarantees, It handles homophily heterophily, and more general cases that arise in multi-class settings, Plus it allows a compact implementation in SQL, The paper also introduces Single-pass Belief Propagation ( SBP ), a localized ( or `` myopic '' ) version of LinBP and for which the final class assignments depend only on the nearest labeled neighbors In addition, SBP allows fast incremental updates in dynamic networks.
2K_dev_736	The stochastic matching problem deals with finding a maximum matching in a graph whose edges are unknown but can be accessed via queries, This is a special case of stochastic k-set packing, where the problem is to find a maximum packing of sets, each of which exists with some probability, for these two problems. We show that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches. In this paper we provide edge and set query algorithms that provably achieve some fraction of the omniscient optimal solution, Our main theoretical result for the stochastic matching ( i, 2-set packing ) problem is the design of an adaptive algorithm that queries only a constant number of edges per vertex and achieves a ( 1-e ) fraction of the omniscient optimal solution, for an arbitrarily small e > 0 Moreover, this adaptive algorithm performs the queries in only a constant number of rounds, We complement this result with a non-adaptive ( i, one round of queries ) algorithm that achieves a ( 0, 5 - e ) fraction of the omniscient optimum, We also extend both our results to stochastic k-set packing by designing an adaptive algorithm that achieves a ( 2/k - e ) fraction of the omniscient optimal solution, again with only O ( 1 ) queries per element, This guarantee is close to the best known polynomial-time approximation ratio of 3/k+1 -e for the deterministic k-set packing problem [ Furer 2013. We empirically explore the application of ( adaptations of ) these algorithms to the kidney exchange problem, where patients with end-stage renal failure swap willing but incompatible donors on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange.
2K_dev_751	Given a large dataset of users ' ratings of movies, what is the best model to accurately predict which movies a person will like ? And how can we prevent spammers from tricking our algorithms into suggesting a bad movie ? Is it possible to infer structure between movies simultaneously ? that accomplishes all of these goals. ( 2 ) We provide proof of our model 's robustness to spam and anomalous behavior, to demonstrate the model 's effectiveness in accurately predicting user 's ratings, avoiding prediction skew in the face of injected spam, and finding interesting patterns in real world ratings data. In this paper we describe a unified Bayesian approach to Collaborative Filtering It models the discrete structure of ratings and is flexible to the often non-Gaussian shape of the distribution, Additionally our method finds a co-clustering of the users and items, which improves the model 's accuracy and makes the model robust to fraud, We offer three main contributions : ( 1 ) We provide a novel model and Gibbs sampling algorithm that accurately models the quirks of real world ratings, such as convex ratings distributions. 3 ) We use several real world datasets.
2K_dev_753	Standard approaches to stochastic discrete systems require numerical solutions for large optimization problems and quickly become infeasible with larger state spaces, Generalizations of these techniques to hybrid systems with stochastic effects are even more challenging It is in principle applicable to a variety of stochastic models from other domains. We address the problem of model checking stochastic systems, checking whether a stochastic system satisfies a certain temporal property with a probability greater ( or smaller ) than a fixed threshold. While the answer to the verification problem is not guaranteed to be correct, we prove that Bayesian SMC can make the probability of giving a wrong answer arbitrarily small We show that our technique enables faster verification than state-of-the-art statistical techniques. In particular we present a Statistical Model Checking ( SMC ) approach based on Bayesian statistics We show that our approach is feasible for a certain class of hybrid systems with stochastic transitions, a generalization of Simulink/Stateflow models, The SMC approach was pioneered by Younes and Simmons in the discrete and non-Bayesian case, It solves the verification problem by combining randomized sampling of system traces ( which is very efficient for Simulink/Stateflow ) with hypothesis testing ( i, testing against a probability threshold ) or estimation ( i, computing with high probability a value close to the true probability ), We believe SMC is essential for scaling up to large Stateflow/Simulink models, The advantage is that answers can usually be obtained much faster than with standard, exhaustive model checking techniques We emphasize that Bayesian SMC is by no means restricted to Stateflow/Simulink models. We apply our Bayesian SMC approach to a representative example of stochastic discrete-time hybrid system models in Stateflow/Simulink : a fuel control system featuring hybrid behavior and fault tolerance.
2K_dev_759	Which song will Smith listen to next ? Which restaurant will Alice go to tomorrow ? Which product will John click next. These applications have in common the prediction of user trajectories that are in a constant state of flux over a hidden network ( e, website links geographic location ), Moreover what users are doing now may be unrelated to what they will be doing in an hour from now to cope with the complex challenges of learning personalized predictive models of non-stationary, transient and time-heterogeneous user trajectories. TribeFlow is more accurate and up to 413x faster than top competitors. Mindful of these challenges we propose TribeFlow, a method designed TribeFlow is a general method that can perform next product recommendation, next song recommendation next location prediction, and general arbitrary-length user trajectory prediction without domain-specific knowledge.
2K_dev_762	Review fraud is a pervasive problem in online commerce, in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services. Fake reviews are often detected based on several signs, including 1 ) they occur in short bursts of time ; 2 ) fraudulent user accounts have skewed rating distributions, However these may both be true in any given dataset, for detecting fraudulent reviews. Show that BIRDNEST successfully spots review fraud in large real-world graphs : the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart. Hence in this paper, we propose an approach which combines these 2 approaches in a principled manner, allowing successful detection even when one of these signs is not present To combine these 2 approaches, we formulate our Bayesian Inference for Rating Data ( BIRD ) model, a flexible Bayesian model of user rating behavior, Based on our model we formulate a likelihood-based suspiciousness metric, Normalized Expected Surprise Total ( NEST ), We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric. Experiments on real data.
2K_dev_771	The ap- proach can, generate nontrivial algebraic invariant equations capturing the airplane behavior during take-off or landing in longitudinal motion. We prove that any invariant algebraic set of a given polynomial vector field can be algebraically represented to efficiently automate the generation. By one polynomial and a finite set of its successive Lie derivatives, This so-called differential radical characterization re- lies on a sound abstraction of the reachable set of solutions by the smallest variety that contains it, The characterization leads to a differential radical invariant proof rule that is sound and complete, which implies that invariance of algebraic equa- tions over real-closed fields is decidable Furthermore, the problem of generating invariant varieties is shown to be as hard as minimizing the rank of a symbolic matrix, and is therefore NP-hard. We investigate symbolic linear algebra tools based on Gaussian elimination.
2K_dev_772	When companies operate on the graphs with monetary incentives to sell Twitter `` Followers '' and Facebook page `` Likes '', the graphs show strange connectivity patterns. Given multimillion-node graphs such as `` who-follows-whom '', `` patent-cites-patent '' `` user-likes-page '' and `` actor/director-makes-movie '' networks, how can we find unexpected behaviors ? to detect users who offer the lockstep behaviors in undirected/directed/bipartite graphs. We report strange deviations from typical patterns like smooth degree distributions, We find that such deviations are often due to `` lockstep behavior '' that large groups of followers connect to the same groups of followees, We discover that ( a ) the lockstep behaviors on the graph shape dense `` block '' in its adjacency matrix and creates `` rays '' in spectral subspaces, and ( b ) partially overlapping of the behaviors shape `` staircase '' in its adjacency matrix and creates `` pearls '' in spectral subspaces The results demonstrate the scalability and effectiveness of our proposed algorithm. The second contribution is that we provide a fast algorithm, using the discovery as a guide for practitioners. In this paper we study a complete graph from a large Twitter-style social network, spanning up to 3, Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep, We carry out extensive experiments on both synthetic and real datasets, as well as public datasets from IMDb and US Patent.
2K_dev_774	Revenue maximization in combinatorial auctions ( and other multidimensional selling settings ) is one of the most important and elusive problems in mechanism design, Such priors do not exist in most applications, Rather in many applications ( such as premium display advertising markets ), there is essentially a point prior, which may not be accurate. The optimal design is unknown, and is known to include features that are not acceptable in many applications, such as favoring some bidders over others and randomization A second challenge in mechanism design for combinatorial auctions is that the prior distribution on each bidder 's valuation can be doubly exponential, for branching upper bounding, lower bounding and lazy bounding. Validate the approach and show that our techniques dramatically improve scalability over a leading general-purpose MIP solver. In this paper we instead study a common revenue-enhancement approach - bundling - in the context of the most commonly studied combinatorial auction mechanism, the Vickrey-Clarke-Groves ( VCG ) mechanism, We adopt the point prior model, and prove robustness to inaccuracy in the prior, Then we present a branch-and-bound framework for finding the optimal bundling, We introduce several techniques. Experiments on CATS distributions.
2K_dev_776	It is well known that strategic behavior in elections is essentially unavoidable ; we therefore ask : how bad can the rational outcome be ? We answer this question. We provide very positive results for plurality and very negative results for Borda, and place veto in the middle of this spectrum. Via the notion of the price of anarchy, using the scores of alternatives as a proxy for their quality and bounding the ratio between the score of the optimal alternative and the score of the winning alternative in Nash equilibrium Specifically, we are interested in Nash equilibria that are obtained via sequences of rational strategic moves, Focusing on three common voting rules -- plurality, veto and Borda --.
2K_dev_777	Given a large graph, like a computer communication network, which $ k $ nodes should we immunize ( or monitor, or remove ) to make it as robust as possible against a computer virus attack ? This problem, referred to as the node immunization problem, is the core building block in many high-impact applications, ranging from public health, cybersecurity to viral marketing, A central component in node immunization is to find the best $ k $ bridges of a given graph. In this setting we typically want to determine the relative importance of a node ( or a set of nodes ) within the graph, for example how valuable ( as a bridge ) a person or a group of persons is in a social network. ( 1 ) the proposed bridging score gives mining results consistent with intuition ; and ( 2 ) the proposed fast solution is up to seven orders of magnitude faster than straightforward alternatives. First of all we propose a novel bridging score $ \Delta \lambda $, inspired by immunology and we show that its results agree with intuition for several realistic settings, Since the straightforward way to compute $ \Delta \lambda $ is computationally intractable, we then focus on the computational issues and propose a surprisingly efficient way ( $ O ( nk^2+m ) $ ) to estimate it. Experimental results on real graphs show that.
2K_dev_778	To the best of our knowledge, our work represents the largest study of propagation patterns of executables. How does malware propagate ? Does it form spikes over time ? Does it resemble the propagation pattern of benign files, such as software patches ? Does it spread uniformly over countries ? How long does it take for a URL that distributes malware to be detected and shut down ? In this work, we answer these questions. Finally we discover the SharkFin temporal propagation pattern of executable files, the GeoSplit pattern in the geographical spread of machines that report executables to Symantec 's servers, the Periodic Power Law ( Ppl ) distribution of the life-time of URLs, and we show how to efficiently extrapolate crucial properties of the data from a small sample. By analyzing patterns from 22 million malicious ( and benign ) files, 6 million hosts worldwide during the month of June 2011, We conduct this study using the WINE database available at Symantec Research Labs Additionally, we explore the research questions raised by sampling on such large databases of executables ; the importance of studying the implications of sampling is twofold : First, sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second, because every such data collection can be perceived as a sample of the real world.
2K_dev_779	Given a large cloud of multi-dimensional points, and an off-theshelf outlier detection method, why does it take a week to finish ? to eliminate the problem. We discovered that duplicate points create subtle issues, that the literature has ignored : if dmax is the multiplicity of the most over-plotted point, typical algorithms are quadratic on dmax, we show that our methods give either exact results, or highly accurate approximate ones. We propose several ways we report wall-clock times and our time savings ; and.
2K_dev_780	Counterfactual Regret Minimization ( CFR ) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games, CFR is an iterative algorithm that repeatedly traverses the game tree, updating regrets at each information set. An improvement that prunes any path of play in the tree, and its descendants that has negative regret. Show an order of magnitude speed improvement, and the relative speed improvement increases with the size of the game. We introduce to CFR It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive, had that path been explored on every iteration The new algorithm maintains CFR 's convergence guarantees while making iterations significantly fastereven if previously known pruning techniques are used in the comparison, This improvement carries over to CFR+, a recent variant of CFR.
2K_dev_781	Influence maximization is a problem of maximizing the aggregate adoption of products, technologies or even beliefs, Most past algorithms leveraged an assumption of submodularity that captures diminishing returns to scale. While submodularity is natural in many domains, early stages of innovation adoption are often better characterized by convexity, which is evident for renewable technologies, such as rooftop solar to scale over a finite time horizon, in which the decision maker faces a budget constraint. Prove that this policy is optimal in a very general setting, that the proposed `` best-time '' algorithm remains quite effective the `` best-time '' policy becomes suboptimal, and is significantly outperformed by our more general heuristic. We formulate a dynamic influence maximization problem under increasing returns We propose a simple algorithm in this model which chooses the best time period to use up the entire budget ( called Best-Stage ), and We also propose a heuristic algorithm for this problem of which Best-Stage decision is a special case. Additionally we experimentally verify even as we relax the assumptions under which optimality can be proved, However we find that when we add a `` learning-by-doing '' effect, in which the adoption costs decrease not as a function of time, but as a function of aggregate adoption.
2K_dev_782	Pareto efficiency is a widely used property in solution concepts for cooperative and non -- cooperative game -- theoretic settings and, more generally in multi -- objective problems. However finding or even approximating ( when the objective functions are not convex ) the Pareto curve is hard, Most of the literature focuses on computing concise representations to approximate the Pareto curve or on exploiting evolutionary approaches to generate approximately Pareto efficient samples of the curve for game-theoretic solution concepts that incorporate Pareto efficiency. In this paper we show that the Pareto curve of a bimatrix game can be found exactly in polynomial time and that it is composed of a polynomial number of pieces, Furthermore each piece is a quadratic function We use this result to provide algorithms.
2K_dev_783	Extensive-form games are a powerful tool for modeling a large range of multiagent scenarios Finally we discuss how our theory applies to several practical problems for which no solution quality bounds could be derived before. However most solution algorithms require discrete, In contrast many real-world domains require modeling with continuous action spaces, This is usually handled by heuristically discretizing the continuous action space without solution quality bounds, In this paper we address this issue, for providing bounds on solution quality for discretization of continuous action spaces in extensive-form games. Leveraging recent results on abstraction solution quality, we develop the first framework For games where the error is Lipschitz-continuous in the distance of a continuous point to its nearest discrete point, we show that a uniform discretization of the space is optimal, When the error is monotonically increasing in distance to nearest discrete point, we develop an integer program for finding the optimal discretization when the error is described by piecewise linear functions, This result can further be used to approximate optimal solutions to general monotonic error functions.
2K_dev_784	Kidney exchange where candidates with organ failure trade incompatible but willing donors, is a life-saving alternative to the deceased donor waitlist, which has inadequate supply to meet demand We conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view. While fielded kidney exchanges see huge benefit from altruistic kidney donors ( who give an organ without a paired needy candidate ), a significantly higher medical risk to the donor deters similar altruism with livers. In this paper we begin by proposing the idea of liver exchange, and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level, We then explore cross-organ donation where kidneys and livers can be bartered for each other, We show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges ; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool. We support this result experimentally on demographically accurate multi-organ exchanges.
2K_dev_786	A multi-faceted graph defines several facets on a set of nodes, Each facet is a set of edges that represent the relationships between the nodes in a specific context. Mining multi-faceted graphs have several applications, including finding fraudster rings that launch advertising traffic fraud attacks, tracking IP addresses of botnets over time, analyzing interactions on social networks and co-authorship of scientific papers that does soft clustering on individual facets, to discover communities across facets. Where NeSim is shown to be superior to MCL, JP and AP the well-established clustering algorithms We also report the success stories of MuFace in finding advertisement click rings. We propose NeSim a distributed efficient clustering algorithm We also propose optimizations to further improve the scalability, the efficiency and the clusters quality We employ generalpurpose graph-clustering algorithms in a novel way Due to the qualities of NeSim, we employ it as a backbone in the distributed MuFace algorithm, which discovers multi-faceted communities. We evaluate the proposed algorithms on several real and synthetic datasets.
2K_dev_787	The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm. Which enables CFR to scale to dramatically larger abstractions and numbers of cores, for generating such abstractions. It won the 2014 Annual Computer Poker Competition, beating each opponent with statistical significance. We introduce a distributed version of the most commonly used equilibrium-finding algorithm, counterfactual regret minimization ( CFR ), The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint We introduce an algorithm while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and the earth-mover's-distance similarity metric, Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency, Prior approaches run slowly on this architecture Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency, Finally we introduce a family of post-processing techniques that outperform prior ones. We applied these techniques to generate an agent for two-player no-limit Texas Hold'em.
2K_dev_788	How can we succinctly describe a million-node graph with a few simple sentences ? Given a large graph, how can we find its most `` important '' structures, so that we can summarize it and easily visualize it ? How can we measure the `` importance '' of a set of discovered subgraphs in a large graph ?. Starting with the observation that real graphs often consist of stars, bipartite cores cliques and chains, our main idea is to find the most succinct description of a graph in these `` vocabulary '' terms. To this end we first mine candidate subgraphs using one or more graph partitioning algorithms Next, we identify the optimal summarization using the minimum description length MDL principle, picking only those subgraphs from the candidates that together yield the best lossless compression of the graph-or, equivalently that most succinctly describe its adjacency matrix.
2K_dev_792	To capture the relevant rotational and translation invariances in geometric data. Showing good performance for modeling previously unseen molecular configurations we show substantial improvement over the state of the art in molecular energy optimization. Motivated by problems such as molecular energy prediction, we derive an ( improper ) kernel between geometric inputs, that is able Since many physical simulations based upon geometric data produce derivatives of the output quantity with respect to the input positions, we derive an approach that incorporates derivative information into our kernel learning, We further show how to exploit the low rank structure of the resulting kernel matrices to speed up learning. Finally we evaluated the method in the context of molecular energy prediction, Integrating the approach into a Bayesian optimization.
2K_dev_793	The leading approach for computing strong game-theoretic strategies in large imperfect-information games is to first solve an abstracted version of the game offline, then perform a table lookup during game play. A modification to this approach for performing endgame solving in large imperfect-information games, for evaluating the performance of an agent that uses endgame solving. Show that our algorithm leads to significantly stronger performance against the strongest agents from the 2013 AAAI Annual Computer Poker Competition. We consider where we solve the portion of the game that we have actually reached in real time to a greater degree of accuracy than in the initial computation, We call this approach endgame solving, Theoretically we show that endgame solving can produce highly exploitable strategies in some games ; however, we show that it can guarantee a low exploitability in certain games where the opponent is given sufficient exploitative power within the endgame, Furthermore despite the lack of a general worst-case guarantee, we describe benefits of endgame solving, We present an efficient algorithm and present a new variance-reduction technique. Experiments on no-limit Texas Hold'em.
2K_dev_796	Living organisms adapt to challenges through evolution and adaptation, Potential application classes include therapeutics at the population, individual and molecular levels ( drug design ), as well as cell repurposing and synthetic biology. This has proven to be a key difficulty in developing therapies, since the organisms develop resistance. Propose the wild idea of computational game theory for ( typically incomplete-information ) multistage games and opponent exploitation techniques, A sequential contingency plan for steering is constructed computationally for the setting at hand In the biological context, the opponent ( e, a disease ) has a systematic handicap because it evolves myopically, This can be exploited by computing trapping strategies that cause the opponent to evolve into states where it can be handled effectively.
2K_dev_806	For dynamic barter marketsand kidney exchange. We show that the mechanism results in significant gains on data from a national kidney exchange that includes 59 % of all US transplant centers. We present a credit-based matching mechanism in particularthat is both strategy proof and efficient, that is it guarantees truthful disclosure of donor-patient pairs from the transplant centers and results in the maximum global matching Furthermore, the mechanism is individually rational in the sense that, in the long run, it guarantees each transplant center more matches than the center could have achieved alone, The mechanism does not require assumptions about the underlying distribution of compatibility graphsa nuance that has previously produced conflicting results in other aspects of theoretical kidney exchange Our results apply not only to matching via 2-cycles : the matchings can also include cycles of any length and altruist-initiated chains, which is important at least in kidney exchanges, The mechanism can also be adjusted to guarantee immediate individual rationality at the expense of economic efficiency, while preserving strategy proofness via the credits, This circumvents a well-known impossibility result in static kidney exchange concerning the existence of an individually rational, strategy-proof and maximal mechanism.
2K_dev_807	The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm. Which enables CFR to scale to dramatically larger abstractions and numbers of cores, for generating such abstractions. That won the 2014 Annual Computer Poker Competition, beating each opponent with statistical significance. We introduce a distributed version of the most commonly used equilibrium-finding algorithm, counterfactual regret minimization ( CFR ), The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint We introduce an algorithm while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and earth-mover 's distance, Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency, Prior approaches run slowly on this architecture, Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency, Finally we introduce a family of post-processing techniques that outperform prior ones. We applied these techniques to generate an agent for two-player no-limit Texas Hold'em.
2K_dev_809	Bayesian theory has provided a compelling conceptualization for perceptual inference in the brain, Central to Bayesian inference is the notion of statistical priors, To understand the neural mechanisms of Bayesian inference, we need to understand the neural representation of statistical regularities in the natural environment These findings demonstrate that there is a relationship between the functional connectivity observed in the visual cortex and the statistics of natural scenes, They also suggest that the Boltzmann machine can be a viable model for conceptualizing computations in the visual cortex and, as such can be used to predict neural circuits in the visual cortex from natural scene statistics. In this paper we investigated empirically how statistical regularities in natural 3D scenes are represented in the functional connectivity of disparity-tuned neurons in the primary visual cortex of primates, to learn from 3D natural scenes. And found that the units in the model exhibited cooperative and competitive interactions, forming a disparity association field, analogous to the contour association field The cooperative and competitive interactions in the disparity association field are consistent with constraints of computational models for stereo matching, and found the results to be consistent with neurophysiological data in terms of the functional connectivity measurements between disparity-tuned neurons in the macaque primary visual cortex. A Boltzmann machine model. We applied In addition, we simulated neurophysiological experiments on the model.
2K_dev_811	Of optimal voting under adversarial noise. Show that our approach produces significantly more accurate rankings than alternative approaches. We present the first model From this viewpoint, voting rules are seen as error-correcting codes : their goal is to correct errors in the input rankings and recover a ranking that is close to the ground truth We derive worst-case bounds on the relation between the average accuracy of the input votes, and the accuracy of the output ranking. Empirical results from real data.
2K_dev_812	The use of deductive techniques, such as theorem provers, has several advantages in safety verification of hybrid systems. There is often a gap, however between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide, To address this deficiency that allows the theorem prover KeYmaera to locally reason about behaviors. We present an extension to the deductive verification framework of differential dynamic logic by leveraging forward invariant sets provided by external methods, such as numerical techniques and designer insights Our key contribution is a new inference rule, the forward invariant cut rule, introduced into the proof calculus of KeYmaera. We demonstrate the cut rule in action on an example involving an automotive powertrain control systems, in which we make use of a simulation-driven numerical technique to compute a local barrier function.
2K_dev_814	When solving extensive-form games with large action spaces, typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective, When this occurs a procedure is needed to interpret actions of the opponent that fall outside of our abstraction ( by mapping them to actions in our abstraction ), This is called an action translation mapping. Prior action translation mappings have been based on heuristics without theoretical justification, We show that the prior mappings are highly exploitable and that most of them violate certain natural desiderata, that satisfies these desiderata. Our mapping performs competitively with the prior mappings. We present a new mapping and has significantly lower exploitability than the prior mappings Furthermore, we observe that the cost of this worst-case performance benefit ( low exploitability ) is not high in practice ; We also observe several paradoxes that can arise when performing action abstraction and translation ; for example, we show that it is possible to improve performance by including suboptimal actions in our abstraction and excluding optimal actions. Against no-limit Texas Hold'em agents submitted to the 2012 Annual Computer Poker Competition.
2K_dev_815	Problems of this nature arise in formal verification of continuous and hybrid dynamical systems, where there is an increasing need for methods to expedite formal proofs. This paper studies sound proof rules for checking positive invariance of algebraic and semi-algebraic sets, that is sets satisfying polynomial equalities and those satisfying finite boolean combinations of polynomial equalities and inequalities, under the flow of polynomial ordinary differential equations. The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting. We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of benchmarks.
2K_dev_817	How can we efficiently decompose a tensor into sparse factors, when the data do not fit in memory ?, enabling reproducibility of our work. Tensor decompositions have gained a steadily increasing popularity in data-mining applications ; however, the current state-of-art decomposition algorithms operate on main memory and do not scale to truly large datasets, for speeding up tensor decompositions. Indicate over 90p sparser outputs and 14 times faster execution, with approximation error close to the current state of the art irrespective of computation and memory requirements, demonstrating its effectiveness for data-mining practitioners. In this work we propose P ar C ube, a new and highly parallelizable method that is well suited to produce sparse approximations In particular, we are the first to analyze the very large N ell dataset using a sparse tensor decomposition, demonstrating that P ar C ube enables us to handle effectively and efficiently very large datasets Finally, we make our highly scalable parallel implementation publicly available. Experiments with even moderately large data We provide theoretical guarantees for the algorithms correctness and we experimentally validate our claims through extensive experiments, including four different real world datasets ( E nron, L bnl F acebook and N ell ).
2K_dev_829	To probabilistic segmentation and modeling of time series data for solving the resulting ( large ) optimization problems for estimating recurring clusters. The resulting methods often perform as well or better than existing latent variable models, while being substantially easier to train. We present a convex approach Our approach builds upon recent advances in multivariate total variation regularization, and seeks to learn a separate set of parameters for the distribution over the observations at each time point, but with an additional penalty that encourages the parameters to remain constant over time, We propose efficient optimization methods, and a two-stage procedure under such models, based upon kernel density estimation. Finally we show on a number of real-world segmentation tasks.
2K_dev_831	Given a large collection of co-evolving online activities, such as searches for the keywords `` Xbox '', `` PlayStation '' and `` Wii '', how can we find patterns and rules ? Are these keywords related ? If so, are they competing against each other ? Can we forecast the volume of user activity for the coming month ?. We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food for mining large-scale co-evolving online activities. Show that ECOWEB is effective, in that it can capture long-range dynamics and meaningful patterns such as seasonalities, and practical in that it can provide accurate long-range forecasts, ECOWEB consistently outperforms existing methods in terms of both accuracy and execution speed. We present ECOWEB ( i, Ecosystem on the Web ), which is an intuitive model designed as a non-linear dynamical system Our second contribution is a novel, parameter-free and scalable fitting algorithm, ECOWEB-FIT that estimates the parameters of ECOWEB. Extensive experiments on real data.
2K_dev_833	With the rise of online social networks and smartphones that record the user 's location, a new type of online social network has gained popularity during the last few years, the so called Location-based Social Networks ( LBSNs ), In such networks users voluntarily share their location with their friends via a check-in, In exchange they get recommendations tailored to their particular location as well as special deals that businesses offer when users check-in frequently, LBSNs started as specialized platforms such as Gowalla and Foursquare, however their immense popularity has led online social networking giants like Facebook to adopt this functionality, The spatial aspect of LBSNs directly ties the physical with the online world, creating a very rich ecosystem where users interact with their friends both online as well as declare their physical ( co- ) presence in various locations. Such a rich environment calls for novel analytic tools that can model the aforementioned types of interactions, By doing so we identify tightly knit, hidden communities of users and locations which they frequent to study the temporal dynamics of hidden communities in LBSNs. In this work we propose to model and analyze LBSNs using Tensors and Tensor Decompositions, powerful analytical tools that have enjoyed great growth and success in fields like Machine Learning, Data Mining and Signal Processing alike, In addition to Tensor Decompositions, we use Signal Processing tools that have been previously used in Direction of Arrival ( DOA ) estimations.
2K_dev_834	Prior work has among other techniques, used canonical correlation analysis to project pre-trained vectors in two languages into a common space. This work focuses on the task of finding latent vector representations of the words in a corpus, In particular we address the issue of what to do when there are multiple languages in the corpus. That our method outperforms prior work on multilingual tasks, matches the performance of prior work on monolingual tasks, and scales linearly with the size of the input data ( and thus the number of languages being embedded ). We propose a simple and scalable method that is inspired by the notion that the learned vector representations should be invariant to translation between languages.
2K_dev_838	For general convex programming. Show that this often improves running times by an order of magnitude or more vs, existing approaches based on conic solvers. We present Epsilon a system using fast linear and proximal operators, As with existing convex programming frameworks, users specify convex optimization problems using a natural grammar for mathematical expressions, composing functions in a way that is guaranteed to be convex by the rules of disciplined convex programming Given such an input, the Epsilon compiler transforms the optimization problem into a mathematically equivalent form consisting only of functions with ecient proximal operators|an intermediate representation we refer to as prox-ane form, By reducing problems to this form, Epsilon enables solving general convex problems using a large library of fast proximal and linear operators ;. Numerical examples on many popular problems from statistics and machine learning.
2K_dev_844	Unease over data privacy will retard consumer acceptance of IoT deployments, The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud. This is a direct consequence of the over-centralization of today 's cloud-based IoT hub designs, We propose a solution. That interposes a locally-controlled software component called a privacy mediator on every raw sensor stream Each mediator is in the same administrative domain as the sensors whose data is being collected, and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain This solution necessitates a logical point of presence for mediators within the administrative boundaries of each organization, Such points of presence are provided by cloudlets, which are small locally-administered data centers at the edge of the Internet that can support code mobility The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility.
2K_dev_853	The widespread use of social networks enables the rapid diffusion of information, news among users in very large communities, It is a substantial challenge to be able to observe and understand such diffusion processes, which may be modeled as networks that are both large and dynamic A key tool in this regard is data summarization, However few existing studies aim to summarize graphs/networks for dynamics, Dynamic networks raise new challenges not found in static settings, including time sensitivity and the needs for online interestingness evaluation and summary traceability, which render existing techniques inapplicable, The study offers insight into the effectiveness and design properties of OSNet. We study the topic of dynamic network summarization : how to summarize dynamic networks with millions of nodes by only capturing the few most interesting nodes or edges over time, and we address the problem by finding interestingness-driven diffusion processes. Based on the concepts of diffusion radius and scope, we define interestingness measures for dynamic networks, and we propose OSNet, an online summarization framework. We report on extensive experiments with both synthetic and real-life data.
2K_dev_854	Session types provide a means to prescribe the communication behavior between concurrent message-passing processes However, in a distributed setting, some processes may be written in languages that do not support static typing of sessions or may be compromised by a malicious intruder, violating invariants of the session types. In such a setting, dynamically monitoring communication between processes becomes a necessity for identifying undesirable actions, In this paper we show how to dynamically monitor communication to enforce adherence to session types in a higher-order setting. We prove that dynamic monitoring does not change system behavior for welltyped processes, and that one of an indicated set of possible culprits must have been compromised in case of an alarm. We present a system of in the case when the monitor detects an undesirable action and an alarm is raised.
2K_dev_860	One typically proves infeasibility in satisfiability/ constraint satisfaction ( or optimality in integer programming ) by constructing a tree certificate. However deciding how to branch in the search tree is hard, and impacts search time drastically, using information gathered by that dart to guide what to do next. We explore the power of a simple paradigm, that of throwing random darts into the assignment space and then This method seems to work well when the number of short certificates of infeasibility is moderate, suggesting that the overhead of throwing darts is more than paid for by the information gained by these darts.
2K_dev_871	This paper introduces reasoning about lawful behavior as an important computational thinking skill and provides examples from a novel introductory programming curriculum using Microsoft 's Kodu Game Lab. Showing that rising 5th and 6th graders can understand the lawfulness of Kodu programs, We also discuss some misconceptions students may develop about Kodu, their causes and potential remedies. We present an analysis of assessment data.
2K_dev_876	User-generated online reviews can play a significant role in the success of retail products. However review systems are often targeted by opinion spammers who seek to distort the perceived quality of a product by creating fraudulent reviews, for spotting fraudsters and fake reviews in online review datasets. Where FRAUDEAGLE successfully reveals fraud-bots in a large online app review database. We propose a fast and effective framework, FRAUDEAGLE Our method has several advantages : ( 1 ) it exploits the network effect among reviewers and products, unlike the vast majority of existing methods that focus on review text or behavioral analysis, ( 2 ) it consists of two complementary steps ; scoring users and reviews for fraud detection, and grouping for visualization and sensemaking, ( 3 ) it operates in a completely unsupervised fashion requiring no labeled data, while still incorporating side information if available, and ( 4 ) it is scalable to large datasets as its run time grows linearly with network size. We demonstrate the effectiveness of our framework on syntheticand real datasets ;.
2K_dev_877	Networked or telematic music performances take many forms, ranging from small laptop ensembles using local area networks to long-distance musical collaborations using audio and video links, Two important concerns for any networked performance are :. ( 1 ) what is the role of communication in the music performance ? In particular, what are the esthetic and pragmatic justifications for performing music at a distance, and ( 2 ) how are the effects of communication latency ameliorated or incorporated into the performance ? In addition to addressing these two concerns. Which achieved a coordinated performance involving 68 computer musicians, each with their own connection to the network. A recent project the Global Net Orchestra, the technical aspects of the project.
2K_dev_878	Given a large graph with several millions or billions of nodes and edges, such as a social network, how can we explore it efficiently and find out what is in the data ? that enables the comprehensive analysis of large graphs It automatically extracts graph invariants It interactively visualizes univariate and bivariate distributions for those invariants It summarizes the properties of the nodes that the user selects It efficiently visualizes the induced subgraph of a selected node and its neighbors. In this demo we present Perseus, a large-scale system by supporting the coupled summarization of graph properties and structures, guiding attention to outliers, and allowing the user to interactively explore normal and anomalous node behaviors Specifically, Perseus provides for the following operations : 1 ) ( e, degree PageRank real eigenvectors ) by performing scalable, offline batch processing on Hadoop ; 2 ) ; 3 ) ; 4 ), by incrementally revealing its neighbors. In our demonstration we invite the audience to interact with Perseus to explore a variety of multi-million-edge social networks including a Wikipedia vote network, a friendship/foeship network in Slashdot, and a trust network based on the consumer review website Epinions.
2K_dev_880	Besides the application to password generation, our proposed Human Usability Model ( HUM ) will have other applications. What can a human compute in his/her head that a powerful adversary can not infer ? To answer this question, we define a model of human computation and a measure of security. We show that our password generation methods are humanly computable and, to a well-defined extent. Then motivated by the special case of password creation, we propose a collection of well-defined password-generation methods, For the proof of security, we posit that password generation methods are public, but that the humans privately chosen seed is not, and that the adversary will have observed only a few input-output pairs.
2K_dev_882	Suppose you are a teacher, and have to convey a set of object-property pairs ( 'lions eat meat ', or 'aspirin is a blood-thinner ' ), A good teacher will convey a lot of information, with little effort on the student side, Specifically given a list of objects ( like animals or medical drugs ) and their associated properties, what is the best and most intuitive way to convey this information to the student, without the student being overwhelmed ? A related, harder problem is : how can we assign a numerical score to each lesson plan ( i, way of conveying information ) ?. Here we give a formal definition of this problem of forming learning units. It is effective achieving excellent results on real data, both with respect to our proposed metric, but also with respect to encoding length, demonstrate the effectiveness of HYTRA. And we provide a metric for comparing different approaches based on information theory, We also design a multi-pronged algorithm, HYTRA for this problem, Our proposed HYTRA is scalable ( near-linear in the dataset size ), and it is intuitive, conforming to well-known educational principles, such as grouping related concepts, and `` comparing '' and `` contrasting ''. Experiments on real and synthetic datasets.
2K_dev_884	Differential dynamic logic is a logic for specifying and verifying safety, liveness and other properties about models of cyber-physical systems, Theorem provers based on differential dynamic logic have been used to verify safety properties for models of self-driving cars and collision avoidance protocols for aircraft, Examples include : an unambiguous separation between proof checking and proof search, the ability to extract program traces corresponding to counter-examples, and synthesis of surely-live deterministic programs from liveness proofs for nondeterministic programs. Unfortunately these theorem provers do not have explicit proof terms, which makes the implementation of a number of important features unnecessarily complicated without soundness-critical and extra-logical extensions to the theorem prover, with such an explicit representation of proofs, To support axiomatic theorem proving. This paper presents a differential dynamic logic The resulting logic extends both the syntax and semantics of differential dynamic logic with proof terms -- syntactic representations of logical deductions the logic allows equivalence rewriting deep within formulas and supports both uniform renaming and uniform substitutions.
2K_dev_888	Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet. In this article we exploit the relations among different modalities in a multimedia database and for general multimodal data mining problem In addition, in order to reduce the demanding computation to develop an effective and efficient solution to the multimodal data mining problem. Present a framework where image annotation and image retrieval are considered as the special cases Specifically, the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables, we propose a new max margin structure learning approach called Enhanced Max Margin Learning ( EMML ) framework, which is much more efficient with a much faster convergence rate than the existing max margin learning methods, as verified through empirical evaluations, Furthermore we apply EMML framework that is highly scalable in the sense that the query response time is independent of the database scale The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database, and excels many existing multimodal data mining methods in the literature that do not scale up at all. The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases.
2K_dev_900	How do people interact with their Facebook wall ? At a high level, this question captures the essence of our work, While most prior efforts focus on Twitter, the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study, Our work provides a solid step towards a systematic and quantitative wall-centric profiling of Facebook user activity. In this work we model Facebook user behavior : we analyze the wall activities of users focusing on identifying common patterns and surprising phenomena to fit our data. Our key results can be summarized in the following points, First we find that many wall activities, including number of posts, number of likes number of posts of type photo, can be described by the PowerWall distribution, What is more surprising is that most of these distributions have similar slope, with a value close to 1 ! Second, we show how our patterns and metrics can help us spot surprising behaviors and anomalies, For example we find a user posting every two days, exactly the same count of posts ; another user posting at midnight, with no other activity before or after. We propose PowerWall a lesser known heavy-tailed distribution. We conduct an extensive study of roughly 7K users over three years during four month intervals each year.
2K_dev_903	The preferred treatment for kidney failure is a transplant ; however, demand for donor kidneys far outstrips supply, Kidney exchange an innovation where willing but incompatible patient-donor pairs can exchange organsvia barter cycles and altruist-initiated chainsprovides a life-saving alternative, Typically fielded exchanges act myopically, considering only the current pool of pairs when planning the cycles and chains. Yet kidney exchange is inherently dynamic, with participants arriving and departing, Also many planned exchange transplants do not go to surgery due to various failures, So it is important to consider the future when matching, for learning to match in a general dynamic model. It results in higher values of the objective it yields better solutions for the efficient objective ( which does not incorporate equity ) than traditional myopic matching that uses the efficiency objective. Motivated by our experience running the computational side of a large nationwide kidney exchange, we present FUTURE-MATCH a framework FUTUREMATCH takes as input a high-level objective ( e, `` maximize graft survival of transplants over time '' ) decided on by experts, then automatically ( i ) learns based on data how to make this objective concrete and ( ii ) learns the `` means '' to accomplish this goala task, in our experience that humans handle poorly It uses data from all live kidney transplants in the US since 1987 to learn the quality of each possible match ; it then learns the potentials of elements of the current input graph offline ( e, potentials of pairs based on features such as donor and patient blood types ), translates these to weights, and performs a computationally feasible batch matching that incorporates dynamic, failure-aware considerations through the weights. We validate FUTUREMATCH on real fielded exchange data Furthermore, even under economically inefficient objectives that enforce equity.
2K_dev_906	There is often a large disparity between the size of a game we wish to solve and the size of the largest instances solvable by the best algorithms ; for example, a popular variant of poker has about 10165 nodes in its game tree, while the currently best approximate equilibrium-finding algorithms scale to games with around 1012 nodes, In order to approximate equilibrium strategies in these games, the leading approach is to create a sufficiently small strategic approximation of the full game, called an abstraction and to solve that smaller game instead, The leading abstraction algorithm for imperfect-information games generates abstractions that have imperfect recall and are distribution aware, using k-means with the earth mover 's distance metric to cluster similar states together, A distribution-aware abstraction groups states together at a given round if their full distributions over future strength are similar ( as opposed to, for example just the expectation of their strength ), The leading algorithm considers distributions over future strength at the final round of the game. However one might benefit by considering the trajectory of distributions over strength in all future rounds, not just the final round, An abstraction algorithm that takes all future rounds into account is called potential aware, for computing potential-aware imperfect-recall abstractions. Show that our algorithm improves performance over the previously best approach. We present the first algorithm using earth mover 's distance. Experiments on no-limit Texas Hold'em.
2K_dev_908	For the important task of binocular depth perception from complex natural-image stimuli, the neurophysiological basis for disambiguating multiple matches between the eyes across similar features has remained a long-standing problem, Recurrent interactions among binocular disparity-tuned neurons in the primary visual cortex ( V1 ) could play a role in stereoscopic computationsbyalteringresponsesto favorthemost likelydepthinterpretation fora givenimagepair, Psychophysicalresearch has shown that binocular disparity stimuli displayed in 1 region of the visualfield can be extrapolated into neighboring regions that contain ambiguous depth information, by cooperative algorithms play an important role in solving the stereo correspondence problem. We tested whether neurons in macaque V1 interact in a similar manner. And found that unambiguous binocular disparity stimuli displayed in the surrounding visualfields of disparity-selective V1 neurons indeed modified their responses when either bistable stereoscopic or uniform featureless stimuli were presented within their receptivefield centers, The delayed timing of the response behavior compared with the timing of classical surround suppression and multiple control experiments suggests that these modulations are carried out by slower disparity-specific recurrentconnectionsamongV1neurons.
2K_dev_909	Regret matching is a widely-used algorithm for learning how to. We begin by proving that regrets on actions in one setting ( game ) can be transferred to warm start the regrets for solving a different setting with same structure but different payoffs that can be written as a function of parameters, for large incomplete-information games. We prove how this can be done by carefully discounting the prior regrets, This provides to our knowledge, the first principled warm-starting method It also extends to warm-starting the widely-adopted counterfactual regret minimization ( CFR ) algorithm We then study optimizing a parameter vector for a player in a two-player zero-sum game ( e, optimizing bet sizes to use in poker ), We propose a custom gradient descent algorithm that provably finds a locally optimal parameter vector while leveraging our warm-start theory to significantly save regret-matching iterations at each step, It optimizes the parameter vector while simultaneously finding an equilibrium This amounts to the first action abstraction algorithm ( algorithm for selecting a small number of discrete actions to use from a continuum of actions -- a key preprocessing step for solving large games using current equilibrium-finding algorithms ) with convergence guarantees for extensive-form games. We show this experimentally as well We present experiments in no-limit Leduc Hold'em and nolimit Texas Hold'em to optimize bet sizing.
2K_dev_910	For many reasons one might consider mechanisms, or social choice functions, that only have access to the ordinal rankings of alternatives by the individual agents rather than their utility functions. We adopt a utilitarian perspective on social choice, assuming that agents have ( possibly latent ) utility functions over some space of alternatives, In this context one possible objective for a social choice function is the maximization of ( expected ) social welfare relative to the information contained in these rankings We study such optimal social choice functions and underscore the important role played by scoring functions. Sample complexity results for the class of scoring functions. Under three different models, In our worst-case model, no assumptions are made about the underlying distribution and we analyze the worst-case distortion-or degree to which the selected alternative does not maximize social welfare-of optimal ( randomized ) social choice functions, In our average-case model, we derive optimal functions under neutral ( or impartial culture ) probabilistic models Finally, a very general learning-theoretic model allows for the computation of optimal social choice functions ( i, ones that maximize expected social welfare ) under arbitrary, sampleable distributions In the latter case, we provide both algorithms and. And further validate the approach empirically.
2K_dev_913	For decades researchers have struggled with the problem of envy-free cake cutting : how to divide a divisible good between multiple agents so that each agent likes his own allocation best. Although an envy-free cake cutting protocol was ultimately devised, it is unbounded in the sense that the number of operations can be arbitrarily large, depending on the preferences of the agents, We ask whether bounded protocols exist when the agents ' preferences are restricted. Our main result is an envy-free cake cutting protocol for agents with piecewise linear valuations, which requires a number of operations that is polynomial in natural parameters of the given instance.
2K_dev_915	For differential dynamic logic. The resulting axiomatization of differential dynamic logic is proved to be sound and relatively complete. This article introduces a relatively complete proof calculus ( dL ) that is entirely based on uniform substitution, a proof rule that substitutes a formula for a predicate symbol everywhere Uniform substitutions make it possible to use axioms instead of axiom schemata, thereby substantially simplifying implementations Instead of subtle schema variables and soundness-critical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones, the resulting calculus adopts only a finite number of ordinary dLformulas as axioms, which uniform substitutions instantiate soundly, The static semantics of differential dynamic logic and the soundness-critical restrictions it imposes on proof steps is captured exclusively in uniform substitutions and variable renamings as opposed to being spread in delicate ways across the prover implementation In addition to sound uniform substitutions, this article introduces differential forms for differential dynamic logic that make it possible to internalize differential invariants, differential substitutions and derivatives as first-class axioms to reason about differential equations axiomatically.
2K_dev_926	How can we analyze large-scale real-world data with various attributes ? Many real-world data ( e, network traffic logs web data, social networks knowledge bases, and sensor streams ) with multiple attributes are represented as multi-dimensional arrays, For analyzing a tensor, tensor decompositions are widely used in many data mining applications : detecting malicious attackers in network traffic logs ( with source IP, destination IP port-number timestamp ), finding telemarketers in a phone call history ( with sender, receiver date ) and identifying interesting concepts in a knowledge base ( with subject. However current tensor decomposition methods do not scale to large and sparse real-world tensors with millions of rows and columns and `fibers for large-scale tensor decompositions. And discover hidden concepts. In this paper we propose HaTen2, a distributed method that runs on the MapReduce framework, Our careful design and implementation of HaTen2 dramatically reduce the size of intermediate data and the number of jobs leading to achieve high scalability compared with the state-of-the-art method. Thanks to HaTen2 we analyze big real-world sparse tensors that can not be handled by the current state of the art.
2K_dev_927	Mental simulation is an important skill for program understanding and prediction of program behavior Finally, we present recommendations for question prompt design to foster better student simulation of program execution. Assessing students ' ability to mentally simulate program execution can be challenging in graphical programming environments and on paper-based assessments, for assessing students ability to mentally simulate and predict code behavior. Analysis of student responses suggest that this type of question can be used to identify misconceptions and misinterpretation of instructions. This poster presents the iterative design and refinement process using a novel introductory computational thinking curriculum for Microsoft 's Kodu Game Lab. We present an analysis of question prompts and student responses from data collected from three rising 3rd - 6th graders where the curriculum was implemented.
2K_dev_928	We use the notion of balance to give a more fine-grained understanding of several well-studied routing questions that are considerably harder in directed graphs for computing low-radius decompositions of directed graphs parameterized by balance. We show that using our approximate maximum flow algorithm, we can efficiently determine whether a given directed graph is -balanced. We introduce the notion of balance for directed graphs : aweighted directed graph is -balanced if for every cut S V, the total weight of edges going from S to V S is within factor of the total weight of edges going from V S to S, We first revisit oblivious routings in directed graphs, Our main algorithmic result is an oblivious routing scheme for single-source instances that achieve an O ( log 3 n / loglog n ) competitive ratio, In the process we make several technical contributions which may be of independent interest, In particular we give an efficient algorithm We also define and construct low-stretch arborescences, a generalization of low-stretch spanning trees to directed graphs, On the negative side, we present new lower bounds for oblivious routing problems on directed graphs We show that the competitive ratio of oblivious routing algorithms for directed graphs is ( n ) in general ; this result improves upon the long-standing best known lower bound of ( n ) by Hajiaghayi et al, We also show that our restriction to single-source instances is necessary by showing an ( n ) lower bound for multiple-source oblivious routing in Eulerian graphs, We also study the maximum flow problem in balanced directed graphs with arbitrary capacities We develop an efficient algorithm that finds an ( 1+ ) -approximate maximum flows in -balanced graphs in time O ( m 2 / 2 ). Additionally we give an application to the directed sparsest cut problem.
2K_dev_929	Virtual machine ( VM ) migration demands distinct properties under resource oversubscription and workload surges. For VMs under contention. We show that our implementation, resolves VM contention up to several times faster than live migration. We present enlightened post-copy, a new mechanism that evicts the target VM with fast execution transfer and short total duration This design contrasts with common live migration, which uses the down time of the migrated VM as its primary metric ; it instead focuses on recovering the aggregate performance of the VMs being affected, In enlightened post-copy the guest OS identifies memory state that is expected to encompass the VM 's working set The hypervisor accordingly transfers its state, mitigating the performance impact on the migrated VM resulting from post-copy transfer. With modest instrumentation in guest Linux.
2K_dev_941	Sequential games of perfect information can be solved by backward induction, where solutions to endgames are propagated up the game tree. However this does not work in imperfect-information games because different endgames can contain states that belong to the same information set and can not be treated independently, In fact we show that this approach can fail even in a simple game with a unique equilibrium and a single endgame, to conduct endgame solving in a scalable way. Show that our approach leads to significant performance improvements in practice. Nonetheless we show that endgame solving can have significant benefits in imperfectinformation games with large state and action spaces : computation of exact ( rather than approximate ) equilibrium strategies, computation of relevant equilibrium refinements, significantly finer-grained action and information abstraction, new information abstraction algorithms that take into account the relevant distribution of players types entering the endgame, being able to select the coarseness of the action abstraction dynamically, additional abstraction techniques for speeding up endgame solving, a solution to the off-tree problem, and using different degrees of probability thresholding in modeling versus playing, We discuss each of these topics in detail, and introduce techniques that enable one even when the number of states and actions in the game is large. Our experiments on two-player no-limit Texas Holdem poker.
2K_dev_946	Many commercial products and academic research activities are embracing behavior analysis as a technique for improving detection of attacks of many sortsfrom retweet boosting, hashtag hijacking to link advertising, Traditional approaches focus on detecting dense blocks in the adjacency matrix of graph data, and recently the tensors of multimodal data. No method gives a principled way to score the suspiciousness of dense blocks with different numbers of modes and rank them to draw human attention accordingly, to spot dense blocks that are worth inspecting. Where it improves the F1 score over previous techniques by 68percent and finds suspicious behavioral patterns in social datasets spanning 0. In this paper we first give a list of axioms that any metric of suspiciousness should satisfy ; we propose an intuitive, principled metric that satisfies the axioms, and is fast to compute ; moreover, we propose CrossSpot an algorithm typically indicating fraud or some other noteworthy deviation from the usual, and sort them in the order of importance ( suspiciousness ). Finally we apply CrossSpot to the real data.
2K_dev_952	How much has a network changed since yesterday ? How different is the wiring of Bobs brain ( a left-handed male ) and Alices brain ( a right-handed female ), and how is it different ? Graph similarity with given node correspondence, the detection of changes in the connectivity of graphs, arises in numerous settings. In this work we formally state the axioms and desired properties of the graph similarity functions, that assesses the similarity between two graphs on the same nodes that enables attribution of change or dissimilarity to responsible nodes and edges. Showcase the advantages of our method over existing similarity measures. We propose D elta C on, a principled intuitive and scalable algorithm ( e, employees of a company, customers of a mobile carrier In conjunction, we propose D elta C on -A ttr. And evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs, Experiments on various synthetic and real graphs Finally, we employ D elta C on and D elta C on -A ttr on real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs, ( b ) do temporal anomaly detection in the who-emails-whom Enron graph and find the top culprits for the changes in the temporal corporate email graph, and ( c ) recover pairs of test-retest large brain scans ( 17M edges, up to 90M edges ) for 21 subjects.
2K_dev_953	Effective enforcement of laws and policies requires expending resources to prevent and detect offenders, as well as appropriate punishment schemes to deter violators, In particular enforcement of privacy laws and policies in modern organizations that hold large volumes of personal information ( e, hospitals banks ) relies heavily on internal audit mechanisms. We study economic considerations in the design of these mechanisms, focusing in particular on effective resource allocation and appropriate punishment schemes, for resource allocation with an additional punishment parameter. We present an audit game model that is a natural generalization of a standard security game model Computing the Stackelberg equilibrium for this game is challenging because it involves solving an optimization problem with non-convex quadratic constraints We present an additive FPTAS that efficiently computes the solution.
2K_dev_957	Given a large collection of time-evolving activities, such as Google search queries, which consist of d keywords/activities for m locations of duration n, how can we analyze temporal patterns and relationships among all these activities and find location-specific trends ? How do we go about capturing non-linear evolutions of local activities and forecasting future patterns ? For example, assume that we have the online search volume for multiple keywords, `` Nokia/Nexus/Kindle '' or `` CNN/BBC '' for 236 countries/territories, from 2004 to 2015. Our goal is to analyze a large collection of multi-evolving activities, and specifically to answer the following questions : ( a ) Is there any sign of interaction/competition between two different keywords If so, who competes with whom ? ( b ) In which country is the competition strong ? ( c ) Are there any seasonal/annual activities ? ( d ) How can we automatically detect important world-wide ( or local ) events ?. Demonstrate that COMPCUBE consistently outperforms the best state-of- the-art methods in terms of both accuracy and execution speed. We present COMPCUBE a unifying non-linear model, which provides a compact and powerful representation of co-evolving activities ; and also a novel fitting algorithm, COMPCUBE-FIT which is parameter-free and scalable Our method captures the following important patterns : ( B ) asic trends, non-linear dynamics of co-evolving activities, signs of ( C ) ompetition and latent interaction, Nexus ( S ) easonality, a Christmas spike for iPod in the U, and Europe and ( D ) eltas, unrepeated local events such as the U, election in 2008 Thanks to its concise but effective summarization, COMPCUBE can also forecast long-range future activities. Extensive experiments on real datasets.
2K_dev_959	Given a large collection of co-evolving online activities, such as searches for the keywords `` Xbox '', `` PlayStation '' and `` Wii '', how can we find patterns and rules ? Are these keywords related ? If so, are they competing against each other ?. Can we forecast the volume of user activity for the coming month ? We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food for mining large-scale co-evolving online activities. Show that EcoWeb is effective, in that it can capture long-range dynamics and meaningful patterns such as seasonalities, and practical in that it can provide accurate long-range forecasts, EcoWeb consistently outperforms existing methods in terms of both accuracy and execution speed. We present EcoWeb ( i, Ecosystem on the Web ), which is an intuitive model designed as a non-linear dynamical system Our second contribution is a novel, parameter-free and scalable fitting algorithm, EcoWeb-Fit that estimates the parameters of EcoWeb. Extensive experiments on real data.
2K_dev_962	A grand challenge for state estimation in newly built smart grid lies in how to deal with the increasing uncertainties, To solve the problem. Results show that the proposed data-driven approach works well in a smart grid setting with increasing uncertainties and it produces an online state estimate excelling current industrial approach. We propose a data-driven state estimation approach based on recent targeted investment on sensors, data storage and computing devices, An architecture is proposed to use power system physics and pattern to systematically clean historical data and conduct supervised learning, where historical similar measurements and their states are used to learn the relationship between the current measurement and the state, In order to deal with nonlinearity, kernel trick is used to produce linear mapping in a carefully selected higher dimensional space To speed up the data-driven approach for online services, we analyze power system data set and discover its clustering property due to the periodic pattern of power systems, This leads to significant dimension reduction and the idea of preorganizing data points in a tree structure for inquiry, leading to 1000 times speedup.
2K_dev_964	Cyber-physical systems ( CPS ) are heterogeneous, be- cause they tightly couple computation, communication and control along with physical dynamics, which are traditionally considered separately, Without a comprehensive modeling formalism, model- based development of CPS involves using a multitude of models in a variety of formalisms that capture various aspects of the system design, such as software design, networking design physical mod- els. Without a rigorous unifying framework, system integration and integration of the analysis results for vari- ous models remains ad hoc to ensure consistency and enable system-level verification in a hierarchical and compositional manner. In this paper we propose a multi-view architecture framework that treats models as views of the under- lying system structure and uses structural and semantic mappings Index TermsControl design, control engineering formal veri- fication. Throughout the paper the theoretical concepts are illustrated using two examples : a quad- rotor and an automotive intersection collision avoidance system.
2K_dev_966	The maximum Nash welfare ( MNW ) solution -- - which selects an allocation that maximizes the product of utilities -- - is known to provide outstanding fairness guarantees when allocating divisible goods, These results lead us to believe that MNW is the ultimate solution for allocating indivisible goods, and underlie its deployment on a popular fair division website. In particular we prove that it selects allocations that are envy free up to one good -- - a compelling notion that is quite elusive when coupled with economic efficiency. Demonstrate that it scales well. And while it seems to lose its luster when applied to indivisible goods, we show that in fact, the MNW solution is unexpectedly, strikingly fair even in that setting, We also establish that the MNW solution provides a good approximation to another popular ( yet possibly infeasible ) fairness property, the maximin share guarantee, in theory and -- - even more so -- - in practice, While finding the MNW solution is computationally hard, we develop a nontrivial implementation.
2K_dev_968	The safety of mobile robots in dynamic environments is predicated on making sure that they do not collide with obstacles, Our verification results are generic in the sense that they are not limited to the particul ar choices of one specific control algorithm but identify conditions that make them simultaneously apply to a broad class of control algorithms. In support of such safety arguments, we analyze fo r avoiding both stationary and moving obstacles that describe and formally verify the robots discrete control decisions along with it s continuous. We prove that provably safe motion is flexible enough to let the r obot still navigate waypoints and pass intersections Moreover, we formally prove that safety can still be guaranteed despite s ensor uncertainty and actuator perturbation, and when control choices for more aggressive maneuvers are introduced. And formally verify a series of increasingly powerful safety properties of controllers ( i ) static safety, which ensures that no collisions can happen with stationary obstacles, ( ii ) passive safety, which ensures that no collisions can happen with stationary or moving obstacles while the robot moves, ( iii ) the stronger passive friendly safety in which the robot further maintains sufficient maneuvering distance for obstacles to avoid collision as well, and ( iv ) passive orientation safety, which allows for imperfect sensor coverage of the robot, the robot is aw are that not everything in its environment will be visible, We complement these provably correct safety properties with liveness properties : We use hybrid system models and theorem proving techniques.
2K_dev_970	Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral, ) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph, Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look `` normal '', Even worse some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. ( c ) is effective in real-world data, show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud, FRAUDAR successfully detected a subgraph of more than 4000 detected accounts, of which a majority had tweets showing that they used follower-buying services. We propose FRAUDAR an algorithm that ( a ) is camouflage-resistant, ( b ) provides upper bounds on the effectiveness of fraudsters. Experimental results under various attacks Additionally, in real-world experiments with a Twitter follower-followee graph of 1.
2K_dev_977	To execute software long after its creation to encapsulate legacy software. We describe a system called Olive that freezes and precisely reproduces the environment necessary It uses virtual machine ( VM ) technology, complete with all its software dependencies, This legacy world can be completely closed-source : there is no requirement for availability of source code, nor a requirement for recompilation or relinking, The entire VM is streamed over the Internet from a web server, much as video is streamed today.
2K_dev_978	What is the growth pattern of social networks, like Facebook and WeChat ? Does it truly exhibit exponential early growth, as predicted by textbook models like the Bass model, SI or the Branching Process ? How about the count of links, over time for which there are few published models ?. ; and we observe power law growth for both nodes and links, a fact that completely breaks the sigmoid models ( like SI, where NETTIDE gives good fitting accuracy, and more importantly applied on the WeChat data, our NETTIDE forecasted more than 730 days into the future, with 3 % error. In its place we propose NETTIDE, along with differential equations for the growth of the count of nodes, as well as links, Our model accurately fits the growth patterns of real graphs ; it is general, encompassing as special cases all the known, traditional models ( including Bass, SI log-logistic growth ) ; while still remaining parsimonious, requiring only a handful of parameters Moreover, our NETTIDE for link growth is the first one of its kind, accurately fitting real data, and naturally leading to the densification phenomenon. We examine the growth of several real networks, including one of the world 's largest online social network, `` WeChat '' with 300 million nodes and 4, 75 billion links by 2013 We validate our model with four real.
2K_dev_981	How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time ? How do people join the social groups, uniformly or with burst ? What is the pattern of people quitting from groups ? Is there a simple universal model to depict the come-and-go patterns of various groups ? for group evolution. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR For example, we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore we propose a new model comeNgo, a concise yet flexible dynamic model Our model has the following advantages : ( a ) unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation, ( b ) succinctness and interpretability : it contains only six parameters with clear physical meanings, ( c ) accuracy : it can capture various kinds of group evolution patterns preciously and the goodness of fit increase by 58 % over baseline, ( d ) usefulness : it can be used in multiple application scenarios such as forecasting and pattern discovery. In this paper we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users.
2K_dev_982	System management includes the selection of maintenance actions depending on the available observations : when a system is made up by components known to be similar, data collected on one is also relevant for the management of others, This is typically the case of wind farms, which are made up by similar turbines, Optimal management of wind farms is an important task due to high cost of turbines operation and maintenance : in this context, we recently proposed a method for planning and learning at system-level, called PLUS built upon the Partially Observable Markov Decision Process ( POMDP ) framework, which treats transition and emission probabilities as random variables, and is therefore suitable for including model uncertainty. PLUS models the components as independent or identical, In this paper we extend that formulation, allowing for a weaker similarity among components. And discuss its potential and computational complexity. The proposed approach called Multiple Uncertain POMDP ( MU-POMDP ), models the components as POMDPs, and assumes the corresponding parameters as dependent random variables Through this framework, we can calibrate specific degradation and emission models for each component while, at the same time, process observations at system-level. We compare the performance of the proposed MU-POMDP with PLUS.
2K_dev_983	Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type, and the tensor-based summaries look for high-order dense blocks by clustering the values ( including timestamps ) in each dimension. However the human behaviors are multicontextual and dynamic : ( 1 ) each behavior takes place within multiple contexts in a few dimensions, which requires the representation to enable non-value and set-values for each dimension ; ( 2 ) many behavior collections, such as tweets or papers, evolve over time we represent the behavioral data for behavioral summary to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way. CatchTartan outperforms the baselines on both the accuracy and speed, providing comprehensive summaries for the events, human life and scientific development. In this paper as a two-level matrix ( temporal-behaviors by dimensional-values ) and propose a novel representation called Tartan that includes a set of dimensions, the values in each dimension, a list of consecutive time slices and the behaviors in each slice, We further develop a propagation method CatchTartan it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner. We apply CatchTartan to four Twitter datasets up to 10 million tweets and the DBLP data.
2K_dev_987	To the group fused lasso, a convex model that approximates a multi-dimensional signal via an approximately piecewise-constant signal. We show that on these problems the proposed method performs very well, solving the problems faster than state-of-the-art methods and to higher accuracy. We present a new algorithmic approach This model has found many applications in multiple change point detection, signal compression and total variation denoising, though existing algorithms typically using first-order or alternating minimization schemes, In this paper we instead develop a specialized projected Newton method, combined with a primal active set approach, which we show to be substantially faster that existing methods, Furthermore we present two applications that use this algorithm as a fast subroutine for a more complex outer loop : segmenting linear regression models for time series data, and color image denoising.
2K_dev_990	Strong Nash equilibrium ( SNE ) is an appealing solution concept when rational agents can form coalitions, A strategy profile is an SNE if no coalition of agents can benefit by deviating. For SNE finding in games with more than two agents. Validate the overall approach and show that the new conditions significantly reduce search tree size compared to using NE conditions alone. We present the first general-purpose algorithms An SNE must simultaneously be a Nash equilibrium ( NE ) and the optimal solution of multiple non-convex optimization problems, This makes even the derivation of necessary and sufficient mathematical equilibrium constraints difficult, We show that forcing an SNE to be resilient only to pure-strategy deviations by coalitions, unlike for NEs is only a necessary condition here, Second we show that the application of Karush-Kuhn-Tucker conditions leads to another set of necessary conditions that are not sufficient, Third we show that forcing the Pareto efficiency of an SNE for each coalition with respect to coalition correlated strategies is sufficient but not necessary We then develop a tree search algorithm for SNE finding At each node, it calls an oracle to suggest a candidate SNE and then verifies the candidate, We show that our new necessary conditions can be leveraged to make the oracle more powerful.
2K_dev_991	How can we predict Smith 's main hobby if we know the main hobby of Smith 's friends ? Can we measure the confidence in our predic- tion if we are given the main hobby of only a few of Smith 's friends ?. In this paper we focus on how to estimate the confidence on the node classi- fication problem. Results demonstrate that our algorithm outperforms other algorithms on graphs with less smoothness and low label density. Providing a confidence level for the classification prob- lem is important because most nodes in real world networks tend to have few neighbors, and thus a small amount of evidence, Our contributions are three-fold : ( a ) novel algorithm ; we propose a semi-supervised learning algorithm that converges fast, and provides the confidence estimate. ( b ) theoretical analysis ; we show the solid theoretical foundation of our algo- rithm and the connections to label propagation and Bayesian inference ( c ) empirical analysis ; we perform extensive experiments on three dif- ferent real networks Specifically.
2K_dev_993	The fair division of indivisible goods has long been an important topic in economics and, more recently computer science. That is allocations where each player values her own allocated set of goods at least as highly as any other player 's allocated set of goods. We show that even when the number of goods is larger than the number of agents by a linear fraction, envy-free allocations are unlikely to exist, We then show that when the number of goods is larger by a logarithmic factor, such allocations exist with high probability show that the asymptotic behavior of the theory holds even when the number of goods and agents is quite small, We demonstrate that there is a sharp phase transition from nonexistence to existence of envy-free allocations, and that on average the computational problem is hardest at that transition. We investigate the existence of envyfree allocations of indivisible goods. Under additive valuations We support these results experimentally and.
2K_dev_997	Kidney exchanges are organized markets where patients swap willing but incompatible donors, In the last decade, kidney exchanges grew from small and regional to large and national -- -and soon, This growth results in more lives saved, but exacerbates the empirical hardness of the $ \mathcal { NP } $ -complete problem of optimally matching patients to donors. State-of-the-art matching engines use integer programming techniques to clear fielded kidney exchanges, but these methods must be tailored to specific models and objective functions, and may fail to scale to larger exchanges the clearing problem is solvable in polynomial time. Show that indeed small numbers of attributes suffice. In this paper we observe that if the kidney exchange compatibility graph can be encoded by a constant number of patient and donor attributes, We give necessary and sufficient conditions for losslessly shrinking the representation of an arbitrary compatibility graph, Then using real compatibility graphs from the UNOS nationwide kidney exchange, we show how many attributes are needed to encode real compatibility graphs.
2K_dev_1001	We investigate synergy or lack thereof, between agents in co-operative games, building on the popular notion of Shapley value. Our main theoretical result is that any graph specifying synergistic and antagonistic pairs can arise even from a restricted class of cooperative games. We think of a pair of agents as synergistic ( resp, antagonistic ) if the Shapley value of one agent when the other agent participates in a joint effort is higher ( resp, lower ) than when the other agent does not participate We also study the computational complexity of determining whether a given pair of agents is synergistic. Finally we use the concepts developed in the paper to uncover the structure of synergies in two real-world organizations, the European Union and the International Monetary Fund.
2K_dev_1002	How do users behave if they can tag each other in social networks ? Twitter lists can be regarded as the tagging process ; a user ( i, tagger ) creates a list with a name ( i, tag ) and adds other users ( i, tagged users ) into the list, This tagging network is by nature different from the resource tagging networks ( e, Flickr and Delicious ) because users on this network can tag each other, This study sheds light on the underlying characteristics of the interactive tagging network, which is relevant to the social scientists and the system designers of the tagging systems. In this paper we answer this question by studying the interactive tagging network constructed by Twitter lists, We address the following research questions : ( RQ1 ) What is the common patterns and the difference between the interactive tagging network and the resource tagging networks ? ( RQ2 ) Do users tag each other on the interactive tagging network ? And if so, to what extent ? ( RQ3 ) What is the difference between the two types of relationships on Twitter : who-tags-whom and who-follows-whom ?. We found the pervasive patterns across the different tagging networks, and the interactive patterns within the interactive tagging network. By quantitatively studying million-scale networks.
2K_dev_1006	That can significantly lower the barrier for modelers to specify and solve convex stochastic optimization problems. We introduce disciplined convex stochastic programming ( DCSP ), a modeling framework by allowing modelers to naturally express a wide variety of convex stochastic programs in a manner that reflects their underlying mathematical representation DCSP allows modelers to express expectations of arbitrary expressions, partial optimizations and chance constraints across a wide variety of convex optimization problem families ( e, linear quadratic second order cone. We illustrate DCSP 's expressivity through a number of sample implementations of problems drawn from the operations research, finance and machine learning literatures.
2K_dev_1010	This is one of the oldest non-trivial problems in computational geometry yet despite a long history of research the previous fastest running times for computing a ( 1+ ) -approximate geometric median were O ( d n 4/3 8/3 ) by Chin et, al O ( d exp 4 log 1 ) by Badoiu et, al O ( nd + poly ( d, 1 ) ) by Feldman and Langberg, and the polynomial running time of O ( ( nd ) O ( 1 ) log1/ ) by Parrilo and Sturmfels and Xue and Ye. For solving the geometric median problem given n points in d compute a point that minimizes the sum of Euclidean distances to the points In this paper we show how to compute such an approximate geometric median in time O ( nd log 3 n / ) and O ( d 2 ). In this paper we provide faster algorithms While our O ( d 2 ) is a fairly straightforward application of stochastic subgradient descent, our O ( nd log 3 n / ) time algorithm is a novel long step interior point method, We start with a simple O ( ( nd ) O ( 1 ) log1/ ) time interior point method and show how to improve it, ultimately building an algorithm that is quite non-standard from the perspective of interior point literature, Our result is one of few cases of outperforming standard interior point theory, Furthermore it is the only case we know of where interior point methods yield a nearly linear time algorithm for a canonical optimization problem that traditionally requires superlinear time.
2K_dev_1014	A novel extension of normal form games. Our main result is that biased games satisfying certain mild conditions always admit an equilibrium. We present that we call biased games, In these games a player 's utility is influenced by the distance between his mixed strategy and a given base strategy, We argue that biased games capture important aspects of the interaction between software agents We also tackle the computation of equilibria in biased games.
2K_dev_1015	A kidney exchange is an organized barter market where patients in need of a kidney swap willing but incompatible donors, Determining an optimal set of exchanges is theoretically and empirically hard Traditionally, exchanges took place in cycles, with each participating patient-donor pair both giving and receiving a kidney, The recent introduction of chains, where a donor without a paired patient triggers a sequence of donations without requiring a kidney in return, increased the efficacy of fielded kidney exchanges -- -while also dramatically raising the empirical computational hardness of clearing the market in practice Finally, we note that our position-indexed chain-edge formulation can be modified in a straightforward way to take post-match edge failure into account, under the restriction that edges have equal probabilities of failure, Post-match edge failure is a primary source of inefficiency in presently-fielded kidney exchanges. While chains can be quite long, unbounded-length chains are not desirable : planned donations can fail before transplant for a variety of reasons, and the failure of a single donation causes the rest of that chain to fail, so parallel shorter chains are better in practice. We show that our new models are competitive with all existing solvers -- -in many cases outperforming all other solvers by orders of magnitude. In this paper we address the tractable clearing of kidney exchanges with short cycles and chains that are long but bounded, This corresponds to the practice at most modern fielded kidney exchanges, We introduce three new integer programming formulations, two of which are compact Furthermore, one of these models has a linear programming relaxation that is exactly as tight as the previous tightest formulation ( which was not compact ) for instances in which each donor has a paired patient, We show how to implement such failure-aware matching in our model, and also extend the state-of-the-art general branch-and-price-based non-compact formulation for the failure-aware problem to run its pricing problem in polynomial time. On real data from the UNOS nationwide exchange in the United States and the NLDKSS nationwide exchange in the United Kingdom, as well as on generated realistic large-scale data.
2K_dev_1016	Kidney exchange is a barter market where patients trade willing but medically incompatible donors, These trades occur via cycles, where each patient-donor pair both gives and receives a kidney, and via chains which begin with an altruistic donor who does not require a kidney in return For logistical reasons, the maximum length of a cycle is typically limited to a small constant, while chains can be much longer, Given a compatibility graph of patient-donor pairs, altruists and feasible potential transplants between them, finding even a maximum-cardinality set of vertex-disjoint cycles and chains is NP-hard, There has been much work on developing provably optimal solvers that are efficient in practice, One of the leading techniques has been branch and price, where column generation is used to incrementally bring cycles and chains into the optimization model on an as-needed basis, This shows incorrectness of two leading branch-and-price solvers that suggested polynomial-time chain pricing algorithms. We prove that finding a positive-price chain is NP-complete. In particular only positive-price columns need to be brought into the model.
2K_dev_1017	An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy, In a system with redundant requests, each job that arrives to the system is copied and dispatched to multiple servers, As soon as the first copy completes service, the job is considered complete, and all remaining copies are deleted. A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists. We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity. We propose a theoretical model of redundancy, the Redundancy-d system in which each job sends redundant copies to d servers chosen uniformly at random, We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers.
2K_dev_1018	Complex networks have been shown to exhibit universal properties, with one of the most consistent patterns being the scale-free degree distribution. But are there regularities obeyed by the r-hop neighborhood in real networks ? We answer this question. And we show the pervasiveness of the power-hop. By identifying another power-law pattern that describes the relationship between the fractions of node pairs C ( r ) within r hops and the hop count r, This scale-free distribution is pervasive and describes a large variety of networks, ranging from social and urban to technological and biological networks In particular, inspired by the definition of the fractal correlation dimension D2 on a point-set, we consider the hop-count r to be the underlying distance metric between two vertices of the network, and we examine the scaling of C ( r ) with r, We find that this relationship follows a power-law in real networks within the range 2 r d, where d is the effective diameter of the network, that is the 90-th percentile distance We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. We provide theoretical justification for this pattern under successful existing network models, while we analyze a large set of real and synthetic network datasets.
2K_dev_1025	An age-old problem in the design of server farms is the choice of the task assignment policy, This is the algorithm that determines how to assign incoming jobs to servers, Popular policies include Round-Robin assignment, Join-the-Shortest-Queue Join-Queue-with-Least-Work and so on. While much research has studied assignment policies, little has taken into account server-side variability -- the fact that the server we choose might be temporarily and unpredictably slow that replicates each arrival to d servers chosen at random task assignment policy. We show that when server-side variability dominates runtime, replication of jobs can be very beneficial. We introduce the Replication-d algorithm, where the job is considered `` done '' as soon as the first replica completes, We provide an exact closed-form analysis of Replication-d, We next introduce a much more general model, one which takes both the inherent job size distribution and the server-side variability into account, This is a departure from traditional queueing models which only allow for one `` size '' distribution We propose and analyze a new, Replicate-Idle-Queue ( RIQ ), which is designed to perform well given these dual sources of variability.
2K_dev_1030	Large graphs are prevalent in many applications and enable a variety of information dissemination processes, meme virus and influence propagation, How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way ( e, stop a virus propagation, facilitate the propagation of a piece of good idea, etc ) ? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models. In this paper we study the problem of how to optimally place a set of edges ( e, edge deletion and edge addition ) to optimize the leading eigenvalue of the underlying graph, so that we can guide the dissemination process in a desired way for edge deletion and edge addition. In addition we reveal the intrinsic relationship between edge deletion and node deletion problems, results validate the effectiveness and efficiency of the proposed algorithms. We propose effective scalable algorithms.
2K_dev_1032	Recently fair division theory has emerged as a promising approach for allocation of multiple computational resources among agents While in reality agents are not all present in the system simultaneously, previous work has studied static settings where all relevant information is known upfront, We believe that our work informs the design of superior multiagent systems, and at the same time expands the scope of fair division theory by initiating the study of dynamic and fair resource allocation mechanisms. Our goal is to better understand the dynamic setting for dynamic resource allocation mechanisms. On the conceptual level, we develop a dynamic model of fair division, and propose desirable axiomatic properties On the technical level, we construct two novel mechanisms that provably satisfy some of these properties. And analyze their performance using real data.
2K_dev_1034	We revisit the classic problem of estimating the population mean of an unknown single-dimensional distribution from samples Our key question is whether the sample median is the best ( in terms of mean squared error ) truthful estimator of the population mean. Our main result is a characterization of worst-case optimal truthful estimators, which provably outperform the median, for possibly asymmetric distributions with bounded support. Taking a game-theoretic viewpoint In our setting, samples are supplied by strategic agents, who wish to pull the estimate as close as possible to their own value In this setting, the sample mean gives rise to manipulation opportunities, whereas the sample median does not We show that when the underlying distribution is symmetric, there are truthful estimators that dominate the median.
2K_dev_1035	For efficiently solving general convex optimization problems specified as disciplined convex programs ( DCP ). And show it often achieves order of magnitude speedups over existing general-purpose optimization solvers. This paper develops an approach a common general-purpose modeling framework, Specifically we develop an algorithm based upon fast epigraph projections, projections onto the epigraph of a convex function, an approach closely linked to proximal operator methods, We show that by using these operators, we can solve any disciplined convex program without transforming the problem to a standard cone form, as is done by current DCP libraries We then develop a large library of efficient epigraph projection operators, mirroring and extending work on fast proximal algorithms, for many common convex functions. Finally we evaluate the performance of the algorithm.
2K_dev_1036	Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines, which could be of interest to studies on website profiling, Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models. Yet there has been relatively little work in modeling the behaviors and temporal properties of websites, as most research focuses on detecting whether a website distributes malware, In this paper we ask : How does web-based malware spread ? In order to conduct this study, to distinguish between compromised vs. First we find that legitimate but compromised websites constitute 33, 1 % of the malicious websites in our dataset, with an accuracy of 95, 3 % Second we find that malicious URLs can be surprisingly long-lived, with 10 % of malicious sites staying active for three months or more Third, we observe that a significant number of URLs exhibit the same temporal pattern that suggests a flush-crowd behavior, inflicting most of their damage during the first few days of appearance, Finally the distribution of the visits to malicious sites per user is skewed, 4 % of users visiting more than 10 malicious sites in 8 months. We develop a classifier. We conduct an extensive study and follow a website-centric and user-centric point of view We collect data from four online databases, including Symantec 's WINE Project, for a total of more than 600K malicious URLs and over 500K users.
2K_dev_1037	The design of revenue-maximizing combinatorial auctions, multi item auctions over bundles of goods, is one of the most fundamental problems in computational economics, unsolved even for two bidders and two items for sale, In the traditional economic models, it is assumed that the bidders ' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution Despite this strong and oftentimes unrealistic assumption, it is remarkable that the revenue-maximizing combinatorial auction remains unknown The most scalable automated mechanism design algorithms take as input samples from the bidders ' valuation distribution and then search for a high-revenue auction in a rich auction class. In recent years automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design. In this work we provide the first sample complexity analysis In particular, we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying, unknown distribution over bidder valuations, for each of the auction classes in the hierarchy In addition to helping set automated mechanism design on firm foundations, our results also push the boundaries of learning theory, In particular the hypothesis functions used in our contexts are defined through multi stage combinatorial optimization procedures, rather than simple decision boundaries, as are common in machine learning.
2K_dev_1040	For learning sparse graphical models, for fitting the MQGM for sampling from the joint distribution that underlies the MQGM estimate. That demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data. We introduce the Multiple Quantile Graphical Model ( MQGM ), which extends the neighborhood selection approach of Meinshausen and Buhlmann The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others, Our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates We establish that, under suitable regularity conditions, the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic Gaussian data model, We develop an efficient algorithm using the alternating direction method of multipliers, We also describe a strategy. Lastly we present detailed experiments.
2K_dev_1041	'Alice ' is submitting one web search per five minutes, for three hours in a row - is it normal ? How to detect abnormal search behaviors, among Alice and other users ? Is there any distinct pattern in Alice 's ( or other users ' ) search behavior ? to describe such an IAT distribution to capture and explain the two-dimensional, heavy-tail distribution of the parameters. For each user we discover and explain a surprising, bi-modal pattern of the inter-arrival time ( IAT ) of landed queries ( queries with user click-through ) we then notice the correlations among its parameters at the group level. In this paper we present a novel, user-and group-level framework M3A : Model, MetaModel and Anomaly detection Specifically, the model Camel-Log is proposed Thus, we further propose the metamodel Meta-Click, Combining Camel-Log and Meta-Click, the proposed M3A has the following strong points : ( 1 ) the accurate modeling of marginal IAT distribution, ( 2 ) quantitative interpretations, and ( 3 ) anomaly detection. We studied what is probably the largest, publicly available query log that contains more than 30 million queries from 0.
2K_dev_1044	Recent computer systems research has proposed using redundant requests to reduce latency, The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request. However there is no exact analysis of systems with redundancy. We find some surprising results First, the response time of a fully redundant class follows a simple exponential distribution and that of the non-redundant class follows a generalized hyperexponential, Second fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant We find that, in many cases redundancy outperforms JSQ and Opt-Split with respect to overall response time, making it an attractive solution. This paper presents the first exact analysis of systems with redundancy We allow for any number of classes of redundant requests, any number of classes of non-redundant requests, any degree of redundancy, and any number of heterogeneous servers, In all cases we derive the limiting distribution of the state of the system, In small ( two or three server ) systems, we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes, and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy. We also compare redundancy with other approaches for reducing latency, such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and join-the-shortest-queue ( JSQ ) routing of a class.
2K_dev_1059	Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns ? to quantify both concepts ( synchronicity and normality ). C atch S ync consistently outperforms existing competitors, both in detection accuracy by 36p on Twitter and 20p on Tencent Weibo, as well as in speed. We propose a fast and effective method, C atch S ync, which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority We introduce novel measures and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots, Thanks to careful design, C atch S ync has the following desirable properties : ( a ) it is scalable to large datasets, being linear in the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology, without needing labeled data, nor timing information and the like, while still capable of using side information if available. We applied C atch S ync on three large, real datasets 1-billion-edge Twitter social graph, 3-billion-edge and 12-billion-edge Tencent Weibo social graphs, and several synthetic ones ;.
2K_dev_1061	Suppose you are a teacher, and have to convey a set of object-property pairs 'lions eat meat ', A good teacher will convey a lot of information, with little effort on the student side, What is the best and most intuitive way to convey this information to the student, without the student being overwhelmed ?. A related harder problem is : how can we assign a numerical score to each lesson plan i, way of conveying information ? Here, we give a formal definition of this problem of forming learning units and for comparing different approaches for this problem. It is effective achieving excellent results on real data, both with respect to our proposed metric, but also with respect to encoding length demonstrate the effectiveness of groupNteach. We provide a metric based on information theory, We also design an algorithm, groupNteach Our proposed groupNteach is scalable near-linear in the dataset size ; and it is intuitive, conforming to well-known educational principles. Experiments on real and synthetic datasets.
2K_dev_1070	Acute hypotensive episodes ( AHEs ) are serious clinical events in intensive care units ( ICUs ), and require immediate treatment to prevent patient injury. Reducing the risks associated with an AHE requires effective and efficient mining of data generated from multiple physiological time series, to effectively predict AHE. HeartCast was found to outperform other state-of-the-art methods found in the literature with a 13, 7 % improvement in classification accuracy. We propose HeartCast a model that extracts essential features from such data HeartCast combines a non-linear support vector machine with best-feature extraction via analysis of the baseline threshold, quartile parameters and window size of the physiological signals, Our approach has the following benefits : ( a ) it extracts the most relevant features ; ( b ) it provides the best results for identification of an AHE event ; ( c ) it is fast and scales with linear complexity over the length of the window ; and ( d ) it can manage missing values and noise/outliers by using a best-feature extraction method. We performed experiments on data continuously captured from physiological time series of ICU patients ( roughly 3 GB of processed data ).
2K_dev_1073	What is a fair way to assign rooms to several housemates, and divide the rent between them ? This is not just a theoretical question : many people have used the Spliddit website to obtain envy-free solutions to rent division instances, But envy freeness in and of itself, is insufficient to guarantee outcomes that people view as intuitive and acceptable, Based on these results, the maximin solution has been deployed on Spliddit since April 2015. We therefore focus on solutions that optimize a criterion of social justice, subject to the envy freeness constraint, in order to pinpoint the `` fairest '' solutions, that enables the computation of such solutions in polynomial time. And identify the maximin solution, which maximizes the minimum utility subject to envy freeness, as the most attractive that the maximin solution gives rise to significant gains in terms of our optimization objectives demonstrates that people find the maximin solution to be significantly fairer than arbitrary envy-free solutions ; this user study is unprecedented in that it asks people about their real-world rent division instances. We develop a general algorithmic framework. We then study the relations between natural optimization objectives, We demonstrate in theory and using experiments on real data from Spliddit, Finally a user study with Spliddit users as subjects.
2K_dev_1075	Component-based modeling can be used to split large models into partial models to reduce modeling complexity. We study a component-based approach to simplify the challenges of verifying large-scale hybrid systems, Yet verification results also need to transfer from components to composites, to define the structure and behavior of components how to compose components. In this paper we propose a component-based hybrid system verification approach that combines the advantages of component-based modeling e, reduced model complexity with the advantages of formal verification e, guaranteed contract compliance Our strategy is to decompose the system into components, verify their local safety individually and compose them to form an overall system that provably satisfies a global contract, without proving the whole system We introduce the necessary formalism and a technique such that safety properties provably emerge from component safety.
2K_dev_1082	Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in imperfect-information games. However imperfect-recall abstractions are poorly understood, and only weak algorithm-specific guarantees on solution quality are known. They show that running counterfactual regret minimization on such abstractions leads to good strategies in the original games. We develop the first general, algorithm-agnostic solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions, when implemented in the original ( perfect-recall ) game, Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions for which any such results have been obtained Further, our analysis is tighter in two ways, each of which can lead to an exponential reduction in the solution quality error bound, We then show that for extensive-form games that satisfy certain properties, the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem, where the increase in our bound is the distance function This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds, We proceed to show a divide in the class of abstraction problems, If payoffs are at the same scale at all information sets considered for abstraction, the input forms a metric space, and this immediately yields a $ 2 $ -approximation algorithm for abstraction Conversely, if this condition is not satisfied, we show that the input does not form a metric space. Finally we provide computational experiments to evaluate the practical usefulness of the abstraction techniques.
2K_dev_1085	Computational offloading services at the edge of the Internet for mobile devices are becoming a reality. We explore how such infrastructure improves latency and energy consumption relative to the cloud. We present experimental results that confirm substantial wins from edge computing for highly interactive mobile applications. Using a wide range of mobile applications. From WiFi and 4G LTE networks.
2K_dev_1086	Demand response is seeing increased popularity worldwide and industrial loads are actively taking part in this trend, As a host of energy-intensive industrial processes, steel plants have both the motivation and potential to provide demand response. However the scheduling of steel plants is very complex and the involved computations are intense, In this paper we focus on these difficulties to make the computations more tractable. And propose methods such as adding cuts and implementing an application-specific branch and bound algorithm.
2K_dev_1090	Given a large-scale and high-order tensor, how can we find dense blocks in it ? Can we find them in near-linear time but with a quality guarantee ? Extensive previous work has shown that dense blocks in tensors as well as graphs indicate anomalous or fraudulent behavior e, lockstep behavior in social networks. However available methods for detecting such dense blocks are not satisfactory in terms of speed, accuracy or flexibility for finding dense blocks in tensors. Upito 114 $ $ \times $ $ faster than state-of-the-art methods with similar accuracy 4 Effective : M-Zoom successfully detected edit wars and bot activities and spotted network attacks with near-perfect accuracy AUCi=i0, The data and software related to this paper are available at http : //www. In this work we propose M-Zoom, a flexible framework which works with a broad class of density measures, M-Zoom has the following properties : 1 Scalable : M-Zoom scales linearly with all aspects of tensors and is 2 Provably accurate : M-Zoom provides a guarantee on the lowest density of the blocks it finds, 3 Flexible : M-Zoom supports multi-block detection and size bounds as well as diverse density measures. In Wikipedia from a TCP dump.
2K_dev_1096	An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer. For a setting in which the human user can only receive assistance from a semi-trusted computer. For these schemes we prove that forging passwords is equivalent to recovering the secret mapping, Thus our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts. We propose several candidate authentication protocols -- - a computer that stores information and performs computations correctly but does not provide confidentiality Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $, The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate, We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $, for a security parameter $ s ( f ) $ that depends on two key properties of $ f $, To obtain our results, we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $, Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate ), and generalize recent results of Feldman et al.
2K_dev_1098	Counterfactual Regret Minimization ( CFR ) is the most popular iterative algorithm for solving zero-sum imperfect-information games, Regret-Based Pruning ( RBP ) is an improvement that allows poorly-performing actions to be temporarily pruned, thus speeding up CFR. That reduces the space requirements of CFR. We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium, This leads to provably faster convergence and lower space requirements, show that Total RBP results in an order of magnitude reduction in space, and the reduction factor increases with game size. We introduce Total RBP, a new form of RBP as actions are pruned.
2K_dev_1100	For ensuring that verification results about models apply to cyber-physical systems ( CPS ) implementations. A method is presented The invention provides correctness guarantees for CPS executions at runtime Offline verification of CPS models are combined with runtime validation of system executions for compliance with the model, The invention ensures that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model, assuming the system dynamics deviation is bounded If, at some point the observed behavior no longer complies with the model, such that offline verification results no longer apply, provably safe fallback actions are initiated, The invention includes a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic.
2K_dev_1104	Meeting tail latency Service Level Objectives ( SLOs ) in shared cloud networks is both important and challenging, One primary challenge is determining limits on the multi-tenancy such that SLOs are met, Doing so involves estimating latency, which is difficult especially when tenants exhibit bursty behavior as is common in production environments, Nevertheless recent papers in the past two years ( Silo, QJump and PriorityMeister ) show techniques for calculating latency based on a branch of mathematical modeling called Deterministic Network Calculus ( DNC ), The DNC theory is designed for adversarial worst-case conditions, which is sometimes necessary, but is often overly conservative. Typical tenants do not require strict worst-case guarantees, but are only looking for SLOs at lower percentiles ( e, 9th ) for tail latency SLOs. SNC-Meister supports 75 % more tenants than the state-of-the-art. This paper describes SNC-Meister, a new admission control system SNC-Meister improves upon the state-of-the-art DNC-based systems by using a new theory, Stochastic Network Calculus ( SNC ), which is designed for tail latency percentiles Focusing on tail latency percentiles, rather than the adversarial worst-case DNC latency, allows SNC-Meister to pack together many more tenants. : in experiments with production traces.
2K_dev_1106	The best prior complete algorithm has significantly worse complexity and has, to our knowledge never been implemented. For finding an epsilon-Nash equilibrium for arbitrarily small epsilon. We present a complete algorithm in games with more than two players, The main components of our tree-search-based method are a node-selection strategy, an exclusion oracle and a subdivision scheme, The node-selection strategy determines the next region to be explored -- -based on the region 's size and an estimate of whether the region contains an equilibrium The exclusion oracle provides a provably correct sufficient condition for there not to exist an equilibrium in the region, The subdivision scheme determines how the region is split if it can not be excluded, Unlike well-known incomplete methods, our method does not need to proceed locally, which avoids it getting stuck in a local minimum that may be far from any actual equilibrium, The run time grows rapidly with the game size, and this suggests a hybrid scheme where one of the relatively fast prior incomplete algorithms is run, and if it fails to find an equilibrium, then our method is used.
2K_dev_1108	We discuss the implications of our results for market design in general, and kidney exchange in particular. We revisit the problem of designing optimal, individually rational matching mechanisms ( in a general sense, allowing for cycles in directed graphs ), where each player -- -who is associated with a subset of vertices -- -matches as many of his own vertices when he opts into the matching mechanism as when he opts out We offer a new perspective on this problem. Our main result asserts that, any fixed optimal matching is likely to be individually rational up to lower-order terms We also show that a simple and practical mechanism is ( fully ) individually rational, and likely to be optimal up to lower-order terms. By considering an arbitrary graph, but assuming that vertices are associated with players at random.
2K_dev_1109	A descending clock auction ( DCA ) is for buying items from multiple sellers, The literature has focused on the case where each bidder has two options : to accept or reject the offered price. However in many settings -- such as the FCC 's imminent incentive auction -- each bidder may be able to sell one from a set of options, for the dynamics of each bidder 's state to optimize the trajectory of price offers to different bidders for different options. Show that the optimization-based approach dramatically outperforms the percentile-based approach -- because it takes feasibility into account in pricing, Both pricing techniques scale to the large. We present a multi-option DCA ( MDCA ) framework where at each round, the auctioneer offers each bidder different prices for different options, and a bidder may find multiple options still acceptable Setting prices during a MDCA is trickier than in a DCA, We develop a Markov chain model ( which options are still acceptable ), We leverage it This is unlike most auctions which only compute the next price vector, Computing the trajectory enables better planning, We reoptimize the trajectory after each round, Each optimization minimizes total payment while ensuring feasibility in a stochastic sense We also introduce percentile-based approaches to decrementing prices. Experiments with real FCC incentive auction interference constraint data.
2K_dev_1112	Which team is the best in the league ? How does my team fare with respect to the rest of the league ? These are questions that every sports fan is interested in knowing the answers to, In other cases such as in college sports, knowing the answer to these questions is crucial for shaping the picture of spe- cific contests, In professional sports sports networks provide power rankings regularly - typically every week or month de- pending on the season length of the league - based on their experts opinion, We finally propose an ad- vanced ranking technique based on tensor decomposition. Way of ranking sports teams. We show that the cycles in the network are significantly correlated with the performance. In this work we propose an alternative, ob- jective and network-based In brief, our method is based on analyzing a directed network formed between the teams of the corresponding leagues that captures their win-lose relationships Using data from the National Football League and the National Basketball As- sociation, we show that even simple network theory metrics ( e, Page Rank ) can provide a ranking that has the same ac- curacy in predicting winners of upcoming match-ups as more complicated systems ( e. We further explore the impact of the network structure on the prediction accuracy and.
2K_dev_1117	Green and Laffonti ? [ 13 ] proved that one can not generically achieve both. We study efficiency and budget balance for designing mechanisms in general quasi-linear domains. Show that the inefficiency for a simple randomized mechanism is 5 -- -100 times smaller than the worst case, This relative difference increases with the number ofi ? agents. We consider strategyproof budget-balanced mechanisms that are approximately efficient For deterministic mechanisms, we show that a strategyproof and budget-balanced mechanism must have a sink agent whose valuation function is ignored in selecting an alternative, and she is compensated with the payments made by the other agents, We assume the valuations of the agents come from a bounded open interval, This result strengthens Green and Laffont 's impossibility result by showing that even in a restricted domain of valuations, there does not exist a mechanism that is strategyproof, budget balanced and takes every agent 's valuation into consideration -- a corollary of which is that it can not be efficient, Using this result we find a tight lower bound on the inefficiencies of strategyproof, budget-balanced mechanisms in this domain, The bound shows that the inefficiency asymptotically disappears when the number of agents is large -- a result close in spirit to Green and Laffonti ? [ 13, However our results provide worst-case bounds and the best possible rate of convergence Next, we consider minimizing any convex combination of inefficiency and budget imbalance, We show that if the valuations are unrestricted, no deterministic mechanism can do asymptotically better than minimizing inefficiency alone, Finally we investigate randomized mechanisms and provide improved lower bounds on expected inefficiency We give a tight lower bound for an interesting class of strategyproof, We also use an optimization-based approach -- in the spirit of automated mechanism design -- to provide a lower bound on the minimum achievable inefficiency of any randomized mechanism. Experiments with real data from two applications.
2K_dev_1119	The Next-Generation Airborne Collision Avoidance System ( ACAS X ) is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft, It is currently being developed by the Federal Aviation Administration ( FAA ) Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety. Under which the advice given by ACAS X is safe formally verify these configurations. In this paper we determine the geometric configurations under a precise set of assumptions and using hybrid systems theorem proving techniques We consider subsequent advisories and show how to adapt our formal verification to take them into account, We examine the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version, demonstrating how formal hybrid systems proving approaches are helping to ensure the safety of ACAS X.
2K_dev_1121	An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy, In a system with redundant requests, each job that arrives to the system is copied and dispatched to multiple servers, As soon as the first copy completes service, the job is considered complete, and all remaining copies are deleted. A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists. We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity. We propose a theoretical model of redundancy, the Redundancy- d system, in which each job sends redundant copies to d servers chosen uniformly at random, We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers.
2K_dev_1124	The approach finds a dual flow solution to this linear system through a sequence of flow adjustments along cycles. We study the performance of linear solvers for graph Laplacians based on the combinatorial cycle adjustment methodology proposed by [ Kelner-Orecchia-Sidford-Zhu STOC-13 ], for handling these adjustments. Our methods demonstrate significant speedups over previous implementations, and are competitive with standard numerical routines. We study both data structure oriented and recursive methods The primary difficulty faced by this approach, updating and querying long cycles, motivated us to study an important special case : instances where all cycles are formed by fundamental cycles on a length $ n $ path.
2K_dev_1134	That learns the optimal parameters of the neural population model. The empirical results show that the patterns of parameters as a seizure approach and the method is efficient in analyzing nonlinear epilepsy electroencephalogram data, The accuracy of estimating the optimal parameters is improved by using the nonlinear dynamic model. We propose a nonlinear dynamic model for an invasive electroencephalogram analysis via the LevenbergMarquardt algorithm, We introduce the crucial windows where the estimated parameters present patterns before seizure onset The optimal parameters minimizes the error between the observed signal and the generated signal by the model, The proposed approach effectively discriminates between healthy signals and epileptic seizure signals. We evaluate the proposed method using an electroencephalogram dataset with normal and epileptic seizure sequences.
2K_dev_1136	Given `` who-trusts/distrusts-whom '' information, how can we propagate the trust and distrust ? With the appearance of fraudsters in social network sites, the importance of trust prediction has increased. Most such methods use only explicit and implicit trust information ( e, if Smith likes several of Johnson 's reviews, then Smith implicitly trusts Johnson ), but they do not consider distrust to handle all three types of interaction information : explicit trust, implicit trust and explicit distrust. Confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy, achieving up to 50, 4 percentage relative improvement. In this paper we propose PIN -TRUST, a novel method The novelties of our method are the following : ( a ) it is carefully designed, to take into account positive, implicit and negative information, ( b ) it is scalable ( i, linear on the input size ), ( c ) most importantly, it is effective and accurate. Our extensive experiments with a real dataset, com data of 100K nodes and 1M edges.
2K_dev_1139	As mobile computing and cloud computing converge, the sensing and interaction capabilities of mobile devices can be seamlessly fused with compute-intensive and data-intensive processing in the cloud, Cloudlets are important architectural components in this convergence, representing the middle tier of a mobile device cloudlet cloud hierarchy. We show how cloudlets enable a new genre of applications called cognitive assistance applications that augment human perception and cognition for cognitive assistance. We describe a plug-and-play architecture. And a proof of concept using Google Glass.
2K_dev_1152	With the potential to enhance the power system 's operational flexibility in a cost-effective way, demand response is gaining increased attention worldwide, Industrial loads such as cement crushing plants consume large amounts of electric energy and therefore are prime candidates for the provision of significant amounts of demand response, They have the capability to turn on/off an arbitrary number of their crushers thereby adjusting their electric power consumption. However the change in power consumption by cement crushing plants and also other industrial loads are often not granular enough to provide valuable ancillary services such as regulation and load following, to overcome the granularity restriction with the help of an energy storage. In this paper we propose a coordination method based on model predictive control.
2K_dev_1157	The core number of a node is the highest k-core in which the node participates, Core numbers are useful in many graph mining tasks, especially ones that involve finding communities of nodes, influential spreaders and dense subgraphs, Large graphs often do not fit on the memory of a single machine. We address the problem of estimating core numbers of nodes by reading edges of a large graph stored in external memory Existing external memory solutions do not give bounds on the required space, In practice existing solutions also do not scale with the size of the graph which estimates core numbers of nodes. Demonstrate that NimbleCore gives space savings up to 60X, while accurately estimating core numbers with average relative error less than 2. We propose NimbleCore an iterative external-memory algorithm, using O ( n log d max ) space, where n is the number of nodes and d max is the maximum node-degree in the graph, We also show that NimbleCore requires O ( n ) space for graphs with power-law degree distributions. Experiments on forty-eight large graphs from various domains.
2K_dev_1159	How can we design a product or movie that will attract, for example the interest of Pennsylvania adolescents or liberal newspaper critics ? What should be the genre of that movie and who should be in the cast. In this work we seek to identify how we can design new movies with features tailored to a specific user population. And show that it is highly scalable and effectively provides movie designs oriented towards different groups of users, including men women and adolescents. We formulate the movie design as an optimization problem over the inference of user-feature scores and selection of the features that maximize the number of attracted users Our approach, PNP is based on a heterogeneous, tripartite graph of users, movies and features ( e, actors directors genres ), where users rate movies and features contribute to movies, We learn the preferences by leveraging user similarities defined through different types of relations, and show that our method outperforms state-of-the-art approaches, including matrix factorization and other heterogeneous graph-based analysis. We evaluate PNP on publicly available real-world data.
2K_dev_1160	The World Wide Web ( WWW ) has become a rapidly growing platform consisting of numerous sources which provide supporting or contradictory information about claims ( e, `` Chicken meat is healthy '' ), In order to decide whether a claim is true or false, one needs to analyze content of different sources of information on the Web, measure credibility of information sources, and aggregate all these information. This is a tedious process and the Web search engines address only part of the overall problem, producing only a list of relevant sources, estimates credibility of sources and correctness of claims. We demonstrate ClaimEval 's capability in determining validity of a set of claims, resulting in improved accuracy compared to state-of-the-art baselines. In this paper we present ClaimEval, a novel and integrated approach which given a set of claims to validate, extracts a set of pro and con arguments from the Web information sources, and jointly ClaimEval uses Probabilistic Soft Logic ( PSL ), resulting in a flexible and principled framework which makes it easy to state and incorporate different forms of prior-knowledge. Through extensive experiments on real-world datasets.
2K_dev_1161	Human-Computer Music Performance for popular music - where musical structure is important, but where musicians often decide on the spur of the moment exactly what the musical form will be - presents many challenges to make computer systems that are flexible and adaptable to human musicians. One particular challenge is that humans easily follow scores and chord charts, adapt these to new performance plans, and understand media locations in musical terms ( beats and measures ), while computer music systems often use rigid and even numerical representations that are difficult to work with, where musical material in various media is synchronized, where musicians can quickly alter the performance order by specifying ( re- ) arrangements of the material, and where interfaces are supported in a natural way by music notation. We present new formalisms and representations, and a corresponding implementation.
2K_dev_1170	Recent computer systems research has proposed using redundant requests to reduce latency, The idea is to replicate a request so that it joins the queue at multiple servers, The request is considered complete as soon as any one copy of the request completes, Redundancy is beneficial because it allows us to overcome server-side variability the fact that the server we choose might be temporarily slow due to factors such as background load, network interrupts and garbage collection, When there is significant server-side variability, replicating requests can greatly reduce response times, In the past few years, queueing theorists have begun to study redundancy, first via approximations and, more recently via exact analysis, Unfortunately for analytical tractability, most existing theoretical analysis has assumed an Independent Runtimes ( IR ) model, wherein the replicas of a job each experience independent runtimes ( service times ) at different servers. The IR model is unrealistic and has led to theoretical results which can be at odds with computer systems implementation results to decouple the inherent job size ( X ) from the server-side slowdown ( S ). And has provably excellent performance. This paper introduces a much more realistic model of redundancy, Our model allows us where we track both S and X for each job, Analysis within the S & X model is, of course much more difficult, Nevertheless we design a policy, Redundant-to-Idle-Queue ( RIQ ) which is both analytically tractable within the S & X model.
2K_dev_1173	Counterfactual Regret Minimization ( CFR ) is a popular iterative algorithm for approximating Nash equilibria in imperfect-information multi-step two-player zero-sum games. For warm starting CFR. Demonstrate that one can improve overall convergence in a game by first running CFR on a smaller, coarser abstraction of the game and then using the strategy in the abstract game to warm start CFR in the full game. We introduce the first general, principled method Our approach requires only a strategy for each player, and accomplishes the warm start at the cost of a single traversal of the game tree, The method provably warm starts CFR to as many iterations as it would have taken to reach a strategy profile of the same quality as the input strategies, and does not alter the convergence bounds of the algorithms, Unlike prior approaches to warm starting, ours can be applied in all cases Our method is agnostic to the origins of the input strategies, For example they can be based on human domain knowledge, the observed strategy of a strong agent, the solution of a coarser abstraction, or the output of some algorithm that converges rapidly at first but slowly as it gets closer to an equilibrium.
2K_dev_1184	Kidney exchange is a type of barter market where patients exchange willing but incompatible donors, These exchanges are conducted via cycleswhere each incompatible patient-donor pair in the cycle both gives and receives a kidneyand chains, which are started by an altruist donor who does not need a kidney in return. Finding the best combination of cycles and chains is hard The leading algorithms for this optimization problem use either branch and pricea combination of branch and bound and column generationor constraint generation, We show a correctness error in the leading prior branch-and-price-based approach [ Glorie et al, 2014 ] fix to it. Algorithms from our group autonomously make the transplant plans for that exchange, our new solver scales significantly better than the prior leading approaches. We develop a provably correct which also necessarily changes the algorithm 's complexity, as well as other improvements to the search algorithm A cap is desirable in practice since if even one edge in the chain fails, the rest of the chain fails : the cap precludes very long chains that are extremely unlikely to execute and instead causes the solution to have more parallel chains and cycles that are more likely to succeed We work with the UNOS nationwide kidney exchange, which uses a chain cap. Next we compare our solver to the leading constraint-generation-based solver and to the best prior correct branch-and-price-based solver, We focus on the setting where chains have a length cap, On that real data and demographically-accurate generated data.
2K_dev_1191	Concurrent C0 is an imperative programming language in the C family with session-typed message-passing concurrency. The previously proposed semantics implements asynchronous ( non-blocking ) output ; we extend it here with non-blocking input. And show the results obtained, While the abstract measure of span always decreases ( or remains unchanged ), only a few of the examples reap a practical benefit. A key idea is to postpone message reception as much as possible by interpreting receive commands as a request for a message, We implemented our ideas as a translation from a blocking intermediate language to a non-blocking language. Finally we evaluated our techniques with several benchmark programs.
2K_dev_1192	A type-safe C-like language. That outperforms traditional message passing techniques. We describe Concurrent C0 with contracts and session-typed communication over channels, Concurrent C0 supports an operation called forwarding which allows channels to be combined in a well-defined way The language 's type system enables elegant expression of session types and message-passing concurrent programs. We provide a Go-based implementation with language based optimizations.
2K_dev_1194	We explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational Bayes ( SGVB ) with global model parameters inference of more flexibly parameterized posteriors often leading to better generalization. Regular SGVB estimators rely on sampling of parameters once per minibatch of data, and have variance that is constant w, The efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch, Such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence, We find an important connection with regularization by dropout : the original Gaussian dropout objective corresponds to SGVB with local noise, a scale-invariant prior and proportionally fixed posterior variance, Our method allows ; specifically, we propose \emph { variational dropout }, a generalization of Gaussian dropout, but with a more flexibly parameterized posterior. The method is demonstrated through several experiments.
2K_dev_1197	Optical music recognition ( OMR ) is the task of recognizing images of musical scores. For the first steps of optical music recognition. In this paper improved algorithms were developed, which facilitated bulk annotation of scanned scores for use in an interactive score display system, Creating an initial annotation by OMR and verifying by hand substantially reduced the manual effort required to process scanned scores to be used in a live performance setting.
2K_dev_1198	Computer music systems can interact with humans at different levels, including scores phrases notes, However most current systems lack basic musicianship skills. As a consequence the results of human-computer interaction are often far less musical than the interaction between human musicians, In this paper we explore the possibility of learning some basic music performance skills from rehearsal data, In particular we consider the piano duet scenario where two musicians expressively interact with each other, Our work extends previous automatic accompaniment systems. And claim that a more human-like interaction is achieved. We have built an artificial pianist that can automatically improve its ability to sense and coordinate with a human pianist, learning from rehearsal experience, We describe different machine learning algorithms to learn musicianship for duet interaction. Explore the properties of the learned models, such as dominant features, limits of validity and minimal training size.
2K_dev_1204	Biological adaptation is a powerful mechanism that makes many disorders hard to combat. In this paper we study steering such adaptation through sequential planning to compute a treatment plan. We show that for the development of regulatory cells, sequential plans yield significantly higher utility than the best static therapy In contrast, for developing effector cells, we find that ( at least for the given simulator, objective function action possibilities, and measurement possibilities ) single-step plans suffice for optimal treatment. We propose a general approach where we leverage Monte Carlo tree search and the biological entity is modeled by a black-box simulator that the planner calls during planning We show that the framework can be used to steer a biological entity modeled via a complex signaling pathway network that has numerous feedback loops that operate at different rates and have hard-to-understand aggregate behavior We apply the framework to steering the adaptation of a patient 's immune system, In particular we apply it to a leading T cell simulator ( available in the biological modeling package BioNetGen. We run experiments with two alternate goals : developing regulatory T cells or developing effector T cells, The former is important for preventing autoimmune diseases while the latter is associated with better survival rates in cancer patients We are especially interested in the effect of sequential plans, an approach that has not been explored extensively in the biological literature.
2K_dev_1205	As publishers gather more information about their users, they can use that information to enable advertisers to create increasingly targeted campaigns, This enables better usage of advertising inventory. However it also dramatically increases the complexity that the publisher faces when optimizing campaign admission decisions and inventory allocation to campaigns, for abstracting fine-grained audience segments into coarser abstract segments that are not too numerous for use in such optimization. It yields two orders of magnitude improvement in run time and significant improvement in abstraction quality These benefits hold both for guaranteed and non-guaranteed campaigns. We develop an optimal anytime algorithm The performance stems from three improvements : 1 ) a quadratic-time ( as opposed to doubly exponential or heuristic ) algorithm for finding an optimal split of an abstract segment, 2 ) a better scoring function for evaluating splits, and 3 ) splitting time lossily like any other targeting attribute ( instead of losslessly segmenting time first ). Compared to the segment abstraction algorithm by Walsh et al, [ 2010 ] for the same problem.
2K_dev_1207	State-of-the-art applications of Stackelberg security games -- including wildlife protection -- offer a wealth of data, which can be used to learn the behavior of the adversary. But existing approaches either make strong assumptions about the structure of the data, or gather new data through online algorithms that are likely to play severely suboptimal strategies, to learning the parameters of the behavioral model of a bounded rational attacker ( thereby pinpointing a near optimal strategy ). We also validate our approach. We develop a new approach, by observing how the attacker responds to only three defender strategies. Using experiments on real and synthetic data.
2K_dev_1212	Hybrid systems verification is quite important for developing correct controllers for physical systems, but is also challenging, Verification engineers thus need to be empowered with ways of guiding hybrid systems verification while receiving as much help from automation as possible, We also share thoughts how the success of such a user interface design could be evaluated and anecdotal observations about it. Due to undecidability verification tools need sufficient means for intervening during the verification and need to allow verification engineers to provide system design insights, We discuss how they make it easier to prove hybrid systems as well as help learn how to conduct proofs in the first place. Unsurprisingly the most difficult user interface challenges come from the desire to integrate automation and human guidance. This paper presents the design ideas behind the user interface for the hybrid systems theorem prover KeYmaera X.
2K_dev_1213	In human-robot teams humans often start with an inaccurate model of the robot capabilities, As they interact with the robot, they infer the robot 's capabilities and partially adapt to the robot, they might change their actions based on the observed outcomes and the robot 's actions, without replicating the robot 's policy. Of human partial adaptation to the robot capturing the evolution of their expectations of the robot 's capabilities. We prove that the optimal policy can be computed efficiently, that the proposed model significantly improves human-robot team performance. We present a game-theoretic model where the human responds to the robot 's actions by maximizing a reward function that changes stochastically over time, The robot can then use this model to decide optimally between taking actions that reveal its capabilities to the human and taking the best action given the information that the human currently has. Under certain observability assumptions We demonstrate through a human subject experiment compared to policies that assume complete adaptation of the human to the robot.
2K_dev_1217	Generalized canonical correlation analysis ( GCCA ) aims at extracting common structure from multiple 'views ', high-dimensional matrices representing the same objects in different feature domains an extension of classical two-view CCA. Existing ( G ) CCA algorithms have serious scalability issues, since they involve square root factorization of the correlation matrices of the views, The memory and computational complexity associated with this step grow as a quadratic and cubic function of the problem dimension ( the number of samples / features ), To circumvent such difficulties. Further reduce the runtime significantly ( by 30 % ) if multiple cores are available, to showcase the effectiveness of the proposed algorithms. We propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements, respectively Consequently the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100, 000 while the current approaches can only handle thousands of features / samples, Our second contribution is a distributed algorithm for GCCA, which computes the canonical components of different views in parallel and thus can. In experiments Judiciously designed synthetic and real-data experiments using a multilingual dataset are employed.
2K_dev_1218	Display appropriation provides a means by which mobile users can cyber-forage local display hardware to provide them with access to a high-quality output device. However displays are of little use without applications to drive them and yet the nature of application support has been largely ignored in the field with the prevailing assumption being that applications will be cloud-based and Web-centric, to execute high-performance applications that would not be possible using purely Web-centric technologies. In this demonstration we show a system that presents an alternative vision in which users are able to cyber-forage for both display and compute resources in their local area enabling them The demonstration leverages a cohesive suite of existing systems, cloudlets Internet Suspend/Resume ( ISR ), Yarely and Tacita to deliver this vision.
2K_dev_1220	In social voting Web sites, how do the user actions up-votes, down-votes and comments evolve over time ? Are there relationships between votes and comments ? What is normal and what is suspicious ? These are the questions we focus on, that models the coevolution of user activities. Our first contribution is two discoveries : ( i ) the number of comments grows as a power-law on the number of votes and ( ii ) the time between a submission creation and a user 's reaction obeys a log-logistic distribution, VnC outperformed state-of-the-art baselines on accuracy Additionally, we illustrate VnC usefulness for forecasting and outlier detection. Based on these patterns, we propose VnC ( Vote-and-Comment ), a parsimonious but accurate and scalable model. We analyzed over 20, 000 submissions corresponding to more than 100 million user interactions from three social voting Web sites : Reddit, Imgur and Digg In our experiments on real data.
2K_dev_1221	Given a heterogeneous network, with nodes of different types - e, products users and sellers from an online recommendation site like Amazon - and labels for a few nodes ( 'honest ', 'suspicious ' etc ), can we find a closed formula for Belief Propagation ( BP ), exact or approximate ? Can we say whether it will converge ?. BP traditionally an inference algorithm for graphical models, exploits so-called `` network effects '' to perform graph classification tasks when labels for a subset of nodes are provided ; and it has been successful in numerous settings like fraudulent entity detection in online retailers and classification in social networks, However it does not have a closed-form nor does it provide convergence guarantees in general, to perform fast BP on undirected heterogeneous graphs. ( 4 ) Effectiveness ZooBP identifies fraudulent users with a near-perfect precision of 92, 3 % over the top 300 results. We propose ZooBP a method with provable convergence guarantees ZooBP has the following advantages : ( 1 ) Generality : It works on heterogeneous graphs with multiple types of nodes and edges ; ( 2 ) Closed-form solution : ZooBP gives a closed-form solution as well as convergence guarantees ; ( 3 ) Scalability : ZooBP is linear on the graph size and is up to 600 faster than BP, running on graphs with 3, 3 million edges in a few seconds. Applied on real data ( a Flipkart e-commerce network with users, products and sellers ).
2K_dev_1223	A k-core is the maximal subgraph where all vertices have degree at least k, This concept has been applied to such diverse areas as hierarchical structure analysis, graph visualization and graph clustering. How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we use them for algorithm design and applications ? Here, we explore pervasive patterns that are related to k-cores and emerging in graphs from several diverse domains. Our discoveries are as follows : ( 1 ) Mirror Pattern : coreness of vertices ( i, maximum k such that each vertex belongs to the k-core ) is strongly correlated to their degree, ( 2 ) Core-Triangle Pattern : degeneracy of a graph ( i, maximum k such that the k-core exists in the graph ) obeys a 3-to-1 power law with respect to the count of triangles, ( 3 ) Structured Core Pattern : degeneracy-cores are not cliques but have non-trivial structures such as core-periphery and communities. Our algorithmic contributions show the usefulness of these patterns, ( 1 ) Core-A, which measures the deviation from Mirror Pattern, successfully finds anomalies in real-world graphs complementing densest-subgraph based anomaly detection methods, ( 2 ) Core-D, a single-pass streaming algorithm based on Core-Triangle Pattern, accurately estimates the degeneracy of billion-scale graphs up to 7 faster than a recent multipass algorithm, ( 3 ) Core-S, inspired by Structured Core Pattern, identifies influential spreaders up to 17 faster than top competitors with comparable accuracy.
2K_dev_1230	Multi-aspect data appear frequently in many web-related applications, For example product reviews are quadruplets of ( user, product keyword timestamp ), How can we analyze such web-scale multi-aspect data ? Can we analyze them on an off-the-shelf workstation with limited amount of memory ? Tucker decomposition has been widely used for discovering patterns in relationships among entities in multi-aspect data, naturally expressed as high-order tensors. However existing algorithms for Tucker decomposition have limited scalability, and especially fail to decompose high-order tensors since they explicitly materialize intermediate data, whose size rapidly grows as the order increases ( 4 ), We call this problem M-Bottleneck ( `` Materialization Bottleneck '' ), To avoid M-Bottleneck to minimize the materialized intermediate data. S-HOT showed better scalability not only with the order but also with the dimensionality and the rank than baseline methods In particular, S-HOT decomposed tensors 1000 larger than baseline methods in terms dimensionality S- HOT also successfully analyzed real-world tensors that are both large-scale and high-order on an off-the-shelf workstation with limited amount of memory, while baseline methods failed The source code of S-HOT is publicly available at http : //dm, kr/shot to encourage reproducibility. We propose S-HOT a scalable high-order tucker decomposition method that employs the on-the-fly computation Moreover, S-HOT is designed for handling disk-resident tensors, too large to fit in memory, without loading them all in memory at once. We provide theoretical analysis on the amount of memory space and the number of scans of data required by S-HOT.
2K_dev_1231	How can we detect fraudulent lockstep behavior in large-scale multi-aspect data ( i, tensors ) ? Can we detect it when data are too large to fit in memory or even on a disk ? Past studies have shown that dense blocks in real-world tensors ( e, social media Wikipedia TCP dumps, ) signal anomalous or fraudulent behavior such as retweet boosting, bot activities and network attacks, Thus various approaches including tensor decomposition and search, have been used for rapid and accurate dense-block detection in tensors. However all such methods have low accuracy, or assume that tensors are small enough to fit in main memory, which is not true in many real-world applications such as social media and web, To overcome these limitations. D-Cube is ( 1 ) Memory Efficient : requires up to 1, 600 times less memory and handles 1, 000 times larger data ( 2, 6TB ) ( 2 ) Fast : up to 5 times faster due to its near-linear scalability with all aspects of data, ( 3 ) Provably Accurate : gives a guarantee on the densities of the blocks it finds, and ( 4 ) Effective : successfully spotted network attacks from TCP dumps and synchronized behavior in rating data with the highest accuracy. We propose D-Cube a disk-based dense-block detection method, which also can be run in a distributed manner across multiple machines. Compared with state-of-the-art methods.
2K_dev_1233	This generalizes a prior decomposition result for an M/M/k/staggeredsetup. We consider the M/G/k/staggered-setup, where idle servers are turned off to save cost, necessitating a setup time for turning a server back on ; however, at most one server may be in setup mode at any time. We show the response time of an M/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/k and the setup time, where the approximation is nearly exact. That for exponentially distributed setup times.
2K_dev_1236	Summary Successful application of two-photon imaging withgenetic tools in awake macaque monkeys will enable fundamental advances in our understanding of higher cognitive function at the level of molecular and neuronal circuits, By providing two-photon imaging access to cortical neuronal populations at single-cell or single dendritic spine resolution in awake monkeys, the techniques reported can help bridge the use of modern genetic and molecular tools and the study of higher cognitive function. For long-term two-photon imaging in awake macaque monkeys. Confirm that fluorescence activity is linearly proportional to neuronal spiking activity across a wide range of firing rates ( 10Hz to 150Hz ). Here we report techniques Using genetically encoded indicators including GCaMP5 and GCaMP6s delivered by AAV2/1 into the visual cortex, we demonstrate that high-quality two-photon imaging of large neuronal populations can be achieved and maintained in awake monkeys for months. Simultaneous intracellular recording and two-photon calcium imaging.
2K_dev_1237	Our work provides a solid step toward a systematic and quantitative wall-centric profiling of Facebook user activity. How do people interact with their Facebook wall ? At a high level, this question captures the essence of our work, While most prior efforts focus on Twitter, the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study. Our key results can be summarized in the following points, First we find that many wall activities, including number of posts, number of likes number of posts of type photo, can be described by the PowerWall distribution, What is more surprising is that most of these distributions have similar slope, with a value close to 1 ! Second, we show how our patterns and metrics can help us spot surprising behaviors and anomalies, For example we find a user posting every two days, exactly the same count of posts ; another user posting at midnight, with no other activity before or after. In this work we model Facebook user behavior : We propose PowerWall, a lesser known heavy-tailed distribution to fit our data. We analyze the wall activities of users focusing on identifying common patterns and surprising phenomena, We conduct an extensive study of roughly 7k users over 3 years during 4-month intervals each year.
2K_dev_1241	Sparse iterative methods in particular first-order methods, are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games, The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on. For solving extensive-form games for the strategy spaces of sequential games. We show that for the first time, the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm. We investigate the acceleration of first-order methods through better design of the dilated entropy function -- -a class of distance-generating functions related to the domains associated with the extensive-form games, By introducing a new weighting scheme for the dilated entropy function, we develop the first distance-generating function that only a logarithmic dependence on the branching factor of the player, This result improves the convergence rate of several first-order methods by a factor of ( b dd ), where b is the branching factor of the player, and d is the depth of the game tree, Thus far counterfactual regret minimization methods have been faster in practice, and more popular than first-order methods despite their theoretically inferior convergence rates. Using our new weighting scheme and practical tuning.
2K_dev_1247	Researchers and educators have designed curricula and resources for introductory programming environments such as Scratch, App Inventor and Kodu to foster computational thinking in K-12. This paper is an empirical study of the effectiveness and usefulness of tiles and flashcards developed for Microsoft Kodu Game Lab to support students in learning how to program and develop games. We found that the students who used physical manipulatives performed well in rule construction, whereas the students who engaged more with the rule editor of the programming environment had better mental simulation of the rules and understanding of the concepts. In particular we investigated the impact of physical manipulatives on 3rd -- 5th grade students ' ability to understand, recognize construct and use game programming design patterns.
2K_dev_1256	Voting systems typically treat all voters equally. We argue that perhaps they should not : Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones, To develop a formal framework for desirable weighting schemes. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule. We draw on no-regret learning, Specifically given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight.
2K_dev_1258	For a component-based modeling and verification approach for hybrid systems. In this paper we present reasoning techniques comprising discrete dynamics as well as continuous dynamics, in which the components have local responsibilities, Our approach supports component contracts i, input assumptions and output guarantees of interfaces that are more general than previous component-based hybrid systems verification techniques in the following ways : We introduce change contracts, which characterize how current values exchanged between components along ports relate to previous values, We also introduce delay contracts, which describe the change relative to the time that has passed since the last value was exchanged Together, these contracts can take into account what has changed between two components in a given amount of time since the last exchange of information, Most crucially we prove that the safety of compatible components implies safety of the composite. The proof steps of the theorem are also implemented as a tactic in KeYmaerai ? X, allowing automatic generation of a KeYmaerai ? X proof for the composite system from proofs of the concrete components.
2K_dev_1266	This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks. To capture the notion of object persistence and continuity in our visual experience. Interestingly the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet. We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network ( DCNN ) that learns to associate different views of each 3D object The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations, It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories, This untangling enables the model to discriminate and recognize objects within the same category, We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects.
2K_dev_1267	Given a collection of seasonal time-series, how can we find regular ( cyclic ) patterns and outliers ( i, rare events ) ? These two types of patterns are hidden and mixed in the time-varying activities. How can we robustly separate regular patterns and outliers, without requiring any prior information ? to capture both cyclic patterns and outliers, which solves the above problem. Demonstrate the benefits of the proposed model and algorithm, in that the model can capture latent cyclic patterns, trends and rare events, and the algorithm outperforms the existing state-of-the-art approaches, CycloneFact was up to 5 times more accurate and 20 times faster than top competitors. We present CycloneM a unifying model and CycloneFact, a novel algorithm We also present an automatic mining framework AutoCyclone, based on CycloneM and CycloneFact Our method has the following properties ; ( a ) effective : it captures important cyclic features such as trend and seasonality, and distinguishes regular patterns and rare events clearly ; ( b ) robust and accurate : it detects the above features and patterns accurately against outliers ; ( c ) fast : CycloneFact takes linear time in the data size and typically converges in a few iterations ; ( d ) parameter free : our modeling framework frees the user from having to provide parameter values. Extensive experiments on 4 real datasets.
2K_dev_1271	Reading tracing and explaining the behavior of code are strongly correlated with the ability to write code effectively, Kodu reasoning problems appear to be a promising tool for assessing computational thinking in young programmers. To investigate program understanding in young children. Explicitly teaching semantics proved helpful with one type of misconception but not with others We found different styles of student reasoning ( analytical and analogical ) that may correspond to distinct neo-Piagetian stages of development as described by Teague and Lister ( 2014 ). We introduced two groups of third graders to Microsoft 's Kodu Game Lab ; the second group was also given four semantic `` Laws of Kodu '' to better scaffold their reasoning and discourage some common misconceptions During each session, students were asked to predict the behavior of short Kodu programs.
2K_dev_1273	When tasked to find fraudulent social network users, what is a practitioner to do ?. Traditional classification can lead to poor generalization and high misclassification given few and possibly biased labels We tackle this problem to handle new and multimodal fraud types. We report the signs of such behaviors, including oddities in local network connectivity, account attributes and similarities and differences across fraud providers We discover several types of fraud behaviors, with the possibility of even more, which give exceptionally strong ( > 0, 95 precision/recall ) discriminative power on ground-truth data, and which reduces misclassification rate by > 18 % over baselines and routes practitioner attention to samples at high-risk of misclassification. And building algorithms First, we set up honeypots, or `` dummy '' social network accounts on which we solicit fake followers ( after careful IRB approval ), We discuss how to leverage these insights in practice, build strongly performing entropy-based features, and propose OEC ( Open-ended Classification ), an approach for `` future-proofing '' existing algorithms to account for the complexities of link fraud, Our contributions are ( b ) features : we engineer features ( c ) algorithm : we motivate and discuss OEC. By analyzing fraudulent behavioral patterns, featurizing users to yield strong discriminative performance, ( a ) observations : we analyze our honeypot fraudster ecosystem and give insights regarding various fraud behaviors.
2K_dev_1287	The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors. How quickly does a piece of news spread over these media ? How does its popularity diminish over time ? Does the rising and falling pattern follow a simple universal law ? the rise and fall patterns of information diffusion for the real-time monitoring of information diffusion. Demonstrate that S pike M accurately and succinctly describes all patterns of the rise and fall spikes in social networks. In this article we propose S pike M, a concise yet flexible analytical model of Our model has the following advantages First, unification power : it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models We provide the threshold of the take-off versus die-out conditions for S pike M and discuss the generality of our model by applying it to an arbitrary graph topology Second, practicality : it matches the observed behavior of diverse sets of real data Third, parsimony : it requires only a handful of parameters, Fourth usefulness : it makes it possible to perform analytic tasks such as forecasting, spotting anomalies and interpretation by reverse engineering the system parameters of interest ( quality of news, number of interested bloggers, ) We also introduce an efficient and effective algorithm namely S pike S tream, which identifies multiple diffusion patterns in a large collection of online event streams. Extensive experiments on real datasets.
2K_dev_1289	The mechanism classes we study are significantly different from well-understood function classes typically found in machine learning, so bounding their complexity requires a sharp understanding of the interplay between mechanism parameters and buyer valuations. We study the design of pricing mechanisms and auctions when the mechanism designer does not know the distribution of buyers ' values, to measure the intrinsic complexity of a variety of widely-studied single- and multi-item auction classes We demonstrate how to determine the precise level in a hierarchy with the optimal tradeoff between profit and generalization using structural profit maximization. We present a single, overarching theorem that uses empirical Rademacher complexity, including affine maximizer auctions, mixed-bundling auctions and second-price item auctions Despite the extensive applicability of our main theorem, we match and improve over the best-known generalization guarantees for many auction classes, This all-encompassing theorem also applies to multi- and single-item pricing mechanisms in both multi- and single-unit settings, such as linear and non-linear pricing mechanisms, Finally our central theorem allows us to easily derive generalization guarantees for every class in several finely grained hierarchies of auction and pricing mechanism classes. Instead the mechanism designer receives a set of samples from this distribution and his goal is to use the sample to design a pricing mechanism or auction with high expected profit, We provide generalization guarantees which bound the difference between average profit on the sample and expected profit over the distribution, These bounds are directly proportional to the intrinsic complexity of the mechanism class the designer is optimizing over.
2K_dev_1293	The same techniques can be applied to monitor other types of traffic data. Is it possible to monitor the entire traffic in Manhattan at a few intersections ? to handle complex. We are able to approximately recover the taxi-pick activities in Manhattan by sampling at only 5 selected intersections. This paper proposes a series of sampling, recovery and representation techniques based on graph signal processing. We validate our proposed techniques on Manhattan 's taxi pickups during the years of 2014 and 2015.
2K_dev_1309	Information cascades are ubiquitous in both physical society and online social media, taking on large variations in structures, dynamics and semantics potentially providing insights into intrinsic mechanisms governing information spreading in nature and new models to forecast as well as to impose good control over information cascades in real applications. Although there has been much progress on understanding the dynamics and semantics of information cascades, little is known about their structural patterns to quantify the structural characteristics of millions of information cascades. We find that the structural complexity of information cascades is far beyond the previous conjectures, finding some brand new structure patterns of information cascades. In this paper we explore a large-scale dataset including 432 million information cascades with explicit records of spreading traces We first propose seven-dimensional metrics, which reflect size and spreading orientation aspects. Further we analyze the correlations of these metrics.
2K_dev_1313	As online fraudsters invest more resources, including purchasing large pools of fake user accounts and dedicated IPs, fraudulent attacks become less obvious and their detection becomes increasingly challenging. Existing approaches such as average degree maximization suffer from the bias of including more nodes than necessary, resulting in lower accuracy and increased need for manual verification, to more accurately detect groups of fraudulent users. Showed that HoloScope achieved significant accuracy improvements on synthetic and real data, compared with state-of-the-art fraud detection methods. Hence we propose HoloScope, which uses information from graph topology and temporal spikes In terms of graph topology, we introduce `` contrast suspiciousness, '' a dynamic weighting approach, which allows us to more accurately detect fraudulent blocks, In terms of temporal spikes, HoloScope takes into account the sudden bursts and drops of fraudsters ' attacking patterns In addition, we provide theoretical bounds for how much this increases the time cost needed for fraudsters to conduct adversarial attacks Additionally, from the perspective of ratings, HoloScope incorporates the deviation of rating scores in order to catch fraudsters more accurately, Moreover HoloScope has a concise framework and sub-quadratic time complexity, making the algorithm reproducible and scalable.
2K_dev_1314	In comparison-shopping services ( CSS ), there exist frauds who perform excessive clicks on a target item in order to boost the popularity of it. In this paper we introduce the problem of detecting frauds in CSS and. Propose three anomaly scores designed based on click behaviors of users in CSS.
2K_dev_1315	As one of the featured initiatives in smart grids, demand response is enabling active participation of electricity consumers in the supply/demand balancing process, thereby enhancing the power systems operational flexibility in a costeffective way, Industrial load plays an important role in demand response because of its intense power consumption, already existing advanced monitoring and control infrastructure, and its strong economic incentive due to the high energy costs, As typical industrial loads, cement plants are able to quickly adjust their power consumption rate by switching on/off the crushers. However in the cement plant as well as other industrial loads, switching on/off the loading units only achieves discrete power changes, which restricts the load from offering valuable ancillary services such as regulation and load following, as continuous power changes are required for these services, In this paper we overcome this restriction of poor granularity. By proposing methods that enable these loads to provide regulation or load following with the support of an on-site energy storage system.
2K_dev_1316	Large graph datasets have caused renewed interest for graph partitioning. However existing well-studied graph partitioners often assume that vertices of the graph are always active during the computation, which may lead to time-varying skewness for traversal-style graph workloads, like Breadth First Search, since they only explore part of the graph in each superstep, Additionally existing solutions do not consider what vertices each partition will have, as a result high-degree vertices may be concentrated into a few partitions, causing imbalance the objective is to create an initial partitioning that will `` hold well '' over time without suffering from skewness. Towards this we introduce the idea of skew-resistant graph partitioning, where Skewresistant graph partitioning tries to mitigate skewness by taking the characteristics of both the target workload and the graph structure into consideration.
2K_dev_1323	Utility maximization under a budget constraint is a classical problem in economics and management science, It is commonly assumed that the utility is a `` nice '' known analytic function, for example continuous and concave, In many domains such as marketing, increased availability of computational resources and data has enabled the development of sophisticated simulations to evaluate the impact of allocating a fixed budget among alternatives ( e, marketing channels ) on outcomes, While simulations enable high resolution evaluation of alternative budget allocation strategies, they significantly complicate the associated budget optimization problem. In particular simulation runs are time consuming, significantly limiting the space of options that can be explored, An important second challenge is the common presence of budget complementarities, where non-negligible budget increments are required for an appreciable marginal impact from a channel, This introduces a combinatorial structure on the decision space We propose to address these challenges for achieving this approximation in an online fashion. Demonstrates the effectiveness of our approach. By first converting the problem into a multi-choice knapsack optimization problem with unknown weights We show that if weights ( corresponding to marginal impact thresholds for each channel ) are well approximated, we can achieve a solution within a factor of 2 of optimal, and this bound is tight, We then develop several parsimonious query algorithms.
2K_dev_1325	In total this work substantially expands the scope and scale of problems that can be solved using semidefinite programming methods. To low-rank structured semidefinite programming. We show that for certain problems, the method is strictly decreasing and guaranteed to converge to a critical point, In all settings we demonstrate improvement over the existing state of the art along various dimensions. In this paper we propose a coordinate descent approach The approach, which we call the Mixing method, is extremely simple to implement, has no free parameters, and typically attains an order of magnitude or better improvement in optimization performance over the current state of the art. We then apply the algorithm to three separate domains : solving the maximum cut semidefinite relaxation, solving a ( novel ) maximum satisfiability relaxation, and solving the GloVe word embedding optimization problem.
2K_dev_1327	In this paper we address the challenge of recovering a time sequence of counts from aggregated historical data, For example given a mixture of the monthly and weekly sums, how can we find the daily counts of people infected with flu ? In general, what is the best way to recover historical counts from aggregated, possibly overlapping historical reports, in the presence of missing values ? Equally importantly, how much should we trust this reconstruction ?. Demonstrates that H-FUSE reconstructs the original data 30 81 % better than the least squares method. We propose H-FUSE a novel method that solves above problems by allowing injection of domain knowl- edge in a principled way, and turning the task into a well- defined optimization problem H-FUSE has the following desirable properties : ( a ) Effectiveness, recovering histori- cal data from aggregated reports with high accuracy ; ( b ) Self-awareness, providing an assessment of when the re- covery is not reliable ; ( c ) Scalability, computationally lin- ear on the size of the input data. Experiments on the real data ( epidemiology counts from the Tycho project [ 13 ].
2K_dev_1329	Open-source face recognition system for denaturing video streams for large camera networks using RTFace. We present OpenFace our new that approaches state-of-the-art accuracy, Integrating OpenFace with inter-frame tracking, we build RTFace a mechanism that selectively blurs faces according to specified policies at full frame rates, This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions, Finally we present a scalable.
2K_dev_1330	As video cameras proliferate, the ability to scalably capture and search their data becomes important. Scalability is improved by performing video analytics on cloudlets at the edge of the Internet, and only shipping extracted index information and meta-data to the cloud, to human-in-the-loop content-based retrospective search. In this setting we describe interactive data exploration ( IDE ), which refers using predicates that may not have been part of any prior indexing We also describe a new technique called just-in-time indexing ( JITI ) that improves response times in IDE.
2K_dev_1335	Despite their growing prominence, optimization in generative adversarial networks ( GANs ) is still a poorly-understood topic which is able to guarantee local stability for both the WGAN and for the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse. We show that even though GAN optimization does not correspond to a convex-concave game, even for simple parameterizations, under proper conditions equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation, On the other hand, we show that the recently-proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates. In this paper we analyze the `` gradient descent '' form of GAN optimization ( i, the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters ).
2K_dev_1336	Consider a stream of retweet events - how can we spot fraudulent lock-step behavior in such multi-aspect data ( i, tensors ) evolving over time ? Can we detect it in real time, with an accuracy guarantee ? Past studies have shown that dense subtensors tend to indicate anomalous or even fraudulent behavior in many tensor data, including social media Wikipedia. Thus several algorithms have been proposed for detecting dense subtensors rapidly and accurately, However existing algorithms assume that tensors are static, while many real-world tensors, including those mentioned above. Updates by our algorithms are up to a million times faster than the fastest batch algorithms Effective : our DENSESALERT successfully spots anomalies especially those overlooked by existing algorithms. We propose DENSESTREAM an incremental algorithm that maintains and updates a dense subtensor in a tensor stream ( i, a sequence of changes in a tensor ), and DENSESALERT an incremental algorithm spotting the sudden appearances of dense subtensors, Our algorithms are : ( 1 ) Fast and `` any time '' :, ( 2 ) Provably accurate : our algorithms guarantee a lower bound on the density of the subtensor they maintain, and ( 3 ).
2K_dev_1337	Abstract : The multidisciplinary goal was to develop an integrated conceptualization of the mid-level encoding of 3D object structure from multiple surface cues, The unprecedented dips of performance reduction in the component psychometric functions was captured. Showed that depth continuity is a prerequisite for facilitation of Gabor target detection in the context of flanking Gabors, and that similarly surface continuity in purely disparity-defined slanted surfaces was strongly enhanced in distributed patch detections as a function of stimulus duration in this dual discrimination task, showing that the perceptual processing of disparity and integration of 3D surface information across depth cues has time courses of several seconds, attesting to complexity of the neural processing hardware showed how surface reconstruction could be accomplished across the typically sparse depth information available, and integrated among sparse. In a computational model based on a novel Leaky Drift Diffusion Theory that we developed for the underlying neural signals, which can serve as an analytic basis for the time course of all neural decision processes, Three complementary computational modeling projects from three collaborating laboratories. Psychophysical studies The time course of depth surface perception was studied in a coordinated trio of psychophysical, neurophysiological and functional imaging studies.
2K_dev_1345	Previous work has replaced structural assumptions on the noise with a worst-case approach that aims to choose an outcome that minimizes the maximum error with respect to any feasible true ranking, This approach underlies algorithms that have recently been deployed on the social choice website RoboVote. We revisit the classic problem of designing voting rules that aggregate objective opinions, in a setting where voters have noisy estimates of a true ranking of the alternatives. We derive ( mostly sharp ) analytical bounds on the expected error and establish the practical benefits of our approach. We take a less conservative viewpoint by minimizing the average error with respect to the set of feasible ground truth rankings.
2K_dev_1347	It is known that such allocations can be computed using O ( n ln ( 1/e ) ) operations in the standard Robertson-Webb Model, implies that allocations that are exactly equitable can not be computed. We are interested in the problem of dividing a cake -- a heterogeneous divisible good -- among n players, in a way that is e- equitable : every pair of players must have the same value for their own allocated pieces, up to a difference of at most e. We establish a lower bound of ( ln ( 1/e ) /lnln ( 1/e ) ) on the complexity of this problem, which is almost tight for a constant number of players.
2K_dev_1354	What can humans compute in their heads ? We are thinking of a variety of Crypto Protocols, games like Sudoku Crossword Puzzles, Speed Chess and so on. The intent of this paper is to apply the ideas and methods of theoretical computer science to better understand what humans can compute in their heads, For example can a person compute a function in their head so that an eavesdropper with a powerful computer -- - who sees the responses to random input -- - still can not infer responses to new inputs ? To address such questions. We propose a rigorous model of human computation and associated measures of complexity We apply the model and measures first and foremost to the problem of ( 1 ) humanly computable password generation, and then consider related problems of ( 2 ) humanly computable `` one-way functions '' and ( 3 ) humanly computable `` pseudorandom generators '' The theory of Human Computability developed here plays by different rules than standard computability, and this takes some getting used to For reasons to be made clear, the polynomial versus exponential time divide of modern computability theory is irrelevant to human computation In human computability, the step-counts for both humans and computers must be more concrete, Specifically we restrict the adversary to at most 10^24 ( Avogadro number of ) steps, An alternate view of this work is that it deals with the analysis of algorithms and counting steps for the case that inputs are small as opposed to the usual case of inputs large-in-the-limit.
2K_dev_1355	How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we exploit them for applications ? A k-core is the maximal subgraph in which all vertices have degree at least k, This concept has been applied to such diverse areas as hierarchical structure analysis, graph visualization and graph clustering. Here we explore pervasive patterns related to k-cores and emerging in graphs from diverse domains. Our discoveries are : ( 1 ) Mirror Pattern : coreness ( i, maximum k such that each vertex belongs to the k-core ) is strongly correlated with degree, ( 2 ) Core-Triangle Pattern : degeneracy ( i, maximum k such that the k-core exists ) obeys a 3-to-1 power-law with respect to the count of triangles, ( 3 ) Structured Core Pattern : degeneracycores are not cliques but have non-trivial structures such as coreperiphery and communities. Our algorithmic contributions show the usefulness of these patterns, ( 1 ) Core-A, which measures the deviation from Mirror Pattern, successfully spots anomalies in real-world graphs, ( 2 ) Core-D, a single-pass streaming algorithm based on Core-Triangle Pattern, accurately estimates degeneracy up to 12\ ( \times \ ) faster than its competitor ( 3 ) Core-S, inspired by Structured Core Pattern, identifies influential spreaders up to 17\ ( \times \ ) faster than its competitors with comparable accuracy.
2K_dev_1357	Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral, ) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph, Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look normal, Even worse some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. Show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud FRAUDAR successfully detected a subgraph of more than 4, 000 detected accounts of which a majority had tweets showing that they used follower-buying services. We propose FRAUDAR an algorithm that ( a ) is camouflage resistant, ( b ) provides upper bounds on the effectiveness of fraudsters, and ( c ) is effective in real-world data. Experimental results under various attacks Additionally, in real-world experiments with a Twitter follower -- followee graph of 1.
2K_dev_1362	Situational awareness involves the timely acquisition of knowledge about real-world events, distillation of those events into higher-level conceptual constructs, and their synthesis into a coherent context-sensitive view. For situational awareness in vehicular systems that span driverless and drivered vehicles. We explore how convergent trends in video sensing, crowd sourcing and edge computing can be harnessed to create a shared real-time information system.
2K_dev_1364	Social media has become a popular and important tool for human communication, However due to this popularity, spam and the distribution of malicious content by computer-controlled users, known as bots has become a widespread problem, At the same time, when users use social media, they generate valuable data that can be used to understand the patterns of human communication. In this article we focus on the following important question : Can we identify and use patterns of human communication to decide whether a human or a bot controls a user ? fit the distribution of IATs that detects if users are bots based only on the timing of their postings. Is characterized by following four patterns : ( i ) heavy-tails, ( ii ) periodic-spikes, ( iii ) correlation between consecutive values, and ( iv ) bimodallity, Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics, Additionally when detecting bots, Act-M provided a precision higher than 93 % and 77 % with a sensitivity of 70 % for the Twitter and Reddit datasets. As our second contribution, we propose a mathematical model named Act-M ( Activity Model ) We show that Act-M can accurately from social media users Finally, we use Act-M to develop a method. The first contribution of this article is showing that the distribution of inter-arrival times ( IATs ) between postings We validate Act-M using data from over 55 million postings from four social media services : Reddit, Twitter Stack-Overflow and Hacker-News.
2K_dev_1369	Abstract Server-side variability the idea that the same job can take longer to run on one server than another due to server-dependent factors isan increasingly important concern in many queueing systems, One strategy for overcoming server-side variability to achieve low response time is redundancy, under which jobs create copies of themselves and send these copies to multiple different servers, waiting for only one copy to complete service, Most of the existing theoretical work on redundancy has focused on developing bounds, approximations and exact analysis to study the response time gains offered by redundancy. However response time is not the only important metric in redundancy systems : in addition to providing low overall response time, the system should also be fair in the sense that no job class should have a worse mean response time in the system with redundancy than it did in the system before redundancy is allowed In this paper we use scheduling to address the simultaneous goals of ( 1 ) achieving low response time and ( 2 ) maintaining fairness across job classes, for per-class response time. Shows that FCFS can be unfair in that it can hurt non-redundant jobs, which is provably fair and also achieves excellent overall mean response time. We develop new exact analysis under First-Come First-Served ( FCFS ) scheduling for a general type of system structure ; We then introduce the Least Redundant First ( LRF ) scheduling policy, which we prove is optimal with respect to overall system response time, but which can be unfair in that it can hurt the jobs that become redundant, Finally we introduce the Primaries First ( PF ) scheduling policy.
2K_dev_1374	How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time ? How do people join the social groups, uniformly or with burst ? What is the pattern of people quitting from groups ? Is there a simple universal model to depict the come-and-go patterns of various groups ? for group evolution. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR, For example we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore we propose a new model come N go, a concise yet flexible dynamic model Our model has the following advantages : ( a ) Unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation, ( b ) Succinctness and interpretability : it contains only six parameters with clear physical meanings ( c ) Accuracy : it can capture various kinds of group evolution patterns preciously, and the goodness of fit increases by 58 % over baseline, ( d ) Usefulness : it can be used in multiple application scenarios, such as forecasting and pattern discovery Furthermore, our model can provide insights about different evolution patterns of social groups, and we also find that group structure and its evolution has notable relations with temporal patterns of group evolution. In this article we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users.
2K_dev_1375	Computer vision based technologies have seen widespread adoption over the recent years, This use is not limited to the rapid adoption of facial recognition technology but extends to facial expression recognition, scene recognition and more. These developments raise privacy concerns and call for novel solutions to ensure adequate user awareness, and ideally control over the resulting collection and use of potentially sensitive data, While cameras have become ubiquitous, most of the time users are not even aware of their presence, enhance user 's awareness of and control over the collection and use of video data about them. In this paper we introduce a novel distributed privacy infrastructure for the Internet-of-Things and discuss in particular how it can help The infrastructure, supports the automated discovery of IoT resources and the selective notification of users This includes the presence of computer vision applications that collect data about users, In particular we describe an implementation of functionality that helps users discover nearby cameras and choose whether or not they want their faces to be denatured in the video streams. Which has undergone early deployment and evaluation on two campuses.
2K_dev_1376	Information cascades are ubiquitous in both physical society and online social media, taking on large variations in structures, Our discoveries also provide a foundation for the microscopic mechanisms for information spreading, potentially leading to implications for cascade prediction and outlier detection. Although the dynamics and semantics of information cascades have been studied, the structural patterns and their correlations with dynamics and semantics are largely unknown to quantify the structural characteristics of information cascades. We find that the structural complexity of information cascades is far beyond the previous conjectures We find that bimodal law governs majority of the metrics, information flows in cascades have four directions, and the self-loop number and average activity of cascades follows power law and finally uncover some notable implications of structural patterns in information cascades. We first propose a ten-dimensional metric, reflecting cascade size silhouette, direction and activity aspects. Here we explore a large-scale dataset including $ 432 $ million information cascades with explicit records of spreading traces, spreading behaviors information content as well as user profiles We then analyze the high-order structural patterns of information cascades Finally, we evaluate to what extent the structural features of information cascades can explain its dynamic patterns and semantics.
2K_dev_1379	How do people make friends dynamically in social networks ? What are the temporal patterns for an individual increasing its social connectivity ? What are the basic mechanisms governing the formation of these temporal patterns ? No matter cyber or physical social systems, their structure and dynamics are mainly driven by the connectivity dynamics of each individual, Our model and discoveries provide a foundation for the microscopic mechanisms of network growth dynamics, potentially leading to implications for prediction, clustering and outlier detection on human dynamics. However due to the lack of empirical data, little is known about the empirical dynamic patterns of social connectivity at microscopic level, let alone the regularities or models governing these microscopic dynamics. We uncover a wide range of long-term power law growth and short-term bursty growth for the social connectivity of different users, We propose three key ingredients, namely average-effect multiscale-effect and correlation-effect, which govern the observed growth patterns at microscopic level, we discover statistical regularities underlying the empirical growth dynamics. As a result we propose the long short memory process incorporating these ingredients, demonstrating that it successfully reproduces the complex growth patterns observed in the empirical data. We examine the detailed growth process of `` WeChat '', the largest online social network in China, with 300 million users and 4, 75 billion links spanning two years, By analyzing modeling parameters.
2K_dev_1381	This generalizes a prior decomposition result for an M/M/k/staggeredM/M/k/staggered-setup. We consider the M/G/k/staggeredM/G/k/staggered-setup, where idle servers are turned off to save cost, necessitating a setup time for turning a server back on ; however, at most one server may be in setup mode at any time. We show that the response time of an M/G/k/staggeredM/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/kM/G/k and the setup time, where the approximation is nearly exact. For exponentially distributed setup times.
2K_dev_1387	The Next-Generation Airborne Collision Avoidance System ACASi ? X is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft, It is currently being developed by the Federal Aviation Administration FAA, Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety. Under which the advice given by ACAS X is safe. In this paper we determine the geometric configurations under a precise set of assumptions and formally verify these configurations using hybrid systems theorem proving techniques, We conduct an initial examination of the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version, demonstrating how formal hybrid approaches are helping ensure the safety of ACAS X.
2K_dev_1389	Facility location and committee selection are classic embodiments of this problem. We consider the mechanism design problem for agents with single-peaked preferences over multi-dimensional domains when multiple alternatives can be chosen derive worst-case approximation ratios for social cost and maximum load for optimizing the choice of percentiles relative to any prior distribution over preferences. Demonstrate the viability of this approach and the value of such optimized mechanisms vis-a-vis mechanisms derived through worst-case analysis. We propose a class of percentile mechanisms, a form of generalized median mechanisms, that are strategy-proof and for L1 and L2 cost models More importantly, we propose a sample-based framework. Our empirical investigations using social cost and maximum load as objectives.
2K_dev_1391	Social choice theory provides insights into a variety of collective decision making settings. But nowadays some of its tenets are challenged by internet environments, which call for dynamic decision making under constantly changing preferences. In this paper we model the problem via Markov decision processes ( MDP ), where the states of the MDP coincide with preference profiles and a ( deterministic, stationary ) policy corresponds to a social choice function We can therefore employ the axioms studied in the social choice literature as guidelines in the design of socially desirable policies, We present tractable algorithms that compute optimal policies under different prominent social choice constraints, Our machinery relies on techniques for exploiting symmetries and isomorphisms between MDPs.
2K_dev_1392	While the analysis of unlabeled networks has been studied extensively in the past, finding patterns in different kinds of labeled graphs is still an open challenge. Given a large edge-labeled network, a time-evolving network how can we find interesting patterns ? which can discover communities appearing over subsets of the labels. We show that Com $ $ ^2 $ $ 2 spots intuitive patterns regarding edge labels that carry temporal or other discrete information, Our findings include large `` star '' -like patterns, near-bipartite cores as well as tiny groups ( five users ), calling each other hundreds of times within a few days, We also show that we are able to automatically identify competing airline companies. We propose Com $ $ ^2 $ $ 2, a novel fast and incremental tensor analysis approach The method is ( a ) scalable, being linear on the input size, ( b ) general, ( c ) needs no user-defined parameters and ( d ) effective, returning results that agree with intuition. We apply our method to real datasets, including a phone call network, a computer-traffic network and a flight information network, The phone call network consists of 4 million mobile users, with 51 million edges ( phone calls ), over 14 days while the flights dataset consists of 7733 airports and 5995 airline companies flying 67.
2K_dev_1394	In this paper we investigate information validation tasks that are initiated as queries from either automated agents or humans new online information validation technique, to automatically evaluate the truth of queries. We show that OpenEval is able to respond to the queries within a limited amount of time while also achieving high F1 score In addition, we show that the accuracy of responses provided by OpenEval is increased as more time is given for evaluation, that illustrate the effectiveness of our approach compared to related techniques. We introduce OpenEval a which uses information on the web that are stated as multiargument predicate instances ( e, DrugHasSideEffect ( Aspirin GI Bleeding ) ) ), OpenEval gets a small number of instances of a predicate as seed positive examples and automatically learns how to evaluate the truth of a new predicate instance by querying the web and processing the retrieved unstructured web pages. We have extensively tested our model and shown empirical results.
2K_dev_1395	How can we correlate the neural activity in the human brain as it responds to typed words, with properties of these terms ( like edible, fits in hand ) ? In short, we want to find latent variables, that jointly explain both the brain activity, as well as the behavioral responses, This is one of many settings of the Coupled Matrix-Tensor Factorization ( CMTF ) problem. Can we accelerate any CMTF solver, so that it runs within a few minutes instead of tens of hours to a day, while maintaining good accuracy ? capable of doing exactly that. By up to 200, along with an up to 65 fold increase in sparsity, with comparable accuracy to the baseline, TURBO-SMT is able to find meaningful latent variables, as well as to predict brain activity with competitive accuracy. We introduce TURBO-SMT a meta-method : it boosts the performance of any CMTF algorithm. We apply TURBO-SMT to BRAINQ, a dataset consisting of a ( nouns, brain voxels human subjects ) tensor and a ( nouns, properties ) matrix with coupling along the nouns dimension.
2K_dev_1396	The typical approach thus far is to use tensors or dynamical systems. Given electroencephalogram time series data from patients with epilepsy, can we find patterns and regularities ?. EEG-MINE ( a ) can successfully reconstruct the signals with high accuracy ; ( b ) can spot surprising patterns within seizure EEG signals ; and ( c ) may provide early warning of epileptic seizures. Here we present EEG-MINE, a nonlinear chaos-based `` gray box model '', that blends domain knowledge with data observations. When applied to numerous.
2K_dev_1398	Live music performance with computers has motivated many research projects in science, engineering and the arts, We conclude with directions for future work. In spite of decades of work, it is surprising that there is not more technology for, and a better understanding of the computer as music performer, Our goal is to enable musicians to ncorporate computers into performances easily and effectively through a better understanding of requirements, new techniques and practical. Outline our efforts to establish a new direction, Human-Computer Music Performance ( HCMP ), as a framework for a variety of coordinated studies, Our work in this area spans performance analysis, synchronization techniques and interactive performance systems. We review the development of techniques for live music performance and.
2K_dev_1400	Complex systems are designed using the model-based design paradigm in which mathematical models of systems are created and checked against specifications, Cyber-physical systems ( CPS ) are complex systems in which the physical environment is sensed and controlled by computational or cyber elements possibly distributed over communication networks, Various aspects of CPS design such as physical dynamics, software control and communication networking must interoperate correctly for correct functioning of the systems, Modeling formalisms analysis techniques and tools for designing these different aspects have evolved ind ependently, and remain dissimilar and disparate, There is no unifying formalism in which one can model all these aspects equally well, In current practice there is no principled approach that deals with this modeling heterogeneity within a formal framework. Therefore model-based design of CPS must make use of a collection of models in several different formalisms and use respective analysis methods and tools together to ensure correct system design, To enable doing this in a formal manner for multi-model verification of cyber-physical systems. Composition of analysis results. This thesis develops a framework based on behavioral semantics Heterogeneity arising from the different interacting aspects of CPS design must be addressed in order to enable system-level verification We develop behavioral semantics to address heterogeneity in a general yet formal manner, Our framework makes no assumptions about the specifics of any particular formalism, therefore it readily supports various formalisms, Models can be analyzed independently in isolation, supporting separation of concerns, Mappings across heterogeneous semantic domains enable associations between analysis results, Interdependencies across different models and specifications can be formally represented as constraints over parameters and verification can be carried out in a semantically consistent manner. Is supported both hierarchically across different levels of abstraction and structurally into interacting component models at a given level of abstraction, The theoretical concepts developed in the thesis are illustrated using a case study on the hierarchical heterogeneous verification of an automotive intersection collision avoidance system.
2K_dev_1405	For single-channel source separation to estimate the correlations between these features and the unobserved signal decomposition, to provide itemized energy usage. We demonstrate that contextual supervision improves significantly over a reasonable baseline and existing unsupervised methods for source separation and show that recovery of the signal components depends only on cross-correlation between features for different signals, not on correlations between features for the same signal. We propose a new framework that lies between the fully supervised and unsupervised setting, Instead of supervision we provide input features for each source signal and use convex methods Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision ; our motivating application is energy disaggregation of hourly smart meter data ( the separation of whole-home power signals into different energy uses ), Here contextual supervision allows us for thousands homes, a task previously impossible due to the need for specialized data collection hardware. On smaller datasets which include labels, Finally we analyze the case of l2 loss theoretically.
2K_dev_1406	Computing equilibria of games is a central task in computer science, A large number of results are known for Nash equilibrium ( NE ). However these can be adopted only when coalitions are not an issue, When instead agents can form coalitions, NE is inadequate and an appropriate solution concept is strong Nash equilibrium ( SNE ), Few computational results are known about SNE, In this paper we first study the problem of verifying whether a strategy profile is an SNE. Showing that the problem is in P, We then design a spatial branch -- and -- bound algorithm to find an SNE. And we experimentally evaluate the algorithm.
2K_dev_1408	Given the re-broadcasts ( i, retweets ) of posts in Twitter, how can we spot fake from genuine user reactions ? What will be the tell-tale sign the connectivity of retweeters, their relative timing or something else ? High retweet activity indicates influential users, and can be monetized, Hence there are strong incentives for fraudulent users to artificially boost their retweets ' volume. Here we explore the identifi- cation of fraudulent and genuine retweet threads. Our main contribu- tions are : ( a ) the discovery of patterns that fraudulent activity seems to follow ( the `` triangles `` a nd `` homogeneity '' patterns, the formation of micro-clusters in appropriate feature spaces ) ; and. ( b ) `` RTGen '', a realistic generator that mimics the behaviors of both honest and fraud- ulent users. We present experiments on a dataset of more than 6 million retweets crawled from Twitter.
2K_dev_1413	Given a multimillion-node social network, how can we sum- marize connectivity pattern from the data, and how can we find unex- pected user behavior ?. In this paper we study a complete graph from a large who-follows-whom network and spot lockstep behavior that large groups of followers connect to the same groups of followees, to detect users who offer the lockstep behavior. We discover that ( a ) the lockstep behavior on the graph shapes dense `` block '' in its adjacency matrix and creates `` ray '' in spectral subspaces, and ( b ) partially overlapping of the behavior shapes `` staircase '' in the matrix and creates `` pearl '' in the subspaces, We demonstrate that our approach is effective. The second contribution is that we provide a fast algorithm, using the discovery as a guide for practi- tioners. Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep, on both synthetic and real data.
2K_dev_1419	To predict accurately trust relationships of a target user even if he/she does not have much interaction information. We propose a novel method The proposed method considers positive, implicit and negative information of all users in a network based on belief propagation to predict trust relationships of a target user.
2K_dev_1421	Given the retweeting activity for the posts of several Twitter users, how can we distinguish organic activity from spammy retweets by paid followers to boost a post 's appearance of popularity ? More gen- erally, given groups of observations, can we spot strange groups ?. Our main intuition is that organic behavior has more variability, while fraud- ulent behavior, like retweets by botnet members, We refer to the detection of such synchronized observations as the Syn- chonization Fraud problem, and we study a specific instance of it, Retweet Fraud Detection manifested in Twitter, for detecting group fraud for characterizing retweet threads. Our method achieves a 97 % accuracy on a real dataset of 12 million retweets crawled from Twitter. Here we propose : ( A ) ND-Sync, an efficient method and ( B ) a set of carefully designed features ND-Sync is effec- tive in spotting retweet fraudsters, robust to different types of abnormal activity, and adaptable as it can easily incorporate additional features.
2K_dev_1423	Refactoring of code is a common device in software engineering, As cyber-physical systems CPS become ever more complex, similar engineering practices become more common in CPS development, Proper safe developments of CPS designs are accompanied by a proof of correctness. Since the inherent complexities of CPS practically mandate iterative development, frequent changes of models are standard practice, but require reverification of the resulting models after every change, To overcome this issue. For some of these we can give strong results that they are correct. We develop proof-aware refactorings for CPS, That is we study model transformations on CPS and show how they correspond to relations on correctness proofs, As the main technical device, we show how the impact of model transformations on correctness can be characterized by different notions of refinement in differential dynamic logic Furthermore, we demonstrate the application of refinements on a series of safety-preserving and liveness-preserving refactorings. By proving on a meta-level Where this is impossible, we construct proof obligations for showing that the refactoring respects the refinement relation.
2K_dev_1424	Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender 's randomized assignment of resources to targets, implies that in some realistic situations, limited surveillance may not need to be explicitly addressed. This assumption has been challenged by recent papers, which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance. That in zero-sum security games, lazy defenders who simply keep optimizing against perfectly informed attackers, are almost optimal against diligent attackers, who go to the effort of gathering a reasonable number of observations.
2K_dev_1429	Designers of human computation systms often face the need to aggregate noisy information provided by multiple people, Our short-term goal is to motivate the design of better human computation systems ; our long-term goal is to spark an interaction between researchers in ( computational ) social choice and human computation. While voting is often used for this purpose, the choice of voting method is typically not principled to better understand how different voting rules perform in practice. Our empirical conclusions show that noisy human voting can differ from what popular theoretical models would predict. We conduct extensive experiments on Amazon Mechanical Turk.
2K_dev_1607	Establishing quantitative bounds on the execution cost of programs is essential in many areas of computer science such as complexity analysis, compiler optimizations security and privacy, Techniques based on program analysis, type systems and abstract interpretation are well-studied. But methods for analyzing how the execution costs of two programs compare to each other have not received attention, Naively combining the worst and best case execution costs of the two programs does not work well in many cases because such analysis forgets the similarities between the programs or the inputs, that is capable of establishing precise bounds on the difference in the execution cost of two programs for a higher-order functional language with recursion and subtyping. We prove our type system sound We demonstrate the precision and generality of our technique. In this work we propose a relational cost analysis technique by making use of relational properties of programs and inputs, We develop Rel Cost, a refinement type and effect system The key novelty of our technique is the combination of relational refinements with two modes of typing-relational typing for reasoning about similar computations/inputs and unary typing for reasoning about unrelated computations/inputs, This combination allows us to analyze the execution cost difference of two programs more precisely than a naive non-relational approach. Using a semantic model based on step-indexed unary and binary logical relations accounting for non-relational and relational reasoning principles with their respective costs.
2K_dev_1614	This is the first amortized analysis, that automatically derives polynomial bounds for higher-order functions and polynomial bounds that depend on user-defined inductive types. This article presents a resource analysis system for OCaml programs. The practicality of the analysis system is the system infers bounds on the number of queries that are sent by OCaml programs to DynamoDB, a commercial NoSQL cloud database service. The system automatically derives worst-case resource bounds for higher-order polymorphic programs with user-defined inductive types, The technique is parametric in the resource and can derive bounds for time, memory allocations and energy usage The derived bounds are multivariate resource polynomials which are functions of different size parameters that depend on the standard OCaml types, Bound inference is fully automatic and reduced to a linear optimization problem that is passed to an off-the-shelf LP solver Technically, the analysis system is based on a novel multivariate automatic amortized resource analysis ( AARA ) It builds on existing work on linear AARA for higher-order programs with user-defined inductive types and on multivariate AARA for first-order programs with built-in lists and binary trees Moreover, the analysis handles a limited form of side effects and even outperforms the linear bound inference of previous systems At the same time, it preserves the expressivity and efficiency of existing AARA techniques.
2K_dev_1627	Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed, Some of these issues might be addressed by future tools which autocomplete more complete handlers. A major complaint is that programmers often write minimal and low quality handlers, to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions. We found that programmers handle exceptions locally in catch blocks much of the time, rather than propagating by throwing an Exception, Programmers make heavy use of actions like Log, Print Return or Throw in catch blocks, and also frequently copy code between handlers We found bad practices like empty catch blocks or catching Exception are indeed widespread, We discuss evidence that programmers may misjudge risk when catching Exception, and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception. We used the Boa tool.
2K_dev_1636	Determine whether a ( T ) X and b ( T ) Y are uncorrelated for every a is an element of R-p, b is an element of R-q or not, Linear independence testing is a fundamental information-theoretic and statistical problem that can be posed as follows : given n points \ { ( X ( i ) ; Y-i ) \ } ( n ) ( i=1 ) from a p + q dimensional multivariate distribution where X-i is an element of R-p and Y-i is an element of R-q. We give minimax lower bound for this problem.
2K_dev_1644	Such incentives are likely aligned with benefits to utilities and grid operators, which might take the form of peak-shaving or ancillary services However, private cost savings are not strictly aligned with public benefits related to the avoidance of health and environmental damages from power plant emissions. This paper assesses the potential cost-saving incentives for content distribution networks to shift traffic load among geographically distributed data centers in response to hourly variation in electricity prices. We find that feasible strategies exist to simultaneously realize public and private benefits and that load shifting can result in substantial cost savings and avoided damages in some circumstances, Concerns over increased latency and bandwidth costs can be mitigated with modifications to the model, However the level of realized savings is dependent upon the specifics of a particular network operator and electricity rate schedule. So we compare private cost minimization with a strategy that minimizes these externalities.
2K_dev_1645	Personal informatics systems are becoming increasing prevalent as their price, form and ease of use improves, Though these systems offer great potential value to users, many systems are hampered by issues that limit their ability to foster engagement, and people often abandon use of these systems without garnering meaningful outcomes, While continued use of these systems is not necessary for all people, there is an opportunity to better support people working towards achievement-based goals and discuss how these strategies could be used to foster engagement with PI systems. In this paper we draw from the literature and our own prior Work to identify a number of problems that hinder engagement with achievement-based personal informatics systems-problems related to inadequate support for goal setting, misalignment of user and system goals, and the burden of system maintenance, for mitigating these problems. We then propose seven strategies for the design community to explore.
2K_dev_1646	Machine learning improves mobile user experience, Interestingly envisioning apps with adaptive interfaces that reduce navigation and selection effort is not standard UX practice, When implementing an adaptive UI for our mobile transit app, we encountered a number of problems. Our original design did not log necessary information nor did it induce users to provide good labels, On reflection we realized UX designers should identify and refine UI adaptions when sketching wireframes, To advance on this insight to communicate planned adaptation and note the information ( logs and labels ) needed to make the desired inferences. Extracted six design patterns where UI adaptation can improve in-app navigation. Next we designed an exemplar set of wireframes, illustrating how UX designers might annotate their interaction flows. We reviewed the interfaces of popular apps and.
2K_dev_1649	Compressed sensing is a simple and efficient technique that has a number of applications in signal processing and machine learning, In machine learning it provides answers to questions such as : `` under what conditions is the sparse representation of data efficient ? { '' } ; `` when is learning a large margin classifier directly on the compressed domain possible ? { '' } ; and `` why does a large margin classifier learn more effectively if the data is sparse ? { '' }. This work tackles the problem of feature representation from the context of sparsity and affine rank minimization in order to provide answers to the aforementioned questions. And show for the high dimensional sparse signals, when the bounds are tight, directly learning in the compressed domain is possible. By leveraging compressed sensing from the learning perspective We show, for a full-rank signal, the high dimensional sparse representation of data is efficient because from the classifiers viewpoint such a representation is in fact a low dimensional problem. We provide practical bounds on the linear classifier to investigate the relationship between the SVM classifier in the high dimensional and compressed domains.
2K_dev_1650	An important research problem in learning analytics is. To expedite the cycle of data leading to the analysis of student progress and the improvement of student support, For this goal in the context of social learning. Which suggests ways in which we might foster these social benefits through intervention. We propose a pipeline that includes data infrastructure, learning analytics and intervention, along with computational models for individual components. Next we describe an example of applying this pipeline to real data in a case study, whose goal is to investigate the positive effects that goal-setting students have on their peers.
2K_dev_1656	Ambiguity arises in requirements when a statement is unintentionally or otherwise incomplete, missing information or when a word or phrase has more than one possible meaning. For web-based and mobile information systems, ambiguity and vagueness in particular, undermines the ability of organizations to align their privacy policies with their data practices, which can confuse or mislead users thus leading to an increase in privacy risk, The theory predicts how vague modifiers to information actions and information types can be composed to increase or decrease overall vagueness. To yield a rank order of vague terms in both isolation and composition, to show how increases in vagueness will decrease users ' acceptance of privacy risk and thus decrease users ' willingness to share personal information. The taxonomy was evaluated in a paired comparison experiment and results were analyzed using the Bradley-Terry model We further provide empirical evidence based on factorial vignette surveys.
2K_dev_1669	Increasing proliferation of mobile and online social networking platforms have given us unprecedented opportunity to observe and study social interactions at a fine temporal scale, A collection of all such social interactions among a group of individuals ( or agents ) observed over an interval of time is referred to as a temporally-detailed ( TD ) social network, A TD social network opens up the opportunity to explore TD questions on the underlying social system, `` How is the betweenness centrality of an individual changing with time ? { '' } To this end, related work has proposed temporal extensions of centrality metrics ( e, betweenness and closeness ). However scalable computation of these metrics for long time-intervals is challenging, This is due to the non-stationary ranking of shortest paths ( the underlying structure of betweenness and closeness ) between a pair of nodes which violates the assumptions of classical dynamic programming based techniques, for addressing the non-stationarity challenge of TD social networks. We prove the correctness and completeness of our algorithm, shows that the proposed algorithm out performs the alternatives by a wide margin. To this end we propose a novel computational paradigm called epoch-point based techniques Using the concept of epoch-points, we develop a novel algorithm for computing shortest path based centrality metric such as betweenness on a TD social network.
2K_dev_1680	Nonverbal behaviors play an important role in communication for both humans and social robots, However adding contextually appropriate animations by hand is time consuming and does not scale well, Previous researchers have developed automated systems for inserting animations based on utterance text, yet these systems lack human understanding of social context and are still being improved. This work proposes a middle ground where untrained human workers label semantic information, which is input to an automatic system to produce appropriate gestures. Results showed untrained workers are capable of providing reasonable labeling of semantic information and that emotional expressions derived from the labels were rated more highly than control videos, More study is needed to determine the effects of emphasis labels. To test this approach, untrained workers from Mechanical Turk labeled semantic information, specifically emotion and emphasis, for each utterance which was used to automatically add animations, Videos of a robot performing the animated dialogue were rated by a second set of participants.
2K_dev_1681	Morphable face models are a powerful tool. But have previously failed to model the eye accurately due to complexities in its material and motion captures eye region shape to allow independent eyeball movement. We present a new multi-part model of the eye that includes a morphable model of the facial eye region, as well as an anatomy-based eyeball model, It is the first morphable model that accurately, since it was built from high-quality head scans, It is also the first, since we treat it as a separate part, To showcase our model we present a new method for illumination-and head-pose invariant gaze estimation from a single RGB image, We fit our model to an image through analysis-by-synthesis, solving for eye region shape, texture eyeball pose and illumination simultaneously, The fitted eyeball pose parameters are then used to estimate gaze direction. Through evaluation on two standard datasets.
2K_dev_1685	Discrete energy minimization is widely-used in computer vision and machine learning for problems such as MAP inference in graphical models The problem, in general is notoriously intractable, and finding the global optimal solution is known to be NP-hard This paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms. However is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time ? We show in this paper that the answer is no. Specifically we show that general energy minimization, even in the 2-label pairwise case, and planar energy minimization with three or more labels are exp-APX-complete, This finding rules out the existence of any approximation algorithm with a sub-exponential approximation ratio in the input size for these two problems, including constant factor approximations. Moreover we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes - PO, APX and exp-APX corresponding to problems that are solvable, approximable and inapproximable in polynomial time, Problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones.
2K_dev_1687	New IT functions have greatly increased the amount of in-car information delivered to drivers, Although valuable that information can distract drivers when delivered during vehicle operation, By inferring driver state from sensor data, prior research has shown that it can accurately identify opportune moments to deliver information, With these results researchers can then build information delivery systems that can deliver information to drivers both when they are interruptible and when they find the information valuable. Now that we know when to best deliver information, it raises the question : what information should we deliver at those interruptible moments ?. We identified driving situations when each of the in-car information items is highly valuable, and verified these situations Results from our study offer important insights for understanding the diversity of drivers ' experiences about the value of in-car information and the ability to determine situations in which this information is valuable to drivers. To answer this question, we conducted a series of surveys and interviews and compiled a list of representative in-car information items and context factors that affect the importance of these items By combining and exploring those context factors through a large online survey of drivers, Lastly we examined what technology is available for detecting these driving situations, and which situations require further advanced technologies for detection.
2K_dev_1695	Recent research has improved our understanding of how to create strong, and suggest ways to ease password entry for mobile users. However this research has generally been in the context of desktops and laptops, while users are increasingly creating and entering passwords on mobile devices, In this paper we study whether recent password guidance carries over to the mobile setting. We compare the strength and usability of passwords created and used on mobile devices with those created and used on desktops and laptops, while varying password policy requirements and input methods.
2K_dev_1714	Playtesting or using play to guide game design, gives designers feedback about whether their game is meeting their goals and the player 's expectations We conclude with lessons learned and next steps in our research on playtesting. We report a of designing, deploying and iterating on a series of playtesting workshops for novice game designers. Novice game designers leveraged playtest methods and tools, employed playtesting and data collection methods appropriate for their goals, and effectively applied playtest data in iterative design. Case study We identify common missteps made by novice designers and address these missteps through the concept of purposefulness, understanding why you are playtesting as well as how to playtest, We ground our workshops in the development of rich player experience goals, which inform playtest design, data collection and iteration, We show that by applying methods taught in our workshops.
2K_dev_1716	For refinement relations on hybrid systems for verifying such relations This paper gives a syntax, semantics and proof calculus for dRL. We demonstrate its usefulness results in easier and better-structured proofs. We introduce differential refinement logic ( dRL ), a logic with first-class support and a proof calculus dRL simultaneously solves several seemingly different challenges common in theorem proving for hybrid systems : By using a refinement relation to arrange proofs hierarchically according to the structure of natural subsystems, we can increase the readability and modularity of the resulting proof, dRL extends an existing specification and verification language for hybrid systems ( differential dynamic logic, dL ) by adding a refinement relation to directly compare hybrid systems. With examples where using refinement.
2K_dev_1719	Patients researching medical diagnoses, scientist exploring new fields of literature, and students learning about new domains are all faced with the challenge of capturing information they find for later use, However saving information is challenging on mobile devices, where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use Furthermore, beyond the challenge of simply selecting a region of bounded text on a mobile device, in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user, In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster. We introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration, to support identifying and saving information in an intentionally uncertain way on mobile devices. We find that this approach reduced selection time and was preferred by participants over the default system text selection method. We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context. In a two part user study.
2K_dev_1732	In this paper we empirically explore how the latency of transcriptions created by participants recruited on Amazon Mechanical Turk vary based on the accuracy of speech recognition output. We present results which indicate that starting with the ASR output is worse unless it is sufficiently accurate ( Word Error Rate of under 30\ % ).
2K_dev_1749	The topic of kinematics of laser rangefinders has received little attention in the robotics literature, even though such sensors have been the perception sensors of choice on commercial AGVs, field robots and aerial robots for some time. In recognition of the uniqueness of optical reflection mechanisms, this paper presents a formulation based on a matrix reflection operator.
2K_dev_1752	This paper presents a method for generating semi-algebraic invariants for systems governed by non-linear polynomial ordinary differential equations under semi-algebraic evolution constraints. The resulting invariant generation method is observed to be much more scalable and efficient than the na `` ive approach, exhibiting orders of magnitude performance improvement on many of the problems. Based on the notion of discrete abstraction, our method eliminates unsoundness and unnecessary coarseness found in existing approaches for computing abstractions for non-linear continuous systems and is able to construct invariants with intricate boolean structure, in contrast to invariants typically generated using template-based methods, In order to tackle the state explosion problem associated with discrete abstraction, we present invariant generation algorithms that exploit sound proof rules for safety verification, such as differential cut ( DC ), and a new proof rule that we call differential divide-and-conquer ( DDC ), which splits the verification problem into smaller sub-problems.
2K_dev_1765	For realtime generation of stylistic human motion to capture the complex relationships between styles of motion, to predict the timings of synthesized poses in the output style. We demonstrate the power of our approach Our method achieves superior performance. This paper presents a novel solution that automatically transforms unlabeled, heterogeneous motion data into new styles, The key idea of our approach is an online learning algorithm that automatically constructs a series of local mixtures of autoregressive models ( MAR ) We construct local MAR models on the fly by searching for the closest examples of each input pose in the database, Once the model parameters are estimated from the training data, the model adapts the current pose with simple linear transformations, In addition we introduce an efficient local regression model. By transferring stylistic human motion for a wide variety of actions, including walking running punching, kicking jumping and transitions between those behaviors in a comparison against alternative methods, We have also performed experiments to evaluate the generalization ability of our data-driven model as well as the key components of our system.
2K_dev_1772	Reconstructs building facades in 3D space. Show that our method compares competitively with the state of the art on both 2D and 3D measures, while yielding a richer interpretation of the 3D scene behind the image. In this paper we propose a novel algorithm that infers the 3D layout of building facades from a single 2D image of an urban scene, Different from existing methods that only yield coarse orientation labels or qualitative block approximations, our algorithm quantitatively using a set of planes mutually related by 3D geometric constraints, Each plane is characterized by a continuous orientation vector and a depth distribution, An optimal solution is reached through inter-planar interactions Due to the quantitative and plane-based nature of our geometric reasoning, our model is more expressive and informative than existing approaches.
2K_dev_1783	A popular approach in this regard is to represent a sequence using a bag of words ( BOW ) representation due to its : ( i ) fixed dimensionality irrespective of the sequence length, and ( ii ) its ability to compactly model the statistics in A drawback to the BOW representation, however is the intrinsic destruction of the temporal ordering information. In this paper we tackle the problem of efficient video event detection, We argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation. Show significant performance improvements across both isolated and continuous event detection tasks. In this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering Our representation, like BOW is of a fixed dimensionality making it easily integrated with a linear detection function. Extensive experiments on CK+, 6DMG and UvA-NEMO databases.
2K_dev_1785	Understanding the purpose of why sensitive data is used could help improve privacy as well as enable new kinds of access control. For inferring the purpose of sensitive data usage in the context of Android smartphone apps. And achieved an accuracy of about 85\ % and 94\ % respectively in inferring purposes, We have also found that text-based features alone are highly effective in inferring purposes. In this paper we introduce a new technique We extract multiple kinds of features from decompiled code, focusing on app-specific features and text-based features These features are then used to train a machine learning classifier. We have evaluated our approach in the context of two sensitive permissions, namely ACCESS FINE LOCATION and READ CONTACT LIST.
2K_dev_1786	Some languages have very consistent mappings between graphemes and phonemes, while in other languages, this mapping is more ambiguous, Consonantal writing systems prove to be a challenge for Text to Speech Systems ( TTS ) because they do not indicate short vowels, which creates an ambiguity in pronunciation Special letter-to-sound rules may be needed for some cases in languages that otherwise have a good correspondence between graphemes and phonemes Our methods can be generalized to other languages that exhibit similar phenomena. In the low-resource scenario, we may not have linguistic resources such as diacritizers or hand-written rules for the language, to automatically learn pronunciations iteratively from acoustics. And show significant improvements for dialects of Arabic. We propose a technique during ITS training and predict pronunciations from text during synthesis time, We conduct experiments on dialects of Arabic for disambiguating homographs and Hindi for discovering the schwa-deletion rules. We evaluate our systems using objective and subjective metrics of TTS.
2K_dev_1789	Ensuring language coverage in dialog systems can be a challenge, since the language in a domain may drift over time, creating a mismatch between the original training data and current input, This in turn degrades performance by increasing misunderstanding and eventually leading to task failure, Without the capability of adapting the vocabulary and the language model based on certain domains or users, recognition errors may degrade the understanding performance, and even lead to a task failure, which incurs more time and effort to recover. This paper investigates how coverage can be maintained by automatically acquiring potential out-of-vocabulary ( OOV ) words. Show that both recognition and semantic parsing accuracy can thereby be improved. By leveraging different types of relatedness between vocabulary items and words retrieved from web-based resources.
2K_dev_1790	We study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems, in particular learning the hierarchical semantic structure from the data. The experiments show that high-level semantic information can accurately estimate the prominence of slots, significantly improving the slot induction performance ; furthermore, a semantic decoder trained on the data with automatically extracted slots achieves about 68\ % F-measure, which is close to the one from hand-crafted grammars. Given unlabelled conversations we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots ( e, both slots `` direction { '' } and `` locale { '' } convey location-related information ) for building a coherent semantic hierarchy, and then estimate the slot importance at different levels, The high-level semantic estimation involves not only within-slot but also cross slot relations.
2K_dev_1805	Multimodal analysis has long been an integral part of studying learning, Historically multimodal analyses of learning have been extremely laborious and time intensive and the implications that this work may have in non-education-related contexts. However researchers have recently been exploring ways to use multimodal computational analysis in the service of studying how people learn in complex learning environments, In an effort to advance this research agenda, In this paper we discuss the algorithms used. We find that affect-and pose-based segmentation are more effective, than traditional approaches for drawing correlations between learning-relevant constructs, and multimodal behaviors We also find that pose-based segmentation outperforms the two more traditional segmentation strategies for predicting student success on the hands-on task. In particular we propose affect-and pose-based data segmentation, as alternatives to human-based segmentation. We present a comparative analysis of four different data segmentation techniques, In a study of ten dyads working on an open-ended engineering design task.
2K_dev_1808	Do we really need 3D labels in order to learn how to predict 3D ?. In this paper we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label to learn the mapping in a completely unsupervised manner. Despite never seeing a 3D label, our method produces competitive results. Rather than use explicit supervision, we use the regularity of indoor scenes. We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D is unavailable.
2K_dev_1810	This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation, We argue that doing well on this task requires the model to learn to recognize objects and their parts. Given only a large, unlabeled image collection we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first.
2K_dev_1811	Mobile applications frequently access sensitive personal information to meet user or business requirements, Because such information is sensitive in general, regulators increasingly require mobile-app developers to publish privacy policies that describe what information is collected, Furthermore regulators have fined companies when these policies are inconsistent with the actual data practices of mobile apps. To help mobile-app developers check their privacy policies against their apps ' code for consistency. We propose a semi-automated framework that consists of a policy terminology-API method map that links policy phrases to API methods that produce sensitive information, and information flow analysis to detect misalignments. We present an implementation of our framework based on a privacy-policy-phrase ontology and a collection of mappings from API methods to policy phrases.
2K_dev_1812	We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training, First we use easy images to train an initial visual representation, We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories.
2K_dev_1815	Varied sources of error contribute to the challenge of facial action unit detection, Previous approaches address specific and known sources, However many sources are unknown. To address the ubiquity of error. With few exceptions CPM outperformed baseline and state-of-the art methods. We propose a Confident Preserving Machine ( CPM that follows an easy-to-hard classification strategy, During training CPM learns two confident classifiers, A confident positive classifier separates easily identified positive samples from all else ; a confident negative classifier does same for negative samples, During testing CPM then learns a person-specific classifier using `` virtual labels { '' } provided by confident classifiers, This step is achieved using a quasi-semi-supervised ( QSS ) approach, Hard samples are typically close to the decision boundary, and the QSS approach disambiguates them using spatio-temporal constraints. To evaluate CPM we compared it with a baseline single-margin classifier and state-of-the-art semi-supervised learning, transfer learning and boosting methods in three datasets of spontaneous facial behavior.
2K_dev_1816	Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent, semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ. Motivated by object recognition literature and recent work on rapidly estimating linear classifiers, we treat semantic correspondence as a constrained detection problem, where an exemplar LDA classifier is learned for each pixel. LDA classifiers have two distinct benefits : ( i ) they exhibit higher average precision than similarity metrics typically used in correspondence problems, and ( ii ) unlike exemplar SVM, can output globally interpretable posterior probabilities without calibration, whilst also being significantly faster to train, We pose the correspondence problem as a graphical model, where the unary potentials are computed via convolution with the set of exemplar classifiers, and the joint potentials enforce smoothly varying correspondence assignment.
2K_dev_1817	Starting with the seminal work by Kempe et al, a broad variety of problems, such as targeted marketing and the spread of viruses and malware, have been modeled as selecting a subset of nodes to maximize diffusion through a network, In cyber-security applications however, a key consideration largely ignored in this literature is stealth, In particular an attacker often has a specific target in mind, but succeeds only if the target is reached ( e, by malware ) before the malicious payload is detected and corresponding countermeasures deployed, The dual side of this problem is deployment of a limited number of monitoring units, such as cyber-forensics specialists, so as to limit the likelihood of such targeted and stealthy diffusion processes reaching their intended targets. We investigate the problem of optimal monitoring of targeted stealthy diffusion processes, and for the setting in which an attacker optimally responds to the placement of monitoring nodes. Show that a number of natural variants of this problem are NP-hard to approximate, On the positive side, we show that if stealthy diffusion starts from randomly selected nodes, the defender 's objective is submodular, and a fast greedy algorithm has provable approximation guarantees, show that the proposed algorithms are highly effective and scalable. In addition we present approximation algorithms by adaptively selecting the starting nodes for the diffusion process.
2K_dev_1818	Poor spelling is a challenge faced by people with dyslexia throughout their lives, Spellcheckers are therefore a crucial tool for people with dyslexia, but current spellcheckers do not detect real-word errors, which are a common type of errors made by people with dyslexia, Real-word errors are spelling mistakes that result in an unintended but real word, for instance form instead of from, Nearly 20\ % of the errors that people with dyslexia make are real-word. To detect real-world errors. And showed that it detects more of these errors than widely used spellcheckers, people with dyslexia corrected sentences more accurately and in less time with Real Check. In this paper we introduce a system called Real Check that uses a probabilistic language model, a statistical dependency parser and Google n-grams.
2K_dev_1821	Modern cyber-physical systems interact closely with continuous physical processes like kinematic movement, Software component frameworks do not provide an explicit way to represent or reason about these processes, Meanwhile hybrid program models have been successful in proving critical properties of discrete-continuous systems, These programs deal with diverse aspects of a cyber-physical system such as controller decisions, component communication protocols and mechanical dynamics, requiring several programs to address the variation, However currently these aspects are often intertwined in mostly monolithic hybrid programs, which are difficult to understand. These issues can be addressed by component-based engineering, making hybrid modeling more practical, This paper lays the foundation for using to provide component-based benefits to developing hybrid programs. Architectural models We build formal architectural abstractions of hybrid programs and formulas, enabling analysis of hybrid programs at the component level, reusing parts of hybrid programs, and automatic transformation from views into hybrid programs and formulas. Our approach is evaluated in the context of a robotic collision avoidance case study.
2K_dev_1823	Research has shown that understanding conversational structure between students is paramount to evaluating the productivity of the collaboration and estimating outcomes, However previous methods often rely on human supplied dialogue act labels or discourse parsing algorithms requiring large labeled datasets. For understanding discussions between students in MOOC forums, for discovering instances in which a response relation exists between a pair of posts in a forum thread. In this paper we present a new method In particular, we introduce a machine learning method, for example when one student provides the answer to a question or comments on something another student previously said Our method, which utilizes a fast, exact optimization process known as spectral optimization, does not require manually annotated training data and is highly scalable and generalizable. Empirical using real world datasets consisting of conversations between students participating in Coursera courses.
2K_dev_1830	This approach is complementary to other efforts in the literature on speeding up computation through GPU implementation, fast matrix operations or quantization, in that any of these optimizations can be incorporated. In this paper we investigate the issue of evaluating efficiently a large set of models on an input image in detection and classification tasks. We are able to dramatically reduce the rate of growth of computation as the number of models increases, show that we are able to maintain, or even exceed the level of performance compared to the default approach of using all the models directly, in both detection and classification tasks. We show that by formulating the visual task as a large matrix multiplication problem, something that is possible for a broad set of modern detectors and classifiers The approach, based on a bilinear separation model, combines standard matrix factorization with a task-dependent term which ensures that the resulting smaller size problem maintains performance on the original task.
2K_dev_1833	Requirements analysts can model regulated data practices to identify and reason about risks of noncompliance, If terminology is inconsistent or ambiguous, however these models and their conclusions will be unreliable, Tregex is a utility to match regular expressions against constituency parse trees, which are hierarchical expressions of natural language clauses, including noun and verb phrases. To study this problem, we investigated an approach to automatically construct an information type ontology by identifying information type hyponymy in privacy policies using Tregex patterns.
2K_dev_1837	Cultural events are kinds of typical events closely related to history and nationality, which play an important role in cultural heritage through generations. However automatically recognizing cultural events still remains a great challenge since it depends on understanding of complex image contents such as people, objects and scene context, Therefore it is intuitive to associate this task with other high-level vision problems, object detection recognition and scene understanding, In this paper we address this problem for object / scene contents mining for representation via CNN. By combining both ideas of object / scene contents mining and strong image representation via CNN into a whole framework, Specifically  we employ selective search to extract a batch of bottom-up region proposals, which are served as key object / scene candidates in each event image ; while, we investigate two state-of-the-art deep architectures, VGGNet and GoogLeNet and adapt them to our task by performing domain-specific ( i, event ) fine-tuning on both global image and hierarchical region proposals, These two models can complementarily exploit feature hierarchies spatially, which simultaneously capture the global context and local evidences within the image.
2K_dev_1839	Automated program repair ( APR ) is a challenging process of detecting bugs, localizing buggy code generating fix candidates and validating the fixes, Effectiveness of program repair methods relies on the generated fix candidates, and the methods used to traverse the space of generated candidates to search for the best ones Existing approaches generate fix candidates based on either syntactic searches over source code or semantic analysis of specification. To enhance the search space of APR, and provide a function to effectively traverse the search space. In this paper we propose to combine both syntactic and semantic fix candidates We present an automated repair method based on structured specifications, deductive verification and genetic programming, Given a function with its specification, we utilize a modular verifier to detect bugs and localize both program statements and sub-formulas in the specification that relate to those bugs While the former are identified as buggy code, the latter are transformed as semantic fix candidates, We additionally generate syntactic fix candidates via various mutation operators, Best candidates which receives fewer warnings via a static verification, are selected for evolution though genetic programming until we find one satisfying the specification, Another interesting feature of our proposed approach is that we efficiently ensure the soundness of repaired code through modular ( or compositional ) verification. We implemented our proposal and tested it on C programs taken from the SIR benchmark that are seeded with bugs.
2K_dev_1846	Applications such as construction monitoring and planning for renovations, require the accurate recovery of existing conditions of structures. Many types of infrastructure are primarily comprised of arbitrarily-shaped thin structures ( e, truss bridges steel frame buildings under construction, and transmission towers ), which existing automatic modeling methods are incapable of handling, to automatically recognize and model beams, planes and joints to recover their topology. We demonstrate the capability and robustness of our approach. To address this issue, this paper presents an approach from a 3D point cloud containing a complex network of thin structures, and In our approach, each beam is evolved from a seed by matching and aligning the cross section images, This growing algorithm can model beams with arbitrary cross sections By performing the algorithm on a point connectivity graph, we distinguish beams from joints and improve the algorithm 's robustness to closely spaced objects, In parallel planes and joints are also extracted and modeled, The connectivity graph of these primitives allows for a compact, object-level understanding of the entire structure. On both synthetic and real datasets.
2K_dev_1847	Many learning-based computer vision algorithms perform poorly when faced with examples that are dissimilar to those on which they were trained. Domain adaptation methods attempt to address this problem, but usually assume that the source domain is specified a priori for situations where more than one source domain available to choose the source domain most similar to the target domain to further adapt the chosen source domain to the target data. Show that the proposed approach outperforms existing single-step methods on a dataset of nine building styles. We propose a two-step approach The first step uses a small number of labeled examples, while the second step uses traditional domain adaptation methods.
2K_dev_1870	A key challenge of developing robots that work closely with people is creating a user interface that allows a user to communicate complex instructions to a robot quickly and easily. We consider a walking logistics support robot, which is designed to carry heavy loads to locations that are too difficult to reach with a wheeled or tracked vehicle to allow a particular user to designate himself as the robot 's leader, and guide the robot along a desired path. To show that the proposed system is able to detect and track a leader through unconstrained and cluttered off-road environments under a wide variety of illumination and motion conditions. This paper presents a marker tracking system that uses near infrared cameras, retro-reflective markers and LIDAR. In this application the robot is carrying equipment and supplies for a group of pedestrians, and the primary task for the user interface is to keep the robot traveling with the overall group in the right formation, We provide an extensive quantitative evaluation.
2K_dev_1875	Robotic swarms are distributed systems whose members interact via local control laws to achieve a variety of behaviors, In many practical applications, human operators may need to change the current behavior of a swarm from the goal that the swarm was going towards into a new goal due to dynamic changes in mission objectives, There are two related but distinct capabilities needed to supervise a robotic swarm The first is comprehension of the swarm 's state and the second is prediction of the effects of human inputs on the swarm 's behavior, Both of them are very challenging, Prior work in the literature has shown that inserting the human input as soon as possible to divert the swarm from its original goal towards the new goal does not always result in optimal performance ( measured by some criterion such as the total time required by the swarm to reach the second goal ), This phenomenon has been called Neglect Benevolence, conveying the idea that in many cases it is preferable to neglect the swarm for some time before inserting human input. In this paper we study how humans can develop an understanding of swarm dynamics so they can predict the effects of the timing of their input on the state and performance of the swarm allowing comparison between human and optimal input timing performance in control of swarms. Our results show that humans can learn to approximate optimal timing and that displays which make consensus variables perceptually accessible can enhance performance. We developed the swarm configuration shape-changing Neglect Benevolence Task as a Human Swarm Interaction ( HSI ) reference task.
2K_dev_1889	That solves rearrangement planning problems. We demonstrate the ability to solve more rearrangement by pushing tasks than existing primitive based solutions, Finally we show the plans we generate are feasible for execution on a real robot. We present a randomized kinodynamic planner We embed a physics model into the planner to allow reasoning about interaction with objects in the environment By carefully selecting this model, we are able to reduce our state and action space, gaining tractability in the search The result is a planner capable of generating trajectories for full arm manipulation and simultaneous object interaction.
2K_dev_1890	To rearrange cluttered environments. And show that on a variety of environments we can achieve a higher planning success rate given a restricted time budget for planning. In this work we present a fast kinodynamic RRT-planner that uses dynamic nonprehensile actions In contrast to many previous works, the presented planner is not restricted to quasi-static interactions and monotonicity, Instead the results of dynamic robot actions are predicted using a black box physics model, Given a general set of primitive actions and a physics model, the planner randomly explores the configuration space of the environment to find a sequence of actions that transform the environment into some goal configuration, In contrast to a naive kinodynamic RRT-planner we show that we can exploit the physical fact that in an environment with friction any object eventually comes to rest, This allows a search on the configuration space rather than the state space, reducing the dimension of the search space by a factor of two without restricting us to non-dynamic interactions. We compare our algorithm against a naive kinodynamic RRT-planner.
2K_dev_1891	Capable of finding statistically significant discrepancies, determining the situations in which they occur, and making simple corrections to the world model to improve performance. We present an execution monitoring framework In our approach, plans are initially based on a model of the world that is only as faithful as computational and algorithmic limitations allow Through experience, the monitor discovers previously unmodeled modes of the world, defined as regions of a feature space in which the experienced outcome of a plan deviates significantly from the predicted outcome, The monitor may then make suggestions to change the model to match the real world more accurately. We demonstrate this approach on the adversarial domain of robot soccer : we monitor pass interception performance of potentially unknown opponents to try to find unforeseen modes of behavior that affect their interception performance.
2K_dev_1895	With the prevalence of social media, such as Twitter short-length text like microblogs have become an important mode of text on the Internet, In contrast to other forms of media, such as newspaper the text in these social media posts usually contains fewer words, and is concentrated on a much narrower selection of topics, For these reasons traditional LDA-based sentiment and topic modeling techniques generally do not work well in case of social media data, Another characteristic feature of this data is the use of special meta tokens, such as hashtags which contain unique semantic meanings that are not captured by other ordinary words. In the recent years, many topic modeling techniques have been proposed for social media data, but the majority of this work does not take into account the specialty of tokens, such as hashtags and treats them as ordinary words, to address the problem of discovering latent topics and their sentiment from social media data, mainly microblogs like Twitter. In this paper we propose probabilistic graphical models We first propose MTM ( Microblog Topic Model ), a generative model that assumes each social media post generates from a single topic, and models both words and hashtags separately, We then propose MSTM ( Microblog Sentiment Topic Model ), an extension of MTM, which also embodies the sentiment associated with the topics. We evaluated our models using Twitter dataset.
2K_dev_1899	Our work is motivated by the potential impact of realistic simulators on the development cycle of software for real robots, Unlike calibration where the goal is to identify and remove error from a signal. We study the problem of building a sensor model for the purpose of simulation our aim to reproduce the signal in its entirety, including its error properties. The case is made for building models from approximate state information, relieving the burden of ground truth, Instead of physically modeling sensor behavior, a data-driven approach is taken. The implementation of our approach to simulate a simple but noisy laser rangefinder is described, Finally approaches to validate the simulator are discussed, We compare not only raw sensor predictions, but also overall performance of algorithms on simulated versus real data.
2K_dev_1902	One example is server-side scheduling for video service, where clients request flows of content from a server with limited capacity, and any content not delivered by its deadline is lost State-of-the-art policies, like Discriminatory Processor Sharing and Weighted Fair Queueing, use a fixed static proportional allocation of service rate and fail to achieve both goals, The well-known Earliest Deadline First policy minimizes overall loss, but fails to provide proportional loss across flows, because it treats packets as independent jobs. We prove that all policies in this broad class minimize overall loss Furthermore, we demonstrate that many EPDF policies accurately differentiate loss fractions in proportion to class weights, satisfying the second goal. This paper introduces the Earliest Progressive Deadline First ( EPDF ) class of policies.
2K_dev_1908	Weighted signed networks ( WSNs ) are networks in which edges are labeled with positive and negative weights, WSNs can capture like/dislike, trust/distrust and other social relationships between people. In this paper we consider the problem of predicting the weights of edges in such networks. We propose two novel measures of node behavior : the goodness of a node intuitively captures how much this node is liked/trusted by other nodes, while the fairness of a node captures how fair the node is in rating other nodes ' likeability or trust level We provide axioms that these two notions need to satisfy and show that past work does not meet these requirements for WSNs, We provide a mutually recursive definition of these two concepts and prove that they converge to a unique solution in linear time, We use the two measures to predict the edge weight in WSNs. That when compared against several individual algorithms from both the signed and unsigned social network literature We then use these as features in different multiple regression models.
2K_dev_1909	The collective buys energy as a group through a central coordinator who also decides about the storage and usage of renewable energy produced by the collective, Minimizing the cost is not only of interest to the consumers but is also socially desirable because it reduces the consumption at times of peak demand. In this paper we focus on demand side management in consumer collectives with community owned renewable energy generation and storage facilities for effective integration of renewable energy with the existing fossil fuel-based power supply system, Our objective is to design coordination algorithms to minimize the cost of electricity consumption of the consumer collective while allowing the consumers to make their own consumption decisions based on their private consumption constraints and preferences. We prove that our algorithm converges, and it achieves the optimal solution We also present simulation results to quantify the performance of our algorithm. We develop an iterative coordination algorithm in which the coordinator makes the storage decision and shapes the demands of the consumers by designing a virtual price signal for the agents. Based on real world consumption data.
2K_dev_1911	To be able to provide better support for collaborative learning in Intelligent Tutoring Systems, it is important to understand how collaboration patterns change Although interactive talk is often held as a gold standard in collaboration, as students become more proficient, it may not be as important. Prior work has looked at the interdependencies between utterances and the change of dialogue over time, but it has not addressed how dialogue changes during a lesson, an analysis that allows us to investigate the adaptivity of student strategies as students gain domain knowledge. We found that over time, the frequency of interactive talk and errors both decrease in dyads working together on conceptual problems.
2K_dev_1913	Collaborative and individual learning appear to have complementary strengths ; however, the best way to combine these learning methods is still unclear, While previous work has demonstrated the effectiveness of Intelligent Tutoring Systems ( ITSs ) for individual learning, collaborative learning with ITSs is much less frequent - especially for young students, In addition we propose future research to understand how to best combine individual and collaborative learning within an ITS. In this paper we discuss our prior and future work with elementary school students that aims to investigate how to best combine individual and collaborative learning using their complementary strengths within an ITS. Our previous findings demonstrate that ITSs are able to support collaboration, as well as individual learning.
2K_dev_1919	Computer vision is increasingly becoming interested in the rapid estimation of object detectors, The canonical strategy of using Hard Negative Mining to train a Support Vector Machine is slow, since the large negative set must be traversed at least once per detector, Recent work has demonstrated that, with an assumption of signal stationarity, Linear Discriminant Analysis is able to learn comparable detectors without ever revisiting the negative set, Even with this insight, the time to learn a detector can still be on the order of minutes, Correlation filters on the other hand, can produce a detector in under a second However, this involves the unnatural assumption that the statistics are periodic, and requires the negative set to be re-sampled per detector size, These two methods differ chiefly in the structure which they impose on the covariance matrix of all examples. This paper is ( i ) to assume periodic statistics without needing to revisit the negative set and ( ii ) to accelerate the estimation of detectors with aperiodic statistics. Verified that periodicity is detrimental. A comparative study It is experimentally.
2K_dev_1925	To improve the automatic detection of events in short sentences when in the presence of a large number of event classes.
2K_dev_1926	Problems of this nature arise in formal verification of continuous and hybrid dynamical systems, where there is an increasing need for methods to expedite formal proofs. This paper presents a theoretical and experimental comparison of sound proof rules for proving invariance of algebraic sets, that is sets satisfying polynomial equalities, under the flow of polynomial ordinary differential equations. The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting. We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of heterogeneous benchmarks.
2K_dev_1933	To date the study of dispatching or load balancing in server farms has primarily focused on the minimization of response time, Server farms are typically modeled by a front-end router that employs a dispatching policy to route jobs to one of several servers, with each server scheduling all the jobs in its queue via Processor-Sharing. However the common assumption has been that all jobs are equally important or valuable, in that they are equally sensitive to delay, Our work departs from this assumption In this context, we ask `` what is a good dispatching policy to minimize the value-weighted response time metric ?. We are able to deduce many unexpected results regarding dispatching. : we model each arrival as having a randomly distributed value parameter, independent of the arrival 's service requirement ( job size ), Given such value heterogeneity, the correct metric is no longer the minimization or response time, but rather the minimization of value-weighted response time, { '' } We propose a number of new dispatching policies that are motivated by the goal of minimizing the value-weighted response time. Via a combination of exact analysis, asymptotic analysis and simulation.
2K_dev_1934	For programming graph- based algorithms in a declarative fashion, In this paper we present the syntax and operational semantics of our language and. LM tends to be more expressive than other logic programming languages, LM programs are naturally concurrent illustrate its use. We have designed a new logic programming language called LM ( Linear Meld ) Our language is based on linear logic, an expressive logical system where logical facts can be consumed, Because LM integrates both classical and linear logic, because facts are partitioned by nodes of a graph data structure, Computation is performed at the node level while communication happens between connected nodes, through a number of examples.
2K_dev_1939	Security requirements patterns represent reusable security practices that software engineers can apply to improve security in their system, Reusing best practices that others have employed could have a number of benefits, such as decreasing the time spent in the requirements elicitation process or improving the quality of the product by reducing product failure risk, Pattern selection can be difficult due to the diversity of applicable patterns from which an analyst has to choose. The challenge is that identifying the most appropriate pattern for a situation can be cumbersome and time-consuming, to review only relevant patterns and quickly select the most appropriate patterns for the situation, to relate patterns based on decisions made by the pattern user, to help the pattern user select the most appropriate patterns for their situation. We propose a new method that combines an inquiry-cycle based approach with the feature diagram notation Similar to patterns themselves, our approach captures expert knowledge The resulting pattern hierarchies allow users to be guided through these decisions by questions, which introduce related patterns in order, thus resulting in better requirement generation. We evaluate our approach using access control patterns in a pattern user study.
2K_dev_1940	Government laws and regulations increasingly place requirements on software systems, Ideally experts trained in law will analyze and interpret legal texts to inform the software requirements process, However in small companies and development teams with short launch cycles, individuals with little or no legal training will be responsible for compliance, Two specific challenges commonly faced by non-experts are deciding if their system is covered by a law, and then deciding whether two legal requirements are similar or different. In this study we assess the ability of laypersons, technical professionals and legal experts to judge the similarity between legal coverage conditions and requirements. In so doing we discovered that legal experts achieved higher rates of consensus more frequently than technical professionals or laypersons and that all groups had slightly greater agreement when judging coverage conditions than requirements, we found that technical professionals and legal experts exhibited consistently greater agreement than that found between laypersons and legal experts, and that each group tended towards different justifications, such as laypersons and technical professionals tendency towards categorizing different coverage conditions or requirements as equivalent if they believed them to possess the same underlying intent. Measured by Fleiss ' K, When comparing judgments between groups using a consensus-based Cohen 's Kappa.
2K_dev_1942	Curse of dimensionality is a practical and challenging problem in image categorization, especially in cases with a large number of classes, Multi-class classification encounters severe computational and storage problems when dealing with these large scale tasks. To effectively reduce dimensionality of parameter space without sacrificing classification accuracy, and at the same time exploit information in semantic taxonomy among categories. Further demonstrate the effectiveness of hierarchical feature hashing. In this paper we propose hierarchical feature hashing. We provide detailed theoretical analysis on our proposed hashing method Moreover, experimental results on object recognition and scene classification.
2K_dev_1949	Research shows that commonly accepted security requirements are not generally applied in practice Instead of relying on requirements checklists, security experts rely on their expertise and background knowledge to identify security vulnerabilities. To understand the gap between available checklists and practice, to evaluate their security requirements methods against how experts transition through different situation awareness levels in their decision-making process. We report our preliminary results of analyzing two interviews that reveal possible decision-making patterns that could characterize how analysts perceive, comprehend and project future threats which leads them to decide upon requirements and their specifications, in addition to how experts use assumptions to overcome ambiguity in specifications. Our goal is to build a model that researchers can use. We conducted a series of interviews to encode the decision-making process of security experts and novices during security requirements analysis, Participants were asked to analyze two types of artifacts : source code, and network diagrams for vulnerabilities and to apply a requirements checklist to mitigate some of those vulnerabilities, We framed our study using Situation Awareness-a cognitive theory from psychology-to elicit responses that we later analyzed using coding theory and grounded analysis.
2K_dev_1968	In this paper we investigate 3D attributes as a means to understand the shape of an object in a single image. ( ii ) we show that such properties can be successfully inferred from a single image ( iv ) we show that the 3D attributes trained on this dataset generalize to images of other ( non-sculpture ) object classes ; and furthermore ( v ) we show that the CNN also provides a shape embedding that can be used to match previously unseen sculptures largely independent of viewpoint.
2K_dev_1971	An increasing number of mobile devices are capable of automatically sensing and recording rich information about the surrounding environment, Spatial locations of such data can help to better learn about the environment. In this work we address the problem of identifying the locations visited by a mobile device as it moves within an indoor environment. We will show robustness the ability to propose coarse sensor noise errors. We focus on devices equipped with odometry sensors that capture changes in motion, Odometry suffers from cumulative errors of dead reckoning but it captures the relative shape of the traversed path well, Our approach will correct such errors by matching the shape of the trajectory from odometry to traversable paths of a known map, Our algorithm is inspired by prior vehicular GPS map matching techniques that snap global GPS measurements to known roads, We similarly wish to snap the trajectory from odometry to known hallways, Several modifications are required to ensure these techniques are robust when given relative measurements from odometry, If we assume an office-like environment with only straight hallways, then a significant rotation indicates a transition to another hallway, As a result we partition the trajectory into line segments based on significant turns, Each trajectory segment is snapped to a corresponding hallway that best maintains the shape of the original trajectory, These snapping decisions are made based on the similarity of the two curves as well as the rotation to transition between hallways. Under different types of noise in complex environments and.
2K_dev_1972	That can greatly improve the level of intelligence and driving quality of autonomous vehicles. The proposed behavioral planning architecture improves the driving quality considerably, 3\ % reduction of required computation time in representative scenarios. In this paper we propose a novel planning framework A reference planning layer first generates kinematically and dynamically feasible paths assuming no obstacles on the road, then a behavioral planning layer takes static and dynamic obstacles into account, Instead of directly commanding a desired trajectory, it searches for the best directives for the controller, such as lateral bias and distance keeping aggressiveness, It also considers the social cooperation between the autonomous vehicle and surrounding cars. Based on experimental results from both simulation and a real autonomous vehicle platform.
2K_dev_1975	Illumination defocus and global illumination effects are major challenges for active illumination scene recovery algorithms. Illumination defocus limits the working volume of projector-camera systems and global illumination can induce large errors in shape estimates. We demonstrate the effectiveness of our approach. In this paper we develop an algorithm for scene recovery in the presence of both defocus and global light transport effects such as interreflections and sub-surface scattering, Our method extends the working volume by using structured light patterns at multiple projector focus settings, A careful characterization of projector blur allows us to decode even partially out-of-focus patterns This enables our algorithm to recover scene shape and the direct and global illumination components over a large depth of field while still using a relatively small number of images ( typically 25-30 ). By recovering high quality depth maps of scenes containing objects made of optically challenging materials such as wax, marble soap colored glass and translucent plastic.
2K_dev_1976	Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D point triangulation to reconstruct a static scene. The triangulation constraint however is invalid for moving points captured in multiple unsynchronized videos and bundle adjustment is not purposed to estimate the temporal alignment between cameras that jointly optimizes four coupled sub-problems : estimating camera intrinsics and extrinsics, triangulating 3D static points, as well as subframe temporal alignment between cameras and estimating 3D trajectories of dynamic points. Because the videos are aligned with sub-frame precision, we reconstruct 3D trajectories of unconstrained outdoor activities at much higher temporal resolution than the input videos. In this paper we present a spatiotemporal bundle adjustment approach Key to our joint optimization is the careful integration of physics-based motion priors within the reconstruction pipeline, validated on a large motion capture corpus, We present an end-to-end pipeline that takes multiple uncalibrated and unsynchronized video streams and produces a dynamic reconstruction of the event.
2K_dev_1977	Turbulence is studied extensively in remote sensing, astronomy meteorology aerodynamics and fluid dynamics, The strength of turbulence is a statistical measure of local variations in the turbulent medium, It influences engineering decisions made in these domains, Turbulence strength ( TS ) also affects safety of aircraft and tethered balloons, and reliability of free-space electromagnetic relays. We show that it is possible to estimate TS, without having to reconstruct instantaneous fluid flow fields, Instead the TS field can be directly recovered. Using videos captured from different viewpoints, We formulate this as a linear tomography problem with a structure unique to turbulence fields, No tight synchronization between cameras is needed Thus, realization is very simple to deploy using consumer-grade cameras. We experimentally demonstrate this both in a lab and in a large-scale uncontrolled complex outdoor environment, which includes industrial rural and urban areas.
2K_dev_1980	To parse human motion in unconstrained Internet videos without labeling any videos for training. Experiments show that our method achieves good performance for parsing human motions Furthermore, we found that our method achieves better performance by using unlabeled video than adding more labeled pose images into the training set. In this paper we propose a method We use the training samples from a public image pose dataset to avoid the tediousness of labeling video streams There are two main problems confronted, First the distribution of images and videos are different, Second no temporal information is available in the training images, To smooth the inconsistency between the labeled images and unlabeled videos, our algorithm iteratively incorporates the pose knowledge harvested from the testing videos into the image pose detector via an adjust-and-refine method, During this process continuity and tracking constraints are imposed to leverage the spatio-temporal information only available in videos. For our experiments we have collected two datasets from YouTube and.
2K_dev_1983	For single-view reasoning about 3D surfaces and their relationships. We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces. In this work we present a method We propose the use of midlevel constraints for 3D scene understanding in the form of convex and concave edges and introduce a generic framework capable of incorporating these and other constraints Our method takes a variety of cues and uses them to infer a consistent interpretation of the scene.
2K_dev_1989	Collaborative learning has been shown to be beneficial for older students, but there has not been much research to show if these results transfer to elementary school students, In addition collaborative and individual modes of instruction may be better for acquiring different types of knowledge, Collaborative Intelligent Tutoring Systems ( ITS ) provide a platform that may be able to provide both the cognitive and collaborative support that students need This work indicates that by embedding collaboration scripts in ITSs, collaborative learning can be an effective instructional method even with young children. This paper presents a study comparing collaborative and individual methods while receiving instruction on either procedural or conceptual knowledge. The collaborative groups had the same learning gains as the individual groups in both the procedural and conceptual learning conditions but were able to do so with fewer problems.
2K_dev_2011	Spoken language interfaces are being incorporated into various devices ( e, smart-phones smart TVs etc ). However current technology typically limits conversational interactions to a few narrow predefined domains/topics, For example dialogue systems for smart-phone operation fail to respond when users ask for functions not supported by currently installed applications. We propose to dynamically add application-based domains according to users ' requests by using descriptions of applications as a retrieval cue to find relevant applications The approach uses structured knowledge resources ( e, Freebase Wikipedia FrameNet ) to induce types of slots for generating semantic seeds, and enriches the semantics of spoken queries with neural word embeddings, where semantically related concepts can be additionally included for acquiring knowledge that does not exist in the predefined domains, The system can then retrieve relevant applications or dynamically suggest users install applications that support unexplored domains, We find that vendor descriptions provide a reliable source of information for this purpose.
2K_dev_2029	Silhouettes provide rich information on three-dimensional shape, since the intersection of the associated visual cones generates the `` visual hull { '' }, which encloses and approximates the original shape However, not all silhouettes can actually be projections of the same object in space : this simple observation has implications in object recognition and multi-view segmentation, and has been ( often implicitly ) used as a basis for camera calibration, and point out some possible directions for future research. In this paper we investigate the conditions for multiple silhouettes, or more generally arbitrary closed image sets, to be geometrically `` consistent { '' }. After discussing some general results. We present a `` dual { '' } formulation for consistency, that gives conditions for a family of planar sets to be sections of the same object, Finally we introduce a more general notion of silhouette `` compatibility { '' } under partial knowledge of the camera projections. We present this notion as a natural generalization of traditional multi-view geometry, which deals with consistency for points.
2K_dev_2031	Large-scale information processing systems are able to extract massive collections of interrelated facts. But unfortunately transforming these candidate facts into useful knowledge is a formidable challenge, In this paper we show how uncertain extractions about entities and their relations. We show that compared to existing methods, our approach is able to achieve improved AUC and F1 with significantly lower running time. Can be transformed into a knowledge graph The extractions form an extraction graph and we refer to the task of removing noise, inferring missing information and determining which candidate facts should be included into a knowledge graph as knowledge graph identification In order to perform this task, we must reason jointly about candidate facts and their associated extraction confidences, identify co-referent entities and incorporate ontological constraints, Our proposed approach uses probabilistic soft logic ( PSL ), a recently introduced probabilistic modeling framework which easily scales to millions of facts. We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL project containing over 1M extractions and 70K ontological relations.
2K_dev_2032	Occlusions are common in real world scenes and are a major obstacle to robust object detection, Previous approaches primarily enforced local coherency or learned the occlusion structure from data, However local coherency ignores the occlusion structure in real world scenes and learning from data requires tediously labeling many examples of occlusions for every view of every object, Other approaches require binary classifications of matching scores. To coherently reason about occlusions on many types of detectors. Our method demonstrates significant improvement in estimating the mask of the occluding region and improves object instance detection on a challenging dataset of objects under severe occlusions. In this paper we present a method We address these limitations by formulating occlusion reasoning as an efficient search over occluding blocks which best explain a probabilistic matching pattern.
2K_dev_2033	Binary codes that are binarizations of features represented by real numbers have recently been used in the object recognition field, in order to achieve reduced memory and robustness with respect to noise. However binarizing features represented by real numbers has a problem in that a great deal of the information within the features drops out, That is why we focus on quantization residual, which is information that drops out when features are binarized in order to take into consideration the possibility that a binary code which has been observed from an image will transition to another binary code. From the results of we confirmed that the proposed method enables an increase in detection performance while maintaining the same levels of memory and computing costs as those for previous methods of binarizing features. With this study we introduce a transition likelihood model into classifiers This enables classifications that consider transitions to the desired binary code, even if the observed binary code differs from the actually desired binary code for some reason.
2K_dev_2034	`` Socially cooperative driving { '' } is an integral part of our everyday driving. Requiring special attention to imbue the autonomous driving with a more natural driving behavior, to enable an autonomous vehicle to perform cooperative social behavior to extract the probability of surrounding agents ' intentions in real time, to predict future scenarios to compute the cost for each scenario and select the decision corresponding to the lowest cost. Compared with approaches that do not take social behavior into account, the iPCB algorithm shows a 41, 7\ % performance improvement based on the chosen cost functions. In this paper an intention-integrated Prediction-and Cost function-Based algorithm iPCB ) framework is proposed An intention estimator is developed Then for each candidate strategy, a prediction engine considering the interaction between host and surrounding agents is used A cost function-based evaluation is applied.
2K_dev_2040	Smartphones are now targets of malicious viruses Furthermore, the increasing `` connectedness { '' } of smartphones has resulted in new delivery vectors for malicious viruses, including proximity- social- and other technology-based methods, In fact Cabir and CommWarrior are two viruses-observed in the wild-that spread, at least in part, using proximity-based techniques ( line-of-sight bluetooth radio ). That describes the spread of two mutually exclusive viruses across heterogeneous composite networks, one static ( social connections ) and one dynamic ( mobility pattern ). Find that the first eigenvalue of the system matrices lambda ( S1 ), lambda ( S2 ) of the two networks ( static and dynamic networks ) appropriately captures the competitive interplay between two viruses and effectively predicts the competition 's `` winner { '' }, which provides a feasible way to defend against smartphone viruses. In this paper we propose and evaluate SI1I2S, a competition model To approximate dynamic network behavior, we use classic mobility models from ad hoc networking, Random Waypoint Random Walk and Levy Flight. We analyze our model using techniques from dynamic systems and.
2K_dev_2044	A robotic swarm is a decentralized group of robots which overcome failure of individual robots with robust emergent behaviors based on local interactions, These behaviors are not well built for accomplishing complex tasks, however because of the changing assumptions required in various applications and environments, A new movement in the research field is to add human input to influence the swarm in order to help make the robots goal directed and overcome these problems Previous studies have all used visual feedback through a computer interface to give the user the swarm state information Researchers in multi-robot systems have shown benefits of haptic feedback in obstacle navigation before. This research in Human Swarm Interaction ( HSI ) focuses on different control laws and ways to integrate the human intent with local control laws of the robots, to give the operator haptic feedback as well as visual feedback. The study shows the benefits of the additional feedback in a target searching class, In most environments operators were able to cover significantly more area, increasing the chance of finding more targets, The other environment found no significant difference, showing that the haptic feedback does not degrade performance in any of the tested environments, This supports our hypothesis that haptic feedback is useful in HSI and requires further research to maximize its potential. This study adapted swarm control algorithms but this study is a novel method because of the decentralized formation of the robotic swarm.
2K_dev_2046	Previous studies have examined the characteristics of physiological tremor under laboratory settings as well as different operating conditions, However different test methods make the comparison of results across trials and conditions difficult. This paper presents the characterization and comparison of physiological tremor for pointing tasks in multiple environments, as a baseline for performance evaluation of microsurgical robotics. Two vitroretinal microsurgeons were evaluated while performing a pointing task with no entry-point constraint, constrained by an artificial eye model, and constrained by a rabbit eye in vivo For the three respective conditions A spectral analysis was also performed.
2K_dev_2047	Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D, Manual needle insertion allows the clinician to control the insertion speed. For automatic 3D steering of manually inserted flexible needles. Demonstrate the performance of the proposed controller, show the feasibility of this technique in 2D and 3D environments. This paper presents a control law A look-ahead proportional controller for position and orientation is presented, The look-ahead distance is a linear function of insertion speed. Simulations in a 3D brain-like environment Experimental results also.
2K_dev_2063	Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. For the task of pose estimation. We demonstrate state-of-the-art performance and outperform competing methods. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation, We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference, Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. On standard benchmarks including the MPII, LSP and FLIC datasets.
2K_dev_2064	Finding meaningful structured representations of 3D point cloud data ( PCD ) has become a core task for spatial perception applications. For constructing compact generative representations of PCD at multiple levels of detail. Our tests showing favorable performance when compared to octree and NDT-based methods. In this paper we introduce a method As opposed to deterministic structures such as voxel grids or octrees, we propose probabilistic subdivisions of the data through local mixture modeling, and show how these subdivisions can provide a maximum likelihood segmentation of the data, The final representation is hierarchical, compact parametric and statistically derived, facilitating run-time occupancy calculations through stochastic sampling, Unlike traditional deterministic spatial subdivision methods, our technique enables dynamic creation of voxel grids according the application 's best needs, In contrast to other generative models for PCD, we explicitly enforce sparsity among points and mixtures, a technique which we call expectation sparsification, This leads to a highly parallel hierarchical Expectation Maximization ( EM ) algorithm well-suited for the GPU and real-time execution. We explore the trade-offs between model fidelity and model size at various levels of detail.
2K_dev_2066	Sudden weight gain in patients living with Congestive Heart Failure ( CHF ) is often an indication that the individual is retaining fluid, which often means that patient 's heart has weakened leading to increased risk of kidney or cardiac failure, leading to the possibility of earlier clinical interventions, potentially preventing deadly medical emergencies. Clinical interventions can be made at this stage, leading to better outcomes, however it is essential that the interventions take place before the patient 's health declines too drastically, allowing us to predict weight values into the future. In this work we present a latent variable autoregression model that tracks patient weight and blood pressure over time We are also able to model continuous heart-rate signals and evaluate a subject 's response to physical activity, This allows us to detect signs of health decline days earlier than existing rule-based systems.
2K_dev_2069	Human communication literature states that people with different culture backgrounds act differently in conversations. Currently most virtual agents are designed for a single targeted popular culture. We found that users from different culture context express engagement differently. We implemented two versions of a virtual agent targeting American and Chinese cultures.
