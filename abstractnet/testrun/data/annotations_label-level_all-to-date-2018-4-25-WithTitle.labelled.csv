segment	label
Online communities have the potential to be supportive	background
cruel or anywhere in between The development of positive norms for interaction can help users build bonds	background
grow and learn	background
Consistent with aspects of imitation theory and deterrence theory	background
This work considers the intersection of tools	background
authority and types of behaviors	background
offering a new frame through which to consider the development of moderation strategies	background
users imitated examples of behavior that they saw	finding
and more so for behaviors from high status users	finding
Proactive moderation tools	finding
such as chat modes which restricted the ability to post certain content	finding
proved effective at discouraging spam behaviors	finding
while reactive bans were able to discourage a wider variety of behaviors	finding
Using millions of messages sent in Twitch chatrooms	method
Recent research has demonstrated that ( a ) groups can be characterized by a collective intelligence ( CI ) factor that measures their ability to perform together on a wide range of different tasks	background
and ( b ) this factor can predict groups ' performance on other tasks in the future	background
we find that CI does	finding
indeed predict the competitive performance of teams controlling for the amount of time played as a team	finding
We also find that CI is positively correlated with the presence of a female team member and with the team members ' average social perceptiveness	finding
Finally unlike in prior studies	finding
tacit coordination in this setting plays a larger role than verbal communication	finding
In this study of teams playing the online game League of Legends	method
Results show that participants evaluated their dates based on evidence beyond externally judged slogan quality	finding
and relied heavily on their dyad-specific judgments in selecting teammates Results show that teams formed from preferred dates performed better on a final creative task compared to random dates or non-dates	finding
We introduce team dating	mechanism
where people interact on brief tasks before working with a dedicated partner for longer	mechanism
more complex tasks Team dating provides a dynamic technique The initial interactions provided information that helped people select and work with an appropriate teammate	mechanism
We studied team dating through two online experiments In Experiment 1	method
workers from a crowd platform independently wrote an ad slogan	method
discussed it with three consecutive people and evaluated their team date interactions	method
They then selected preferred teammates from a list showing average ratings for people they had dated and not dated In Experiment 2	method
we replicated the individual and team dating tasks	method
and formed teams	method
either i ) by honoring pairwise team dating preferences	method
ii ) randomly from their pool of dates	method
or iii ) randomly from those not dated	method
Collective intelligence ( CI )	background
a group 's capacity to perform a wide variety of tasks	background
is a key factor in successful collaboration	background
Our results have important implications for online collaborations and distributed teams	background
We find that synchrony in facial expressions ( indicative of shared experience ) was associated with CI and synchrony in skin conductance ( indicative of shared arousal ) with group satisfaction	finding
Furthermore various forms of synchrony mediated the effect of member diversity and social perceptiveness on CI and group satisfaction	finding
We present results from a laboratory experiment where 60 dyads completed the Test of Collective Intelligence ( TCI ) together online and rated their group satisfaction	method
while wearing physiological sensors	method
Feedback is information that can improve task performance	background
Online communities educational forums	background
and crowd-based feedback platforms all support feedback exchange among a more diverse set of sources than ever before	background
with greater control over how to moderate this exchange Our findings provide design implications for platforms to support more fruitful feedback exchange	background
We found that critiques with positive affec-tive language increased positive emotions and reduced participants ' annoyance and frustration	finding
which led to an increase in work quality	finding
compared to critiques without positive language	finding
Feedback without positive affective language led to more edits	finding
but not better work outcomes	finding
Participants reacted more positively to feedback from an anonymous source than from a peer or an authority	finding
We conducted an online experiment manipulating affective language and source of feedback on a writing task	method
How do individuals perceive algorithmic vs	background
group-made decisions ?	background
about one third of the participants perceived algorithmic decisions as less than fair ( 30 % for self	finding
36 % for group )	finding
often because algorithmic assumptions about users did not account for multiple concepts of fairness or social behaviors	finding
and the process of quantifying preferences through interfaces was prone to error	finding
algorithmic decisions were perceived to be less fair than discussion-based decisions	finding
dependent on participants ' interpersonal power and computer programming knowledge	finding
Our work suggests that for algorithmic mediation to be fair	finding
algorithms and their interfaces should account for social and altruistic behaviors that may be difficult to define in mathematical terms	finding
In our first qualitative study	method
In our second experiment	method
Social identities carry widely agreed upon meanings	background
called stereotypes that have important effects on social processes	background
Our work provides unique insights into the stereotypes of these users	finding
as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel	finding
parsimonious way	finding
Our method is grounded in two distinct strands of theory	mechanism
one that represents stereotypes as identities ' affective meanings and the other that represents stereotypes as semantic relationships between identities	mechanism
After validating our approach via a prediction task	method
we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies	method
A key issue	background
whenever people work together to solve a complex problem	background
is	background
The early results suggest that the method can	finding
indeed work at scale as intended	finding
with groups of contests called contest webs Based on the analogy of supply chains for physical products	mechanism
the method provides incentives for people to ( a ) reuse work done by themselves and others	mechanism
( b ) simultaneously explore multiple ways of combining interchangeable parts	mechanism
and ( c ) work on parts of the problem where they can contribute the most	mechanism
The paper also describes a field test of this method in an online community of over 50	method
000 people who are developing proposals for what to do about global climate change	method
Crowd workers are distributed and decentralized	background
Crowd guilds produced reputation signals more strongly correlated with ground-truth worker quality than signals available on current crowd working platforms	finding
and more accurate than in the traditional model	finding
In this paper	mechanism
we draw inspiration from historical worker guilds ( e	mechanism
g	mechanism
in the silk trade ) to design and implement crowd guilds: centralized groups of crowd workers who collectively certify each other 's quality through double-blind peer assessment	mechanism
A two-week field experiment compared crowd guilds to a traditional decentralized crowd work model	method
Social science researchers spend significant time annotating behavioral events in video data in order to quantitatively assess interactions [ 2 ]	background
These behavioral events may be instantaneous changes	background
continuous actions that span unbounded periods of time	background
or behaviors that would be best described by severity or other scalar ratings	background
These new features allow analysts to acquire more specific information about events in video datasets	finding
Glance [ 4 ] introduced a means of leveraging human intelligence by recruiting crowds of paid online workers to accurately analyze hours of video data in a matter of minutes	mechanism
This approach has been shown to expedite work in human-centered fields	mechanism
as well as generate training data for automated recognition systems	mechanism
In this paper	mechanism
we describe an interactive demonstration of an improved	mechanism
more expressive version of Glance that expands the initial set of supported annotation formats ( e	mechanism
g	mechanism
time range classification	mechanism
etc	mechanism
) from one to nine	mechanism
Worker interfaces for each of these options are dynamically generated	mechanism
along with tutorials	mechanism
based on the analyst 's question	mechanism
How effective are call and SMS logs in modeling tie strength ? Frequency and duration of communication has long been cited as a major aspect of tie strength	background
Intuitively this makes sense: people communicate with those that they feel close to	background
Consistent with theory	finding
we found that frequent or long-duration communication likely indicates a strong tie	finding
However the use of call and SMS logs produced many errors in separating strong and weak ties	finding
suggesting this approach is incomplete	finding
Follow-up interviews indicate fundamental challenges for inferring tie strength from communication logs	finding
We collected call and SMS logs and ground truth relationship data from 36 participants	method
Social influence is key in technology adoption	background
Our results suggest that social influence affects one 's likelihood to adopt a security feature	finding
but its effect varies based on the observability of the feature	finding
the current feature adoption rate among a potential adopter 's friends	finding
and the number of distinct social circles from which those feature-adopting friends originate Curiously	finding
there may be a threshold higher than which having more security feature adopting friends predicts for higher adoption likelihood	finding
but below which having more feature-adopting friends predicts for lower adoption likelihood	finding
Furthermore the magnitude of this threshold is modulated by the attributes of a feature-features that are more noticeable ( Login Approvals	finding
Trusted Contacts ) have lower thresholds	finding
Here we analyzed how three Facebook security features ' Login Approvals	method
Login Notifications and Trusted Contacts-diffused through the social networks of 1	method
5 million people	method
Feedback is an important component of the design process	background
but We conclude with implications for the design of crowd feedback services	background
In the first study	finding
we compared crowd and expert critiques and found evidence that aggregated crowd critique approaches expert critique In a second study	finding
we found that designers who got crowd feedback perceived that it improved their design process The third study showed that designers were enthusiastic about crowd critiques and used them to change their designs	finding
We present CrowdCrit	mechanism
a web-based system that allows designers to receive design critiques from non-expert crowd workers	mechanism
We evaluated CrowdCrit in three studies focusing on the designer 's experience and benefits of the critiques	method
Massive Open Online Courses ( MOOCs ) enable everyone to receive high-quality education	background
Our experiment show that ACD and PCD can detect usage of a cheat sheet with good accuracy and can reduce the overall human resources required to monitor MOOCs for cheating	finding
In this paper	mechanism
we propose a Massive Open Online Proctoring ( MOOP ) framework	mechanism
which combines both automatic and collaborative approaches to detect cheating behaviors in online tests The MOOP framework consists of three major components: Automatic Cheating Detector ( ACD )	mechanism
Peer Cheating Detector ( PCD )	mechanism
and Final Review Committee ( FRC )	mechanism
ACD uses webcam video or other sensors to monitor students and automatically flag suspected cheating behavior	mechanism
Ambiguous cases are then sent to the PCD	mechanism
where students peer-review flagged webcam video to confirm suspicious cheating behaviors	mechanism
Finally the list of suspicious cheating behaviors is sent to the FRC to make the final punishing decision	mechanism
Online communities much like companies in the business world	background
often need to transfer best practices internally from one unit to another to improve their performance	background
The current research introduces a contingency perspective on practice transfer	finding
holding that the value of modifications depends on when they are introduced and who introduces them modifications are more helpful if they are introduced after the receiving project has had experience with the imported practice Furthermore	finding
modifications are more effective if they are introduced by members who have experience in a variety of other projects	finding
Empirical research on the transfer of a quality-improvement practice between projects within Wikipedia shows that	method
Online crowds are a promising source of new innovations	background
However crowd innovation quality does not always match its quantity	background
A series of controlled experiments show that experienced facilitators increased the quantity and creativity of workers ' ideas compared to unfacilitated workers	finding
while Novice facilitators reduced workers ' creativity	finding
Analyses of inspiration strategies suggest these opposing results stem from differential use of successful inspiration strategies ( e	finding
g	finding
provoking mental simulations )	finding
The results show that expert facilitation can significantly improve crowd innovation	finding
but inexperienced facilitators may need scaffolding to be successful	finding
One approach would for experts to provide personalized feed-back	mechanism
but this scales poorly	mechanism
and may lead to premature convergence during creative work	mechanism
Drawing on strategies for facilitating face-to-face brainstorms	mechanism
we introduce a crowd ideation system where experts monitor incoming ideas through a dashboard and offer high-level `` inspirations '' to guide ideation	mechanism
support a broad range of collaborative and cooperative tasks	background
discuss possible incentives and safeguards to context sharing from a user standpoint	background
Through two prototypes	finding
we demonstrate how GCF can be used to We then show how our framework 's architecture allows devices to opportunistically detect and collaborate with one another	finding
even when running different applications Finally	finding
we present two real-world domains that show how GCF 's ability to form groups increases users ' access to relevant and timely information	finding
and	finding
In this paper	mechanism
we present the Group Context Framework ( GCF )	mechanism
a general-purpose toolkit that GCF provides a standardized way for developers to request contextual data for their applications The framework then intelligently groups with other devices to satisfy these requirements	mechanism
Telepresence means business people can make deals in other countries	background
doctors can give remote medical advice	background
and soldiers can rescue someone from thousands of miles away	background
When interaction is mediated	background
people are removed from and lack context about the person they are making decisions about We discuss implications of our results for theory and future research	background
The results suggest that technological mediation influences decision making	finding
but its influence depends on an individual 's self-construal: participants who saw themselves as defined through their relationships ( interdependent self-construal ) recommended riskier and more painful treatments in video conferencing than when face-to-face	finding
We conducted a laboratory experiment involving medical treatment decisions	method
Online question and answer ( Q & A ) sites	background
which are platforms for users to post and answer questions on a wide range of topics	background
are becoming large repositories of valuable knowledge and important to societies	background
In order to sustain success	background
Q & A sites face the challenges of ensuring content quality and encouraging user contributions	background
This work has implications for understanding and designing large-scale social computing systems	background
we found that the benefits of collaborative editing outweigh its risks	finding
For example each substantive edit from other users can increase the number of positive votes by 181 % for the questions and 119 % for the answers	finding
On the other hand	finding
each edit only decreases askers and answerers ' subsequent contributions by no more than 5 %	finding
By examining five years ' archival data of Stack Overflow	method
The Internet has the potential to accelerate scientific problem solving by engaging a global pool of contributors	background
Existing approaches focus on broadcasting problems to many independent solvers	background
A better understanding of such collaborative strategies can inform the design of tools to support distributed collaboration on complex problems	background
We contribute a simple taxonomy of collaborative acts derived from a Our results indicate a diversity of ways in which mathematicians are reaching a solution	finding
including by iteratively advancing a solution	finding
process-level examination of collaborations and a quantitative analysis relating collaborative acts to solution quality	mechanism
by examining a community for mathematical problem solving -- MathOverflow -- in which contributors communicate and collaborate to solve new mathematical 'micro-problems ' online	method
Crowdsourcing has become a popular and indispensable component of many problem-solving pipelines in the research literature	background
with crowd workers often treated as computational resources that can reliably solve problems that computers have trouble with	background
such as image labeling/classification	background
natural language processing	background
or document writing	background
Yet obviously crowd workers are human	background
and long sequences of the same monotonous tasks might intuitively reduce the amount of good quality work done by the workers	background
We find that micro-diversions can significantly improve worker retention rate while retaining the same work quality	finding
diversions containing small amounts of entertainment We call these small period of entertainment ``micro-diversions ''	mechanism
which we hypothesize	mechanism
We experimentally test micro-diversions on Amazon 's Mechanical Turk	method
a large paid-crowdsourcing platform	method
When health services involve long-term treatment over months or years	background
providers have the ability	background
not present in acute emergency care	background
to collaboratively reflect on clients ' changing health data and adjust interventions Our fieldwork in this context complements and provides contrasts to previous CSCW studies performed in time-critical hospital settings Current literature shows a bias toward standardized records and routines in the implementation of health information technology	background
a policy that may not be appropriate for long-term health services We discuss how the design of information systems should vary based on temporal factors	background
We define a temporal spectrum ranging from time-critical services that benefit from standardization to long-term services that require more flexibility	finding
We provide empirical evidence from fieldwork that we performed in organizations providing long-term behavioral and mental health services for children	method
inform a model of factors contributing to impression formation in this specific context	background
as well as experiments testing and providing design recommendations for improving members ' ability to interact and effectively learn about each other	background
I have conducted interviews with professionals in different domains who post and share their work online	method
Despite benefits and uses of social networking sites ( SNSs ) users are not always satisfied with their behaviors on the sites These desires for behavior change both provide insight into users ' perceptions of how SNSs impact their lives ( positively or negatively ) and can inform tools for helping users achieve desired behavior changes	background
Based on these results we provide insights both into how participants perceive different SNSs	background
as well as potential designs for behavior-change mechanisms to target SNS behaviors	background
While some participants want to reduce site use	finding
others want to improve their use or increase a range of behaviors These desired changes differ by SNS	finding
and for Twitter	finding
by participants ' levels of site use	finding
Participants also expect a range of benefits from these goals	finding
including increased time	finding
contact with others	finding
intrinsic benefits better security/privacy	finding
and improved self presentation	finding
We use a 604-participant online survey	method
Analysts synthesize complex	background
qualitative data to uncover themes and concepts	background
but the process is time-consuming	background
cognitively taxing and automated techniques show mixed success Crowdsourcing could help this process through on-demand harnessing of flexible and powerful human cognition	background
but incurs other challenges including limited attention and expertise Further	background
text data can be complex	background
high-dimensional and ill-structured	background
We demonstrate a classification-plus-context approach elicits the most accurate categories at the most useful level of abstraction	finding
To address these challenges we present an empirical study of a two-stage approach to A ) we draw on cognitive theory to assess how re-representing data can shorten and focus the data on salient dimensions ; and B ) introduce an iterative clustering approach that provides workers a global overview of data	mechanism
Hackathons are events where people who are not normally collocated converge for a few days to write code together Hackathons	background
it seems are everywhere	background
Our findings have implications for technology support that needs to be in place for hackathons and for understanding the role of brief interludes of collocation in loosely-coupled	background
geographically distributed work	background
suggest the way that hackathon-style collocation advances technical work varies across technical domain	finding
community structure and expertise of participants	finding
Building social ties	finding
in contrast seems relatively constant across hackathons Results from different hackathon team formation strategies suggest a tradeoff between advancing technical work and building social ties	finding
We present results from a multiple-case study that	method
A growing number of large collaborative idea generation platforms promise that by generating ideas together	background
people can create better ideas than any would have alone	background
But how might these platforms best leverage the number and diversity of contributors to help each contributor generate even better ideas ? Prior research suggests that seeing particularly creative or diverse ideas from others can inspire you	background
We see this work as a step toward building more effective online systems for supporting large scale collective ideation	background
Our validation study reveals that human raters agree with the estimates of dissimilarity derived from our idea map as much or more than they agree with each other	finding
People seeing the diverse sets of examples from our idea map generate more diverse ideas than those seeing randomly selected examples	finding
Our results also corroborate findings from prior research showing that people presented with creative examples generated more creative ideas than those who saw a set of random examples	finding
We contribute a new scalable crowd-powered method The method relies on similarity comparisons ( is idea A more similar to B or C ) generated by non-experts to create an abstract spatial idea map	mechanism
One main challenge in large creative online communities is helping their members find inspirational ideas from a large pool of ideas	background
A high-level approach to address this challenge is to create a synthesis of emerging solution space that can be used to provide participants with creative and diverse inspirational ideas of others	background
This feedback in turn helps the community identify diverse inspirational ideas that can prompt community members to generate more high-quality and diverse ideas	finding
We built IDEAHOUND a collaborative idea generation system that demonstrates an alternative `` organic '' human computation approach	mechanism
where community members ( rather than external crowds ) contribute feedback about ideas as a byproduct of an activity that naturally integrates into the ideation process	mechanism
Previous work has shown the promise of crowdsourcing analogical idea generation	background
where distributing the stages of analogical processing across many people can reduce fixation	background
identify inspirations from more diverse domains	background
and lead to more creative ideas	background
Our results show that crowds find the most useful inspirations when the problem domain is represented abstractly and constraints are represented more concretely	finding
This paper contributes a systematic crowdsourcing approach	mechanism
People are more creative at solving difficult design problems when they use relevant examples from outside of the problem 's domain as inspirations	background
We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own	finding
or drew inspiration from domains closer to or not sharing structural correspondence with the original problem	finding
In this paper	mechanism
we demonstrate an approach in which non-experts identify domains that have the potential to yield useful and non-obvious inspirations for solutions	mechanism
Eye tracking is a compelling tool for revealing people 's spatial-temporal distribution of visual attention Such an approach will allow designers to evaluate and refine their visual design without requiring the use of limited/expensive eye trackers	background
which demonstrated good accuracy when compared to a real eye tracker	finding
and showed that it accurately generated gaze heatmaps and trajectory maps	finding
we introduce a new approach that harnesses the crowd to understand allocation of visual attention In our approach	mechanism
crowdsourcing participants use mouse clicks to self-report the positions and trajectory for the following valuable eye tracking measures: first gaze	mechanism
last gaze and all gazes	mechanism
We validate our crowdsourcing approach with a user study	method
We then deployed our prototype	method
GazeCrowd in a crowdsourcing setting	method
Researchers and theorists have proposed that feelings of attachment to subgroups within a larger online community or site can increase users ' loyalty to the site	background
They have identified two types of attachment	background
with distinct causes and consequences With bond-based attachment	background
people feel connections to other group members	background
while with identity-based attachment they feel connections to the group as a whole	background
Communication with other people in a subgroup but not simple awareness of them increases attachment to the larger community	finding
the experiments show that bond- and identity-based attachment have different causes	finding
But the experiments show no evidence that bond and identity attachment have different consequences	finding
We consider both theoretical and methodological reasons why the consequences of bond-based and identity-based attachment are so similar	finding
By varying how the communication is structured	mechanism
between dyads or with all group members simultaneously	mechanism
In two experiments we show	method
People spend an enormous amount of time searching for complex information online ; for example	background
consumers researching new purchases or patients learning about their conditions for others with similar interests	background
Through a controlled experiment we show that having access to others ' schemas while foraging for information helps new users to induce more useful	finding
prototypical and better-structured schemas than gathering information alone	finding
In this paper we introduce a novel approach for integrating the schemas individuals develop as they gather information online and surfacing them for others with similar interests	mechanism
Online collaboration tools enable developers of interactive systems to quickly reach potential users for usability testing Online needfinding may help designers create products and services that can target a more diverse user population	background
We found that video can sufficiently capture nuanced reactions to preliminary concept storyboards	finding
but that feedback providers need guidance and structure	finding
The case study demonstrates that combining online crowdsourcing with a video survey tool provides a simple and cost-efficient way to collect early-stage feedback	finding
To explore this	method
we conducted a feasibility study to compare face-to-face methods with online needfinding sessions conduct a case study with a professional design team	method
The team conducted needfinding activities with local participants	method
as well as a cost-equivalent number of online participants	method
Some significant differences were discovered between the two collections	finding
namely in the clients used to post them	finding
their conversational aspects	finding
the sentiment vocabulary present in them	finding
and the days of the week they were posted	finding
However in other dimensions for which analysis was possible	finding
no substantial differences were found	finding
Finally we discuss some ramifications of this work for understanding Twitter usage and management of one 's privacy	finding
This paper describes an empirical study of 1	method
6M deleted tweets collected over a continuous one-week period from a set of 292K Twitter users	method
We show that we can use this to characterize distinct classes of articles	finding
We also find that social media reactions can help predict future visitation patterns early and accurately	finding
We show that it is possible to model accurately the overall traffic articles will ultimately receive by observing the first ten to twenty minutes of social media reactions	finding
Achieving the same prediction accuracy with visits alone would require to wait for three hours of data	finding
We also describe significant improvements on the accuracy of the early prediction of shelf-life for news stories	finding
We describe the interplay between website visitation patterns and social media reactions to news content hybrid observation method	mechanism
We validate our methods using qualitative analysis as well as quantitative analysis on data from a large international news network	method
for a set of articles generating more than 3	method
000 000 visits and 200	method
000 social media reactions	method
A challenge for many online production communities is to direct their members to accomplish tasks that are important to the group	background
even when these tasks may not match individual members ' interests	background
Finally we discuss design and managerial implications based on our findings	background
Results demonstrate that 1 ) publicizing important group goals via COTW can have a strong motivating influence on editors who have voluntarily identified themselves as group members compared to those who have not self-identified ; 2 ) the effects of goals spill over to non-goal related tasks ; and 3 ) editors exposed to group role models in COTW are more likely to perform similarly to the models on group-relevant citizenship behaviors	finding
We tested our hypotheses in the context of Wikipedia 's Collaborations of the Week ( COTW )	method
a group goal setting mechanism and a social event within Wikiprojects	method
In crowd-collaborative innovation platforms	background
other contributors ' ideas can serve as sources of inspiration for creative ideas	background
We discuss implications for research and development of crowd-collaborative innovation platforms	background
Surprisingly we find that innovators who cite conceptually near sources of inspiration achieve a higher success rate than those who prefer far sources	finding
We predict the success rate of 2	method
344 ideas for 12 different design challenges in a collaborative Web-based innovation platform based on their cited sources ' conceptual distance from the target domain ( measured using probabilistic topic modeling of the ideas )	method
thus improving their ability to facilitate one-time or spontaneous exchanges of information	background
show how leveraging multiple contexts improves our ability to detect and form relevant groupings Through two prototypes	finding
we demonstrate how DIDJA enhances existing user experiences	finding
and show how developers can use our toolkit to easily facilitate frictionless collaborations between users and their environment We then perform an extended experiment and show how DIDJA is able to accurately form groups under realistic conditions	finding
In our approach	mechanism
devices share context with each other	mechanism
and form groups when these readings are found to be similar to one another Through a formative study	mechanism
We then present DIDJA	mechanism
a robust software toolkit that automatically collects and analyzes contextual information in order to find and form groups	mechanism
A significant challenge for crowdsourcing has been increasing worker engagement and output quality The findings of this paper provide strategies for harnessing the crowd to perform complex tasks	background
as well as insight into crowd workers ' motivation	background
we show that 1 ) using these strategies together increased workers ' engagement and the quality of their work ; 2 ) a social strategy was most effective for increasing engagement ; 3 ) a learning strategy was most effective in improving quality	finding
Through three experiments	method
We designed and implemented a prototype and our early experiences with it indicate the promise of offering quick video messaging at home and the challenges of a no-touch interface	finding
A physical artifact dedicated to remote family members makes it easier to chat with them over video	mechanism
HomeProxy combines a form factor designed for the home environment with a `` no-touch '' user experience and an interface that quickly transitions between recorded and live video communication	mechanism
Sharing scientific data	background
software and instruments is becoming increasingly common as science moves toward large-scale	background
distributed collaborations	background
Sharing these resources requires extra work to make them generally useful	background
Although we know much about the extra work associated with sharing data	background
Our results have important implications for future empirical studies as well as funding policy	background
Our findings indicate that they conduct a rich set of extra work around community management	finding
code maintenance education and training	finding
developer-user interaction and foreseeing user needs	finding
We identify several conditions under which they are likely to do this work	finding
as well as design principles that can facilitate it	finding
This paper presents a qualitative	method
interview-based study of the extra work that developers and end users of scientific software undertake	method
Crowd feedback systems offer designers an emerging approach for improving their designs	background
Results showed that the crowd feedback system prompted deep and cosmetic changes and led to improved designs	finding
the crowd recognized the design improvements	finding
and structured workflows generated more interpretative	finding
diverse and critical feedback than free-form prompts	finding
Users in an introductory visual design course created initial designs satisfying a design brief and received crowd feedback on the designs	method
Users revised the designs and the system was used to generate feedback again	method
This format enabled us to detect the changes between the initial and revised designs and how the feedback related to those changes	method
Further we analyzed the value of crowd feedback by comparing it with expert evaluation and feedback generated via free-form prompts	method
In a variety of peer production settings	background
from Wikipedia to open source software development to crowdsourcing	background
individuals may encounter	background
edit or review the work of unknown others	background
Typically this is done without much context to the person 's past behavior or performance	background
This work provides insight into the impact of activity history design factors on psychological and behavioral outcomes that can be of use in other related settings	background
Surprisingly negative work history did not lead to negative outcomes	finding
but in contrast	finding
a positive work history led to positive initial impressions that persisted in the face of contrary information	finding
we conducted an online experiment on Mechanical Turk varying the content	method
quality and presentation of information about another Turker 's work history	method
When personalities clash	background
teams operate less effectively Personality differences affect face-to-face collaboration and may lower trust in virtual teams This work demonstrates a simple personality matching strategy for forming more effective teams in crowdsourcing contexts	background
Results show that balancing for personality leads to significantly better performance on a collaborative task Balanced teams exhibited less conflict and their members reported higher levels of satisfaction and acceptance	finding
Using the DISC personality test	method
we composed 14 five-person teams ( N=70 ) with either a harmonious coverage of personalities ( balanced ) or a surplus of leader-type personalities ( imbalanced	method
We close by identifying several ways in which crowd labor platform operators and/or individual task requestors could improve the accessibility of this increasingly important form of employment	background
Our findings establish that people with a variety of disabilities currently participate in the crowd labor marketplace	finding
despite challenges such as crowdsourcing workflow designs that inadvertently prohibit participation by	finding
and may negatively affect the worker reputations of	finding
people with disabilities Despite such challenges	finding
we find that crowdwork potentially offers different opportunities for people with disabilities relative to the normative office environment	finding
such as job flexibility and lack of a need to rely on public transit	finding
via in-depth open-ended interviews of 17 people ( disabled crowdworkers and job coaches for people with disabilities ) and a survey of 631 adults with disabilities	method
Increasingly the advice people receive on the Internet is socially transparent in the sense that it displays contextual information about the advice-givers or their actions CSCW research usually emphasizes how to increase information sharing ; this work suggests when shared information may be inappropriate We suggest ways to counter activity transparency 's potential downsides	background
We found that the presence of a web history increased the likelihood of following a financial advisor 's advice and reduced participant earnings ( Exp	finding
1 ) especially when the web history implied greater task focus ( Exp	finding
2 3 )	finding
We report three experiments	method
Social networking sites ( SNSs ) offer users a platform to build and maintain social connections	background
Understanding when people feel comfortable sharing information about themselves on SNSs is critical to a good user experience	background
because self-disclosure helps maintain friendships and increase relationship closeness	background
Results show that women self-disclose more than men	finding
People with a stronger desire to manage impressions self-disclose less	finding
Network size is negatively associated with self-disclosure	finding
while tie strength and network density are positively associated	finding
Features include emotional valence	mechanism
social distance between the poster and people mentioned in the post	mechanism
the language similarity between the post and the community and post topic	mechanism
we applied it to de-identified	method
aggregated status updates from Facebook users	method
Participatory sensing systems ( PSS ) require frequent injection of information that has a short shelf-life The use of crowds to gather information for PSS is therefore particularly challenging	background
Prior research has shown that request for help in crowdsourced system is an effective mechanism to increase contributions	background
Thus crowdsource system designers should consider imposing quid-pro-quo type policies for PSS that concentrate on fewer users	background
but makes them more productive	background
Our results confirmed that quid-pro-quo led to more contribution	finding
but at a cost of faster departure from the study	finding
When a participant was simply requested to contribute	finding
but could still access community-generated data if they ignored a request	finding
was largely ineffective and was statistically similar to the control condition where no request for contribution occurred	finding
During a large-scale experimental study within a publicly deployed	method
crowdsourced transit information system	method
we analyzed metrics associated with frequency of contribution and commitment to long-term use over a 10-month period	method
Expert feedback is valuable but hard to obtain for many designers	background
We found that rubrics helped novice workers provide feedback that was rated nearly as valuable as expert feedback showed that student designers found feedback most helpful when it was emotionally positive and specific	finding
and that a rubric increased the occurrence of these characteristics in feedback The analysis also found that expertise correlated with longer critiques	finding
but not the other favorable characteristics	finding
indicates that experts may instead have produced value by providing clearer justifications	finding
To evaluate this	method
we conducted an experiment with a 2x2 factorial design	method
Student designers received feedback on a visual design from both experts and novices	method
who produced feedback using either an expert rubric or no rubric A follow-up analysis on writing style An informal evaluation	method
A novel method of using task fingerprinting The technique focuses on the way workers work rather than the products they produce	mechanism
The technique captures behavioral traces from online crowd workers and uses them to build predictive models of task performance	mechanism
The effectiveness of the approach is evaluated across three contexts including classification	method
generation and comprehension tasks	method
Fast Fourier transform algorithms on large data sets achieve poor performance on various platforms because of the inefficient strided memory access patterns	background
we demonstrate DRAM-optimized accelerator designs over a large tradeoff space given various problem ( single/double precision 1D	finding
2D and 3D FFTs ) and hardware platform ( off-chip DRAM	finding
3D-stacked DRAM ASIC	finding
FPGA etc	finding
) parameters	finding
We show that Spiral generated pareto optimal designs can achieve close to theoretical peak performance of the targeted platform offering 6x and 6	finding
5x system performance and power efficiency improvements respectively over conventional row-column FFT algorithms	finding
In this paper 1D	mechanism
2D and 3D FFTs targeting a generic machine model with a two-level memory hierarchy requiring block data transfers	mechanism
and derive using custom block data layouts These algorithms need to be carefully mapped to the targeted platform 's architecture	mechanism
particularly the memory subsystem	mechanism
to fully utilize performance and energy efficiency potentials Using the Kronecker product formalism	mechanism
we integrate our optimizations into Spiral framework	mechanism
and evaluate a family of DRAM-optimized FFT algorithms and their hardware implementation design space via automated techniques	method
In our evaluations	method
Languages for music audio processing typically offer a large assortment of unit generators There is great duplication among different language implementations	background
as each language must implement many of the same ( or nearly the same ) unit generators	background
We suggest that these techniques might eliminate most of the effort of building unit generator libraries and could help with the implementation of embedded audio systems where unit generators are needed but a full embedded Csound engine is not required	background
Using Aura as an example	mechanism
we modified Csound to allow efficient	mechanism
dynamic allocation of individual unit generators without using the Csound compiler or writing Csound instruments	mechanism
We then extended Aura using automatic code generation so that Csound unit generators can be accessed in the normal way from within Aura	mechanism
In this scheme	mechanism
Csound details are completely hidden from Aura users	mechanism
The results suggest that 1 ) the online disclosure of certain personal traits can influence the hiring decisions of U	background
S	background
firms and 2 ) the likelihood of hiring discrimination via online searches varies across employers	background
The findings also highlight the surprisingly lasting behavioral influence of traditional	background
offline networks in processes and scenarios where online interactions are becoming increasingly common	background
We find evidence of employers searching online for the candidates	finding
we find no difference in callback rates for the gay candidate compared to the straight candidate	finding
but a 13 % lower callback rate for the Muslim candidate compared to the Christian candidate	finding
While the difference is not significant at the national level	finding
it exhibits significant and robust heterogeneity in bias at the local level	finding
compatible with existing theories of discrimination	finding
In particular employers in Republican areas exhibit significant bias both against the Muslim candidate	finding
and in favor of the Christian candidate This bias is significantly larger than the bias in Democratic areas	finding
We create profiles for job candidates on popular social networks	method
manipulating information protected under U	method
S	method
laws and submit job applications on their behalf to over 4	method
000 employers After comparing interview invitations for a Muslim versus a Christian candidate	method
and a gay versus a straight candidate	method
The results are robust to using state- and county-level data	method
to controlling for firm	method
job and geographical characteristics	method
and to several model specifications	method
Motivated by a radically new peer review system that the National Science Foundation recently experimented with	background
An ( m ; k ) -selection mechanism asks each PI to review m proposals	mechanism
and uses these reviews to select ( at most ) k proposals	mechanism
We are interested in impartial mechanisms	mechanism
which guarantee that the ratings given by a PI to others ' proposals do not affect the likelihood of the PI 's own proposal being selected We design an impartial mechanism that selects a k-subset of proposals that is nearly as highly rated as the one selected by the non-impartial ( abstract version of ) the NSF pilot mechanism	mechanism
even when the latter mechanism has the `` unfair '' advantage of eliciting honest reviews	mechanism
The fairness notion of maximin share ( MMS ) guarantee underlies a deployed algorithm for allocating indivisible goods under additive valuations	background
Previous work has shown that such an MMS allocation may not exist	background
but the counterexample requires a number of goods that is exponential in the number of players ;	background
we give a new construction that uses only a linear number of goods	mechanism
On the positive side	mechanism
we formalize the intuition that these counterexamples are very delicate by designing an algorithm when valuations are drawn at random	mechanism
A paradigmatic problem in social choice theory deals with the aggregation of subjective preferences of individuals represented as rankings of alternatives into a social ranking	background
We show that ignoring uncertainty altogether can lead to suboptimal outcomes	finding
Under the classic objective of minimizing the ( expected ) sum of Kendall tau distances between the input rankings and the output ranking	mechanism
we establish that preference elicitation is surprisingly straightforward and near-optimal solutions can be obtained in polynomial time	mechanism
both in theory and using real data	method
Simultaneously reverse engineering a collection of condition-specific gene networks from gene expression microarray data to uncover dynamic mechanisms is a key challenge in systems biology	background
We show the quantitative advantages of our approach reveals interesting results	finding
some of which are confirmed by previously validated results	finding
In this work	mechanism
we develop a more robust framework Just like microarray measurements across conditions must undergo proper normalization on their magnitudes before entering subsequent analysis	mechanism
we argue that networks across conditions also need to be normalized on their density when they are constructed	mechanism
and we provide an algorithm that allows such normalization to be facilitated while estimating the networks	mechanism
on synthetic and real data Our analysis of a hematopoietic stem cell dataset	method
We show that envy-free allocations of sellable goods are significantly more efficient than their unsellable counterparts	finding
Our novel setting includes an option to sell each good for a fraction of the minimum value any player has for the good	mechanism
we reason about the price of envy-freeness of allocations of sellable goods -- the ratio between the maximum social welfare and the social welfare of the best envy-free allocation	mechanism
Some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives	background
results call attention to situations where approval voting is suboptimal	finding
by proposing a probabilistic framework of noisy voting	mechanism
and asking whether approval voting yields an alternative that is most likely to be the best alternative	mechanism
given k-approval votes	mechanism
While the answer is generally positive	method
our theoretical and empirical	method
Personal photos are enjoying explosive growth with the popularity of photo-taking devices and social media	background
The vast amount of online photos largely exhibit users ' interests	background
emotion and opinions	background
demonstrate the effectiveness of our model	finding
We propose a User Image Latent Space Model User interests are modeled as latent factors and each user is assumed to have a distribution over them	mechanism
By inferring the latent factors and users ' distributions	mechanism
we can discover what the users are interested in We model image contents with a four-level hierarchical structure where the layers correspond to themes	mechanism
semantic regions visual words and pixels respectively	mechanism
Users ' latent interests are embedded in the theme layer	mechanism
Given image contents	mechanism
users ' interests can be discovered by doing posterior inference	mechanism
We use variational inference to approximate the posteriors of latent variables and learn model parameters	mechanism
Experiments on 180K Flickr photos	method
We introduce the simultaneous model We show that this model enables the computation of divisions that satisfy proportionality -- a popular fairness notion -- using a protocol that circumvents a standard lower bound via parallel information elicitation	mechanism
Cake divisions satisfying another prominent fairness notion	mechanism
envy-freeness are impossible to compute in the simultaneous model	mechanism
but admit arbitrarily good approximations	mechanism
Motivated by applications to crowdsourcing	background
We show that there is such a voting rule	mechanism
which we call the modal ranking rule	mechanism
Moreover we establish that the modal ranking rule is the unique rule with the preceding robustness property within a large family of voting rules	mechanism
which includes a slew of well-studied rules	mechanism
Classic social choice theory assumes that votes are independent ( but possibly conditioned on an underlying objective ground truth )	background
We establish a general framework -- based on random utility theory -- on a social network with arbitrarily many alternatives ( in contrast to previous work	mechanism
which is restricted to two alternatives )	mechanism
We identify a family of voting rules which	mechanism
without knowledge of the social network structure	mechanism
are guaranteed with high probability in large networks	mechanism
with respect to a wide range of models of correlation among input votes	mechanism
Limited lookahead has been studied for decades in perfect-information games	background
The limited-lookahead player often obtains the value of the game if she knows the expected values of nodes in the game tree for some equilibrium	finding
but we prove this is not sufficient in general	finding
This uncovers a lookahead pathology	finding
We characterize the hardness of finding a Nash equilibrium or an optimal commitment strategy for either player	mechanism
showing that in some of these variations the problem can be solved in polynomial time while in others it is PPAD-hard or NP-hard	mechanism
We proceed to design algorithms for when the opponent breaks ties 1 ) favorably	mechanism
2 ) according to a fixed rule	mechanism
or 3 ) adversarially	mechanism
The impact of limited lookahead is then investigated experimentally Finally	method
we study the impact of noise in those estimates and different lookahead depths	method
We present an efficient algorithm When combined with a result of Chillingworth	mechanism
our algorithm is applicable to convex simplicial complexes embedded in R3 The running time of our algorithm is nearly-linear in the size of the complex and is logarithmic on its numerical properties	mechanism
Our algorithm is based on projection operators and combinatorial steps for transferring between them The former relies on decomposing flows into circulations and potential flows using fast solvers for graph Laplacians	mechanism
and the latter relates Gaussian elimination to topological properties of simplicial complexes	mechanism
Specifically we show that each protocol in the class of generalized cut and choose ( GCC ) protocols which includes the most important discrete cake cutting protocols is guaranteed to have approximate subgame perfect Nash equilibria	finding
or even exact equilibria if the protocol 's tie-breaking rule is flexible	finding
We further observe that the ( approximate ) equilibria of proportional protocols which guarantee each of the n agents a 1/n-fraction of the cake must be ( approximately ) proportional	finding
thereby answering the above question in the positive ( at least for one common notion of fairness )	finding
we adopt a novel algorithmic approach	mechanism
proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model	mechanism
We study the paradigmatic fair division problem of fairly allocating a divisible good among agents with heterogeneous preferences	method
commonly known as cake cutting	method
This disclosure relates to a three-dimensional ( 3D ) integrated circuit ( 3DIC ) memory chip including computational logic-in-memory ( LiM ) Related memory systems and methods are also disclosed In one embodiment	mechanism
the 3DIC memory chip includes at least one memory layer that provides a primary memory configured to store data	mechanism
The 3DIC memory chip also includes a computational LiM layer	mechanism
The computational LiM layer is a type of memory layer having application-specific computational logic integrated into local memory while externally appearing as regular memory The computational LiM layer and the primary memory are interconnected through through-silica vias ( TSVs )	mechanism
In this manner	mechanism
the computational LiM layer may load data from the primary memory with the 3DIC memory chip without having to access an external bus coupling the 3DIC memory chip to a central processing unit ( CPU ) or other processors to computationally process the data and generate a computational result	mechanism
An adversary who has obtained the cryptographic hash of a user 's password can mount an offline attack to crack the password by comparing this hash value with the cryptographic hashes of likely password guesses	background
This offline attacker is limited only by the resources he is willing to invest to crack the password	background
Key-stretching techniques like hash iteration and memory hard functions have been proposed to mitigate the threat of offline attacks by making each password guess more expensive for the adversary to verify	background
Our analysis shows that CASH can significantly reduce ( up to 50 % ) the fraction of password cracked by a rational offline adversary	finding
We introduce a novel Stackelberg game model In the game the defender first commits to a key-stretching mechanism	mechanism
and the offline attacker responds in a manner that optimizes his utility ( expected reward minus expected guessing costs )	mechanism
We then introduce Cost Asymmetric Secure Hash ( CASH )	mechanism
a randomized key-stretching mechanism without increasing amortized authentication costs for the legitimate authentication server CASH is motivated by the observation that the legitimate authentication server will typically run the authentication procedure to verify a correct password	mechanism
while an offline adversary will typically use incorrect password guesses	mechanism
By using randomization we can ensure that the amortized cost of running CASH to verify a correct password guess is significantly smaller than the cost of rejecting an incorrect password Using our Stackelberg game framework we can quantify the quality of the underlying CASH running time distribution in terms of the fraction of passwords that a rational offline adversary would crack	mechanism
We provide an efficient algorithm to compute high quality CASH distributions for the defender	mechanism
Finally we analyze CASH using empirical data from two large scale password frequency datasets	method
Com2 spots intuitive patterns	finding
that is temporal communities ( comet communities )	finding
We report our findings	finding
which include large star-like patterns	finding
near-bipartite-cores as well as tiny groups ( 5 users )	finding
calling each other hundreds of times within a few days	finding
We propose Com2	mechanism
a novel and fast	mechanism
incremental tensor analysis approach	mechanism
The method is ( a ) scalable	mechanism
being linear on the input size ( b ) general	mechanism
( c ) needs no user-defined parameters and ( d ) effective	mechanism
returning results that agree with intuition	mechanism
We apply our method on real datasets	method
including a phone-call network and a computer-traffic network The phone call network consists of 4 million mobile users	method
with 51 million edges ( phonecalls )	method
over 14 days	method
Many graph mining and analysis services have been deployed on the cloud	background
which can alleviate users from the burden of implementing and maintaining graph algorithms	background
we show how to apply our methods to perform analytics on encrypted graphs	finding
demonstrate the correctness and feasibility of our methods	finding
we propose CryptGraph	mechanism
which runs graph analytics on encrypted graph In CryptGraph	mechanism
users encrypt their graphs before uploading them to the cloud	mechanism
The cloud runs graph analysis on the encrypted graphs and obtains results which are also in encrypted form that the cloud can not decipher	mechanism
During the process of computing	mechanism
the encrypted graphs are never decrypted on the cloud side	mechanism
The encrypted results are sent back to users and users perform the decryption to obtain the plaintext results	mechanism
In this process	mechanism
users ' graphs and the analytics results are both encrypted and the cloud knows neither of them Thereby	mechanism
users ' privacy can be strongly protected	mechanism
Meanwhile with the help of homomorphic encryption	mechanism
the results analyzed from the encrypted graphs are guaranteed to be correct	mechanism
we propose hard computation outsourcing	mechanism
Using two graph algorithms as examples	method
Experiments on two datasets	method
A key challenge in solving extensive-form games is dealing with large	background
or even infinite	background
action spaces	background
In games of imperfect information	background
the leading approach is to find a Nash equilibrium in a smaller abstract version of the game that includes only a few actions at each decision point	background
and then map the solution back to the original game	background
show it can outperform fixed abstractions at every stage of the run : early on it improves as quickly as equilibrium finding in coarse abstractions	finding
and later it converges to a better solution than does equilibrium finding in fine-grained abstractions	finding
We introduce a method that combines abstraction with equilibrium finding by enabling actions to be added to the abstraction at run time	mechanism
This allows an agent to begin learning with a coarse abstraction	mechanism
and then to strategically insert actions at points that the strategy computed in the current abstraction deems important The algorithm can quickly add actions to the abstraction while provably not having to restart the equilibrium finding	mechanism
It enables anytime convergence to a Nash equilibrium of the full game even in infinite games	mechanism
Experiments	method
The success of Amazon Mechanical Turk ( MTurk ) as an online research platform has come at a price : MTurk exhibits slowing rates of population replenishment	background
and growing participants non-naivety	background
We found that both platforms participants were more naive and less dishonest compared to MTurk	finding
CF showed the best response rate	finding
but CF participants failed more attention-check questions and did not reproduce known effects replicated on ProA and MTurk Moreover	finding
ProA participants produced data quality that was higher than CFs and comparable to MTurks	finding
We examined two such platforms	method
CrowdFlower ( CF ) and Prolific Academic ( ProA )	method
Anecdotal evidence and scholarly research have shown that a significant portion of Internet users experience regrets over their online disclosures We discuss limitations of the current nudge designs and future directions for improvement	background
Our system logs	finding
results from exit surveys	finding
and interviews suggest that privacy nudges could be a promising way to prevent unintended disclosure	finding
we employed lessons from behavioral decision research and research on soft paternalism We developed three such privacy nudges on Facebook	mechanism
The first nudge provides visual cues about the audience for a post	mechanism
The second nudge introduces time delays before a post is published	mechanism
The third nudge gives users feedback about their posts	mechanism
We tested the nudges in a three-week exploratory field trial with 21 Facebook users	method
and conducted 13 follow-up interviews	method
Finally we illustrate our results	finding
First we find necessary and sufficient conditions for an attacker to create a dynamically undetectable sensor attack and relate these conditions to properties of the system dynamics eigenvectors	mechanism
Next we provide an index that gives the minimum number of sensors that must be attacked in order for an attack to be undetectable	mechanism
with a numerical example on the Quadruple Tank Process	method
Tree structured graphical models are powerful at expressing long range or hierarchical dependency among many variables	background
and have been widely applied in different areas of computer science and statistics	background
The usefulness of the proposed methods are illustrated	finding
In this paper	mechanism
we propose new nonparametric methods based on reproducing kernel Hilbert space embeddings of distributions	mechanism
by thorough numerical results	method
In this paper we present HiveMind	mechanism
a system of methods simply by adjusting the value of one variable	mechanism
Active learning has shown to reduce the number of exper- iments needed to obtain high-confidence drug-target predictions	background
How- ever in order to actually save experiments using active learning	background
it is crucial to have a method to evaluate the quality of the current pre- diction and decide when to stop the experimentation process	background
that applying the stopping criteria can result in upto 40 % savings of the total experiments for highly accurate predictions	finding
We compute active learning traces on simulated drug-target matrices in order to learn regression model By analyzing the perfor- mance of the regression model on simulated data	mechanism
we design stopping criteria for previously unseen experimental matrices	mechanism
We demonstrate on four previously characterized drug effect data sets	method
These results show that this system could be a valuable addition to vehicle anomaly detection and safety systems	background
The experiment shows that the system is capable of predicting the vehicle speed and gear position with near-perfect accuracy over 99 %	finding
This paper presents a machine learning system from the sound it makes Therefore	mechanism
we investigate predicting the state of a vehicle using audio features in a classification task We improve the classification results using correlation matrices	mechanism
calculated from signals correlating with the audio	mechanism
In an experiment	method
the sound of a moving vehicle is classified into discretized speed intervals and gear positions	method
Training large machine learning ( ML ) models with many variables or parameters can take a long time if one employs sequential procedures even with stochastic updates	background
We provide theoretical guarantees for our scheduler	finding
and demonstrate its efficacy versus static block structures	finding
We propose and showcase a general-purpose scheduler	mechanism
STRADS  which harnesses the aforementioned opportunities in a systematic way	mechanism
by exploring the dynamic block structures and workloads therein present during ML program execution	method
which offers new opportunities for improving convergence	method
correctness and load balancing in distributed ML	method
on Lasso and Matrix Factorization	method
Interface-confinement is a common mechanism that secures untrusted code by executing it inside a sandbox	background
The sandbox limits ( confines ) the code 's interaction with key system resources to a restricted set of interfaces This practice is seen in web browsers	background
hypervisors and other security-critical systems	background
and prove the soundness of System M relative to the model	finding
Motivated by these systems	mechanism
we present a program logic	mechanism
called System M In addition to using computation types to specify effects of computations	mechanism
System M includes a novel invariant type The interpretation of invariant type includes terms whose effects satisfy an invariant We construct a step-indexed model built over traces System M is the first program logic that allows proofs of safety for programs that execute adversary-supplied code without forcing the adversarial code to be available for deep static analysis System M can be used to model and verify protocols as well as system designs	mechanism
We demonstrate the reasoning principles of System M by verifying the state integrity property of the design of Memoir	method
a previously proposed trusted computing system	method
Combined our results confer strong correctness guarantees for communicating systems	finding
To this end	mechanism
we develop a logically motivated theory of parametric polymorphism	mechanism
reminiscent of the Girard-Reynolds polymorphic -calculus	mechanism
but casted in the setting of concurrent processes	mechanism
In our theory	mechanism
polymorphism accounts for the exchange of abstract communication protocols and dynamic instantiation of heterogeneous interfaces	mechanism
as opposed to the exchange of data types and dynamic instantiation of individual message types	mechanism
Our polymorphic session-typed process language satisfies strong forms of type preservation and global progress	mechanism
is strongly normalizing	mechanism
and enjoys a relational parametricity principle In particular	mechanism
parametricity is key to derive non-trivial results about internal protocol independence	mechanism
a concurrent analogous of representation independence	mechanism
and non-interference properties of modular	mechanism
distributed systems	mechanism
A dataset has been classified by some unknown classifier into two types of points	background
What were the most important factors in determining the classification outcome ?	background
In this work	mechanism
we employ an axiomatic approach We show that our influence measure takes on an intuitive form when the unknown classifier is linear	mechanism
Finally we employ our influence measure in order to analyze the effects of user profiling on Google 's online display advertising	method
Namely if the sample size is above the threshold	finding
then $ l_1/l_2 $ -regularized Lasso correctly recovers the support union ; and if the sample size is below the threshold	finding
$ l_1/l_2 $ -regularized Lasso fails to recover the support union	finding
In particular the threshold precisely captures the impact of the sparsity of regression vectors and the statistical properties of the design matrices on sample complexity Therefore	finding
the threshold function also captures the advantages of joint support union recovery using multi-task Lasso over individual support recovery using single-task Lasso	finding
We characterize sufficient and necessary conditions on sample complexity \emph { as a sharp threshold } to guarantee successful recovery of the support union	mechanism
In this paper	method
we investigate a multivariate multi-response ( MVMR ) linear regression problem	method
which contains multiple linear regression models with differently distributed design matrices	method
and different regression and output vectors	method
Upsampling of a multi-dimensional data-set is an operation with wide application in image processing and quantum mechanical calculations using density functional theory	background
For small up sampling factors as seen in the quantum chemistry code ONETEP	background
a time-shift based implementation that shifts samples by a fraction of the original grid spacing to fill in the intermediate values using a frequency domain Fourier property can be a good choice Readily available highly optimized multidimensional FFT implementations are leveraged at the expense of extra passes through the entire working set	background
We demonstrate speed-ups in isolation averaging 3x and within ONETEP of up to 15 %	finding
In this paper we present Since ONETEP handles threading	mechanism
we address the memory hierarchy and SIMD vectorization	mechanism
and focus on problem dimensions relevant for ONETEP	mechanism
We present a formalization of this operation within the SPIRAL framework and demonstrate auto-generated and auto-tuned interpolation libraries	mechanism
We compare the performance of our generated code against the previous best implementations using highly optimized FFT libraries ( FFTW and MKL	method
When asked to mentally simulate coin tosses	background
people generate sequences that differ systematically from those generated by fair coins	background
It has been rarely noted that this divergence is apparent already in the very 1st mental toss This bias has far-reaching implications extending well beyond the context of randomness cognition ; in particular	background
to binary surveys ( e	background
g	background
accept vs	background
reject ) and tests ( e	background
g	background
TrueFalse )	background
In binary choice	background
there is an advantage to what presents first	background
reveals that about 80 % of respondents start their sequence with Heads	finding
We attributed this to the linguistic convention describing coin toss outcomes as Heads or Tails	finding
not vice versa	finding
found the first-toss bias reversible	finding
in terms of a novel response bias	mechanism
which we call reachability	mechanism
It is more general than the 1st-toss bias	mechanism
and it reflects the relative ease of reaching 1 option compared to its alternative in any binary choice context	mechanism
When faced with a choice between 2 options ( e	mechanism
g	mechanism
Heads and Tails	mechanism
when tossing mental coins )	mechanism
whichever of the 2 is presented first by the choice architecture ( hence	mechanism
is more reachable ) will be favored	mechanism
Analysis of several existing data sets However	method
our subsequent experiments under minor changes in the experimental setup	method
such as mentioning Tails before Heads in the instructions	method
This result shows that our distributed Kalman filter can track with bounded MSE any arbitrary linear dynamics	finding
of a distributed Kalman filter that we have previously proposed	mechanism
Given information about medical drugs and their properties	background
how can we automatically discover that Aspirin has blood-thinning properties	background
and thus prevents Expressed in more general terms	background
if we have a large in- formation network that integrates data from heterogeneous data sources	background
how can we extract semantic information that provides a better understanding of the in- tegrated data and also helps us to identify missing links ?	background
We demonstrate the effectiveness and scalability of the proposed method	finding
The discovered concepts provide semantic information as well as an abstract view on the integrated data and thus improve the understanding of complex systems	mechanism
Our proposed method has the following desirable properties : ( a ) it is parameter-free and therefore requires no user-defined parameters ( b ) it is fault-tolerant	mechanism
allowing for the detection of missing links and ( c ) it is scalable	mechanism
being linear on the input size	mechanism
on real publicly available graphs	method
Multilinear analysis is pervasive in a wide variety of fields	background
ranging from Signal Processing to Chemometrics	background
and from Machine Vision to Data Mining	background
Determining the quality of a given tensor decomposition is a task of utmost importance that spans all fields of application of tensors	background
This task by itself is hard in its nature	background
since even determining the rank of a tensor is an NP-hard problem	background
Fortunately there exist heuristics in the literature that can be effectively used for this task ; one of these heuristics is the so-called Core Consistency Diagnostic ( CORCONDIA ) which is very intuitive and simple	background
However simple computation of this diagnostic proves to be a very daunting task even for data of medium scale	background
let alone big tensor data	background
In this work we derive a fast and exact algorithm for CORCONDIA which exploits data sparsity and scales very well as the tensor size increases	mechanism
As Machine Learning ( ML ) applications embrace greater data size and model complexity	background
practitioners turn to distributed clusters to satisfy the increased computational and memory demands Effective use of clusters for ML programs requires considerable expertise in writing distributed code	background
but existing highly-abstracted frameworks like Hadoop that pose low barriers to distributed-programming have not	background
in practice matched the performance seen in highly specialized and advanced ML implementations	background
The recent Parameter Server ( PS ) paradigm is a middle ground between these extremes	background
allowing easy conversion of single-machine parallel ML programs into distributed ones	background
while maintaining high throughput through relaxed `` consistency models '' that allow asynchronous ( and	background
hence inconsistent ) parameter reads	background
We then use the gleaned insights to improve a consistency model using an `` eager '' PS communication mechanism	mechanism
and implement it as a new PS system	mechanism
Inspired by this challenge	method
we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models	method
Principal Component Analysis ( PCA ) has wide applications in machine learning	background
text mining and computer vision	background
Classical PCA based on a Gaussian noise model is fragile to noise of large magnitude	background
Experimental results demonstrate the robustness of Cauchy PCA to various noise patterns	finding
In this paper	mechanism
we propose Cauchy Principal Component Analysis ( Cauchy PCA )	mechanism
a very simple yet effective PCA method which is robust to various types of noise	mechanism
We utilize Cauchy distribution to model noise and derive Cauchy PCA under the maximum likelihood estimation ( MLE ) framework with low rank constraint	mechanism
Our method can robustly estimate the low rank matrix regardless of whether noise is large or small	mechanism
dense or sparse	mechanism
We analyze the robustness of Cauchy PCA from a robust statistics view and present an efficient singular value projection optimization method	mechanism
on both simulated data and real applications	method
People with disabilities can be reluctant to friendsource help from their own friends for fear of appearing dependent or annoying	background
Our social microvolunteering approach has volunteers post friendsourcing tasks on behalf of people with disabilities	mechanism
We demonstrate this approach via a Facebook application that answers visual questions on behalf of blind users	method
In prior research we have developed a Curry-Howard interpretation of linear sequent calculus as session-typed processes	background
via a linear contextual monad that isolates session-based concurrency	mechanism
Monadic values are open process expressions and are first class objects in the language	mechanism
thus providing a logical foundation for higher-order session typed processes We illustrate how the combined use of the monad and recursive types allows us to cleanly write a rich variety of concurrent programs	mechanism
including higher-order programs that communicate processes We show the standard metatheoretic result of type preservation	mechanism
as well as a global progress theorem	mechanism
which to the best of our knowledge	mechanism
is new in the higher-order session typed setting	mechanism
Multi-modal data is dramatically increasing with the fast growth of social media Learning a good distance measure for data with multiple modalities is of vital importance for many applications	background
including retrieval clustering	background
classification and recommendation	background
and present empirical results on retrieval and classification to demonstrate the effectiveness and scalability	finding
In this paper	mechanism
we propose Based on the multi-wing harmonium model	mechanism
our method provides a principled way to embed data of arbitrary modalities into a single latent space	mechanism
of which an optimal distance metric can be learned under proper supervision	mechanism
i	mechanism
e	mechanism
by minimizing the distance between similar pairs whereas maximizing the distance between dissimilar pairs	mechanism
The parameters are learned by jointly optimizing the data likelihood under the latent space model and the loss induced by distance supervision	mechanism
thereby our method seeks a balance between explaining the data and providing an effective distance metric	mechanism
which naturally avoids overfitting	mechanism
We apply our general framework to text/image data	method
shows DOT2DOT correctly groups nodes for which good connection paths can be constructed	finding
while separating distant nodes	finding
in terms of the Minimum Description Length principle : a set of paths is simple when we need few bits to describe each path from one node to another	mechanism
For example we want to avoid high-degree nodes	mechanism
unless we need to visit many of its spokes	mechanism
As such the best partitioning requires the least number of bits to describe the paths that visit all marked nodes	mechanism
We show that our formulation for finding simple paths between groups of nodes has connections to well-known other problems in graph theory	mechanism
and is NP-hard	mechanism
We propose fast effective solutions	mechanism
and introduce DOT2DOT	mechanism
an efficient algorithm	mechanism
Experimentation	method
We validate the proposed methods	finding
Graphs model data with complex structure as signals on a graph	mechanism
Graph signal recovery recovers one or multiple smooth graph signals from noisy	mechanism
corrupted or incomplete measurements We formulate as an optimization problem	mechanism
for which we provide a general solution through the alternating direction methods of multipliers We show how signal inpainting	mechanism
matrix completion robust principal component analysis	mechanism
and anomaly detection all relate to graph signal recovery and provide corresponding specific solutions and theoretical analysis	mechanism
on real-world recovery problems	method
including online blog classification	method
bridge condition identification	method
temperature estimation recommender system for jokes	method
and expert opinion combination of online blog classification	method
and the performance of the new method The adjacency matrices estimated using the new method are shown to be close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset	finding
This paper presents a computationally tractable algorithm is presented	mechanism
The algorithm is demonstrated on simulated and real network time series datasets is compared to that of related methods for estimating graph structure	method
and solidly outperformed the state-of-the-art algorithms under four evaluation protocols with a high accuracy of 89	finding
69 % a top score among image-restricted and unsupervised protocols	finding
The advancement of Spartans is also proven In addition	finding
our learning method based on advanced correlation filters is much more effective	finding
in both linear and non-linear cases	finding
In this paper	mechanism
we investigate a single-sample periocular-based alignment-robust face recognition technique Our Spartans framework starts by utilizing one single sample per subject class	mechanism
and generate new face images under a wide range of 3D rotations using the 3D generic elastic model which is both accurate and computationally economic	mechanism
Then we focus on the periocular region where the most stable and discriminant features on human faces are retained	mechanism
and marginalize out the regions beyond the periocular region since they are more susceptible to expression variations and occlusions A novel facial descriptor	mechanism
high-dimensional Walsh local binary patterns	mechanism
is uniformly sampled on facial images with robustness toward alignment	mechanism
During the learning stage	mechanism
subject-dependent advanced correlation filters are learned for pose-tolerant non-linear subspace modeling in kernel feature space followed by a coupled max-pooling mechanism which further improve the performance Given any unconstrained unseen face image	mechanism
the Spartans can produce a highly discriminative matching score	mechanism
thus achieving high verification rate	mechanism
We have evaluated our method on the challenging Labeled Faces in the Wild database in the Face Recognition Grand Challenge and Multi-PIE databases in terms of learning subject-dependent pose-tolerant subspaces	method
compared with many well-established subspace methods	method
Summary form only given	background
We show that fractals and self-similarity can explain several of the observed patterns	finding
and we conclude and a surprising result on virus propagation and immunization	finding
We present a long list of static and temporal laws	mechanism
and	mechanism
some recent observations on real graphs	method
with cascade analysis	method
Facial hair detection and segmentation play an important role in forensic facial analysis	background
results have demonstrated the robustness and effectiveness of our proposed system in detecting and segmenting facial hair	finding
In this paper	mechanism
we propose a fast	mechanism
robust fully automatic and self-training system In order to overcome the limitations of illumination	mechanism
facial hair color and near-clear shaving	mechanism
our facial hair detection self-learns a transformation vector to separate a hair class and a non-hair class from the testing image itself A feature vector	mechanism
consisting of Histogram of Gabor ( HoG ) and Histogram of Oriented Gradient of Gabor ( HOGG ) at different directions and frequencies	mechanism
is proposed for both beard/moustache detection and segmentation in this paper	mechanism
A feature-based segmentation is then proposed to segment the beard/moustache from a region on the face that is discovered to contain facial hair	mechanism
Experimental in images drawn from three entire databases i	method
e	method
the Multiple Biometric Grand Challenge ( MBGC ) still face database	method
the NIST color Facial Recognition Technology FERET database and a large subset from Pinellas County database	method
we demonstrate the appeal of this approach on synthetic examples and real power networks significantly larger than those previously considered in the literature	finding
Our focus is on an improved algorithm with convergence times that are several orders of magnitude faster than existing algorithms	mechanism
In particular we develop an efficient proximal Newton method which minimizes per-iteration cost with a coordinate descent active set approach and fast numerical solutions to the Lyapunov equations	mechanism
Experimentally	method
Our result is promising : we found an average of 38	finding
6 % more bugs than three previous fuzzers over 8 applications using the same amount of fuzzing time	finding
We present the design of an algorithm given a program and a seed input The major intuition is to leverage white-box symbolic analysis on an execution trace for a given program-seed pair to detect dependencies among the bit positions of an input	mechanism
and then use this dependency relation to compute a probabilistically optimal mutation ratio for this program-seed pair	mechanism
Crowdsourcing can solve problems beyond the reach of state-of-the-art fully automated systems	background
A common pattern found in many such systems is for the workers to discover	background
in parallel a number of candidate solutions and then vote on the best one to pass forward	background
often within a fixed amount of time	background
We present the propose-vote-abstain mechanism Each crowd worker is given a choice among proposing an answer	mechanism
voting among the answers proposed so far	mechanism
or abstaining i	mechanism
e	mechanism
doing nothing	mechanism
When a stopping condition is reached	mechanism
the mechanism returns the answer with the most votes	mechanism
Workers are paid a base amount	mechanism
with bonuses if they propose or vote for the winning answer	mechanism
Crowdsourcing systems leverage short bursts of focused attention from many contributors to achieve a goal	background
We discuss the design space for low-effort crowdsourcing	mechanism
and through a series of prototypes	mechanism
demonstrate interaction techniques	mechanism
mechanisms and emerging principles	mechanism
In this paper	method
we study opportunities for low-effort crowdsourcing that enable people to contribute to problem solving in such settings	method
In large scale machine learning and data mining problems with high feature dimensionality	background
the Euclidean distance between data points can be uninformative	background
and Distance Metric Learning ( DML ) is often desired to learn a proper similarity measure ( using side information such as example data pairs being similar or dissimilar )	background
and we show that	finding
our program is able to complete a DML task	finding
22-thousand features and 200 million labeled data pairs	finding
in 15 hours ; and the learned metric shows great effectiveness in properly measuring distances	finding
In this paper	mechanism
we present a distributed algorithm for DML	mechanism
and a large-scale implementation on a parameter server architecture	mechanism
Our approach builds on a parallelizable reformulation of Xing et al	mechanism
( 2002 )	mechanism
and an asynchronous stochastic gradient descent optimization procedure	mechanism
To our knowledge	mechanism
this is the first distributed solution to DML	mechanism
on a system with 256 CPU cores on a dataset with 1 million data points	method
We discuss an initial outline for Chorus : Mnemonic	mechanism
a system that augments the crowd 's collective memory of a conversation by automatically recovering past knowledge based on topic	mechanism
allowing the system to support consistent multi-session interactions We present the design of the system itself	mechanism
and discuss methods for testing its effectiveness	method
Examples include adaptive front lighting in vehicles	background
dynamic stage performance lighting	background
adaptive dynamic range imaging and volumetric displays	background
and demonstrate dis-illumination of falling snow-like particles and photography of fast moving scenes	finding
A simulator is developed Simulations are conducted to characterize system performance by analyzing the effects of end-to-end latency	mechanism
jitter and prediction algorithm complexity	mechanism
Key operating points are identified where systems with simple prediction algorithms can outperform systems with more complex prediction algorithms Based on the lessons learned from simulations	mechanism
a low latency and low jitter	mechanism
tight closed-loop reactive visual system is built	mechanism
For the first time	method
we measure end-to-end latency	method
perform jitter analysis	method
investigate various prediction algorithms and their effect on system performance	method
compare our system 's performance to previous work	method
Thousands of web APIs expose data and services that would be useful to access with natural dialog	background
from weather and sports to Twitter and movies	background
and present results for each stage	finding
We present a crowd-powered system able to generate a natural languageinterface for arbitrary web APIs from scratch without domain-dependent training data or knowledge	mechanism
Our approach combines two types of crowd workers : non-expert Mechanical Turk workers interpret the functions of the API and elicit information from the user	mechanism
and expert oDesk workers provide a minimal sufficient scaffolding around the API to allow us to make general queries	mechanism
We describe our multi-stage process	mechanism
and suggest required modifications	mechanism
In this work	method
we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols	method
Many big data applications collect a large number of time series	background
for example the financial data of companies quoted in a stock exchange	background
the health care data of all patients that visit the emergency room of a hospital	background
or the temperature sequences continuously measured by weather stations across the US	background
A first task in the analytics of these data is to derive a low dimensional representation	background
a graph or discrete manifold	background
that describes well the interrelations among the time series and their intrarelations across time	background
The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested	finding
This paper presents a computationally tractable algorithm This graph is directed and weighted	mechanism
possibly representing causation relations	mechanism
not just correlations as in most existing approaches in the literature	mechanism
The algorithm is demonstrated on random graph and real network time series datasets	method
and its performance is compared to that of related methods	method
How can we correlate neural activity in the human brain as it responds to words	background
with behavioral data expressed as answers to questions about these same words ?	background
We show that this is an instance of the Coupled Matrix-Tensor Factorization ( CMTF ) problem	finding
we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm for CMTF	finding
along with a 5 fold increase in sparsity Scoup-SMT is able to find meaningful latent variables	finding
as well as to predict brain activity with competitive accuracy Finally	finding
we demonstrate the generality of Scoup-SMT there	finding
Scoup-SMT spots spammer-like anomalies	finding
We propose Scoup-SMT	mechanism
a novel fast	mechanism
and parallel algorithm and produces a sparse latent low-rank subspace of the data Moreover	mechanism
we extend Scoup-SMT to handle missing data without degradation of performance	mechanism
In our experiments We apply Scoup-SMT to BrainQ	method
a dataset consisting of a ( nouns	method
brain voxels human subjects ) tensor and a ( nouns	method
properties ) matrix	method
with coupling along the nouns dimension by applying it on a Facebook dataset ( users	method
friends wall-postings ) ;	method
results show that	finding
albeit simple our model achieves state-of-the-arts results	finding
We propose a method We improve upon local descriptor based methods that have been among the most popular and successful models for representing videos	mechanism
The desired local descriptors need to satisfy two requirements : 1 ) to be representative	mechanism
2 ) to be discriminative	mechanism
Therefore they need to occur frequently enough in the videos and to be be able to tell the difference among different types of motions	mechanism
In this paper	mechanism
we introduce a long-short term motion feature that generates descriptors from video blocks with multiple lengths	mechanism
Experimental on several benchmark datasets	method
How can we find useful patterns and anomalies in large scale real-world data with multiple attributes ? For example	background
network intrusion logs	background
with ( source-ip	background
target-ip port-number timestamp ) ? Tensors are suitable for modeling these multi-dimensional data	background
and widely used for the analysis of social networks	background
web data network traffic	background
and in many other settings	background
and discover hidden concepts	finding
In this paper	mechanism
we propose HaTen2	mechanism
a scalable distributed suite of tensor decomposition algorithms running on the MapReduce platform By carefully reordering the operations	mechanism
and exploiting the sparsity of real world tensors	mechanism
HaTen2 dramatically reduces the intermediate data	mechanism
and the number of jobs	mechanism
As a result	mechanism
using HaTen2	mechanism
we analyze big real-world tensors that can not be handled by the current state of the art	method
Building information models ( BIMs ) provide opportunities to serve as an information repository to store and deliver as-built information	background
Since a building is not always constructed exactly as the design information specifies	background
there will be discrepancies between a BIM created in the design phase ( called as-designed BIM ) and the as-built conditions	background
Point clouds captured by laser scans can be used as a reference to update an as-designed BIM into an as-built BIM ( i	background
e	background
the BIM that captures the as-built information )	background
Occlusions and construction progress prevent a laser scan performed at a single point in time to capture a complete view of building components	background
Progressively scanning a building during the construction phase and combining the progressively captured point cloud data together can provide the geometric information missing in the point cloud data captured previously	background
This paper provides the details of an approach developed	mechanism
Both SAT and # SAT can represent difficult problems in seemingly dissimilar areas such as planning	background
verification and probabilistic inference	background
# SAT problems require counting the number of satisfiable formulas in a concisely-describable set of existentially-quantified	background
propositional formulas	background
Our experiments show that	finding
despite the formidable worst-case complexity of # PNP [ 1 ]	finding
many of the instances can be solved efficiently by noticing and exploiting a particular type of frequent structure	finding
We characterize the expressiveness and worst-case difficulty of # SAT by proving it is complete for the complexity class # PNP [ 1 ]	mechanism
and relating this class to more familiar complexity classes	mechanism
We also experiment with three new general-purpose # SAT solvers on a battery of problem distributions including a simple logistics domain	method
Classic cake cutting protocols -- which fairly allocate a divisible good among agents with heterogeneous preferences -- are susceptible to manipulation	background
GCC protocols are guaranteed to have exact subgame perfect Nash equilibria	finding
we adopt a novel algorithmic approach	mechanism
proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model Specifically	mechanism
we show that each protocol in the class of generalized cut and choose ( GCC ) protocols -- which includes the most important discrete cake cutting protocols -- is guaranteed to have approximate subgame perfect Nash equilibria Moreover	mechanism
we observe that the ( approximate ) equilibria of proportional protocols -- which guarantee each of the n agents a 1/n-fraction of the cake -- must be ( approximately ) proportional	mechanism
and design a GCC protocol where all Nash equilibrium outcomes satisfy the stronger fairness notion of envy-freeness	mechanism
Finally we show that under an obliviousness restriction	method
which still allows the computation of approximately envy-free allocations	method
The paper introduces a method by which In the scenario considered	mechanism
sensor nodes communicate with each other within a graph structure to update their data according to linear dynamics using neighbor node data A subset of sensors can also report their state to a central location One physical interpretation of this situation would be a set of spatially distributed wireless sensors which can communicate with other sensors within range to update data and can possibly connect to a network backbone	mechanism
The costs would then be related to transmission energy	mechanism
The objective is to recover the vector of initial sensor measurements from the backbone outputs over time	mechanism
which requires that the dynamics of the overall networked system be observable	mechanism
The topology of the network is then determined by the nonzero elements of the optimal observable dynamics The following text contributes an efficient algorithm for designing the optimal observable dynamics and the network topology for a given set of sensors and cost function	mechanism
providing proof of correctness and example implementation	method
The Bayesian paradigm has provided a useful conceptual theory for understanding perceptual computation in the brain	background
While the detailed neural mechanisms of Bayesian inference are not fully understood	background
recent computational and neurophysiological works have illuminated the underlying computational principles and representational architecture	background
The fundamental insights are that the visual system is organized as a modular hierarchy to encode an internal model of the world	background
and that perception is realized by statistical inference based on such internal model	background
We will argue for a unified theoretical framework	mechanism
that this approach outperforms the Network Time Protocol ( NTP ) on smartphones by an order of magnitude	finding
providing an average 720s synchronization accuracy with clock drift rates as low as 2ppm	finding
In this paper	mechanism
we present the design and evaluation of a platform The platform uses the Time-Difference-Of-Arrival ( TDOA ) of multiple ultrasonic chirps broadcast from a network of beacons placed throughout the environment to find an initial location as well as synchronize a receivers clock with the infrastructure	mechanism
These chirps encode identification data and ranging information that can be used to compute the receivers location Once the clocks have been synchronized	mechanism
the system can continue performing localization directly using Time-of-Flight ( TOF ) ranging as opposed to TDOA	mechanism
This provides similar position accuracy with fewer beacons ( for tens of minutes ) until the mobile device clock drifts enough that a TDOA signal is once again required	mechanism
Our hardware platform uses RF-based time synchronization to distribute clock synchronization from a subset of infrastructure beacons connected to a GPS source	mechanism
Mobile devices use a novel time synchronization technique leverages the continuously free-running audio sampling subsystem of a smartphone to synchronize with global time Once synchronized	mechanism
each device can determine an accurate proximity from as little as one beacon using TOF measurements	mechanism
This significantly decreases the number of beacons required to cover an indoor space and improves performance in the face of obstructions	mechanism
We show through experiments	method
Document clustering and topic modeling are two closely related tasks which can mutually benefit each other	background
Topic modeling can project documents into a topic space which facilitates effective document clustering	background
Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters	background
demonstrate the effectiveness of our model	finding
In this paper	mechanism
we propose a multi-grain clustering topic model ( MGCTM ) which integrates document clustering and topic modeling Our model tightly couples two components : a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters We employ variational inference to approximate the posterior of hidden variables and learn model parameters	mechanism
Experiments on two datasets	method
In the United States	background
over three billion dollars are spent due to office equipment being left on when not in use during the weekend and at night	background
by applying persuasive technologies We then proceeded to develop `` dashboard-controllers '' with expert feedback to save energy	mechanism
To this end	method
we conducted a literature review to investigate the persuasive methods appropriate to the field of building controls	method
Differential game logic ( dG L ) is a logic for specifying and verifying properties of hybrid games	background
i	background
e	background
games that combine discrete	background
continuous and adversarial dynamics	background
Unlike hybrid systems	background
hybrid games allow choices in the system dynamics to be resolved adversarially by different players with different objectives	background
Finally dG L is proved to be strictly more expressive than the corresponding logic of hybrid systems	finding
The logic dG L can be used i	mechanism
e	mechanism
ways of resolving the players choices in some way so that he wins by achieving his objective for all choices of the opponent	mechanism
Hybrid games are determined	mechanism
i	mechanism
e	mechanism
from each state	mechanism
one player has a winning strategy	mechanism
yet computing their winning regions may take transfinitely many steps	mechanism
The logic dG L	mechanism
nevertheless has a sound and complete axiomatization relative to any expressive logic	mechanism
Separating axioms are identified that distinguish hybrid games from hybrid systems	mechanism
by characterizing the expressiveness of both	method
we detect two types of copycats ( deceptive and non-deceptive ) Our results indicate significant heterogeneity in the interactions between copycats and original apps over time : ( 1 ) Non-deceptive copycats are reluctant to enter the market when the original app is popular and free	finding
However this negative effect does not hold in other cases ; ( 2 ) Copycats can be either friends or foes of the original apps High quality copycats always have a negative effect on the original app downloads	finding
Interestingly low quality deceptive copycats have a positive effect on the original app downloads	finding
suggesting a potential positive spillover effect	finding
Using machine learning techniques on large-scale unstructured data	mechanism
Based on our detected copycats	mechanism
we model the key drivers of mobile app copycats as well as their major impacts	mechanism
from 10 100 action game apps from iOS App Store over five years	method
Protocols for tasks such as authentication	background
electronic voting and secure multiparty computation ensure desirable security properties if agents follow their prescribed programs Thus	background
our definition applies to relevant security properties	background
First we prove that violations of a specific class of safety properties always have an actual cause	finding
Specifically we define in an interacting program model what it means for a set of program actions to be an actual cause of a violation We present a sound technique We demonstrate the value of this formalism in two ways	mechanism
Second we provide a cause analysis of a representative protocol designed to address weaknesses in the current public key certification infrastructure	method
Privacy has become a significant concern in modern society as personal information about individuals is increasingly collected	background
used and shared	background
often using digital technologies	background
by a wide range of organizations To mitigate privacy concerns	background
organizations are required to respect privacy laws in regulated sectors e	background
g	background
HIPAA in healthcare	background
GLBA in financial sector and to adhere to self-declared privacy policies in self-regulated sectors e	background
g	background
privacy policies of companies such as Google and Facebook in Web services	background
producing the first complete logical specification and audit of all disclosure-related clauses of the HIPAA Privacy Rule	finding
We formalize privacy policies that prescribe and proscribe flows of personal information as well as those that place restrictions on the purposes for which a governed entity may use personal information Recognizing that traditional preventive access control and information flow control mechanisms are inadequate for enforcing such privacy policies	mechanism
we develop principled accountability mechanisms that seek to encourage policy-compliant behavior by detecting policy violations	mechanism
assigning blame and punishing violators	mechanism
We apply these techniques to several U	method
S	method
privacy laws and organizational privacy policies	method
in particular	method
with 1 million topics and a 1-million-word vocabulary ( for a total of 1 trillion parameters )	finding
on a document collection with 200 billion tokens -- - a scale not yet reported even with thousands of machines	finding
evidence showing how this development puts massive data and models within reach on a small cluster	finding
while still enjoying proportional time cost reductions with increasing cluster size	finding
and show that with a modest cluster of as few as 8 machines	mechanism
we can train a topic model Our major contributions include : 1 ) a new	mechanism
highly-efficient O ( 1 ) Metropolis-Hastings sampling algorithm	mechanism
whose running cost is ( surprisingly ) agnostic of model size	mechanism
and empirically converges nearly an order of magnitude more quickly than current state-of-the-art Gibbs samplers ; 2 ) a model-scheduling scheme to handle the big model challenge	mechanism
where each worker machine schedules the fetch/use of sub-models as needed	mechanism
resulting in a frugal use of limited memory capacity and network bandwidth ; 3 ) a differential data-structure for model storage	mechanism
which uses separate data structures for high- and low-frequency words to allow extremely large models to fit in memory	mechanism
while maintaining high inference speed	mechanism
These contributions are built on top of the Petuum open-source distributed ML framework	mechanism
and we provide experimental	method
In distributed ML applications	background
shared parameters are usually replicated among computing nodes to minimize network overhead	background
Therefore proper consistency model must be carefully chosen to ensure algorithm 's correctness and provide high throughput	background
Existing consistency models used in general-purpose databases and modern distributed ML systems are either too loose to guarantee correctness of the ML algorithms or too strict and thus fail to fully exploit the computing power of the underlying distributed system	background
Many ML algorithms fall into the category of \emph { iterative convergent algorithms } which start from a randomly chosen initial point and converge to optima by repeating iteratively a set of procedures	background
In this paper	mechanism
we present several relaxed consistency models The proposed consistency models are implemented in a distributed parameter server	mechanism
theoretically prove their algorithmic correctness and evaluated in the context of a popular ML application : topic modeling	method
Modern organizations ( e	background
g	background
hospitals social networks	background
government agencies ) rely heavily on audit to detect and punish insiders who inappropriately access and disclose confidential information	background
that this transformation significantly speeds up computation of solutions for a class of audit games and security games	finding
We significantly generalize this audit game model where each resource is restricted to audit a subset of all potential violations	mechanism
thus We provide an FPTAS that computes an approximately optimal solution to the resulting non-convex optimization problem The main technical novelty is in the design and correctness proof of an optimization transformation that enables the construction of this FPTAS	mechanism
In addition we experimentally demonstrate	method
To partly address people 's concerns over web tracking	background
Google has created the Ad Settings webpage to provide information about and some choice over the profiles Google creates on users Nevertheless	background
these results can form the starting point for deeper investigations by either the companies themselves or by regulatory bodies	background
In particular we found that visiting webpages associated with substance abuse changed the ads shown but not the settings page	finding
We also found that setting the gender to female resulted in getting fewer instances of an ad related to high paying jobs than setting it to male We can not determine who caused these findings due to our limited visibility into the ad ecosystem	finding
which includes Google	finding
advertisers websites and users	finding
We present AdFisher	mechanism
an automated tool AdFisher can run browser-based experiments and analyze data using machine learning and significance tests	mechanism
Our tool uses a rigorous experimental design and statistical analysis to ensure the statistical soundness of our results	mechanism
We use AdFisher to find that the Ad Settings was opaque about some features of a user 's profile	mechanism
that it does provide some choice on ads	mechanism
and that these choices can lead to seemingly discriminatory ads	mechanism
password reuse benefits users not only by reducing the number of passwords that the user has to memorize	finding
but more importantly by in- creasing the natural rehearsal rate for each password	finding
We introduce quantitative usability and security models In the same way that security proofs in cryptography are based on complexity- theoretic assumptions ( e	mechanism
g	mechanism
hardness of factoring and discrete loga- rithm )	mechanism
we quantify usability by introducing usability assumptions	mechanism
I n particular	mechanism
password management relies on assumptions about human memory	mechanism
e	mechanism
g	mechanism
that a user who follows a particular rehearsal schedule will successfully maintain the corresponding memory These assumptions are informed by research in cognitive science and can be tested empirically Given rehearsal requirements and a user 's visitation schedule for each account	mechanism
we use the total number of extra rehearsals that the user would have to do to remember all of his passwords as a measure of the usability of the password scheme Our usability model leads us to a key observa- tion : We also present a security model which accounts with multiple accounts and associated threats	mechanism
including online offline	mechanism
and plaintext password leak attacks	mechanism
we present Shared Cues a new scheme in which the underlying secret is strategi- cally shared across accounts to ensure that most rehearsal requirements are satisfied naturally while simultaneously providing strong security The construction uses the Chinese Remainder Theorem to achieve these competing goals	mechanism
Many important applications fall into the broad class of iterative convergent algorithms	background
Parallel implementations of these algorithms are naturally expressed using the Bulk Synchronous Parallel ( BSP ) model of computation	background
This paper presents the Stale Synchronous Parallel ( SSP ) model as a generalization of BSP Algorithms using SSP can execute efficiently	mechanism
even with significant delays in some threads	mechanism
addressing the oft-faced straggler problem	mechanism
Ductal Carcinoma In Situ ( DCIS ) is a precursor lesion of Invasive Ductal Carcinoma ( IDC ) of the breast Investigating its temporal progression could provide fundamental new insights for the development of better diagnostic tools to predict which cases of DCIS will progress to IDC	background
The approach provides new insights into mechanisms of clonal progression in breast cancers and helps illustrate the power of the ILP approach for similar problems in reconstructing tumor evolution scenarios under complex sets of constraints	background
show that the corresponding predicted progression models are classifiable into categories having specific evolutionary characteristics	finding
Specifically by using a number of assumptions derived from the observation of cellular atypia occurring in IDC	mechanism
we design a possible predictive model using integer linear programming ( ILP )	mechanism
Computational experiments carried out on a preexisting data set of 13 patients with simultaneous DCIS and IDC	method
A variety of problems in computing	background
service and manufacturing systems can be modeled via infinite repeating Markov chains with an infinite number of levels and a finite number of phases	background
We present a procedure	mechanism
which we call Clearing Analysis on Phases ( CAP ) The CAP method yields the limiting probability of each state in the repeating portion of the chain as a linear combination of scalar bases raised to a power corresponding to the level of the state	mechanism
The weights in these linear combinations can be determined by solving a finite system of linear equations	mechanism
Any strong Nash equilibrium outcome is Pareto efficient for each coalition	background
Our main result	finding
in its simplest form	finding
states that if a game has a strong Nash equilibrium with full support ( that is	finding
both players randomize among all pure strategies )	finding
then the game is strictly competitive	finding
we use the indifference principle fulfilled by any Nash equilibrium	mechanism
and the classical KKT conditions ( in the vector setting )	mechanism
that are necessary conditions for Pareto efficiency	mechanism
Our characterization enables us to design a strong-Nash-equilibrium-finding algorithm with complexity in Smoothed- $ \mathcal { P } $	mechanism
So this problem -- -that Conitzer and Sandholm [ Conitzer	mechanism
V	mechanism
Sandholm T	mechanism
2008	mechanism
New complexity results about Nash equilibria	mechanism
Games Econ	mechanism
Behav	mechanism
63 621 -- 641 ] proved to be computationally hard in the worst case -- -is generically easy	mechanism
Hence although the worst case complexity of finding a strong Nash equilibrium is harder than that of finding a Nash equilibrium	mechanism
once small perturbations are applied	mechanism
finding a strong Nash is easier than finding a Nash equilibrium Next we switch to the setting with more than two players	mechanism
We demonstrate that a strong Nash equilibrium can exist in which an outcome that is strictly Pareto dominated by a Nash equilibrium occurs with positive probability Finally	mechanism
we prove that games that have a strong Nash equilibrium where at least one player puts positive probability on at least two pure strategies are extremely rare : they are of zero measure	mechanism
First we analyze the two -- player setting	method
Many real world network problems often concern multivariate nodal attributes such as image	background
textual and multi-view feature vectors on nodes	background
rather than simple univariate nodal attributes	background
demonstrate performance of our method under various conditions	finding
In this paper	mechanism
we propose a new principled framework Instead of estimating the partial correlation as in current literature	mechanism
our method estimates the partial canonical correlations that naturally accommodate complex nodal features Computationally	mechanism
we provide an efficient algorithm which utilizes the multi-attribute structure	mechanism
Theoretically we provide sufficient conditions which guarantee consistent graph recovery	mechanism
Extensive simulation studies	method
Proceedings : AACR 106th Annual Meeting 2015 ; April 18-22	background
2015 ; Philadelphia	background
PA Intratumor heterogeneity has long been a confounding factor in interpreting cancer genomic data	background
but has also been useful in reconstructing progression processes based on variation between clonal populations in single tumors We previously developed a strategy of applying computational deconvolution algorithms to gene expression or DNA copy number data to reconstruct models of progression of cell populations from bulk tumor samples	background
Citation Format : Theodore Roman	background
Russell Schwartz	background
All methods perform comparably on unstructured data but the new method substantially outperforms the others on data consistent with simple scenarios for tumor progression along multiple discrete subtypes	finding
The novel method	finding
however shows lower tolerance for noisy data than a Gaussian mixture model	finding
showed our method could partition tumors into discrete subcategories associated with HER2+	finding
ER/PR+ and triple-negative status and could exploit the resulting substructure of the data to deconvolve tumor data and infer progression models reflecting partial sharing of progression states between subtypes	finding
showed association of deconvolved cell populations with a variety of gene functional categories suggestive of distinct progression mechanisms of the subtypes	finding
We present a novel approach designed to leverage the fact that tumors that partition into subtypes with similar evolutionary trajectories would be expected to lead to a mathematical substructure in the genomic data	mechanism
known as a simplicial complex	mechanism
which can be modeled computationally We have developed a computational pipeline while taking into consideration this kind of mathematical structure The pipeline clusters tumors to identify genetically distinct subgroups	mechanism
fits mixture models to these subgroups	mechanism
and uses overlap between them to infer a refined deconvolution of major cellular populations and possible pathways of progression between them	mechanism
We apply our methods to a set of RNASeq data from the TCGA breast cancer data set and to synthetic data modeling distinct scenarios of tumor progression	method
We first compare our methods on the synthetic data to our earlier work and to a comparative Gaussian mixture model	method
Application to the TCGA RNASeq data Gene enrichment analysis	method
Signals and datasets that arise in physical and engineering applications	background
as well as social	background
genetics biomolecular and many other domains	background
are becoming increasingly larger and more complex	background
In contrast to traditional time and image signals	background
data in these domains are supported by arbitrary graphs	background
Signal processing on graphs extends concepts and techniques from traditional signal processing to data indexed by generic graphs	background
In traditional signal processing	background
these concepts are easily defined because of a natural frequency ordering that has a physical interpretation	background
For signals residing on graphs	background
in general there is no obvious frequency ordering	background
We propose a definition of total variation that naturally leads to a frequency ordering on graphs and defines low-	mechanism
high- and band-pass graph signals and filters	mechanism
We study the design of graph filters with specified frequency response	method
and illustrate our approach with applications to sensor malfunction detection and data classification	method
Block tridiagonal matrices arise in applied mathematics	background
physics and signal processing	background
Many applications require knowledge of eigenvalues and eigenvectors of block tridiagonal matrices	background
which can be prohibitively expensive for large matrix sizes	background
that our work can lead to fast algorithms for the eigenvector expansion for block tridiagonal matrices	finding
by studying a connection between their eigenvalues and zeros of appropriate matrix polynomials We use this connection with matrix polynomials to derive a closed-form expression for the eigenvectors of block tridiagonal matrices	mechanism
which eliminates the need for their direct calculation and can lead to a faster calculation of eigenvalues	mechanism
We also demonstrate with an example	method
and confirm the effectiveness of the proposed approach	finding
Instead of indifferently pooling the shots	mechanism
we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event detector	mechanism
Next we propose a new isotonic regularizer that is able to exploit the semantic ordering information The resulting nearly-isotonic SVM classifier exhibits higher discriminative power Computationally	mechanism
we develop an efficient implementation using the proximal gradient algorithm	mechanism
and we prove new	mechanism
closed-form proximal steps	mechanism
We conduct extensive experiments on three real-world video datasets	method
which may be of independent interest	background
We present faster algorithms such as bounded genus	mechanism
minor free and geometric graphs Given such a graph with n vertices	mechanism
m edges along with a recursive n-vertex separator structure	mechanism
our algorithm finds an 1 -- e approximate maximum flow in time O ( m6/5poly ( e -- 1 ) )	mechanism
ignoring poly-logarithmic terms Similar speedups are also achieved for separable graphs with larger size separators albeit with larger run times These bounds also apply to image problems in two and three dimensions	mechanism
Key to our algorithm is an intermediate problem that we term grouped L2 flow	mechanism
which exists between maximum flows and electrical flows	mechanism
Our algorithm also makes use of spectral vertex sparsifiers in order to remove vertices while preserving the energy dissipation of electrical flows We also give faster spectral vertex sparsification algorithms on well separated graphs	mechanism
The cake cutting problem models the fair division of a heterogeneous good between multiple agents	background
Previous work assumes that each agent derives value only from its own piece	background
We extend the classical model Our technical results characterize the relationship between these generalized properties	mechanism
establish the existence or nonexistence of fair allocations	mechanism
and explore the computational feasibility of fairness in the face of externalities	mechanism
Our first contribution is the discovery that the relative frequencies obey a power-law ( sub-linear	finding
or super-linear )	finding
for a wide variety of diverse settings : tasks in a phone- call network	finding
like count of friends	finding
count of phone-calls	finding
total count of minutes ; tasks in a twitter-like network	finding
like count of tweets	finding
count of followees etc	finding
We show how to use our observations to spot clusters and outliers	finding
like e	finding
g	finding
telemarketers in our phone-call network	finding
The second contribution is that we further provide a full	mechanism
digitized 2-d distribution	mechanism
which we call the Almond-DG model	mechanism
thanks to the shape of its iso-surfaces	mechanism
The Almond-DG model matches all our empirical observations : super-linear relationships among variables	mechanism
and ( provably ) log-logistic marginals	mechanism
We illustrate our observations on two large	method
real network datasets	method
spanning 2	method
2M and 3	method
1M individu- als with 5 features each	method
Kidney exchange provides a life-saving alternative to long waiting lists for patients in need of a new kidney	background
Fielded exchanges typically match under utilitarian or near-utilitarian rules ; this approach marginalizes certain classes of patients	background
we formally adapt a recently introduced measure of the tradeoff between fairness and efficiency -- -the price of fairness -- -to the standard kidney exchange model	mechanism
We show that the price of fairness in the standard theoretical model is small	mechanism
We then introduce two natural definitions of fairness	mechanism
and empirically explore the tradeoff between matching more hard-to-match patients and the overall utility of a utilitarian matching	method
on real data from the UNOS nationwide kidney exchange and simulated data from each of the standard kidney exchange distributions	method
Abstract : Privacy policies in sectors as diverse as Web services	background
finance and healthcare often place restrictions on the purposes for which a governed entity may use personal information	background
using a formalism based on planning	mechanism
We model planning using Partially Observable Markov Decision Processes ( POMDPs )	mechanism
which supports an explicit model of information We argue that information use is for a purpose if and only if the information is used while planning to optimize the satisfaction of that purpose under the POMDP model We determine information use by simulating ignorance of the information prohibited by the purpose restriction	mechanism
which we relate to noninterference	mechanism
We use this semantics to develop a sound audit algorithm	mechanism
PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task	finding
We present power low rank ensembles ( PLRE )	mechanism
a flexible framework where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context	mechanism
Our method can be understood as a generalization of ngram modeling to non-integer n	mechanism
and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases	mechanism
contributing to the general theory of large deviations	background
When the topology is deterministic	finding
we establish the large deviations principle and find exactly the corresponding rate function	finding
equal at all nodes	finding
We show that the dependence of the rate function on the stochastic weight matrix associated with the network is fully captured by its left eigenvector corresponding to the unit eigenvalue	finding
Further when the sensors observations are Gaussian	finding
the rate function admits a closed-form expression	finding
we show that the network design problem can be formulated as a semidefinite ( convex ) program	finding
and hence can be solved efficiently the system exhibits an interesting property : the graph of the rate function always lies between the graphs of the rate function of an isolated node and the rate function of a fusion center that has access to all observations We prove that this fundamental property holds even when the topology and the associated system matrices change randomly over time	finding
with arbitrary distribution	finding
Motivated by these observations	mechanism
we formulate the optimal network design problem of	mechanism
for a given target accuracy	mechanism
This eigenvector therefore minimizes the time that the inference algorithm needs to reach the desired accuracy	mechanism
Due to the generality of its assumptions	mechanism
the latter result requires more subtle techniques than the standard large deviations tools	mechanism
For Gaussian observations When observations are identically distributed across agents	method
Our study has the potential to help advertisers design keyword portfolios	background
and help search engines improve the quality of sponsored ads	background
We find that consumer click behaviors vary significantly across keywords	finding
and keyword category and contextual ambiguity significantly affect such variation	finding
Specifically higher contextual ambiguity can lead to higher click-through rate ( CTR ) on top-positioned ads	finding
but the CTR tends to decay faster with position	finding
We propose based on topic models from machine learning and computational linguistics We quantify the effect of contextual ambiguity on keyword click-through performance using a hierarchical Bayesian model	mechanism
and validate our study using a novel dataset from a major search engine containing information on click activities for 12	method
790 keywords across multiple categories from over 4	method
6 million impressions	method
Function to function regression ( FFR ) covers a large range of interesting applications including timeseries prediction problems	background
and also more general tasks like studying a mapping between two separate types of distributions	background
Furthermore we show an improvement of several orders of magnitude in terms of prediction speed and a reduction in error over previous estimators	finding
we develop a novel scalable nonparametric estimator	mechanism
the Triple-Basis Estimator ( 3BE )	mechanism
which is capable of operating over data-sets with many instances	mechanism
To the best of our knowledge	mechanism
the 3BE is the first nonparametric FFR estimator that can scale to massive data-sets	mechanism
We analyze the 3BEs risk and derive an upperbound rate	method
in various real-world datasets	method
An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer	background
For these schemes	finding
we prove that forging passwords is equivalent to recovering the secret mapping Thus	finding
our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts	finding
We propose several candidate authentication protocols -- - a computer that stores information and performs computations correctly but does not provide confidentiality	mechanism
Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $	mechanism
The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate	mechanism
We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $	mechanism
for a security parameter $ s ( f ) $ that depends on two key properties To obtain our results	mechanism
we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $	mechanism
Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate )	mechanism
and generalize recent results of Feldman et al	mechanism
To achieve maximum security	background
defenders must perfectly synchronize their randomized allocations of resources	background
indicate that the loss may be extremely high in the worst case	finding
establish a smaller yet significant loss in practice	finding
We introduce two notions under different assumptions : the price of miscoordination	mechanism
and the price of sequential commitment	mechanism
Generally speaking our theoretical bounds while our simulations	method
In real world industrial applications of topic modeling	background
the ability to capture gigantic conceptual space by learning an ultra-high dimensional topical representation	background
i	background
e	background
the so-called `` big model ''	background
is becoming the next desideratum after enthusiasms on `` big data ''	background
especially for fine-grained downstream tasks such as online advertising	background
where good performances are usually achieved by regression-based predictors built on millions if not billions of input features	background
demonstrate the ability of this system to handle topic modeling with unprecedented amount of 200 billion model variables only on a low-end cluster with very limited computational resources and bandwidth	finding
we explore another type of parallelism	mechanism
namely model-parallelism By integrating data-parallelism with model-parallelism	mechanism
we show that dependencies between distributed elements can be handled seamlessly	mechanism
achieving not only faster convergence but also an ability to tackle significantly bigger model size We describe an architecture for model-parallel inference of LDA	mechanism
and present a variant of collapsed Gibbs sampling algorithm tailored for it	mechanism
Experimental results	method
The proliferation of mobile devices that are capable of estimating their position	background
has lead to the emergence of a new class of social networks	background
namely location-based social networks ( LBSNs for short )	background
The main interaction between users in an LBSN is location sharing	background
To the best of our knowledge	background
this is the first attempt to model this problem using tensor analysis	background
In this work	mechanism
we propose the use of tensor decomposition	mechanism
The computational characterization of game-theoretic solution concepts is a central topic in artificial intelligence	background
with the aim of developing computationally efficient tools for finding optimal ways to behave in strategic interactions	background
The central solution concept in game theory is Nash equilibrium ( NE )	background
However it fails to capture the possibility that agents can form coalitions ( even in the 2-agent case )	background
Strong Nash equilibrium ( SNE ) refines NE to this setting	background
It is known that finding an SNE is NP-complete when the number of agents is constant	background
This hardness is solely due to the existence of mixed-strategy SNEs	background
given that the problem of enumerating all pure-strategy SNEs is trivially in P	background
First we develop worst-case instances for support-enumeration algorithms	mechanism
These instances have only one SNE and the support size can be chosen to be of any size-in particular	mechanism
arbitrarily large	mechanism
Second we prove that	mechanism
unlike NE finding an SNE is in smoothed polynomial time : generic game instances ( i	mechanism
e	mechanism
all instances except knife-edge cases ) have only pure-strategy SNEs	mechanism
If we know most of Smith 's friends are from Boston	background
what can we say about the rest of Smith 's friends ? which is one of the most important topics in AI and Web communities	background
We also prove the theoretical connections of our algorithm to the semi-supervised learning ( SSL ) algorithms and to random-walks demonstrate the benefits of the proposed algorithm	finding
where OMNI-Prop outperforms the top competitors	finding
Our proposed algorithm which is referred to as OMNI-Prop has the following properties : ( a ) seamless and accurate ; it works well on any label correlations ( i	mechanism
e	mechanism
homophily het-erophily and mixture of them ) ( b ) fast ; it is efficient and guaranteed to converge on arbitrary graphs ( c ) quasi-parameter free ; it has just one well-interpretable parameter with heuristic default value of 1	mechanism
Experiments on four real	method
different network datasets	method
Single virus epidemics over complete networks are widely explored in the literature as the fraction of infected nodes is	background
under appropriate microscopic modeling of the virus infection	background
a Markov process	background
With non-complete networks	background
this macroscopic variable is no longer Markov	background
we show that the peer-to-peer local random rules of virus infection lead	finding
in the limit of large multipartite networks	finding
to the emergence of structured dynamics at the macroscale The exact fluid limit evolution of the fraction of nodes infected by each virus strain across islands obeys a set of nonlinear coupled differential equations	finding
see this http URL	finding
In this paper	mechanism
we develop methods	mechanism
In companying work this http URL	method
Many modern machine learning ( ML ) algorithms are iterative	background
converging on a final solution via many iterations over the input data	background
show that both approaches significantly increase convergence speeds	finding
behaving similarly when there are no stragglers	finding
but SSP outperforms BSP in the presence of stragglers	finding
Specifically we focus on bounded staleness	mechanism
in which each thread can see a view of the current intermediate solution that may be a limited number of iterations out-of-date	mechanism
Allowing staleness reduces communication costs ( batched updates and cached reads ) and synchronization ( less waiting for locks or straggling threads ) One approach is to increase the number of iterations between barriers in the oft-used Bulk Synchronous Parallel ( BSP ) model of parallelizing	mechanism
which mitigates these costs when all threads proceed at the same speed	mechanism
A more flexible approach	mechanism
called Stale Synchronous Parallel ( SSP )	mechanism
avoids barriers and allows threads to be a bounded number of iterations ahead of the current slowest thread	mechanism
Extensive experiments with ML algorithms for topic modeling	method
collaborative filtering and PageRank	method
as a function of decimal-number representations of regions of the first and second images	mechanism
The decimal-number representations are generated by performing discrete transforms on the regions so as to obtain discrete-transform coefficients	mechanism
performing local-bit-pattern encoding of the coefficients to create data streams	mechanism
and converting the data streams to decimal numbers In one embodiment	mechanism
the first and second images depict periocular facial regions	mechanism
and the disclosed techniques Subspace modeling may be used to improve accuracy	mechanism
Practical applications of Bayesian nonparametric ( BNP ) models have been limited	background
due to their high computational complexity and poor scaling on large data	background
and the near-linear scalability indicates great potential for even bigger problem sizes	background
our system learns a 10K-node DNT topic model on 8M documents that captures both high-frequency and longtail topics	finding
Our data and model scales are orders-of-magnitude larger than recent results on the hierarchical Dirichlet process	finding
and develop a large-scale distributed training system Our major contributions include : ( 1 ) an effective memoized variational inference for DNTs	mechanism
with a novel birth-merge strategy for exploring the unbounded tree space ; ( 2 ) a model-parallel scheme for concurrent tree growing/pruning and efficient model alignment	mechanism
through conflict-free model partitioning and lightweight synchronization ; ( 3 ) a data-parallel scheme for variational parameter updates that allows distributed processing of massive data	mechanism
Using 64 cores in 36 hours	method
It is unsolved even for two bidders and two items for sale	background
show that our algorithms create mechanisms that yield significantly higher revenue than the VCG and scale dramatically better than prior automated mechanism design algorithms The algorithms yielded deterministic mechanisms with the highest known revenues for the settings tested	finding
including the canonical setting with two bidders	finding
two items and uniform additive valuations	finding
Rather than pursuing the manual approach of attempting to characterize the optimal CA	mechanism
we introduce a family of CAs and then seek a high-revenue auction within that family	mechanism
The family is based on bidder weighting and allocation boosting ; we coin such CAs virtual valuations combinatorial auctions ( VVCAs ) VVCAs are the Vickrey-Clarke-Groves ( VCG ) mechanism executed on virtual valuations that are affine transformations of the bidders valuations	mechanism
The auction family is parameterized by the coefficients in the transformations The problem of designing a CA is thereby reduced to search in the parameter space of VVCAor the more general space of affine maximizer auctions	mechanism
We first construct VVCAs with logarithmic approximation guarantees in canonical special settings : ( 1 ) limited supply with additive valuations and ( 2 ) unlimited supply	mechanism
In the main part of the paper	mechanism
we develop algorithms that design high-revenue CAs for general valuations using samples from the prior distribution over bidders valuations	mechanism
( Priors turn out to be necessary for achieving high revenue	mechanism
) We prove properties of the problem that guide our design of algorithms	mechanism
We then introduce a series of algorithms that use economic insights to guide the search and thus reduce the computational complexity	mechanism
Experiments	method
How many listens will an artist receive on a online radio ? How about plays on a YouTube video ? How many of these visits are new or returning users ? Modeling and mining popularity dynamics of social activity has important implications for researchers	background
content creators and providers	background
we show the effect of revisits in the popularity evolution of such objects	finding
Secondly we propose the Phoenix-R model Phoenix-R has the desired properties of being : ( 1 ) parsimonious	mechanism
being based on the minimum description length principle	mechanism
and achieving lower root mean squared error than state-of-the-art baselines ; ( 2 ) applicable	mechanism
the model is effective for predicting future popularity values of objects	mechanism
Using four datasets of social activity	method
with up to tens of millions media objects ( e	method
g	method
YouTube videos Twitter hashtags or LastFM artists )	method
Social microvolunteering lets people volunteer despite temporal	background
financial or physical limitations	background
We propose social microvolunteering	mechanism
in which people can do charitable microwork themselves for free	mechanism
but also grant access to their Facebook friends as additional volunteers	mechanism
For the year of 2013	finding
we show a reduction of up to 24	finding
8 % in the monthly bill is possible we show that EV aggregations decrease their contribution to the system peak load by approximately 37 % ( median ) when charging is controlled within arrival and departure times	finding
Our results also show that it could be expected to shift approximately 0	finding
25kWh ( 2	finding
8 % ) of energy per non-residential EV charging session from peak periods ( 12PM6PM ) to off-peak periods ( after 6PM ) in Northern California for the year of 2013	finding
We develop a smart charging framework by relaxing the assumptions made in these studies regarding : ( i ) driving patterns	mechanism
driver behavior and driver types ; ( ii ) the scalability of a limited number of simulated vehicles to represent different load aggregation points in the power system with different customer characteristics ; and ( iii ) the charging profile of EVs	mechanism
Then following a similar aggregation strategy	mechanism
In this paper	method
we use data collected from over 2000 non-residential electric vehicle supply equipments ( EVSEs ) located in Northern California for the year of 2013 First	method
we study the benefits of EV aggregations behind-the-meter	method
where a time-of-use pricing schema is used to understand the benefits to the owner when EV aggregations shift load from high cost periods to lower cost periods	method
Given the pace of discovery in medicine	background
accessing the literature to make informed decisions at the point of care has become increasingly difficult Advances in social computation and human computer interactions offer a potential solution to this problem	background
Journal of Hospital Medicine 2014 ; 9:451456	background
2014 Society of Hospital Medicine	background
85 registered users logged 1544 page views and sent 45 consult questions	finding
The median initial first response from the crowd occurred within 19 minutes Review of the transcripts revealed several dominant themes	finding
including complex medical decision making and inquiries related to prescription medication use	finding
Feedback from the post-trial survey identified potential hurdles related to medical crowdsourcing	finding
including a reluctance to expose personal knowledge gaps and the potential risk for distracted doctoring	finding
Users also suggested program modifications that could support future adoption	finding
including changes to the mobile interface and mechanisms that could expand the crowd of participating healthcare providers	finding
We developed and piloted the mobile application DocCHIRP	mechanism
which uses a system of point-to-multipoint push notifications designed	mechanism
Over the 244-day pilot period	method
We conclude with a discussion of ways the modeling approach might be applied	background
along with caveats from limitations	background
and directions for future work	background
In each of three MOOCs we find evidence that participation in two to four subcommunities out of the twenty is associated with significantly higher or lower dropout rates than average	finding
illustrates how the learned models can be used as a lens for understanding the values and focus of discussions within the subcommunities	finding
and in the illustrative example to think about the association between those and detected higher or lower dropout rates than average in the three courses	finding
demonstrates that the patterns that emerge make sense : It associates evidence of stronger expressed motivation to actively participate in the course as well as evidence of stronger cognitive engagement with the material in subcommunities associated with lower attrition	finding
and the opposite in subcommunities associated with higher attrition	finding
In this paper	mechanism
we describe a novel methodology	mechanism
grounded in techniques from the field of machine learning	mechanism
with an eye towards application in the threaded discussions of massive open online courses ( MOOCs )	mechanism
This modeling approach integrates two simpler	mechanism
well established prior techniques	mechanism
namely one related to social network structure and another related to thematic structure of text	mechanism
As an illustrative application of the integrated techniques use and utility	mechanism
We then use a survival model to measure the impact of participation in identified subcommunities on attrition along the way for students who have participated in the course discussion forums of the three courses	mechanism
we use it as a lens for exploring student dropout behavior in three different MOOCs	method
In particular we use the model to identify twenty emerging subcommunities within the threaded discussions of each of the three MOOCs A qualitative post-hoc analysis Our qualitative analysis	method
Given a large number of taxi trajectories	background
we would like to find interesting and unexpected patterns from the data	background
How can we summarize the major trends	background
and how can we spot anomalies ? The anal- ysis of trajectories has been an issue of considerable interest with many applications such as tracking trails of migrating animals and predicting the path of hurricanes	background
In fact F-Trail does produce concise	finding
informative and interesting patterns	finding
we develop a novel method	mechanism
called F-Trail w hich al- lows us Our approach has the following advantages : ( a ) it is fast	mechanism
and scales linearly on the input size	mechanism
( b ) it is effective	mechanism
leading to novel discoveries	mechanism
and surprising outliers	mechanism
We demonstrate the effectiveness of our approach	method
by performing exper- iments on real taxi trajectories	method
The present invention discloses CrowdScape	mechanism
a system that through interactive visualization and mixed initiative machine learning	mechanism
The system combines information about worker behavior with worker outputs and aggregate worker behavioral traces to allow the isolation of target worker clusters This approach allows users to develop and test their mental models of tasks and worker behaviors	mechanism
and then ground those models in worker outputs and majority or gold standard verifications	mechanism
The increasing performance of modern processors makes virtualization a viable solution for consolidating real-time systems into a single hardware platform	background
Our experimental results indicate that vINT achieves timely interrupt handling while providing as good task schedulability as when it is not used	finding
shows that vINT yields significant benefits in reducing interrupt handling time and in protecting real-time tasks against interrupt storms permeating into the virtual machine	finding
In this paper	mechanism
we propose vINT	mechanism
an interrupt handling scheme vINT provides a pseudo-VCPU abstraction dedicated for interrupt handling	mechanism
which overcomes the limits imposed by the timing parameters of virtual CPUs in an analyzable way vINT also accounts for and enforces interrupt handling and resulting execution flows within a guest virtual machine vINT does not require any change to the guest OS code	mechanism
so it can be used for virtualizing proprietary	mechanism
closed-source OSs	mechanism
We analyze interrupt handling time as well as VCPU and task schedulability	method
with and without vINT	method
Our case study based on a prototype implementation on the KVM hyper visor	method
Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in incomplete-information games	background
In this paper	mechanism
we show the first general	mechanism
algorithm-agnostic solution quality guarantees and approximate self-trembling equilibria computed in imperfect-recall abstractions	mechanism
when implemented in the original ( perfect-recall ) game	mechanism
Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions where any results had been obtained	mechanism
Further our analysis is tighter in two ways	mechanism
each of which can lead to an exponential reduction in the solution quality error bound We then show that for extensive-form games that satisfy certain properties	mechanism
the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem	mechanism
where the increase in our bound is the distance function This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds We proceed to show a divide in the class of abstraction problems If payoffs are at the same scale at all information sets considered for abstraction	mechanism
the input forms a metric space	mechanism
Conversely if this condition is not satisfied	mechanism
we show that the input does not form a metric space	mechanism
Finally we use these results to experimentally investigate the quality of our bound for single-level abstraction	method
demonstrate the promising performance of our method	finding
both for event detection and evidence recounting	finding
In this work	mechanism
we propose a flexible deep CNN infrastructure	mechanism
namely Deep Event Network ( DevNet )	mechanism
that simultaneously detects pre-defined events and provides key spatial-temporal evidences Taking key frames of videos as input	mechanism
we first detect the event of interest at the video level by aggregating the CNN features of the key frames	mechanism
The pieces of evidences which recount the detection results	mechanism
are also automatically localized	mechanism
both temporally and spatially	mechanism
The challenge is that we only have video level labels	mechanism
while the key evidences usually take place at the frame levels	mechanism
Based on the intrinsic property of CNNs	mechanism
we first generate a spatial-temporal saliency map by back passing through DevNet	mechanism
which then can be used to find the key frames which are most indicative to the event	mechanism
as well as to localize the specific spatial position	mechanism
usually an object	mechanism
in the frame of the highly indicative area	mechanism
Experiments on the large scale TRECVID 2014 MEDTest dataset	method
Formal verification of industrial systems is very challenging	background
due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams	background
More importantly industrial systems are rarely designed for verification	background
but rather for operational needs The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team	background
In this paper we present an overview of our experience using hybrid systems theorem proving an airborne collision avoidance system for airliners scheduled to be operational around 2020 The methods and proof techniques presented here are an overview of the work already presented in [ 8 ]	mechanism
while the evaluation of ACAS X has been significantly expanded and updated to the most recent version of the system	mechanism
run 13	mechanism
Modern day law enforcement banks heavily on the use of commercial off-the-shelf ( COTS ) face recognition systems ( FRS ) as a tool for biometric evaluation and identification	background
However in many real-world scenarios	background
when the face of an individual is occluded or degraded in some way	background
commercial recognition systems fail to accept the face for evaluation or simply return unusable matched faces	background
In these kinds of cases	background
forensic experts rely on image processing techniques and tools	background
to make the face fit to be processed by the commercial recognition systems ( e	background
g	background
use partial face images from another subject to fill in the occluded parts of the face of interest	background
or have a tight crop around the face )	background
Our results indicate that COTS FRS can be sensitive to the subjectivity in facial part swapping and cropping	finding
resulting in inconsistencies in the identification rankings and similarity scores	finding
More specifically we study the change in the rank-1 identification result that is caused by forensic processing of faces-of-interest that are unusable by the commercial recognition systems Further	method
forensic processing of such faces is more of an art and it is extremely difficult to process faces consistently such that there is a predictable effect on the rank-n identification result	method
Third parties play a prominent role in network-based explanations for successful knowledge transfer Third parties can be either shared or unshared Shared third parties signal insider status and have a predictable positive effect on knowledge transfer Unshared third parties	background
however signal outsider status and are believed to undermine knowledge transfer Surprisingly	background
unshared third parties have been ignored in empirical analysis	background
and so we do not know if or how much unshared third parties contribute to the process	background
Our results provide a more complete view of how third parties contribute to knowledge sharing	background
The results also advance our understanding of network-based dynamics defined more broadly	background
results indicate that unshared third parties undermine knowledge sharing	finding
and they also indicate that the magnitude of the negative unshared-third-party effect declines the more unshared third parties overlap in what they know By documenting how knowledge overlap among unshared third parties moderates their negative influence	finding
our results show when the benefits provided by third parties and by bridges i	finding
e	finding
relationships with outsiders will be opposed versus when both can be enjoyed	finding
Using knowledge transfer data from an online technical forum Empirical	method
How much did a network change since yesterday ? How different is the wiring between Bob 's brain ( a left-handed male ) and Alice 's brain ( a right-handed female ) ? Graph similarity with known node correspondence	background
i	background
e	background
the detection of changes in the connectivity of graphs	background
arises in numerous settings	background
showcase the advantages of our method over existing similarity measures	finding
We propose DeltaCon	mechanism
a principled intuitive	mechanism
and scalable algorithm ( e	mechanism
g employees of a company	mechanism
customers of a mobile carrier )	mechanism
Experiments on various synthetic and real graphs Finally	method
we employ DeltaCon to real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs	method
and ( b ) do temporal anomaly detection in the who-emails-whom Enron graph	method
Why does Smith follow Johnson on Twitter ?	background
results show that TagF uncovers different	finding
but explainable reasons why users follow other users	finding
by proposing TagF	mechanism
which analyzes the who-follows-whom network ( matrix ) and the who-tags-whom network ( tensor ) simultaneously Concretely	mechanism
our method decomposes a coupled tensor constructed from these matrix and tensor	mechanism
The experimental on million-scale Twitter networks	method
A lot of real-world data is spread across multiple domains	background
Handling such data has been a challenging task	background
Heterogeneous face biometrics has begun to receive attention in recent years	background
In real-world scenarios	background
many surveillance cameras capture data in the NIR ( near infrared ) spectrum	background
results report state-of-the-art results	finding
by developing a method to reconstruct VIS images in the NIR domain and vice-versa This approach is more applicable to real-world scenarios since it does not involve having to project millions of VIS database images into learned common subspace for subsequent matching We present a cross-spectral joint l 0 minimization based dictionary learning approach to learn a mapping function between the two domains	mechanism
One can then use the function to reconstruct facial images between the domains	mechanism
Our method is open set and can reconstruct any face not present in the training data	mechanism
We present on the CASIA NIR-VIS v2	method
0 database and	method
How often do individuals perform a given communication activity in the Web	background
such as posting comments on blogs or news ? Could we have a generative model to create communication events with realistic inter-event time distributions ( IEDs ) ? Which properties should we strive to match ?	background
reveal that the SFP mimics their properties very well	finding
being corner cases of the proposed Self-Feeding Process ( SFP )	mechanism
We show that the SFP ( a ) exhibits a unifying power	mechanism
which generates power law tails ( including the so-called `` top-concavity '' that real data exhibits )	mechanism
as well as short-term Poisson behavior ; ( b ) avoids the `` i	mechanism
i	mechanism
d fallacy ''	mechanism
which none of the prevailing models have studied before ; and ( c ) is extremely parsimonious	mechanism
requiring usually only one	mechanism
and in general	mechanism
at most two parameters	mechanism
Experiments conducted on eight large	method
diverse real datasets ( e	method
g	method
Youtube and blog comments	method
e-mails SMSs etc )	method
Gaussian processes ( GPs ) are a flexible class of methods with state of the art performance on spatial statistics applications	background
However GPs require O ( n3 ) computations and O ( n2 ) storage	background
and popular GP kernels are typically limited to smoothing and interpolation	background
Using our model	finding
we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts	finding
We propose new scalable Kronecker methods using a Laplace approximation which involves linear conjugate gradients for inference	mechanism
and a lower bound on the GP marginal likelihood for kernel learning	mechanism
Our approach has near linear scaling	mechanism
requiring O ( DnD+1/D ) operations and O ( Dn2/D ) storage	mechanism
for n training data-points on a dense D > 1 dimensional grid	mechanism
Moreover we introduce a log Gaussian Cox process	mechanism
with highly expressive kernels	mechanism
for modelling spatiotemporal count processes	mechanism
The difference is that hybrid games also provide all the features of hybrid systems and discrete games	background
but only deterministic differential equations	background
Differential games instead	background
provide differential equations with continuous-time game input by both players	background
but not the luxury of hybrid games	background
such as mode switches and discrete-time or alternating adversarial interaction	background
This article introduces differential hybrid games	mechanism
which combine differential games with hybrid games	mechanism
In both kinds of games	mechanism
two players interact with continuous dynamics This article augments differential game logic with modalities and introduces differential game invariants and differential game variants	mechanism
Most state-of-the-art action feature extractors involve differential operators	background
which act as highpass filters and tend to attenuate low frequency action information	background
This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices	background
performance on challenging action recognition and event detection tasks Specifically	finding
our method exceeds and is comparable to MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost	finding
we propose a novel feature enhancing technique called Multi-skIp Feature Stacking ( MIFS )	mechanism
which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space	mechanism
MIFS compensates for information lost from using differential operators by recapturing information at coarse scales	mechanism
This recaptured information allows us to match actions at different speeds and ranges of motion	mechanism
We prove that MIFS enhances the learnability of differential-based features exponentially	mechanism
The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods	mechanism
results show significantly improved	mechanism
Experimental the state-of-the-arts on Hollywood2	method
UCF101 and UCF50 datasets state-of-the-arts on HMDB51 and Olympics Sports datasets	method
Biometrics has come a long way over the past decade in terms of technologies and devices that are used to verify user identities	background
Three of the more well studied modalities in this field are the face	background
iris and fingerprint	background
with the latter two reporting very high user identification/verification rates	background
Our approach yields high values of verification rates	finding
which shows the promise of using these modalities as user specific biometric signatures	finding
In this paper	mechanism
we propose using electromyograph ( EMG ) signals as a person 's biometric signature	mechanism
The EMG records the motor unit action potentials ( MUAP ) during any physical motion	mechanism
Keypress timings alone if used as a biometric	mechanism
are very easy to spoof and hence we fuse this modality with EMG signals	mechanism
In order to classify these features	mechanism
we use subspace modeling as well as Bayesian classifiers	mechanism
Our study is done within the context of a person using a keyboard to type a password or any other fixed phrase	method
Along with EMG signals	method
we log key press times for the user and study the feasibility of using this data too as a biometric feature	method
The experiments have been performed within the context of a user typing a fixed pass phrase at a workstation	method
The idea is to monitor both biometric modalities when this action is performed and study user verification across data capture sessions and within capture sessions	method
Generative score spaces provide a principled method to exploit generative information	background
e	background
g	background
data distribution and hidden variables	background
in discriminative classifiers	background
The underlying methodology is to derive measures or score functions from generative models	background
The derived score functions	background
spanning the so-called score space	background
provide features of a fixed dimension for discriminative classification	background
shows that performance of the score space approach coupled with the proposed discriminative learning method is competitive with state-of-the-art classification methods	finding
In this paper	mechanism
we propose a simple yet effective score space We further propose a discriminative learning method by constraining the classification margin over the score space	mechanism
The form of score function allows the formulation of simple learning rules	mechanism
which are essentially the same learning rules for a generative model with an extra posterior imposed over its hidden variables	mechanism
Experimental evaluation of this approach over two generative models	method
A group 's collective action is an outcome of the group 's decision-making process	background
which may be reached by either averaging of the individual preferences or following the choices of certain members in the group	background
Results of those comparisons	finding
We propose a generic statistical framework from the spatio-temporal data of group trajectories	mechanism
where each `` trajectory '' is a sequence of group actions	mechanism
This is achieved by systematically comparing each agent type 's influence on the group actions based on an array of spatio-temporal criteria	method
are then aggregated into a score	method
Bayesian nonparametric models	background
such as Gaussian processes	background
provide a compelling framework for automatic statistical modelling : these models have a high degree of flexibility	background
and automatically calibrated complexity	background
In this paper	mechanism
we create function extrapolation problems and acquire human responses	mechanism
and then design a kernel learning framework We use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels	mechanism
Finally we investigate Occam 's razor in human and Gaussian process based function learning	method
This paper introduces a new proof calculus that is entirely based on uniform substitution	mechanism
a proof rule that substitutes a formula for a predicate symbol everywhere	mechanism
Uniform substitutions make it possible to rely on axioms rather than axiom schemata	mechanism
substantially simplifying implementations	mechanism
Instead of nontrivial schema variables and soundness-critical side conditions on the occurrence patterns of variables	mechanism
the resulting calculus adopts only a finite number of ordinary dL formulas as axioms	mechanism
The static semantics of differential dynamic logic is captured exclusively in uniform substitutions and bound variable renamings as opposed to being spread in delicate ways across the prover implementation In addition to sound uniform substitutions	mechanism
this paper introduces differential forms that make it possible	mechanism
While the growth of the mobile apps market has created significant market opportunities and economic incentives for mobile app developers to innovate	background
it has also inevitably invited other developers to create rip-offs	background
Practitioners and developers of original apps claim that copycats steal the original apps idea and demand and have called for app platforms to take action against such copycats	background
Our study contributes to the growing literature on mobile app consumption by presenting a method to identify copycats and providing evidence of the impact of copycats on an original apps demand	background
Based on the detection results Our final results indicate that the effect of copycats on an original apps demand is determined by the quality and level of imitation of the copycat	finding
High-quality non-deceptive copycats negatively affect demand for the originals	finding
In contrast low-quality	finding
deceptive copycats positively affect demand for the originals	finding
Using a combination of machine learning techniques such as natural language processing	mechanism
latent semantic analysis	mechanism
network-based clustering and image analysis	mechanism
we propose a method	mechanism
we conduct an econometric analysis to determine the impact of copycat apps on the demand for the original apps on a sample of 10	method
100 action game apps by 5	method
141 developers that were released in the iOS App Store over five years	method
Hidden information derived from probabilistic generative models of data distributions can be used to construct features for discriminative classifiers	background
This observation has motivated the development of approaches that attempt to couple generative and discriminative models together for classification	background
this new framework produces a general classification tool with state-of-the-art performance	finding
In this paper	mechanism
we propose a coupling mechanism developed under the PAC-Bayes framework that can fine-tune the generative models and the feature mapping functions iteratively In our approach	mechanism
a stochastic feature mapping	mechanism
which is a function over the random variables of a generative model	mechanism
is derived to generate feature vectors for a stochastic classifier	mechanism
We construct a stochastic classifier over the feature mapping and derive the PAC-Bayes generalization bound for the classifier	mechanism
for both supervised and semi-supervised learning This allows us to jointly learn the feature mapping and the classifier by minimizing the bound with an EM-like iterative algorithm using labeled and unlabeled data	mechanism
The resulting framework integrates the learning of the discriminative classifier and the generative model and allows iterative fine-tuning of the generative models	mechanism
and the feedforward feature mappings based on task performance feedback	mechanism
Our experiments show	method
in three distinct applications	method
Community detection plays a key role in understanding the structure of real-life graphs with impact on recommendation systems	background
load balancing and routing	background
Previous community detection methods look for uniform blocks in adjacency matrices	background
we provide empirical evidence that communities are best represented as having an hyperbolic structure	finding
We show that our method is effective in finding communities with a similar structure to self-declared ones	finding
We report findings	finding
We detail HyCoM - the Hyperbolic Community Model - and show improvements in compression compared to standard methods We also introduce HyCoM-FIT	mechanism
a fast parameter free algorithm	mechanism
However after studying four real networks with ground-truth communities in real social networks	method
including a community in a blogging platform with over 34 million edges in which more than 1000 users established over 300 000 relations	method
our new representation improves the Mean Average Precision ( mAP ) from 27	finding
6 % to 36	finding
8 % for the TRECVID MEDTest 14 dataset and from 34	finding
0 % to 44	finding
6 % for the TRECVID MEDTest 13 dataset	finding
In this paper	mechanism
we propose a discriminative video representation for event detection over a large scale video dataset when only limited hardware resources are available	mechanism
This paper makes two contributions to the inference of CNN video representation	mechanism
First while average pooling and max pooling have long been the standard approaches to aggregating frame level static features	mechanism
we show that performance can be significantly improved by taking advantage of an appropriate encoding method	mechanism
Second we propose using a set of latent concept descriptors as the frame descriptor	mechanism
which enriches visual information while keeping it computationally affordable	mechanism
The integration of the two contributions results in a new state-of-the-art performance in event detection over the largest video datasets	mechanism
Compared to improved Dense Trajectories	method
which has been recognized as the best video representation for event detection	method
We show that our model can outperform state-of-art performances of gated Boltzmann machines ( GBM ) Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information We show it achieves state-of-art performances and possesses the ability to interpolate missing frames	finding
a function that is lacking in GBM	finding
We propose a new neurally-inspired model that can learn and by synthesis process in a predictive coding framework	mechanism
The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes	mechanism
In contrast to standard predictive coding models	mechanism
the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer	mechanism
and is thus more consistent with neurophysiological observations	mechanism
We establish the computational feasibility of this model by demonstrating its ability in several aspects	method
in estimation of contextual information	method
in terms of prediction accuracy in a variety of tasks	method
We propose the use of contracts concisely capturing the conditions for a safe operation in the context of a traffic network This reduces the analysis of flows in the full traffic network to simple arithmetic checks of the local compatibility of the traffic component contracts	mechanism
while retaining higher-fidelity correctness guarantees of the global hybrid systems models that inherits from correct contracts of the hybrid system components	mechanism
We evaluate our approach in a case study of a modular traffic network and a prototypical implementation in a model-based analysis and design tool for traffic flow networks	method
Background Computer-assisted diagnosis of dermoscopic images of skin lesions has the potential to improve melanoma early detection	background
Conclusions Our classifier may aid clinicians in deciding if a skin lesion should be biopsied and can easily be incorporated into a portable tool ( that uses no proprietary equipment ) that could aid clinicians in noninvasively evaluating cutaneous lesions	background
Results The classifier sensitivity for melanoma was 97	finding
4 % ; specificity was 44	finding
2 % in a test set of images	finding
In the reader study	finding
the classifier 's sensitivity to melanoma was higher ( P P Limitations This is a retrospective study using existing images primarily chosen for biopsy by a dermatologist	finding
The size of the test set is small	finding
We sought to evaluate the performance of a novel classifier that uses decision forest classification of dermoscopic images	mechanism
Methods Severity scores were calculated for 173 dermoscopic images of skin lesions with known histologic diagnosis ( 39 melanomas	method
14 nonmelanoma skin cancers	method
and 120 benign lesions )	method
A threshold score was used to measure classifier sensitivity and specificity A reader study was conducted to compare the sensitivity and specificity of the classifier with those of 30 dermatology clinicians	method
Most Lamb wave localization techniques require that we know the waves velocity characteristics ; yet	background
in many practical scenarios	background
velocity estimates can be challenging to acquire	background
are unavailable or are unreliable because of the complexity of Lamb waves	background
We show that both methods can achieve less than 1 cm localization error and have less systematic error than traditional time-of-arrival localization methods	finding
through two novel source localization methods designed for sparse sensor arrays in isotropic media Both methods exploit the fundamental sparse structure of a Lamb wave 's frequencywavenumber representation The first method uses sparse recovery techniques to extract velocities from calibration data	mechanism
The second method uses kurtosis and the support earth movers distance to measure the sparseness of a Lamb waves approximate frequency-wavenumber representation These measures are then used to locate acoustic sources with no prior calibration data	mechanism
We experimentally study each method with a collection of acoustic emission data measured from a 1	method
22 m by 1	method
22 m isotropic aluminum plate	method
and found it was effective at producing reasonable drafts	finding
However the workers often needed more structure and the authors more context	finding
In this paper we introduce a paradigm for completing complex tasks from wearable devices by leveraging crowdsourcing	mechanism
and demonstrate its validity for academic writing We explore this paradigm using a collaborative authoring system	mechanism
called WearWrite which is designed using an Android smartwatch and Google Docs to produce academic papers	mechanism
including this one	mechanism
WearWrite allows expert authors who do not have access to large devices to contribute bits of expertise and big picture direction from their watch	mechanism
while freeing them of the obligation of integrating their contributions into the overall document	mechanism
Crowd workers on desktop computers actually write the document WearWrite addresses these issues by focusing workers on specific tasks and providing select context to authors on the watch	mechanism
We used this approach to write several simple papers We demonstrate the system 's feasibility by writing this paper using it	method
Road intersections are considered to be serious bottlenecks in urban transportation	background
More than 44 % of all reported crashes in U	background
S Occur within intersection areas	background
which in turn lead to 8	background
500 fatalities and approximately 1 million injuries every year Furthermore	background
because traffic traveling in one direction is generally stopped at busy intersections to allow traffic to flow in another direction	background
an intersection creates traffic congestion and frustration	background
The impact of road intersections on traffic delays leads to enormous waste of human and natural resources	background
According to the 2011 Urban Mobility Report	background
the delay endured by the average commuter was 34 hours	background
which costs in aggregate more than $ 100 billion each year in the U	background
S	background
With the advances in Cyber-Physical Systems ( CPS )	background
autonomous driving as a part of Intelligent Transportation Systems ( ITS ) is likely to be at the heart of urban transportation in the future	background
Autonomous vehicles have been demonstrated successfully at the DARPA Urban Challenge	background
General Motors ' Electrical-Networked Vehicle	background
CMU 's autonomous vehicle and Google 's car are just a few other recently unveiled examples	background
results show that we are able to avoid collisions and increase the throughput of the intersections by up to 96	finding
24 % compared to common signalized intersections	finding
Under BRIP the optimal intersection capacity utilization of 100 % is achievable in certain cases	finding
In this paper	mechanism
we propose a spatio-temporal technique called the Ballroom Intersection Protocol ( BRIP ) BRIP aims to maximize the utilization of the capacity of the intersection area by increasing parallelism	mechanism
By enforcing a synchronized arrival of autonomous vehicles at intersections	mechanism
BRIP allows vehicles approaching from all directions to simultaneously and continuously cross without stopping behind or inside the intersection area	mechanism
Our simulation	method
The harmful effects of cell phone usage on driver behavior have been well investigated and the growing problem has motivated several several research efforts aimed at developing automated cell phone usage detection systems	background
Computer vision based approaches for dealing with this problem have only emerged in recent years	background
demonstrate the method 's efficacy	finding
In this paper	mechanism
we present a vision based method To the best of our knowledge	mechanism
this is the first such evaluation carried out using this relatively new data	mechanism
Our approach utilizes the Supervised Descent Method ( SDM ) based facial landmark tracking algorithm to track the locations of facial landmarks in order to extract a crop of the region of interest	mechanism
Following this features are extracted from the crop and are classified using previously trained classifiers in order to determine if a driver is holding a cell phone	mechanism
and quantitatively on challenging Strategic Highway Research Program ( SHRP2 ) face view videos from the head pose validation data that was acquired to monitor driver head pose variation under naturalistic driving conditions	method
We adopt a through approach and benchmark the performance obtained using raw pixels and Histogram of Oriented Gradients ( HOG ) features in combination with various classifiers	method
The use of deductive techniques	background
such as theorem provers	background
has several advantages in safety verification of hybrid systems ; however	background
state-of-the-art theorem provers require manual intervention to handle complex systems	background
This paper presents an extension to KeYmaera	mechanism
a deductive verification tool ; the new technique using system designer intuition about performance within particular modes as part of a proof task	mechanism
Our approach allows the theorem prover to leverage forward invariants	mechanism
discovered using numerical techniques	mechanism
as part of a proof of safety	mechanism
We introduce a new inference rule into the proof calculus of KeYmaera	mechanism
the forward invariant cut rule	mechanism
and we present a methodology	mechanism
which are then used with the new cut rule	mechanism
We demonstrate how our new approach can be used to complete verification tasks that lie out of the reach of existing automatic verification approaches using several examples	method
including one involving an automotive powertrain control system	method
In recent years	background
many lawsuits have been filed by individuals seeking legal redress for harms caused by the loss or theft of their personal information	background
By providing the first comprehensive empirical analysis of data breach litigation	background
our findings offer insight into the debate over privacy litigation versus privacy regulation	background
Our results suggest that the odds of a firm being sued are 3	finding
5 times greater when individuals suffer financial harm	finding
but 6 times lower when the firm provides free credit monitoring Moreover	finding
defendants settle 30 percent more often when plaintiffs allege financial loss	finding
or when faced with a certified class action suit	finding
Using a unique and manually collected database	method
we analyze court dockets for more than 230 federal data breach lawsuits from 2000 to 2010	method
Epidemics in large complete networks is well established	background
We establish the fluid limit macroscopic dynamics of a multi-virus spread over a multipartite network as the number of nodes at each partite or island grows large	mechanism
The virus spread follows a peer-to-peer random rule of infection in line with the Harris contact process The model conforms to an SIS ( susceptible-infected-susceptible ) type	mechanism
where a node is either infected or it is healthy and prone to be infected The local ( at node level ) random infection model induces the emergence of structured dynamics at the macroscale	mechanism
We apply a novel semantic scan statistic approach Our semantic scan approach successfully addresses this problem	mechanism
eliminates the need for classifying cases into pre-defined syndromes and identifies emerging clusters that public health officials could not have predicted in advance	mechanism
we discovered that duplicate points create subtle issues	finding
that the literature has ignored : if d max is the multiplicity of the most over-plotted point	finding
typical algorithms are quadratic on d max	finding
we report wall-clock times and our time savings ; and we show that our methods give either exact results	finding
or highly accurate approximate ones	finding
We propose several ways ;	mechanism
After careful analysis	method
Instant access to computing	background
when and where we need it	background
has long been one of the aims of research areas such as ubiquitous computing	background
In this paper	mechanism
we describe the WorldKit system	mechanism
which makes use of a paired depth camera and projector Using this system	mechanism
touch-based interactivity can	mechanism
without prior calibration	mechanism
be placed on nearly any unmodified surface literally with a wave of the hand	mechanism
as can other new forms of sensed interaction From a user perspective	mechanism
such interfaces are easy enough to instantiate that they could	mechanism
if desired be recreated or modified `` each time we sat down '' by `` painting '' them next to us From the programmer 's perspective	mechanism
our system encapsulates these capabilities in a simple set of abstractions that make the creation of interfaces quick and easy	mechanism
Further it is extensible to new	mechanism
custom interactors in a way that closely mimics conventional 2D graphical user interfaces	mechanism
hiding much of the complexity of working in this new domain	mechanism
We detail the hardware and software implementation of our system	method
and several example applications built using the library	method
We propose using the statistical measurement of the sample skewness of the distribution of mean firing rates of a tuning curve For some features	mechanism
like binocular disparity	mechanism
tuning curves are best described by relatively complex and sometimes diverse functions	mechanism
making it difficult to quantify sharpness with a single function and parameter	mechanism
Skewness provides a robust nonparametric measure of tuning curve sharpness that is invariant with respect to the mean and variance of the tuning curve and is straightforward to apply to a wide range of tuning	mechanism
including simple orientation tuning curves and complex object tuning curves that often can not even be described parametrically	mechanism
Because skewness does not depend on a specific model or function of tuning	mechanism
it is especially appealing to cases of sharpening where recurrent interactions among neurons produce sharper tuning curves that deviate in a complex manner from the feedforward function of tuning	mechanism
Since tuning curves for all neurons are not typically well described by a single parametric function	mechanism
this model independence additionally allows skewness to be applied to all recorded neurons	mechanism
maximizing the statistical power of a set of data We also compare skewness with other nonparametric measures of tuning curve sharpness and selectivity Compared to these other nonparametric measures tested	mechanism
skewness is best used for capturing the sharpness of multimodal tuning curves defined by narrow peaks maximum and broad valleys minima Finally	mechanism
we provide a more formal definition of sharpness using a shape-based information gain measure and derive and show that skewness is correlated with this definition	mechanism
How can we describe a large	background
dynamic graph over time ? Is it random ? If not	background
what are the most apparent deviations from randomness -- a dense block of actors that persists over time	background
or perhaps a star with many satellite nodes that appears with some fixed periodicity ? In practice	background
these deviations indicate patterns -- for example	background
botnet attackers forming a bipartite core with their victims over the duration of an attack	background
family members bonding in a clique-like fashion over a difficult period of time	background
or research collaborations forming and fading away over the years	background
We show that TIMECRUNCH is able to compress these graphs	finding
( a ) formulation : we show how to formalize this problem as minimizing the encoding cost in a data compression paradigm	mechanism
( b ) algorithm : we propose TIMECRUNCH	mechanism
an effective scalable and parameter-free method for finding coherent	mechanism
temporal patterns in dynamic graphs and ( c ) practicality : by summarizing important temporal structures and finds patterns that agree with intuition	mechanism
we apply our method to several large	method
diverse real-world datasets with up to 36 million edges and 6	method
3 million nodes	method
Generating three-dimensional ( 3D ) as-is Building Information Models ( BIMs )	background
representative of the existing conditions of buildings	background
from point cloud data collected by laser scanners is becoming common practice	background
The results show that the deviation analysis method is capable of identifying almost six times more errors with more than 40 % time savings compared to the physical measurement method	finding
This paper presents a method by analyzing the patterns of geometric deviations between the model and the point cloud data The fundamental assumption is that the point cloud and the as-is BIM generated from the point cloud should corroborate in the depiction of the components and their spatial attributes Major geometric deviations between as-is models and point clouds can indicate potential errors introduced during data collection	mechanism
processing and/or model generation The research described in this paper provides a taxonomy for patterns of deviations and sources of errors and demonstrates that it is possible to identify the source	mechanism
magnitude and nature of errors by analyzing the deviation patterns	mechanism
The method is validated through a comparison with the currently adopted physical measurement method in a case study	method
Learning whether motor	background
sensory or cognitive	background
requires networks of neurons to generate new activity patterns	background
As some behaviours are easier to learn than others1	background
2 These results suggest that the existing structure of a network can shape learning	background
On a timescale of hours	background
it seems to be difficult to learn to generate neural activity patterns that are not consistent with the existing network structure These findings offer a network-level explanation for the observation that we are more readily able to learn new skills when they are related to the skills that we already possess3	background
4	background
Here we show that the animals could readily learn to proficiently control the cursor using neural activity patterns that were within the intrinsic manifold	finding
However animals were less able to learn to proficiently control the cursor using activity patterns that were outside of the intrinsic manifold	finding
We employed a closed-loop intracortical braincomputer interface learning paradigm in which Rhesus macaques ( Macaca mulatta ) controlled a computer cursor by modulating neural activity patterns in the primary motor cortex	mechanism
Using the braincomputer interface paradigm	mechanism
we could specify and alter how neural activity mapped to cursor velocity	mechanism
The activity of a neural population can be represented in a high-dimensional space ( termed the neural space )	mechanism
wherein each dimension corresponds to the activity of one neuron	mechanism
These characteristic activity patterns comprise a low-dimensional subspace ( termed the intrinsic manifold ) within the neural space	mechanism
The intrinsic manifold presumably reflects constraints imposed by the underlying neural circuitry	mechanism
At the start of each session	method
we observed the characteristic activity patterns of the recorded neural population	method
AbstractUnderstanding the value that individuals assign to the protection of their personal data is of great importance for business	background
law and public policy	background
The results highlight the sensitivity of privacy valuations to contextual	background
nonnormative factors	background
Individuals assigned markedly different values to the privacy of their data depending on ( 1 ) whether they were asked to consider how much money they would accept to disclose otherwise private information or how much they would pay to protect otherwise public information and ( 2 ) the order in which they considered different offers for their data The gap between such values is large compared with that observed in comparable studies of consumer goods	finding
We use a field experiment informed by behavioral economics and decision research	method
Modern offices are crowded with personal computers	background
While studies have shown these to be idle most of the time	background
they remain powered	background
consuming up to 60p of their peak power Hardware-based solutions engendered by PC vendors ( e	background
g	background
low-power states Wake-on-LAN ) have proved unsuccessful because	background
in spite of user inactivity	background
these machines often need to remain network active in support of background applications that maintain network presence	background
can deliver 44 -- 91p energy savings during idle periods of at least 10 minutes	finding
while providing low migration latencies of about 4 seconds and migrating minimal state that is under an order of magnitude of the VMs memory footprint	finding
We present partial VM migration	mechanism
an approach It creates a partial replica of the desktop VM on the consolidation server by copying only VM metadata	mechanism
and it transfers pages to the server on-demand	mechanism
as the VM accesses them	mechanism
This approach places desktop PCs in low-power mode when inactive and switches them to running mode when pages are needed by the VM running on the consolidation server	mechanism
To ensure that desktops save energy	mechanism
we have developed sleep scheduling and prefetching algorithms	mechanism
as well as the context-aware selective resume framework	mechanism
a novel approach to reduce the latency of power mode transition operations in commodity PCs	mechanism
Jettison our software prototype of partial VM migration for off-the-shelf PCs	method
Event detection from real surveillance videos with complicated background environment is always a very hard task	background
By virtue of such easily-distinguished mid-level patterns	mechanism
our framework realizes an effective labor division between computers and human participants	mechanism
The task of computers is to train classifiers on a bunch of mid-level discriminative representations	mechanism
and to sort all the possible mid-level representations in the evaluation sets based on the classifier scores	mechanism
The task of human participants is then to readily search the events based on the clues offered by these sorted mid-level representations	mechanism
For computers such mid-level representations	mechanism
with more concise and consistent patterns	mechanism
can be more accurately detected than video fragments utilized in the conventional framework	mechanism
and on the other hand	mechanism
a human participant can always much more easily search the events of interest implicated by these location-anchored mid-level representations than conventional video fragments containing entire scenes	mechanism
Both of these two properties facilitate the availability of our framework in real surveillance event detection applications	mechanism
These problems are motivated by the LASSO framework and have applications in machine learning and computer vision	background
While this connection demonstrates the difficulties of obtaining runtime guarantees	mechanism
it also suggests an approach of using techniques originally developed for graph algorithms We then show that most of these problems can be formulated as a grouped least squares problem	mechanism
and give efficient algorithms for this formulation Our algorithms rely on routines for solving quadratic minimization problems	mechanism
which in turn are equivalent to solving linear systems	mechanism
Some preliminary experimental work on image processing tasks are also presented	method
Fluctuations in the growth rate of a bacterial culture during unbalanced growth are generally considered undesirable in quantitative studies of bacterial physiology Under well-controlled experimental conditions	background
however these fluctuations are not random but instead reflect the interplay between intra-cellular networks underlying bacterial growth and the growth environment	background
Our method has implications for both basic understanding of bacterial physiology and for the classification of bacterial strains	background
Here we present a method by time-frequency analysis of unbalanced growth curves measured with high temporal resolution	mechanism
The signatures are then applied to differentiate amongst different bacterial strains or the same strain under different growth conditions	mechanism
and to identify the essential architecture of the gene network underlying the observed growth dynamics	mechanism
At the core of Machine Learning ( ML ) analytics is often an expert-suggested model	background
whose parameters are refined by iteratively processing a training dataset until convergence	background
The completion time ( i	background
e	background
convergence time ) and quality of the learned model not only depends on the rate at which the refinements are generated but also the quality of each refinement	background
show that our mechanism significantly improves upon static communication schedules	finding
This paper presents Bosen	mechanism
a system that maximizes the network communication efficiency under a given inter-machine network bandwidth budget to minimize parallel error	mechanism
while ensuring theoretical convergence guarantees for large-scale data-parallel ML applications	mechanism
Furthermore Bosen prioritizes messages most significant to algorithm convergence	mechanism
further enhancing algorithm convergence	mechanism
Finally Bosen is the first distributed implementation of the recently presented adaptive revision algorithm	mechanism
which provides orders of magnitude improvement over a carefully tuned fixed schedule of step size refinements for some SGD algorithms	mechanism
Experiments on two clusters with up to 1024 cores	method
Autonomous driving will play an important role in the future of transportation	background
Various autonomous vehicles have been demonstrated at the DARPA Urban Challenge [ 3 ]	background
General Motors has recently unveiled their Electrical-Networked Vehicles ( EN-V ) in Shanghai	background
China [ 5 ]	background
One of the main challenges of autonomous driving in urban areas is transition through cross-roads and intersections	background
In addition to safety concerns	background
current intersection management technologies such as stop signs and traffic lights can introduce significant traffic delays even under light traffic conditions	background
and show significant improvements in throughput	finding
We also prove that our protocols avoid deadlock situations inside the intersection area	finding
results show that our new proposed V2V intersection protocols provide both safe passage through the intersection and significantly decrease the delay at the intersection and our latest V2V intersection protocol yields over 85 % overall performance improvement over the common traffic light models	finding
Our goal is to design and develop efficient and reliable intersection protocols	mechanism
We study how our proposed V2V intersection protocols can be beneficial for autonomous driving The simulation	method
Massive Open Online Courses ( MOOCs ) enable everyone to receive high-quality education	background
show that ACD and PCD can detect usage of a cheat sheet with good accuracy and can reduce the overall human resources required to monitor MOOCs for cheating	finding
In this paper	mechanism
we propose a Massive Open Online Proctoring ( MOOP ) framework	mechanism
which combines both automatic and collaborative approaches The MOOP framework consists of three major components : Automatic Cheating Detector ( ACD )	mechanism
Peer Cheating Detector ( PCD )	mechanism
and Final Review Committee ( FRC )	mechanism
ACD uses webcam video or other sensors to monitor students and automatically flag suspected cheating behavior	mechanism
Ambiguous cases are then sent to the PCD	mechanism
where students peer-review flagged webcam video to confirm suspicious cheating behaviors	mechanism
Finally the list of suspicious cheating behaviors is sent to the FRC to make the final punishing decision	mechanism
Our experiment	method
Visible light communication ( VLC ) between LED light bulbs and smart-phone cameras has already begun to gain traction for identification and indoor localization applications	background
To support detection by cameras	background
the frequencies and data rates are typically limited to below 1kHz and tens of bytes per second ( Bps )	background
We show that we are able to reliably simultaneously transmit low-speed data at 1	finding
3 Bps to camera enabled devices and higher-speed data at 104 Bps to low-power embedded devices that consumes less then 204 uA and can be triggered in less then 10ms	finding
In this paper	mechanism
we present a technique used for interior ambient lighting in a manner that is imperceptible to occupants This allows the camera communication VLC channel to also act as a higher speed downstream link and low-power wakeup mechanism for energy-constrained devices Our approach uses Manchester encoding and Binary Frequency Shift Keying ( BFSK ) to modulate the high-speed data stream and applies duty-cycle adjustment to generate the slower camera communication signal	mechanism
We explore the trade-off between the performance of the two communication channels Our hybrid communication protocol is also compatible with existing IR receivers	mechanism
This allows lights to communicate with low-cost commodity chipsets and control home appliances such as TVs	mechanism
AV receivers AC window units	mechanism
etc Since the majority of energy in many RF communication protocols often goes towards media access and receiving	mechanism
VLC-triggered wakeup can significantly decrease system energy consumption	mechanism
We also demonstrate a proof-of-concept wakeup circuit	method
Consider networks in harsh environments	background
where nodes may be lost due to failure	background
attack or infection -- how is the topology affected by such events ?	background
We propose a new generative model of network evolution in dynamic and harsh environments	mechanism
Our model can reproduce the range of topologies observed across known robust and fragile biological networks	mechanism
as well as several additional transport	mechanism
communication and social networks	mechanism
We also develop a new optimization measure based on preserving high connectivity following random or adversarial bursty node loss	mechanism
propose a new distributed algorithm	mechanism
Using this measure	method
we evaluate the robustness of several real-world networks and	method
Such datasets arise in many social	background
economic biological and physical networks As a potential application of the graph Fourier transform	background
we consider the efficient representation of structured data that utilizes the sparseness of graph signals in the frequency domain	background
We demonstrate their relation to the generalized eigenvector basis of the graph adjacency matrix	finding
We propose a novel discrete signal processing framework Our framework extends traditional discrete signal processing theory to structured datasets by viewing them as signals represented by graphs	mechanism
so that signal coefficients are indexed by graph nodes and relations between them are represented by weighted graph edges We discuss the notions of signals and filters on graphs	mechanism
and define the concepts of the spectrum and Fourier transform for graph signals	mechanism
and study their properties	method
Computational methods have been widely used to infer properties of complex systems that one can not directly observe experimentally	background
Viral capsid assembly is a key model system for complex self-assembly for which we lack direct experimental data on critical information	background
such as kinetic parameters	background
needed to build models and reveal detailed assembly pathways	background
We previously sought to learn such hidden parameters with a heuristic optimization approach using gradient and response surface methods applied to the light scattering measurements of three in vitro viral assembly systems : human papillomavirus ( HPV )	background
hepatitis B virus ( HBV )	background
and cowpea chlorotic mottle virus ( CCMV )	background
This method successfully learned plausible kinetic parameters for all the three viruses leading to reconstruction of detailed models of assembly pathways Work is continuing on evaluating different DFO methods and customizing them to inference of kinetic parameters in order to determine the best strategies for inferring unobservable physical parameters in complex biological self-assembly systems	background
Preliminary tests show improvements over our custom gradient-based method using a DFO strategy	finding
We explore here improvements based on the idea of derivative free optimization ( DFO )	mechanism
a class of optimization algorithm that can achieve faster and more accurate fitting	mechanism
especially on systems characterized by costly	mechanism
noisy evaluations of quality of fit	mechanism
Background : Intracortical electrode arrays that can record extracellular action potentials from small	background
targeted groups of neurons are critical for basic neuroscience research and emerging clinical applications In general	background
these electrode devices suffer from reliability and variability issues	background
which have led to comparative studies of existing and emerging electrode designs to optimize performance	background
Conclusions : A more extensive spatial and temporal insight into the chronic electrophysiological performance over time will help uncover the biological and mechanical failure mechanisms of the neural electrodes and direct future research toward the elucidation of design optimization for specific applications	background
Results For example	finding
performance metrics in Layer V and stratum pyramidale were initially higher than Layer II/III	finding
but decrease more rapidly	finding
On the other hand	finding
Layer II/III maintained recording metrics longer In addition	finding
chronic changes at the level of layer IV are evaluated using visually evoked current source density	finding
The use of MU and LFP activity for evaluation and tracking biological depth provides a more comprehensive characterization of the electrophysiological performance landscape of microelectrodes	finding
New method : In this study	mechanism
we optimize the methods and parameters in eight mice visual cortices	mechanism
These findings quantify the large recording differences stemming from anatomical differences in depth and the layer dependent relative changes to SU and MU recording performance over 6-months	method
Comparison with existing method ( s ) :	method
Multimedia event detection ( MED ) is an emerging area of research	background
The results show that our approach outperforms several other state-of-the-art detection algorithms	finding
Moreover our solution only uses few positive examples since precisely labeled multimedia content is scarce in the real world	mechanism
As the information from these few positive examples is limited	mechanism
we propose using knowledge adaptation to facilitate event detection Different from the state of the art	mechanism
our algorithm is able to adapt knowledge from another source for MED even if the features of the source and the target are partially different	mechanism
but overlapping	mechanism
Avoiding the requirement that the two domains are consistent in feature types is desirable as data collection platforms change or augment their capabilities and we should be able to respond to this with little or no effort	mechanism
We perform extensive experiments on real-world multimedia archives consisting of several challenging events	method
ABSTRACT In spite of their many advantages	background
real -world application of guided -waves for structural health monitoring ( SHM ) of pipelines is still quite limited	background
The challenges can be discussed under three headings : ( 1 ) Multiple m odes	background
( 2 ) Multi -path reflections	background
and ( 3 ) Sensitivity to environmental and operational conditions ( EOCs ) These challenges are UHYLHZHG LQ WKH DXWKRUV SUHYLRXV ZRUN This paper is part of a study whose objective is to overcome these challenges for damage diagnosis of pipes	background
while addressing the limitations of the current approaches	background
That is develop methods that simplify signal while retaining damage information	background
and perform well as EOC s vary	background
Moreover the potential of the proposed met hod for online monitoring is illustrated	background
for wide range of temperature variations and different damage scenarios	background
The results suggest that	finding
for practical ranges of monitoring and damage sizes of interest	finding
the proposed method has low sensitivity to such training factors	finding
High detection performances are obtained for temperature differences up to 14 (	finding
The finding s reported in this paper suggest that although the proposed method is a supervised approach	finding
labeling of the training data does not require prior knowledge about the damage characteristics ( e	finding
g	finding
size location )	finding
In this paper	mechanism
a s upervised method is proposed That is	mechanism
a discriminant vector is calculated so that the projection s of undamaged and damaged pipes on this vector is separated	mechanism
In the training stage	mechanism
data is recorded from intact pipe	mechanism
and from a pipe with an artificial structural abnormality ( to simulate any variation from intact condition )	mechanism
During the monitoring stage	mechanism
test signals are projected on the discriminant vector	mechanism
and these projections are used as damage -sensitive features for detection purposes	mechanism
Being a supervised metho d	mechanism
factors such as EOC variations	mechanism
and difference in the characteristics of the structural abnormality in training and test data	mechanism
may affect the detection performance	mechanism
This paper reports the experiments investigating the extent to which the differences in damage size and damage location	method
as well as temperatures	method
can influence the discriminatory power of the extracted damage -sensitive features	method
We present an adaptive graph filtering approach Adaptive graph filters combine decisions from multiple graph filters using a weighting function that is optimized in a semi-supervised manner	mechanism
We also demonstrate the multiresolution property of adaptive graph filters by connecting them to the diffusion wavelets	mechanism
In our experiments	method
we apply the adaptive graph filters to the classification of online blogs and damage identification in indirect bridge structural health monitoring	method
Most noticeably the accuracy of our algorithm reaches 51	finding
8 % on the challenging HMDB dataset which outperforms the state-of-the-art of 7	finding
3 % relatively	finding
We propose a novel content driven pooling that leverages space-time context while being robust toward global space-time transformations	mechanism
Being robust to such transformations is of primary importance in unconstrained videos where the action localizations can drastically shift between frames	mechanism
Our pooling identifies regions of interest using video structural cues estimated by different saliency functions To combine the different structural information	mechanism
we introduce an iterative structure learning algorithm	mechanism
WSVM ( weighted SVM )	mechanism
that determines the optimal saliency layout of an action model through a sparse regularizer	mechanism
A new optimization method is proposed to solve the WSVM ' highly non-smooth objective function	mechanism
We evaluate our approach on standard action datasets ( KTH	method
UCF50 and HMDB )	method
Large scale integration of stochastic energy resources in power systems requires probabilistic analysis approaches for comprehensive system analysis	background
The large-varying grid condition on the aging and stressed power system infrastructures also requires merging of offline security analyses into online operation Meanwhile in computing	background
the recent rapid hardware performance growth comes from the more and more complicated architecture	background
Given the challenges and opportunities in both the power system and the computing fields	mechanism
this paper presents the unique commodity high performance computing system solutions 1 ) a high performance Monte Carlo simulation ( MCS ) based distribution probabilistic load flow solver 2 ) A high performance MCS based transmission probabilistic load flow solver 3 ) A SIMD accelerated based on Woodbury matrix identity on multi-core CPUs By aggressive algorithm level and computer architecture level performance optimizations including optimized data structures	mechanism
optimization for superscalar out-of-order execution	mechanism
SIMDization and multi-core scheduling	mechanism
our software fully utilizes the modern commodity computing systems	mechanism
makes the critical and computational intensive power system probabilistic and security analysis problems solvable in real-time on commodity computing systems	mechanism
Lamb waves are powerful tools in nondestructive evaluation and structural health monitoring	background
Researchers use Lamb waves to detect and locate damage across large areas	background
To best utilize Lamb waves	background
they are analyzed through two processing steps : baseline subtraction and velocity calibration	background
Baseline subtraction removes background information from our data and velocity calibration tunes our algorithms Yet	background
in many scenarios	background
these steps are challenging to implement	background
We show these combined approaches to be effective	finding
we present two approaches that combine environmental compensation with self-calibrating localization We discuss temperature compensation strategies based on the scale transform and singular value decomposition We then integrate these with a localization framework known as data-driven matched field processing	mechanism
in a variety of scenarios	method
Modern robots like todays smartphones	background
are complex devices with intricate software systems	background
This paper focuses on teaching with Tekkotsu	mechanism
an open source robot application development framework designed specifically for education But	mechanism
the curriculum described here can also be taught using ROS	mechanism
the Robot Operating System that is now widely used for robotics research	mechanism
As airspace becomes ever more crowded	background
air traffic management must reduce both space and time between aircraft to increase throughput	background
making on-board collision avoidance systems ever more important	background
These safety-critical systems must be extremely reliable	background
and as such	background
many resources are invested into ensuring that the protocols they implement are accurate	background
Still it is challenging to guarantee that such a controller works properly under every circumstance	background
This is an important step in formally verified	background
flyable and distributed air traffic control	background
We prove that the controllers never allow the aircraft to get too close to one another	finding
even when new planes approach an in-progress avoidance maneuver that the new plane may not be aware of	finding
Because these safety guarantees always hold	finding
the aircraft are protected against unexpected emergent behavior which simulation and testing may miss	finding
We consider a class of distributed collision avoidance controllers designed to work even in environments with arbitrarily many aircraft or UAVs	method
Brand Associations one of central concepts in marketing	background
describe customers ' top-of-mind attitudes or feelings toward a brand	background
Thus this consumer-driven brand equity often attains the grounds for purchasing products or services of the brand	background
we demonstrate that our approach can discover complementary views on the brand associations that are hardly mined from the text data show that our approach outperforms other candidate methods on the both visualization tasks	finding
As a first technical step toward the study of photo-based brand associations	mechanism
we aim to jointly achieve the following two visualization tasks in a mutually-rewarding way : ( i ) detecting and visualizing core visual concepts associated with brands	mechanism
and ( ii ) localizing the regions of brand in the images	mechanism
With experiments on about five millions of images of 48 brands crawled from five popular online photo sharing sites	method
We also quantitatively	method
the importance of studying the implications of sampling is twofold : First	background
sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second	background
because every such data collection can be perceived as a sample of the real world To the best of our knowledge	background
our work represents the largest study of propagation patterns of executables	background
We discover the SharkFin temporal propagation pattern of executable files	mechanism
the GeoSplit pattern in the geographical spread of machines that report executables to Symantecs servers	mechanism
the Periodic Power Law ( Ppl ) distribution of the lifetime of URLs	mechanism
and we show how	mechanism
by analyzing patterns from 22 million malicious ( and benign ) files	method
found on 1	method
6 million hosts worldwide during the month of June 2011 We conduct this study using the WINE database available at Symantec Research Labs Additionally	method
we explore the research questions raised by sampling on such large databases of executables ; We further investigate the propagation pattern of benign and malicious executables	method
unveiling latent structures in the way these files spread	method
and outline ideas for future research in this	background
We demonstrate that crowd storage is feasible	finding
This paper introduces the concept of crowd storage Similar to human memory	mechanism
crowd storage is ephemeral	mechanism
which means that storage is temporary and the quality of the stored information degrades over time	mechanism
Crowd storage may be preferred over storing information directly in the cloud	mechanism
or when it is desirable for information to degrade inline with normal human memories	mechanism
we created WeStore	mechanism
a system that stores and then later retrieves digital files in the existing memories of crowd workers	mechanism
WeStore does not store information directly	mechanism
but rather encrypts the files using details of the existing memories elicited from individuals within the crowd as cryptographic keys	mechanism
The fidelity of the retrieved information is tied to how well the crowd remembers the details of the memories they provided	mechanism
using an existing crowd marketplace ( Amazon Mechanical Turk )	method
explore design considerations important for building systems that use crowd storage area	method
Our approach is not limited to images	background
but they provide a convenient query space to test search optimizations	background
We present a cloud-based approach that is sensitive to bandwidth and energy constraints Our approach is inspired by the long-established practice of photographers using contact sheets to rapidly visualize a new collection of photographs	mechanism
and then selecting a subset on which to focus attention	mechanism
On behalf of each smartphone	mechanism
the cloud maintains a virtual contact sheet of images that have been captured but not yet uploaded	mechanism
The virtual contact sheet consists of a set of low-fidelity images as well as full or partial meta-data associated with each image	mechanism
If search processing on the cloud indicates that a particular low-fidelity object is relevant	mechanism
then its full-fidelity image can be obtained just-in-time from the corresponding smartphone for further search processing or presentation to the user	mechanism
Autonomous driving is likely to be the heart of urban transportation in the future Autonomous vehicles have the potential to increase the safety of passengers and also to make road trips shorter and more enjoyable	background
As the first steps toward these goals	background
many car manufacturers are investing in designing and equipping their vehicles with advanced driver-assist systems	background
Road intersections are considered to be serious bottlenecks of urban transportation	background
as more than 44 % of all reported crashes in U	background
S	background
occur within intersection areas which in turn lead to 8	background
500 fatalities and approximately 1 million injuries every year	background
Furthermore the impact of road intersections on traffic delays leads to enormous waste of human and natural resources	background
We show that	finding
in addition to intersections	finding
these protocols are also applicable to vehicle crossings at roundabouts	finding
results show that we are able to avoid collisions and also increase the throughput of the intersections up to 87	finding
82 % compared to common traffic-light signalized intersections	finding
We have designed and developed efficient and reliable intersection protocols In this paper	mechanism
we introduce new V2V intersection protocols and suggest required modifications	mechanism
We have been investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous driving	method
Additionally we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols Our simulation	method
Parameterized probabilistic complex computational ( P 2 C 2 ) models are being increasingly used in computational systems biology for analyzing biological systems	background
A key challenge is to build mechanistic P 2 C 2 models by combining prior knowledge and empirical data	background
given that certain system properties are unknown	background
that guarantee a set of desired clinical outcomes with high probability	finding
We present a new algorithmic procedure Our approach uses Bayesian model checking	mechanism
sequential hypothesis testing	mechanism
and stochastic optimization to synthesize parameters of P 2 C 2 models	mechanism
We demonstrate our algorithm by discovering the amount and schedule of doses of bacterial lipopolysaccharide in a clinical agent-based model of the dynamics of acute inflammation	method
we demonstrate that the proposed joint summarization approach outperforms other baselines and our own methods using videos or images only	finding
Starting from the intuition that the characteristics of the two media types are different yet complementary	mechanism
we develop a fast and easily-parallelizable approach The storyline graphs can illustrate various events or activities associated with the topic in a form of a branching network	mechanism
The video summarization is achieved by diversity ranking on the similarity graphs between images and video frames	mechanism
The reconstruction of storyline graphs is formulated as the inference of sparse time-varying directed graphs from a set of photo streams with assistance of videos	mechanism
For evaluation we collect the datasets of 20 outdoor activities	method
consisting of 2	method
7M Flickr images and 16K YouTube videos	method
Due to the large-scale nature of our problem	method
we evaluate our algorithm via crowdsourcing using Amazon Mechanical Turk	method
In our experiments	method
Although widely touted as a replacement for glass slides and microscopes in pathology	background
digital slides present major challenges in data storage	background
transmission processing and interoperability OpenSlide is in use today by many academic and industrial organizations world-wide	background
including many research sites in the United States that are funded by the National Institutes of Health	background
can transparently handle multiple vendor formats	finding
In this paper	mechanism
we present the design and implementation of OpenSlide	mechanism
a vendor-neutral C library The library is extensible and easily interfaced to various programming languages	mechanism
An application written to the OpenSlide interface	method
we show that these 'skin buttons ' can have high touch accuracy and recognizability	finding
while being low cost and power-efficient	finding
We propose using tiny projectors integrated into the smartwatch These icons can be made touch sensitive	mechanism
significantly expanding the interactive region without increasing device size	mechanism
Through a series of experiments	method
We demonstrate High Assurance SPIRALs capability	finding
In this paper we introduce High Assurance SPIRAL High Assurance SPIRAL is a scalable methodology to translate a high level specification of a high assurance controller into a highly resource-efficient	mechanism
platform-adapted verified control software implementation for a given platform in a language like C or C++	mechanism
High Assurance SPIRAL proves that the implementation is equivalent to the specification written in the control engineers domain language	mechanism
Our approach scales to problems involving floating-point calculations and provides highly optimized synthesized code It is possible to estimate the available headroom to enable assurance/performance trade-offs under real-time constraints	mechanism
and enables the synthesis of multiple implementation variants to make attacks harder	mechanism
At the core of High Assurance SPIRAL is the Hybrid Control Operator Language ( HCOL ) that leverages advanced mathematical constructs expressing the controller specification to provide high quality translation capabilities	mechanism
Combined with a verified/certified compiler	mechanism
High Assurance SPIRAL provides a comprehensive complete solution to the efficient synthesis of verifiable high assurance controllers	mechanism
by co-synthesizing proofs and implementations for attack detection and sensor spoofing algorithms and deploy the code as ROS nodes on the Landshark unmanned ground vehicle and on a Synthetic Car in a real-time simulator	method
A responsibility we have as researchers is to disseminate the results of our research widely	background
A primary way we do this is through research publications	background
We offer thoughts on research challenges and future work that may make our community 's research more accessible	background
Second we reflect on our experience making papers accessible for any CHI 2015 author who requested it	mechanism
First we report on the accessibility of 1	method
811 papers in the technical program of several top conferences related to accessibility and human-computer interaction	method
show that under different clutter backgrounds the proposed method not only works more stably for different target sizes and signal-to-clutter ratio values	finding
but also has better detection performance compared with conventional baseline methods	finding
A novel small target detection method in a single infrared image is proposed in this paper Initially	mechanism
the traditional infrared image model is generalized to a new infrared patch-image model using local patch construction	mechanism
Then because of the non-local self-correlation property of the infrared background image	mechanism
based on the new model small target detection is formulated as an optimization problem of recovering low-rank and sparse matrices	mechanism
which is effectively solved using stable principle component pursuit	mechanism
Finally a simple adaptive segmentation method is used to segment the target image and the segmentation result can be refined by post-processing	mechanism
Extensive synthetic and real data experiments	method
our experimental results show that the proposed algorithm is more successful in time-sensitive image retrieval than other candidate methods	finding
including ranking SVM	finding
a PageRank-based image ranking	finding
and a generative temporal topic model	finding
in which given a query keyword	mechanism
a query time point	mechanism
and optionally user information	mechanism
we retrieve the most relevant and temporally suitable images from the database	mechanism
Inspired by recently emerging interests on query dynamics in information retrieval research	mechanism
our time-sensitive image retrieval algorithm can infer users ' implicit search intent better and provide more engaging and diverse search results according to temporal trends of Web user photos We model observed image streams as instances of multivariate point processes represented by several different descriptors	mechanism
and develop a regularized multi-task regression framework that automatically selects and learns stochastic parametric models to solve the relations between image occurrence probabilities and various temporal factors that influence them	mechanism
Using Flickr datasets of more than seven million images of 30 topics	method
As part of a collaboration with a major California school district	background
show that a nontrivial implementation of the leximin mechanism scales gracefully in terms of running time ( even though the problem is intractable in theory )	finding
and performs extremely well with respect to a number of efficiency objectives	finding
Our approach revolves around the randomized leximin mechanism We extend previous work to the classroom allocation setting	mechanism
showing that the leximin mechanism is proportional	mechanism
envy-free efficient and group strategyproof	mechanism
We also prove that the leximin mechanism provides a ( worst-case ) 4-approximation to the maximum number of classrooms that can possibly be allocated	mechanism
We take great pains to establish the practicability of our approach	mechanism
and discuss issues related to its deployment	mechanism
Our experiments which are based on real data	method
The LD results answer a fundamental question on how to quantify the rate at which the distributed scheme approaches the centralized performance as the inter-sensor communication rate increases	background
it is shown that the network achieves weak consensus	finding
i	finding
e	finding
the conditional estimation error covariance at a randomly selected sensor converges weakly ( in distribution ) to a unique invariant measure	finding
Further it is proved that as $ \overline { \gamma } \rightarrow \infty $ this invariant measure satisfies the Large Deviation ( LD ) upper and lower bounds	finding
implying that this measure converges exponentially fast ( in probability ) to the Dirac measure $ \delta_ { P^* } $	finding
where $ P^* $ is the stable error covariance of the centralized ( Kalman ) filtering setup	finding
A gossip network protocol termed Modified Gossip Interactive Kalman Filtering ( M-GIKF ) is proposed	mechanism
where sensors exchange their filtered states ( estimates and error covariances ) and propagate their observations via inter-sensor communications of rate $ \overline { \gamma } $ ; $ \overline { \gamma } $ is defined as the averaged number of inter-sensor message passages per signal evolution epoch The filtered states are interpreted as stochastic particles swapped through local interaction	mechanism
The paper shows that the conditional estimation error covariance sequence at each sensor under M-GIKF evolves as a random Riccati equation ( RRE ) with Markov modulated switching	mechanism
By formulating the RRE as a random dynamical system	method
Given a set of k networks	background
possibly with different sizes and no overlaps in nodes or links	background
how can we quickly assess similarity between them ? Analogously	background
are there a set of social theories which	background
when represented by a small number of descriptive	background
numerical features effectively serve as a `` signature '' for the network ?	background
NETSIMILE outperforms baseline competitors	finding
We propose a novel	mechanism
effective and scalable method	mechanism
called NETSIMILE Our approach has the following desirable properties : ( a ) It is supported by a set of social theories	mechanism
( b ) It gives similarity scores that are size-invariant	mechanism
( c ) It is scalable	mechanism
being linear on the number of links for graph signature extraction our approach enables several mining tasks such as clustering	mechanism
visualization discontinuity detection	mechanism
network transfer learning	mechanism
and re-identification across networks	mechanism
In extensive experiments on numerous synthetic and real networks from disparate domains	method
We also demonstrate how	method
Short-term forecasting is a ubiquitous practice in a wide range of energy systems	background
including forecasting demand	background
renewable generation and electricity pricing	background
we show that this probabilistic model greatly outperforms other methods on the task of accurately modeling potential distributions of power ( as would be necessary in a stochastic dispatch problem	finding
for example )	finding
In this paper	mechanism
we apply a recently-proposed algorithm for modeling high-dimensional conditional Gaussian distributions to forecasting wind power and extend it to the non-Gaussian case using the copula transform	mechanism
On a wind power forecasting task	method
The omnipresence of indoor lighting makes it an ideal vehicle for pervasive communication with mobile devices	background
We show how a binary frequency shift keying modulation scheme can be used to transmit data at 1	finding
25 bytes per second ( fast enough to send an ID code ) from up to 29 unique light sources simultaneously in a single collision domain We also show how tags can demodulate the same signals using a light sensor instead of a camera for low-power applications	finding
In this paper	mechanism
we present a communication scheme that enables interior ambient LED lighting systems using either cameras or light sensors	mechanism
By exploiting rolling shutter camera sensors that are common on tablets	mechanism
laptops and smartphones	mechanism
it is possible to detect high-frequency changes in light intensity reflected off of surfaces and in direct line-of-sight of the camera We present a demodulation approach that allows smartphones to accurately detect frequencies as high as 8kHz with 0	mechanism
2kHz channel separation	mechanism
In order to avoid humanly perceivable flicker in the lighting	mechanism
our system operates at frequencies above 2kHz and compensates for the non-ideal frequency response of standard LED drivers by adjusting the light 's duty-cycle	mechanism
By modulating the PWM signal commonly used to drive LED lighting systems	mechanism
we are able to encode data that can be used as localization landmarks	mechanism
through experiments	method
With the rapid increase in cloud services collecting and using user data to offer personalized experiences	background
ensuring that these services comply with their privacy policies has become a business imperative for building user trust	background
In this paper	mechanism
we present our experience building and operating a system Central to the design of the system are ( a ) Legal ease-a language that impose restrictions on how user data is handled	mechanism
and ( b ) Grok-a data inventory for Map-Reduce-like big data systems among programs	mechanism
Grok maps code-level schema elements to data types in Legal ease	mechanism
in essence annotating existing programs with information flow types with minimal human input	mechanism
Compliance checking is thus reduced to information flow analysis of big data systems	mechanism
The system bootstrapped by a small team	method
checks compliance daily of millions of lines of ever-changing source code written by several thousand developers	method
Question answering ( Q & A ) communities have been gaining popularity in the past few years	background
The success of such sites depends mainly on the contribution of a small number of expert users who provide a significant portion of the helpful answers	background
Interestingly we find that while the majority of questions on the site are asked by low reputation users	finding
on average a high reputation user asks more questions than a user with low reputation and find they are effective in detecting extreme behaviors such as those of spam users we predict who will become influential long-term contributors	finding
We consider a number of graph analysis methods	mechanism
We present a study of the popular Q & A website StackOverflow ( SO )	method
in which users ask and answer questions about software development	method
algorithms math and other technical topics	method
The dataset includes information on 3	method
5 million questions and 6	method
9 million answers created by 1	method
3 million users in the years 2008 -- 2012	method
Participation in activities on the site ( such as asking and answering questions ) earns users reputation	method
which is an indicator of the value of that user to the site	method
We describe an analysis of the SO reputation system	method
and the participation patterns of high and low reputation users	method
The contributions of very high reputation users to the site indicate that they are the primary source of answers	method
and especially of high quality answers	method
Lastly we show an application of our analysis : by considering user contributions over first months of activity on the site	method
The results suggest that even if a notice contains information users care about	background
it is unlikely to be recalled if only shown in the app store	background
Showing the notice during app use significantly increased recall rates over showing it in the app store	finding
which improved recall but did not perform as well as notices shown during app use	finding
In a series of experiments In a web survey and a field experiment	method
we isolated different timing conditions for displaying privacy notices : in the app store	method
when an app is started	method
during app use	method
and after app use Participants installed and played a history quiz app	method
either virtually or on their phone	method
After a distraction or delay they were asked to recall the privacy notice 's content	method
Recall was used as a proxy for the attention paid to and salience of the notice	method
In a follow-up web survey	method
we tested alternative app store notices	method
In commercial-off-the-shelf ( COTS ) multi-core systems	background
a task running on one core can be delayed by other tasks running simultaneously on other cores due to interference in the shared DRAM main memory	background
results show that our approach provides an upper bound very close to our measured worst-case interference	finding
In this paper	mechanism
we present techniques in a COTS-based multi-core system We explicitly model the major resources in the DRAM system	mechanism
including banks buses and the memory controller By considering their timing characteristics	mechanism
we analyze the worst-case memory interference delay imposed on a task by other tasks running in parallel	mechanism
To the best of our knowledge	mechanism
this is the first work bounding the request re-ordering effect of COTS memory controllers Our work also enables the quantification of the extent by which memory interference can be reduced by partitioning DRAM banks	mechanism
We evaluate our approach on a commodity multi-core platform running Linux/RK Experimental	method
Virus capsid assembly has been widely studied as a biophysical system	background
both for its biological and medical significance and as an important model for complex self-assembly processes No current technology can monitor assembly in detail and what information we have on assembly kinetics comes exclusively from invitro studies	background
There are many differences between the intracellular environment and that of an invitro assembly assay	background
however that might be expected to alter assembly pathways These models may help us understand how complicated assembly systems may have evolved to function with high efficiency and fidelity in the densely crowded environment of the cell	background
Simulations suggest a striking difference depending on whether or not a system uses nucleation-limited assembly	finding
with crowding tending to promote off-pathway growth in a nonnucleation-limited model but often enhancing assembly efficiency at high crowding levels even while impeding it at lower crowding levels in a nucleation-limited model	finding
We combine prior particle simulation methods for estimating crowding effects with coarse-grained stochastic models of capsid assembly	mechanism
using the crowding models to adjust kinetics of capsid simulations	mechanism
We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models demonstrate the the superior performance of our proposed models	finding
We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it	mechanism
In this paper	method
we analyze a flexible stochastic process model	method
the generalized linear auto-regressive process ( GLARP ) and identify the conditions under which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood	method
In particular we examine three examples	method
including two popular models for count data	method
i	method
e Poisson and Conwey-Maxwell Poisson vector auto-regressive processes	method
and one powerful model for extreme value data	method
i	method
e	method
Gumbel vector auto-regressive processes Experiments on two synthetic datasets	method
one social network dataset and one climatology dataset	method
Kidney exchanges allow incompatible donor-patient pairs to swap kidneys	background
but each donation must pass three tests : blood	background
tissue and crossmatch In practice a matching is computed based on the first two tests	background
and then a single crossmatch test is performed for each matched patient	background
Our main result is a polynomial time algorithm that almost surely computes optimal -- - up to lower order terms -- - solutions on random large kidney exchange instances	mechanism
The Internet has the potential to accelerate scientific problem solving by engaging a global pool of contributors	background
A better understanding of such collaborative strategies can inform the design of tools to support distributed collaboration on complex problems	background
Our results indicate a diversity of ways in which mathematicians are reaching a solution	finding
including by iteratively advancing a solution	finding
-- MathOverflow -- in which contributors communicate and collaborate to solve new mathematical 'micro-problems ' online We contribute a simple taxonomy of collaborative acts derived from a process-level examination of collaborations and a quantitative analysis relating collaborative acts to solution quality	mechanism
Recent computer systems research has proposed using redundant requests to reduce latency	background
The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request )	background
We find some surprising results	finding
First the response time of a fully redundant class follows a simple Exponential distribution and that of the non-redundant class follows a Generalized Hyperexponential	finding
Second fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant	finding
We find that	finding
in many cases	finding
redundancy outperforms JSQ and Opt-Split with respect to overall response time	finding
making it an attractive solution	finding
We allow for any number of classes of redundant requests	method
any number of classes of non-redundant requests	method
any degree of redundancy	method
and any number of heterogeneous servers	method
In all cases we derive the limiting distribution on the state of the system	method
In small ( two or three server ) systems	method
we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes	method
and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy We also compare redundancy with other approaches for reducing latency	method
such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and Join-the-Shortest-Queue ( JSQ ) routing of a class	method
Detecting dyslexia is crucial so that people who have dyslexia can receive training to avoid associated high rates of academic failure	background
These differences suggest that Dytective could be used to help identify those likely to have dyslexia	background
show significant differences between groups who played Dytective	finding
In this paper we present Dytective	mechanism
a game designed	mechanism
The results of a within-subjects experiment with 40 children ( 20 with dyslexia )	method
we show that the proposed algorithm improves other candidate methods for both storyline reconstruction and image prediction tasks	finding
In this paper	mechanism
we investigate an approach The storyline graphs can be an effective summary that visualizes various branching narrative structure of events or activities recurring across the input photo sets of a topic class we leverage them to perform the image sequential prediction tasks	mechanism
from which photo recommendation applications can benefit	mechanism
We formulate the storyline reconstruction problem as an inference of sparse time-varying directed graphs	mechanism
and develop an optimization algorithm that successfully addresses a number of key challenges of Web-scale problems	mechanism
including global optimality	mechanism
linear complexity and easy parallelization	mechanism
With experiments on more than 3	method
3 millions of images of 24 classes and user studies via Amazon Mechanical Turk	method
For example the Project Tycho provides open access to the count infections for U	background
S	background
states from 1888 to 2013	background
for 56 contagious diseases ( e	background
g	background
measles influenza )	background
which include missing values	background
possible recording errors	background
sudden spikes ( or dives ) of infections	background
etc	background
demonstrate that FUNNELFIT does indeed discover important properties of epidemics : ( P1 ) disease seasonality	finding
e	finding
g	finding
influenza spikes in January	finding
Lyme disease spikes in July and the absence of yearly periodicity for gonorrhea ; ( P2 ) disease reduction effect	finding
e	finding
g	finding
the appearance of vaccines ; ( P3 ) local/state-level sensitivity	finding
e	finding
g	finding
many measles cases in NY ; ( P4 ) external shock events	finding
e	finding
g	finding
historical flu pandemics ; ( P5 ) detect incongruous values	finding
i	finding
e	finding
data reporting errors	finding
In this paper	mechanism
we present FUNNEL	mechanism
a unifying analytical model as well as a novel fitting algorithm	mechanism
FUNNELFIT Our method has the following properties : ( a ) Sense-making : it detects important patterns of epidemics	mechanism
such as periodicities	mechanism
the appearance of vaccines	mechanism
external shock events	mechanism
and more ; ( b ) Parameter-free : our modeling framework frees the user from providing parameter values ; ( c ) Scalable : FUNNELFIT is carefully designed to be linear on the input size ; ( d ) General : our model is general and practical	mechanism
which can be applied to various types of epidemics	mechanism
including computer-virus propagation	mechanism
as well as human diseases	mechanism
Extensive experiments on real data	method
From Twitter to Facebook to Reddit	background
users have become accustomed to sharing the articles they read with friends or followers on their social networks	background
demonstrating that our approach is effective	finding
we model the content of news articles and blog posts by attributes of the people who are likely to share them	mechanism
For example many Twitter users describe themselves in a short profile	mechanism
labeling themselves with phrases such as `` vegetarian '' or `` liberal	mechanism
'' By assuming that a user 's labels correspond to topics in the articles he shares	mechanism
we can learn a labeled dictionary from a training corpus of articles shared on Twitter Thereafter	mechanism
we can code any new document as a sparse non-negative linear combination of user labels	mechanism
where we encourage correlated labels to appear together in the output via a structured sparsity penalty	mechanism
Finally we show that our approach yields a novel document representation that can be effectively used in many problem settings	mechanism
from recommendation to modeling news dynamics For example	mechanism
while the top politics stories will change drastically from one month to the next	mechanism
the `` politics '' label will still be there to describe them	mechanism
We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012	method
The M/M/k/setup model	background
where there is a penalty for turning servers on	background
is common in data centers	background
call centers and manufacturing systems	background
Setup costs take the form of a time delay	background
and sometimes there is additionally a power penalty	background
as in the case of data centers	background
by a new way of combining renewal reward theory and recursive techniques to solve Markov chains with a repeating structure	mechanism
Our renewal-based approach uses ideas from renewal reward theory and busy period analysis to obtain closed-form expressions for metrics of interest such as the transform of time in system and the transform of power consumed by the system	mechanism
The simplicity intuitiveness	mechanism
and versatility of our renewal-based approach makes it useful for analyzing Markov chains far beyond the M/M/k/setup	mechanism
In general our renewal-based approach should be used to reduce the analysis of any 2-dimensional Markov chain which is infinite in at most one dimension and repeating to the problem of solving a system of polynomial equations	mechanism
In the case where all transitions in the repeating portion of the Markov chain are skip-free and all up/down arrows are unidirectional	mechanism
the resulting system of equations will yield a closed-form solution	mechanism
Our analysis is made possible	method
Multimedia event detection ( MED ) is an effective technique for video indexing and retrieval	background
have validated the efficacy of our proposed approach	finding
we use a statistical method on both the positive and negative examples Based on these decisive attributes	mechanism
we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation	mechanism
The resulting fine-grained labels may be not accurate enough to characterize the negative videos	mechanism
Hence we propose to jointly optimize the fine-grained labels with the knowledge from the visual features and the attributes representations	mechanism
which brings mutual reciprocality	mechanism
Our model obtains two kinds of classifiers	mechanism
one from the attributes and one from the features	mechanism
which incorporate the informative cues from the fine-grained labels The outputs of both classifiers on the testing videos are fused for detection	mechanism
Extensive experiments on the challenging TRECVID MED 2012 development set	method
Autonomous driving technologies have been emerging over the past few years	background
and semi-autonomous driving functionalities have been deployed to vehicles available in the market	background
Since autonomous driving is realized by the intelligent processing of data from various types of sensors such as LIDAR	background
radar camera etc	background
the complexity of designing a dependable real-time autonomous driving system is rather high	background
Although there has been much research on building a reliable real-time system using hardware replication	background
the resulting systems tend to add significant extra cost due to hardware replication	background
we summarize SAFER ( System-level Architecture for Failure Evasion in Real-time applications ) our previous work on flexible system design	background
We then present a conceptual framework for autonomous vehicles	mechanism
We motivate our proposed framework with various scenarios	method
and we describe how SAFER can be extended to support the proposed conceptual framework	method
Coding behavioral video is an important method used by researchers to understand social phenomenon	background
Recent work has shown that these tasks can be completed quickly by leveraging the parallelism of large online crowds This trade-off between coding quality and privacy protection suggests that researchers can use online crowds to code for some key behaviors in video without compromising participant identity	background
We conclude with a discussion of how researchers can balance privacy and accuracy on their own data using a system we introduce called Incognito	background
We find accuracy and privacy to be the researchers ' primary concerns	finding
and show that the crowd yields accurate results	finding
and find as expected	finding
that workers ' ability to identify participants decreases as blur level increases The workers ' ability to accurately and reliably code behaviors also decreases	finding
but not as steeply as the identity test	finding
Then we demonstrate a method for obfuscating participant identity with a video blur filter	mechanism
we conducted interviews with 12 researchers who frequently code behavioral video	method
we used sample videos	method
For pipe guided wave inspection systems	background
it can often be difficult to achieve accurate localization performance due to the pipe 's geometry Many localization techniques focus on the first arrival for processing	background
but this often results in a poor circumferential resolution	background
Furthermore the pipe 's circular geometry generates multipath arrivals that make data interpretation difficult	background
we show that our method significantly improves circumferential resolution and reduces localization artifacts when compared with the standard delay-and-sum method	finding
by combining the standard delay-and-sum localization method with a simple multipath model for a pipe	mechanism
Using experimental data from a transmitting source	method
If Lisa visits Dr	background
Brown and there is no record of the drug he prescribed her	background
can we find it ? Data sources	background
much to analysts ' dismay	background
are too often plagued with incompleteness	background
making business analytics over the data difficult	background
We introduce a principled way of performing value imputation on missing values	mechanism
allowing a user We achieve this by turning our data into a graph network and performing link prediction on nodes of interest using the belief propagation algorithm	mechanism
A cognitive assistance application combines a wearable device such as Google Glass with cloudlet processing to provide step-by-step guidance on a complex task	background
We then reflect on the difficulties we faced in building these applications	background
and suggest future research that could simplify the creation of similar applications	background
In this paper	mechanism
we focus on user assistance We describe proof-of-concept implementations for four different tasks : assembling 2D Lego models	mechanism
freehand sketching playing ping-pong	mechanism
and recommending context-relevant YouTube tutorials	mechanism
The utilization of Building Information Modeling ( BIM ) has been growing significantly and translating into the support of various tasks within the construction industry	background
In relation to such a growth	background
many approaches that leverage dimensions of information stored in BIM model are being developed	background
Through this it is possible to allow all stakeholders to retrieve and generate information from the same model	background
enabling them to work cohesively It is believed to benefit the industry by providing a computable BIM and enabling all project participants to extract any information required for decision making	background
Finally the framework is used to identify areas to extend BIM research	background
and the result reveals a research gap for BIM applications in the project domains of quality	finding
safety and environmental management	finding
a BIM application framework is developed and discussed in this paper Such a framework gives an overview of BIM applications in the construction industry	mechanism
A computable multi-dimensional ( nD ) model is difficult to establish in these areas because with continuously changing conditions	mechanism
the decision making rules for evaluating whether an individual component is considered good quality	mechanism
or whether a construction site is safe	mechanism
also vary as the construction progresses	mechanism
A process of expanding from 3D to computable nD models	mechanism
specifically a possible way to integrate safety	mechanism
quality and carbon emission variables into BIM during the construction phase of a project is explained in this paper	mechanism
A literature review	method
within this framework	method
has been conducted As examples	method
the processes of utilizing nD models on real construction sites are described	method
Let us consider that someone is starting a research on a topic that is unfamiliar to them	background
we show the effectiveness and efficiency of our approach	finding
First we propose an algorithm We also address the performance and scalability issues of this sophisticated algorithm Next	mechanism
we discuss the measures to decide how much a paper is influenced by another paper	mechanism
Then we propose an algorithm by using the influence measure and citation information	mechanism
Finally through extensive experiments with a large volume of a real-world academic literature data	method
Online social networks and the World Wide Web lead to large underlying graphs that might not be completely known because of their size	background
To compute reliable statistics	background
we have to resort to sampling the network	background
node sampling yields Pareto optimal sample sizes in terms of the Kolomogorov-Smirnov statistic for the degree distribution	finding
while node-by-edge sampling yields optimal sample sizes for the biased distribution	finding
We also find that random walk sampling performs better than the Metropolis-Hastings random walk	finding
We measure the quality of our estimates of the degree distributions by using the Kolmogorov-Smirnov statistic	method
Among all four sampling methods	method
As kidney exchange programs are growing	background
manipulation by hospitals becomes more of an issue	background
suggest that in practice our mechanism performs much closer to optimal	finding
We study mechanisms for two-way exchanges that are strategyproof	mechanism
i	mechanism
e	mechanism
make it a dominant strategy We establish lower bounds on the welfare loss of strategyproof mechanisms	mechanism
both deterministic and randomized	mechanism
and propose a randomized mechanism that guarantees at least half of the maximum social welfare in the worst case	mechanism
Simulations using realistic distributions for blood types and other parameters	method
AbstractVulnerability assessment serves to identify vulnerabilities	background
develop responses and drive the risk-management process	background
In identifying vulnerabilities	background
it is fundamental to identify and rank critical assets	background
which include vital systems	background
facilities processes and information necessary to maintain continuity of service	background
During emergencies in the facility management domain	background
first responders typically search for critical assets	background
both related to business continuity and value to the organization	background
This paper presents a formalized approach The developed reasoning approach enables a first responder to perform flexible searches and prioritize critical spaces and pieces of equipment that need to be protected in an emergency by leveraging existing building and content representations found in building information models ( BIM )	mechanism
Data locality and parallelism are critical optimization objectives for performance on modern multi-core machines	background
exhibiting significant performance improvements over existing compilers	finding
by proposing a 3-step framework	mechanism
which We define the concept of vectorizable codelets	mechanism
with properties tailored to achieve effective SIMD code generation for the codelets	mechanism
We leverage the power of a modern high-level transformation framework to restructure a program to expose good ISA-independent vectorizable codelets	mechanism
exploiting multi-dimensional data reuse	mechanism
Then we generate ISA-specific customized code for the codelets	mechanism
using a collection of lower-level SIMD-focused optimizations	mechanism
We demonstrate our approach on a collection of numerical kernels that we automatically tile	method
parallelize and vectorize	method
Abstract Guided wave ultrasonics is an attractive monitoring technique for damage diagnosis in large-scale plate and pipe structures	background
Damage can be detected by comparing incoming records with baseline records collected on intact structure	background
Researchers developed temperature compensation methods to eliminate the effects of temperature variation	background
but they have limitations in practical implementations	background
We show that our method accurately detects the presence of a mass scatterer	finding
and is robust to the environmental and operational variations exhibited in the practical system	finding
In this paper	mechanism
we develop a robust damage detection method based on singular value decomposition ( SVD We show that the orthogonality of singular vectors ensures that the effect of damage and that of environmental and operational variations are separated into different singular vectors	mechanism
We report on our field ultrasonic monitoring of a 273	method
05mm outer diameter pipe segment	method
which belongs to a hot water piping system in continuous operation	method
We demonstrate the efficacy of our method on experimental pitchcatch records collected during seven months	method
We propose a non-intrusive approach At the core of this approach is a mechanism for selective real-time monitoring of guest file updates within VM instances	mechanism
This mechanism is agentless	mechanism
requiring no guest VM support	mechanism
It has low virtual I/O overhead	mechanism
low latency for emitting file updates	mechanism
and a scalable design	mechanism
Its central design principle is distributed streaming of file updates inferred from introspected disk sector writes	mechanism
The mechanism called DS-VMI	mechanism
enables many system administration tasks that involve monitoring files to be performed outside VMs	mechanism
In social settings	background
individuals interact through webs of relationships	background
This paper extends to signals on graphs DSP and its basic tenets	background
including filters convolution	background
z -transform impulse response	background
spectral representation Fourier transform	background
frequency response and illustrates DSP on graphs by classifying blogs	background
linear predicting and compressing data from irregularly located weather stations	background
or predicting behavior of customers of a mobile service provider	background
We label the data by its source	mechanism
or formally stated	mechanism
we index the data by the nodes of the graph	mechanism
The resulting signals ( data indexed by the nodes ) are far removed from time or image signals indexed by well ordered time samples or pixels DSP	mechanism
discrete signal processing	mechanism
provides a comprehensive	mechanism
elegant and efficient methodology	mechanism
Spatial Pyramid Matching ( SPM ) assumes that the spatial Bag-of-Words ( BoW ) representation is independent of data	background
validate that JS Tiling outperforms the SPM and the state-of-the-art methods	finding
The runtime comparison demonstrates that selecting BoW representations by JS Tiling is more than 1	finding
000 times faster than running classifiers	finding
Besides JS Tiling is an important component contributing to CMU Teams ' final submission in TRECVID 2012 Multimedia Event Detection	finding
In this paper	mechanism
we propose a novel method called Jensen-Shannon ( JS ) Tiling The proposed JS Tiling is especially appropriate for large-scale datasets as it is orders of magnitude faster than existing methods	mechanism
but with comparable or even better classification precision	mechanism
Experimental results on four benchmarks including two TRECVID12 datasets	method
imply improved parallel randomized algorithms for several problems	background
including single-source shortest paths	background
maximum flow minimum-cost flow	background
and approximate maximum flow	background
our results	finding
We present the design and analysis of a nearly-linear work parallel algorithm On input an SDD n-by-n matrix A with m nonzero entries and a vector b	mechanism
our algorithm computes a vector $ \tilde { x } $ such that $ \|\tilde { x } - A^ { + } b\|_ { A } \leq\varepsilon\cdot\| { A^ { + } b } \|_ { A } $ in $ O ( m\log^ { O ( 1 ) } { n } \log { \frac { 1 } { \varepsilon } } ) $ work and $ O ( m^ { 1/3+\theta } \log\frac { 1 } { \varepsilon } ) $ depth for any ? > 0	mechanism
where A + denotes the Moore-Penrose pseudoinverse of A	mechanism
The algorithm relies on a parallel algorithm for generating low-stretch spanning trees or spanning subgraphs To this end	mechanism
we first develop a parallel decomposition algorithm that in O ( mlog O ( 1 ) n ) work and polylogarithmic depth	mechanism
partitions a graph with n nodes and m edges into components with polylogarithmic diameter such that only a small fraction of the original edges are between the components	mechanism
This can be used to generate low-stretch spanning trees with average stretch O ( n ? ) in O ( mlog O ( 1 ) n ) work and O ( n ? ) depth for any ? > 0	mechanism
Alternatively it can be used to generate spanning subgraphs with polylogarithmic average stretch in O ( mlog O ( 1 ) n ) work and polylogarithmic depth	mechanism
We apply this subgraph construction to derive a parallel linear solver	mechanism
By using this solver in known applications	method
Heating Ventilation and Air-Conditioning ( HVAC ) systems account for more than 15 % of the total energy consumption in the US In order to improve the energy efficiency of HVAC systems	background
researchers have developed hundreds of algorithms to automatically analyze their performance	background
we envision a framework that automatically integrates the required information items and provides them to the performance analysis algorithms for HVAC systems	mechanism
This paper presents an approach We extend the Information Delivery Manual ( IDM ) approach so that the identified information requirements can be mapped to multiple information sources that use various formats and schemas	mechanism
A password composition policy restricts the space of allowable passwords to eliminate weak passwords that are vulnerable to statistical guessing attacks	background
theoretical results	finding
We introduce the first theoretical model Our main positive result is an algorithm that -- with high probability -- - constructs almost optimal policies ( which are specified as a union of subsets of allowed passwords )	mechanism
and requires only a small number of samples of users ' preferred passwords	mechanism
We study the computational and sample complexity of this problem under different assumptions on the structure of policies and on users ' preferences over passwords	method
We complement our with simulations using a real-world dataset of 32 million passwords	method
Time synchronization in wireless sensor networks is important for event ordering and efficient communication scheduling	background
We show that our new synchronization circuit consumes 60 % less power than the original design and is able to correct clock drift rates to within 0	finding
01 ppm without power hungry and expensive precision clocks	finding
In this paper	mechanism
we introduce an external hardwarebased clock tuning circuit that can be used without waking up the host MCU This is accomplished through two main hardware sub-systems First	mechanism
we improve upon the circuit presented in [ 1 ] that synchronizes clocks using the ambient magnetic fields emitted from power lines	mechanism
The new circuit uses an electric field front-end as opposed to the original magnetic-field sensor	mechanism
which makes the design more compact	mechanism
lower-power lower-cost exhibit less jitter and improves robustness to noise generated by nearby appliances Second	mechanism
we present a low-cost hardware tuning circuit that can be used to continuously trim a micro-controller 's low-power clock at runtime	mechanism
Most time synchronization approaches require a CPU to periodically adjust internal counters to accommodate for clock drift	mechanism
Periodic discrete updates can introduce interpolation errors as compared to continuous update approaches and they require the CPU to expend energy during these wake up periods	mechanism
Our hardware-based external clock tuning circuit allows the main CPU to remain in a deep-sleep mode for extended periods while an external circuit compensates for clock drift	mechanism
Split fabrication the process of splitting an IC into an untrusted and trusted tier	background
facilitates access to the most advanced semiconductor manufacturing capabilities available in the world without requiring disclosure of design intent	background
shows that they are vulnerable to recognition attacks at the untrusted foundry due to the use of standardized floorplans and leaf cell layouts	finding
and demonstrate their effectiveness using 130nm split fabricated testchips	finding
Our security analysis of IP block designs	method
specifically embedded memory and analog components We propose methodologies	method
We present a distributed Kalman-type estimator such that The challenges with distributed estimation by a network of sensors lie in the estimation of fields with unstable dynamics Our distributed Kalman filter type estimator	mechanism
which includes a consensus step on the pseudo-innovations	mechanism
a modified version of the filter innovations	mechanism
is able to track arbitrary unstable dynamics	mechanism
as long as the sensor network connectivity is above a threshold determined by the degree of instability of the field dynamics	mechanism
regardless of the specifics of the local observations	mechanism
Self-powered vehicles that interact with the physical world	background
such as spacecraft	background
require computing platforms with predictable timing behavior and a low energy demand	background
Energy consumption can be reduced by choosing energy-efficient designs for both hardware and software components of the platform	background
shows that neither balancing the load nor assigning all load to the cheapest core is the best load distribution strategy	finding
unless the cores are extremely alike or extremely different	finding
We leverage the state-of-the-art in energy-efficient hardware design by adopting Heterogeneous Multi-core Processors with support for Dynamic Voltage and Frequency Scaling and Dynamic Power Management	mechanism
Our approach is to start from an analytically justified target load distribution and find a task assignment heuristic that approximates it	mechanism
The optimal load distribution is then formulated as a solution to a convex optimization problem A heuristic that approximates this load distribution and an alternative method that leverages the solution explicitly are proposed as viable task assignment methods	mechanism
Our analysis The proposed methods are compared to state-of-the-art on simulated problem instances and in a case study of a soft-real-time application on an off-the-shelf ARM big	method
LITTLE heterogeneous processor	method
Harnessing crowds can be a powerful mechanism for increasing innovation	background
Our results have implications for improving creativity and building systems for distributed crowd innovation	background
we show that distributed analogical idea generation leads to better ideas than example-based approaches	finding
We introduce a new approach called distributed analogical idea generation Drawing from the literature in cognitive science on analogy and schema induction	mechanism
our approach decomposes the creative process in a structured way amenable to using crowds	mechanism
In three experiments	method
and investigate the conditions under which crowds generate good schemas and ideas	method
Reranking has been a focal technique in multimedia retrieval due to its efficacy in improving initial retrieval results	background
results validate the efficacy and the efficiency of the proposed method on both image and video search tasks	finding
Notably SPaR achieves by far the best result on the challenging TRECVID multimedia event search task	finding
In this paper	mechanism
we propose a novel reranking approach called Self-Paced Reranking ( SPaR ) As its name suggests	mechanism
SPaR utilizes samples from easy to more complex ones in a self-paced fashion	mechanism
SPaR is special in that it has a concise mathematical objective to optimize and useful properties that can be theoretically verified It on one hand offers a unified framework providing theoretical justifications for current reranking methods	mechanism
and on the other hand generates a spectrum of new reranking schemes	mechanism
This paper also advances the state-of-the-art self-paced learning research which potentially benefits applications in other fields	mechanism
Experimental	method
Memory layout transformations via data reorganization are very common operations	background
which occur as a part of the computation or as a performance optimization in data-intensive applications	background
and demonstrate that HAMLeT can achieve close to peak system utilization	finding
offering up to an order of magnitude performance improvement compared to the CPU and GPU memory subsystems which does not employ HAMLeT	finding
This paper proposes a high-bandwidth and energy-efficient hardware accelerated memory layout transform ( HAMLeT ) system integrated within a 3D-stacked DRAM	mechanism
HAMLeT uses a low-overhead hardware that exploits the existing infrastructure in the logic layer of 3D-stacked DRAMs	mechanism
and does not require any changes to the DRAM layers	mechanism
yet it can fully exploit the locality and parallelism within the stack by implementing efficient layout transform algorithms	mechanism
We analyze matrix layout transform operations ( such as matrix transpose	method
matrix blocking and 3D matrix rotation )	method
This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities	background
We defined over 100 data features in seven categories We have achieved area under the ROC curve values of up to 0	finding
979 and lift values of 65 on the top 50 user-days identified on two months of real data	finding
Our system combines structural and semantic information from a real corporate database of monitored activity on their users ' computers We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior	mechanism
indicators of unusual activities	mechanism
high-dimensional statistical patterns	mechanism
temporal sequences and normal graph evolution Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams	mechanism
We have also developed a visual language for specifying combinations of features	mechanism
baselines peer groups	mechanism
time periods and algorithms to detect anomalies suggestive of instances of insider threat behavior	mechanism
based on approximately 5	method
5 million actions per day from approximately 5	method
500 users	method
Support for multiple concurrent applications is an important enabler for promoting the use of sensor networks as an infrastructure technology	background
where multiple users can deploy their applications independently In such a scenario	background
different applications on a node may transmit packets at distinct periods	background
causing the node to change from sleep to active state more often	background
which negatively impacts the energy consumption of the whole network	background
and show how it can achieve a duty-cycle comparable to an ideal TDMA approach	finding
by defining a harmonizing period to align the transmissions from multiple applications at periodic boundaries	mechanism
This harmonizing period is then leveraged to design a protocol that coordinates the transmissions across nodes and provides real-time guarantees in a multi-hop network This protocol	mechanism
which we call Network- Harmonized Scheduling ( NHS )	mechanism
takes advantage of the periodicity introduced to assign offsets to nodes at different hop-levels such that collisions are always avoided	mechanism
and deterministic behavior is enforced NHS is a light-weight and distributed protocol that does not require any global state-keeping mechanism	mechanism
We implemented NHS on the Contiki operating system	method
We describe a new algorithm The running time of our algorithm is O ( f log n log ) where f is the output complexity of the Voronoi diagram and is the spread of the input	mechanism
the ratio of largest to smallest pairwise distances	mechanism
Despite the simplicity of the algorithm and its analysis	mechanism
it improves on the state of the art for all inputs with polynomial spread and near-linear output size	mechanism
The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation Then	mechanism
the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity	mechanism
yielding the output sensitive bound	mechanism
The removal only involves local flips and is inspired by kinetic data structures	mechanism
Color descriptors are one of the important features used in content-based image retrieval	background
The dominant color descriptor ( DCD ) represents a few perceptually dominant colors in an image through color quantization	background
For image retrieval based on DCD	background
the earth movers distance ( EMD ) and the optimal color composition distance were proposed to measure the dissimilarity between two images	background
The results reveal that our approach achieves almost the same results with the EMD in linear time	finding
we propose a new distance function that calculates an approximate earth movers distance in linear time To calculate the dissimilarity in linear time	mechanism
the proposed approach employs the space-filling curve for multidimensional color space	mechanism
To improve the accuracy	mechanism
the proposed approach uses multiple curves and adjusts the color positions	mechanism
As a result	mechanism
our approach achieves order-of-magnitude time improvement but incurs small errors	mechanism
We have performed extensive experiments to show the effectiveness and efficiency of the proposed approach	method
In previous work	background
we developed the scaled SIS process	background
which models the dynamics of SIS epidemics over networks	background
With the scaled SIS process	background
we can consider networks that are finite-sized and of arbitrary topology ( i	background
e	background
we are not restricted to specific classes of networks )	background
We derived for the scaled SIS process a closed-form expression for the time-asymptotic probability distribution of the states of all the agents in the network	background
This closed-form solution of the equilibrium distribution explicitly exhibits the underlying network topology through its adjacency matrix	background
We illustrate our results	finding
We prove that	mechanism
for a range of epidemics parameters	mechanism
this combinatorial problem leads to a submodular optimization problem	mechanism
which is exactly solvable in polynomial time	mechanism
We relate the most-probable configuration to the network structure	mechanism
in particular to the existence of high density subgraphs Depending on the epidemics parameters	mechanism
subset of agents may be more likely to be infected than others ; these more-vulnerable agents form subgraphs that are denser than the overall network	mechanism
with a 193 node social network and the 4941 node Western US power grid under different epidemics parameters	method
demonstrate that the proposed approach achieves high classification accuracy	finding
We propose a novel discrete signal processing framework Our framework extends traditional discrete signal processing theory to datasets with complex structure that can be represented by graphs	mechanism
so that data elements are indexed by graph nodes and relations between elements are represented by weighted graph edges	mechanism
We interpret such datasets as signals on graphs	mechanism
introduce the concept of graph filters for processing such signals	mechanism
and discuss important properties of graph filters	mechanism
including linearity shift-invariance	mechanism
and invertibility We then demonstrate the application of graph filters to data classification by demonstrating that a classifier can be interpreted as an adaptive graph filter	mechanism
Our experiments	method
Semantic search in video is a novel and challenging problem in information and multimedia retrieval	background
We share our observations and lessons in building such a state-of-the-art system	background
which may be instrumental in guiding the design of the future system for semantic search in video	background
where the proposed system achieves the best performance	finding
This paper presents a state-of-the-art system for event search without any textual metadata or example videos	mechanism
The system relies on substantial video content understanding and allows for semantic search over a large collection of videos	mechanism
The novelty and practicality is demonstrated by the evaluation in NIST TRECVID 2014	method
We have shown that the proposed ALDA method with the aid of facial asymmetry features significantly outperforms other well-established facial descriptors ( LBP	finding
LTP LTrP )	finding
and the ALDA subspace method does a much better job in distinguishing identical twins than LDA	finding
We are able to achieve 48	finding
50 % VR at 0	finding
1 % FAR for identifying family membership of identical twin individuals in the crowd and an averaged 82	finding
58 % VR at 0	finding
1 % FAR for verifying identical twin individuals within the same family	finding
a significant improvement over traditional descriptors and traditional LDA method	finding
In this work	mechanism
we have proposed an Augmented Linear Discriminant Analysis ( ALDA ) approach It learns a common subspace that not only can identify from which family the individual comes	mechanism
but also can distinguish between individuals within the same family	mechanism
We evaluate the ALDA against the traditional LDA approach for subspace learning on the Notre Dame twin database	method
Accurate inference of molecular and functional interactions among genes	background
especially in multicellular organisms such as Drosophila	background
often requires statistical analysis of correlations not only between the magnitudes of gene expressions	background
but also between their temporal-spatial patterns	background
The ISH ( in-situ-hybridization ) -based gene expression micro-imaging technology offers an effective approach to perform large-scale spatial-temporal profiling of whole-body mRNA abundance	background
we demonstrate the effectiveness of our approach in network building Furthermore	finding
we report results	finding
where GINI makes novel and interesting predictions of gene interactions	finding
In this paper	mechanism
we present GINI	mechanism
a machine learning system GINI builds on a computer-vision-inspired vector-space representation of the spatial pattern of gene expression in ISH images	mechanism
enabled by our recently developed system ; and a new multi-instance-kernel algorithm that learns a sparse Markov network model	mechanism
in which every gene ( i	mechanism
e	mechanism
node ) in the network is represented by a vector-valued spatial pattern rather than a scalar-valued gene intensity as in conventional approaches such as a Gaussian graphical model By capturing the notion of spatial similarity of gene expression	mechanism
and at the same time properly taking into account the presence of multiple images per gene via multi-instance kernels	mechanism
GINI is well-positioned to infer statistically sound	mechanism
and biologically meaningful gene interaction networks from image data Software for GINI is available at http : //sailing	mechanism
cs	mechanism
cmu	mechanism
edu/Drosophila_ISH_images/	mechanism
Using both synthetic data and a small manually curated data set	method
on a large publicly available collection of Drosophila embryonic ISH images from the Berkeley Drosophila Genome Project	method
Background : Association analysis using genome-wide expression quantitative trait locus ( eQTL ) data investigates the effect that genetic variation has on cellular pathways and leads to the discovery of candidate regulators Furthermore	background
this analysis demonstrates the potential of GFlasso as a powerful computational tool for eQTL studies that exploit the rich structural information among expression traits due to correlation	background
regulation or other forms of biological dependencies	background
Results : While eQTL hotspots in yeast have been reported previously as genomic regions controlling multiple genes	finding
our analysis reveals additional novel eQTL hotspots and	finding
more interestingly uncovers groups of multiple contributing eQTL hotspots that affect the expression level of functional gene modules To our knowledge	finding
our study is the first to report this type of gene regulation stemming from multiple eQTL hotspots Additionally	finding
we report the results Not only do we find that many of these candidate regulators contain mutations in the promoter and coding regions of the genes	finding
in the case of the Ribi group	finding
we provide experimental evidence suggesting that the identified candidates do regulate the target genes predicted by GFlasso Conclusions : Thus	finding
this structured association analysis of a yeast eQTL dataset via GFlasso	finding
coupled with extensive bioinformatics analysis	finding
discovers a novel regulation pattern between multiple eQTL hotspots and functional gene modules	finding
We employ a new eQTL mapping algorithm	mechanism
GFlasso which we have previously developed for sparse structured regression	mechanism
GFlasso fully takes into account the dependencies among expression traits to suppress false positives and to enhance the signal/noise ratio	mechanism
Thus GFlasso leverages the gene-interaction network to discover the pleiotropic effects of genetic loci that perturb the expression level of multiple ( rather than individual ) genes	mechanism
which enables us to gain more power in detecting previously neglected signals that are marginally weak but pleiotropically significant We suggest candidate regulators for the functional gene modules that map to each group of hotspots	mechanism
from in-depth bioinformatics analysis for three groups of these eQTL hotspots : ribosome biogenesis	method
telomere silencing and retrotransposon biology	method
Human action may be observed from multi-view	background
which are highly related but sometimes look different from each other	background
show that the exploring of consistency properties of different views by graph model is very useful	finding
moreover GM-GS-DSDL for each view	finding
which are learnt simultaneously	finding
can further improve the fusion performance	finding
demonstrate that our proposed algorithm can obtain competing performance against the state-of-the-art methods	finding
Thus multi-view discriminative and structured dictionary learning with group sparsity and graph model ( GM-GS-DSDL ) is proposed to fuse different views and recognize human actions	mechanism
First spatio-temporal interest points are extracted for each view	mechanism
and then multi-view bag of words ( MVBoW ) representation is employed	mechanism
at the same time	mechanism
the graph model is also utilized to fuse different views	mechanism
which will remove overlapped interest points to explore their consistency properties	mechanism
Furthermore GM-GS-DSDL is formulated to discover the latent correlation among multiple views	mechanism
In addition we also issue a new multi-view action dataset with RGB	mechanism
depth and skeleton data ( called CVS-MV-RGBD ) Multi-view bag of words ( MVBoW ) representation is employed	mechanism
The graph model is also utilized to fuse different views	mechanism
We formulate the multi-view action recognition task as a joint dictionary learning ( DL ) problem	mechanism
We release a new multi-view action dataset which contains RGB	mechanism
depth and skeleton data	mechanism
Large-scale experimental results on multi-view IXMAX and CVS-MV-RGBD datasets Comparative experiments	method
Analyzing and interpreting the activity of a heterogeneous population of neurons can be challenging	background
especially as the number of neurons	background
experimental trials and experimental conditions increases	background
One approach is to extract a set of latent variables that succinctly captures the prominent co-fluctuation patterns across the neural population	background
Significance	background
DataHigh was developed to fulfil a need for visualization in exploratory neural data analysis	background
which can provide intuition that is critical for building scientific hypotheses and models of population activity	background
Main results	finding
Approach we developed a Matlab graphical user interface ( called DataHigh ) We also implemented a suite of additional visualization tools ( including playing out population activity timecourses as a movie and displaying summary statistics	mechanism
such as covariance ellipses and average timecourses ) and an optional tool for performing dimensionality reduction	mechanism
To demonstrate the utility and versatility of DataHigh	method
we used it to analyze single-trial spike count and single-trial timecourse population activity recorded using a multi-electrode array	method
as well as trial-averaged population activity recorded using single electrodes	method
Current applications have produced graphs on the order of hundreds of thousands of nodes and millions of edges	background
To take advantage of such graphs	background
one must be able to find patterns	background
outliers and communities	background
These tasks are better performed in an interactive environment	background
where human expertise can guide the process	background
we propose an innovative framework suited for any kind of tree-like graph visual design GMine integrates 1 ) a representation for graphs organized as hierarchies of partitions-the concepts of SuperGraph and Graph-Tree ; and 2 ) a graph summarization methodology-CEPS Our graph representation deals with the problem of tracing the connection aspects of a graph hierarchy with sub linear complexity	mechanism
allowing one to grasp the neighborhood of a single node or of a group of nodes in a single click	mechanism
As a proof of concept	method
the visual environment of GMine is instantiated as a system in which large graphs can be investigated globally and locally	method
Many content-based video search ( CBVS ) systems have been proposed to analyze the rapidly-increasing amount of user-generated videos on the Internet	background
which potentially opens the door to interactive web-scale CBVS for the general public	background
showed that can achieve an 10	finding
000-fold speedup while retaining 80 % accuracy of a state-of-the-art CBVS system	finding
demonstrated that our system can complete the search in 0	finding
975 seconds with a single core	finding
through a combination of effective features	mechanism
highly compressed representations	mechanism
and one iteration of reranking	mechanism
our proposed system	mechanism
we perform a comprehensive study on the different components in a CBVS system Directions investigated include exploring different low-level and semantics-based features	method
testing different compression factors and approximations during video search	method
and understanding the time v	method
s	method
accuracy trade-off of reranking Extensive experiments on data sets consisting of more than 1	method
000 hours of video We further performed search over 1 million videos and	method
We demonstrate a Visual Light Communication ( VLC ) system Each LED pulses at a frequency above the humanly perceivable flicker threshold where cameras and photodiodes can still detect changes in light intensity	mechanism
Our modulation scheme supports multiple light sources in a single collision domain	mechanism
and works for both	mechanism
line-of-sight ( LOS ) operation as well as from reflected surfaces like those found in architectural lighting	mechanism
The spatial confinement of light makes this system ideal for use as localization landmarks	mechanism
A mobile device receiving and processing the signal displays the ID and RSSI of the closest landmark	mechanism
Interacting with the system will allow users to see the practical effects of multiple-access	mechanism
frequency of operation	mechanism
distance from the lights	mechanism
camera parameters and camera motion	mechanism
Our demonstration includes four LED ambient lights acting as location landmarks transmitting modulated data	method
The relationship between occupant activity and electricity consumption is inextricably linked	background
It has been difficult to both gather detailed energy data and information about occupants ' daily lives as well as understand their relationship quantitatively	background
Initial findings include the identification of device groups but highlight the challenges of modeling complex patterns and event rarity	finding
Our work begins by characterizing power data as provided by plug-level meters from one household	mechanism
Association and sequential rule mining techniques are applied	mechanism
To reduce costs	background
organizations may outsource data storage and data processing to third-party clouds	background
This raises confidentiality concerns	background
since the outsourced data may have sensitive information	background
and observe low to moderate overheads	finding
we present two database encryption schemes that reveal just enough information about structured data Our main contribution is a definition and proof of security for the two schemes	mechanism
This definition captures confidentiality offered by the schemes using a novel notion of equivalence of databases from the adversary 's perspective	mechanism
As a specific application	method
we adapt an existing algorithm for finding violations of a rich class of privacy policies to run on logs encrypted under our schemes	method
Developers use cryptographic APIs in Android with the intent of securing data such as passwords and personal information on mobile devices These numbers show that applications do not use cryptographic APIs in a fashion that maximizes overall security We then suggest specific remediations based on our analysis towards improving overall cryptographic security in Android applications	background
and find that 10	finding
327 out of 11	finding
748 applications that use cryptographic APIs -- 88 % overall -- make at least one mistake	finding
We develop program analysis techniques on the Google Play marketplace	mechanism
As a way to relieve the tedious work of manual annotation	background
active learning plays important roles in many applications of visual concept recognition	background
In typical active learning scenarios	background
the number of labelled data in the seed set is usually small	background
demonstrate its advantages	finding
In this paper	mechanism
we propose a semi-supervised batch mode multi-class active learning algorithm for visual concept recognition	mechanism
Our algorithm exploits the whole active pool to evaluate the uncertainty of the data	mechanism
Considering that uncertain data are always similar to each other	mechanism
we propose to make the selected data as diverse as possible	mechanism
for which we explicitly impose a diversity constraint on the objective function	mechanism
As a multi-class active learning algorithm	mechanism
our algorithm is able to exploit uncertainty across multiple classes	mechanism
An efficient algorithm is used to optimize the objective function	mechanism
Extensive experiments on action recognition	method
object classification scene recognition	method
and event detection	method
Existing methods are typically superlinear in space or execution time	background
Effectiveness : it is accurate	finding
providing results with equal or better quality compared to top related works Halite was in average at least 12 times faster than seven representative works	finding
and always presented highly accurate results	finding
Halite was at least 11 times faster than others	finding
increasing their accuracy in up to 35 percent	finding
This paper proposes Halite	mechanism
a novel fast	mechanism
and scalable clustering method Halite 's strengths are that it is fast and scalable	mechanism
while still giving highly accurate results	mechanism
Specifically the main contributions of Halite are : 1 ) Scalability : it is linear or quasi linear in time and space regarding the data size and dimensionality	mechanism
and the dimensionality of the clusters ' subspaces ; 2 ) Usability : it is deterministic	mechanism
robust to noise	mechanism
does n't take the number of clusters as an input parameter	mechanism
and detects clusters in subspaces generated by original axes or by their linear combinations	mechanism
including space rotation ; 3 ) ; and 4 ) Generality : it includes a soft clustering approach	mechanism
Experiments on synthetic data ranging from five to 30 axes and up to 1 \rm million points were performed On real data Finally	method
we report experiments in a real scenario where soft clustering is desirable	method
Analysts synthesize complex	background
qualitative data to uncover themes and concepts	background
but the process is time-consuming	background
cognitively taxing and automated techniques show mixed success	background
Crowdsourcing could help this process through on-demand harnessing of flexible and powerful human cognition	background
but incurs other challenges including limited attention and expertise	background
Further text data can be complex	background
high-dimensional and ill-structured	background
We demonstrate a classification-plus-context approach elicits the most accurate categories at the most useful level of abstraction	finding
B ) introduce an iterative clustering approach that provides workers a global overview of data	mechanism
To address these challenges we present an empirical study of a two-stage approach to enable crowds to create an accurate and useful overview of a dataset : A ) we draw on cognitive theory to assess how re-representing data can shorten and focus the data on salient dimensions ; and	method
Sequencing of RNAs ( RNA-Seq ) has revolutionized the field of transcriptomics	background
but the reads obtained often contain errors	background
Read error correction can have a large impact on our ability to accurately assemble transcripts	background
This is especially true for de novo transcriptome analysis	background
where a reference genome is not available	background
Supporting website : http : //sb	background
cs	background
cmu	background
edu/seecer/	background
we show that SEECER greatly improves on previous methods in terms of quality of read alignment to the genome and assembly accuracy Our corrected assembled transcripts shed new light on two important stages in sea cucumber development	finding
has also revealed novel transcripts that are unique to sea cucumber	finding
some of which we have validated	finding
Here we present SEquencing Error CorrEction in Rna-seq data ( SEECER )	mechanism
a hidden Markov Model ( HMM ) based method	mechanism
which is SEECER efficiently learns hundreds of thousands of HMMs and uses these to correct sequencing errors	mechanism
Using human RNA-Seq data	method
To illustrate the usefulness of SEECER for de novo transcriptome studies	method
we generated new RNA-Seq data to study the development of the sea cucumber Parastichopus parvimensis	method
Comparison of the assembled transcripts to known transcripts in other species experimentally	method
The HMT3522 progression series of human breast cells have been used to discover how tissue architecture	background
microenvironment and signaling molecules affect breast cell growth and behaviors	background
Thus analysis of various reversion conditions ( including non-reverted ) of HMT3522 cells using Treegl can be a good model system to study drug effects on breast cancer	background
We found that different breast cell states contain distinct gene networks	finding
The network specific to non-malignant HMT3522-S1 cells is dominated by genes involved in normal processes	finding
whereas the T4-2-specific network is enriched with cancer-related genes	finding
The networks specific to various conditions of the reverted T4-2 cells are enriched with pathways suggestive of compensatory effects	finding
consistent with clinical data showing patient resistance to anticancer drugs	finding
and showed that aberrant expression values of certain hubs in the identified networks are associated with poor clinical outcomes	finding
We employed a pan-cell-state strategy	mechanism
using a tree-lineage multi-network inference algorithm	mechanism
Treegl	mechanism
and analyzed jointly microarray profiles obtained from different state-specific cell populations from this progression and reversion model of the breast cells We validated the findings using an external dataset	method
For urban driving	background
knowledge of ego-vehicle 's position is a critical piece of information that enables ad- vanced driver-assistance systems or self-driving cars to execute safety-related	background
autonomous driving maneuvers This is because	background
without knowing the current location	background
it is very hard to autonomously execute any driving maneuvers for the future	background
The existing solutions for localization rely on a combination of Global Navigation Satellite System ( GNSS )	background
an inertial mea- surement unit	background
and a digital map However	background
on urban driving environments	background
due to poor satellite geometry and disruption of radio signal reception	background
their longitudinal and lateral errors are too significant to be used to guide an autonomous system	background
showed promising results in terms of counting the number of road-lanes and the indices of the current road-lanes	finding
this work presents an effort of developing a vision-based lateral localization algorithm The algorithm aims at reliably counting	mechanism
with or without observations of lane-markings	mechanism
the number road-lanes and identifying the index of the road-lane on the roadway that our vehicle happens to be driving	mechanism
Testings the proposed algorithms against inter-city and inter-state highway videos	method
This approach outperformed the results of all other teams submissions in TRECVID SED 2012 on four of the seven event types	finding
We investigate a statistical approach with spatio-temporal features applied to seven event classes	mechanism
which were defined by the SED task	mechanism
This approach is based on local spatio-temporal descriptors	mechanism
called MoSIFT and generated by pair-wise video frames A Gaussian Mixture Model ( GMM ) is learned to model the distribution of the low level features	mechanism
Then for each sliding window	mechanism
the Fisher vector encoding [ improvedFV ] is used to generate the sample representation	mechanism
The model is learnt using a Linear SVM for each event	mechanism
The main novelty of our system is the introduction of Fisher vector encoding into video event detection	mechanism
Fisher vector encoding has demonstrated great success in image classification	mechanism
The key idea is to model the low level visual features as a Gaussian Mixture Model and to generate an intermediate vector representation for bag of features	mechanism
FV encoding uses higher order statistics in place of histograms in the standard BoW	mechanism
FV has several good properties : ( a ) it can naturally separate the video specific information from the noisy local features and ( b ) we can use a linear model for this representation	mechanism
We build an efficient implementation for FV encoding which can attain a 10 times speed-up over real-time	mechanism
We also take advantage of non-trivial object localization techniques to feed into the video event detection	mechanism
e	mechanism
g	mechanism
multi-scale detection and non-maximum suppression	mechanism
Background : Drug discovery and development has been aided by high throughput screening methods that detect compound effects on a single target	background
However when using focused initial screening	background
undesirable secondary effects are often detected late in the development process after significant investment has been made	background
The methods described are also likely to find widespread application outside drug discovery	background
such as for characterizing the effects of a large number of compounds or inhibitory RNAs on a large number of cell or tissue phenotypes	background
Results Conclusions : An average of nearly 60 % of all hits in the dataset were found after exploring only 3 % of the experimental space which suggests that active learning can be used to enable more complete characterization of compound effects than otherwise affordable	finding
This paper describes methods by constructing predictive models for many target responses to many compounds and using them to guide experimentation We demonstrate for the first time that by jointly modeling targets and compounds using descriptive features and using active machine learning methods	mechanism
accurate models can be built by doing only a small fraction of possible experiments	mechanism
The methods were evaluated by computational experiments using a dataset of 177 assays and 20	method
000 compounds constructed from the PubChem database	method
With the increasing availability of metropolitan transportation data	background
such as those from vehicle Global Positioning Systems ( GPSs ) and road-side sensors	background
and offer first-hand lessons learned from developing the system VAIT beats state-of-the-art methods and systems in terms of scalability	finding
efficiency and effectiveness and offers us an easy-to-use	finding
efficient and scalable platform to shed more light on intelligent transportation research	finding
We report our experience in building the Visual Analytics for Intelligent Transportation ( VAIT ) system	mechanism
which is the first system on real-life large-scale data sets Our key observation is that metropolitan transportation data are inherently visual as they are spatio-temporal around road networks Therefore	mechanism
we visualize and manage traffic data	mechanism
together with digital maps	mechanism
and support analytical queries through this interactive visual interface We discuss the technical challenges in data calibration	mechanism
storage visualization and query processing	mechanism
As a case study	method
we demonstrate VAIT on real-world taxi GPS and meter data sets from 15 000 taxis running for two months in a Chinese city of over 10 million people Based on our extensive empirical experiment results	method
Computers are often used in performance of popular music	background
but most often in very restricted ways	background
such as keyboard synthesizers where musicians are in complete control	background
or pre-recorded or sequenced music where musicians follow the computer 's drums or click track	background
An interesting and yet little-explored possibility is the computer as highly autonomous performer of popular music	background
capable of joining a mixed ensemble of computers and humans	background
and our experience with them	finding
We describe a general architecture	mechanism
and describe some early implementations	method
Fast Fourier transform algorithms on large data sets achieve poor performance on various platforms because of the inefficient strided memory access patterns	background
we demonstrate that Spiral generated hardware designs achieve close to theoretical peak performance of the targeted platform and offer significant speed-up ( up to 6	finding
5x ) compared to naive baseline algorithms	finding
In this paper with a two-level memory hierarchy requiring block data transfers	mechanism
using custom block data layouts	mechanism
Using the Kronecker product formalism	mechanism
we integrate our optimizations into Spiral framework	mechanism
In our evaluations	method
How can we detect suspicious users in large online networks ? Online popularity of a user or product ( via follows	background
page-likes etc	background
) can be monetized on the premise of higher ad click-through rates or increased sales	background
Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck	background
Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent ( but sometimes honest ) users	background
( b ) it is shown to be highly effective on real data and and with high precision identify many suspicious accounts which have persisted without suspension even to this day	finding
and propose fBox	mechanism
an algorithm designed Our algorithm has the following desirable properties : ( a ) it has theoretical underpinnings	mechanism
( c ) it is scalable ( linear on the input size )	mechanism
We evaluate fBox on a large	method
public 41	method
7 million node	method
1	method
5 billion edge who-follows-whom social graph from Twitter in 2010	method
As the complexity of software for Cyber-Physical Systems ( CPS ) rapidly increases	background
multi-core processors and parallel programming models such as OpenMP become appealing to CPS developers for guaranteeing timeliness Hence	background
a parallel task on multi-core processors is expected to become a vital component in CPS such as a self-driving car	background
where tasks must be scheduled in real-time	background
we achieve a resource augmentation bound of 3	finding
73 In other words	finding
any task set that is feasible on m unit-speed processors can be scheduled by the proposed algorithm on m processors that are 3	finding
73 times faster	finding
In this paper	mechanism
we extend the fork-join parallel task model we develop the task stretch transform Using this transform for global Deadline Monotonic scheduling for fork-join real-time tasks	mechanism
The proposed scheme is implemented on Linux/RK as a proof of concept	method
and ported to Boss	method
the self-driving vehicle that won the 2007 DARPA Urban Challenge	method
We evaluate our scheme on Boss by showing its driving quality	method
i	method
e	method
curvature and velocity profiles of the vehicle	method
A well-studied approach to the design of voting rules views them as maximum likelihood estimators ; given votes that are seen as noisy estimates of a true ranking of the alternatives	background
the rule must reconstruct the most likely true ranking	background
and show that for all rules in this family the number of samples required from the Mallows noise model is logarithmic in the number of alternatives	finding
and that no rule can do asymptotically better ( while some rules like plurality do much worse ) and find voting rules that are accurate in the limit for all noise models in such general families	finding
We define the family of pairwise-majority consistent rules Taking a more normative point of view	mechanism
we consider voting rules that surely return the true ranking as the number of samples tends to infinity ( we call this property accuracy in the limit ) ; this allows us to move to a higher level of abstraction We characterize the distance functions that induce noise models for which pairwise-majority consistent rules are accurate in the limit	mechanism
and provide a similar result for another novel family of position-dominance consistent rules These characterizations capture three well-known distance functions	mechanism
We study families of noise models that are parametrized by distance functions	method
The quality and effectiveness of the load following services provided by centralized control of thermostatically controlled loads depend highly on the communication requirements and the underlying cyberinfrastructure characteristics	background
Specifically ensuring end-user comfort while providing real-time demand response services depends on the availability of the information provided from the thermostatically controlled loads to the main controller regarding their operating statuses and internal temperatures	background
The results show that some improvement is possible for scenarios when loads are expected to be toggled frequently	finding
In this paper	mechanism
we introduce a moving horizon mean squared error state estimator with constraints which assumes a linear model without constraints	mechanism
Reading text on the Web is a challenging task for many people	background
such as those with cognitive impairments	background
reading difficulties or people who are learning a new language	background
In this paper we present a web browser plug-in The plug-in is freely available for Chrome and presents definitions and simpler synonyms on demand for the selected web text	mechanism
The tool was modified following the suggestions of 5 people ( 2 with diagnosed dyslexia ) who tested the tool using the think aloud protocol and undertook a subsequent interview	method
It also significantly contributes to CMU Team 's final submission in TRECVID-13 Multimedia Event Detection	background
the approach improves the baseline by up to 158 % in terms of the mean average precision	finding
We propose a novel method MultiModal Pseudo Relevance Feedback ( MMPRF which requires no search examples from the user	mechanism
Pseudo Relevance Feedback has shown great potential in retrieval tasks	mechanism
but previous works are limited to unimodal tasks with only a single ranked list	mechanism
To tackle the event search task which is inherently multimodal	mechanism
our proposed MMPRF takes advantage of multiple modalities and multiple ranked lists to enhance event search performance in a principled way The approach is unique in that it leverages not only semantic features	mechanism
but also non-semantic low-level features for event search in the absence of training data	mechanism
Evaluated on the TRECVID MEDTest dataset	method
Big graphs are everywhere	background
ranging from social networks and mobile call networks to biological networks and the World Wide Web Mining big graphs leads to many interesting applications including cyber security	background
fraud detection Web search	background
recommendation and many more	background
Our findings include anomalous spikes in the connected component size distribution	finding
the 7 degrees of separation in a Web graph	finding
and anomalous adult advertisers in the who-follows-whom Twitter social network	finding
In this paper we describe Pegasus built on top of MapReduce	mechanism
a modern distributed We introduce GIM-V	mechanism
an important primitive that Pegasus uses for its algorithms to analyze structures of large graphs We also introduce HEigen	mechanism
a large scale eigensolver which is also a part of Pegasus Both GIM-V and HEigen are highly optimized	mechanism
achieving linear scale up on the number of machines and edges	mechanism
and providing 9	mechanism
2x and 76x faster performance than their naive counterparts	mechanism
respectively	mechanism
Using Pegasus we analyze very large	method
real world graphs with billions of nodes and edges	method
and it gains promising performance	finding
In this paper	mechanism
we propose an algorithm which adaptively utilizes the related exemplars by cross-feature learning	mechanism
Ordinal labels are used to represent the multiple relevance levels of the related videos	mechanism
Label candidates of related exemplars are generated by exploring the possible relevance levels of each related exemplar via a cross-feature voting strategy	mechanism
Maximum margin criterion is then applied in our framework to discriminate the positive and negative exemplars	mechanism
as well as the related exemplars from different relevance levels	mechanism
We test our algorithm using the large scale TRECVID 2011 dataset	method
Multi-core CPUs with multiple levels of parallelism ( i	background
e	background
data level instruction level and task/core level ) have become the mainstream CPUs for commodity computing systems	background
The optimized ACCC solver achieves close to linear speedup ( SIMD width multiply core numbers ) comparing to scalar implementation and is able to solve a complete N-1 line outage AC contingency calculation of the Polish grid within one second on a commodity CPU	finding
It enables the complete ACCC as a real-time application on commodity computing systems	finding
Based on the multi-core CPUs	mechanism
in this paper we developed a high performance computing framework for AC contingency calculation ( ACCC ) Using Woodbury matrix identity based compensation method	mechanism
we transform and pack multiple contingency cases of different outages into a fine grained vectorized data parallel programming model	mechanism
We implement the data parallel programming model using SIMD instruction extension on x86 CPUs	mechanism
therefore fully taking advantages of the CPU core with SIMD floating point capability We also implement a thread pool scheduler for ACCC on multi-core CPUs which automatically balances the computing loads across CPU cores to fully utilize the multi-core capability	mechanism
We test the ACCC solver on the IEEE test systems and on the Polish 3000-bus system using a quad-core Intel Sandy Bridge CPU	method
demonstrate orders of magnitude of performance and power efficiency improvements compared with the traditional multithreaded software implementation on modern CPU	finding
This paper introduces a 3D-stacked logic-in-memory ( LiM ) system that integrates the 3D die-stacked DRAM architecture with the application-specific LiM IC The proposed system comprises a fine-grained rank-level 3D die-stacked DRAM device and extra LiM layers implementing logic-enhanced SRAM blocks that are dedicated to a particular application	mechanism
Through silicon vias ( TSVs ) are used for vertical interconnections providing the required bandwidth to support the high performance LiM computing We performed a comprehensive 3D DRAM design space exploration and exploit the efficient architectures to accelerate the computing that can balance the performance and power	mechanism
Our experiments	method
illustrate the analytical results	finding
for a class of deterministic distributed augmented Lagrangian methods The expressions for the convergence rates show the dependence on the underlying network parameters	mechanism
Simulations	method
Brain ? computer interfaces ( BCIs ) are being developed to assist paralyzed people and amputees by translating neural activity into movements of a computer cursor or prosthetic limb	background
Significance	background
The use of the instructed path task has the potential to accelerate the development of BCI systems and their clinical translation	background
Main results We demonstrate that monkeys are able to perform the instructed path task in a closed-loop BCI setting	finding
Here we introduce a novel BCI task paradigm Through this task	mechanism
we can push the performance limits of BCI systems	mechanism
we can quantify more accurately how well a BCI system captures the user ? s intent	mechanism
and we can increase the richness of the BCI movement repertoire Approach	mechanism
We have implemented an instructed path task	mechanism
wherein the user must drive a cursor along a visible path The instructed path task provides a versatile framework to increase the difficulty of the task and thereby push the limits of performance	mechanism
Relative to traditional point-to-point tasks	mechanism
the instructed path task allows more thorough analysis of decoding performance and greater richness of movement kinematics	mechanism
We further investigate how the performance under BCI control compares to native arm control	method
whether users can decrease their movement variability in the face of a more demanding task	method
and how the kinematic richness is enhanced in this task	method
Guided waves such as Lamb waves	background
are attractive tools for monitoring large civil infrastructures due to their sensitivity to damage	background
shows that the estimates all correspond to expected paths	finding
In this paper	mechanism
we present a method by combining sparse wavenumber analysis	mechanism
a methodology for accurately recovering multimodal and dispersive properties	mechanism
with additional l 1 minimization techniques Its application to experimental Lamb wave data	mechanism
Pipes carrying fluids under pressure are critical components in infrastructure and industry	background
We build a singular value decomposition ( SVD ) based change detector that is sensitive to the mass scatterer but insensitive to the changes produced by operational and environmental variations	mechanism
and	mechanism
we show examples of its successful performance on field experiments data	method
The convergence of mobile computing and cloud computing is predicated on a reliable	background
high-bandwidth end-to-end network	background
This basic requirement is hard to guarantee in hostile environments such as military operations and disaster recovery This article is part of a special issue on the edge of the cloud	background
how VM-based cloudlets that are located in close proximity to associated mobile devices	mechanism
ABSTRACT One of the main challenges in real-world application of guided-waves based nondestructive evaluation ( NDE ) of pipelines is their sensitivity to changes in environmental and operational conditions ( EOC ) that these structures are subject to In spite of many favorable characteristics of gu ided-waves for NDE of pipes	background
their multi-modal dispersive	background
and multi-path characteristics result in complex signa ls whose interpretation is a difficult task	background
For damage detection	mechanism
a linear supervised classification method	mechanism
namely linear discriminant analysis ( LDA )	mechanism
Principal components obtained through principal component analysis ( PCA )	mechanism
and Fourier transforms of the signals are two sets of damage-sensitive features ( DSF ) that are examined for LDA-based classification	mechanism
is applied to experimental guided-wave data recorded from a hot water piping system under regular operation	method
The effects of temperature and flow rate difference among testing and training datasets on ( A ) detection performance and ( B ) goodness of fit of the method to the data are investigated	method
Over the last several years there has been an explosion of powerful	background
affordable multi-touch devices	background
We describe the results that suggest users of Kinetica were able to explore multiple dimensions of data at once	finding
identify outliers and discover trends with minimal training	finding
In this paper we describe an approach that uses physics-based affordances that are easy to intuit	mechanism
constraints that are easy to apply and visualize	mechanism
and a consistent view as data is manipulated in order to promote data exploration and interrogation	mechanism
We provide a framework for exploring this problem space	mechanism
and	mechanism
an example proof of concept system called Kinetica	method
of a user study	method
Signal recovery from noisy measurements is an important task that arises in many areas of signal processing	background
using a recently developed framework of discrete signal processing on graphs	mechanism
We formulate graph signal denoising as an optimization problem and derive an exact closed-form solution expressed by an inverse graph filter	mechanism
as well as an approximate iterative solution expressed by a standard graph filter	mechanism
We evaluate the obtained algorithms by applying them to measurement denoising for temperature sensors and opinion combination for multiple experts	method
design Finally we summarize key lessons learned and hypothesis about additional hardware that could be used to ease the tracing of faults like short circuits and downed lines within microgrids	background
from a wirelessly managed microgrid deployment The system consists of a three-tiered architecture with a cloud-based monitoring and control service	mechanism
a local embedded gateway infrastructure and a mesh network of wireless smart meters deployed at 52 buildings Each smart meter device has an 802	mechanism
15	mechanism
4 radio that enables remote monitoring and control of electrical service The meters communicate over a scalable multi-hop TDMA network back to a central gateway that manages load within the system	mechanism
The gateway also provides an 802	mechanism
11 interface for an on-site operator and a cellular modem connection to a cloud-backend that manages and stores billing and usage data The cloud backend allows occupants in each home to pre-pay for electricity at a particular peak power limit using a text messaging service	mechanism
The system activates each meter within seconds and locally enforces power limits with provisioning for theft detection This paper provides a chronology of our deployment and installation strategy that involved GPS-based site mapping along with various network conditioning actions required as the network evolved	mechanism
Proceedings : AACR 104th Annual Meeting 2013 ; Apr 6-10	background
2013 ; Washington	background
DC Understanding tumors as evolutionary systems is an important area of study with far-reaching implications in diagnostic and treatment paradigms Computational phylogenetics is a valuable method for inferring tumor evolution in terms of evolutionary trees	background
phylogenies where paths in a tree correspond to possible tumor progression pathways The location of specific cell-types and patient samples in the tree provide information on tumor sub-types and development of heterogeneity	background
We previously developed a tumor phylogeny inference pipeline for array comparative genome hybridization ( aCGH ) -based tumor copy number profiles	background
Steps in the pipeline included extraction of robust progression markers from the data	background
which could differentiate stages of tumor evolution or the different paths in the tree	background
and assigning amplification states to the inferred markers in those stages We introduced a novel multi-sample model for amplicon identification and calling	background
HMMCNA which jointly extracted markers from and assigned amplification states to small sets of tumor aCGH profiles	background
HMMCNA employs a Hidden Markov Model ( HMM )	background
a probabilistic model	background
to classify data into normal and amplified states based on an underlying distribution for the two copy number states and a hidden state space of possible amplification states We assumed two possible amplification states per sample : normal ( 0 ) or amplified ( 1 )	background
Joint segmentation and calling is performed by identifying a most likely sequence of amplification states across all genomic sites probes and samples	background
Further steps include tuning the parameters of the HMM to handle noise-levels across different datasets	background
Citation Format : Ayshwarya Subramanian	background
Stanley Shackney Russell Schwartz	background
Inference of tumor phylogenetic markers from large copy number datasets	background
The introduction of this heuristic reduces the state space on average by 99 %	finding
gives a further reduction of 80 %	finding
Our method was able to quickly segment the data into sets of robust normal and amplified segments suitable for downstream phylogeny building	finding
The amplicons inferred carried several known markers of tumor progression	finding
We incorporate a heuristic prior to the HMM classification by first screening out amplification states not strongly supported at any individual genome coordinates We further reduce the set of possible amplification states based on the frequency of occurrence of the states by only allowing those states occuring at multiple aCGH probes or array genome coordinate	mechanism
This step accounts for the presence of random noise in the data and We demonstrate the method on a breast tumor aCGH dataset comprising copy number profiles derived from sectioned biopsy samples ( NCBI GEO [ GSE16672 ] [ 1 ]	mechanism
Navin et al	mechanism
2010 )	mechanism
Spliddit is a first-of-its-kind fair division website	mechanism
which In this note	mechanism
we discuss Spliddit 's goals	mechanism
methods and implementation	mechanism
Hybrid systems with both discrete and continuous dynamics are an important model for real-world cyber-physical systems	background
The key challenge is to ensure their correct functioning w	background
r	background
t	background
safety requirements	background
Promising techniques to ensure safety seem to be model-driven engineering to develop hybrid systems in a well-defined and traceable manner	background
and formal verification to prove their correctness	background
Their combination forms the vision of verification-driven engineering	background
Often hybrid systems are rather complex in that they require expertise from many domains ( e	background
g	background
robotics control systems	background
computer science software engineering	background
and mechanical engineering )	background
This paper introduces a verification-driven engineering toolset that extends our previous work on hybrid and arithmetic verification with This toolset makes it easier	mechanism
Data imbalance problem often exists in our real life dataset	background
especial for massive video dataset	background
demonstrate that our proposed algorithm has much more powerful performance than that of traditional machine learning algorithms	finding
and keeps stable and robust when different kinds of features are employed also prove that our EHS algorithm is efficient and effective	finding
and reaches top performance in four of seven events	finding
In this paper	mechanism
the data imbalance problem in semantic extraction under massive video dataset is exploited	mechanism
and enhanced and hierarchical structure ( called EHS ) algorithm is proposed In proposed algorithm	mechanism
data sampling filtering and model training are considered and integrated together compactly via hierarchical structure algorithm	mechanism
thus the performance of model can be improved step by step	mechanism
and is robust and stability with the change of features and datasets	mechanism
Experiments on TRECVID2010 Semantic Indexing Extended experiments on TRECVID2010 Surveillance Event Detection	method
Cake cutting is a common metaphor for the division of a heterogeneous divisible good	background
There are numerous papers that study the problem of fairly dividing a cake	background
where for the first time our notion of dominant strategy truthfulness is the ubiquitous one in social choice and computer science	mechanism
We design both deterministic and randomized cake cutting mechanisms that are truthful and fair under different assumptions with respect to the valuation functions of the agents	mechanism
Mobile crowdsensing is becoming a vital technique for environment monitoring	background
infrastructure management and social computing	background
and to offer our initial thoughts on the potential solutions to lowering the barriers	mechanism
suggest required modifications	mechanism
In this work	method
we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols and	method
Smart metering systems in distribution networks provide near real-time	background
two-way information exchange between end users and utilities	background
enabling many advanced smart grid technologies	background
The demonstration shows the feasibility of our proposed privacy preserving protocol for advanced smart grid technologies which includes load management and retail level electricity market support	finding
In this work we propose a secure multi-party computation ( SMC ) based Using the proposed SMC protocol	mechanism
a utility is able to perform advanced market based demand management algorithms without knowing the actual values of private end user consumption and configuration data	mechanism
Using homomorphic encryption	mechanism
billing is secure and verifiable	mechanism
We implemented a demonstration system that includes a graphical user interface and simulates real-world network communication of the proposed SMC-enabled smart meters	method
Rating data is ubiquitous on websites such as Amazon	background
TripAdvisor or Yelp	background
Since ratings are not static but given at various points in time	background
a temporal analysis of rating data provides deeper insights into the evolution of a product 's quality	background
we show the effectiveness of our method and we present interesting discoveries on multiple real world datasets	finding
We propose a Bayesian model that represents the rating data as sequence of categorical mixture models	mechanism
In contrast to existing methods	mechanism
our method does not require any aggregation of the input but it operates on the original time stamped data To capture the dynamic effects of the ratings	mechanism
the categorical mixtures are temporally constrained : Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time	mechanism
Our method automatically determines the intervals where anomalies occur	mechanism
and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions we propose an efficient algorithm combining principles from variational inference and dynamic programming	mechanism
In our experimental study	method
We introduce the Plug-Level Appliance Identification Dataset ( PLAID )	mechanism
a public and crowd-sourced dataset for load identification research consisting of short voltage and current measurements ( in the order of a few seconds ) for different residential appliances	mechanism
PLAID currently contains measurements for more than 200 different appliance instances	mechanism
representing 11 appliance classes	mechanism
and totaling more than a thousand records	mechanism
In this demo we summarize the existing dataset	mechanism
demonstrate how new records can be added to the library using a web interface and	mechanism
finally	mechanism
walk through a live example of how the library can be integrated into an existing non-intrusive load monitoring ( NILM ) algorithm framework	method
Abstraction has emerged as a key component in solving extensive-form games of incomplete information	background
However lossless abstractions are typically too large to solve	background
so lossy abstraction is needed	background
show that it finds a lossless abstraction when one is available and lossy abstractions when smaller abstractions are desired	finding
We introduce a theoretical framework that can be used The framework uses a new notion for mapping abstract strategies to the original game	mechanism
and it leverages a new equilibrium refinement for analysis	mechanism
Using this framework	mechanism
we develop the first general lossy extensive-form game abstraction method with bounds While our framework can be used for lossy abstraction	mechanism
it is also a powerful tool for lossless abstraction if we set the bound to zero	mechanism
Prior abstraction algorithms typically operate level by level in the game tree	mechanism
We introduce the extensive-form game tree isomorphism and action subset selection problems	mechanism
both important problems for computing abstractions on a level-by-level basis We show that the former is graph isomorphism complete	mechanism
and the latter NP-complete	mechanism
We also prove that level-by-level abstraction can be too myopic and thus fail to find even obvious lossless abstractions	mechanism
Experiments	method
Abstract Image patterns at different spatial levels are well organized	background
such as regions within one image and feature points within one region	background
These classes of spatial structures are hierarchical in nature	background
demonstrate that the proposed approach has better performance of region tagging than the current state of the art methods	finding
we propose an approach	mechanism
called Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation This approach consists of two steps : region representation and region reconstruction	mechanism
In the first step	mechanism
rather than using the l 1 -norm as it is commonly done in sparse coding	mechanism
we add a hierarchical structure to the process of sparse coding and form a framework of tree-guided dictionary learning	mechanism
In this framework	mechanism
the hierarchical structures among feature points	mechanism
regions and images are encoded by forming a tree-guided multi-task learning process	mechanism
With the learned dictionary	mechanism
we obtain a better representation of training and testing regions	mechanism
In the second step	mechanism
we propose to use a sub-hierarchical structure to guide the sparse reconstruction for testing regions	mechanism
i	mechanism
e	mechanism
the structure between regions and images	mechanism
Thanks to this hierarchy	mechanism
the obtained reconstruction coefficients are more discriminate	mechanism
Finally tags are propagated to testing regions by the learned reconstruction coefficients	mechanism
Extensive experiments on three public benchmark image data sets	method
ABSTRACT Guided wave ultrasonics is an attractive technique for structural health monitoring	background
especially on pressurized pipes	background
The results show that the framework can effectively detect the presence of a scatterer and is robust to large environmental and operational variations	finding
We developed a damage detection method based on singular value decomposition ( SVD ) that is robust to those benign variations We further develop an online novelty detection framework based on our SVD method	mechanism
We examine the framework with both synthetic simulations and field experimental data	method
The execution of an agent 's complex activities	background
comprising sequences of simpler actions	background
sometimes leads to the clash of conflicting functions that must be optimized	background
These functions represent satisfaction	background
short-term as well as long-term objectives	background
costs and individual preferences	background
The way that these functions are weighted is usually unknown even to the decision maker	background
But if we were able to understand the individual motivations and compare such motivations among individuals	background
then we would be able to actively change the environment so as to increase satisfaction and/or improve performance	background
Our results show that our method is not only useful	finding
but also performs much better than the previous methods	finding
in terms of accuracy	finding
efficiency and scalability	finding
A novel algorithm is proposed	mechanism
We also present a methodology that allows researchers through the minimization of an error measure between the current description and the observed behaviors	mechanism
based on observations of such an agent during the fulfillment of a series of complex activities ( called sequential decisions in our work )	method
This work was validated using not only a synthetic dataset representing the motivations of a passenger in a public transportation network	method
but also real taxi drivers ' behaviors from their trips in an urban network	method
We propose a new framework The presented framework leads to a systematic design of iterative algorithms that compute the consensus exactly	mechanism
are guaranteed to converge in finite time	mechanism
are computationally efficient	mechanism
and require no online memory	mechanism
We demonstrate that our approach is applicable to a broad class of networks For remaining networks	mechanism
our framework leads to the construction of approximating algorithms for consensus that are also guaranteed to compute in finite time	mechanism
Our approach is inspired by graph filters introduced by the theoretical framework of signal processing on graphs	mechanism
The primary goal of a vehicular headlight is to improve safety in low-light and poor weather conditions The typical headlight however has very limited flexibility - switching between high and low beams	background
turning off beams toward the opposing lane or rotating the beam as the vehicle turns - and is not designed for all driving environments	background
In this talk	finding
we will lay out the engineering challenges in building this headlight and share our experiences	finding
We will describe a new DMD-based design for a headlight that can be programmed to perform several tasks simultaneously and For example	mechanism
we will be able to drive with high-beams without glaring any other driver and we will be able to see better during rain and snowstorms when the road is most treacherous to drive	mechanism
The headlight can also increase contrast of lanes	mechanism
markings and sidewalks and can alert drivers to sudden obstacles	mechanism
with the prototypes developed over the past two years	method
Estimating the number of people within a room is important for a wide variety of applications including : HVAC load management	background
scheduling room allocations and guiding first responders to areas with trapped people	background
we show that our approach is able to capture the number of people in a wide-variety of room configurations with people counting accuracy below 10 % of the maximum room capacity count with as few as two training points	finding
In this paper	mechanism
we present an active sensing technique that uses changes in a room 's acoustic properties Frequency dependent models of reverberation and room capacity are often used when designing auditoriums and concert halls	mechanism
We leverage this property by using measured changes in the ultrasonic spectrum reflected back from a wide-band transmitter to estimate occupancy	mechanism
A centrally located beacon transmits an ultrasonic chirp and then records how the signal dissipates over time By analyzing the frequency response over the chirp 's bandwidth at a few known occupancy levels	mechanism
we are able to extrapolate the response as the number of people in the room changes We explore the design of an excitation signal that best senses the environment with the fewest number of training samples	mechanism
Finally we provide a simple mechanism that allows our system	mechanism
Through experimentation	method
Organizations collect personal information from individuals to carry out their business functions	background
Federal privacy regulations	background
such as the Health Insurance Portability and Accountability Act ( HIPAA )	background
mandate how this collected information can be shared by the organizations	background
It is thus incumbent upon the organizations to have means to check compliance with the applicable regulations	background
Prior work by Barth et	background
al	background
introduces two notions of compliance	background
weak compliance ( WC ) and strong compliance ( SC )	background
WC ensures that present requirements of the policy can be met whereas SC also ensures obligations can be met	background
An action is compliant with a privacy policy if it is both weakly and strongly compliant	background
We prove that checking WC is feasible whereas checking SC is undecidable	finding
To this end	mechanism
we present a policy specification language based on a restricted subset of first order temporal logic ( FOTL ) We then formally specify WC and SC for policies of our form	mechanism
We then formally specify the property WC entails SC	mechanism
denoted by which requires that each weakly compliant action is also strongly compliant	mechanism
To check whether an action is compliant with such a policy	mechanism
it is sufficient to only check whether the action is weakly compliant with that policy We also prove that when a policy has the -property	mechanism
the present requirements of the policy reduce to the safety requirements imposed by We then develop a sound	mechanism
semi-automated technique for checking whether practical policies have the -property	mechanism
We finally use HIPAA as a case study to demonstrate the efficacy of our policy analysis technique	method
The increase in the number of bloggers and the amount of information diffused in the blogosphere makes the blogosphere an important medium through which to communicate and exchange information	background
Accordingly the interest in understanding the nature of the information diffusion in the blogosphere has also been increased	background
BlogCast a functionality provided by blog-service providers to expose a high quality post on the portal main page	finding
is found to be one of the main causes of the information diffusion without explicit relationships	finding
We analyze the characteristics of the information diffusion through the BlogCast and its halo effect on the bloggers whose post has been exposed on the portal main page	method
In addition we examine the sustainability of the halo effect of the BlogCast over time	method
Many of the visual questions that blind people ask can not be easily answered with a single image or a short response	background
especially when questions are of an exploratory nature	background
e	background
g	background
what is in this area	background
or what tools are available on this work bench	background
We introduce RegionSpeak with fewer interactions	mechanism
RegionSpeak helps blind users capture all of the relevant visual information using an interface designed to support stitching multiple images together We use a parallel crowdsourcing workflow that asks workers to define and describe regions of interest	mechanism
allowing even complex images to be described quickly The regions and descriptions are displayed on an auditory touchscreen interface	mechanism
allowing users to know what is in a scene and how it is laid out	mechanism
The proliferation of Bluetooth Low-Energy ( BLE ) chipsets on mobile devices has lead to a wide variety of user-installable tags and beacons designed for location-aware applications	background
that our system can estimate three-dimensional beacon location with a Euclidean distance error of 16	finding
1cm and can generate maps with room measurements with a two-dimensional Euclidean distance error of 19	finding
8cm	finding
we saw that the system can identify Non-Line-Of-Sight ( NLOS ) signals with over 80 % accuracy and track a user 's location to within less than 100cm	finding
In this paper	mechanism
we present the Acoustic Location Processing System ( ALPS )	mechanism
a platform that augments BLE transmitters with ultrasound in a manner A user places three or more beacons in an environment and then walks through a calibration sequence with their mobile device where they touch key points in the environment like the floor and the corners of the room This process automatically computes the room geometry as well as the precise beacon locations without needing auxiliary measurements	mechanism
Once configured the system can track a user 's location referenced to a map	mechanism
The platform consists of time-synchronized ultrasonic transmitters that utilize the bandwidth just above the human hearing limit	mechanism
where mobile devices are still sensitive and can detect ranging signals	mechanism
To aid in the mapping process	mechanism
the beacons perform inter-beacon ranging during setup	mechanism
Each beacon includes a BLE radio that can identify and trigger the ultrasonic signals	mechanism
By using differences in propagation characteristics between ultrasound and radio	mechanism
the system can classify if beacons are within Line-Of-Sight ( LOS ) to the mobile phone In cases where beacons are blocked	mechanism
we show how the phone 's inertial measurement sensors can be used to supplement localization data	mechanism
We experimentally evaluate When tested in six different environments	method
Real-time captioning provides deaf and hard of hearing ( DHH ) users with access to spoken content during live events	background
and the web has allowed these services to be provided via remotely- located captioning services	background
and for web content itself	background
and then suggest future work that builds on these insights	background
show that by providing users with a tool to more easily track their place in a transcript while viewing live video	finding
it is possible for them to follow visual content that might otherwise have been missed	finding
Both pausing and highlighting have a positive impact on students ' scores on comprehension tests	finding
but highlighting is preferred to pausing	finding
and yields nearly twice as large of an improvement We then discuss several issues with captioning that we observed during our design process and user study	finding
We explore methods by allowing users to more easily switch their gaze between multiple visual information sources	mechanism
In this paper	mechanism
we explore pausing and highlighting as a means of helping DHH students keep up with live classroom content by helping them track their place when reading text involving visual references	mechanism
Our experiments	method
demonstrates more than two orders of magnitude of performance and energy efficiency improvements compared with the traditional multithreaded software implementation on modern processors	finding
This paper introduces a 3D-stacked logic-in-memory ( LiM ) system We build a customized content addressable memory ( CAM ) hardware structure to exploit the inherent sparse data patterns and model the LiM based hardware accelerator layers that are stacked in between DRAM dies for the efficient sparse matrix operations	mechanism
Through silicon vias ( TSVs ) are used to provide the required high inter-layer bandwidth Furthermore	mechanism
we adapt the algorithm and data structure to fully leverage the underlying hardware capabilities	mechanism
and develop the necessary design framework to facilitate the design space evaluation and LiM hardware synthesis	mechanism
Our simulation	method
Automatic face recognition performance has been steadily improving over years of research	background
We show significant performance improvements in verification rates and also demonstrate the resilience of the proposed method with respect to degrading input quality	finding
We find that the proposed technique is able to match non-frontal images to other non-frontal images of varying angles	finding
We propose a method that relies on two fundamental components : ( a ) A 3D modeling step For this purpose	mechanism
we extend a recent technique for efficient synthesis of 3D face models called 3D Generic Elastic Model	mechanism
( b ) A sparse feature extraction step using subspace modeling and l1-minimization to induce pose-tolerance in coefficient space This in return enables the synthesis of an equivalent frontal-looking face	mechanism
which can be used towards recognition	mechanism
compared commercial matchers	method
Autonomous agents that operate as components of dynamic spatial systems are becoming increasingly popular and mainstream	background
Applications can be found in consumer robotics	background
in road rail	background
and air transportation	background
manufacturing and military operations	background
In this article	mechanism
we discuss reasoning approaches	mechanism
which requires a sufficiently detailed description of the agents behavior and environment but may still be conducted in a qualitative manner	mechanism
We introduce a conceptual reference model	mechanism
which summarizes the current understanding of the characteristics of dynamic spatial systems based on a catalog of evaluation criteria derived from the model We provide a comparative summary of the modeling features	mechanism
discuss lessons learned	mechanism
and introduce a research roadmap	mechanism
We survey logic-based qualitative and hybrid modeling and commonsense reasoning approaches with respect to their features for describing and analyzing dynamic spatial systems in general	method
and the actions of autonomous agents operating therein in particular	method
We assess the modeling features provided by logic-based qualitative commonsense and hybrid approaches for projection	method
planning simulation and verification of dynamic spatial systems	method
with the same asymptotic guarantees as the best sequential algorithm	finding
We show an improved parallel algorithm with a small fraction of the edges in between These decompositions form critical subroutines in a number of graph algorithms	mechanism
Our algorithm builds upon the shifted shortest path approach introduced in [ Blelloch	mechanism
Gupta Koutis Miller	mechanism
Peng Tangwongsan SPAA 2011 ]	mechanism
By combining various stages of the previous algorithm	mechanism
we obtain a significantly simpler algorithm	mechanism
Human computation allows computer systems to leverage human intelligence in computational processes	background
In this paper	mechanism
we present techniques	mechanism
and then discuss how and when to use them	mechanism
Large-scale collaboration systems often separate their content from the deliberation around how that content was produced	background
where we found that surfacing deliberation generally led to decreases in perceptions of quality for the article under consideration	finding
especially - but not only - if the discussion revealed conflict The effect size depends on the type of editors ' interactions Finally	finding
this decrease in actual article quality rating was accompanied by self-reported improved perceptions of the article and Wikipedia overall	finding
In this paper we report the results of an experiment	method
This can potentially lead to a deeper understanding of programming	background
bringing students closer to true computational thinking	background
We describe a three-stage model beginning with a simple	mechanism
highly scaffolded programming environment ( Kodu ) and progressing to more challenging frameworks ( Alice and Lego NXT-G )	mechanism
In moving between frameworks	mechanism
students explore the similarities and differences in how concepts such as variables	mechanism
conditionals and looping are realized Some novel strategies for teaching with Kodu are outlined	mechanism
Finally we briefly report on our methodology and select preliminary results from a pilot study using this curriculum with students ages 10-17	method
including several with disabilities	method
Proceedings : AACR Annual Meeting 2014 ; April 5-9	background
2014 ; San Diego	background
CA Experimental techniques for assessing heterogeneity in tumor cell populations have undergone great advances	background
In continuing work	background
we are exploring extension of these approaches to better modeling and analysis of tumor evolution using single-cell sequencing data and to more detailed models of tumor evolution	background
The evolutionary tree models reveal robust features of evolutionary processes distinguishing progression stages and predicting future progression that lead to improved classification accuracy relative to predictions from cellular heterogeneity data alone	finding
Our software is freely available at ftp : //ftp	finding
ncbi	finding
nlm	finding
nih	finding
gov/pub/FISHtrees	finding
We describe computational methods by developing computer algorithms for building phylogenetic trees describing evolution of individual tumors based on copy numbers of fluorescence in situ hybridization ( FISH ) probes from single cells in these tumors	mechanism
These algorithms reconstruct evolutionary trees for observed cell populations so as to heuristically minimize the number of mutational events needed to explain the observed combinations of probe counts by evolution from a common diploid ancestral cell We have extended this work from initial simple evolutionary models of evolution by single copy number changes to account for distinct mechanisms of evolution at the gene	mechanism
chromosome or whole-genome scale	mechanism
with potentially different rates of evolution by mutation type	mechanism
We have applied these algorithms to several FISH data sets	method
including cervical cancers probed for four genes ( LAMP3	method
PROX1 PRKAA1 and CCND1 ) measured for up to 250 cells of paired primary and metastatic samples from 16 patients	method
head-and-neck cancers probed for four genes ( TERC	method
CCND1 EGFR and TP53 ) measured on up to 250 cells per patient for 65 patients at four tumor stages	method
prostate cancers probed for six genes ( TBL1XR1	method
CTTNBP2 MYC PTEN	method
MEN1 and PDGFB ) measured for up to 407 cells in 6 non-progressive and 7 progressive carcinomas	method
and breast cancers probed for eight genes ( COX-2	method
MYC CCND1 HER-2	method
ZNF217 DBC2 CDH1 and TP53 ) measured on up to 220 cells of paired of ductal carcinoma in situ and invasive ductal carcinoma samples from 13 patients We have then applied statistical and machine learning analysis to examine the ability of these trees to classify tumors by stage or potential for progression	method
These ndings coupled with recent results on naturallyrehearsing password schemes	background
suggest that 4 PAO stories couldbe used to create usable and strong passwords for 14 sensitiveaccounts following this spaced repetition schedule	background
possibly witha few extra upfront rehearsals stories	background
These ndings yield concrete advice for improving constructionsof password management schemes and future user studies	background
the best results were obtained whenusers initially returned after 12 hours and then in 1:5 increasingintervals : 77 % of the participants successfully recalled all 4stories in 10 tests over a period of 158 days Much of theforgetting happened in the rst test period ( 12 hours ) : 89 % of participants who remembered their stories during the rsttest period successfully remembered them in every subsequentround In addition	finding
we nd statisticallysignicant evidence that with 8 tests over 64 days users whowere asked to memorize 4 PAO stories outperform users whoare given 4 random action-object pairs	finding
but with 9 tests over 128days the advantage is not signicant Furthermore	finding
there is aninterference effect across multiple PAO stories : the recall rate of100 % ( resp	finding
90 % ) for participants who were asked to memorize1 PAO story ( resp	finding
2 PAO stories ) is signicantly better than therate for participants who were asked to memorize 4 PAO	finding
time	method
Remote research participants were asked to memorize 4 Person-Action-Object ( PAO ) stories where they chose a famous personfrom a drop-down list and were given machine-generated randomaction-object pairs Users were also shown a photo of a scene andasked to imagine the PAO story taking place in the scene ( e	method
g	method
Bill Gatesswallowingbike on a beach )	method
Subsequently theywere asked to recall the action-object pairs when prompted withthe associated scene-person pairs following a spaced repetitionschedule over a period of 127+ days	method
While we evaluated severalspaced repetition schedules	method
Non-Intrusive Load Monitoring ( NILM ) is a set of techniques used to estimate the electricity consumed by individual appliances in a building from measurements of the total electrical consumption	background
Most commonly NILM works by first attributing any significant change in the total power consumption ( also known as an event ) to a specific load and subsequently using these attributions ( i	background
e	background
the labels for the events ) to estimate energy for each load	background
show that the framework can learn data-driven models based on event labels and use that to estimate energy with lower error margins ( e	finding
g	finding
1	finding
142	finding
3 % ) than when using the heuristic models used by others	finding
In this paper	mechanism
we present a framework based on classification labels and aggregate power measurements that can help Our framework automatically builds models for appliances to perform energy estimation The model relies on feature extraction	mechanism
clustering via affinity propagation	mechanism
perturbation of extracted states to ensure that they mimic appliance behavior	mechanism
creation of finite state models	mechanism
correction of any errors in classification that might violate the model	mechanism
and estimation of energy based on corrected labels	mechanism
We evaluate our framework on 3 houses from standard datasets in the field and	method
It is becoming more and more evident that the mechanical forces previously taken for granted actually play a pivotal role in influencing biological phenomenon from cancer metastases to vasculogenesis	background
Recent literature provides strong evidence for a causative link between the mechanical stretching of the cytoskeleton and the release of mechanotransductive signaling molecules	background
Our model for the mechanotransductive release of signaling factors represents a potentially versatile mechanistic platform for examining biophysical interactions that link mechanical stimulus at the cellular level to response at the protein level	background
Here we present a model that integrates actin filament network remodeling under stretch with a novel biophysical model of molecular release stretch is applied to a model of the actin filament network	mechanism
the distribution of bond angles in the network transitions from a more peaked to a flatter distribution	mechanism
High variability is observed from site-to-site within the network upon an applied stretch	mechanism
with a nearly uniform distribution of difference between stretched and unstretched angles ( delta angles ) at high levels of stretching	mechanism
We used our approach to explore various thresholding models of how actin filament network deformations might influence rates of release of bound signaling molecules These models allow us to project how a biochemical response might appear from a given applied mechanical stimulus	mechanism
We validate these simulations using experimental data and use our model to then test different predictive capabilities of how mechanotransduction may function	method
Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval	background
results validate the efficacy and the efficiency of the proposed method	finding
The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance Specifically	finding
the proposed method ( with reranking ) achieves the best result on the challenging TRECVID Multimedia Event Detection ( MED ) zero-example task	finding
It only takes 0	finding
2 second on a single CPU core to search a collection of 100 million Internet videos	finding
This paper proposes a scalable solution	mechanism
The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index	mechanism
The proposed adjustment model relies on a concise optimization framework with interpretations	mechanism
The proposed index leverages the text-based inverted index for video retrieval	mechanism
Experimental	method
we prove that $ { { \cal Q } { \cal D } } $ -learning	finding
a $ \rm consensus + innovations $ algorithm with mixed time-scale stochastic dynamics	finding
converges asymptotically almost surely to the desired value function and to the optimal stationary control policy at each network agent	finding
The paper develops $ { { \cal Q } { \cal D } } $ -learning	mechanism
a distributed version of reinforcement $ Q $ -learning	mechanism
The network agents minimize a network-averaged infinite horizon discounted cost	mechanism
by local processing and by collaborating through mutual information exchange over a sparse ( possibly stochastic ) communication network The agents respond differently ( depending on their instantaneous one-stage random costs ) to a global controlled state and the control actions of a remote controller	mechanism
When each agent is aware only of its local online cost data and the inter-agent communication network is weakly connected	method
In a Stackelberg Security Game	background
a defender commits to a randomized deployment of security resources	background
and an attacker best-responds by attacking a target that maximizes his utility	background
via an online learning approach	mechanism
We are interested in algorithms that prescribe a randomized strategy for the defender at each step against an adversarially chosen sequence of attackers	mechanism
and obtain feedback on their choices ( observing either the current attacker type or merely which target was attacked We design no-regret algorithms whose regret ( when compared to the best fixed strategy in hindsight ) is polynomial in the parameters of the game	mechanism
and sublinear in the number of times steps	mechanism
Detection of neuronal cell differentiation is essential to study cell fate decisions under various stimuli and/or environmental conditions	background
Many tools exist that quantify differentiation by neurite length measurements of single cells	background
However quantification of differentiation in whole cell populations remains elusive so far	background
In conclusion this enables long-term	background
large-scale studies of cell populations with minimized costs and efforts for detecting effects of external manipulation of neuronal cell differentiation	background
which we confirmed	finding
by supervised classification	mechanism
Using nerve growth factor induced differentiation of PC12 cells	mechanism
we monitor the changes in cell morphology over days by phase-contrast live-cell imaging	mechanism
For general applicability	mechanism
the classification procedure starts out with many features to identify those that maximize discrimination of differentiated and undifferentiated cells and to eliminate features sensitive to systematic measurement artifacts	mechanism
The resulting image analysis determines the optimal post treatment day for training and achieves a near perfect classification of differentiation	mechanism
Our approach allows to monitor neuronal cell populations repeatedly over days without any interference	mechanism
It requires only an initial calibration and training step and is thereafter capable to discriminate further experiments	mechanism
in technically and biologically independent as well as differently designed experiments	method
Mechanotransduction has been divided into mechanotransmission	background
mechanosensing and mechanoresponse	background
Our results suggest that filamin-FilGAP mechanotransduction response is best explained by a bandpass mechanism favoring release when crosslinking angles fall outside of a specific range and finds that a more disordered actin network may allow a cell to more finely tune control of molecular release enabling a more robust response	finding
through a multiscale model linking these three phases Our model incorporates a discrete network of actin filaments and associated proteins that responds to stretching through geometric relaxation Our model further investigates the difference between ordered versus disordered networks	mechanism
We assess three potential activating mechanisms at mechanosensitive crosslinks as inputs to a mixture model of molecular release and benchmark each using experimental data of mechanically-induced Rho GTPase FilGAP release from actin-filamin crosslinks	method
Understanding the unique biochemical and physical differences between typical in vitro experimental systems and the in vivo environment of a living cell is a question of great importance in building and interpreting reliable models of complex reaction systems	background
Virus capsids make an excellent model system for such questions because they tend to have few components	background
making them amenable to in vitro and modeling studies	background
yet their assembly can be described by enormously complex networks of possible reactions that can not be resolved by any current experimental technology We have previously attempted to bridge the gap between the complexity of the system and the limitations of data for tracking detailed assembly pathways using simulation-based model inference	background
learning kinetic parameters of coarse-grained rule models by fitting simulations to light scattering data from in vitro capsid assembly systems	background
Results suggest surprisingly complex and often counterintuitive mechanisms by which crowding or nucleic acids can alternately promote or inhibit assembly for different virus and assembly conditions	finding
using coarse-grained biophysical models and suggest how these adjustments to fine-scale interactions may alter high-level pathway selection	mechanism
from a series of virus capsid models	method
Abstract Residential electricity users need more detail than monthly bills to reduce consumption	background
Using national average penetration rates	background
the Residential Energy Consumption Survey ( RECS )	background
estimates that 42 unique appliances account for 93 % of electricity consumption	background
while 12 appliances account for 80 % of average household electric load These results can be used to design and maximize the value of residential energy information and management systems	background
find that eight appliances are responsible for 80 % of a household 's electric load in the United States	finding
It is concluded that RECS can not be used as a representative household as it overestimates the number of appliances that contribute to a household electric load	finding
The number of significant appliances is affected by appliance ownership and use	finding
which is more variable between homes than between census divisions	finding
A typical scenario is developed from national and regional penetration rates and Four household scenarios are developed : a house that uses electric appliances	method
gas appliances the average household	method
and typical household	method
A genome-wide association study involves examining a large number of single-nucleotide polymorphisms ( SNPs ) to identify SNPs that are significantly associated with the given phenotype	background
while trying to reduce the false positive rate	background
show that there is a significant advantage in incorporating the prior knowledge on linkage disequilibrium structure for marker identification under whole-genome association	finding
In this paper	mechanism
we propose a new approach called stochastic block lasso that exploits prior knowledge on linkage disequilibrium structure in the genome such as recombination rates and distances between adjacent SNPs in order to increase the power of detecting true associations while reducing false positives Following a typical linear regression framework with the genotypes as inputs and the phenotype as output	mechanism
our proposed method employs a sparsity-enforcing Laplacian prior for the regression coefficients	mechanism
augmented by a first-order Markov process along the sequence of SNPs that incorporates the prior information on the linkage disequilibrium structure	mechanism
The Markov-chain prior models the structural dependencies between a pair of adjacent SNPs	mechanism
and allows us to look for association SNPs in a coupled manner	mechanism
combining strength from multiple nearby SNPs	mechanism
Our results on HapMap-simulated datasets and mouse datasets	method
The agents are sparsely connected and each of them observes a strict subset of the state vector	mechanism
The distributed algorithm that we propose enables each agent with bounded mean-squared error	mechanism
To achieve this	mechanism
the ratio of the algebraic connectivity and the largest eigenvalue of the graph Laplacian has to be larger than a lower bound determined by the spectral radius of the system 's dynamics matrix This extends the notion of Network Tracking Capacity introduced by other authors in prior work We accomplish this by introducing a new class of estimation algorithm of dynamical systems that	mechanism
besides a ( consensus + innovations ) term	mechanism
also includes consensus on the innovations	mechanism
Acoustruments adds a new method to the toolbox HCI practitioners and researchers can draw upon	background
while introducing a cheap and passive method for adding interactive controls to consumer products	background
show that Acoustruments can achieve 99 % accuracy with minimal training	finding
is robust to noise	finding
and can be rapidly prototyped	finding
We introduce Acoustruments : low-cost	mechanism
passive and power-less mechanisms	mechanism
made from plastic	mechanism
Through a structured exploration	mechanism
we identified an expansive vocabulary of design primitives	mechanism
providing building blocks for the construction of tangible interfaces utilizing smartphones ' existing audio functionality By combining design primitives	mechanism
familiar physical mechanisms can all be constructed from passive elements	mechanism
On top of these	mechanism
we can create end-user applications with rich	mechanism
tangible interactive functionalities	mechanism
Our experiments	method
With the widespread availability of video cameras	background
we are facing an ever-growing enormous collection of unedited and unstructured video data	background
demonstrating the effectiveness of online video highlighting	finding
In this work	mechanism
we propose online video highlighting	mechanism
a principled way of generating short video summarizing the most important and interesting contents of an unedited and unstructured video	mechanism
costly both time-wise and financially for manual processing	mechanism
Specifically our method learns a dictionary from given video using group sparse coding	mechanism
and updates atoms in the dictionary on-the-fly	mechanism
A summary video is then generated by combining segments that can not be sparsely reconstructed using the learned dictionary	mechanism
The online fashion of our proposed method enables it to process arbitrarily long videos and start generating summaries before seeing the end of the video	mechanism
Moreover the processing time required by our proposed method is close to the original video length	mechanism
achieving quasi real-time summarization speed	mechanism
Theoretical analysis together with experimental results on more than 12 hours of surveillance and YouTube videos are provided	method
Finally we discuss some ramifications of this work for understanding Twitter usage and management of one 's privacy	background
Some significant differences were discovered between the two collections	finding
namely in the clients used to post them	finding
their conversational aspects	finding
the sentiment vocabulary present in them	finding
and the days of the week they were posted	finding
However in other dimensions for which analysis was possible	finding
no substantial differences were found	finding
This paper describes collected over a continuous one-week period from a set of 292K Twitter users We examine several aggregate properties of deleted tweets	method
including their connections to other tweets ( e	method
g	method
whether they are replies or retweets )	method
the clients used to produce them	method
temporal aspects of deletion	method
and the presence of geotagging information	method
Background Disease progression in the absence of therapy varies significantly in HIV-1 infected individuals	background
we performed a comparative miRNA and mRNA microarray analysis using PBMCs obtained from infected individuals with distinct viral load and CD4 counts	method
In the Architecture	background
Engineering and Construction ( AEC ) domain	background
semantically rich 3D information models are increasingly used throughout a facility 's life cycle for diverse applications	background
such as planning renovations	background
space usage planning	background
and managing building maintenance	background
These models which are known as building information models ( BIMs )	background
are often constructed using dense	background
three dimensional ( 3D ) point measurements obtained from laser scanners	background
Laser scanners can rapidly capture the as-is conditions of a facility	background
which may differ significantly from the design drawings	background
This paper presents a method Our algorithm is capable of identifying and modeling the main visible structural components of an indoor environment ( walls	mechanism
floors ceilings windows	mechanism
and doorways ) despite the presence of significant clutter and occlusion	mechanism
which occur frequently in natural indoor environments	mechanism
Our method begins by extracting planar patches from a voxelized version of the input point cloud	mechanism
The algorithm learns the unique features of different types of surfaces and the contextual relationships between them and uses this knowledge to automatically label patches as walls	mechanism
ceilings or floors Then	mechanism
we perform a detailed analysis of the recognized surfaces to locate openings	mechanism
such as windows and doorways	mechanism
This process uses visibility reasoning to fuse measurements from different scan locations and to identify occluded regions and holes in the surface	mechanism
Next we use a learning algorithm to intelligently estimate the shape of window and doorway openings even when partially occluded Finally	mechanism
occluded surface regions are filled in using a 3D inpainting algorithm	mechanism
We evaluated the method on a large	method
highly cluttered data set of a building with forty separate rooms	method
results confirm that TransPart offers low overhead and startup cost	finding
while improving user experience	finding
This article investigates the transient use of free local storage We use the term TransientPC systems to refer to these types of systems	mechanism
The solution we propose	mechanism
called TransPart uses the higher-performing local storage of host hardware Our solution constructs a virtual storage device on demand ( which we call transient storage ) by borrowing free disk blocks from the hosts storage	mechanism
In this article	mechanism
we present the design	mechanism
implementation	mechanism
and evaluation of a TransPart prototype	method
which requires no modifications to the software or hardware of a host computer Experimental	method
Real-time system level implementations of complex Synthetic Aperture Radar ( SAR ) image reconstruction algorithms have always been challenging due to their data intensive characteristics	background
results indicate that this proposed algorithm/hardware co-optimized system can achieve an accuracy of 91 dB PSNR compared to a reference algorithm implemented in Matlab and energy efficiency of 72 GFLOPS/W for a 8k8k SAR image reconstruction	finding
In this paper	mechanism
we propose a basis vector transform based novel algorithm and a 3D-stacked logic in memory based hardware accelerator as the implementation platform	mechanism
Experimental	method
The findings of this paper provide strategies for harnessing the crowd to perform complex tasks	background
as well as insight into crowd workers ' motivation	background
we show that 1 ) using these strategies together increased workers ' engagement and the quality of their work ; 2 ) a social strategy was most effective for increasing engagement ; 3 ) a learning strategy was most effective in improving quality	finding
We explore the effects of social	method
learning and financial strategies	method
and their combinations	method
on increasing worker retention across tasks and change in the quality of worker output	method
Through three experiments	method
due to the recent penetration of distributed green energy	background
distributed intelligence and plug-in electric vehicles Finally	background
the proposed method can be implemented given recent advances in machine learning	background
which are becoming drivers and sources of data previously unavailable in the electric power industry	background
results of the proposed method show that the new method produces a topology estimate excelling the current industrial approach	finding
Instead of taking the traditional complex physical model based approach	mechanism
this paper proposes a data-driven method	mechanism
leading to an effective Specifically	mechanism
we first introduce the data-driven topology estimation problem	mechanism
Then a novel Logistic Kernel Regression is proposed in a Bayesian framework based on Nearest Neighbors search	mechanism
Notably unlike many machine learning approaches that do not account for physical constraints	mechanism
and distinctive from deterministic engineering modeling defined solely by physical laws	mechanism
this paper for the first time combines the two into one single regression modeling for topology estimation	mechanism
Simulation	method
We close by identifying several ways in which crowd labor platform operators and/or individual task requestors could improve the accessibility of this increasingly important form of employment	background
Our findings establish that people with a variety of disabilities currently participate in the crowd labor marketplace	finding
despite challenges such as crowdsourcing workflow designs that inadvertently prohibit participation by	finding
and may negatively affect the worker reputations of	finding
people with disabilities Despite such challenges	finding
we find that crowdwork potentially offers different opportunities for people with disabilities relative to the normative office environment	finding
such as job flexibility and lack of a need to rely on public transit	finding
via in-depth open-ended interviews of 17 people ( disabled crowdworkers and job coaches for people with disabilities ) and a survey of 631 adults with disabilities	method
Effortless one-touch capture of video is a unique capability of wearable devices such as Google Glass	background
We use this capability in which users receive queries relevant to their current location and opt-in preferences	mechanism
In response they can send back live video snippets of their surroundings	mechanism
A system of result caching	mechanism
geolocation and query similarity detection shields users from being overwhelmed by a flood of queries	mechanism
In this paper we propose a common file format and API for public Non-Intrusive Load Monitoring ( NILM ) datasets such that The proposed file format enables storing the power demand of the whole house along with individual appliance consumption	mechanism
and other relevant metadata in a single compact file	mechanism
whereas the API supports the creation and manipulation of individual files and datasets in the proposed format	mechanism
It is known that in this setting both revenue and social welfare can be maximized by a threshold policy	background
whereby customers are barred from entry once the queue length reaches a certain threshold	background
allowing for settings with multiple servers	background
Finally we present a generalization of our results	finding
This paper presents the first derivation of the optimal threshold in closed form	mechanism
and a surprisingly simple formula Utilizing properties of the Lambert W function	mechanism
we also provide explicit scaling results of the optimal threshold as the customer valuation grows	mechanism
The virtualization of real-time systems has received much attention for its many benefits	background
such as the consolidation of individually developed real-time applications while maintaining their implementations	background
results indicate that	finding
under vMPCP deferrable server outperforms periodic server when overrun is used	finding
with as much as 80 % more task sets being schedulable	finding
shows that vMPCP yields significant benefits compared to a virtualization-unaware multi-core synchronization protocol	finding
with 29 % shorter response time on average	finding
In this paper	mechanism
we propose vMPCP	mechanism
a synchronization framework Vmpcp exposes the executions of critical sections of tasks in a guest virtual machine to the hyper visor	mechanism
Using this approach	mechanism
vMPCP reduces and bounds blocking time on accessing resources shared within and across virtual CPUs ( VCPUs ) assigned on different physical CPU cores Vmpcp supports periodic server and deferrable server policies for the VCPU budget replenish policy	mechanism
with an optional budget overrun to reduce blocking times	mechanism
We provide the VCPU and task schedulability analyses under vMPCP	mechanism
with different VCPU budget supply policies	mechanism
with and without overrun	mechanism
Experimental The case study using our hyper visor implementation	method
Real-time captioning enables deaf and hard of hearing ( DHH ) people to follow classroom lectures and other aural speech by converting it into visual text with less than a five second delay	background
These results show the potential to reliably capture speech even during sudden bursts of speed	background
as well as for generating enhanced captions	background
unlike other human-powered captioning approaches	background
We show that both hearing and DHH participants preferred and followed collaborative captions better than those generated by automatic speech recognition ( ASR ) or professionals due to the more consistent flow of the resulting captions	finding
We first surveyed the audio characteristics of 240 one-hour-long captioned lectures on YouTube	method
such as speed and duration of speaking bursts We then analyzed how these characteristics impact caption generation and readability	method
considering specifically our human-powered collaborative captioning approach	method
We note that most of these characteristics are also present in more general domains	method
For our caption comparison evaluation	method
we transcribed a classroom lecture in real-time using all three captioning approaches	method
We recruited 48 participants ( 24 DHH ) to watch these classroom transcripts in an eye-tracking laboratory	method
We presented these captions in a randomized	method
balanced	method
Event detection in social media is an important but challenging problem	background
and present empirical evaluations illustrating the effectiveness and efficiency of our proposed approach	finding
This paper presents Non-Parametric Heterogeneous Graph Scan ( NPHGS )	mechanism
a new approach that considers the entire heterogeneous network : we first model the network as a `` sensor '' network	mechanism
in which each node senses its `` neighborhood environment '' and reports an empirical p-value measuring its current level of anomalousness for each time interval ( e	mechanism
g	mechanism
hour or day ) Then	mechanism
we efficiently maximize a nonparametric scan statistic over connected subgraphs to identify the most anomalous network clusters	mechanism
Finally the event represented by each cluster is summarized with information such as type of event	mechanism
geographical locations time	mechanism
and participants	mechanism
As a case study	method
we consider two applications using Twitter data	method
civil unrest event detection and rare disease outbreak detection	method
Clustering is one of the fundamental data mining tasks	background
we show the strengths of our novel clustering technique	finding
In this work	mechanism
we present a Bayesian framework We exploit the ideas of subspace clustering where the relevance of dimensions might be different for each cluster	mechanism
Combining the relevance of the dimensions with the cluster membership degree of the objects	mechanism
we propose a novel type of mixture model able to represent data containing mixed membership subspace clusters	mechanism
we develop an efficient algorithm based on variational inference allowing easy parallelization	mechanism
In our empirical study on synthetic and real data	method
Monte Carlo simulation ( MCS ) is a numerical method to solve the probabilistic load flow ( PLF ) problem	background
Comparing to analytical methods	background
MCS for PLF has advantages such as flexibility	background
general purpose able to deal with large nonlinearity and large variances	background
and embarrassingly parallelizable	background
In this paper	finding
we showed that the PLF for radial distribution system has the similar properties and can be a good candidate for QMC method	finding
have shown the effectiveness of the proposed method	finding
In this paper	mechanism
we proposed a Quasi-Monte Carlo ( QMC ) based method QMC uses samples from low-discrepancy sequence intended to cover the high dimension random sample space as uniformly as possible The QMC method is particularly suitable for the high dimension problems with low effective dimensions	mechanism
and has been successfully used to solve large scale problems in econometrics and statistical circuit design	mechanism
The proposed method possesses the advantage of MCS method and significantly increases the convergence rate and overall speed	mechanism
Numerical experiment results on IEEE test feeders	method
People spend an enormous amount of time searching for and saving information online	background
Existing tools capture only a small portion of the cognitive processing a user engages in while making sense of a new domain and practical implications for the development of tools to capture and share online information	background
Our results contribute empirical knowledge relevant to theories of information seeking and sensemaking	finding
In this paper we introduce a novel interface We use this interface as a platform	mechanism
to experimentally characterize the costs and benefits of structuring information during the sensemaking process	method
ABSTRACT Multiple ultrasonic guided -wave modes propagating along a pipe travel with different velocities which are themselves a function of frequency	background
Reflections from the features of the structure ( e	background
g	background
boundaries pipe welding	background
damage etc	background
) and their complex s uperposition	background
adds to the complexity of guided -waves	background
Guided -wave based damage d iagnosis of pipelines becomes even more challenging when environmental and operational conditions ( EOCs ) vary ( e	background
g	background
temperature flow rate	background
inner pressure etc	background
)	background
These complexities make guided -wave based damage diagnosis of operating pipelines a challenging task	background
In this paper	finding
the general concept of this method is proved The potential of the proposed method for real -time damage detection is illustrated	finding
for wide range of temperature variation scenarios ( i	finding
e	finding
temperature difference between training and test data varying between -2 ( and 13 ( )	finding
A method is proposed while retaining optimal damage information for detection purpose	mechanism
t hrough an extensive set of experiments	method
Effects of temperature variation on detection performnce of the proposed method	method
and on discriminatory power of the extracted damage -sensitive features are investigated	method
Preliminary performance and scalability study is also reported	finding
This paper reports the design and development of an HTML5-empowered Virtual Sensor Editor ( VSE ) over Internet of Things cloud VSE is a scalable tool by visually aggregating existing sensors	mechanism
either physical sensors or user-defined virtual sensors	mechanism
VSE supports a real-time and historical visualization of sensor values and analytical studies	mechanism
and is a cross-platform and customizable tool equipped with ability to support verifiable sensor data service compos ability A discussion on design decisions is presented	mechanism
Our preliminary work has been applied to NASA Sustainability Base for Smart Building monitoring	method
results are provided to demonstrate the advantages and properties of the proposed sensor selection approach	finding
Kernel-based minimization framework with a weighted kernel is adopted	mechanism
where the kernel weight parameters represent sensors ' contributions to decision making	mechanism
L 1 regularization on weight parameters is introduced into the risk function so that the resulting optimal decision rule contains a sparse vector of nonzero weight parameters	mechanism
In this way	mechanism
sensor selection is naturally performed because only sensors corresponding to nonzero weight parameters contribute to decision making	mechanism
A gradient projection algorithm and a Gauss-Seidel algorithm are developed to jointly perform weight selection ( i	mechanism
e	mechanism
sensor selection ) and optimize decision rules	mechanism
Both algorithms are shown to converge to critical points for this non-convex optimization problem	mechanism
Numerical	method
ABSTRACT Guided waves can propagate long distances and are sensitive to subtle structural damage Guided-wave based damage localization often requires extracting the scatter signal ( s ) produced by damage	background
which is typically obtained by subtracting an intact baseline record from a record to be tested However	background
in practical applications	background
environmental and operational conditions ( EOC ) dramatically affect guided wave si gnals	background
In this case	background
the baseline subtraction process can no longer perfectly remove the baseline	background
thereby defeating localization algorithms	background
In previous work	background
we showed that singular value decomposition ( SVD ) can be used to detect the presence of damage under large EOC variations	background
because it can differentiate the tr ends of damage from other EO C variations	background
We show that our SVD-based approach successfully localize damage while current temperature-compensated baseline subtraction methods fail	finding
In this work	mechanism
we use to approach We collect pitch-catch records from randomly placed PZT transducers on an aluminum plate while undergoing temperature variations	mechanism
Damage is introduced to the plate during the monitoring period	mechanism
We then use our SVD method to extract the scatter signal from the records	mechanism
and use the scatter signal to localize damage using the delay-and-sum method	mechanism
To compare results	method
we also apply several temperature compensation methods to the records and then perform baseline subtraction	method
We use exponential start time clustering Previous algorithms usually rely on graph decomposition routines with strict restrictions on the diameters of the decomposed pieces	mechanism
We weaken these bounds in favor of stronger local probabilistic guarantees	mechanism
This allows more direct analyses of the overall process	mechanism
giving : Linear work parallel algorithms that construct spanners with O ( k ) stretch and size O ( n 1+1/ k ) in unweighted graphs	mechanism
and size O ( n 1+1/ k log k ) in weighted graphs Hopsets that lead to the first parallel algorithm for approximating shortest paths in undirected graphs with O ( m poly log n ) work	mechanism
Testing Cyber-Physical Systems is becoming increasingly challenging as they incorporate advanced autonomy features	background
Beyond demonstrating feasibility	finding
the experience emphasized a number of remaining research challenges	finding
including : approximating system intent based on limited system state observability	finding
how to best balance the simplicity and expressiveness of the specification language used to define monitored properties	finding
how to warm up monitoring of system variable state after mode change discontinuities	finding
and managing the differences between simulation and real vehicles when conducting such tests	finding
We investigate using an external runtime monitor as a partial test oracle Despite limited source code access and using only existing network messages	mechanism
we were able to monitor a hardware-in-the-loop vehicle simulator and analyze prototype vehicle log data to detect violations of high-level critical properties Interface robustness testing was useful to further exercise the monitors	mechanism
Disparity tuning measured in the primary visual cortex ( V1 ) is described well by the disparity energy model	background
but not all aspects of disparity tuning are fully explained by the model	background
Our model predicted sharper disparity tuning for larger stimuli	finding
In this case	finding
our model predicted reduced sharpening and strength of inverted disparity tuning	finding
the dynamics of disparity tuning observed from the neurophysiological recordings in macaque V1 matched model simulation predictions	finding
Overall the results of this study support the notion that	finding
while the disparity energy model provides a primary account of disparity tuning in V1 neurons	finding
neural disparity processing in V1 neurons is refined by recurrent interactions among elements in the neural circuit	finding
Here we propose a neuronal circuit model with recurrent connections The model is based on recurrent connections inferred from neurophysiological observations on spike timing correlations	mechanism
and is in good accord with existing data on disparity tuning dynamics	mechanism
We further performed two additional experiments to test predictions of the model	method
First we increased the size of stimuli to drive more neurons and provide a stronger recurrent input Second	method
we displayed anti-correlated stereograms	method
where dots of opposite luminance polarity are matched between the left- and right-eye images and result in inverted disparity tuning in the disparity energy model	method
For both experiments	method
Behavioral researchers spend considerable amount of time coding video data to systematically extract meaning from subtle human actions and emotions	background
- opening up new possibilities for naturally exploring video data	background
Our show that Glance can code nearly 50 minutes of video in 5 minutes by recruiting over 60 workers simultaneously	finding
and can get initial feedback to analysts in under 10 seconds for most clips	finding
Glance 's rapid responses to natural language queries	finding
feedback regarding question ambiguity and anomalies in the data	finding
and ability to build on prior context in followup queries allow users to have a conversation-like interaction with their data	finding
In this paper	mechanism
we present Glance	mechanism
a tool Glance takes advantage of the parallelism available in paid online crowds to interpret natural language queries and then aggregates responses in a summary view of the video data	mechanism
Glance provides analysts with rapid responses when initially exploring a dataset	mechanism
and reliable codings when refining an analysis	mechanism
We present and compare new methods for accurately aggregating the input of multiple workers marking the spans of events in video data	mechanism
and for measuring the quality of their coding in real-time before a baseline is established by measuring the variance between workers	mechanism
experiments	method
Inference of gene interaction networks from expression data usually focuses on either supervised or unsupervised edge prediction from a single data source	background
Results are reported	finding
where NP-MuScL outperforms baseline algorithms significantly	finding
even in the presence of noisy data sources where NP-MuScL predicts a higher number of known gene interactions than existing techniques	finding
We propose ISH	mechanism
which are expected to reflect the same underlying relationships between the genes	mechanism
NP-MuScL casts the network estimation problem as estimating the structure of a sparse undirected graphical model We use the semiparametric Gaussian copula to model the distribution of the different data sources	mechanism
with the different copulas sharing the same precision ( i	mechanism
e	mechanism
inverse covariance ) matrix	mechanism
and we present an efficient algorithm to estimate such a model in the high-dimensional scenario	mechanism
on synthetic data Experiments are also run on two real-world scenarios : two yeast microarray datasets and three Drosophila embryonic gene expression datasets	method
Intracortical braincomputer interface ( BCI ) decoders are typically retrained daily to maintain stable performance Significance	background
We believe that the development of classifiers that require no daily retraining will accelerate the clinical translation of BCI systems	background
Future work should test these results in a closed-loop setting	background
Main results	finding
Further drift is constrained across time	finding
which is reflected in the performance of a standard classifier which does not progressively worsen if it is not retrained daily	finding
though overall performance is reduced by more than 10 % compared to a daily retrained classifier	finding
produce a increase in classification accuracy over that achieved by the non-retrained classifier to nearly recover the performance of the daily retrained classifier	finding
We show that for the purposes of developing a self-recalibrating classifier	mechanism
tuning parameters can be considered as fixed within days and that parameters on the same electrode move up and down together between days	mechanism
Two novel self-recalibrating classifiers	mechanism
Approach	method
We recorded threshold crossings from 96-electrode arrays implanted in the motor cortex of two rhesus macaques performing center-out reaches in 7 directions over 41 and 36 separate days spanning 48 and 58days in total for offline analysis	method
Low-cost genetic sequencing	background
coupled with novel social media platforms and visualization techniques	background
present a new frontier for scientific participation	background
whereby people can learn	background
share and act on data embedded within their own bodies	background
Our findings show that personal genetics serves as a site for public engagement with science	finding
whereby communities of biological citizens creatively interpret	finding
debate and act on professional research	finding
We conclude with design trajectories at the intersection of genetics and creativity support tools : platforms for aggregating hybrid knowledge ; tools for creative reflection on professional science ; and strategies for supporting collaborations across communities	mechanism
Our study of 23andMe	method
a popular genetic testing service	method
We frame user groups as citizen science publics groups that coalesce around scientific issues and work towards resolving shared concerns	method
In the real-world unconstrained face recognition scenarios	background
automatic facial landmarking scheme using the active shape model ( ASM ) usually gives non-ideal results	background
especially at the facial boundary This is because the local subspace methods such as the principal component analysis ( PCA ) used in the ASM does not properly discern skin texture and background with very similar photometric and textual properties	background
thus fails to accurately locate the facial boundary	background
We have shown the effectiveness of our proposed methods where the refined landmarks yield much lower MSE from the ground truth	finding
In this work	mechanism
we have novelly developed a robust image statistics approach Moreover	mechanism
with the aid of banana wavelets to highlight the facial boundary	mechanism
our proposed approach can deal with even more difficult task This algorithm can dramatically increase the accuracy of landmarks on facial boundary for unconstrained facial images with minimum computational expense since this method is purely based on image statistics with no training stages involved at all	mechanism
on the GBU database	method
One of the most significant challenges for many online communities is increasing members ' contributions over time	background
Prior studies on peer feedback in online communities have suggested its impact on contribution	background
but have been limited by their correlational nature This research provides insights into the mechanisms underlying peer feedback in online communities and practical guidance to design more effective peer feedback systems	background
Our results characterize the effects of different feedback types	finding
and suggest trade-offs in the effects of feedback between the focal task and general motivation	finding
as well as differences in how newcomers and experienced editors respond to peer feedback	finding
In this paper	method
we conducted a field experiment on Wikipedia	method
In commercial-off-the-shelf ( COTS ) multi-core systems	background
the execution times of tasks become hard to predict because of contention on shared resources in the memory hierarchy	background
In particular a task running in one processor core can delay the execution of another task running in another processor core	background
This is due to the fact that tasks can access data in the same cache set shared among processor cores or in the same memory bank in the DRAM memory ( or both )	background
Such cache and bank interference effects have motivated the need to create isolation mechanisms for resources accessed by more than one One popular isolation mechanism is cache coloring that divides the cache into multiple partitions	background
With cache coloring	background
each task can be assigned exclusive cache partitions	background
thereby preventing cache interference from other tasks	background
Similarly bank coloring allows assigning exclusive bank partitions to tasks	background
While cache coloring and some bank coloring mechanisms have been studied separately	background
interactions between the two schemes have not been studied	background
Specifically while memory accesses to two different bank colors do not interfere with each other at the bank level	background
they may interact at the cache level	background
Similarly two different cache colors avoid cache interference but may not prevent bank interference	background
we observed that the execution time can increase by 60 % due to inter-task interference when we use only cache Our coordinated approach can reduce this figure down to 12 % ( an 80 % reduction )	finding
In this paper	mechanism
we present a coordinated cache and bank coloring scheme We also developed color allocation algorithms	mechanism
which has been implemented in the Linux kernel	method
In our experiments	method
Anecdotal evidence and scholarly research have shown that Internet users may regret some of their online disclosures	background
We discuss implications and challenges for designing and evaluating systems to assist users with online disclosures	background
We found that reminders about the audience of posts can prevent unintended disclosures without major burden ; however	finding
introducing a time delay before publishing users ' posts can be perceived as both beneficial and annoying On balance	finding
some participants found the nudges helpful while others found them unnecessary or overly intrusive	finding
we designed two modifications to the Facebook web interface	mechanism
We implemented and evaluated these two nudges in a 6-week field trial with 28 Facebook users We analyzed participants ' interactions with the nudges	method
the content of their posts	method
and opinions collected through surveys	method
Virus capsid assembly has been a powerful model system for biological self-assembly in general due to the combination of experimental tractability but complicated pathway space	background
Detailed experimental resolution of viral assembly processes	background
however has so far proven impossible	background
Computational approaches have provided a solution	background
allowing us to learn models of assembly consistent with indirect experimental measures of bulk in vitro assembly and thus fill the gaps between coarse-grained experimental measurements and detailed theoretical models	background
We previously developed a heuristic optimization approach to learn rate parameters of coat-coat interactions by minimizing the deviation between real and simulated static light scattering measurements	background
Advancing such simulation-based data fitting methods provides a general technology for greatly enhancing our ability to learn fine-scale details of complex assembly processes from experimental data	background
a strategy with potential application to developing accurate quantitative models of numerous other assembly systems found through biology	background
suggest that other feasible technologies providing richer data on assembly progress can more precisely pin down true parameters and assembly pathways	finding
Here we describe progress in learning accurate kinetic models of capsid assembly systems by computationally fitting assembly simulations to experimental data	mechanism
using an alternative class of methods called derivative-free optimization	mechanism
designed	mechanism
Simultaneously simulated exploration of potential alternative sources of experimental data for monitoring bulk assembly ( e	method
g non-covalent mass spectrometry )	method
Automatic understanding of human activities is a huge challenge in multimedia analysis field	background
This challenge is especially critical in small-scale activities	background
such as finger motions	background
and activities in complex scenes	background
For typical camera views	background
both global feature and local feature analysis methods are unsuitable	background
To solve this problem	background
many studies focus on using spatio-temporal features and feature selection methods to get video representation	background
validate our activity analysis approach is more effective than state-of-the-art methods	finding
In this paper	mechanism
we propose a graph based Co-Attention model Without reducing the dimensionality	mechanism
our Co-Attention model considers the number of interest points	mechanism
Our model is derived from correlations among individual tiny activities	mechanism
whose salient regions are identified by combining an integrated top-down and bottom-up visual attention model	mechanism
and a motion attention model built by spatio-temporal features instead of optical flow directly	mechanism
Different from typical attention models	mechanism
the Co-Attention model allows multiple regions of interest in video co-existing for further analysis	mechanism
Experimental results on the KTH dataset	method
YouTube dataset and a new tiny activity dataset	method
Pump dataset which consist of visual observation data from patients operating an infusion pump	method
Machine-learning ( ML ) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices	background
making medical diagnoses	background
and facial recognition	background
In a model inversion attack	background
recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al	background
adversarial access to an ML model is abused to learn sensitive genomic information about individuals	background
show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and	finding
in the other context	finding
show how to recover recognizable images of people 's faces given only their name and access to the ML model The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility	finding
We develop a new class of model inversion attack that exploits confidence values revealed along with predictions	mechanism
Our new attacks are applicable in a variety of settings	mechanism
and we explore two in depth : decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition	mechanism
In both cases confidence values are revealed to those with the ability to make prediction queries to models	mechanism
We experimentally We also initiate experimental exploration of natural countermeasures	method
investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning	method
as well as revealing only rounded confidence values	method
In our previous work	background
we derived the closed form description of the equilibrium distribution that explicitly accounts for the network topology and showe dt hat themost probable equilibrium statedemonstrates threshol db ehavior	background
We model a over as tatic	mechanism
fi nite-sized network as ac ontinuous-time Markov process using the scaled SIS epidemics model	mechanism
Multimedia Event Detection ( MED ) is a multimedia retrieval task with the goal of finding videos of a particular event in a large-scale Internet video archive	background
given example videos and text descriptions	background
based on their visual semantics using a Visual Concept Signature ( VCS ) generated for each event only derived from the event description provided as the query	mechanism
Visual semantics are described using the Semantic INdexing ( SIN ) feature which represents the likelihood of predefined visual concepts in a video	mechanism
To generate a VCS for an event	mechanism
we project the given event description to a visual concept list using the proposed textual semantic similarity	mechanism
Exploring SIN feature properties	mechanism
we harmonize the generated visual concept signature and the SIN feature to improve retrieval performance	mechanism
We conduct different experiments to assess the quality of generated visual concept signatures with respect to human expectation	method
and in the context of the MED task to retrieve the SIN feature of videos in the test dataset when we have no or only very few training videos	method
Given a large graph	background
like who-calls-whom or who-likes-whom	background
what behavior is normal and what should be surprising	background
possibly due to fraudulent activity ? How do graphs evolve over time ? How does influence/news/viruses propagate	background
over time ? We conclude with some open research questions for graph mining	background
as well as some discoveries such settings	finding
For the third	finding
we show that for virus propagation	finding
a single number is enough to characterize the connectivity of graph	finding
and	finding
For the first	mechanism
we present a list of static and temporal laws	mechanism
including advances patterns like 'eigenspokes ' ; we show how to use them to spot suspicious activities	mechanism
in on-line buyer-and-seller settings	mechanism
in FaceBook in twitter-like networks	mechanism
For the second	mechanism
we show how to handle time-evolving graphs as tensors	mechanism
how to handle large tensors in map-reduce environments	mechanism
thus we show how to do efficient immunization for almost any type of virus ( SIS - no immunity ; SIR - lifetime immunity ; etc )	mechanism
The space around the body provides a large interaction volume that can allow for big interactions on small mobile devices	background
We demonstrate three types of around-body interaction including canvas	mechanism
modal and context-aware interactions in six demonstration applications We also present using standard smartphone hardware : a phone 's front camera	mechanism
accelerometer and inertia measurement units	mechanism
Our solution allows a person to interact with a mobile device by holding and positioning it between a normal field of view and its vicinity around the body By leveraging a user 's proprioceptive sense	mechanism
around-body Interaction opens a new input channel that enhances conventional interaction on a mobile device without requiring additional hardware	mechanism
Recent improvements in content-based video search have led to systems with promising accuracy	background
shows that our system is capable of assisting users to incrementally build effective queries over complex event topics	finding
We present an interactive system based on a state-of-the-art content-based video search pipeline which enables users through relevance feedback and model visualization	mechanism
Also the comprehensive functionalities enhance a flexible formulation of multimodal queries with different characteristics	mechanism
Quantitative and qualitative analysis	method
We show that offload shaping can produce significant reduction in resource demand	finding
with little loss of application-level fidelity	finding
When offloading computation from a mobile device	mechanism
we show that it can pay to perform additional on-device work We call this offload shaping	mechanism
and	mechanism
demonstrate its application at many different levels of abstraction using a variety of techniques	method
In recent years	background
progresses in data mining and business analytics have fostered the advent of recommender systems	background
behavioral advertising and other ways of using consumer data to personalize offers and products	background
Copyright Springer Science+Business Media New York 2014	background
We find that there are two types of equilibria : sellers will target all potential buyers	finding
hence their targeted ads or purchase recommendations provide no benefit to the consumer	finding
ads and recommendations will be accurate will be inversely related	finding
For some parameter values	method
But for other values In particular	method
the incentive for the seller to provide accurate ads and recommendations to the difference between the cost of producing the good and its average market evaluation	method
These interrelated issues underpin advanced correctness analysis in models of structured communications	background
we prove that all proof conversions induced by the logic interpretation actually express observational equivalences	finding
and explain how type isomorphisms resulting from linear logic equivalences are realized by coercions between interface types of session-based concurrent systems	finding
by developing a theory of logical relations	mechanism
Defined upon a linear type structure	mechanism
our logical relations remain remarkably similar to those for functional languages	mechanism
We also introduce a natural notion of observational equivalence Strong normalization and confluence come in handy in the associated coinductive reasoning :	mechanism
The starting point for our study is an interpretation of linear logic propositions as session types for communicating processes	method
proposed in prior work	method
as applications	method
Given a large social graph	background
what can we say about its robustness ? Broadly speaking	background
the property of robustness is crucial in real graphs	background
since it is related to the structural behavior of graphs to retain their connectivity properties after losing a portion of their edges/nodes	background
Can we estimate a robustness index for a graph quickly ? Additionally	background
if the graph evolves over time	background
how this property changes ?	background
and we observe interesting properties for both static and time-evolving social graphs	finding
we show how to spot outliers and anomalies in graphs over time	finding
Finally we examine how graph generating models that mimic several properties of real-world graphs and behave in terms of robustness dynamics	finding
First we present a measure that characterizes the robustness properties of a graph and also serves as global measure of the community structure ( or lack thereof )	mechanism
We show how to compute this measure efficiently by exploiting the special spectral properties of real-world networks	mechanism
We apply our method on several diverse real networks with millions of nodes	method
As an application example	method
User provided rating data about products and services is one key feature of websites such as Amazon	background
TripAdvisor or Yelp	background
Since these ratings are rather static but might change over time	background
a temporal analysis of rating distributions provides deeper insights into the evolution of a products ' quality	background
and we present interesting findings	finding
we model the base behavior of users regarding a product as a latent multivariate autoregressive process	mechanism
This latent behavior is mixed with a sparse anomaly signal finally leading to the observed data We propose an efficient algorithm	mechanism
on various real world datasets	method
Self-reported behavioral data is frequently relied upon to understand the population of social network users	background
These data often consist of self-reported posting	background
commenting or general engagement frequency within the social network over the last few days or a month	background
we show that these data can be quite inaccurate Indeed	finding
the regularity of the behavior is a strong predictor of self-report accuracy	finding
are the most accurate in their reporting our study suggests that questions should only refer to a very narrow and recent time window to improve response accuracy	finding
Our study also highlights the importance of considering the granularity of privacy concern measurements when investigating the so-called privacy paradox	finding
do take more privacy actions In particular	finding
this group is less likely to enter profile information	finding
more likely to limit the visibility of their posts and more likely to delete posts	finding
Using a sample of 397 Google+ users Users who exhibit a behavior either very frequently or very infrequently For social networks	method
in which it is often the case that most users are `` lurkers '' who do not post or comment much	method
Within our sample	method
those users who report that the ability to control post visibility and/or delete posts are more important than other	method
non-privacy related features	method
demonstrate that HYDRA correctly identifies real user linkage across different platforms	finding
and outperforms existing state-of-the-art algorithms by at least 20 % under different settings	finding
and 4 times better in most settings	finding
This paper proposes HYDRA	mechanism
a solution framework which consists of three key steps : ( I ) modeling heterogeneous behavior by long-term behavior distribution analysis and multi-resolution temporal information matching ; ( II ) constructing structural consistency graph to measure the high-order structure consistency on users ' core social structures across different platforms ; and ( III ) learning the mapping function by multi-objective optimization composed of both the supervised learning on pair-wise ID linkage information and the cross-platform structure consistency maximization	mechanism
Extensive experiments on 10 million users across seven popular social network platforms	method
have demonstrated the effectiveness of the proposed algorithm	finding
In this paper	mechanism
we propose a new multi-task feature selection algorithm and apply it to multimedia ( e	mechanism
g	mechanism
video and image ) analysis	mechanism
Instead of evaluating the importance of each feature individually	mechanism
our algorithm selects features in a batch mode	mechanism
by which the feature correlation is considered	mechanism
While feature selection has received much research attention	mechanism
less effort has been made on improving the performance of feature selection by leveraging the shared knowledge from multiple related tasks	mechanism
Our algorithm builds upon the assumption that different related tasks have common structures	mechanism
Multiple feature selection functions of different tasks are simultaneously learned in a joint framework	mechanism
which enables our algorithm to utilize the common knowledge of multiple tasks as supplementary information to facilitate decision making	mechanism
An efficient iterative algorithm is proposed to optimize it	mechanism
whose convergence is guaranteed	mechanism
Experiments on different databases	method
We present an architecture for the Security Behavior Observatory ( SBO )	mechanism
a client-server infrastructure from hundreds of participants over several years	mechanism
The SBO infrastructure had to be carefully designed to fulfill several requirements	mechanism
First the SBO must scale with the desired length	mechanism
breadth and depth of data collection Second	mechanism
we must take extraordinary care to ensure the security of the collected data	mechanism
which will inevitably include intimate participant behavioral data	mechanism
Third the SBO must serve our research interests	mechanism
which will inevitably change as collected data is analyzed and interpreted This short paper summarizes some of our design and implementation benefits and discusses a few hurdles and trade-offs to consider when designing such a data collection system	mechanism
Very recently there has been a perfect storm of technical advances that has culminated in the emergence of a new interaction modality : on-body interfaces	background
Such systems enable the wearer to use their body as an input and output platform with interactive graphics	background
Projects such as PALMbit and Skinput sought to answer the initial and fundamental question : whether or not on-body interfaces were technologically possible	background
point the way towards more comfortable	background
efficacious and enjoyable on-body user experiences	background
The results of this complimentary	finding
structured exploration	finding
we employed a mixed-methods research process involving more than two thousand individuals	method
This started with high-resolution	method
but low-detail crowdsourced data	method
We then combined this with rich	method
expert interviews exploring aspects ranging from aesthetics to kinesthetics	method
QuMinS scales linearly on the data size	finding
being up to 40 times faster than top competitors ( GCap )	finding
still achieving better or equal accuracy	finding
it spots images that potentially require unpredicted labels	finding
and it works even with tiny initial label sets	finding
i	finding
e	finding
nearly five examples to show that QuMinS is a viable tool for automatic coffee crop detection from remote sensing images	finding
Specifically we propose QuMinS	mechanism
a fast scalable - given an image set	mechanism
very few images have labels in the same setting	mechanism
find clusters the top-N '' O outlier images	mechanism
and the N '' R images	mechanism
Experiments on satellite images spanning up to 2	method
25 GB show that	method
contrasting to the state-of-the-art labeling techniques	method
We also report a case study of our method 's practical usage	method
by extending todays unmodified cloud to a second level consisting of self-managed data centers with no hard state called cloudlets These are located at the edge of the Internet	mechanism
just one wireless hop away from associated mobile devices By leveraging lowlatency offload	mechanism
cloudlets enable a new class of real-time cognitive assistive applications on wearable devices By processing high data rate sensor inputs such as video close to the point of capture	mechanism
cloudlets can reduce ingress bandwidth demand into the cloud By serving as proxies for distant cloud services that are unavailable due to failures or cyberattacks	mechanism
cloudlets can improve robustness and availability	mechanism
We caution that proprietary software ecosytems surrounding cloudlets will lead to a fragmented marketplace that fails to realize the full business potential of mobile-cloud convergence	mechanism
Instead we urge that the software ecosystem surrounding cloudlets be based on the same principles of openness and end-to-end design that have made the Internet so successful	mechanism
Introductory programming activities for students often include graphical user interfaces or other visual media that are inaccessible to students with visual impairments	background
and suggests future directions for integrating data analysis and 3D printing into programming instruction for blind students	background
This paper describes outcomes from our workshop	finding
This paper describes the planning and execution of a four-day computer science education workshop in which blind and visually impaired students wrote Ruby programs to analyze data from Twitter regarding a fictional ecological crisis	mechanism
Students then wrote code to produce accessible tactile visualizations of that data	mechanism
Botnets are large networks of bots ( compromised machines ) that are under the control of a small number of bot masters	background
They pose a significant threat to Internets communications and applications	background
A botnet relies on command and control ( C2 ) communications channels traffic between its members for its attack execution	background
C2 traffic occurs prior to any attack ; hence	background
the detection of botnets C2 traffic enables the detection of members of the botnet before any real harm happens	background
This is due to the pre-programmed behavior of bots that check for updates to download them every T seconds	background
and find that it exhibits a periodic behavior	finding
We exploit this periodic behavior The detection involves evaluating the periodogram of the monitored traffic	mechanism
Then applying Walkers large sample test to the periodograms maximum ordinate in order to determine if it is due to a periodic component or not If the periodogram of the monitored traffic contains a periodic component	mechanism
then it is highly likely that it is due to a bots C2 traffic	mechanism
The test looks only at aggregate control plane traffic behavior	mechanism
which makes it more scalable than techniques that involve deep packet inspection ( DPI ) or tracking the communication flows of different hosts	mechanism
We apply the test to two types of botnet	method
tinyP2P and IRC that are generated by SLINGbot	method
We verify the periodic behavior of their C2 traffic and compare it to the results we get on real traffic that is obtained from a secured enterprise network	method
We further study the characteristics of the test in the presence of injected HTTP background traffic and the effect of the duty cycle on the periodic behavior	method
Memory reservations are used to provide real-time tasks with guaranteed memory access to a specified amount of physical memory However	background
previous work on memory reservation primarily focused on private pages	background
and did not pay attention to shared pages	background
which are widely used in current operating systems	background
With previous schemes	background
a real-time task may experience unexpected timing delays from other tasks through shared pages that are shared by another process	background
even though the task has enough free pages in its own reservation	background
We then propose which enhances the temporal isolation provided by memory reservations in resource kernels that use the resource reservation approach	mechanism
Our proposed solution consists of two schemes	mechanism
Shared-Page Conservation ( SPC ) and Shared-Page Eviction Lock ( SPEL )	mechanism
each of which prevents timing penalties caused by the seemingly arbitrary eviction of shared pages	mechanism
The framework can manage shared data for inter-process communication and shared libraries	mechanism
as well as pages shared by the kernel 's copy-on-write technique and file caches	mechanism
We have implemented and evaluated our schemes on the Linux/RK platform	method
but it can also be applied to other operating systems with paged virtual memory	method
The promise of affordable	background
automatic approaches to real-time captioning imagines a future in which deaf and hard of hearing ( DHH ) users have immediate access to speech in the world around them my simply picking up their phone or other mobile device	background
This is in contrast to current human-powered approaches	background
which use highly-trained professional captionists who can type up to 250 words per minute ( WPM )	background
but also can cost over $ 100/hr	background
In this paper	mechanism
we describe a real-time demo of Legion : Scribe ( or just `` Scribe '' )	mechanism
a crowd-powered captioning system that allows untrained participants and volunteers by computationally merging their input into a single collective answer that is more accurate and more complete than any one worker could have generated alone	mechanism
Distributed augmented Lagrangian ( AL ) methods have good empirical performance on several signal processing and learning applications	background
illustrate our analytical findings	finding
This paper establishes globally linear ( geometric ) convergence rates of a class of deterministic and randomized distributed AL methods	mechanism
when the $ f_ { i } $ 's are twice continuously differentiable and have a bounded Hessian We give explicit dependence of the convergence rates on the underlying network parameters	mechanism
Simulations	method
Dispersion curves characterize many propagation mediums	background
When known many methods use these curves to analyze waves	background
In the results	finding
orthogonal matching pursuit provides two to three orders of magnitude improvement in speed and a small average reduction in prediction capability	finding
The analysis is demonstrated	finding
This paper presents a fast implementation of sparse wavenumber analysis	mechanism
a method	mechanism
This approach based on orthogonal matching pursuit	method
is compared with a prior implementation	method
based on basis pursuit denoising across multiple scenarios and parameters	method
Experimental results show that our method outperforms several state-of-the-art algorithms	finding
Most notably much better performances have been achieved when there are only a few labeled training samples	finding
This paper presents a semi-supervised method using multiple visual features	mechanism
The proposed algorithm simultaneously learns multiple features from a small number of labeled videos	mechanism
and automatically utilizes data distributions between labeled and unlabeled data to boost the recognition performance	mechanism
Shared structural analysis is applied in our approach to discover a common subspace shared by each type of feature	mechanism
In the subspace	mechanism
the proposed algorithm is able to characterize more discriminative information of each feature type	mechanism
Additionally data distribution information of each type of feature has been preserved The aforementioned attributes make our algorithm robust for action recognition	mechanism
especially when only limited labeled training samples are provided	mechanism
Extensive experiments have been conducted on both the choreographed and the realistic video datasets	method
including KTH Youtube action and UCF50	method
Such performance guarantee makes the greedy algorithm very attractive in the practical scenario of multi-stage installations for utilities with limited budgets	background
and show that it achieves an approximation ratio of ( 1-1/ e ) for any PMU placement budget We further show that the performance is the best that one can achieve	finding
in the sense that it is NP-hard to achieve any approximation ratio beyond ( 1-1/ e ) results demonstrate near-optimal performance of the proposed PMU placement algorithm	finding
This paper presents an information-theoretic approach Different from the conventional `topological observability ' based approaches	mechanism
this paper advocates a much more refined	mechanism
information-theoretic criterion namely the mutual information ( MI ) between PMU measurements and power system states	mechanism
The proposed MI criterion not only includes observability as a special case	mechanism
but also rigorously models the uncertainty reduction on power system states from PMU measurements	mechanism
Thus it can generate highly informative PMU configurations	mechanism
The MI criterion can also facilitate robust PMU placement by explicitly modeling probabilistic PMU outages We propose a greedy PMU placement algorithm	mechanism
Finally simulation	method
Robust facial hair detection and segmentation is a highly valued soft biometric attribute for carrying out forensic facial analysis	background
results demonstrate the effectiveness of our proposed system in detecting and segmenting facial hair regions	finding
In this paper	mechanism
we propose a novel and fully automatic system	mechanism
called SparCLeS SparCLeS uses the multiscale self-quotient ( MSQ ) algorithm to preprocess facial images and deal with illumination variation	mechanism
Histogram of oriented gradients ( HOG ) features are extracted from the preprocessed images and a dynamic sparse classifier is built using these features to classify a facial region as either containing skin or facial A level set based approach	mechanism
which makes use of the advantages of both global and local information	mechanism
is then used to segment the regions of a face containing	mechanism
Experimental in images drawn from three databases	method
i	method
e	method
the NIST Multiple Biometric Grand Challenge ( MBGC ) still face database	method
the NIST Color Facial Recognition Technology FERET database	method
and the Labeled Faces in the Wild ( LFW ) database	method
A descending ( multi-item ) clock auction ( DCA ) is a mechanism for buying items from multiple potential sellers	background
In the DCA	background
bidder-specific prices are decremented over the course of the auction	background
In each round	background
each bidder might accept or decline his offer price	background
Accepting means the bidder is willing to sell at that price	background
Rejecting means the bidder will not sell at that price or a lower price DCAs have been proposed as the method for procuring spectrum from existing holders in the FCC 's imminent incentive auctions so spectrum can be repurposed to higher-value uses	background
An unexpected paradox about DCAs is that sometimes when the number of rounds allowed increases	finding
the final payment increases	finding
We provide an explanation for this	finding
We develop a percentile-based approach which provides a means We also develop an optimization model for setting prices so as while stochastically satisfying the feasibility constraint	mechanism
( The DCA has a final adjustment round that obtains feasibility after feasibility has been lost in the final round of the main DCA	mechanism
) We prove attractive properties of this	mechanism
such as symmetry and monotonicity	mechanism
We develop computational methods for solving the model	mechanism
( We also develop optimization models with recourse	mechanism
but they are not computationally practical	mechanism
)	mechanism
We present experiments both on the homogeneous items case and the case of FCC incentive auctions	method
where we use real interference constraint data to get a fully faithful model of feasibility	method
Aggregations of thermostatically controlled loads ( TCLs ) have been shown to hold promise as demand response resources	background
Our results show that the effects of invalid assumptions about the disturbances and time-invariant properties of individual HRUs may be mitigated by a faster sampling of the state variables and that	finding
when this is not possible	finding
the proposed LTI system reduces the plant-model mismatch	finding
In this paper	mechanism
we first propose a data-driven modeling strategy Specifically	mechanism
we fit probability distributions to a year-long dataset of power measurements for HRUs and use these models to create more realistic simulations We then derive the aggregate system equations using a bottom-up approach that results in a more flexible [ linear time invariant ( LTI ) ] system	mechanism
Finally we quantify the plant-model mismatch and evaluate the proposed strategy with the more realistic simulation	method
We show an algorithm Our approach follows the recursive preconditioning framework	mechanism
which aims to reduce graphs to trees using iterative methods	mechanism
We improve two key components of this framework : random sampling and tree embeddings	mechanism
Both of these components are used in a variety of other algorithms	mechanism
and our approach also extends to the dual problem of computing electrical flows	mechanism
We show that preconditioners constructed by random sampling can perform well without meeting the standard requirements of iterative methods	mechanism
In the graph setting	mechanism
this leads to ultra-sparsifiers that have optimal behavior in expectation The improved running time makes previous low stretch embedding algorithms the running time bottleneck in this framework In our analysis	mechanism
we relax the requirement of these embeddings to snowflake spaces	mechanism
We then obtain a two-pass approach algorithm This algorithm is also readily parallelizable	mechanism
Social media offers a targeted way for mainstream technology companies to communicate with people with disabilities about the accessibility problems that they face	background
and suggests the extent to which a company is able to leverage this input depends greatly on how they choose to present themselves and interact on social media	background
We find that while many users want to interact directly with companies about accessibility	finding
companies prefer to redirect them to other channels and use Twitter for broadcast messages promoting their accessibility work instead Our analysis demonstrates that users want to use social media to become part of the process of improving accessibility of mainstream technology	finding
In this paper	method
we describe current use patterns of six corporate accessibility teams and their users on Twitter	method
and present an analysis of these interactions	method
We motivate the necessity of this framework in the context of self-driving technologies	background
and we believe that our frameworks for GPU programming are useful contributions given the increasing emphasis on parallel heterogeneous computing	background
In this paper	mechanism
we present two conceptual frameworks for GPU applications These frameworks enable smart GPU resource management when many applications share GPU resources while the workloads of those applications vary Application developers can explicitly adjust the number of GPU cores depending on their needs An implicit adjustment will be supported by a run-time framework	mechanism
which dynamically allocates the number of cores to tasks based on the total workload	mechanism
The runtime support of the proposed system can be realized using functions which measure the execution times of the tasks on GPU and change the number of GPU cores	mechanism
For very large dynamic networks	background
monitoring the behavior of a subset of agents provides an efficient framework for detecting changes in network topology In addition	background
we show how well-known tools in dynamic control systems may be useful for identifying abnormal events ; in particular	background
as a proof of concept	finding
In general we assume that the temporal behavior of a network agent is captured by a ( local ) dynamic state	mechanism
which may reflect either a physical property such as the number of connections or an abstract quantity such as opinions or beliefs	mechanism
Further assuming coupled linear inter-agent dynamics in which the local agent states evolve as weighted linear combinations of the neighboring agents ' states	mechanism
we focus on tracking network-wide agent dynamics	mechanism
Due to the large-scale nature of the problem	mechanism
directly monitoring data streams of the state dynamics for every individual agent is infeasible	mechanism
we propose a method that identifies a relatively small subset of agents whose state streams enable us Using structural properties of the coupled inter-agent dynamics	mechanism
we provide an algorithm	mechanism
which is polynomial in the number of agents	mechanism
we use a fault detection and isolation scheme	mechanism
Finally we illustrate our method and algorithms in a small test network	method
this method demonstrated promising results	finding
in terms of the accuracy of the initial detection accuracy and the reliability of the tracking	finding
This paper presents a computer vision algorithm	mechanism
by analyzing lane-marking detection results using an unscented Kalman filter To detect lateral and longitudinal lane-markings	mechanism
this method applies a spatial filter emphasizing the intensity contrast between lane-marking pixels and their neighboring pixels The authors then examine the detected lane-markings to identify perpendicular	mechanism
geometry layouts between longitudinal and lateral lane-markings for stop-line detection	mechanism
To provide reliable stop-line recognition	mechanism
the authors developed an unscented Kalman filter to track the detected stop-line over frames	mechanism
Through the tests with real-world	method
busy urban street videos	method
results show significant performance improvements over the baseline methods by using the estimated ROI for action recognition	finding
we introduce a cognitive assistive system Being able to accurately recognize user operations is one of the most important functionalities of the proposed system	mechanism
However even though various action recognition algorithms have been proposed in recent years	mechanism
it is still unknown whether they are adequate for recognizing operations in using home medical devices	mechanism
Since the lack of the corresponding database is the main reason causing the situation	mechanism
at the first part of this paper	mechanism
we present a database specially designed for studying the use of home medical devices	mechanism
Then we evaluate the performance of the existing approaches on the proposed database	mechanism
Although using state-of-art approaches which have demonstrated near perfect performance in recognizing certain general human actions	mechanism
we observe significant performance drop when applying it to recognize device operations	mechanism
We conclude that the tiny action involved in using devices is one of the most important reasons leading to the performance decrease	mechanism
To accurately recognize tiny actions	mechanism
it 's critical to focus on where the target action happens	mechanism
namely the region of interest ( ROI ) and have more elaborate action modeling based on the ROI	mechanism
Therefore in the second part of this paper	mechanism
we introduce a simple but effective approach to estimating ROI for recognizing tiny actions	mechanism
The key idea of this method is to analyze the correlation between an action and the sub-regions of a frame	mechanism
The estimated ROI is then used as a filter for building more accurate action representations	mechanism
Experimental	method
Given a real world graph	background
how should we lay-out its edges ? How can we compress it ? These questions are closely related	background
and the typical approach so far is to find clique-like communities	background
like the cavemen graph	background
and compress them	background
we show that SlashBurn consistently outperforms other methods for all data sets	finding
resulting in better compression and faster running time	finding
Moreover we show that SlashBurn with the appropriate spokes ordering can further improve compression while hardly sacrificing the running time	finding
Based on the idea	mechanism
we propose the SlashBurn method to recursively split a graph into hubs and spokes connected only by the hubs	mechanism
We also propose techniques to select the hubs and give an ordering to the spokes	mechanism
in addition to the basic SlashBurn	mechanism
We give theoretical analysis of the proposed hub selection methods	mechanism
Our view point has several advantages : ( a ) it avoids the no good cuts problem	mechanism
( b ) it gives better compression	mechanism
and ( c ) it leads to faster execution times for matrix-vector operations	mechanism
which are the back-bone of most graph processing tools	mechanism
Through experiments	method
We propose and study a new distributed Kalman filter algorithm The Network Tracking Capacity ( NTC ) of this algorithm depends only on the diffusion rate of the network and is independent of the local observation patterns	mechanism
only requiring global observability	mechanism
We analyze and compare the NTC for different network models	method
Big data has been becoming ubiquitous and applied in numerous fields recently	background
Our method achieves very high classification accuracies in face recognition in the presence of occlusions It also outperforms the state of the art methods in object recognition It hence shows its applicability to general computer vision and pattern recognition problems show our distributed method achieves high speedup of 7	finding
85x with just 10 machine nodes and can gain even more with more computing resources	finding
we propose a new machine learning approach named Distributed Class-dependent Feature Analysis ( DCFA ) The classifier is based on the estimation of class-specific optimal filters	mechanism
by solving an i-norm optimization problem We demonstrate how this problem is solved using the Alternating Direction Method of Multipliers and also explore relevant convergency details	mechanism
More importantly our proposed framework can be efficiently implemented on a robust distributed framework	mechanism
Thus it improves both accuracy and computational time in large-scale databases	mechanism
on AR database on two challenging large-scale object databases	method
i	method
e	method
Caltech101 and Caltech256	method
In addition computational time experiments on Caltech256 databases compared to the non-distributed version	method
Our model is underpinned by two key concepts	mechanism
a structural graph model ( composite network ) and a viral propagation model ( SI 1 I 2 S )	mechanism
Using this framework	mechanism
we formulate a non-linear dynamic system and perform an eigenvalue analysis to identify the tipping point of the epidemic behavior Based on insights gained from this analysis	mechanism
we demonstrate an effective and accurate prediction method to determine viral dominance	mechanism
which we call the EigenPredictor	mechanism
Next using a combination of synthetic and real composite networks	method
we evaluate the effectiveness of various viral suppression techniques by either a ) concurrently suppressing both memes or b ) unilaterally suppressing a single meme while leaving the other relatively unaffected	method
The Gates Hillman prediction market ( GHPM ) was an internet prediction market designed to predict the opening day of the Gates and Hillman Centers	background
the new computer science complex at Carnegie Mellon University Unlike a traditional continuous double auction format	background
the GHPM was mediated by an automated market maker	background
a central agent responsible for pricing transactions with traders over the possible opening days	background
The GHPMs event partition was	background
at the time	background
the largest ever elicited in any prediction market by an order of magnitude	background
and dealing with the markets size required new advances	background
including a novel span-based elicitation interface that simplified interactions with the market maker	background
We use the large set of identity-linked trades generated by the GHPM	method
Braincomputer interfaces ( BCIs ) are a promising technology for restoring motor ability to paralyzed patients	background
Spiking-based BCIs have successfully been used in clinical trials to control multi-degree-of-freedom robotic devices	background
Current implementations of these devices require a lengthy spike-sorting step	background
which is an obstacle to moving this technology from the lab to the clinic	background
Significance	background
Our results indicate that simple automated spike-sorting performs as well as the more computationally or manually intensive methods used here	background
Even basic spike-sorting adds value to the low-threshold waveform-crossing methods often employed in BCI decoding	background
Main results no spikes should be discarded spike-sorting is useful a fast and simple method is competitive	finding
Decoding using the joint waveform and tuning model shows promise but is not consistently superior	finding
Approach We present a full analysis of the effects of spike-sorting schemes on decoding performance	mechanism
Specifically we compare how well two common decoders	mechanism
the optimal linear estimator and the Kalman filter	mechanism
reconstruct the arm movements of non-human primates performing reaching tasks	mechanism
when receiving input from various sorting schemes The schemes we tested included : using threshold crossings without spike-sorting ; expert-sorting discarding the noise ; expert-sorting	mechanism
including the noise as if it were another neuron ; and automatic spike-sorting using waveform features	mechanism
We also decoded from a joint statistical model for the waveforms and tuning curves	mechanism
which does not involve an explicit spike-sorting step	mechanism
Discarding the threshold crossings that can not be assigned to neurons degrades decoding Decoding based on spike-sorted units outperforms decoding based on electrodes voltage crossings The four waveform based spike-sorting methods tested here yield similar decoding efficiencies	method
A common approach in crowdsourcing is to break large tasks into small microtasks so that they can be parallelized across many crowd workers and so that redundant work can be more easily compared for quality control	background
that non-sequential microtasks and the introduction of delays significantly decreases worker performance	finding
We show that interruptions where a large delay occurs between two related tasks can cause up to a 102 % slowdown in completion time	finding
and interruptions where workers are asked to perform different tasks in sequence can slow down completion time by 57 %	finding
We conclude with a set of design guidelines and instructions for implementing these changes in existing interfaces for crowd work	mechanism
In this paper	method
we demonstrate in a study of 338 crowd workers	method
We describe a new algorithm The running time of our algorithm is $ $ O ( f \log n \log \varDelta ) $ $ O ( flognlog ) where $ $ f $ $ f is the output complexity of the Voronoi diagram and $ $ \varDelta $ $ is the spread of the input	mechanism
the ratio of largest to smallest pairwise distances Despite the simplicity of the algorithm and its analysis	mechanism
it improves on the state of the art for all inputs with polynomial spread and near-linear output size	mechanism
The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation	mechanism
Then the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity	mechanism
yielding the output sensitive bound	mechanism
The removal only involves local flips and is inspired by kinetic data structures	mechanism
We prove our results we prove the result for general networks	finding
( ODE ) that models the dynamics of bi-virus epidemics over bilayer networks	mechanism
Each layer is a weighted digraph associated with a strain of virus ; the weights $ \gamma ^ { z } _ { ij } $ represent the rates of infection from node $ i $ to node $ j $ of strain $ z $	mechanism
We establish a sufficient condition on the $ \gamma $ s that guarantees survival of the fittestonly one strain survives	mechanism
We propose an ordering of the weighted digraphs	mechanism
the $ \star $ -order	mechanism
and show that if the weighted digraph of strain $ y $ is $ \star $ -dominated by the weighted digraph of strain $ x $	mechanism
then $ y $ dies out in the long run	mechanism
We prove that the orbits of the ODE accumulate to an attractor that captures the survival of the fittest phenomenon	mechanism
Due to the coupled nonlinear high-dimension nature of the ODEs	mechanism
there is no natural Lyapunov function to study their global qualitative behavior	mechanism
by combining two important properties of these ODEs : ( i ) monotonicity under a partial ordering on the set of graphs ; and ( ii ) dimension-reduction under symmetry of the graphs	method
Property ( ii ) allows us to fully address the survival of the fittest for regular graphs	method
Then by bounding the epidemics dynamics for generic networks by the dynamics on regular networks	method
The Internet of Things ( IoT ) offers the promise of integrating the digital world of the Internet with the physical world in which we live	background
This paper reports the design and development of an open community-oriented platform aiming	mechanism
featuring interoperability and reusability of heterogeneous sensor data and data services The concepts of virtual sensors and virtual devices are identified as central autonomic units to model scalable and context-aware configurable/reconfigurable sensor data and services The decoupling of the storage and management of sensor data and platform-oriented metadata enables the handling of both discrete and streaming sensor data	mechanism
A cloud computing-empowered prototyping system has been established as a proof of concept to host smart community-oriented sensor data and services	method
has an approximation ratio of 3/2 to the maximum cardinality matching	finding
This is an improvement over a recent upper bound of 2 ( Ashlagi et al	finding
2010 2 ] ) and	finding
furthermore our mechanism beats for the first time the lower bound on the approximation ratio of deterministic truthful mechanisms We complement our positive result with new lower bounds	finding
Among other statements	finding
we prove that the weaker incentive compatibility property of truthfulness in expectation in our mechanism is necessary ; universally truthful mechanisms that have an inclusion-maximality property have an approximation ratio of at least 2	finding
We study a mechanism design version of matching computation in graphs that models We present a new randomized matching mechanism for two agents which is truthful in expectation and	mechanism
Self-powered systems that interact with the physical world require computing platforms with predictable timing behavior and a low energy demand	background
we expose the hardware characteristics that violate assumptions of conventional energy models Our analysis shows that neither balancing the load nor assigning all load to the `` cheapest '' core is the best load distribution strategy	finding
unless the cores are extremely alike or extremely different	finding
We leverage the state-of-the-art in hardware design by adopting Heterogeneous Multi-core Processors with support for Dynamic Voltage and Frequency Scaling and Dynamic Power Management and propose a revised model suitable We then address the problem of allocating real-time software components onto heterogeneous cores such that total energy is minimized	mechanism
Our approach is to start from an analytically justified target load distribution and find a task assignment heuristic that approximates it	mechanism
The optimal load distribution is then formulated as a solution to a convex optimization problem A heuristic that approximates this load distribution and an alternative method that leverages the solution explicitly are proposed as viable task assignment methods	mechanism
Through experiments on one such platform The proposed methods are compared to state-of-the-art on simulated problem instances and in a case study of a soft-real-time application on an off-the-shelf ARM big	method
LITTLE heterogeneous processor	method
Poor posture and incorrect muscle usage are a leading cause of many injuries in sports and fitness	background
For this reason	background
non- invasive fine-grained sensing and monitoring of human motion and muscles is important for mitigating injury and improving fitness efficacy	background
Current sensing systems either de- pend on invasive techniques or unscalable approaches whose accuracy is highly dependent on body sensor placement	background
the system achieves greater than 95 % accuracy in identifying muscle groups	finding
We present MARS	mechanism
a system that by only using unobtrusive	mechanism
non-invasive in- ertial sensors MARS not only accurately senses and recreates human motion down to the muscles	mechanism
but also allows for fast personalized system setup by determining the individual identities of the instrumented muscles	mechanism
obtained with minimal system training	mechanism
In a real world human study con- ducted to evaluate MARS	method
The openness of wireless communication and the recent development of software-defined radio technology	background
respectively provide a low barrier and a wide range of capabilities for misbehavior	background
attacks and defenses against attacks	background
Matching our intuition	finding
the aggressiveness of an attacker is related to how much of a discount is placed on data delay	finding
This results in the defender often choosing to sleep despite the latency implication	finding
because the threat of jamming is high	finding
We also present several other findings	finding
In this work we present finite-energy jamming games	mechanism
a game model We also allow the jammer A major addition in finite-energy jamming games is that the jammer and sender both have a limited amount of energy which is drained according to the actions a player takes We develop a model of our system as a zero-sum finite-horizon stochastic game with deterministic transitions	mechanism
We leverage the zero-sum and finite-horizon properties of our model to design a simple polynomial-time algorithm to compute optimal randomized strategies for both players	mechanism
The utility function of our game model can be decoupled into a recursive equation	mechanism
Our algorithm exploits this fact to use dynamic programming to construct solutions in a bottom-up fashion	mechanism
For each state of energy levels	mechanism
a linear program is solved to find Nash equilibrium strategies for the subgame	mechanism
With these techniques	mechanism
our algorithm has only a linear dependence on the number of states	mechanism
and quadratic dependence on the number of actions	mechanism
allowing us to solve very large instances	mechanism
By computing Nash equilibria for our game models	mechanism
we explore what kind of performance guarantees can be achieved both for the sender and jammer	mechanism
when playing against an optimal opponent	mechanism
We also use the optimal strategies to simulate finite-energy jamming games and provide insights into robust communication among reconfigurable	mechanism
yet energy-limited radio systems	mechanism
To test the performance of the optimal strategies we compare their performance with a random and adaptive strategy from simulations where we vary the strategies for one or both of the players	method
Multimedia event detection ( MED ) and multimedia event recounting ( MER ) are fundamental tasks in managing large amounts of unconstrained web videos	background
and have attracted a lot of attention in recent years	background
and obtain very promising results for both MED and MER	finding
we propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events	mechanism
Our premise is that a good recounting algorithm should not only explain the detection result	mechanism
but should also be able to assist detection in the first place	mechanism
Coupled in a joint optimization framework	mechanism
recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences	mechanism
To better utilize the powerful and interpretable semantic video representation	mechanism
we segment each video into several shots and exploit the rich temporal structures at shot level	mechanism
The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm	mechanism
which after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps	mechanism
enables us to efficiently process extremely large video corpora	mechanism
We test the proposed method on the large scale TRECVID MEDTest 2014 and MEDTest 2013 datasets	method
Despite persistent effort	background
many web pages are still not accessible to everyone	background
We also present our user study results where web developers who had varying knowledge of web accessibility all found our system an effective and interesting platform to learning web accessibility	finding
In this paper	mechanism
we present the design	mechanism
development and study of CAN ( Composable Accessibility Infrastructure )	mechanism
a crowdsourcing infrastructure that collects web accessibility issues and their fixes	mechanism
dynamically composes solutions on-the-fly	mechanism
and delivers the crowd-sourced content as teaching materials Our unique CAN user interaction and system design enables end users with disabilities to both benefit from and contribute to the system without additional effort in their daily web browsing	mechanism
and allows web developers to experience real accessibility issues and initiate a learning process with first-hand materials CAN also provides an opportunity for data-driven discovery of the common implementation practices that cause accessibility issues	mechanism
We show how CAN addresses a set of accessibility issues on the top 100 popular websites	method
If the people belong to multiple online communities	background
their joint membership can influence the survival of each of the communities to which they belong	background
Communities with many joint memberships may struggle to get enough of their members ' time and attention	background
but find it easy to import best practices from other communities	background
Our contributions are two-fold	background
Theoretically by examining the impact of membership overlap on the survival of online communities we identified an important mechanism underlying the success of online communities	background
Practically our findings may guide community creators on how to effectively manage their members	background
and tool designers on how to support this task	background
we find that higher levels of membership overlap are positively associated with higher survival rates of online communities	finding
Furthermore we find that it is beneficial for young communities to have shared members who play a central role in other mature communities	finding
By analyzing the historical data of 5673 Wikia communities	method
We demonstrate the Acoustic Location Processing System ( ALPS )	mechanism
a platform that augments BLE proximity beacons with ultrasonic transmitters in a manner { \em ALPS } uses Time-Difference-Of-Arrival ( TDOA ) and Time-Of-Flight ( TOF ) ranging to accurately localize mobile devices such as off-the-shelf smartphones and tablets in 2D space	mechanism
Users inside the demo area will be able to determine their location and can directly plot it relatively to a map of the area using our app Once a receiving device has determined its initial position	mechanism
it can synchronize its audio clock with the transmission infrastructure to perform TOF-based localization	mechanism
which provides similar position accuracy to TDOA based localization with fewer beacons	mechanism
Multilateration and trilateration processing for each device 's location is offloaded onto a cloud-based solver that can provide localization as a service to ALPS and similar TOF/TDOA based systems	mechanism
These results suggest that the instantaneous details of single-trial movement speed are difficult to extract using commonly assumed coding schemes	background
This apparent paucity of speed information takes particular importance in the context of brain-machine interfaces ( BMIs )	background
which rely on extracting kinematic information from motor cortex Previous studies have highlighted subjects ' difficulties in holding a BMI cursor stable at targets	background
These studies along with our finding of relatively little speed information in motor cortex	background
BMI systems enabling stable stops will be more effective and user-friendly when translated into clinical applications	background
we found that single units carry relatively little speed-related information compared with direction-related information This result is not mitigated at the population level : simultaneously recorded population activity predicted speed with significantly lower accuracy relative to direction predictions revealed that speed accuracy would likely remain lower than direction accuracy	finding
even given larger populations	finding
SDKF improved success rates by a factor of 1	finding
7 relative to a standard Kalman filter in a closed-loop BMI task requiring stable stops at targets	finding
inspired a speed-dampening Kalman filter ( SDKF ) that automatically slows the cursor upon detecting changes in decoded movement direction Effectively	mechanism
SDKF by using prevalent directional signals	mechanism
rather than requiring speed to be directly decoded from neural activity	mechanism
Using information theoretic techniques	method
Furthermore a unit-dropping analysis	method
Traditional hard real-time scheduling algorithms require the use of the worst-case execution times to guarantee that deadlines will be met	background
showing that ZS-QRAM is able to obtain 4 as much UDR as ZSRM	finding
a previous overbooking approach	finding
and almost 2 as much UDR as Rate-Monotonic with Period Transformation ( RM/TP We show that	finding
by using our approach	finding
we are able to keep the tasks that render the most utility by degrading lower-utility ones even in the presence of highly dynamic execution times	finding
In this article	mechanism
we present ZS-QRAM	mechanism
a scheduling approach that enables the use of flexible execution times and application-derived utility to tasks in order In particular	mechanism
we provide a detailed description of the algorithm	mechanism
the formal proofs for its temporal protection	mechanism
and a detailed	mechanism
evaluation	mechanism
Our evaluation uses the Utility Degradation Resilience ( UDR ) We then evaluate a Linux kernel module implementation of our scheduler on an Unmanned Air Vehicle ( UAV ) platform	method
Complex events essentially include human	background
scenes objects and actions that can be summarized by visual attributes	background
so leveraging relevant attributes properly could be helpful for event detection	background
Many works have exploited attributes at image level for various applications	background
validate the efficacy of the proposed approach	finding
Hence we propose to leverage attributes at video level ( named as video attributes in this work )	mechanism
i	mechanism
e	mechanism
the semantic labels of external videos are used as attributes	mechanism
Compared to complex event videos	mechanism
these external videos contain simple contents such as objects	mechanism
scenes and actions which are the basic elements of complex events	mechanism
Specifically building upon a correlation vector which correlates the attributes and the complex event	mechanism
we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos	mechanism
Extensive experiments on a real-world large-scale dataset	method
Traditional power system state estimation methods lack the ability to track and manage increasing uncertainties inherent in the new technologies	background
such as recent and ongoing massive penetration of renewable energy	background
distribution intelligence and plug-in electric vehicles	background
To deal with the inability	background
a recent work proposes to utilize the unused historical data for power system state estimation First	background
because the power systems are with periodic patterns	background
which create clustered measurement data	background
results show that the new method can dramatically reduce the necessary computational time for online data-driven state estimation	finding
while producing a highly accurate state estimate	finding
dimension reduction is proposed	mechanism
but still able to retrieve similar measurements the k-dimensional tree indexing approach is employed in step two resulting in a log-reduction over searching time	mechanism
Finally we verify the obtained historical power system states via AC power system model and the current measurements to filter out bad historical data	method
Simulation	method
The Internet of Things ( IoT ) aims to integrate the digital world of the Internet with our encompassing physical world	background
This paper reports our on-going work developing a sensor service federation and provisioning infrastructure novel approach has been presented to build social sensor networks A two-way publish/subscribe pattern on top of a message bus is established Workflow provenance is carried by a dynamic virtual device concept that we have introduced	mechanism
A case study is reported A case study is reported	method
A key idea in object-oriented programming is that objects encapsulate state and interact with each other by message exchange This perspective suggests a model of computation that is inherently concurrent ( to facilitate simultaneous message exchange ) and that accounts for the effect of message exchange on an object 's state ( to express valid sequences of state transitions )	background
we show that our language supports the typical patterns of object-oriented programming ( e	finding
g	finding
encapsulation dynamic dispatch	finding
and subtyping ) while guaranteeing session fidelity in a concurrent setting In addition	finding
we show that our language facilitates new forms of expression ( e	finding
g	finding
type-directed reuse internal choice )	finding
which are not available in current object-oriented languages	finding
We introduce an object-oriented programming language that has processes as its only objects and employs linear session types	mechanism
Based on various examples We have implemented our language in a prototype compiler	method
The Pascaline was the first working mechanical calculator	background
created in 1642 by the French polymath Blaise Pascal	background
Over the next two decades Pascal built 40 of these machines	background
of which nine survive today	background
Several good web resources describe the Pascaline	background
but to properly appreciate the sautoir	background
Pascal 's kinetic energy solution to jam-free ripple carry	background
building a working replica is invaluable	background
The Pascaline kit	finding
designed in SolidWorks	finding
is open source and available at http : //www	finding
cs	finding
cmu	finding
edu/~dst/Pascaline	finding
I 've created a Pascaline kit using laser-cut acrylic and standard fasteners that can be assembled with just a screwdriver	mechanism
pliers and Loctite High school or college students with minimal skills can put it together in a few hours and have a functioning calculator	mechanism
Exploring the Pascaline 's design is an engaging way to connect a milestone in the early history of computing with more modern theoretical concepts	mechanism
Students can investigate questions such as : What makes a device `` digital '' ? ( Slide rules have numeric scales but are analog devices	mechanism
) How does nonlinearity produce discrete states in a continuous world ? How are nonlinearities induced in the Pascaline vs	mechanism
in digital electronics ? How do the logic design concepts `` half adder '' and `` full adder '' map onto the components of the Pascaline ? Is the Pascaline really adding	mechanism
or merely counting ? How does the Pascaline use nines complement arithmetic to perform subtraction	mechanism
and why is n't it tens complement ?	mechanism
User review is a crucial component of open mobile app markets such as the Google Play Store	background
How do we automatically summarize millions of user reviews and make sense out of them ? We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users	background
Results using our techniques are reported	finding
In this paper	mechanism
we propose Wiscom	mechanism
a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail	mechanism
Our system is able to ( a ) discover inconsistencies in reviews ; ( b ) identify reasons why users like or dislike a given app	mechanism
and provide an interactive	mechanism
zoomable view of how users ' reviews evolve over time ; and ( c ) provide valuable insights into the entire app market	mechanism
identifying users ' major concerns and preferences of different types of apps	mechanism
on a 32GB dataset consisting of over 13 million user reviews of 171	method
493 Android apps in the Google Play Store	method
In an effort to address persistent consumer privacy concerns	background
policy makers and the data industry seem to have found common grounds in proposals that aim at making online privacy more `` transparent	background
'' Such self-regulatory approaches rely on	background
among other things	background
providing more and better information to users of Internet services about how their data is used These findings cast doubts on the likelihood of initiatives predicated around notices and transparency to address	background
by themselves online privacy concerns	background
we demonstrate that the impact of privacy notices on disclosure is sensitive to relative judgments	finding
even when the objective risks of disclosure actually stay constant	finding
we show that the impact of privacy notices on disclosure can be muted by introducing simple misdirections that do not alter the objective risk of disclosure	finding
in a series of experiments In a first experiment	method
In a second experiment	method
The commoditization of wireless sensing systems makes it feasible to include BAS functionality in small and medium-sized buildings	background
In this demo we introduce a platform called Mortar	mechanism
io which Unlike cloud-reliant systems	mechanism
Mortar	mechanism
io distributes storage and control functionality across end devices making it robust to network and internet outages The system	mechanism
once initialized can run autonomously on a low-cost controller within a building or connect to the cloud for remote monitoring and configuration	mechanism
We will also show our efficient multi-resolution data store that buffers data locally and replicates aggregate data across devices for reliability	mechanism
A publish-subscribe model built on top of XMPP is used for messaging with per-device access control and a transducer schema	mechanism
Finally a web portal provides an interface to monitor and schedule lighting	mechanism
plug-loads environmental sensors and HVAC from a single uniform interface	mechanism
ApplianceReader broadly demonstrates the potential of hybrid approaches that combine human and machine intelligence to effectively realize intelligent	background
interactive access technology today	background
we present ApplianceReader - a system that combines a wearable point-of-view camera with on-demand crowdsourcing and computer vision ApplianceReader sends photos of appliance interfaces that it has not seen previously to the crowd	mechanism
who work in parallel to quickly label and describe elements of the interface	mechanism
Computer vision techniques then track the user 's finger pointing at the controls and read out the labels previously provided by the crowd This enables visually impaired users to interactively explore and use appliances without asking the crowd repetitively	mechanism
Advances in real-time	background
embedded and distributed systems along with control and communication theory have catalyzed the rapid emergence of cyber-physical systems such as a self-driving car The importance of fault-tolerance support on a cyber-physical system ( CPS ) has been greatly emphasized by recent research due to the nature of CPS that senses its surroundings	background
processes sensor data	background
and reacts using its actuators	background
In order to tackle this challenge	background
we proposed SAFER ( System-level Architecture for Failure Evasion in Real-time Applications ) in our previous work	background
SAFER is able to tolerate fail-stop processor and/or task failures for distributed embedded real-time systems	background
This paper provides a method by ( 1 ) deploying a small piece of hardware to avoid a dedicated connection between a processor and an actuator	mechanism
( 2 ) adding a software module that monitors and controls the hardware	mechanism
and ( 3 ) enhancing the failure detection and recovery mechanisms of SAFER to support these changes	mechanism
The detailed implementation and evaluation of the SAFER extension is on-going work	method
Behavioral coding is a common technique in the social sciences and human computer interaction for extracting meaning from video data [ 3 Rapid coding allows participants to have a `` conversation with their data '' to rapidly develop and refine research hypotheses in ways not previously possible	background
We show that Glance can accurately code events in video in a fraction of the time it would take a single person showing that Glance is able to code 80 % of an hour-long video in just 5 minutes	finding
We present Glance	mechanism
a tool Glance uses the crowd to interpret natural language queries	mechanism
and then aggregates and summarizes the content of the video	mechanism
We also investigate speed improvements made possible by recruiting large crowds	method
The model checker CBMC automatically verifies 5208 lines of C code in about 80 seconds using less than 2GB of RAM indicate that XMHF 's performance is comparable to popular high-performance general-purpose hypervisors for the single guest that it supports	finding
We present the design	mechanism
implementation and verification of XMHF- an eXtensible and Modular Hypervisor Framework XMHF is designed XMHF includes a core that provides functionality common to many hypervisor-based security architectures and supports extensions that augment the core with additional security or functional properties while preserving the fundamental hypervisor security property of memory integrity ( i	mechanism
e	mechanism
ensuring that the hypervisor 's memory is not modified by software running at a lower privilege level )	mechanism
We verify the memory integrity of the XMHF core -- 6018 lines of code -- using a combination of automated and manual techniques We manually audit the remaining 422 lines of C code and 388 lines of assembly language code that are stable and unlikely to change as development proceeds	method
Our experiments	method
The promise of `` smart '' homes	background
workplaces schools and other environments has long been championed	background
Unattractive however has been the cost to run wires and install sensors	background
More critically raw sensor data tends not to align with the types of questions humans wish to ask	background
e	background
g	background
do I need to restock my pantry ? Through our API	background
Zensors can enable a variety of rich end-user applications and moves us closer to the vision of responsive	background
intelligent environments	background
We propose Zensors	mechanism
a new sensing approach that fuses real-time human intelligence from online crowd workers with automatic approaches With Zensors	mechanism
users can go from question to live sensor feed in less than 60 seconds	mechanism
Video analysis has been attracting increasing research due to the proliferation of internet videos	background
and the comparison to other state-of-the-art multi-feature learning algorithms has validated the efficacy of our framework	finding
For better exploitation of multiple features	mechanism
we propose to mine the importance of different features and cast it into the learning of the classification model	mechanism
Our method is based on multiple graphs from different features and uses the Riemannian metric to evaluate the feature importance	mechanism
On the other hand	mechanism
to be able to use limited labeled training videos for a respectable accuracy we formulate our method in a semi-supervised way The main contribution of this paper is a novel scheme of evaluating the feature importance that is further casted into a unified framework of harnessing multiple weighted features with limited labeled training videos	mechanism
We perform extensive experiments on video action recognition and multimedia event recognition	method
the proposed distributed algorithms are shown to achieve optimal learning asymptotically	finding
i	finding
e	finding
almost surely ( a	finding
s	finding
) each network agent is shown to learn the value function and the optimal stationary control policy of the collaborative MDP asymptotically Further	finding
convergence rate estimates for the proposed class of distributed learning algorithms are obtained	finding
Distributed reinforcement learning algorithms The networked setup consists of a collection of agents ( learners ) which respond differently ( depending on their instantaneous one-stage random costs ) to a global controlled state and the control actions of a remote controller	mechanism
the paper presents distributed variants of Q-learning of the consensus + innovations type in which each agent sequentially refines its learning parameters by locally processing its instantaneous payoff data and the information received from neighboring agents	mechanism
Under broad conditions on the multi-agent decision model and mean connectivity of the inter-agent communication network	method
With the advancement of information systems	background
means of communications are becoming cheaper	background
faster and more available	background
Today millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want	background
They can access their e-mails	background
comment on weblogs	background
watch and post videos and photos ( as well as comment on them )	background
and make phone calls or text messages almost ubiquitously	background
We also show three potential applications of the SFP : as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications	background
as a technique to detect anomalies	background
and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems	background
Moreover we propose the use of the Self-Feeding Process ( SFP ) The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data	mechanism
we analyzed eight different datasets from real and modern communication data and found four well-defined patterns seen in all the eight datasets	method
Multimedia Event Detection ( MED ) is a multimedia retrieval task with the goal of finding videos of a particular event in video archives	background
given example videos and event descriptions ; different from MED	background
multimedia classification is a task that classifies given videos into specified classes Generally	background
early fusion and late fusion are two popular combination strategies	background
The former one fuses features before performing classification and the latter one combines output of classifiers from different features	background
Early fusion can better capture the relationship among features yet is prone to over-fit the training data	background
Late fusion deals with the over-fitting problem better but does not allow classifiers to train on all the data at the same time	background
Results are reported For the MED 2010 dataset	finding
we get a mean minimal normalized detection cost ( MMNDC ) of 0	finding
49 which exceeds the state-of-the-art performance by more than 12 percent	finding
On the TRECVID MED 2011 test dataset	finding
we achieve a MMNDC of 0	finding
51 which is the second best among all 19 participants	finding
On UCF50 and HMDB51	finding
we obtain classification accuracy of 88	finding
1 % and 48	finding
7 % respectively	finding
which are the best reported results to date	finding
In this paper	mechanism
we introduce a fusion scheme named double fusion	mechanism
which simply combines early fusion and late fusion together to incorporate their advantages	mechanism
on the TRECVID MED 2010	method
MED 2011 UCF50 and HMDB51 datasets	method
demonstrate that AutoPlait does indeed detect meaningful patterns correctly	finding
and it outperforms state-of-the-art competitors as regards accuracy and speed : AutoPlait achieves near-perfect	finding
over 95 % precision and recall	finding
and it is up to 472 times faster than its competitors	finding
In this paper we present AutoPlait	mechanism
a fully automatic mining algorithm Our method has the following properties : ( a ) effectiveness : it operates on large collections of time-series	mechanism
and finds similar segment groups that agree with human intuition ; ( b ) scalability : it is linear with the input size	mechanism
and thus scales up very well ; and ( c ) AutoPlait is parameter-free	mechanism
and requires no user intervention	mechanism
no prior training	mechanism
and no parameter tuning	mechanism
Extensive experiments on 67GB of real datasets	method
For the various test cases	finding
in-memory data reorganization provides orders of magnitude performance and energy efficiency improvements via low overhead hardware	finding
This paper presents a two pronged approach	mechanism
which combines ( i ) a proposed DRAM-aware reshape accelerator integrated within 3D-stacked DRAM	mechanism
and ( ii ) a mathematical framework that is used to represent and optimize the reorganization operations	mechanism
We evaluate our proposed system through two major use cases First	method
we demonstrate the reshape accelerator in performing a physical address remapping via data layout transform to utilize the internal parallelism/locality of the 3D-stacked DRAM structure more efficiently for general purpose workloads Then	method
we focus on offloading and accelerating commonly used data reorganization routines selected from the Intel Math Kernel Library package We evaluate the energy and performance benefits of our approach by comparing it against existing optimized implementations on state-of-the-art GPUs and CPUs	method
Can we identify patterns of temporal activities caused by human communications in social media ? Is it possible to model these patterns and tell if a user is a human or a bot based only on the timing of their postings ? Social media services allow users to make postings	background
generating large datasets of human activity time-stamps	background
and find that the distribution of postings inter-arrival times ( IAT ) is characterized by four patterns : ( i ) positive correlation between consecutive IATs	finding
( ii ) heavy tails	finding
( iii ) periodic spikes and ( iv ) bimodal distribution	finding
by showing that it can accurately fit real time-stamp data from Reddit and Twitter	finding
We also show that RSC can be used to spot outliers and detect users with non-human behavior	finding
such as bots	finding
RSC consistently provides a better fit to real data and clearly outperform existing models for human dynamics	finding
RSC was also able to detect bots with a precision higher than 94 %	finding
Based on our findings	mechanism
we propose Rest-Sleep-and-Comment ( RSC )	mechanism
a generative model	mechanism
We demonstrate the utility of RSC We validate RSC using real data consisting of over 35 million postings from Twitter and Reddit	method
Detecting and quantifying the timing and the genetic contributions of parental populations to a hybrid population is an important but challenging problem in reconstructing evolutionary histories from genetic variation data	background
With the advent of high throughput genotyping technologies	background
new methods suitable for large-scale data are especially needed	background
On simulated sequences	finding
our methods show better accuracy and faster runtime than leading competitive methods in estimating admixture fractions and divergence times Analysis on the real data further shows our methods to be effective at matching our best current knowledge about the relevant populations	finding
Here we propose a novel method that combines prior work for inferring nonreticulate population structures with an MCMC scheme for sampling over admixture scenarios using genome-scale admixed genetic variation data	mechanism
We validated our method using coalescent simulations and a collection of real bovine and human variation data	method
From a technical viewpoint	background
the proposed distributed estimator leads to non-Markovian mixed timescale stochastic recursions and the analytical methods developed in the paper contribute to the general theory of distributed stochastic approximation	background
is shown to yield consistent parameter estimates at each network agent	finding
Further it is shown that the distributed estimator is asymptotically efficient	finding
in that the asymptotic covariances of the agent estimates coincide with that of the optimal centralized estimator	finding
i	finding
e	finding
the inverse of the centralized Fisher information rate	finding
in multi-agent networks with exponential family observation statistics A certainty-equivalence type distributed estimator of the consensus + innovations form is proposed in which	mechanism
at each each observation sampling epoch agents update their local parameter estimates by appropriately combining the data received from their neighbors and the locally sensed new information ( innovation )	mechanism
Under global observability of the networked sensing model	mechanism
i	mechanism
e	mechanism
the ability to distinguish between different instances of the parameter value based on the joint observation statistics	mechanism
and mean connectivity of the inter-agent communication network	mechanism
the proposed estimator	mechanism
Thus it appears that disorders of the Rb/E2F axis can arise at multiple organ sites and produce tumors that simultaneously overexpress multiple E2F-responsive genes	background
In breast cancer	finding
a group of tumors was identified	finding
each of which simultaneously overexpressed multiple E2F-responsive genes	finding
Seventy percent of these genes were concerned with cell cycle progression	finding
DNA repair or mitosis These E2F-responsive gene overexpressing ( ERGO ) tumors frequently exhibited additional evidence of Rb/E2F axis dysfunction	finding
were mostly triple negative	finding
and preferentially overexpressed multiple basal cytokeratins	finding
suggesting that they overlapped substantially with the basal-like tumor subset	finding
ERGO tumors were also identified in serous ovarian cancer and prostate cancer	finding
In these cancer types	finding
there was no evidence for a tumor subset comparable to the breast cancer basal-like subset	finding
A core group of about 30 E2F-responsive genes were overexpressed in all three cancer types	finding
we compiled a list of E2F-responsive genes from the literature and evaluated their expression in publicly available gene expression microarray data of patients with breast cancer	method
serous ovarian cancer	method
and prostate cancer	method
We conclude with example applications and thoughts on future avenues of research	background
which demonstrate the feasibility of our approach	finding
In particular by manipulating the internal air pressure of various pneumatic elements	mechanism
we can create mechanisms that require different levels of actuation force and can also change their shape	mechanism
We describe the challenges that we faced and the methods that we used to overcome some of the limitations of current 3D printing technology	mechanism
We introduce and discuss a series of example 3D printed pneumatic controls This includes conventional controls	method
such as buttons	method
knobs and sliders	method
but also extends to domains such as toys and deformable interfaces	method
Many people would find the Web easier to use if content was a little bigger	background
even those who already find the Web possible to use now We believe this concept applies generally across a wide range of accessibility improvements designed to help people with diverse abilities	background
by magnifying existing web pages 1	finding
6x on average without introducing negative side effects	finding
This paper introduces the idea of opportunistic accessibility improvement in which We explore this idea with oppaccess	mechanism
js an easily-deployed system for magnifying web pages that iteratively increases magnification until it notices negative side effects	mechanism
such as horizontal scrolling or overlapping text	mechanism
We validate this approach	method
These theoretical results have direct practical implications	background
we show that such allocations may not exist	finding
but allocations guaranteeing each player 2/3 of the above value always exist	finding
and can be computed in polynomial time when the number of players is constant	mechanism
Assuming additive valuation functions	method
In particular while our generic procedure is computationally inefficient	finding
for the specific definition of H as graphs of bounded degree	finding
we exhibit efficient ways of constructing f H using different projection-based techniques	finding
We demonstrate that the restricted sensitivity of such queries can be significantly lower than their smooth sensitivity Thus	finding
using restricted sensitivity we can maintain privacy whether or not D HH	finding
while providing more accurate results in the event that HH holds true	finding
We demonstrate the usefulness of this notion by considering the task of answering queries regarding social-networks	method
We then analyze two important query classes : subgraph counting queries ( e	method
g	method
number of triangles ) and local profile queries ( e	method
g	method
number of people who know a spy and a computer-scientist who know each other )	method
Activity recognition can provide computers with the context underlying user inputs	background
enabling more relevant responses and more fluid interaction	background
Prior work has enabled the crowd to provide labels in real-time to train automated systems on-the-fly	background
but numerous examples are still needed before the system can recognize an activity on its own	background
show that over seven times as many examples can be collected using our approach versus relying on direct observation alone	finding
demonstrating that by leveraging the understanding of the crowd	finding
it is possible to more easily train automated systems	finding
we introduce ARchitect	mechanism
a system that uses the crowd to capture the dependency structure of the actions that make up activities	mechanism
Our tests	method
Multimedia event detection ( MED ) plays an important role in many applications such as video indexing and retrieval	background
demonstrate the effectiveness of the proposed approach	finding
In this paper	mechanism
we propose an approach that exploits the external concepts-based videos and event-based videos simultaneously to learn an intermediate representation from video features	mechanism
Our algorithm integrates the classifier inference and latent intermediate representation into a joint framework	mechanism
The joint optimization of the intermediate representation and the classifier makes them mutually beneficial and reciprocal	mechanism
Effectively the intermediate representation and the classifier are tightly correlated	mechanism
The classifier dependent intermediate representation not only accurately reflects the task semantics but is also more suitable for the specific classifier	mechanism
Thus we have created a discriminative semantic analysis framework based on a tightly coupled intermediate representation	mechanism
Extensive experiments on multimedia event detection using real-world videos	method
Ultrasonic guided-waves propagating in pipes with varying environmental and operational conditions ( EOCs ) are usually the results of complex superposition of multiple modes travelling in multiple paths	background
Among all of the components forming a complex guided-wave signal	background
the arrivals scattered by damage ( so called scatter signal ) are of importance for damage diagnosis purposes	background
Current approaches for extracting scatter signal can be categorized as ( A ) baseline subtraction methods	background
and ( B ) linear decomposition	background
The simulation results show that different wave modes may have significantly different sensitivities to temperature variations This brings about challenges such as shape distortion and nonlinear relations between the signals recorded at different temperatures	finding
which prevent the aforementioned methods to be extensible to wide range of temperatures	finding
It is observed that NLPCA can successfully remove nonlinear relations between the signal bases	finding
hence extract scatter signal	finding
for temperature variations up to 10	finding
with detection accuracies above 99 %	finding
we examine the potential of a nonlinear decomposition method	mechanism
namely nonlinear principal component analysis ( NLPCA )	mechanism
To better analyze the experimental results	method
the effects of temperature on multi-modal signals are simulated Ultrasonic pitch-catch measurements from an aluminum pipe segment in a thermally controlled laboratory are used to evaluate the detection performance of the damage-sensitive features extracted by the proposed approach	method
Abstract In case of an emergency in a building	background
first responders need to know current blockages in the building ( e	background
g	background
blocked passageways and exits ) and safe evacuating paths so that the occupants can be guided to the unblocked exits and safe paths toward those exits The estimated blockage information can be used to create a topological map of the damaged building	background
indicating safe paths toward available unblocked exits	background
The results demonstrated the technical feasibility of the proposed system and the findings of the decision tree highlight that by using less number of sensors	finding
a cost-effective configuration can be achieved	finding
a system that fuses data from multiple sensors and video camera was proposed A prototype was developed and a decision tree method was used	mechanism
and tested on an experimental model of a pilot building 's hallway	method
A series of damage tests were conducted on the hallway model and recorded by the sensors and the video camera	method
Individual performances of sensors and video camera were evaluated	method
Topic models are effective probabilistic tools for processing large collections of unstructured data	background
The open-source C++ implementation of the proposed algorithm is available at https : //github	background
com/xunzheng/light_medlda	background
We focus on the Gibbs MedLDA model [ 27 ] that is able to simultaneously discover latent structures and make accurate predictions	mechanism
By combining a set of sampling techniques we are able to reduce the O ( K 3 + DK 2 + DNK complexity in [ 27 ] to O ( DK + DN ) when there are K topics and D documents with average length N	mechanism
To our best knowledge	mechanism
this is the first linear time sampling algorithm for supervised topic models	mechanism
Our algorithm requires minimal modifications to incorporate most loss functions in a variety of supervised tasks	mechanism
and we observe in our experiments an order of magnitude speedup over the current state-of-the-art implementation	mechanism
while achieving similar prediction performances	mechanism
for extending prior work where both are assumed to be deterministic	background
We show that the Time Reversal Likelihood Ratio Test performs much better than the conventional Weighted Energy Detector	finding
results show that the Linear Quadratic detector is a good approximation to the Time Reversal Likelihood Ratio Test and that both show a significant improvement of 4 to 7 dB effective signal-to-noise ratio gain over the Weighted Energy Detector	finding
This gain is dependent on the target and clutter power spectral densities	finding
We derive the time reversal likelihood ratio optimal detector We suppress the stationary clutter through adaptive power allocation	mechanism
We provide analytical on the performance of the time reversal detector by approximating it with the Time Reversal Linear Quadratic detector	method
which models the received signal as a Complex Gaussian Our simulations	method
Text can often be complex and difficult to read	background
especially for people with cognitive impairments or low literacy skills	background
Text simplification is a process that reduces the complexity of both wording and structure in a sentence	background
while retaining its meaning	background
This may allow simplification systems and system builders to get better feedback about how well content is being simplified	background
as compared to standard measures which classify content into 'simplified ' or 'not simplified ' categories Our study provides evidence that the crowd could be used to evaluate English text simplification	background
as well as to create simplified text in future work	background
We show that leveraging crowds can result in a collective decision that is accurate and converges to a consensus rating	finding
show that the crowd can effectively rate levels of simplicity	finding
This paper focuses on the evaluation of English text simplification using the crowd	method
Our results from 2	method
500 crowd annotations	method
Seeking solutions from one domain to solve problems in another is an effective process of innovation	background
This work provides a useful method for finding analogies	background
and can streamline innovation for both novices and professional designers	background
we show the benefits of using abstract structural representations to search for ideas that are analogous to a source problem	finding
and that these analogies result in better solutions than alternative approaches	finding
In this paper	mechanism
we present a novel approach We propose a crowdsourcing process that helps people navigate a large dataset to find analogies	mechanism
Through two experiments	method
In 1876 Charles Lutwidge Dodgson suggested the intriguing voting rule that today bears his name Although Dodgsons rule is one of the most well-studied voting rules	background
it suffers from serious deficiencies	background
both from the computational point of viewit is NP-hard even to approximate the Dodgson score within sublogarithmic factorsand from the social choice point of viewit fails basic social choice desiderata such as monotonicity and homogeneity	background
Furthermore we show that a slight variation on a known voting rule yields a monotonic	finding
homogeneous polynomial-time O ( m log m ) -approximation algorithm and establish that it is impossible to achieve a better approximation ratio even if one just asks for homogeneity we prove that algorithms with an approximation ratio that depends only on m do not exist	finding
We design a monotonic exponential-time algorithm that yields a 2-approximation to the Dodgson score	mechanism
while matching this result with a tight lower bound We also present a monotonic polynomial-time O ( log m ) -approximation algorithm ( where m is the number of alternatives ) ; this result is tight as well due to a complexity-theoretic lower bound	mechanism
We complete the picture by studying several additional social choice properties ; for these properties	method
shows high accuracy in the detection of seed nodes	finding
in addition to the correct automatic identification of their number Moreover	finding
NetSleuth scales linearly in the number of nodes of the graph	finding
and give an efficient method called NetSleuth for the well-known susceptible-infected virus propagation model	mechanism
We propose to employ the minimum description length principle as the one by which we can most succinctly describe the infected graph	mechanism
We give an highly efficient algorithm given a snapshot	mechanism
Then given these seed nodes	mechanism
we show we can optimize the virus propagation ripple in a principled way by maximizing likelihood	mechanism
With all three combined	mechanism
NetSleuth can automatically identify the correct number of seed nodes	mechanism
as well as which nodes are the culprits	mechanism
Experimentation on our method	method
that Respawn is able to run on ARM-based edge node devices connected to a cloud-backend with the ability to serve thousands of clients and terabytes of data with sub-second latencies	finding
In this paper	mechanism
we present a cloud-to-edge partitioned architecture called Respawn Respawn targets sensing systems where resource-constrained edge node devices may only have limited or intermittent network connections linking them to a cloud-backend	mechanism
The cloud-backend provides aggregate storage and transparent dispatching of data queries to edge node devices	mechanism
Data is downsampled as it enters the system creating a multi-resolution representation capable of lowlatency range-base queries	mechanism
Lower-resolution aggregate data is automatically migrated from edge nodes to the cloud-backend both for improved consistency and caching	mechanism
In order to further mask latency from users	mechanism
edge nodes automatically identify and migrate blocks of data that contain statistically interesting features	mechanism
We show through simulation and micro-benchmarking	method
Online instructional videos have become a popular way for people to learn new skills encompassing art	background
cooking and sports	background
show that the examples harvested are of reasonably good quality	finding
and action detectors trained on data collected by our unsupervised method yields comparable performance with detectors trained with manually collected data on the TRECVID Multimedia Event Detection task	finding
We propose to utilize the large amount of instructional videos available online The key observation is that in instructional videos	mechanism
the instructor 's action is highly correlated with the instructor 's narration	mechanism
By leveraging this correlation	mechanism
we can exploit the timing of action corresponding terms in the speech transcript to temporally localize actions in the video and harvest action examples The proposed method is scalable as it requires no human intervention	mechanism
Experiments	method
We introduce GOTCHAs ( Generating panOptic Turing Tests to Tell Computers and Humans Apart ) as A GOTCHA is a randomized puzzle generation protocol	mechanism
which involves interaction between a computer and a human	mechanism
Informally a GOTCHA should satisfy two key properties : ( 1 ) The puzzles are easy for the human to solve	mechanism
( 2 ) The puzzles are hard for a computer to solve even if it has the random bits used by the computer to generate the final puzzle -- - unlike a CAPTCHA [ 44 ]	mechanism
Our main theorem demonstrates that GOTCHAs can be used to mitigate the threat of offline dictionary attacks against passwords by ensuring that a password cracker must receive constant feedback from a human being while mounting an attack Finally	mechanism
we provide a candidate construction of GOTCHAs based on Inkblot images	mechanism
Our construction relies on the usability assumption that users can recognize the phrases that they originally used to describe each Inkblot image -- - a much weaker usability assumption than previous password systems based on Inkblots which required users to recall their phrase exactly	mechanism
We conduct a user study to evaluate the usability of our GOTCHA construction	method
We also generate a GOTCHA challenge where we encourage artificial intelligence and security researchers to try to crack several passwords protected with our scheme	method
Viral videos that gain popularity through the process of Internet sharing are having a profound impact on society The application of our work is not only important for advertising agencies to plan advertising campaigns and estimate costs	background
but also for companies to be able to quickly respond to rivals in viral marketing campaigns The proposed method is unique in that it is the first attempt to incorporate video metadata into the peak day prediction	background
we discover some interesting characteristics of viral videos	finding
The empirical results demonstrate that the proposed method outperforms the state-of-the-art methods	finding
with statistically significant differences	finding
Based on our analysis	mechanism
in the second half of the paper	mechanism
we propose a model	mechanism
We collect by far the largest open benchmark for viral video study called CMU Viral Video Dataset	method
and share it with researchers from both academia and industry	method
Having verified existing observations on the dataset	method
Space-Time Adaptive Processing ( STAP ) is a technique for processing signals from multiple antenna elements over multiple time periods for target detection	background
As STAP algorithms are typical run on airborne platforms	background
they need to be both high performance and energy-efficient Due to the high rate of processing required	background
many existing algorithms focus on reducing the dimensionality of the data	background
or exploiting structure in the underlying mathematical formulation in order to reduce the total number of floating-point operations ( FLOPs )	background
and consequently the time for computation	background
We show that more than 11x improvement in time	finding
and 77x improvement in energy efficiency can be expected when a 3D stack is used together with memory-side accelerators to target the memory-bounded operations within STAP	finding
In this paper using a 3D stacked Logic-in-Memory system The imminent arrival of 3D stacked memory makes avail high memory bandwidth	mechanism
which opens up a new and orthogonal dimension for optimizing STAP algorithms	mechanism
For deeply scaled digital integrated systems	background
the power required for transporting data between memory and logic can exceed the power needed for computation	background
thereby limiting the efficacy of synthesizing logic and compiling memory independently Logic-in-Memory ( LiM ) architectures address this challenge by embedding logic within the memory block to perform basic operations on data locally for specific functions	background
results shown in this paper demonstrate a 250x performance improvement and 310x energy savings for a data-intensive application example	finding
In this paper we present a tool and design methodology for LiM physical synthesis The resulting layouts and timing models can be incorporated within any physical synthesis tool	mechanism
Silicon	method
We consider an adaptive cruise control system in which based on position and velocity information received from other vehicles via V2V wireless communication If the vehicles follow each other at a close distance	mechanism
they have better wireless reception but collisions may occur when a follower car does not receive notice about the decelerations of the leader car fast enough to react before it is too late	mechanism
If the vehicles are farther apart	mechanism
they would have a bigger safety margin	mechanism
but the wireless communication drops out more often	mechanism
so that the follower car no longer receives what the leader car is doing	mechanism
In order to guarantee safety	mechanism
such a system must return control to the driver if it does not receive an update from a nearby vehicle within some timeout period	mechanism
The value of this timeout parameter encodes a tradeoff between the likelihood that an update is received and the maximum safe acceleration Combining formal verification techniques for hybrid systems with a wireless communication model	mechanism
we analyze how the expected efficiency of a provably-safe adaptive cruise control system is affected by the value of this timeout	method
Accurate models of the cross-talk between signaling pathways and transcriptional regulatory networks within cells are essential to understand complex response programs	background
Consequently our method is widely applicable and can be used to derive accurate	background
dynamic response models in several species	background
demonstrate the predictive power of our method	finding
We present a new computational method that combines condition-specific time-series expression data with general protein interaction data These networks characterize the pathways involved in the response	mechanism
their time of activation	mechanism
and the affected genes The signaling and regulatory components of our networks are linked via a set of common transcription factors that serve as targets in the signaling network and as regulators of the transcriptional response network	mechanism
Our method correctly identifies the core signaling proteins and transcription factors of the response programs	mechanism
It further predicts the involvement of additional transcription factors and other proteins not previously implicated in the response pathways	mechanism
Our approach requires little condition-specific data : only a partial set of upstream initiators and time-series gene expression data	mechanism
which are readily available for many conditions and species	mechanism
Detailed case studies of stress responses in budding yeast We experimentally verify several of these predictions for the osmotic stress response network	method
People spend an enormous amount of time searching for complex information online ; for example	background
consumers researching new purchases or patients learning about their conditions	background
we show that having access to others ' schemas while foraging for information helps new users to induce more useful	finding
prototypical and better-structured schemas than gathering information alone	finding
In this paper we introduce a novel approach	mechanism
Through a controlled experiment	method
Multimedia event detection ( MED ) is a retrieval task with the goal of finding videos of a particular event in a large scale internet video archive	background
given example videos and text descriptions	background
show that our proposed method outperforms the state-of-the-art methods by up to 4 %	finding
In this paper	mechanism
we investigate the possibility of using the high-level face information to assist multimedia event detection	mechanism
Moreover since the labeled data in TRECVID MED dataset are limited	mechanism
we propose a semi-supervised kernel ridge regression which works well in practice to explore the useful information from unlabeled data to assist the event detection	mechanism
Extensive experimental results on TRECVID MED dataset	method
Current research is interested in identifying how topology impacts epidemics in networks	background
We show that for k-regular graph topologies	finding
the most probable network state transitions from the state where everyone is healthy to one where everyone is infected at a threshold that depends on k but not on the size of the graph	finding
In this paper	mechanism
we model SIS ( susceptible-infected-susceptible ) epidemics as a continuous-time Markov process and for which Such distribution describes the long-run behavior of the epidemics	mechanism
The adjacency matrix of the network topology is reflected explicitly in the formulation of the equilibrium distribution	mechanism
Secondly we are interested in analyzing the model in the regime where the topology dependent infection process opposes the topology independent healing process	method
Specifically how will network topology affect the most probable long-run network state ?	method
Provision of training data sets is one of the core requirements for event-based supervised NILM ( Non-Intrusive Load Monitoring ) algorithms	background
Due to diversity in appliances ' technologies	background
in-situ training by users is often required	background
This process might require continuous user-interaction to ensure that a high quality training data set is provided	background
The algorithm performance in accurate partitioning of the feature space and the effect of different feature extraction techniques were presented and discussed	finding
In this study	mechanism
a heuristic unsupervised clustering algorithm is presented and evaluated The algorithm is based on hierarchical clustering and uses the characteristics of a cluster binary tree to determine the distance threshold for pruning the tree without a priori information	mechanism
The algorithm determines the partition of a feature space recursively to account for multi-scale nature of the binary cluster tree	mechanism
Evaluation of the algorithm was carried out using metrics for accuracy and cluster quality ( proposed in this study ) on a fully labeled data set that was collected and processed in a real residential setting	method
demonstrate the effectiveness and intuitiveness of our discovered patterns	finding
We propose TSum	mechanism
a method ordered by their `` representativeness It can decide both which these patterns are	mechanism
as well as how many are necessary to properly summarize the data	mechanism
Our main contribution is formulating a general framework	mechanism
TSum using compression principles	mechanism
TSum can easily accommodate different optimization strategies for selecting and refining patterns	mechanism
The discovered patterns can be used to both represent the data efficiently	mechanism
as well as interpret it quickly	mechanism
Extensive experiments	method
Matched field processing is a powerful tool for accurately localizing targets in dispersive media	background
However matched field processing requires a precise model of the medium under test	background
In underwater acoustics	background
where matched field processing has been extensively studied	background
authors often resort to extremely detailed numerical models of the propagation medium	background
which are computationally expensive and impractical for many applications	background
We demonstrate the effectiveness of this model The results visually illustrate our approach to significantly improve localization accuracy and reduce artifacts when compared to a conventional narrowband technique	finding
As an alternative	mechanism
this paper uses convex sparse recovery techniques directly from measured data based on its dispersion characteristics	mechanism
From this data-driven model	mechanism
the Green 's function between two points can be readily predicted	mechanism
by localizing a source in a dispersive plate medium	method
Restricted Boltzmann Machine ( RBM ) has shown great effectiveness in document modeling	background
It utilizes hidden units to discover the latent topics and can learn compact semantic representations for documents which greatly facilitate document retrieval	background
clustering and classification	background
demonstrate that with diversification	finding
the document modeling power of DRBM can be greatly improved	finding
we propose Diversified RBM ( DRBM ) which diversifies the hidden units	mechanism
to make them cover not only the dominant topics	mechanism
but also those in the long-tail region	mechanism
We define a diversity metric and use it as a regularizer to encourage the hidden units to be diverse	mechanism
Since the diversity metric is hard to optimize directly	mechanism
we instead optimize its lower bound and prove that maximizing the lower bound with projected gradient ascent can increase this diversity metric	mechanism
Experiments on document retrieval and clustering	method
Despite benefits and uses of social networking sites ( SNSs ) users are not always satisfied with their behaviors on the sites	background
Based on these results we provide insights both into how participants perceive different SNSs	background
as well as potential designs for behavior-change mechanisms to target SNS behaviors	background
While some participants want to reduce site use	finding
others want to improve their use or increase a range of behaviors	finding
These desired changes differ by SNS	finding
and for Twitter	finding
by participants ' levels of site use Participants also expect a range of benefits from these goals	finding
including increased time	finding
contact with others	finding
intrinsic benefits better security/privacy	finding
and improved self presentation	finding
We use a 604-participant online survey	method
Non-Intrusive Load Monitoring ( NILM ) has been studied for a few decades now as a method of disaggregating information about appliance level power consumption in a building from aggregate measurements of voltage and/or current obtained at a centralized location in the electrical system When such information is provided to the electricity consumer as feedback	background
they can then take the necessary steps to modify their behavior and conserve Research has shown potential for savings of up to 20 % through this kind of feedback	background
The training phase required to allow the algorithms to recognize appliances in the home at the beginning of a NILM setup is a big hindrance to wide adoption of the technique	background
One of the recent advances in this research area includes the addition of an Electro-Magnetic Field ( EMF ) sensor that measures the electric and magnetic field nearby an appliance to detect its operational state	background
This information when coupled with the aggregate power consumption data for the home	background
can help to train a NILM system	background
which is a significant step forward in automating the training phase	background
Possible reasons behind the findings are discussed and areas for further exploration are proposed	background
A vector subspace obtained using Independent Component Analysis ( ICA )	finding
along with a k-NN classifier	finding
was found to perform best among the different alternatives explored	finding
automating the training and classification process using these devices	mechanism
Various dimensionality reduction techniques are applied to the collected data	mechanism
and the resulting feature vectors are used to train a variety of common machine learning classifiers	mechanism
A case study is presented	method
where magnetic field measurements of eight appliances are analyzed to determine the viability of using these signals alone to determine the type of appliance that the EMF sensor has been placed next to	method
High-data-rate sensors such as video cameras	background
are becoming ubiquitous in the Internet of Things	background
This article is part of a special issue on smart spaces	background
This article describes GigaSight	mechanism
with strong enforcement of privacy preferences and access controls The GigaSight architecture is a federated system of VM-based cloudlets that perform video analytics at the edge of the Internet	mechanism
thus reducing the demand for ingress bandwidth into the cloud	mechanism
Denaturing which is an owner-specific reduction in fidelity of video content to preserve privacy	mechanism
is one form of analytics on cloudlets	mechanism
Content-based indexing for search is another form of cloudlet-based analytics	mechanism
Many vision tasks require a multi-class classifier to discriminate multiple categories	background
on the order of hundreds or thousands	background
Empirical results demonstrate the effectiveness of our proposed approach	finding
In this paper	mechanism
we propose sparse output coding	mechanism
by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem	mechanism
Specifically sparse output coding is composed of two steps : efficient coding matrix learning with scalability to thousands of classes	mechanism
and probabilistic decoding	mechanism
on object recognition and scene classification	method
Computers have the potential to significantly extend the practice of popular music based on steady tempo and mostly determined form	background
We describe an approach to synchronization across media that We also address with repeats and other structures to an actual performance	mechanism
which can involve both flattening the score and rearranging it	mechanism
as is common in popular music	mechanism
Finally we illustrate the possibilities of the score as a bidirectional user interface in a real-time system for music performance	mechanism
allowing the user to direct the computer through a digitally displayed score	mechanism
and allowing the computer to indicate score position back to human performers	mechanism
Noisy high-dimensional time series observations can often be described by a set of low-dimensional latent variables	background
Commonly used methods to extract these latent variables typically assume instantaneous relationships between the latent and observed variables	background
In many physical systems	background
changes in the latent variables manifest as changes in the observed variables after time delays	background
demonstrate that when time delays are present	finding
TD-GPFA is able to correctly identify these delays and recover the latent space	finding
TD-GPFA is able to better describe the neural activity using a more parsimonious latent space than GPFA	finding
a method that has been used to interpret motor cortex data but does not account for time delays	finding
In this work	mechanism
we introduce a novel probabilistic technique	mechanism
time-delay gaussian-process factor analysis TD-GPFA in the presence of a different time delay between each pair of latent and observed variables We demonstrate how using a gaussian process to model the evolution of each latent variable allows us to tractably learn these delays over a continuous domain	mechanism
Additionally we show how TD-GPFA combines temporal smoothing and dimensionality reduction into a common probabilistic framework	mechanism
We present an expectation/conditional maximization either ECME algorithm to learn the model parameters More broadly	mechanism
TD-GPFA can help to unravel the mechanisms underlying high-dimensional time series data by taking into account physical delays in the system	mechanism
Our simulations We then applied TD-GPFA to the activity of tens of neurons recorded simultaneously in the macaque motor cortex during a reaching task	method
There has been significant interest and progress recently in algorithms that solve regression problems involving tall and thin matrices in input sparsity time	background
Our results build upon the close connection between randomized matrix algorithms	background
iterative methods and graph sparsification	background
We show these iterative methods can be adapted Our approaches are based on computing the importances of the rows	mechanism
known as leverage scores	mechanism
in an iterative manner We show that alternating between computing a short matrix estimate and finding more accurate approximate leverage scores leads to a series of geometrically smaller instances	mechanism
This gives an algorithm whose runtime is input sparsity plus an overhead comparable to the cost of solving a regression problem on the smaller approximation	mechanism
Abstract Genes are often combinatorially regulated by multiple transcription factors ( TFs )	background
Such combinatorial regulation plays an important role in development and facilitates the ability of cells to respond to different stresses	background
we show that the predicted combinatorial sets agree with other high throughput genomic datasets and improve upon prior methods	finding
Here we present cDREM	mechanism
a new method cDREM integrates time series gene expression data with ( static ) protein interaction data	mechanism
The method is based on a hidden Markov model and utilizes the sparse group Lasso to identify small subsets of combinatorially active TFs	mechanism
their time of activation	mechanism
and the logical function they implement	mechanism
We tested cDREM on yeast and human data sets Using yeast	method
Finally we design a dynamic attack detector	mechanism
We assume the attack detector has access to a linear function of the initial system state that can not be altered by an attacker	method
First we provide a necessary and sufficient condition for an attack to be undetectable by any dynamic attack detector under each specific side information pattern	method
Second we characterize attacks that can be sustained for arbitrarily long periods without being detected	method
Third we define the zero state inducing attack	method
the only type of attack that remains dynamically undetectable regardless of the side initial state information available to the attack detector	method
Several researchers proposed using non-Euclidean metrics on point sets in Euclidean space for clustering noisy data	background
Almost always a distance function is desired that recognizes the closeness of the points in the same cluster	background
even if the Euclidean cluster diameter is large	background
which we call the nearest neighbor metric	mechanism
Given a point set P and a path	mechanism
t his metric is the integral of the distance to P along	mechanism
W e describe a ( 3 +e ) - approximation algorithm and a more intricate ( 1 + e ) -approximation algorithm to compute the nearest neighbor metric Both approximation algorithms work in near-linear time	mechanism
The former uses shortest paths on a sparse graph defined over the input points	mechanism
The latter uses a sparse sample of the ambient space	mechanism
to find good approximate geodesic paths	mechanism
We present an algorithm that The algorithm runs in time $ \tilde { O } ( ( m \log { n } + n\log^2 { n } ) \log ( 1/p ) )	mechanism
$ As a result	mechanism
we obtain an algorithm that on input of an $ n\times n $ symmetric diagonally dominant matrix $ A $ with $ m $ nonzero entries and a vector $ b $ computes a vector $ { x } $ satisfying $ || { x } -A^ { + } b||_A < \epsilon ||A^ { + } b||_A $	mechanism
in expected time $ \tilde { O } ( m\log^2 { n } \log ( 1/\epsilon ) )	mechanism
$ The solver is based on repeated applications of the incremental sparsifier that produces a chain of graphs which is then used as input to the recursive preconditioned Chebyshev iteration	mechanism
Linear subspace learning methods such as Fisher 's Linear Discriminant Analysis ( LDA )	background
Unsupervised Discriminant Projection ( UDP )	background
and Locality Preserving Projections ( LPP ) have been widely used in face recognition applications as a tool to capture low dimensional discriminant information However	background
when these methods are applied in the context of face recognition	background
they often encounter the small-sample-size problem	background
In order to overcome this problem	background
a separate Principal Component Analysis ( PCA ) step is usually adopted to reduce the dimensionality of the data	background
that our proposed FKDA significantly outperforms traditional linear discriminant subspace learning methods as well as five other competing algorithms	finding
In this work	mechanism
we propose a new idea which we named Multi-class Fukunaga Koontz Discriminant Analysis ( FKDA ) by incorporating the Fukunaga Koontz Transform within the optimization In contrast to traditional LDA	mechanism
UDP and LPP	mechanism
our approach can work with very high dimensional data as input	mechanism
without requiring a separate dimensionality reduction step to make the scatter matrices full rank In addition	mechanism
the FKDA formulation seeks optimal projection direction vectors that are orthogonal which the existing methods can not guarantee	mechanism
and it has the capability of finding the exact solutions to the `` trace ratio '' objective in discriminant analysis problems while traditional methods can only deal with a relaxed and inexact `` ratio trace '' objective	mechanism
HighlightsSolve small-sample-size problem in LDA	mechanism
UDP LPP using FKT formulation	mechanism
Can work with high dimensional data without inverting any scatter matrices	mechanism
Finds optimal projection direction vectors that are orthogonal	mechanism
Finds exact solutions to the objective in the form of trace ratio	mechanism
Improvement in unconstrained face recognition scenarios	mechanism
We have shown using six face database	method
in the context of large scale unconstrained face recognition	method
face recognition with occlusions	method
and illumination invariant face recognition	method
under `` closed set ''	method
`` semi-open set ''	method
and `` open set '' recognition scenarios	method
Online consumers are uncertain about subjective product quality e	background
g	background
fit and feel of clothing and texture of materials because of the absence of experiential information	background
This implies that online consumers are reluctant to buy expensive products with only digitally transferred information	background
whereas they tend to purchase more of the cheaper products online along with their accumulated online shopping experience	background
Our study on the dynamics in the set of products purchased online expands the understanding of consumer purchase behavior under uncertainty	background
we find that consumers purchase products with a high degree of product uncertainty as their online shopping experiences help them better estimate product quality	finding
Our results also show that the average and highest prices of market baskets decrease around 1 % when online shopping experience increases 10 %	finding
When online consumers buy products priced under $ 50	finding
they readily buy products with a high degree of product uncertainty regardless of their online shopping experience	finding
But consumers are unlikely to buy expensive products online if there is a high degree of product uncertainty	finding
even when they have accumulated much online shopping experience	finding
In addition we find that online vendors can effectively overcome product-level uncertainty by taking advantage of retailer reputation in the physical world and through the use of digitized video commercials This paper was accepted by Lorin Hitt	finding
information systems	finding
Using individual-level transaction data We also verify the interaction effects of product uncertainty and product price on online consumers ' purchase decision	method
the systems and methods determine that resources associated with an execution client performing symbolic execution of a target program are below	finding
at or above a threshold performance level	finding
generate checkpoints for active executing paths of the online symbolic execution	finding
and cause the execution client to perform symbolic execution in response to the determination that the resources are at or above the threshold performance level	finding
Systems and methods are described	mechanism
In some example embodiments	method
reveal the bottlenecks for video upload	finding
denaturing indexing and content-based search	finding
They also provide insight on how parameters such as frame rate and resolution impact scalability	finding
We propose a scalable Internet system from devices such as Google Glass	mechanism
Our hybrid cloud architecture	mechanism
GigaSight is effectively a Content Delivery Network ( CDN ) in reverse It achieves scalability by decentralizing the collection infrastructure using cloudlets based on virtual machines~ ( VMs )	mechanism
Based on time	mechanism
location and content	mechanism
privacy sensitive information is automatically removed from the video	mechanism
This process which we refer to as denaturing	mechanism
is executed in a user-specific VM on the cloudlet Users can perform content-based searches on the total catalog of denatured videos	mechanism
Our experiments	method
Self-paced learning ( SPL ) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training	background
We demonstrate that our method significantly outperforms the conventional SPL Specifically	finding
SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets	finding
we propose an approach called self-paced learning with diversity ( SPLD ) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective	mechanism
and thus can be easily generalized into various learning tasks	mechanism
Albeit non-convex the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time	mechanism
on three real-world datasets	method
Suspicious graph patterns show up in many applications	background
from Twitter users who buy fake followers	background
manipulating the social network	background
to botnet members performing distributed denial of service attacks	background
disturbing the network traffic graph	background
CatchSync consistently outperforms existing competitors	finding
both in detection accuracy by 36 % on Twitter and 20 % on Tencent Weibo	finding
as well as in speed	finding
We propose a fast and effective method	mechanism
CatchSync which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior pattern	mechanism
because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority	mechanism
We introduce novel measures and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots Thanks to careful design	mechanism
CatchSync has the following desirable properties : ( a ) it is scalable to large datasets	mechanism
being linear on the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology	mechanism
without needing labeled data	mechanism
nor timing information	mechanism
etc	mechanism
while still capable of using side information	mechanism
if available	mechanism
We applied CatchSync on two large	method
real datasets 1-billion-edge Twitter social graph and 3-billion-edge Tencent Weibo social graph	method
and several synthetic ones	method
Regularization has played a key role in deriving sensible estimators in high dimensional statistical inference	background
A substantial amount of recent works has argued for nonconvex regularizers in favor of their superior theoretical properties and excellent practical performances	background
In a dierent but analogous vein	background
nonconvex loss functions are promoted because of their robustness against \outliers ''	background
prove its nice convergence properties	finding
and illustrate its eectiveness demonstrate that our method compares favorably against other alternatives	finding
we propose a new proximal gradient meta-algorithm by rigorously extending the proximal average to the nonconvex setting	mechanism
We formally on two applications : multi-task graph-guided fused lasso and robust support vector machines Experiments	method
that a uniform team	finding
consisting of multiple instances of any single agent	finding
must make a significant number of mistakes	finding
whereas a diverse team converges to perfection as the number of agents grows provide evidence for the effectiveness of voting when agents are diverse	finding
With teams of computer Go agents in mind	mechanism
we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning	mechanism
This model which	mechanism
furthermore apply randomized algorithms to evaluate alternatives and produce votes ( captured by the second-stage noise models )	mechanism
We analytically demonstrate Our experiments	method
which pit teams of computer Go agents against strong agents	method
Navigation models are explicit representations of geometrical and topological information of physical environments that can be utilized for map-matching of indoor positioning data	background
This research paper presents algorithms The abovementioned navigation models have been generated in an automated fashion from Industry Foundation Classes ( IFC ) -based building information models ( BIM )	mechanism
Specifically we have 1 ) built on and targeted addressing limitations of existing algorithms that generate centerline-based network navigation models for polygonal shapes	mechanism
2 ) developed an approach for creating metric-based navigation models	mechanism
and 3 ) modified an existing algorithm using geometry and topology extracted from BIM	mechanism
The abovementioned three types of navigation models have been generated for six different testbeds with varying shape	method
size and density of spaces We have validated the generality of the developed algorithms by evaluating the accuracy of geometrical and topological information contained within the three types of navigation models generated from testbeds with varying spatial characteristics	method
Motivation : Synaptic connections underlie learning and memory in the brain and are dynamically formed and eliminated during development and in response to stimuli	background
Quantifying changes in overall density and strength of synapses is an important pre-requisite for studying connectivity and plasticity in these cases or in diseased conditions	background
Results We detected thousands of synapses in these images and Our algorithms are highly efficient and scalable and are freely available for others to use	finding
Availability : Code is available at http : //www	finding
cs	finding
cmu	finding
edu/ saketn/detect_synapses/	finding
and we developed a machine-learning framework We also used a semi-supervised algorithm that leverages unlabeled data	mechanism
we used a 50-year-old experimental technique to selectively stain for synapses in electron microscopy images	method
To validate our method	method
we experimentally imaged brain tissue of the somatosensory cortex in six mice	method
demonstrate the accuracy of our approach using crossvalidation with manually labeled data and by comparing against existing algorithms and against tools that process standard electron microscopy images	method
which demonstrate that our method scales well with large data and model sizes	finding
while beating learning strategies that fail to take both data and model partitioning into account	finding
We present a scheme Unlike algorithms that focus on distributed learning in either the big data or big model setting ( but not both )	mechanism
our scheme partitions both the data and model variables simultaneously This not only leads to faster learning on distributed clusters	mechanism
but also enables machine learning applications where both data and model are too large to fit within the memory of a single machine	mechanism
Furthermore our scheme allows worker machines to perform additional updates while waiting for slow workers to finish	mechanism
which provides users with a tunable synchronization strategy that can be set based on learning needs and cluster conditions	mechanism
We prove the correctness of such strategies	method
as well as provide bounds on the variance of the model variables under our scheme	method
Finally we present empirical results for latent space models such as topic models	method
The integration of synthetic and cell-free biology has made tremendous strides towards creating artificial cellular nanosystems using concepts from solution-based chemistry : only the concentrations of reacting species modulate gene expression rates However	background
it is known that macromolecular crowding	background
a key feature of natural cells	background
can dramatically influence biochemical kinetics by volume exclusion effects that reduce diffusion rates and enhance binding rates of macromolecules	background
our work has implications for efficient and robust control of both synthetic and natural cellular circuits	background
through integrating synthetic cellular components of biological circuits and artificial cellular nanosystems	mechanism
Phylogenetic algorithms have begun to see widespread use in cancer research to reconstruct processes of evolution in tumor progression	background
Developing reliable phylogenies for tumor data requires quantitative models of cancer evolution that include the unusual genetic mechanisms by which tumors evolve	background
such as chromosome abnormalities	background
and allow for heterogeneity between tumor types and individual patients	background
Results identifies key genomic events in disease progression consistent with prior literature	finding
lead to improved prediction accuracy for the metastasis of primary cervical cancers and for tongue cancer survival	finding
Availability and implementation : Our software ( FISHtrees ) and two datasets are available at ftp : //ftp	finding
ncbi	finding
nlm	finding
nih	finding
gov/pub/FISHtrees	finding
We propose a framework from single-cell gene copy number data	mechanism
including variable rates for different gain and loss events We propose a new algorithm We extend it via dynamic programming to include genome duplications	mechanism
We implement an expectation maximization ( EM ) -like method	mechanism
Application of our algorithms to real cervical cancer data Classification experiments on cervical and tongue cancer datasets	method
Studying temporal dynamics of topics in social media is very useful to understand online user behaviors	background
Most of the existing work on this subject usually monitors the global trends	background
ignoring variation among communities Since users from different communities tend to have varying tastes and interests	background
capturing communitylevel temporal change can improve the understanding and management of social content	background
Additionally it can further facilitate the applications such as community discovery	background
temporal prediction and online marketing	background
and demonstrate the superiority of proposed model on tasks of time stamp prediction	finding
link prediction and topic perplexity	finding
In this paper	mechanism
we take a unified solution towards the communitylevel topic dynamic extraction	mechanism
A probabilistic model	mechanism
CosTot ( Community Specific Topics-over-Time ) is proposed to uncover the hidden topics and communities	mechanism
as well as capture community-specific temporal dynamics	mechanism
Specifically CosTot considers text	mechanism
time and network information simultaneously	mechanism
and well discovers the interactions between community and topic over time We then discuss the approximate inference implementation to enable scalable computation of model parameters	mechanism
especially for large social data	mechanism
Based on this	method
the application layer support for multi-scale temporal analysis and community exploration is also investigated	method
We conduct extensive experimental studies on a large real microblog dataset	method
The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values	background
In this paper	mechanism
we consider a feature-wise kernelized Lasso We first show that	mechanism
with particular choices of kernel functions	mechanism
non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures such as the Hilbert-Schmidt independence criterion ( HSIC ) We then show that the globally optimal solution can be efficiently computed ; this makes the approach scalable to high-dimensional problems	mechanism
The effectiveness of the proposed method is demonstrated through feature selection experiments for classification and regression with thousands of features	method
We also discuss potential future research directions in this area	background
demonstrate the proposed approach is effective for human activity analysis	finding
In this paper	mechanism
we focus on automatic human activities analysis in video surveillance recorded in complicated environments at a nursing home	mechanism
This will enable the automatic exploration of the statistical patterns between patients ' daily activities and their clinical diagnosis	mechanism
Experiment	method
We present an autonomous driving research vehicle with minimal appearance modifications including smooth and comfortable trajectory generation and following ; lane keeping and lane changing ; intersection handling with or without V2I and V2V ; and pedestrian	mechanism
bicyclist and workzone detection	mechanism
Safety and reliability features include a fault-tolerant computing system ; smooth and intuitive autonomous-manual switching ; and the ability to fully disengage and power down the drive-by-wire and computing system upon E-stop	mechanism
The vehicle has been tested extensively on both a closed test field and public roads	method
Game-theoretic algorithms for physical security have made an impressive real-world impact These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game	background
where the attacker observes the defender 's strategy and best-responds	background
We design an algorithm	mechanism
by observing the attacker 's responses to randomized deployments of resources and learning his priorities In contrast to previous work	mechanism
our algorithm requires a number of queries that is polynomial in the representation of the game	mechanism
The paper explains how to use sensors as the eyes	mechanism
ears hands and feet for the cloud This paper describes the opportunities and challenges	mechanism
Tablet computers are often called upon to emulate classical pen-and-paper input	background
Our system improves upon previous approaches	finding
reducing accidental palm inputs to 0	finding
016 per pen stroke	finding
while correctly passing 98 % of stylus inputs	finding
We present a probabilistic touch filtering approach that uses the temporal evolution of touch contacts	mechanism
Most algorithmic matches in fielded kidney exchanges do not result in an actual transplant	background
We show that failure-aware kidney exchange can significantly increase the expected number of lives saved and show that this new solver scales well	finding
From the computational viewpoint	mechanism
we design a branch-and-price-based optimal clearing algorithm specifically	mechanism
( i ) in theory	method
on random graph models ; ( ii ) on real data from kidney exchange match runs between 2010 and 2012 ; ( iii ) on synthetic data generated via a model of dynamic kidney exchange on large simulated data	method
unlike prior clearing algorithms	method
Cloud offload is an important technique in mobile computing	background
VM-based cloudlets have been proposed as offload sites for the resource-intensive and latency-sensitive computations typically associated with mobile multimedia applications	background
we demonstrate a prototype system that is capable of provisioning a cloudlet with a non-trivial VM image in 10 seconds	finding
we describe just-in-time ( JIT ) provisioning of cloudlets under the control of an associated mobile device This speed is achieved through dynamic VM synthesis and a series of optimizations to aggressively reduce transfer costs and startup latency	mechanism
Using a suite of five representative mobile applications	method
The average person can skillfully manipulate a plethora of tools	background
from hammers to tweezers	background
users were able to summon a variety of virtual tools by replicating their corresponding real-world grasps	finding
We propose that touch gesture design be inspired by the manipulation of physical tools from the real world	mechanism
In this way	mechanism
we can leverage user familiarity and fluency with such tools	mechanism
With only a few minutes of training on a proof-of-concept system	method
Location sharing is a popular feature of online social networks	background
but challenges remain in the effective presentation of privacy choices to users	background
whose location sharing preferences are complex and diverse	background
One proposed approach for capturing these nuances builds on the observation that key attributes of users ' location sharing preferences can be represented by a small number of privacy profiles	background
which can provide a basis for configuring individual preferences	background
This further suggests that the provision of profiles for privacy settings must be carefully considered	background
as they can substantially alter sharing behavior	background
The results suggest that this approach can influence users to share significantly more without a substantial difference in comfort	finding
We present a study	method
The proliferation of touchscreen devices has made soft keyboards a routine part of life	background
After eight practice trials	finding
users achieved an average of 9	finding
3 words per minute	finding
with accuracy comparable to a full-sized physical keyboard This compares favorably to existing mobile text input methods	finding
In this work	mechanism
we present a soft keyboard interaction technique called ZoomBoard Our approach uses iterative zooming to enlarge otherwise impossibly tiny keys to comfortable size	mechanism
We based our design on a QWERTY layout	mechanism
so that it is immediately familiar to users and leverages existing skill	mechanism
As the ultimate test	method
we ran a text entry experiment on a keyboard measuring just 16 x 6mm - smaller than a US penny	method
PriorityMeister outperforms most recent reactive request scheduling approaches	finding
with more workloads satisfying latency SLOs at higher latency percentiles	finding
This paper describes PriorityMeister -- a system that employs a combination of per-workload priorities and rate limits	mechanism
even with bursty workloads	mechanism
PriorityMeister automatically and proactively configures workload priorities and rate limits across multiple stages ( e	mechanism
g	mechanism
a shared storage stage followed by a shared network stage ) to meet end-to-end tail latency SLOs PriorityMeister is also robust to mis-estimation of underlying storage device performance and contains the effect of misbehaving workloads	mechanism
In real system experiments and under production trace workloads	method
With an explosion of popularity of online photo sharing	background
we can trivially collect a huge number of photo streams for any interesting topics such as scuba diving as an outdoor recreational activity class	background
our empirical results show that the proposed algorithms are more successful than other candidate methods for both tasks	finding
In this paper	mechanism
we propose an approach The alignment task discovers the matched images between different photo streams	mechanism
and the image segmentation task parses each image into multiple meaningful regions to facilitate the image understanding	mechanism
We close a loop between the two tasks so that solving one task helps enhance the performance of the other in a mutually rewarding way	mechanism
To this end	mechanism
we design a scalable message-passing based optimization framework to jointly achieve both tasks for the whole input image set at	mechanism
With evaluation on the new Flickr dataset of 15 outdoor activities that consist of 1	method
5 millions of images of 13 thousands of photo streams	method
Regret-based methods have largely been favored in practice	background
in spite of their theoretically inferior convergence rates	background
we find that mirror prox and the excessive gap technique outperform the prior regret-based methods for finding medium accuracy solutions	finding
In this paper we investigate the acceleration of first-order methods both theoretically and experimentally An important component of many first-order methods is a distance-generating function	mechanism
Motivated by this	mechanism
we investigate a specific distance-generating function	mechanism
namely the dilated entropy function	mechanism
over treeplexes which are convex polytopes that encompass the strategy spaces of perfect-recall extensive-form games	mechanism
We develop significantly stronger bounds on the associated strong convexity parameter	mechanism
In terms of extensive-form game solving	mechanism
this improves the convergence rate of several first-order methods by a factor of O ( ( # information sets depth M ) / ( 2 depth ) ) where M is the maximum value of the l 1 norm over the treeplex encoding the strategy spaces	mechanism
In order to instantiate stochastic mirror prox	mechanism
we develop a class of gradient sampling schemes for game trees	mechanism
Equipped with our distance-generating function and sampling scheme	mechanism
Experimentally we investigate the performance of three first-order methods ( the excessive gap technique	method
mirror prox and stochastic mirror prox ) and compare their performance to the regret-based algorithms	method
How does a new startup drive the popularity of competing websites into oblivion like Facebook famously did to MySpace ? This question is of great interest to academics	background
technologists and financial investors alike	background
The resulting model not only accurately fits the observed Daily Active Users ( DAU ) of Facebook and its competitors but also predicts their fate four years into the future	finding
Our model provides new insights into what Nobel Laure- ate Herbert A	mechanism
Simon called the `` marketplace of attention	mechanism
'' which we recast as the attention-activity marketplace	mechanism
Our model design is further substantiated by user-level activity of 250	method
000 MySpace users obtained between 2004 and 2009	method
We present a new method the `` Non-Parametric Heterogeneous Graph Scan ( NPHGS ) ''	mechanism
NPHGS enables fast and accurate detection of emerging space-time clusters using Twitter and other social media streams where standard parametric model assumptions are incorrect	mechanism
Distributed machine learning has typically been approached from a data parallel perspective	background
where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness	background
In this paper	mechanism
we develop a system for model-parallelism	mechanism
STRADS that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs	mechanism
STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models	mechanism
and improves memory efficiency of distributed ML	mechanism
We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling	method
matrix factorization and Lasso	method
Computational cancer phylogenetics seeks to enumerate the temporal sequences of aberrations in tumor evolution	background
thereby delineating the evolution of possible tumor progression pathways	background
molecular subtypes and mechanisms of action	background
We previously developed a pipeline for constructing phylogenies describing evolution between major recurring cell types computationally inferred from whole-genome tumor profiles	background
which confirms its effectiveness for tumor phylogeny inference and suggests avenues for future advances	finding
Here we present a novel hidden Markov model ( HMM ) scheme through joint segmentation and calling of multisample tumor data Our method classifies sets of genome-wide DNA copy number measurements into a partitioning of samples into normal ( diploid ) or amplified at each probe	mechanism
It differs from other similar HMM methods in its design specifically for the needs of tumor phylogenetics	mechanism
by seeking to identify robust markers of progression conserved across a set of copy number profiles	mechanism
We show an analysis of our method in comparison to other methods on both synthetic and real tumor data	method
We present Air+Touch	mechanism
a new class of interactions that interweave touch events with in-air gestures	mechanism
We demonstrate how air and touch are highly complementary : touch is used to designate targets and segment in-air gestures	mechanism
while in-air gestures add expressivity to touch events	mechanism
For example a user can draw a circle in the air and tap to trigger a context menu	mechanism
do a finger 'high jump ' between two touches to select a region of text	mechanism
or drag and in-air 'pigtail ' to copy text to the clipboard we devised a basic taxonomy of Air+Touch interactions	mechanism
based on whether the in-air component occurs before	mechanism
between or after touches	mechanism
Through an observational study	method
To illustrate the potential of our approach	method
we built four applications that showcase seven exemplar Air+Touch interactions we created	method
Recent trends in System-on-a-Chip show that an increasing number of special-purpose processors are being added to improve the efficiency of common operations	background
Unfortunately the use of these processors may introduce suspension delays incurred by communication	background
synchronization and external I/O operations When these processors are used in real-time systems	background
conventional schedulability analyses incorporate these delays in the worst-case execution/response time	background
hence significantly reducing the schedulable utilization	background
While RMS is shown to not be optimal	finding
it can be used effectively in some special cases that we have identified	finding
show that the proposed scheme provides up to 40 times more schedulable utilization than RMS	finding
and propose segment-fixed priority scheduling We model the tasks as segments of execution separated by suspensions	mechanism
We then derive a utilization bound for the cases as a function of the ratio of the suspension duration to the period of the tasks	mechanism
For general cases	mechanism
we develop a segment-fixed priority scheduling scheme Our scheme assigns individual segments different priorities and phase offsets that are used for phase enforcement to control the unexpected self-suspending nature	mechanism
We start from providing response-time analyses for self-suspending tasks under Rate Monotonic Scheduling ( RMS ) With the exact schedulability analysis designed for our scheme	method
our experiments	method
Cloud-sourced virtual appliances ( VAs ) have been touted as powerful solutions for many software maintenance	background
mobility backward compatibility	background
and security challenges	background
supports fluid interaction even in challenging network conditions	finding
such as 4G LTE	finding
to create a VA cloud service More specifically	mechanism
we wish to support a YouTube-like streaming service for executable content	mechanism
such as games	mechanism
interactive books research artifacts	mechanism
etc	mechanism
Users should be able to post	mechanism
browse through and interact with executable content swiftly and without long interruptions Intuitively	mechanism
this seems impossible ; the bandwidths	mechanism
latencies and costs of last-mile networks would be prohibitive given the sheer sizes of virtual machines ! Yet	mechanism
we show that a set of carefully crafted	mechanism
novel prefetching and streaming techniques can bring this goal surprisingly close to reality	mechanism
We show that vTube	method
a VA streaming system that incorporates our techniques	method
improving upon past bounds with convergence rates that depend logarithmically on the data dimension	finding
and demonstrating state-of-the-art performance on two real-world tasks	finding
This paper considers the sparse Gaussian conditional random field	mechanism
a discriminative extension of sparse inverse covariance estimation	mechanism
where we use convex methods The model has been proposed by multiple researchers within the past year	mechanism
yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems In this paper	mechanism
we make three contributions : 1 ) we develop a second-order active-set method which is several orders of magnitude faster than previously proposed optimization approaches for this problem	mechanism
2 ) we analyze the model from a theoretical standpoint	method
3 ) we apply the method to large-scale energy forecasting problems	method
Previously it has been shown that	background
under some conditions on the distribution of votes	background
if the number of manipulators is o ( n )	background
where n is the number of voters	background
then the probability that a random profile is manipulable by the coalition goes to zero as the number of voters goes to infinity	background
whereas if the number of manipulators is ( n )	background
then the probability that a random profile is manipulable goes to one This result analytically validates recent empirical results	background
and suggests that deciding the coalitional manipulation problem may be of limited computational hardness in practice	background
and we show that as c goes from zero to infinity	finding
the limiting probability that a random profile is manipulable goes from zero to one in a smooth fashion	finding
i	finding
e	finding
there is a smooth phase transition between the two regimes	finding
Here we consider the critical window	mechanism
where a coalition has size cn	mechanism
Real-time captioning provides people who are deaf or hard of hearing access to speech in settings such as classrooms and live events	background
We have shown that the accuracy of Scribe captions approaches that of a professional stenographer	finding
while its latency and cost is dramatically lower	finding
We introduce Legion Scribe ( Scribe )	mechanism
a system that allows 3-5 ordinary people who can hear and type Each person is unable to type at natural speaking rates	mechanism
and so is asked only to type part of what they hear Scribe automatically stitches all of the partial captions together to form a complete caption stream	mechanism
Distributed online groups have great potential for generating interdependent and complex products like encyclopedia articles or product design	background
These results have implications for small group theory and crowdsourcing research	background
Our results indicate that	finding
contrary to prior work	finding
a sequential work structure was more effective than a simultaneous work structure as the size of the group increased	finding
suggests that social processes such as territoriality partially accounts for these results	finding
mitigated the detrimental effects of the simultaneous work structure	finding
We conducted an experiment comparing the effectiveness of two coordination strategies ( simultaneous vs	method
sequential work ) on a complex creative task as the number of group members increased	method
A mediation analysis A follow up experiment giving workers specific roles	method
In a multimillion-node network of who-follows-whom like Twitter	background
since a high count of followers leads to higher profits	background
users have the incentive to boost their in-degree	background
Moreover we show it is effective	finding
we propose CatchSync	mechanism
which exploits two tell-tale signs of the suspicious behavior : ( a ) synchronized behavior : the zombie followers have extremely similar following behavior pattern	mechanism
because say they are generated by a script ; and ( b ) abnormal behavior : their behavior pattern is very different from the majority	mechanism
Our CatchSync introduces novel measures to quantify both concepts and catches the suspicious behavior	mechanism
in a real-world social network	method
Contemporary parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce	background
or even specialized operators relying on graphical representations of ML programs	background
We demonstrate how such a design in light of ML-first principles leads to significant performance improvements versus well-known implementations of several ML programs	finding
allowing them to run in much less time and at considerably larger model sizes	finding
We propose a general-purpose framework that systematically addresses data- and model-parallel challenges in large-scale ML	mechanism
by leveraging several fundamental properties underlying ML programs that make them different from conventional operation-centric programs : error tolerance	mechanism
dynamic structure and nonuniform convergence ; all stem from the optimization-centric nature shared in ML programs ' mathematical definitions	mechanism
and the iterative-convergent behavior of their algorithmic solutions	mechanism
These properties present unique opportunities for an integrative system design	mechanism
built on bounded-latency network synchronization and dynamic load-balancing scheduling	mechanism
which is efficient	mechanism
programmable and enjoys provable correctness guarantees	mechanism
on modestly-sized computer clusters	method
Our method outperforms other well known baselines Finally	finding
we also provide a fast	finding
parallel approximation of the same	finding
Assuming the social network to be an integer-weighted graph ( where the weights can be intuitively defined as the number of common friends	mechanism
followers documents exchanged	mechanism
etc	mechanism
) we transform the social network to a more efficient representation	mechanism
In this new representation	mechanism
each user is a bag of her one-hop neighbors	mechanism
We propose a mixed-membership model to identify compact communities using this transformation	mechanism
Next we augment the representation and the model to incorporate user-content information imposing topical consistency in the communities	mechanism
In our model a user can belong to multiple communities and a community can participate in multiple topics This allows us to discover community memberships as well as community and user interests	mechanism
on two real-world social networks	method
Recently data with complex characteristics such as epilepsy electroencephalography ( EEG ) time series has emerged	background
Epilepsy EEG data has special characteristics including nonlinearity	background
nonnormality and nonperiodicity	background
results show that when compared to previous methods	finding
the proposed method can forecast faster and accurately	finding
In this paper	mechanism
we propose a coercively adjusted autoregression ( CA-AR ) method that forecasts future values from a multivariable epilepsy EEG time series	mechanism
We use the technique of random coefficients	mechanism
which forcefully adjusts the coefficients with 1 and 1	mechanism
The fractal dimension is used to determine the order of the CA-AR model	mechanism
We applied the CA-AR method reflecting special characteristics of data to forecast the future value of epilepsy EEG data	mechanism
Experimental	method
With the continuous improvement in genotyping and molecular phenotyping technology and the decreasing typing cost	background
it is expected that in a few years	background
more and more clinical studies of complex diseases will recruit thousands of individuals for pan-omic genetic association analyses	background
and report some interesting findings	finding
We present GenAMap	mechanism
an interactive analytics software platform that 1 ) automates the execution of principled machine learning methods that detect genome- and phenome-wide associations among genotypes	mechanism
gene expression data	mechanism
and clinical or other macroscopic traits	mechanism
and 2 ) provides new visualization tools specifically designed to aid in the exploration of association mapping results Algorithmically	mechanism
GenAMap is based on a new paradigm for GWAS and PheWAS analysis	mechanism
termed structured association mapping	mechanism
which leverages various structures in the omic data GenAMap is available from http : //sailing	mechanism
cs	mechanism
cmu	mechanism
edu/genamap	mechanism
We demonstrate the function of GenAMap via a case study of the Brem and Kruglyak yeast dataset	method
and then apply it on a comprehensive eQTL analysis of the NIH heterogeneous stock mice dataset	method
Oral tongue squamous cell carcinoma ( OTSCC ) is associated with poor prognosis	background
Whats new ? Oral tongue squamous cell carcinoma ( OTSCC ) is a rare head and neck cancer that typically is asymptomatic in early stages	background
Unsupervised hierarchical clustering of the FISH data distinguished three clusters related to smoking status Copy number increases of all five markers were found to be correlated to non-smoking habits	finding
while smokers in this cohort had low-level copy number gains The patients whose tumors were modeled as progressing by a more diverse distribution of copy number changes across the four genes have poorer prognosis	finding
This is consistent with the view that multiple genetic pathways need to become deregulated in order for cancer to progress	finding
Analyses of the models showed that the more diverse the changes within the four marker genes	finding
the worse the outcome in OTSCC	finding
The markers predicted survival independent of smoking behavior and tumor stage	finding
Using the phylogenetic modeling software FISHtrees	mechanism
we constructed models of tumor progression for each patient based on the four gene probes	mechanism
Then we derived test statistics on the models that are significant predictors of disease-free and overall survival	mechanism
independent of tumor stage and smoking status in multivariate analysis	mechanism
Here using four fluorescence in situ hybridization ( FISH ) gene probes and the software FISHtrees	mechanism
phylogenetic tree models of tumor progression in OTSCC patients were constructed	mechanism
we analyzed four gene probes ( TERC	method
CCND1 EGFR and TP53 ) and the centromere probe CEP4 as a marker of chromosomal instability	method
using fluorescence in situ hybridization ( FISH ) in single cells from the tumors of sixty-five OTSCC patients ( Stage I	method
n=15 ; Stage II	method
n=30 ; Stage III	method
n=7 ; Stage IV	method
n=13 )	method
and derive some recommendations	background
Smartphone users are often unaware of the data collected by apps running on their devices	background
participants reassessing their permissions	finding
and 58 % of them further restricting some of their permissions We discuss how participants interacted both with the permission manager and the privacy nudges	finding
analyze the effectiveness of both solutions	finding
Our study provides both qualitative and quantitative evidence that these approaches are complementary and can each play a significant role in empowering users to more effectively control their privacy	finding
For instance even after a week with access to the permission manager	finding
participants benefited from nudges showing them how often some of their sensitive data was being accessed by apps	finding
with 95 % of	finding
that evaluates the benefits of giving users an app permission manager and sending them nudges	method
We report surprising patterns	finding
the most striking of which are : ( a ) the FIZZLE pattern	finding
i	finding
e	finding
excitement about Polly shows a power-law decay over time with ex- ponent of -1	finding
2 ; ( b ) the RENDEZVOUS pattern	finding
that obeys a power law ( we explain RENDEZVOUS in the text ) ; ( c ) the DISPERSION pattern	finding
we find that the more a person uses Polly	finding
the fewer friends he will use it with	finding
but in a reciprocal fashion	finding
Finally we also propose a generator of influence networks	mechanism
using data from the Polly telephone-based application	method
a large influence network of 72	method
000 people with about 173	method
000 in- teractions	method
spanning 500MB of log data and 200 GB of audio data	method
Inadequate information interoperability in facility management ( FM ) activities costs time and money being wasted for searching for the needed information in many different data sources	background
An integrated building information model ( BIM )	background
depicting as-is conditions	background
has a high potential to minimize such wastes	background
However facility managers still face the challenge of generating as-is building information models for existing facilities	background
The comparative analysis results reveal several information gaps among different sources	finding
The research described in this paper targets by leveraging heterogeneous existing information sources	mechanism
such as drawings and operation and maintenance manuals	mechanism
This approach was investigated through detailed case studies done in two old academic buildings	method
Existing information obtained from documents has been compared to Industry Foundation Classes ( IFC )	method
COBie and the data generated from a BIM authoring system	method
For correctly specified latent tree graphical models	finding
we show that a global optimum of the optimization problem can be obtained via a recursive decomposition algorithm	finding
This algorithm recovers previous spectral algorithms for hidden Markov models ( Hsu et al	finding
2009 ; Foster et al	finding
2012 ) and latent tree graphical models ( Parikh et al	finding
2011 ; Song et al	finding
2011 ) as special cases	finding
elucidating the global objective these algorithms are optimizing	finding
this new estimator significantly improves over the state-of-the-art	finding
In this new view	mechanism
the marginal probability table of the observed variables is treated as a tensor	mechanism
and we show that : ( i ) the latent variables induce low rank structures in various matricizations of the tensor ; ( ii ) this collection of low rank matricizations induces a hierarchical low rank decomposition of the tensor	mechanism
We further derive an optimization problem for estimating ( alternative ) parameters of a latent tree graphical model	mechanism
allowing us to represent the marginal probability table of the observed variables in a compact and robust way	mechanism
The optimization problem aims to find the best hierarchical low rank approximation of a tensor in Frobenius norm	mechanism
For misspecified latent tree graphical models	mechanism
we derive a novel decomposition based on our framework	mechanism
and provide approximation guarantee and computational complexity analysis	mechanism
In both synthetic and real world data	method
Much work in optimal control and inverse control has assumed that the controller has perfect knowledge of plant dynamics	background
Due to sensory feedback delay	background
the subject uses an internal model to generate an internal prediction of the current plant state	background
which may differ from the actual plant state	background
We discovered that the subject 's internal model deviated from the true BMI plant dynamics and provided significantly better explanation of the recorded neural control signals than did the true plant dynamics	finding
We develop a probabilistic framework and exact EM algorithm	mechanism
We applied this framework to demonstrations by a nonhuman primate of brain-machine interface ( BMI ) control	method
We describe the architecture and prototype implementation of an assistive system based on Google Glass devices It combines the first-person image capture and sensing capabilities of Glass with remote processing to perform real-time scene interpretation	mechanism
The system architecture is multi-tiered It offers tight end-to-end latency bounds on compute-intensive operations	mechanism
while addressing concerns such as limited battery capacity and limited processing capability of wearable devices The system gracefully degrades services in the face of network failures and unavailability of distant architectural tiers	mechanism
Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown	background
We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods	finding
In this paper	mechanism
we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us	mechanism
Nowadays facility management ( FM ) teams are facing challenges to generate accurate and semantically-rich as-is BIMs for existing buildings	background
In order to address these challenges	background
formalized approaches are required to support conflict resolution	background
data extraction and integration	background
The initial findings from the case study highlighted two main challenges associated with model generation from existing data sources : information extraction and integration Existing information for different components is typically stored in heterogeneous data sources with various formats and quality	finding
and hence requires different approaches to extract information	finding
The findings also showed that almost 40 % of the component attributes investigated had conflicting values in existing sources	finding
that aimed at leveraging existing data sources ( e	method
g	method
archived documents and data in FM systems ) to generate accurate and semanticallyrich as-is BIMs	method
Formal verification and validation play a crucial role in making cyber-physical systems ( CPS ) safe	background
Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained	background
including models of the controller and of the physical dynamics	background
In CPS models are essential ; but any model we could possibly build necessarily deviates from the real world	background
Overall ModelPlex generates provably correct monitor conditions that	finding
are provably guaranteed to imply that the offline safety verification results about the CPS model apply to the present run of the actual CPS implementation	finding
This article introduces ModelPlex	mechanism
a method ModelPlex provides correctness guarantees for CPS executions at runtime : it combines offline verification of CPS models with runtime validation of system executions for compliance with the model	mechanism
ModelPlex ensures in a provably correct way that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model If	mechanism
at some point	mechanism
the observed behavior no longer complies with the model so that offline verification results no longer apply	mechanism
ModelPlex initiates provably safe fallback actions	mechanism
assuming the system dynamics deviation is bounded This article	mechanism
furthermore develops a systematic technique by a correct-by-construction approach	mechanism
leading to verifiably correct runtime model validation	mechanism
if checked to hold at runtime	method
Demand response has gained significant attention in recent years as it demonstrates potentials to enhance the power system 's operational flexibility in a cost-effective way	background
Industrial loads such as steel manufacturing plants consume large amounts of electric energy	background
and their electricity bills account for a remarkable percentage of their total operation cost	background
Meanwhile lots of industrial loads are very flexible in terms of adjusting their power consumption rate	background
e	background
g	background
through switching the transformer tap position	background
In this paper	mechanism
we focus on the steel plant and optimize its scheduling from both the energy and the spinning reserve markets	mechanism
Black-box mutational fuzzing is a simple yet effective technique to find bugs in software	background
show that one of our new scheduling algorithms outperforms the multi-armed bandit algorithm in the current version of the CERT Basic Fuzzing Framework ( BFF ) by finding 1	finding
5x more unique bugs in the same amount of time	finding
We develop an analytic framework using a mathematical model of black-box mutational fuzzing	mechanism
and use it to evaluate 26 existing and new randomized online scheduling algorithms	method
Our experiments	method
Much research on human action recognition has been oriented toward the performance gain on lab-collected datasets	background
with promising results	finding
The paucity of labeled real-world videos motivates us to `` borrow '' strength from other resources	mechanism
Specifically considering that many lab datasets are available	mechanism
we propose to harness lab datasets given that the lab and real-world datasets are related As their action categories are usually inconsistent	mechanism
we design a multi-task learning framework to jointly optimize the classifiers for both sides	mechanism
The general Schatten $ $ p $ $ p -norm is exerted on the two classifiers to explore the shared knowledge between them	mechanism
In this way	mechanism
our framework is able to mine the shared knowledge between two datasets even if the two have different action categories	mechanism
which is a major virtue of our method	mechanism
The shared knowledge is further used to improve the action recognition in the real-world videos	mechanism
Extensive experiments are performed on real-world datasets	method
The costs are convex	background
have Lipschitz continuous gradient ( with constant L )	background
and bounded gradient	background
achieves rates O ( logK/K ) and O ( logk/k )	finding
It achieves rates O ( 1/ K 2- ) and O ( 1/k 2 ) ( > 0 arbitrarily small )	finding
examples illustrate our findings	finding
We propose two fast distributed gradient algorithms based on the centralized Nesterov gradient algorithm and establish their convergence rates in terms of the per-node communications K and the per-node gradient evaluations k	mechanism
Our first method	mechanism
Distributed Nesterov Gradient	mechanism
Our second method	mechanism
Distributed Nesterov gradient with Consensus iterations	mechanism
assumes at all nodes knowledge of L and ( W ) - the second largest singular value of the N N doubly stochastic weight matrix W	mechanism
Further we give for both methods explicit dependence of the convergence constants on N and W	method
Simulation	method
In spite of many favorable characteristics of guided-waves for Nondestructive Evaluation ( NDE ) of pipes	background
real-world application of these systems is still quite limited	background
It is observed that masking effects of flow rate for damage detection can be at least as significant as temperature effects	finding
and that such effects become more dominant when flow rate and temperature variations co-occur	finding
We first provide a review of the studies to date in the field of guided-wave based testing to identify research gaps for enhancing the application of these systems in pipeline NDE To study the identified gaps	method
guided-wave data from a fully operational piping system	method
with continuously varying flow rate and temperature	method
is used Time-shift and amplitude drift effects due to flow rate variations are evaluated along with those of temperature	method
Given a graph with billions of nodes and edges	background
how can we find patterns and anomalies ? Are there nodes that participate in too many or too few triangles ? Are there close-knit near-cliques ? These questions are expensive to answer unless we have the first several eigenvalues and eigenvectors of the graph adjacency matrix	background
We report important discoveries about nearcliques and triangles on several real-world graphs	finding
including a snapshot of the Twitter social network ( 56 Gb	finding
2 billion edges ) and the YahooWeb data set	finding
one of the largest publicly available graphs ( 120 Gb	finding
1	finding
4 billion nodes	finding
6	finding
6 billion edges )	finding
with the proposed HEIGEN algorithm	mechanism
which we carefully design to be accurate	mechanism
efficient and able to run on the highly scalable MAPREDUCE ( HADOOP ) environment	mechanism
This enables HEIGEN to handle matrices more than 1 ; 000 larger than those which can be analyzed by existing algorithms	mechanism
We implement HEIGEN and run it on the M45 cluster	method
one of the top 50 supercomputers in the world	method
results show that the logic-in-memory approach has the potential to enable substantial improvements in energy efficiency without sacrificing image quality	finding
In this paper we present a local interpolation-based variant of the well-known polar format algorithm used We develop the algorithm	mechanism
which off-loads lightweight computation directly into the SRAM and DRAM Our proposed algorithm performs filtering	mechanism
an image perspective transformation	mechanism
and a local 2D interpolation	mechanism
and supports partial and low-resolution reconstruction We implement our customized SAR grid interpolation logic-in-memory hardware in advanced 14 nm silicon technology	mechanism
Our high-level design tools allow to instantiate various optimized design choices to fit image processing and hardware needs of application designers	mechanism
Our simulation	method
Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets	background
We demonstrate the efficacy of our model and inference algorithm	finding
In this paper	mechanism
we present a distribution over collections of time-dependent	mechanism
infinite-dimensional trees and present an efficient and scalable algorithm	mechanism
on both synthetic data and real-world document corpora	method
In modern crowdsourcing markets	background
requesters face the challenge of training and managing large transient workforces	background
Requesters can hire peer workers to review others ' work	background
but the value may be marginal	background
especially if the reviewers lack requisite knowledge	background
The results show that workers who review others ' work perform better on subsequent tasks than workers who just produce	finding
We also find that interactive reviewer teams outperform individual reviewers on all quality measures	finding
However aggregating individual reviewers into nominal groups produces better quality assessments than interactive teams	finding
except in task domains where discussion helps overcome individual misconceptions	finding
An online between-subjects experiment compares the trade-offs of reviewing versus producing work using three different organization strategies : working individually	method
working as an interactive team	method
and aggregating individuals into nominal groups	method
Understanding where electricity is being used in buildings is an important tool for Cyber-Physical Systems ( CPS ) used in building energy conservation and efficiency	background
The system is able to detect appliance state transitions with an accuracy of 95	finding
8 % and estimate the overall energy with an accuracy of 98	finding
1 %	finding
In this paper	mechanism
we present an energy measurement system using a wireless sensor network consisting of contactless electromagnetic field ( EMF ) sensors deployed near each appliance	mechanism
and a whole-house power meter	mechanism
We present the design of a battery-operated EMF sensor	mechanism
based on magnetic and electric field fluctuations	mechanism
Each detector wirelessly transmits state change events to a circuit-panel energy meter	mechanism
in a time-synchronized fashion	mechanism
so that the overall power measurements can be used to estimate appliance-level energy usage	mechanism
Our EMF sensors are able to detect significant power state changes from a few inches away	mechanism
thus making it possible to externally monitor in-wall wiring to devices	mechanism
We experimentally evaluate our proposed EMF sensor	method
three-phase power meter and communication protocol in a residential building collecting data for over a week	method
Methods for the analysis of chromatin immunoprecipitation sequencing ( ChIP-seq ) data start by aligning the short reads to a reference genome	background
indicates that our method outperforms alignment based methods that utilize closely related species	finding
Here we develop methods for de novo analysis of ChIP-seq data	mechanism
Our methods combine de novo assembly with statistical tests enabling motif discovery without the use of a reference genome	mechanism
We validate the performance of our method using human and mouse data	method
Analysis of fly data	method
Consumers who are close to one another in a social network often make similar purchase decisions This similarity can result from latent homophily or social influence	background
as well as common exogenous factors	background
Latent homophily means consumers who are connected to one another are likely to have similar characteristics and product preferences Social influence refers to the ability of one consumer to directly influence another consumer 's decision based upon their communication	background
Identification is achieved due to our dynamic	finding
panel data structure and the availability of detailed communication data	finding
We find strong influence effects and latent homophily effects in both the purchase timing and product choice decisions of consumers	finding
This paper was accepted by Sandra Slaughter	finding
information systems	finding
We present an empirical study We simultaneously measure latent homophily and social influence	method
while also accounting for exogenous factors	method
How can web services that depend on user generated content discern fraudulent input by spammers from legitimate input ? as well as potential extensions to anomaly detection problems in other domains	background
Finally we demonstrate and discuss the effectiveness of CopyCatch	finding
Our method which we refer to as CopyCatch	mechanism
by analyzing only the social graph between users and Pages and the times at which the edges in the graph ( the Likes ) were created We offer the following contributions : ( 1 ) We give a novel problem formulation	mechanism
with a simple concrete definition of suspicious behavior in terms of graph structure and edge constraints ( 2 ) We offer two algorithms - one provably-convergent iterative algorithm and one approximate	mechanism
scalable MapReduce implementation 3 ) We show that our method severely limits `` greedy attacks '' and analyze the bounds from the application of the Zarankiewicz problem to our setting CopyCatch is actively in use at Facebook	mechanism
searching for attacks on Facebook 's social graph of over a billion users	mechanism
many millions of Pages	mechanism
and billions of Page Likes	mechanism
at Facebook and on synthetic data	method
Iris masks play an important role in iris recognition They indicate which part of the iris texture map is useful and which part is occluded or contaminated by noisy image artifacts such as eyelashes	background
eyelids eyeglasses frames	background
and specular reflections	background
The accuracy of the iris mask is extremely important	background
The performance of the iris recognition system will decrease dramatically when the iris mask is inaccurate	background
even when the best recognition algorithm is used	background
results show that the masks generated by the proposed algorithm increase the iris recognition rate	finding
verifying the effectiveness and importance of our proposed method for iris occlusion estimation	finding
In this work	mechanism
we propose to use Figueiredo and Jain 's Gaussian Mixture Models ( FJ-GMMs ) We also explored possible features and found that Gabor Filter Bank ( GFB ) provides the most discriminative information for our goal Finally	mechanism
we applied Simulated Annealing ( SA ) technique to optimize the parameters of GFB in order to achieve the best recognition rate	mechanism
Experimental on both ICE2 and UBIRIS dataset	method
Given a simple noun such as { \em apple }	background
and a question such as `` is it edible ? ``	background
what processes take place in the human brain ?	background
GeBM produces brain activity patterns that are strikingly similar to the real ones	finding
and the inferred functional connectivity is able to provide neuroscientific insights towards a better understanding of the way that neurons interact with each other	finding
as well as detect regularities and outliers in multi-subject brain activity measurements	finding
In this work we present a simple	mechanism
novel good-enough brain model	mechanism
or GeBM in short	mechanism
and a novel algorithm Sparse-SysId Moreover	mechanism
GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text )	mechanism
We evaluate GeBM by using both synthetic and real brain data	method
Using the real data	method
Given a network with attributed edges	background
how can we identify anomalous behavior ? Networks with edge attributes are ubiquitous	background
and capture rich information about interactions between nodes	background
: we show that EdgeCentric successfully spots numerous such anomalies where it achieved 0	finding
87 precision over the top 100 results	finding
Our work has a number of notable contributions	mechanism
including ( a ) formulation : while most other graph-based anomaly detection works use structural graph connectivity or node information	mechanism
we focus on the new problem of leveraging edge information	mechanism
( b ) methodology : we introduce EdgeCentric	mechanism
an intuitive and scalable compression-based approach and ( c ) practicality	mechanism
in several large	method
edge-attributed real-world graphs	method
including the Flipkart e-commerce graph with over 3 million product reviews between 1	method
1 million users and 545 thousand products	method
Finally we demonstrate the algorithm 's utility	finding
In contrast to previous matrix recovery work	mechanism
we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix	mechanism
We develop a set of sufficient conditions for the recovery of a SPSD matrix from a set of its principal submatrices	mechanism
present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met	mechanism
The proposed algorithm is naturally generalized to the problem of noisy matrix recovery	mechanism
and we provide a worst-case bound on reconstruction error for this scenario	mechanism
on noiseless and noisy simulated datasets	method
This paper explores a PAC ( probably approximately correct ) learning model in cooperative games	mechanism
Specifically we are given m random samples of coalitions and their values	mechanism
taken from some unknown cooperative game ; We also establish a novel connection between PAC learnability and core stability : for games that are efficiently learnable	mechanism
it is possible to find payoff divisions that are likely to be stable using a polynomial number of samples	mechanism
We study the PAC learnability of several well-known classes of cooperative games	method
such as network flow games	method
threshold task games	method
and induced subgraph games	method
We then demonstrate that our approach can discover complementary views on the brand associations that are hardly obtained from text data	finding
show the superior performance of our algorithm for the two tasks over other candidate methods	finding
In this paper	mechanism
we study an approach by leveraging large-scale online photo collections contributed by the general public	mechanism
Brand Associations one of central concepts in marketing	mechanism
describe customers ' top-of-mind attitudes or feelings toward a brand	mechanism
( e	mechanism
g	mechanism
what comes to mind when you think of Burberry ? ) Traditionally	mechanism
brand associations are measured by analyzing the text data from consumers ' responses to the survey or their online conversation logs In this paper	mechanism
we go beyond textual media and take advantage of large-scale photos shared on the Web More specifically	mechanism
we jointly achieve the following two fundamental tasks in a mutually-rewarding way : ( i ) detecting exemplar images as key visual concepts associated with brands	mechanism
and ( ii ) localizing the regions of brand in images	mechanism
For experiments we collect about five millions of images of 48 brands crawled from five popular online photo sharing sites We also quantitatively	method
Abstract : Security-sensitive applications that execute untrusted code often check the codes integrity by comparing its syntax to a known good value or sandbox the code to contain its effects	background
We illustrate both new reasoning principles of System M	finding
System M is a new program logic System M extends Hoare Type Theory ( HTT ) First	mechanism
its type system internalizes logical equality	mechanism
facilitating reasoning about applications that check code integrity	mechanism
Second a confinement rule assigns an effect type to a computation based solely on knowledge of the computations sandbox	mechanism
We prove the sound-ness of System M relative to a step-indexed trace-based semantic model	method
by verifying the main integrity property of the design of Memoir	method
a previously proposed trusted computing system for ensuring state continuity of isolated security-sensitive applications	method
There has been recent interest in applying Stackelberg games to infrastructure security	background
in which a defender must protect targets from attack by an adaptive adversary	background
In real-world security settings the adversaries are humans and are thus boundedly rational	background
We propose a new solution concept	mechanism
monotonic maximin We propose a mixed-integer linear program formulation We also consider top-monotonic maximin	mechanism
a related solution concept that is more conservative	mechanism
and propose a polynomial-time algorithm for top-monotonic maximin	mechanism
The development of accurate clinical biomarkers has been challenging in part due to the diversity between patients and diseases	background
MSS provides a straightforward approach for modeling highly divergent subclasses of patients	background
which may be adaptable for diverse applications	background
that yielded similar classification accuracy to several other classification algorithms	finding
revealed subclasses of patients based on distinct marker states	finding
Here we present a new strategy that accounts for completely distinct patient subclasses	mechanism
Marker State Space ( MSS ) defines marker states based on all possible patterns of high and low values among a panel of markers	mechanism
Each marker state is defined as either a case state or a control state	mechanism
and a sample is classified as case or control based on the state it occupies	mechanism
MSS was used to define multi-marker panels that were robust in cross validation and training-set/test-set analyses and A three-marker panel for discriminating pancreatic cancer patients from control subjects	method
Background Serum albumin is a major pharmacokinetic effector of	background
the crystal structures of six oncology agents were determined in complex with human serum albumin at resolutions of 2	method
8 to 2	method
0 A : camptothecin	method
9-amino-camptothecin etoposide teniposide	method
bicalutamide and idarubicin	method
Multimedia data are usually represented by multiple features	background
results show that it is beneficial to combine multiple features	finding
The performance of the proposed algorithm is remarkable when only a small amount of labeled training data are available	finding
In this paper	mechanism
we propose a new algorithm	mechanism
namely Multi-feature Learning via Hierarchical Regression where two issues are considered	mechanism
First labeling large amount of training data is labor-intensive	mechanism
It is meaningful to effectively leverage unlabeled data to facilitate multimedia semantics understanding	mechanism
Second given that multimedia data can be represented by multiple features	mechanism
it is advantageous to develop an algorithm which combines evidence obtained from different features to infer reliable multimedia semantic concept classifiers	mechanism
We design a hierarchical regression model to exploit the information derived from each type of feature	mechanism
which is then collaboratively fused to obtain a multimedia semantic concept classifier	mechanism
Both label information and data distribution of different features representing multimedia data are considered	mechanism
The algorithm can be applied to a wide range of multimedia applications and	mechanism
experiments are conducted on video data for video concept annotation and action recognition	method
Using Trecvid and CareMedia video datasets	method
the experimental	method
Matrix-parametrized models including multiclass logistic regression and sparse coding	background
are used in machine learning ( ML ) applications ranging from computer vision to computational biology	background
corroborate its efficiency	finding
we propose a Sufficient Factor Broadcasting ( SFB ) computation model for efficient distributed learning of a large family of matrix-parameterized models	mechanism
which share the following property : the parameter update computed on each data sample is a rank-1 matrix	mechanism
i	mechanism
e	mechanism
the outer product of two `` sufficient factors '' ( SFs By broadcasting the SFs among worker machines and reconstructing the update matrices locally at each worker	mechanism
SFB improves communication efficiency -- - communication costs are linear in the parameter matrix 's dimensions	mechanism
rather than quadratic -- - without affecting computational correctness	mechanism
We present a theoretical convergence analysis of SFB	method
and empirically on four different matrix-parametrized ML models	method
BACKGROUND There is a need of such studies in African Americans	background
because they display a higher incidence of aggressive CRC tumors	background
This WES study in African American patients with CRC provides insight into the identification of novel somatic mutations in APC Our data suggest an association between specific mutations in the Wnt signaling pathway and an increased risk of CRC	background
The analysis of the pathogenicity of these novel variants may shed light on the aggressive nature of CRC in African Americans	background
RESULTS We identified somatic mutations in genes that are known targets in CRC such as APC	finding
BRAF KRAS and PIK3CA	finding
We detected novel alterations in the Wnt pathway gene	finding
APC within its exon 15	finding
of which mutations are highly associated with CRC CONCLUSIONS	finding
METHODS We performed whole exome sequencing ( WES ) on DNA from 12 normal/tumor pairs of African American CRC patient tissues	method
Data analysis was performed using the software package GATK ( Genome Analysis Tool Kit )	method
Normative population databases ( eg	method
1000 Genomes SNP database	method
dbSNP and HapMap ) were used for comparison	method
Variants were annotated using analysis of variance and were validated via Sanger sequencing	method
Numeric time series data has unique storage requirements and access patterns that can benefit from specialized support	background
given its importance in Big Data analyses	background
and illustrates its potential for satisfying key requirements	finding
This paper describes the support needed suggests an architecture	mechanism
We present a new algorithm that produces in any dimension with guaranteed optimal output size We also provide an approximate Delaunay graph Our algorithm runs in expected time O ( 2 O ( d ) ( n log n + m ) )	mechanism
where n is the input size	mechanism
m is the output point set size	mechanism
and d is the ambient dimension The constants only depend on the desired element quality bounds	mechanism
To gain this new efficiency	mechanism
the algorithm approximately maintains the Voronoi diagram of the current set of points by storing a superset of the Delaunay neighbors of each point By retaining quality of the Voronoi diagram and avoiding the storage of the full Voronoi diagram	mechanism
a simple exponential dependence on d is obtained in the running time Thus	mechanism
if one only wants the approximate neighbors structure of a refined Delaunay mesh conforming to a set of input points	mechanism
the algorithm will return a size 2 O ( d ) m graph in 2 O ( d ) ( n log n + m ) expected time	mechanism
If m is superlinear in n	mechanism
then we can produce a hierarchically well-spaced superset of size 2 O ( d ) n in 2 O ( d ) n log n expected time	mechanism
Distributions over matrices with exchangeable rows and infinitely many columns are useful in constructing nonparametric latent variable models	background
and can achieve better performance	finding
In this paper	mechanism
we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models Such models allow us to specify the distribution over the number of features per data point	mechanism
on data sets where the number of features is not well-modeled by the original distribution	method
Stochastic Differential Equation SDE models are used to describe the dynamics of complex systems with inherent randomness	background
The primary purpose of these models is to study rare but interesting or important behaviours	background
such as the formation of a tumour	background
we introduce a new algorithm specifically designed Our approach relies on temporal logics for specifying rare behaviours of interest	mechanism
and on the ability of bit-vector decision procedures to reason exhaustively about fixed-precision arithmetic	mechanism
We apply our algorithm to a minimal parameterised model of the cell cycle	method
and take Brownian noise into account while investigating the likelihood of irregularities in cell size and time between cell divisions	method
our method is several orders of magnitude faster	finding
with competitive or improved accuracy for latent space recovery and link prediction	finding
We propose a scalable approach With a succinct representation of networks as a bag of triangular motifs	mechanism
a parsimonious statistical model	mechanism
and an efficient stochastic variational inference algorithm	mechanism
we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours	mechanism
a setting that is out of reach for many existing methods	mechanism
When compared to the state-of-the-art probabilistic approaches	method
As renewable generation capacity in the power grid increases	background
keeping the balance between the supply and demand becomes difficult	background
This threatens the grids stability and security	background
Existing power reserve assets and regulation methodologies fail to provide the short-term responses required to keep the load and generation balanced as the amount of renewable generation increases	background
using a centralized control strategy	mechanism
to propose a strategy	mechanism
We focus on the challenges associated with simulating a realistic TCL population using the models that are proposed in the literature	method
Specifically we use data collected from residential refrigeration units operating in 214 different households	method
Fusion of multiple features can boost the performance of large-scale visual classification and detection tasks like TRECVID Multimedia Event Detection ( MED ) competition [ 1 ]	background
our approach achieves promising improvements on HMDB [ 8 ] action recognition dataset and CCV [ 5 ] video classification dataset show that our approach outperforms the state-of-the-art fusion methods for complex event detection	finding
In this paper	mechanism
we propose a novel feature fusion approach	mechanism
namely Feature Weighting via Optimal Thresholding ( FWOT ) FWOT learns the weights	mechanism
thresholding and smoothing parameters in a joint framework to combine the decision values obtained from all the individual features and the early fusion	mechanism
To the best of our knowledge	mechanism
this is the first work to consider the weight and threshold factors of fusion problem simultaneously	mechanism
Compared to state-of-the-art fusion algorithms	method
In addition experiments on two TRECVID MED 2011 collections	method
Background Aging infrastructure in the US has gained quite a bit of attention in the past decade	background
Being one type of a critical infrastructure	background
embankment dams in the US require significant investment to upgrade the deteriorated parts	background
Due to limited budgets	background
understanding the behavior of structures over time through risk assessment is essential to prioritize dams	background
During the risk assessment for embankment dams	background
engineers utilize current and historical data from the design	background
construction and operation phases of these structures The challenge is that during risk assessment	background
various engineers from different disciplines ( e	background
g	background
geotechnical hydraulics ) come together	background
and how they would like to visualize the available datasets changes based on the discipline-specific analyses they need to perform	background
Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously	background
as well as the identification of the types of neurons being recorded ( e	background
g	background
excitatory vs	background
inhibitory )	background
This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function	background
and found that gLARA provides a better description of the recordings than pCCA	finding
Rather than attempting to identify direct interactions between neurons ( where the number of interactions grows with the number of neurons squared )	mechanism
we propose to extract a smaller number of latent variables from each population Specifically	mechanism
we propose extensions to probabilistic canonical correlation analysis ( pCCA )	mechanism
and study how these latent variables interact	method
We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2	method
Our findings have both behavioral and policy implications	background
as they highlight how technologies that make individuals feel more in control over the publication of personal information may have the paradoxical and unintended consequence of eliciting their disclosure of more sensitive information	background
Our findings suggest	finding
paradoxically that more control over the publication of their private information decreases individuals privacy concerns and increases their willingness to publish sensitive information	finding
even when the probability that strangers will access and use that information stays the same or	finding
in fact increases	finding
On the other hand	finding
less control over the publication of personal information increases individuals privacy concerns and decreases their willingness to publish sensitive information	finding
even when the probability that strangers will access and use information actually decreases	finding
We designed three experiments in the form of online surveys administered to students at a North-American University	method
In all experiments we manipulated the participants control over information publication	method
but not their control over the actual access to and usage by others of the published information	method
A large number of facility management tasks relying on sensor measurements require knowledge of the context under which the readings were collected	background
However this context information ( i	background
e	background
'spatial metadata ' ) is generally recorded manually	background
a process that is error-prone and time consuming considering the number of sensors located in a building	background
Therefore as other researchers have pointed out	background
there is a need to automatically determine the location information	background
We conclude that a linear correlation ( the normalized covariance matrix ) captures the spatial relationship in most situations although it is significantly sensitive to choosing the appropriate window size	finding
We conducted analyses on three different test beds where temperature measurements from 10 sensors were collected every minute	method
We consider every possible size of data subsets within a year to explore time-windowing effects	method
We also examine how different physical distances between sensors affect the results	method
Most everyday electrical and electromechanical objects emit small amounts of electromagnetic ( EM ) noise during regular operation	background
Our studies show that discrimination between dozens of objects is feasible	background
independent of wearer	background
time and local environment	background
By modifying a small	mechanism
low-cost software-defined radio	mechanism
we can detect and classify these signals in real-time	mechanism
enabling robust on-touch object detection	mechanism
Unlike prior work	mechanism
our approach requires no instrumentation of objects or the environment ; our sensor is self-contained and can be worn unobtrusively on the body	mechanism
We call our technique EM-Sense and built a proof-of-concept smartwatch implementation	mechanism
Given a small amount of labeled training data	finding
StarScan learns appropriate penalties for both compact and elongated clusters	finding
resulting in improved detection performance	finding
We propose StarScan	mechanism
a new star-shaped scan statistic StarScan generalizes the traditional	mechanism
circular spatial scan statistic by allowing the radius of the cluster around a center location to vary continuously with the angle	mechanism
but penalizes the log-likelihood ratio score proportional to the total change in radius	mechanism
StarScan was compared with circular scan and fast subset scan on simulated respiratory outbreaks and bioterrorist anthrax attacks injected into real-world Emergency Department data	method
which collected 618 high-quality answers to questions asked over 12 days	finding
illustrating the feasibility of the approach	finding
we introduce the idea of social microvolunteering	mechanism
a type of intermediated friendsourcing in which a person can provide access to their friends as potential workers for microtasks supporting causes that they care about	mechanism
We explore this idea by creating Visual Answers	mechanism
an exemplar social microvolunteering application for Facebook that posts visual questions from people who are blind	mechanism
We present results of a survey of 350 participants on the concept of social microvolunteering	method
and a deployment of the Visual Answers application with 91 participants	method
We prove that our algorithm generates asymptotically exact samples and demonstrate its ability to parallelize burn-in and sampling in several models	finding
In this paper	mechanism
we present a parallel Markov chain Monte Carlo ( MCMC ) algorithm in which subsets of data are processed independently	mechanism
with very little communication	mechanism
First we arbitrarily partition data onto multiple machines	mechanism
Then on each machine	mechanism
any classical MCMC method ( e	mechanism
g	mechanism
Gibbs sampling ) may be used to draw samples from a posterior distribution given the data subset	mechanism
Finally the samples from each machine are combined to form samples from the full posterior	mechanism
This embarrassingly parallel algorithm allows each machine to act independently on a subset of the data ( without communication ) until the final combination stage	mechanism
empirically	method
Stochastic gradient optimization is a class of widely used algorithms for training machine learning models	background
To optimize an objective	background
it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset	background
On both problems	finding
our approach shows faster convergence and better performance than the classical approach	finding
In this paper	mechanism
we develop a general approach of using control variate Data statistics such as low-order moments ( pre-computed or estimated online ) is used to form the control variate	mechanism
We demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization	method
One is convexthe MAP estimation for logistic regression	method
and the other is non-convexstochastic variational inference for latent Dirichlet allocation	method
and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain FGSS substantially decreased run time and improved detection power for massive multivariate data sets	finding
We propose Fast Generalized Subset Scan ( FGSS )	mechanism
a new method We frame the pattern detection problem as a search over subsets of data records and attributes	mechanism
maximizing a nonparametric scan statistic over all such subsets We prove that the nonparametric scan statistics possess a novel property that allows for efficient optimization over the exponentially many subsets of the data without an exhaustive search	mechanism
enabling FGSS to scale to massive and high-dimensional data sets	mechanism
We evaluate the performance of FGSS in three real-world application domains ( customs monitoring	method
disease surveillance and network intrusion detection )	method
As compared to three other recently proposed detection algorithms	method
Paid crowd work offers remarkable opportunities for improving productivity	background
social mobility and the global economy by engaging a geographically distributed workforce to complete complex tasks on demand and at scale	background
Drawing on theory from organizational behavior and distributed computing	mechanism
as well as direct feedback from workers	mechanism
we outline a framework The framework lays out research challenges in twelve major areas : workflow	mechanism
task assignment hierarchy	mechanism
real-time response synchronous collaboration	mechanism
quality control crowds guiding AIs	mechanism
AIs guiding crowds	mechanism
platforms job design	mechanism
reputation and motivation	mechanism
We show that our congestion model is effective in modeling dynamic congestion conditions	finding
We also show that our routing algorithm generates significantly faster routes compared to standard baselines	finding
and achieves near-optimal performance We also present the results which showcases the efficacy of our approach	finding
we first propose a Gaussian Process Dynamic Congestion Model that can effectively characterize both the dynamics and the uncertainty of congestion conditions	mechanism
Our model is efficient and thus facilitates real-time adaptive routing in the face of uncertainty	mechanism
Using this congestion model	mechanism
we develop an efficient algorithm for non-myopic adaptive routing A key property of our approach is the ability to efficiently reason about the long-term value of exploration	mechanism
which enables collectively balancing the exploration/exploitation trade-off for entire fleets of vehicles	mechanism
We validate our approach based on traffic data from two large Asian cities compared to an omniscient routing algorithm	method
from a preliminary field study	method
This enables radial interactions in an area many times larger than a mobile device ; for example	background
virtual buttons that lie above	background
below and to the left and right	background
which shows that Toffee can accurately resolve the bearings of touch events ( mean error of 4	finding
3 with a laptop prototype )	finding
we have developed Toffee	mechanism
a sensing approach and onto ad hoc adjacent surfaces	mechanism
most notably tabletops This is achieved using a novel application of acoustic time differences of arrival ( TDOA ) correlation	mechanism
Previous time-of-arrival based systems have required semi-permanent instrumentation of the surface and were too large for use in mobile devices	mechanism
Our approach requires only a hard tabletop and gravity -- the latter acoustically couples mobile devices to surfaces	mechanism
We conducted an evaluation	method
that highlight its immediate feasibility and utility	finding
We present Lumitrack that uses projected structured patterns and linear optical sensors	mechanism
Each sensor unit is capable of recovering 2D location within the projection area	mechanism
while multiple sensors can be combined for up to six degree of freedom ( DOF ) tracking	mechanism
Our structured light approach is based on special patterns	mechanism
called m-sequences in which any consecutive sub-sequence of m bits is unique	mechanism
Lumitrack can utilize both digital and static projectors	mechanism
as well as scalable embedded sensing configurations	mechanism
The resulting system enables high-speed	mechanism
high precision and low-cost motion tracking for a wide range of interactive applications	mechanism
We detail the hardware	method
operation and performance characteristics of our approach	method
as well as a series of example applications	method
There are many security tools and techniques for analyzing software	background
but many of them require access to source code	background
A decompiler should focus on two properties to be used for security	background
First it should recover abstractions as much as possible to minimize the complexity that must be handled by the security analysis that follows	background
Second it should aim to recover these abstractions correctly	background
Previous work in control-flow structuring	background
an abstraction recovery problem used in decompilers	background
does not provide either of these properties	background
Our evaluation is an order of magnitude larger than previous systematic studies of endto-end decompilers	finding
We show that our decompiler outperforms the de facto industry standard decompiler Hex-Rays in correctness by 114 %	finding
and recovers 30 more control-flow structure than existing structuring algorithms in the literature	finding
We propose leveraging decompilation	mechanism
the study of recovering abstractions from compiled code	mechanism
We propose a new structuring algorithm in this paper	mechanism
We evaluate our decompiler	method
Phoenix and our new structuring algorithm	method
on a set of 107 real world programs from GNU coreutils	method
Identifying a suspect wearing a mask ( where only the suspect 's periocular region is visible ) is one of the toughest real-world challenges in biometrics that exist	background
This is an important problem faced in many law-enforcement applications on almost a daily basis	background
show that our reconstruction technique	finding
based on a modified sparsifying dictionary learning algorithm	finding
can effectively reconstruct faces which we show are actually very similar to the original ground-truth faces	finding
We show the real-world applicability of method to show that they still match competitively	finding
In this paper	mechanism
we present a practical method We propose in this paper	mechanism
an approach that will reconstruct the entire frontal face using just the periocular region Further	mechanism
our method is open set	mechanism
thus can reconstruct any face not seen in training	mechanism
We empirically by benchmarking face verification results using the reconstructed faces compared to the original faces when evaluated under a large-scale face verification protocol such as NIST 's FRGC protocol where over 256 million face matches are made	method
A device just like Harry Potter 's Marauder 's Map	background
which pinpoints the location of each person-of-interest at all times	background
provides invaluable information for analysis of surveillance videos	background
show that our algorithm performs robust localization and tracking of persons-of-interest not only in outdoor scenes	finding
but also in a complex indoor real-world nursing home environment	finding
We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem	mechanism
Given a set of person detection outputs	mechanism
our framework takes advantage of all important cues such as color	mechanism
person detection face recognition and non-background information to perform tracking	mechanism
Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints	mechanism
Nonnegative discretization is used to enforce the mutual exclusion constraint	mechanism
which guarantees a person detection output to only belong to exactly one individual	mechanism
Experiments	method
Using this method	background
optimal strategies can be designed for control resource allocation to manage risk in a business process	background
Our work contributes to the literature on both ex ante risk management-based business process design and ex post risk assessments of existing business processes and control models This research applies not only to the literature on and practice of process design and risk management but also to business decision support systems in general	background
We develop a process modeling-based methodology Our method focuses on the topological structure of a process and takes into account its effect on error propagation and risk mitigation using both expected loss and conditional value-at-risk risk measures	mechanism
An order-fulfillment process of an online pharmacy is used to illustrate the methodology	method
Design construction and operation of building heating	background
ventilation and air conditioning ( HVAC ) systems are complicated processes that generally involve several stakeholders	background
such as mechanical designers	background
control system integrators	background
commissioning agents and facilities managers	background
It is important for all these stakeholders at various phases of the project to have a thorough understanding of the system components as well as the control strategy according to the design intent of the mechanical designers	background
For example when assessing the behavior of a HVAC system during operation phase	background
it is important for facilities managers to check for the correctness of every components behavior and its control logic against the design specifications	background
Challenges such as missing information for controlled parameters as well as textual descriptions that are open to interpretations are common and result in inaccurate interpretation of the system behavior	background
This may adversely affect the overall performance of systems and lead to energy inefficiencies	background
Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets	background
It optimizes the variational objective with stochastic optimization	background
following noisy estimates of the natural gradient Operationally	background
stochastic inference iteratively subsamples from the data	background
analyzes the subsample	background
and updates parameters with a decreasing learning rate	background
Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates	finding
by developing an adaptive learning rate for stochastic variational inference	mechanism
Our method requires no tuning and is easily implemented with computations already made in the algorithm	mechanism
We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora	method
Confessions are people 's way of coming clean	background
sharing unethical acts with others	background
Although confessions are traditionally viewed as categorical one either comes clean or not people often confess to only part of their transgression Such partial confessions may seem attractive	background
because they offer an opportunity to relieve ones guilt without having to own up to the full consequences of the transgression It seems that although partial confessions seem attractive	background
they come at an emotional cost	background
we found a high frequency of partial confessions	finding
especially among people cheating to the full extent possible	finding
People found partial confessions attractive because they ( correctly ) expected partial confessions to be more believable than not confessing People failed	finding
however to anticipate the emotional costs associated with partially confessing In fact	finding
partial confessions made people feel worse than not confessing or fully confessing	finding
a finding corroborated in a laboratory setting as well as in a study assessing peoples everyday confessions	finding
Using a novel experimental design	mechanism
In this paper	method
we explored the occurrence	method
antecedents consequences and everyday prevalence of partial confessions	method
Compared to visual concepts such as actions	background
scenes and objects	background
complex event is a higher level abstraction of longer video sequences	background
For example a `` marriage proposal '' event is described by multiple objects ( e	background
g	background
ring faces )	background
scenes ( e	background
g	background
in a restaurant	background
outdoor ) and actions ( e	background
g	background
kneeling down )	background
demonstrate that our algorithm is able to utilize related exemplars adaptively	finding
and the algorithm gains good performance for complex event detection	finding
our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis	mechanism
Experiments	method
Summary : T cells are activated through interaction with antigen-presenting cells ( APCs )	background
During activation receptors and signalingintermediates accumulate in diverse spatiotemporal distributions	background
Thesedistributions control the probability of signaling interactions and thusgovern information ow through the signaling system	background
Spatiotemporal-ly resolved system-scale investigation of signaling can extract the regu-latory information thus encoded	background
allowing unique insight into thecontrol of T-cell function	background
Substantial technical challenges exist	background
andthese are briey discussed herein	background
We suggest that the regulation ofactin dynamics	finding
by controlling signaling distributions and membranetopology	finding
is an important rheostat of T-cell signaling	finding
Keywords	finding
Spatiotemporalsignaling distributions are driven by cell biologically distinct structures	mechanism
a large protein assembly at the interface center	mechanism
a large invagination	mechanism
the actin-supported interface periphery as extended by smaller indi-vidual lamella	mechanism
and a newly discovered whole-interface actin-drivenlamellum	mechanism
The more than 60 elements of T-cell activation studied todate are dynamically distributed between these structures	mechanism
generating acomplex organization of the signaling system	mechanism
Signal initiation and coresignaling prefer the interface center	mechanism
while signal amplication islocalized in the transient lamellum	mechanism
Actin dynamics control signalingdistributions through regulation of the underlying structures and drivea highly undulating T-cell/APC interface that imposes substantialconstraints on T-cell organization	mechanism
This setting was recently studied in [ 15 ]	background
where the \KernelKernel '' estimator was introduced and shown to have a polynomial rate of convergence	background
To this end	mechanism
we propose the Double-Basis estimator	mechanism
which looks in two ways : rst	mechanism
the Double-Basis estimator is shown to have a computation complexity that is independent of the number of of instances N when evaluating new predictions after training ; secondly	mechanism
the Double-Basis estimator is shown to have a fast rate of convergence for a general class of mappings f2F	mechanism
Primary motor-cortex multi-unit activity ( MUA ) and local-field potentials ( LFPs ) have both been suggested as potential control signals for brain-computer interfaces ( BCIs ) aimed at movement restoration	background
Some studies report that LFP-based decoding is comparable to spiking-based decoding	background
while others offer contradicting evidence	background
Our results suggest that a velocity and speed encoding model is most appropriate for both MUA and LFP H	background
whereas a speed only encoding model is adequate for LFP L	background
We find that in addition to previously reported directional tuning	finding
MUA also contains prominent speed tuning	finding
LFP activity in low-frequency bands ( 15-40Hz	finding
LFP L ) is primarily speed tuned	finding
and contains more speed information than both high-frequency LFP ( 100-300Hz	finding
LFP H ) and MUA	finding
LFP H contains more directional information compared to LFP L	finding
but less information when compared with MUA	finding
Here we use regression and mutual information analyses	method
Stencil computations are an integral component of applications in a number of scientific computing domains Short-vector SIMD instruction sets are ubiquitous on modern processors and can be used to significantly increase the performance of stencil computations	background
Performance increases are demonstrated for a number of stencils	finding
In this paper	mechanism
we propose a domain specific language and compiler and automates both locality and short-vector SIMD optimizations	mechanism
along with effective utilization of multi-core parallelism Loop transformations are combined with a data layout transformation	mechanism
on several modern SIMD architectures	method
We discuss how design teams can use our approach to reflect on prototypes or begin user studies within seconds	background
and how over time	background
Apparition prototypes can become fully-implemented versions of the systems they simulate	background
In this paper	mechanism
we introduce crowdsourcing techniques and tools Our Apparition system uses paid microtask crowds to make even hard-to-automate functions work immediately	mechanism
allowing more fluid prototyping of interfaces that contain interactive elements and complex behaviors As users sketch their interface and describe it aloud in natural language	mechanism
crowd workers and sketch recognition algorithms translate the input into user interface elements	mechanism
add animations and provide Wizard-of-Oz functionality	mechanism
Powering Apparition is the first self-coordinated	mechanism
real-time crowdsourcing infrastructure We anchor this infrastructure on a new	mechanism
lightweight write-locking mechanism that workers can use to signal their intentions to each other	mechanism
Our model consistently outperforms competing models	finding
We present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features	mechanism
We learn our model in an efficient online fashion that is scalable for large	mechanism
streaming data	mechanism
With five streaming datasets from two different genres economics news articles and social mediawe evaluate our model on the task of sequential language modeling	method
Embankment dams like most other civil infrastructure systems	background
are exposed to harsh and largely unpredictable environments	background
However unlike bridges	background
buildings and other structures	background
their design specifications and as-is properties are not generally known in the same level of detail due to	background
among other things	background
their age and the difficulties associated with assessing their internal structure	background
Hence making sense of measurements collected from instruments used to monitor their behavior requires sound engineering judgment and analysis	background
as well as robust statistical analysis techniques to prevent misinterpretation	background
In the United States ( US )	background
the current practice of analyzing the structural integrity of embankment dams relies primarily on manual a posteriori analysis of instrument data by engineers	background
leaving much room for improvement through the application of automated data analysis techniques	background
In our previous work	background
we presented the effectiveness of applying statistical anomaly detection techniques such as Principal Component Analysis and Robust Regression Analysis when analyzing piezometer data collected from embankment dams	background
the detection accuracy came out to be 98	finding
5 %	finding
a physics-based model of an embankment dam was developed	mechanism
By varying a hydraulic conductivity of a soil material in the model	method
corresponding detection accuracies and sensitivities of the statistical anomaly detection algorithm were evaluated When we applied our proposed anomaly detection on more realistically simulated anomalous data using the numerical model	method
It is typically expected that if a mechanism is truthful	background
then the agents would	background
indeed truthfully report their private information	background
We wish to design truthful mechanisms that are simple	mechanism
that is whose Our approach involves three steps : ( i ) specifying the structure of mechanisms	mechanism
( ii ) constructing a verification algorithm	mechanism
and ( iii ) measuring the quality of verifiably truthful mechanisms	mechanism
We demonstrate this approach using a case study : approximate mechanism design without money for facility location	method
and present results of a pilot evaluation of our initial system	finding
In this paper	mechanism
we review previous work on audio and video processing	mechanism
and define the task of topic-oriented multimedia summarization ( TOMS ) using natural language generation ( NLG ) : given a set of automatically extracted features from a video	mechanism
a TOMS system will automatically generate a paragraph of natural language	mechanism
which summarizes the important information in a video belonging to a certain topic	mechanism
and for example provides explanations for why a video was matched and retrieved	mechanism
Possible features include visual semantic concepts	mechanism
objects and actions	mechanism
environmental sounds and transcripts from automatic speech recognition ( ASR )	mechanism
We see this as a first step towards systems that will be able to discriminate visually similar	mechanism
but semantically different videos	mechanism
compare two videos and provide textual output or summarize a large number of videos at once In this paper	mechanism
we introduce our approach We extract various visual concept features	mechanism
environmental sounds and ASR transcription features from a given video	mechanism
and develop a template-based NLG system to produce a textual recounting based on the extracted features	mechanism
We also propose possible experimental designs for continuously evaluating and improving TOMS systems	method
From this comparison of Twitter and in-person regrets	background
we provide preliminary ideas for tools to help Twitter users avoid and cope with regret	background
Participants generally reported similar types of regrets in person and on Twitter	finding
In particular they often regretted messages that were critical of others However	finding
regretted messages that were cathartic/expressive or revealed too much information were reported at a higher rate for Twitter	finding
Regretted messages on Twitter also reached broader audiences	finding
In addition we found that participants who posted on Twitter became aware of	finding
and tried to repair	finding
regret more slowly than those reporting in-person regrets	finding
We present the results of an online survey of 1	method
221 Twitter users	method
either saying during in-person conversations or posting on Twitter	method
Fragments of first-order temporal logic are useful for representing many practical privacy and security policies	background
Past work has proposed two strategies for checking event trace ( audit log ) compliance with policies : online monitoring and offline audit	background
We prove the correctness of our algorithm	finding
This paper proposes a new online monitoring algorithm Our key technical insight is a new called the temporal mode check	mechanism
which determines subformulas for which such caching is feasible and those for which it is not and	mechanism
hence guides our algorithm	mechanism
and evaluate its performance over synthetic traces and realistic policies	method
Abstract Host factor protein Cyclophilin A ( CypA ) regulates HIV-1 viral infectivity through direct interactions with the viral capsid	background
by an unknown mechanism	background
CypA can either promote or inhibit viral infection	background
depending on host cell type and HIV-1 capsid ( CA ) protein sequence	background
These results suggest that CypA loop dynamics is a determining factor in HIV-1 's escape from CypA dependence	background
we demonstrate that assembled CA is dynamic	finding
particularly in loop regions exhibits unprecedented mobility on the nanosecond to microsecond timescales	finding
and the experimental NMR dipolar order parameters are in quantitative agreement with those calculated from MD trajectories Remarkably	finding
the CypA loop dynamics of wild-type CA HXB2 assembly is significantly attenuated upon CypA binding	finding
and the dynamics profiles of the A92E and G94D CypA escape mutants closely resemble that of wild-type CA assembly in complex with CypA	finding
Through the analysis of backbone 1H-15N and 1H-13C dipolar tensors and peak intensities from 3D MAS NMR spectra of wild-type and the A92E and G94D CypA escape mutants	method
The CypA loop in assembled wild-type CA from two strains	method
that guarantees the new algorithm is safe for all possible inputs	finding
We then applied QdL to guide the development of a new algorithm	mechanism
We applied quantified differential-dynamic logic ( QdL ) We identified problems with the algorithm	method
proved that it was in general unsafe	method
and described exactly what could go wrong	method
Using \KeYmaeraD ( a tool that mechanizes QdL )	method
we created a machine-checked proof	method
The rise of socially targeted marketing suggests that decisions made by consumers can be predicted not only from their personal tastes and characteristics	background
but also from the decisions of people who are close to them in their networks	background
we present a hierarchical auto-probit model for individual binary outcomes that uses and extends the machinery of the auto-probit method for binary data	mechanism
We demonstrate the behavior of the parameters estimated by the multiple network-regime auto-probit model ( m-NAP ) under various sensitivity conditions	method
such as the impact of the prior distribution and the nature of the structure of the network	method
We also demonstrate several examples of correlated binary data outcomes in networks of interest to information systems	method
including the adoption of caller ring-back tones	method
whose use is governed by direct connection but explained by additional network topologies	method
DOI : 10	background
2514/1	background
I010178 Complex software systems are becoming increasingly prevalent in aerospace applications : in particular	background
to accomplish critical tasks	background
Ensuring the safety of these systems is crucial	background
as they can have subtly different behaviors under slight variations in operating and proposals are given on how to address these issues	background
The challenges that naturally arise when applying such technology to industrial-scale applications is then detailed	finding
As an illustration of these techniques	method
a novel lateral midair collision-avoidance maneuver is studied in an ideal setting	method
without accounting for the uncertainties of the physical reality	method
Existing Bayesian models	background
especially nonparametric Bayesian methods	background
rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations	background
Such results contribute to push forward the interface between these two important subfields	background
which have been largely treated as isolated in the community	background
which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics	finding
In this paper	mechanism
we present regularized Bayesian inference ( RegBayes )	mechanism
a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation	mechanism
RegBayes is more flexible than the procedure that elicits expert knowledge via priors	mechanism
and it covers both directed Bayesian networks and undirected Markov networks	mechanism
When the regularization is induced from a linear operator on the posterior distributions	mechanism
such as the expectation operator	mechanism
we present a general convex-analysis theorem to characterize the solution of RegBayes	mechanism
Furthermore we present two concrete examples of RegBayes	mechanism
infinite latent support vector machines ( iLSVM ) and multi-task infinite latent support vector machines ( MT-iLSVM )	mechanism
which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning	mechanism
respectively	mechanism
We present efficient inference methods and	mechanism
report empirical studies on several benchmark data sets	method
Network robustness is an important principle in biology and engineering	background
Previous studies of global networks have identified both redundancy and sparseness as topological properties used by robust networks	background
Modules internal to the cell that are less exposed to environmental noise are more connected and less robust than external modules	background
A similar design principle is used by several other biological networks	background
leads to novel algorithms and insights benefiting both fields	finding
We propose a simple change to the evolutionary gene duplication model	mechanism
We apply these observations to evaluate and design communication networks that are specifically optimized for noisy or malicious environments Combined	method
joint analysis of biological and computational networks	method
The convergence of mobile computing and cloud computing enables new multimedia applications that are both resource-intensive and interaction-intensive	background
For these applications	background
end-to-end network bandwidth and latency matter greatly when cloud resources are used to augment the computational power and battery life of a mobile device	background
We then describe an architectural solution that is a seamless extension of today 's cloud computing infrastructure	mechanism
We first present quantitative evidence	method
Our analysis reveals how the potential speedups over BCFW depend on the minibatch size and how one can provably obtain large problem dependent speedups	finding
obtaining significant speedups over competing state-of-the-art ( and synchronous ) methods on structural SVMs	finding
We develop mini-batched parallel Frank-Wolfe ( conditional gradient ) methods Our work includes the basic ( batch ) Frank-Wolfe algorithm as well as the recently proposed Block-Coordinate Frank-Wolfe ( BCFW ) method\citep { lacoste2012block } as special cases Our algorithm permits asynchronous updates within the minibatch	mechanism
and is robust to stragglers and faulty worker threads	mechanism
We present several experiments to indicate empirical behavior of our methods	method
How can we tell when accounts are fake or real in a social network ? And how can we tell which accounts belong to liberal	background
conservative or centrist users ? Often	background
we can answer such questions and label nodes in a network based on the labels of their neighbors and appropriate assumptions of homophily ( `` birds of a feather flock together '' ) or heterophily ( `` opposites attract '' )	background
One of the most widely used methods for this kind of inference is Belief Propagation ( BP ) which iteratively propagates the information from a few nodes with explicit labels throughout a network until convergence	background
show that LinBP and SBP are orders of magnitude faster than standard BP	finding
while leading to almost identical node labels	finding
This paper introduces Linearized Belief Propagation ( LinBP )	mechanism
a linearization of BP via intuitive matrix equations and	mechanism
thus comes with exact convergence guarantees	mechanism
It handles homophily	mechanism
heterophily and more general cases that arise in multi-class settings	mechanism
Plus it allows a compact implementation in SQL	mechanism
The paper also introduces Single-pass Belief Propagation ( SBP )	mechanism
a localized ( or `` myopic '' ) version of LinBP and for which the final class assignments depend only on the nearest labeled neighbors In addition	mechanism
SBP allows fast incremental updates in dynamic networks	mechanism
Our runtime experiments	method
we show that the proposed approach is more successful than other candidate methods for the topic modeling of competition	finding
We also demonstrate the generalization power of the proposed method for three prediction tasks	finding
We propose a dynamic topic model by jointly leveraging tweets and their associated images	mechanism
For a market of interest ( e	mechanism
g	mechanism
luxury goods )	mechanism
we aim at automatically detecting the latent topics ( e	mechanism
g	mechanism
bags clothes luxurious ) that are competitively shared by multiple brands ( e	mechanism
g	mechanism
Burberry Prada and Chanel )	mechanism
and tracking temporal evolution of the brands ' stakes over the shared topics	mechanism
One of key applications of our work is social media monitoring that can provide companies with temporal summaries of highly overlapped or discriminative topics with their major competitors We design our model to correctly address three major challenges : multiview representation of text and images	mechanism
modeling of competitiveness of multiple brands over shared topics	mechanism
and tracking their temporal evolution	mechanism
As far as we know	mechanism
no previous model can satisfy all the three challenges	mechanism
For evaluation we analyze about 10 millions of tweets and 8 millions of associated images of the 23 brands in the two categories of luxury and beer	method
Through experiments quantitatively	method
We show that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches	finding
In this paper	mechanism
we provide edge and set query algorithms that provably achieve some fraction of the omniscient optimal solution	mechanism
Our main theoretical result for the stochastic matching ( i	mechanism
e	mechanism
2-set packing ) problem is the design of an adaptive algorithm that queries only a constant number of edges per vertex and achieves a ( 1-e ) fraction of the omniscient optimal solution	mechanism
for an arbitrarily small e > 0 Moreover	mechanism
this adaptive algorithm performs the queries in only a constant number of rounds	mechanism
We complement this result with a non-adaptive ( i	mechanism
e	mechanism
one round of queries ) algorithm that achieves a ( 0	mechanism
5 - e ) fraction of the omniscient optimum	mechanism
We also extend both our results to stochastic k-set packing by designing an adaptive algorithm that achieves a ( 2/k - e ) fraction of the omniscient optimal solution	mechanism
again with only O ( 1 ) queries per element	mechanism
This guarantee is close to the best known polynomial-time approximation ratio of 3/k+1 -e for the deterministic k-set packing problem [ Furer 2013	mechanism
We empirically explore the application of ( adaptations of ) these algorithms to the kidney exchange problem	method
where patients with end-stage renal failure swap willing but incompatible donors on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange	method
Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data	background
For example they are often used to explore complex systems where connections between entities are not well understood	background
such as in functional brain networks or genetic networks	background
demonstrate the effectiveness of the method under various conditions	finding
In this paper	mechanism
we propose a new principled framework The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes	mechanism
Under a Gaussian model	mechanism
this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection ( Dempster	mechanism
1972 )	mechanism
We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective	mechanism
We provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles	mechanism
and uncovering brain connectivity graph from positron emission tomography data	mechanism
Finally we provide sufficient conditions under which the true graphical structure can be recovered correctly	mechanism
Extensive simulation studies	method
Many large-scale machine learning ( ML ) applications use iterative algorithms to converge on parameter values that make the chosen model fit the input data	background
show that such exploitation reduces per-iteration time by 33 -- 98 %	finding
for three real ML workloads	finding
and that these improvements are robust to variation in the patterns over time	finding
This paper shows that these repeating patterns can and should be exploited Focusing on the increasingly popular `` parameter server '' approach to sharing model parameters among worker threads	mechanism
we describe and demonstrate how the repeating patterns can be exploited Examples include replacing dynamic cache and server structures with static pre-serialized structures	mechanism
informing prefetch and partitioning decisions	mechanism
and determining which data should be cached at each thread to avoid both contention and slow accesses to memory banks attached to other sockets	mechanism
Experiments	method
Dashboards are increasingly being used in commercial buildings to show building data in an intuitive way to occupants and facility operators	background
Such dashboards make relevant parties aware of the impact that they have on a buildings behavior and enable them to understand the dynamics of building systems and current/historical energy use	background
and as a result	background
support reduction in energy use and improvement of operations of such systems	background
The findings show that effective building energy dashboards should contain query-based and quick-access based functionalities for showing building energy data through the use of various widgets and user interactions Such dashboards should also enable decomposing data within spatial and temporal dimensions	finding
interacting with static and dynamic data sources	finding
and providing information about directly measurable energy usage vs	finding
resulting energy use indicators	finding
This paper gives an overview of an approach for a monitored building that includes highly sensed building automation systems and sensors for energy consumption	mechanism
This project is part of the Energy Efficient Buildings Hub in the Philadelphia Navy Yard	mechanism
The developed approach incorporates formalization of a taxonomy for building energy dashboards	mechanism
identification of visualization aids and requirements through a questionnaire	mechanism
and a prototype implementation	method
Many modern multi-core processors sport a large shared cache with the primary goal of enhancing the statistic performance of computing workloads	background
However due to resulting cache interference among tasks	background
the uncontrolled use of such a shared cache can significantly hamper the predictability and analyzability of multi-core real-time systems	background
Software cache partitioning has been considered as an attractive approach to address this issue because it does not require any hardware support beyond that available on many modern processors	background
results indicate that	finding
compared to the traditional approaches	finding
our scheme is up to 39 % more memory space efficient and consumes up to 25 % less cache partitions while maintaining cache predictability	finding
Our scheme also yields a significant utilization benefit that increases with the number of tasks	finding
In this paper	mechanism
we propose a practical OS-level cache management scheme Our scheme provides predictable cache performance	mechanism
addresses the aforementioned problems of existing software cache partitioning	mechanism
and efficiently allocates cache partitions to schedule a given task set	mechanism
We have implemented and evaluated our scheme in Linux/RK running on the Intel Core i7 quad-core processor	method
Experimental	method
Existing algorithms for trajectory-based clustering usually rely on simplex representation and a single proximity-related distance ( or similarity ) measure Consequently	background
additional information markers ( e	background
g	background
social interactions or the semantics of the spatial layout ) are usually ignored	background
leading to the inability to fully discover the communities in the trajectory database	background
Experimental results demonstrate that TODMIS correctly and efficiently discovers the real grouping behaviors in these diverse settings	finding
we propose TODMIS : a general framework for Trajectory cOmmunity Discovery using Multiple Information Sources	mechanism
TODMIS combines additional information with raw trajectory data and creates multiple similarity metrics In our proposed approach	mechanism
we first develop a novel approach by constructing a Markov Random Walk model from the semantically-labeled trajectory data	mechanism
and then measuring similarity at the distribution level In addition	mechanism
we also extract and compute pair-wise similarity measures related to three additional markers	mechanism
namely trajectory level spatial alignment ( proximity )	mechanism
temporal patterns and multi-scale velocity statistics Finally	mechanism
after creating a single similarity metric from the weighted combination of these multiple measures	mechanism
we apply dense sub-graph detection	mechanism
We evaluated TODMIS extensively using traces of ( i ) student movement data in a campus	method
( ii ) customer trajectories in a shopping mall	method
and ( iii ) city-scale taxi movement data	method
Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence	background
and may enable a useful new way of interacting with the crowds powering other systems	background
demonstrate that Chorus can provide accurate	finding
topical responses answering nearly 93 % of user queries appropriately	finding
and staying on-topic in over 95 % of responses We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed	finding
quality and breadth of assistance	finding
In this paper	mechanism
we introduce Chorus	mechanism
a crowd-powered conversational assistant	mechanism
When using Chorus	mechanism
end users converse continuously with what appears to be a single conversational partner	mechanism
Behind the scenes	mechanism
Chorus leverages multiple crowd workers to propose and vote on responses	mechanism
A shared memory space helps the dynamic crowd workforce maintain consistency	mechanism
and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting	mechanism
Studies with 12 end users and 100 crowd workers	method
Organizations that collect and use large volumes of personal information often use security audits to protect data subjects from inappropriate uses of this information by authorized insiders	background
In face of unknown incentives of employees	background
a reasonable audit strategy for the organization is one that minimizes its regret	background
we introduce a richer class of games called bounded-memory games	mechanism
We introduce the notion of k-adaptive regret	mechanism
which compares the reward obtained by playing actions prescribed by the algorithm against a hypothetical k-adaptive adversary with the reward obtained by the best expert in hindsight against the same adversary	mechanism
Roughly a hypothetical k-adaptive adversary adapts her strategy to the defender 's actions exactly as the real adversary would within each window of karounds	mechanism
A k-adaptive adversary is a natural model for temporary adversaries ( e	mechanism
g	mechanism
company employees ) who stay for a certain number of audit cycles and are then replaced by a different person Our definition is parameterized by a set of experts	mechanism
which can include both fixed and adaptive defender strategies	mechanism
We investigate the inherent complexity of and design algorithms for adaptive regret minimization in bounded-memory games of perfect and imperfect information	mechanism
but the task of determining which SNPs have functional consequences remains an open challenge	background
This brought the effective coverage in the assemblies to eightfold	finding
reducing the number and size of gaps in the final assembly over what would be obtained with 5	finding
11-fold coverage	finding
The two assembly strategies yielded very similar results that largely agree with independent mapping data The assemblies effectively cover the euchromatic regions of the human chromosomes	finding
More than 90 % of the genome is in scaffold assemblies of 100	finding
000 bp or more	finding
and 25 % of the genome is in scaffolds of 10 million bp or larger Analysis of the genome sequence revealed 26	finding
588 protein-encoding transcripts for which there was strong corroborating evidence and an additional 12	finding
000 computationally derived genes with mouse matches or other weak supporting evidence Although gene-dense clusters are obvious	finding
almost half the genes are dispersed in low G+C sequence separated by large tracts of apparently noncoding sequence	finding
Only 1	finding
1 % of the genome is spanned by exons	finding
whereas 24 % is in introns	finding
with 75 % of the genome being intergenic DNA	finding
Duplications of segmental blocks	finding
ranging in size up to chromosomal lengths	finding
are abundant throughout the genome and reveal a complex evolutionary history indicates vertebrate expansions of genes associated with neuronal function	finding
with tissue-specific developmental regulation	finding
and with the hemostasis and immune systems provided locations of 2	finding
1 million single-nucleotide polymorphisms ( SNPs ) A random pair of human haploid genomes differed at a rate of 1 bp per 1250 on average	finding
but there was marked heterogeneity in the level of polymorphism across the genome	finding
Less than 1 % of all SNPs resulted in variation in proteins	finding
A 2	mechanism
91-billion base pair ( bp ) consensus sequence of the euchromatic portion of the human genome was generated by the whole-genome shotgun sequencing method	mechanism
The 14	mechanism
8-billion bp DNA sequence was generated over 9 months from 27	mechanism
271 853 high-quality sequence reads ( 5	mechanism
11-fold coverage of the genome ) from both ends of plasmid clones made from the DNA of five individuals	mechanism
Two assembly strategiesa whole-genome assembly and a regional chromosome assemblywere used	mechanism
each combining sequence data from Celera and the publicly funded genome effort	mechanism
The public data were shredded into 550-bp segments to create a 2	mechanism
9-fold coverage of those genome regions that had been sequenced	mechanism
without including biases inherent in the cloning and assembly procedure used by the publicly funded group	mechanism
Comparative genomic analysis DNA sequence comparisons between the consensus sequence and publicly funded genome data	method
Developments in health information technology have encouraged the establishment of distributed systems known as Health Information Exchanges ( HIEs ) to enable the sharing of patient records between institutions In many cases	background
the parties running these exchanges wish to limit the amount of information they are responsible for holding because of sensitivities about patient information	background
Hence there is an interest in broker-based HIEs that keep limited information in the exchange repositories	background
In this paper	mechanism
we consider some of the requirements and present a design in a way that controls the information available in audit logs and regulates their release for investigations Our approach is based on formal rules for audit and the use of Hierarchical Identity-Based Encryption ( HIBE ) to support staged release of data needed in audits and a balance between automated and manual reviews	mechanism
We test our methodology via an extension of a standard for auditing HIEs called the Audit Trail and Node Authentication Profile ( ATNA ) protocol	method
we show improved performance over scalable Gaussian processes with flexible kernel learning models	finding
and stand-alone deep architectures	finding
We introduce scalable deep kernels	mechanism
which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods	mechanism
Specifically we transform the inputs of a spectral mixture base kernel with a deep architecture	mechanism
using local kernel interpolation	mechanism
inducing points and structure exploiting ( Kronecker and Toeplitz ) algebra for a scalable kernel representation	mechanism
We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process	mechanism
Inference and learning cost $ O ( n ) $ for $ n $ training points	mechanism
and predictions cost $ O ( 1 ) $ per test point	mechanism
On a large and diverse collection of applications	method
including a dataset with 2 million examples	method
Significant efforts are being made in the improvement of building automation and control systems in order to optimize the performance of buildings ( e	background
g	background
reduction of energy consumption )	background
As sensor networks in buildings increase	background
the complexity of managing them also increases	background
For instance the generation and maintenance of metadata about sensors	background
such as their location within a building	background
currently requires significant manual labor	background
Being able to understand the relationships between different measurement types and building characteristics is fundamental in achieving an automatic mapping of sensors in buildings	background
The energy contained in the conditioned air delivered to each room is presented as a characteristic feature in order to understand the differences between rooms	method
( 2 ) We provide proof of our model 's robustness to spam and anomalous behavior	finding
to demonstrate the model 's effectiveness in accurately predicting user 's ratings	finding
avoiding prediction skew in the face of injected spam	finding
and finding interesting patterns in real world ratings data	finding
In this paper we describe a unified Bayesian approach to Collaborative Filtering It models the discrete structure of ratings and is flexible to the often non-Gaussian shape of the distribution	mechanism
Additionally our method finds a co-clustering of the users and items	mechanism
which improves the model 's accuracy and makes the model robust to fraud	mechanism
We offer three main contributions : ( 1 ) We provide a novel model and Gibbs sampling algorithm that accurately models the quirks of real world ratings	mechanism
such as convex ratings distributions	mechanism
3 ) We use several real world datasets	method
Standard approaches to stochastic discrete systems require numerical solutions for large optimization problems and quickly become infeasible with larger state spaces	background
Generalizations of these techniques to hybrid systems with stochastic effects are even more challenging It is in principle applicable to a variety of stochastic models from other domains	background
e	background
g	background
systems biology	background
While the answer to the verification problem is not guaranteed to be correct	finding
we prove that Bayesian SMC can make the probability of giving a wrong answer arbitrarily small We show that our technique enables faster verification than state-of-the-art statistical techniques	finding
In particular we present a Statistical Model Checking ( SMC ) approach based on Bayesian statistics We show that our approach is feasible for a certain class of hybrid systems with stochastic transitions	mechanism
a generalization of Simulink/Stateflow models	mechanism
The SMC approach was pioneered by Younes and Simmons in the discrete and non-Bayesian case	mechanism
It solves the verification problem by combining randomized sampling of system traces ( which is very efficient for Simulink/Stateflow ) with hypothesis testing ( i	mechanism
e	mechanism
testing against a probability threshold ) or estimation ( i	mechanism
e	mechanism
computing with high probability a value close to the true probability )	mechanism
We believe SMC is essential for scaling up to large Stateflow/Simulink models	mechanism
The advantage is that answers can usually be obtained much faster than with standard	mechanism
exhaustive model checking techniques We emphasize that Bayesian SMC is by no means restricted to Stateflow/Simulink models	mechanism
We apply our Bayesian SMC approach to a representative example of stochastic discrete-time hybrid system models in Stateflow/Simulink : a fuel control system featuring hybrid behavior and fault tolerance	method
Existing literature proposes distributed gradient-like methods that are computationally cheap and resilient to link failures	background
but have slow convergence rates	background
For comparison the standard distributed gradient method can not do better than ( 1/k 2/3 ) and ( 1/ K 2/3 )	background
on the same class of cost functions ( even for static networks )	background
We prove their convergence rates Then the modified D-NG achieves rates O ( logk/k ) and O ( logK/ K )	finding
and the modified D-NC rates O ( 1/k 2 ) and O ( 1/ K 2- )	finding
where > 0 is arbitrarily small illustrate our analytical findings	finding
In this paper	mechanism
we propose accelerated distributed gradient methods that 1 ) are resilient to link failures ; 2 ) computationally cheap ; and 3 ) improve convergence rates over other gradient methods	mechanism
We model the network by a sequence of independent	mechanism
identically distributed random matrices { W ( k ) } drawn from the set of symmetric	mechanism
stochastic matrices with positive diagonals	mechanism
The network is connected on average and the cost functions are convex	mechanism
differentiable with Lipschitz continuous and bounded gradients	mechanism
We design two distributed Nesterov-like gradient methods that modify the D-NG and D-NC methods that we proposed for static networks	mechanism
in terms of the expected optimality gap at the cost function	method
Let k and K be the number of per-node gradient evaluations and per-node communications	method
respectively	method
Simulation examples	method
In search advertising	background
the search engine needs to select the most profitable advertisements to display	background
which can be formulated as an instance of online learning with partial feedback	background
also known as the stochastic multi-armed bandit ( MAB ) problem	background
We then propose simple bias-correction methods with benefits to both the search engine and the advertisers	mechanism
Smartwatches promise to bring enhanced convenience to common communication	background
creation and information retrieval tasks	background
In this work	mechanism
we propose a complementary input approach : using the watch face as a multi-degree-of-freedom	mechanism
mechanical interface	mechanism
We developed a proof of concept smartwatch that supports continuous 2D panning and twist	mechanism
as well as binary tilt and click	mechanism
To illustrate the potential of our approach	method
we developed a series of example applications	method
many of which are cumbersome -- or even impossible -- on today 's smartwatch devices	method
Which song will Smith listen to next ? Which restaurant will Alice go to tomorrow ? Which product will John click next	background
TribeFlow is more accurate and up to 413x faster than top competitors	finding
Mindful of these challenges we propose TribeFlow	mechanism
a method designed TribeFlow is a general method that can perform next product recommendation	mechanism
next song recommendation	mechanism
next location prediction	mechanism
and general arbitrary-length user trajectory prediction without domain-specific knowledge	mechanism
For instance although `` eats '' and `` stares at '' seem unrelated in text	background
they share semantics visually	background
When people are eating something	background
they also tend to stare at the food	background
We show improvements on three tasks : common-sense assertion classification	finding
visual paraphrasing and text-based image retrieval	finding
Our code and datasets are available online	finding
We propose a model We note that the visual grounding of words depends on semantics	mechanism
and not the literal pixels	mechanism
We thus use abstract scenes created from clipart to provide the visual grounding	mechanism
We find that the embeddings we learn capture fine-grained	mechanism
visually grounded notions of semantic relatedness	mechanism
over text-only word embeddings ( word2vec )	method
Learning about a new area of knowledge is challenging for novices partly because they are not yet aware of which topics are most important	background
The first experiment shows that a high context	finding
low structure interface helps crowdworkers perform faster	finding
higher quality synthesis	finding
while the second experiment shows that a tournament-style ( parallelized ) crowd workflow produces faster	finding
higher quality more diverse outlines than a linear ( serial/iterative ) workflow	finding
we present Crowdlines	mechanism
a system that uses crowdsourcing to help people synthesize diverse online information	mechanism
Crowdworkers make connections across sources to produce a rich outline that surfaces diverse perspectives within important topics	mechanism
We evaluate Crowdlines with two experiments	method
Review fraud is a pervasive problem in online commerce	background
in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services	background
show that BIRDNEST successfully spots review fraud in large real-world graphs : the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart	finding
Hence in this paper	mechanism
we propose an approach which combines these 2 approaches in a principled manner	mechanism
allowing successful detection even when one of these signs is not present To combine these 2 approaches	mechanism
we formulate our Bayesian Inference for Rating Data ( BIRD ) model	mechanism
a flexible Bayesian model of user rating behavior	mechanism
Based on our model we formulate a likelihood-based suspiciousness metric	mechanism
Normalized Expected Surprise Total ( NEST )	mechanism
We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric	mechanism
Experiments on real data	method
that illustrate the value and immediate feasibility of our approach	finding
We describe a novel approach offering two additional	mechanism
analog degrees of freedom for interactive functions	mechanism
Further we show that our approach can be achieved on off-the-shelf consumer touchscreen devices : a smartphone and smartwatch	mechanism
We validate our technique though a user study on both devices and conclude with several demo applications	method
User identification and differentiation have implications in many application domains	background
including security personalization	background
and co-located multiuser systems	background
In response dozens of approaches have been developed	background
from fingerprint and retinal scans	background
to hand gestures and RFID tags	background
Our user study demonstrates twenty-participant authentication accuracies of 99	finding
6 %	finding
For twenty-user identification	finding
our software achieved 94	finding
0 % accuracy and 98	finding
2 % on groups of four	finding
simulating family use	finding
In this work	mechanism
we propose CapAuth	mechanism
a technique that uses existing	mechanism
low-level touchscreen data	mechanism
combined with machine learning classifiers	mechanism
As a proof-of-concept	method
we ran our software on an off-the-shelf Nexus 5 smartphone	method
Health information exchanges ( HIEs ) are healthcare information technology efforts designed to foster coordination of patient care across the fragmented U	background
S	background
healthcare system	background
Their purpose is to improve efficiency and quality of care through enhanced sharing of patient data	background
Across the United States	background
numerous states have enacted laws that provide various forms of incentives for HIEs and address growing privacy concerns associated with the sharing of patient data Our results contribute to the burgeoning literature on health information technology and the debate on the impact of privacy regulation on technology innovation	background
In particular they show that the impact of privacy regulation on the success of information technology efforts is heterogeneous : both positive and negative effects can arise from regulation	background
depending on the specific attributes of privacy laws	background
This paper was accepted by Anandhi Bharadwaj	background
information systems	background
Although we observe that privacy regulation alone can result in a decrease in planning and operational HIEs	finding
we also find that	finding
when coupled with incentives	finding
privacy regulation with requirements for patient consent can actually positively impact the development of HIE efforts Among all states with laws creating HIE incentives	finding
only states that combined incentives with consent requirements saw a net increase in operational HIEs ; HIEs in those states also reported decreased levels of privacy concern relative to HIEs in states with other legislative approaches	finding
focusing on the impact of laws that include requirements for patient consent	method
Natural language dialog is an important and intuitive way for people to access information and services This hybrid systems approach will help make dialog systems both more general and more robust going forward	background
This paper introduces Guardian	mechanism
a crowdpowered framework that wraps existing Web APIs into immediately usable spoken dialog systems Guardian takes as input the Web API and desired task	mechanism
and the crowd determines the parameters necessary to complete it	mechanism
how to ask for them	mechanism
and interprets the responses from the API	mechanism
The system is structured so that	mechanism
over time it can learn to take over for the crowd	mechanism
Recent research has highlighted the need for upstream planning in healthcare service delivery systems	background
patient scheduling and resource allocation in the hospital inpatient setting DRGs are a payment scheme employed at patients discharge	background
where the DRG and length of stay determine the revenue that the hospital obtains	background
The largest improvements were observed at and before admission	finding
when information such as procedures and diagnoses is typically incomplete	finding
but performance was improved even after a substantial portion of the patients length of stay	finding
and under multiple scenarios making different assumptions about the available information	finding
Using the improved DRG predictions within our resource allocation model improves contribution margin by 2	finding
9 % and the utilization of scarce resources such as operating rooms and beds from 66	finding
3 % to 67	finding
3 % and from 70	finding
7 % to 71	finding
7 % respectively	finding
This enables 9	finding
0 % more nonurgent elective patients to be admitted as compared to the baseline	finding
based on machine learning ( ML ) and mixed-integer programming ( MIP )	mechanism
We show that early and accurate DRG classification using ML methods	mechanism
incorporated into an MIP-based resource allocation model	mechanism
can increase the hospitals contribution margin	mechanism
the number of admitted patients	mechanism
and the utilization of resources such as operating rooms and beds	mechanism
We test these methods on hospital data containing more than 16	method
000 inpatient records and demonstrate improved DRG classification accuracy as compared to the hospitals current approach	method
Touchscreens with dynamic electrostatic friction are a com-pelling	background
low-latency and solid-state haptic feedback technology	background
Work to date has focused on minimum perceptual difference	background
texture rendering and fingertip-surface models	background
Our results show that can improve targeting speed by 7	finding
5 % compared to conventional flat touchscreens	finding
electrostatic haptic feedback	method
Background Effective management and treatment of cancer continues to be complicated by the rapid evolution and resulting heterogeneity of tumors	background
Phylogenetic study of cell populations in single tumors provides a way to delineate intra-tumoral heterogeneity and identify robust features of evolutionary processes	background
Here we investigate a strategy	mechanism
When training large machine learning models with many variables or parameters	background
a single machine is often inadequate since the model may be too large to fit in memory	background
while training can take a long time even with stochastic updates	background
A natural recourse is to turn to distributed cluster computing	background
in order to harness additional memory and processors	background
We develop a framework of primitives	mechanism
STRADS thus improving their memory efficiency while presenting new opportunities to speed up convergence without compromising inference correctness	mechanism
We demonstrate the efficacy of model-parallel algorithms implemented in STRADS versus popular implementations for Topic Modeling	method
Matrix Factorization and Lasso	method
The ap- proach can	background
e	background
g	background
generate nontrivial algebraic invariant equations capturing the airplane behavior during take-off or landing in longitudinal motion	background
by one polynomial and a finite set of its successive Lie derivatives	mechanism
This so-called differential radical characterization re- lies on a sound abstraction of the reachable set of solutions by the smallest variety that contains it	mechanism
The characterization leads to a differential radical invariant proof rule that is sound and complete	mechanism
which implies that invariance of algebraic equa- tions over real-closed fields is decidable Furthermore	mechanism
the problem of generating invariant varieties is shown to be as hard as minimizing the rank of a symbolic matrix	mechanism
and is therefore NP-hard	mechanism
We investigate symbolic linear algebra tools based on Gaussian elimination	method
When companies operate on the graphs with monetary incentives to sell Twitter `` Followers '' and Facebook page `` Likes ''	background
the graphs show strange connectivity patterns	background
We report strange deviations from typical patterns like smooth degree distributions	finding
We find that such deviations are often due to `` lockstep behavior '' that large groups of followers connect to the same groups of followees	finding
We discover that ( a ) the lockstep behaviors on the graph shape dense `` block '' in its adjacency matrix and creates `` rays '' in spectral subspaces	finding
and ( b ) partially overlapping of the behaviors shape `` staircase '' in its adjacency matrix and creates `` pearls '' in spectral subspaces The results demonstrate the scalability and effectiveness of our proposed algorithm	finding
The second contribution is that we provide a fast algorithm	mechanism
using the discovery as a guide for practitioners	mechanism
In this paper	method
we study a complete graph from a large Twitter-style social network	method
spanning up to 3	method
33 billion edges	method
Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep	method
We carry out extensive experiments on both synthetic and real datasets	method
as well as public datasets from IMDb and US Patent	method
Characterizing the spatial distribution of proteins directly from microscopy images is a difficult problem with numerous applications in cell biology ( e	background
g	background
identifying motor-related proteins ) and clinical research ( e	background
g	background
identification of cancer biomarkers ) Such models are expected to be valuable for representing and summarizing each pattern and for constructing systems biology simulations of cell behaviors	background
We were able to show that these patterns could be distinguished from each other with high accuracy	finding
and we were able to assign to one of these subclasses hundreds of proteins whose subcellular localization had not previously been well defined	finding
Here we describe the design of a system including quantification of their relationships to microtubules	mechanism
We constructed the system using confocal immunofluorescence microscopy images from the Human Protein Atlas project for 11 punctate proteins in three cultured cell lines These proteins have previously been characterized as being primarily located in punctate structures	mechanism
but their images had all been annotated by visual examination as being simply vesicular	mechanism
In addition to providing these novel annotations	mechanism
we built a generative approach to modeling of punctate distributions	mechanism
Revenue maximization in combinatorial auctions ( and other multidimensional selling settings ) is one of the most important and elusive problems in mechanism design	background
Such priors do not exist in most applications	background
Rather in many applications ( such as premium display advertising markets )	background
there is essentially a point prior	background
which may not be accurate	background
validate the approach and show that our techniques dramatically improve scalability over a leading general-purpose MIP solver	finding
In this paper	mechanism
we instead study a common revenue-enhancement approach - bundling - in the context of the most commonly studied combinatorial auction mechanism	mechanism
the Vickrey-Clarke-Groves ( VCG ) mechanism	mechanism
We adopt the point prior model	mechanism
and prove robustness to inaccuracy in the prior	mechanism
Then we present a branch-and-bound framework for finding the optimal bundling	mechanism
We introduce several techniques	mechanism
Experiments on CATS distributions	method
Most noticeably the accuracy of our algorithm reaches 51	finding
8 % on the challenging HMDB dataset which outperforms the state-of-the-art of 7	finding
3 % relatively	finding
We propose a novel content driven pooling that leverages space-time context while being robust toward global space-time transformations	mechanism
Being robust to such transformations is of primary importance in unconstrained videos where the action localizations can drastically shift between frames	mechanism
Our pooling identifies regions of interest using video structural cues estimated by different saliency functions	mechanism
To combine the different structural information	mechanism
we introduce an iterative structure learning algorithm	mechanism
WSVM ( weighted SVM )	mechanism
that determines the optimal saliency layout of an action model through a sparse regularizer	mechanism
A new optimization method is proposed to solve the WSVM highly non-smooth objective function	mechanism
We evaluate our approach on standard action datasets ( KTH	method
UCF50 and HMDB )	method
we provide very positive results for plurality and very negative results for Borda	finding
and place veto in the middle of this spectrum	finding
via the notion of the price of anarchy	mechanism
using the scores of alternatives as a proxy for their quality and bounding the ratio between the score of the optimal alternative and the score of the winning alternative in Nash equilibrium Specifically	mechanism
we are interested in Nash equilibria that are obtained via sequences of rational strategic moves	mechanism
Focusing on three common voting rules -- plurality	mechanism
veto and Borda --	mechanism
Given a large graph	background
like a computer communication network	background
which $ k $ nodes should we immunize ( or monitor	background
or remove )	background
to make it as robust as possible against a computer virus attack ? This problem	background
referred to as the node immunization problem	background
is the core building block in many high-impact applications	background
ranging from public health	background
cybersecurity to viral marketing	background
A central component in node immunization is to find the best $ k $ bridges of a given graph	background
( 1 ) the proposed bridging score gives mining results consistent with intuition ; and ( 2 ) the proposed fast solution is up to seven orders of magnitude faster than straightforward alternatives	finding
First of all	mechanism
we propose a novel bridging score $ \Delta \lambda $	mechanism
inspired by immunology	mechanism
and we show that its results agree with intuition for several realistic settings	mechanism
Since the straightforward way to compute $ \Delta \lambda $ is computationally intractable	mechanism
we then focus on the computational issues and propose a surprisingly efficient way ( $ O ( nk^2+m ) $ ) to estimate it	mechanism
Experimental results on real graphs show that	method
To the best of our knowledge	background
our work represents the largest study of propagation patterns of executables	background
Finally we discover the SharkFin temporal propagation pattern of executable files	finding
the GeoSplit pattern in the geographical spread of machines that report executables to Symantec 's servers	finding
the Periodic Power Law ( Ppl ) distribution of the life-time of URLs	finding
and we show how to efficiently extrapolate crucial properties of the data from a small sample	finding
by analyzing patterns from 22 million malicious ( and benign ) files	method
found on 1	method
6 million hosts worldwide during the month of June 2011	method
We conduct this study using the WINE database available at Symantec Research Labs Additionally	method
we explore the research questions raised by sampling on such large databases of executables ; the importance of studying the implications of sampling is twofold : First	method
sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second	method
because every such data collection can be perceived as a sample of the real world	method
we discovered that duplicate points create subtle issues	finding
that the literature has ignored : if dmax is the multiplicity of the most over-plotted point	finding
typical algorithms are quadratic on dmax	finding
we show that our methods give either exact results	finding
or highly accurate approximate ones	finding
We propose several ways we report wall-clock times and our time savings ; and	mechanism
After careful analysis	method
Counterfactual Regret Minimization ( CFR ) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games	background
CFR is an iterative algorithm that repeatedly traverses the game tree	background
updating regrets at each information set	background
show an order of magnitude speed improvement	finding
and the relative speed improvement increases with the size of the game	finding
We introduce to CFR It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive	mechanism
had that path been explored on every iteration The new algorithm maintains CFR 's convergence guarantees while making iterations significantly fastereven if previously known pruning techniques are used in the comparison	mechanism
This improvement carries over to CFR+	mechanism
a recent variant of CFR	mechanism
Experiments	method
Influence maximization is a problem of maximizing the aggregate adoption of products	background
technologies or even beliefs	background
Most past algorithms leveraged an assumption of submodularity that captures diminishing returns to scale	background
prove that this policy is optimal in a very general setting	finding
that the proposed `` best-time '' algorithm remains quite effective the `` best-time '' policy becomes suboptimal	finding
and is significantly outperformed by our more general heuristic	finding
We formulate a dynamic influence maximization problem under increasing returns We propose a simple algorithm in this model which chooses the best time period to use up the entire budget ( called Best-Stage )	mechanism
and We also propose a heuristic algorithm for this problem of which Best-Stage decision is a special case	mechanism
Additionally we experimentally verify even as we relax the assumptions under which optimality can be proved	method
However we find that when we add a `` learning-by-doing '' effect	method
in which the adoption costs decrease not as a function of time	method
but as a function of aggregate adoption	method
Pareto efficiency is a widely used property in solution concepts for cooperative and non -- cooperative game -- theoretic settings and	background
more generally in multi -- objective problems	background
In this paper	mechanism
we show that the Pareto curve of a bimatrix game can be found exactly in polynomial time and that it is composed of a polynomial number of pieces	mechanism
Furthermore each piece is a quadratic function We use this result to provide algorithms	mechanism
Extensive-form games are a powerful tool for modeling a large range of multiagent scenarios Finally we discuss how our theory applies to several practical problems for which no solution quality bounds could be derived before	background
Leveraging recent results on abstraction solution quality	mechanism
we develop the first framework For games where the error is Lipschitz-continuous in the distance of a continuous point to its nearest discrete point	mechanism
we show that a uniform discretization of the space is optimal	mechanism
When the error is monotonically increasing in distance to nearest discrete point	mechanism
we develop an integer program for finding the optimal discretization when the error is described by piecewise linear functions	mechanism
This result can further be used to approximate optimal solutions to general monotonic error functions	mechanism
Kidney exchange where candidates with organ failure trade incompatible but willing donors	background
is a life-saving alternative to the deceased donor waitlist	background
which has inadequate supply to meet demand We conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view	background
In this paper	mechanism
we begin by proposing the idea of liver exchange	mechanism
and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level	mechanism
We then explore cross-organ donation where kidneys and livers can be bartered for each other	mechanism
We show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges ; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool	mechanism
We support this result experimentally on demographically accurate multi-organ exchanges	method
The human brain is widely hypothesized to construct inner beliefs about how the world works	background
It is thought that we need this conception to coordinate our movements and anticipate rapid events that go on around us	background
A driver for example	background
needs to predict how the car should behave in response to every turn of the steering wheel and every tap on the brake	background
But on icy roads	background
these predictions will often not reflect how the car would behave	background
Applying the brakes sharply in these conditions could send the car skidding uncontrollably rather than stopping	background
The brain constructs such inner beliefs over time through experience and learning Taken together	background
this work provides a framework for understanding how the brain transforms sensory information into instructions for movement	background
The findings could also help improve the performance of brain-machine interfaces and suggest how we can learn new skills more rapidly and proficiently in everyday life	background
The monkeys cursor movements were remarkably precise	finding
In fact the experiment showed that the monkeys could internally predict their cursor movements just as a driver predicts how a car will move when turning the steering wheel These findings indicate that the monkeys have likely developed inner beliefs to predict how their neural signals drive the cursor	finding
and that these beliefs helped coordinate their performance	finding
In addition when the monkeys did make mistakes	finding
their neural signals were not entirely wrongin fact they were typically consistent with the monkeys inner beliefs about how the cursor moves	finding
A mismatch between these inner beliefs and reality explained most of the monkeys mistakes	finding
This experiment uncovered that	finding
during the course of learning	finding
the monkeys inner beliefs realigned to better match the movements of the new cursor	finding
by conducting a brain-machine interface experiment	method
In this experiment	method
neural signals from the brains of two rhesus macaques were recorded using arrays of electrodes and translated into movements of a cursor on a computer screen	method
The monkeys were then trained to mentally move the cursor to hit targets on the screen	method
next conducted an experiment in which the cursor moved in a way that was substantially different from the monkeys inner beliefs	method
A multi-faceted graph defines several facets on a set of nodes	background
Each facet is a set of edges that represent the relationships between the nodes in a specific context	background
where NeSim is shown to be superior to MCL	finding
JP and AP	finding
the well-established clustering algorithms We also report the success stories of MuFace in finding advertisement click rings	finding
We propose NeSim	mechanism
a distributed efficient clustering algorithm We also propose optimizations to further improve the scalability	mechanism
the efficiency and the clusters quality We employ generalpurpose graph-clustering algorithms in a novel way Due to the qualities of NeSim	mechanism
we employ it as a backbone in the distributed MuFace algorithm	mechanism
which discovers multi-faceted communities	mechanism
We evaluate the proposed algorithms on several real and synthetic datasets	method
The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm	background
It won the 2014 Annual Computer Poker Competition	finding
beating each opponent with statistical significance	finding
We introduce a distributed version of the most commonly used equilibrium-finding algorithm	mechanism
counterfactual regret minimization ( CFR )	mechanism
The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint We introduce an algorithm while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and the earth-mover's-distance similarity metric	mechanism
Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency	mechanism
Prior approaches run slowly on this architecture Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency	mechanism
Finally we introduce a family of post-processing techniques that outperform prior ones	mechanism
We applied these techniques to generate an agent for two-player no-limit Texas Hold'em	method
How can we succinctly describe a million-node graph with a few simple sentences ? Given a large graph	background
how can we find its most `` important '' structures	background
so that we can summarize it and easily visualize it ? How can we measure the `` importance '' of a set of discovered subgraphs in a large graph ?	background
To this end	mechanism
we first mine candidate subgraphs using one or more graph partitioning algorithms Next	mechanism
we identify the optimal summarization using the minimum description length MDL principle	mechanism
picking only those subgraphs from the candidates that together yield the best lossless compression of the graph-or	mechanism
equivalently that most succinctly describe its adjacency matrix	mechanism
demonstrates superior performance and reliability compared to a basic decision tree approach We also briefly discuss how the method has assisted in debugging a commercial autonomous ground vehicle system	finding
We propose a method Our method models the set of fault-triggering inputs as a Cartesian product and identifies this set by actively querying the system under test	mechanism
The active sampling scheme is very efficient in the common case that few fields in the interface are relevant to causing the fault This scheme also solves the problem of efficiently finding sufficient examples to model rare faults	mechanism
which is problematic for other learning-based methods	mechanism
Compared to other techniques	mechanism
ours requires no parameter turning or post-processing in order to produce useful results	mechanism
We analyze the method qualitatively	method
theoretically and empirically An experimental evaluation	method
Latent Variable Models ( LVMs ) are a large family of machine learning models providing a principled and effective way to extract underlying patterns	background
structure and knowledge from observed data	background
We show that the monotonicity of the lower bound is closely aligned with the MAR to qualify the lower bound as a desirable surrogate of the MAR we demonstrate that MAR can effectively capture long-tail patterns	finding
reduce model complexity without sacrificing expressivity and improve interpretability	finding
we develop a novel regularization technique for LVMs	mechanism
which controls the geometry of the latent space during learning to enable the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually different from each other	mechanism
to accomplish long-tail coverage	mechanism
low redundancy and better interpretability We propose a mutual angular regularizer ( MAR ) to encourage the components in LVMs to have larger mutual angles	mechanism
The MAR is non-convex and non-smooth	mechanism
entailing great challenges for optimization	mechanism
To cope with this issue	mechanism
we derive a smooth lower bound of the MAR and optimize the lower bound instead	mechanism
Using neural network ( NN ) as an instance	method
we analyze how the MAR affects the generalization performance of NN On two popular latent variable models -- - restricted Boltzmann machine and distance metric learning	method
and exhibits a higher fitting accuracy on all of them	finding
We propose a facial alignment algorithm Our approach proceeds from sparse to dense landmarking steps using a set of specific models trained to best account for the shape and texture variation manifested by facial landmarks and facial shapes across pose and various expressions We also propose the use of a novel $ \ell _1 $ -regularized least squares approach that we incorporate into our shape model	mechanism
which is an improvement over the shape model used by several prior Active Shape Model ( ASM ) based facial landmark localization algorithms	mechanism
Our approach is compared against several state-of-the-art methods on many challenging test datasets	method
showing good performance for modeling previously unseen molecular configurations we show substantial improvement over the state of the art in molecular energy optimization	finding
Motivated by problems such as molecular energy prediction	mechanism
we derive an ( improper ) kernel between geometric inputs	mechanism
that is able Since many physical simulations based upon geometric data produce derivatives of the output quantity with respect to the input positions	mechanism
we derive an approach that incorporates derivative information into our kernel learning	mechanism
We further show how to exploit the low rank structure of the resulting kernel matrices to speed up learning	mechanism
Finally we evaluated the method in the context of molecular energy prediction	method
Integrating the approach into a Bayesian optimization	method
The leading approach for computing strong game-theoretic strategies in large imperfect-information games is to first solve an abstracted version of the game offline	background
then perform a table lookup during game play	background
show that our algorithm leads to significantly stronger performance against the strongest agents from the 2013 AAAI Annual Computer Poker Competition	finding
We consider where we solve the portion of the game that we have actually reached in real time to a greater degree of accuracy than in the initial computation	mechanism
We call this approach endgame solving	mechanism
Theoretically we show that endgame solving can produce highly exploitable strategies in some games ; however	mechanism
we show that it can guarantee a low exploitability in certain games where the opponent is given sufficient exploitative power within the endgame	mechanism
Furthermore despite the lack of a general worst-case guarantee	mechanism
we describe benefits of endgame solving	mechanism
We present an efficient algorithm and present a new variance-reduction technique	mechanism
Experiments on no-limit Texas Hold'em	method
The proliferation of mobile technologies makes it possible for mobile advertisers to go beyond the realtime snapshot of the static location and contextual information about consumers	background
This indicates closely targeted mobile ads may constrict consumer focus and significantly reduce the impulsive purchase behavior	background
Our finding suggests marketers should carefully design mobile advertising strategy	background
depending on different business contexts	background
We found the new mobile trajectory-based advertising is significantly more effective for focal advertising store compared to several existing baselines	finding
It is especially effective in attracting highincome consumers	finding
Interestingly it becomes less effective during the weekend	finding
In this study	mechanism
we propose a novel mobile advertising strategy	mechanism
To evaluate the effectiveness of this strategy	method
we design a large-scale randomized field experiment in a large shopping mall in Asia based on 83	method
370 unique user responses for two weeks in 2014	method
Simultaneously achieving high level of faithfulness and expressiveness is very rare among other methods	background
The illumination normalized faces using our proposed Pokerface not only exhibit very high fidelity against neutrally illuminated face	finding
but also allow for a significant improvement in face verification experiments using even the simplest classifier	finding
We propose a new method called the Pokerface The Pokerface is a two-phase approach	mechanism
It first aims at maximizing the minimum gap between adjacently-valued pixels while keeping the partial ordering of the pixels in the face image under extreme illumination condition	mechanism
an intuitive effort based on order theory to unveil the underlying structure of a dark image	mechanism
This optimization can be formulated as a feasibility search problem and can be efficiently solved by linear programming	mechanism
It then smooths the intermediate representation by repressing the energy of the gradient map	mechanism
The smoothing step is carried out by total variation minimization and sparse approximation	mechanism
These conclusions are drawn after benchmarking our algorithm against 22 prevailing illumination normalization techniques on both the CMU Multi-PIE database and Extended YaleB database that are widely adopted for face illumination problems	method
Living organisms adapt to challenges through evolution and adaptation	background
Potential application classes include therapeutics at the population	background
individual and molecular levels ( drug design )	background
as well as cell repurposing and synthetic biology	background
propose the wild idea of computational game theory for ( typically incomplete-information ) multistage games and opponent exploitation techniques	mechanism
A sequential contingency plan for steering is constructed computationally for the setting at hand In the biological context	mechanism
the opponent ( e	mechanism
g	mechanism
a disease ) has a systematic handicap because it evolves myopically	mechanism
This can be exploited by computing trapping strategies that cause the opponent to evolve into states where it can be handled effectively	mechanism
and it showed improved retrieval accuracy and efficiency	finding
We propose a retrieval method based on a bag-of-visual-words ( BoVW ) to identify discriminative characteristics between different medical images with Pruned Dictionary based on Latent Semantic Topic description We refer to this as the PD-LST retrieval	mechanism
Our method has two main components	mechanism
First we calculate a topic-word significance value for each visual word given a certain latent topic to evaluate how the word is connected to this latent topic	mechanism
The latent topics are learnt	mechanism
based on the relationship between the images and words	mechanism
and are employed to bridge the gap between low-level visual features and high-level semantics	mechanism
These latent topics describe the images and words semantically and can thus facilitate more meaningful comparisons between the words	mechanism
Second we compute an overall-word significance value to evaluate the significance of a visual word within the entire dictionary	mechanism
We designed an iterative ranking method to measure overall-word significance by considering the relationship between all latent topics and words	mechanism
The words with higher values are considered meaningful with more significant discriminative power in differentiating medical images	mechanism
We evaluated our method on two public medical imaging datasets	method
A system and method The system is based on a sensor network and is efficient	mechanism
scalable and requires only short-range communication	mechanism
The system allows for sensor-to-sensor communication as well as the traditional sensor-to-anchor communication	mechanism
the present invention pairs each resource with an inexpensive	mechanism
low-powered sensor possessing minimal communication and computation capabilities	mechanism
The sensors communicate with only nearby resources or anchors and those resources communicate with their nearby resources or anchors until a wireless	mechanism
linked network of resources and anchors is formed	mechanism
Deep learning ( DL ) has achieved notable successes in many machine learning tasks	background
A number of frameworks have been developed to expedite the process of designing and training deep neural networks ( DNNs )	background
such as Caffe	background
Torch and Theano	background
show that Poseidon converges to same objectives as a single machine	finding
and achieves state-of-art training speedup Poseidon with 8 nodes achieves better speedup and competitive accuracy to recent CPU-based distributed systems such as Adam and Le et al	finding
which use 10s to 1000s of nodes	finding
we propose Poseidon	mechanism
a scalable system architecture for distributed inter-machine communication in existing DL frameworks We integrate Poseidon with Caffe Poseidon features three key contributions that accelerate DNN training on clusters : ( 1 ) a three-level hybrid architecture that allows Poseidon to support both CPU-only and GPU-equipped clusters	mechanism
( 2 ) a distributed wait-free backpropagation ( DWBP ) algorithm to improve GPU utilization and to balance communication	mechanism
and ( 3 ) a structure-aware communication protocol ( SACP ) to minimize communication overheads	mechanism
and evaluate its performance at training DNNs for object recognition	method
We empirically across multiple models and well-established datasets using a commodity GPU cluster of 8 nodes ( e	method
g	method
4	method
5x speedup on AlexNet	method
4x on GoogLeNet	method
4x on CIFAR-10 )	method
On the much larger ImageNet22K dataset	method
We find that there exist conditions under which the intermediary obtains the highest proportion of benefits from targeting and	finding
in general the intermediary 's incentives regarding the type of consumer information to be used for targeting are misaligned with the incentives of firms and/or consumers Furthermore	finding
consumers ' surplus from targeting is higher when only specific types of personal information are made available during the targeting process	finding
We develop a three '' players model that includes firms	mechanism
consumers and an intermediary `` the ad exchange	mechanism
and analyze three scenarios that differ in the type of consumers ' data available during the targeting : a case in which only the horizontal information ( consumers ' brand preferences ) is available ; a case in which only vertical information ( consumers ' purchasing power ) is available ; a case in which both pieces of information are available	method
results are provided to demonstrate the advantages and properties of the proposed approach based on weighted kernel	finding
In contrast with the uniform kernel used in the previous work	mechanism
a weighted kernel is proposed	mechanism
where weight parameters serve to selectively incorporate sensors information into the fusion centers decision rule based on quality of sensors observations	mechanism
Furthermore weight parameters also serve as sensor selection parameters with nonzero parameters corresponding to sensors being selected	mechanism
By introducing the $ l_1 $ regularization on weight parameters into the risk minimization framework	mechanism
sensor selection is jointly performed with decision rules for sensors and the fusion center with the resulting optimal decision rule having only sparse nonzero weight parameters A gradient projection algorithm and a Gauss-Seidel algorithm are developed to solve the risk minimization problem	mechanism
which is nonconvex	mechanism
and both algorithms are shown to converge to critical points	mechanism
Conditions on the sample complexity to guarantee asymptotically small estimation error are characterized based on analysis of Rademacher complexity	mechanism
Connection between the probability of error and the risk function is also studied	mechanism
Numerical	method
We ultimately envision this technique being integrated into future smartwatches	background
allowing hand gestures and direct touch manipulation to work synergistically to support interactive tasks on small screens	background
Our wrist location achieved 97 % and 87 % accuracies on these gesture sets respectively	finding
while our arm location achieved 93 % and 81 %	finding
We present Tomo	mechanism
a wearable low-cost system using Electrical Impedance Tomography ( EIT ) This is achieved by measuring the cross-sectional impedances between all pairs of eight electrodes resting on a user 's skin	mechanism
Our approach is sufficiently compact and low-powered that we integrated the technology into a prototype wrist- and armband	mechanism
which can monitor and classify gestures in real-time	mechanism
We conducted a user study that evaluated two gesture sets	method
one focused on gross hand gestures and another using thumb-to-finger pinches	method
Next-generation information technologies will process unprecedented amounts of loosely structured data that overwhelm existing computing systems	background
1 000-fold	finding
N3XT by using new logic and memory technologies	mechanism
3D integration with fine-grained connectivity	mechanism
and new architectures for computation immersed in memory	mechanism
The rise of big data has led to new demands for machine learning ( ML ) systems to learn complex models	background
with millions to billions of parameters	background
that promise adequate capacity to digest massive datasets and offer powerful predictive analytics ( such as high-dimensional latent features	background
intermediate representations and decision functions ) thereupon	background
we present opportunities for ML researchers and practitioners to further shape and enlarge the area that lies between ML and systems	background
discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions	mechanism
These principles and strategies span a continuum from application	mechanism
to engineering and to theoretical research and development of big ML systems and architectures	mechanism
with the goal of understanding how to make them efficient	mechanism
generally applicable and supported with convergence and scaling guarantees	mechanism
They concern four key questions that traditionally receive little attention in ML research : By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs	mechanism
and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks	mechanism
Semantic search or text-to-video search in video is a novel and challenging problem in information and multimedia retrieval	background
We share our observations and lessons in building such a state-of-the-art system	background
which may be instrumental in guiding the design of the future system for video search and analysis	background
The novelty and practicality are demonstrated by where the proposed system achieves the best performance	finding
This paper presents a state-of-the-art system for event search without any user-generated metadata or example videos	mechanism
known as text-to-video search	mechanism
The system relies on substantial video content understanding and allows for searching complex events over a large collection of videos	mechanism
The proposed text-to-video search can be used to augment the existing text-to-text search for video	mechanism
the evaluation in NIST TRECVID 2014	method
We show that the mechanism results in significant gains on data from a national kidney exchange that includes 59 % of all US transplant centers	finding
We present a credit-based matching mechanism in particularthat is both strategy proof and efficient	mechanism
that is it guarantees truthful disclosure of donor-patient pairs from the transplant centers and results in the maximum global matching Furthermore	mechanism
the mechanism is individually rational in the sense that	mechanism
in the long run	mechanism
it guarantees each transplant center more matches than the center could have achieved alone	mechanism
The mechanism does not require assumptions about the underlying distribution of compatibility graphsa nuance that has previously produced conflicting results in other aspects of theoretical kidney exchange Our results apply not only to matching via 2-cycles : the matchings can also include cycles of any length and altruist-initiated chains	mechanism
which is important at least in kidney exchanges	mechanism
The mechanism can also be adjusted to guarantee immediate individual rationality at the expense of economic efficiency	mechanism
while preserving strategy proofness via the credits	mechanism
This circumvents a well-known impossibility result in static kidney exchange concerning the existence of an individually rational	mechanism
strategy-proof and maximal mechanism	mechanism
empirically	method
The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm	background
that won the 2014 Annual Computer Poker Competition	finding
beating each opponent with statistical significance	finding
We introduce a distributed version of the most commonly used equilibrium-finding algorithm	mechanism
counterfactual regret minimization ( CFR )	mechanism
The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint We introduce an algorithm while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and earth-mover 's distance	mechanism
Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency	mechanism
Prior approaches run slowly on this architecture	mechanism
Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency	mechanism
Finally we introduce a family of post-processing techniques that outperform prior ones	mechanism
We applied these techniques to generate an agent for two-player no-limit Texas Hold'em	method
called Tartanian7	method
The results from our studies indicate that the proposed SC-FAC model is reliable and accurately perform prior shape weak object segmentation	finding
The average performance of the mouth segmentation using proposed SC-FAC on 1918 images from the MBGC database under different illuminations	finding
expressions and complex background reaches to a Precision of 91	finding
30 % a Recall of 91	finding
32 % and an F-measure of 90	finding
62 %	finding
In this paper	mechanism
we propose a novel joint formulation of feature-based active contour ( FAC ) and prior shape constraints ( CS Our proposed SC-FAC model is able to robustly segment the lips/mouth that belongs to a given mouth shape space while minimizing the energy functional	mechanism
The shape space is defined by a 2D Modified Active Shape Model ( MASM ) whereas the active contour model is based on the Chan-Vese functional Our SC-FAC energy functional is able to overcome the drawback of noise while minimizing the fitting forces under the shape constraints HighlightsPropose a novel joint formulation of contour and shape for lips segmentation	mechanism
Robustly segment lips under challenging environment and complex background	mechanism
The energy functional is composed of 5 terms based on the Chan-Vese functional	mechanism
The shape space is defined by a 2D	mechanism
Experiments are conducted from the MBGC	mechanism
VidTIMIT JAFFE and LFW databases	mechanism
We conducted our experiments on images captured under challenging conditions such as varying illumination	method
low contrast facial expression	method
low resolution blurring	method
wearing beard/moustache and cosmetic affection from the MBGC	method
VidTIMIT JAFFE and LFW databases	method
Bayesian theory has provided a compelling conceptualization for perceptual inference in the brain	background
Central to Bayesian inference is the notion of statistical priors	background
To understand the neural mechanisms of Bayesian inference	background
we need to understand the neural representation of statistical regularities in the natural environment These findings demonstrate that there is a relationship between the functional connectivity observed in the visual cortex and the statistics of natural scenes	background
They also suggest that the Boltzmann machine can be a viable model for conceptualizing computations in the visual cortex and	background
as such can be used to predict neural circuits in the visual cortex from natural scene statistics	background
and found that the units in the model exhibited cooperative and competitive interactions	finding
forming a disparity association field	finding
analogous to the contour association field The cooperative and competitive interactions in the disparity association field are consistent with constraints of computational models for stereo matching	finding
and found the results to be consistent with neurophysiological data in terms of the functional connectivity measurements between disparity-tuned neurons in the macaque primary visual cortex	finding
a Boltzmann machine model	mechanism
We applied In addition	method
we simulated neurophysiological experiments on the model	method
AbstractEngineering analysis to quantify the effects of earthquake forces on the structural strength of components requires determining the damage mode and severity of the components	background
This study develops a building-information-modeling ( BIM ) based approach In the proposed approach	mechanism
the damage information is represented along with the geometric	mechanism
topological and structural information	mechanism
Transformation and reasoning mechanisms are proposed to utilize the information contained in the BIM	mechanism
The approach is validated on a case study building	method
which contains 42 damaged piers and spandrels	method
show that our approach produces significantly more accurate rankings than alternative approaches	finding
We present the first model From this viewpoint	mechanism
voting rules are seen as error-correcting codes : their goal is to correct errors in the input rankings and recover a ranking that is close to the ground truth We derive worst-case bounds on the relation between the average accuracy of the input votes	mechanism
and the accuracy of the output ranking	mechanism
Empirical results from real data	method
The use of deductive techniques	background
such as theorem provers	background
has several advantages in safety verification of hybrid systems	background
we present an extension to the deductive verification framework of differential dynamic logic by leveraging forward invariant sets provided by external methods	mechanism
such as numerical techniques and designer insights Our key contribution is a new inference rule	mechanism
the forward invariant cut rule	mechanism
introduced into the proof calculus of KeYmaera	mechanism
We demonstrate the cut rule in action on an example involving an automotive powertrain control systems	method
in which we make use of a simulation-driven numerical technique to compute a local barrier function	method
Kernel methods are ubiquitous tools in machine learning	background
They have proven to be effective in many domains and tasks	background
Furthermore we show that BaNK outperforms several other scalable approaches for kernel learning	finding
In this paper we introduce Bayesian nonparmetric kernel ( BaNK ) learning	mechanism
a generic data-driven framework We show that this framework can be used for performing both regression and classification tasks and scale to large datasets	mechanism
on a variety of real world datasets	method
When solving extensive-form games with large action spaces	background
typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective	background
When this occurs	background
a procedure is needed to interpret actions of the opponent that fall outside of our abstraction ( by mapping them to actions in our abstraction )	background
This is called an action translation mapping	background
our mapping performs competitively with the prior mappings	finding
We present a new mapping and has significantly lower exploitability than the prior mappings Furthermore	mechanism
we observe that the cost of this worst-case performance benefit ( low exploitability ) is not high in practice ; We also observe several paradoxes that can arise when performing action abstraction and translation ; for example	mechanism
we show that it is possible to improve performance by including suboptimal actions in our abstraction and excluding optimal actions	mechanism
against no-limit Texas Hold'em agents submitted to the 2012 Annual Computer Poker Competition	method
Problems of this nature arise in formal verification of continuous and hybrid dynamical systems	background
where there is an increasing need for methods to expedite formal proofs	background
The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting	finding
We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of benchmarks	method
How can we efficiently decompose a tensor into sparse factors	background
when the data do not fit in memory ?	background
enabling reproducibility of our work	background
indicate over 90p sparser outputs and 14 times faster execution	finding
with approximation error close to the current state of the art irrespective of computation and memory requirements	finding
demonstrating its effectiveness for data-mining practitioners	finding
In this work	mechanism
we propose P ar C ube	mechanism
a new and highly parallelizable method that is well suited to produce sparse approximations In particular	mechanism
we are the first to analyze the very large N ell dataset using a sparse tensor decomposition	mechanism
demonstrating that P ar C ube enables us to handle effectively and efficiently very large datasets Finally	mechanism
we make our highly scalable parallel implementation publicly available	mechanism
Experiments with even moderately large data We provide theoretical guarantees for the algorithms correctness and we experimentally validate our claims through extensive experiments	method
including four different real world datasets ( E nron	method
L bnl F acebook and N ell )	method
In the U	background
S	background
the current practice of analyzing the structural integrity of embankment dams relies primarily on manual a posteriori analysis of instrument data by engineers	background
leaving much room for improvement through the application of advanced data analysis techniques	background
In general KL performs better than MPCA and AR	finding
and delivers more consistent results throughout the different piezometers and anomaly scenarios	finding
Given that KL is a nonparametric technique	finding
the authors conclude that the prior assumptions about piezometer data do not always provide the best performance for anomaly prediction	finding
In this research	method
different types of anomaly detection techniques are examined in an effort Moreover	method
both the parametric ( Auto Regressive AR and Moving Principal Component Analysis MPCA ) and nonparametric ( Kullback-Leibler Divergence KL ) techniques are applied in order to test if the widely-held assumptions about piezometer data	method
i	method
e	method
linearity between piezometer data and pool levels	method
as well as normally distributed piezometer data	method
are necessary in the anomaly detection task	method
Modeling cell shape variation is critical to our understanding of cell biology The open-source tools provide a powerful basis for future studies of the molecular basis of cell organization	background
We find that these are frequently dependent on each other and show that tagged C1QBP reduces the correlation between cell and nuclear shape	finding
use this as the motivation for the development of combined cell and nuclear shape space models	mechanism
extending nonparametric cell representations to multiple-component three-dimensional cellular shapes and identifying modes of joint shape variation We learn a first-order dynamics model given shapes at a previous time point	mechanism
Using these methods	method
we explore the relationship between cell shape and nuclear shape	method
We use this to determine the effects of endogenous protein tags or drugs on the shape dynamics of cell lines and we demonstrate the ability to reconstruct shape spaces using a fraction of computed pairwise distances	method
Building automation systems are believed to hold the key to significantly reducing the average energy consumption of our residential and commercial building stock	background
which in the U	background
S	background
is responsible for 41 % of the total annual energy use in 2014	background
As these systems become more widespread and inexpensive	background
the complexity and challenges associated with their installation	background
maintenance and upkeep will increase	background
One of the primary challenges is the generation and update of the meta-data associated with the sensors and control points distributed throughout the facility Previous research has attempted to reduce the human input required to perform these activities	background
by leveraging different signal processing and statistical analysis approaches to infer the sensor types and locations from measurements and/or tags obtained through a BAS	background
provide recommendations for future work in this area	background
show the feasibility of applying data driven approaches in the real world	finding
We present the results of our study and	finding
In this paper	mechanism
we propose a meta-data inference framework	mechanism
Furthermore we evaluate the framework on two large buildings instrumented with thousands sensors and	method
and show that it converges to a stationary distribution	finding
In particular we show that	finding
depending on the local dynamical rule	finding
different network substructures	finding
such as hub or triangle subgraphs	finding
are more prone to failure	finding
The Dynamic Bond Percolation ( DBP ) process models	mechanism
through stochastic local rules	mechanism
the dependence of an edge $ ( a	mechanism
b ) $ in a network on the states of its neighboring edges Unlike previous models	mechanism
DBP does not assume statistical independence between different edges In applications	mechanism
this means for example that failures of transmission lines in a power grid are not statistically independent	mechanism
or alternatively relationships between individuals ( dyads ) can lead to changes in other dyads in a social network	mechanism
We consider the time evolution of the probability distribution of the network state	method
the collective states of all the edges ( bonds ) We use this distribution to study the emergence of global behaviors like consensus ( i	method
e	method
catastrophic failure or full recovery of the entire grid ) or coexistence ( i	method
e	method
some failed and some operating substructures in the grid	method
Image clustering and visual codebook learning are two fundamental problems in computer vision and they are tightly related	background
On one hand	background
a good codebook can generate effective feature representations which largely affect clustering performance	background
On the other hand	background
class labels obtained from image clustering can serve as supervised information to guide codebook learning	background
demonstrate the effectiveness of two models	finding
In this paper	mechanism
we propose a Double Layer Gaussian Mixture Model ( DLGMM ) In DLGMM	mechanism
two tasks are seamlessly coupled and can mutually promote each other	mechanism
Cluster labels and codebook are jointly estimated to achieve the overall best performance	mechanism
To incorporate the spatial coherence between neighboring visual patches	mechanism
we propose a Spatially Coherent DL-GMM which uses a Markov Random Field to encourage neighboring patches to share the same visual word label	mechanism
We use variational inference to approximate the posterior of latent variables and learn model parameters	mechanism
Experiments on two datasets	method
While prior research has studied the motivations of individuals to consume content on social media platforms Implications for research and practice are also discussed	background
Given that content creation efforts are driven not only by their personal preferences	mechanism
but also by the content creation decisions of others in the network neighbors	mechanism
we develop a new method with panel data	mechanism
We face a novel set of big data challenges	mechanism
i	mechanism
e	mechanism
both statistical and quantitative	mechanism
in estimating peer influence	mechanism
We face computational challenges in that we can not reasonably estimate peer influence over the entire YouTube network	mechanism
which has billions of nodes	mechanism
We employ graph sampling methods to address this issue	mechanism
Identification of social influence in large-scale social networks such as YouTube is difficult due to the interdependence in decisions of users	mechanism
correlations between the video 's observable and unobservable characteristics and attributes over time These patterns can not be modeled with existing autocorrelation models	mechanism
We design a new method	mechanism
the Network Auto-Probit Model with Fixed Effects ( NAFE )	mechanism
Despite the enormous medical impact of cancers and intensive study of their biology	background
detailed characterization of tumor growth and development remains elusive	background
This difficulty occurs in large part because of enormous heterogeneity in the molecular mechanisms of cancer progression	background
both tumor-to-tumor and cell-to-cell in single tumors	background
Advances in genomic technologies	background
especially at the single-cell level	background
are improving the situation	background
but these approaches are held back by limitations of the biotechnologies for gathering genomic data from heterogeneous cell populations and the computational methods for making sense of those data	background
One popular way to gain the advantages of whole-genome methods without the cost of single-cell genomics has been the use of computational deconvolution ( unmixing ) methods to reconstruct clonal heterogeneity from bulk genomic data	background
a key step in the process of accurately deconvolving tumor genomic data and inferring clonal heterogeneity from bulk data	background
We show that this new method substantially improves our ability to resolve discrete tumor subgroups	finding
Here we present a new method by better identifying subspaces corresponding to tumors produced from mixtures of distinct combinations of clonal subpopulations We develop a nonparametric clustering method based on medoidshift clustering for identifying subgroups of tumors expected to correspond to distinct trajectories of evolutionary progression	mechanism
on synthetic and real tumor copy-number data	method
Over the last decade	background
the looming power wall has spurred a flurry of interest in developing heterogeneous systems with hardware accelerators	background
On average the legacy code using our proposed MEmory Accelerated Library ( MEALib ) improves performance and energy efficiency for individual operations in Intel 's Math Kernel Library ( MKL ) by 38 and 75	finding
respectively	finding
MEALib attains more than 10 better energy efficiency	finding
Our accelerator design approach stems from the observation that many efficient and portable software implementations rely on high performance software libraries with well-established application programming interfaces ( APIs ) We propose the integration of hardware accelerators on 3D-stacked memory The fixed APIs with limited configurability simplify the design of the accelerators	mechanism
while ensuring that the accelerators have wide applicability	mechanism
With our software support that automatically converts library APIs to accelerator invocations	mechanism
an additional advantage of our approach is that library-based legacy code automatically gains the benefit of memory-side accelerators without requiring a reimplementation	mechanism
For a real-world signal processing application that employs Intel MKL	method
The study of representations invariant to common transformations of the data is important to learning	background
to illustrate and validate our methods	finding
In this paper	mechanism
we study kernels that are invariant to the unitary group while having theoretical guarantees We present a theoretically motivated alternate approach to the invariant kernel SVM Unlike previous approaches to the invariant SVM	mechanism
the proposed formulation solves both issues mentioned	mechanism
We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability	mechanism
We present experiments on the UCI ML datasets	method
The assembly of virus capsids proceeds by a complicated cascade of association and dissociation steps	background
the great majority of which can not be directly experimentally observed This has made capsid assembly a rich field for computational models	background
The results show that advances in both the data and the algorithms can improve model inference More informative data sources lead to high-quality fits for all methods	finding
but DFO methods show substantial advantages on less informative data sources that better represent current experimental practice	finding
Here we describe progress on fitting kinetic rate constants defining capsid assembly models to experimental data a model of time-resolved mass spectrometry data	mechanism
a technology that can be expected to provide much richer data than previously used static light scattering approaches	mechanism
We evaluate the merits of data-fitting methods based on derivative-free optimization ( DFO ) relative to gradient-based methods used in prior work	method
the resulting methods often perform as well or better than existing latent variable models	finding
while being substantially easier to train	finding
We present a convex approach Our approach builds upon recent advances in multivariate total variation regularization	mechanism
and seeks to learn a separate set of parameters for the distribution over the observations at each time point	mechanism
but with an additional penalty that encourages the parameters to remain constant over time	mechanism
We propose efficient optimization methods	mechanism
and a two-stage procedure under such models	mechanism
based upon kernel density estimation	mechanism
Finally we show on a number of real-world segmentation tasks	method
This typically arises in the tourism setting where attractions can often be bundled and sold as a package to visitors	background
While the problem of predicting future locations given the current and past trajectories is well-established	background
Our predictions show improved accuracies by at least 20	finding
one of which comes from the spatiotemporal analysis domain	finding
we take a radical approach by looking at it from an economic point of view	mechanism
We view an agent 's past trajectories as revealed preference ( RP ) data	mechanism
where the choice of locations is a solution to an optimisation problem according to some unknown utility function and subject to the prevailing prices and budget constraint We approximate the prices and budget constraint as the time costs to finish visiting the chosen locations We leverage on a recent line of work that has established algorithms to efficiently learn from RP data ( i	mechanism
e	mechanism
recover the utility functions ) and make predictions of future purchasing behaviours	mechanism
We adopt and adapt those work to our original setting while incorporating techniques from spatiotemporal analysis	mechanism
We experiment with real-world trajectory data collected from a theme park	method
in comparison with the baseline methods	method
Given a large collection of co-evolving online activities	background
such as searches for the keywords `` Xbox ''	background
`` PlayStation '' and `` Wii ''	background
how can we find patterns and rules ? Are these keywords related ? If so	background
are they competing against each other ? Can we forecast the volume of user activity for the coming month ?	background
show that ECOWEB is effective	finding
in that it can capture long-range dynamics and meaningful patterns such as seasonalities	finding
and practical in that it can provide accurate long-range forecasts	finding
ECOWEB consistently outperforms existing methods in terms of both accuracy and execution speed	finding
We present ECOWEB	mechanism
( i	mechanism
e	mechanism
Ecosystem on the Web )	mechanism
which is an intuitive model designed as a non-linear dynamical system Our second contribution is a novel	mechanism
parameter-free and scalable fitting algorithm	mechanism
ECOWEB-FIT that estimates the parameters of ECOWEB	mechanism
Extensive experiments on real data	method
Software lineage refers to the evolutionary relationship among a collection of software	background
The goal of software lineage inference is to recover the lineage given a set of program binaries	background
Software lineage can provide extremely useful information in many security scenarios such as malware triage and software vulnerability tracking	background
Our results reveal that partial order mismatches and graph arc edit distance often yield the most meaningful comparisons Even without assuming any prior information about the data sets	finding
ILINE proved to be effective in lineage inference -- it achieves a mean accuracy of over 84 % for goodware and over 72 % for malware in our data sets	finding
we build ILINE	mechanism
a system and also IEVAL	mechanism
a system	mechanism
We evaluated ILINE on two types of lineage -- straight line and directed acyclic graph -- with large-scale real-world programs : 1	method
777 goodware spanning over a combined 110 years of development history and 114 malware with known lineage collected by the DARPA Cyber Genome program We used IEVAL to study seven metrics to assess the diverse properties of lineage	method
in our experiments	method
With the rise of online social networks and smartphones that record the user 's location	background
a new type of online social network has gained popularity during the last few years	background
the so called Location-based Social Networks ( LBSNs )	background
In such networks	background
users voluntarily share their location with their friends via a check-in	background
In exchange they get recommendations tailored to their particular location as well as special deals that businesses offer when users check-in frequently	background
LBSNs started as specialized platforms such as Gowalla and Foursquare	background
however their immense popularity has led online social networking giants like Facebook to adopt this functionality	background
The spatial aspect of LBSNs directly ties the physical with the online world	background
creating a very rich ecosystem where users interact with their friends both online as well as declare their physical ( co- ) presence in various locations	background
In this work	mechanism
we propose to model and analyze LBSNs using Tensors and Tensor Decompositions	mechanism
powerful analytical tools that have enjoyed great growth and success in fields like Machine Learning	mechanism
Data Mining and Signal Processing alike	mechanism
In addition to Tensor Decompositions	mechanism
we use Signal Processing tools that have been previously used in Direction of Arrival ( DOA ) estimations	mechanism
in order	mechanism
Prior work has	background
among other techniques	background
used canonical correlation analysis to project pre-trained vectors in two languages into a common space	background
that our method outperforms prior work on multilingual tasks	finding
matches the performance of prior work on monolingual tasks	finding
and scales linearly with the size of the input data ( and thus the number of languages being embedded )	finding
We propose a simple and scalable method that is inspired by the notion that the learned vector representations should be invariant to translation between languages	mechanism
We show empirically	method
our algorithm performs favorably without the need for careful initialization	finding
We propose a spectral approach Our approach is grammarless we directly learn the bracketing structure of a given sentence without using a grammar model The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples	mechanism
Although finding the minimal latent tree is NP-hard in general	mechanism
for the case of projective trees we find that it can be found using bilexical parsing algorithms	mechanism
Empirically compared to the constituent context model of Klein and Manning ( 2002 )	method
show that this often improves running times by an order of magnitude or more vs	finding
existing approaches based on conic solvers	finding
We present Epsilon	mechanism
a system using fast linear and proximal operators	mechanism
As with existing convex programming frameworks	mechanism
users specify convex optimization problems using a natural grammar for mathematical expressions	mechanism
composing functions in a way that is guaranteed to be convex by the rules of disciplined convex programming Given such an input	mechanism
the Epsilon compiler transforms the optimization problem into a mathematically equivalent form consisting only of functions with ecient proximal operators|an intermediate representation we refer to as prox-ane form	mechanism
By reducing problems to this form	mechanism
Epsilon enables solving general convex problems using a large library of fast proximal and linear operators ;	mechanism
numerical examples on many popular problems from statistics and machine learning	method
it is shown that	finding
at every network agent	finding
$ \mathcal { CIWNLS } $ leads to consistent parameter estimates the distributed estimator is shown to yield order-optimal convergence rates	finding
i	finding
e	finding
as far as the order of pathwise convergence is concerned	finding
the local parameter estimates at each agent are as good as the optimal centralized nonlinear least-squares estimator that requires access to all the observations across all the agents at all times	finding
where the individual agents observe sequentially over time an independent and identically distributed time-series consisting of a nonlinear function of the true but unknown parameter corrupted by noise A distributed recursive estimator of the consensus+innovations type	mechanism
namely $ \mathcal { CIWNLS } $	mechanism
is proposed in which the agents update their parameter estimates at each observation sampling epoch in a collaborative way by simultaneously processing the latest locally sensed information ( innovations ) and the parameter estimates from other agents ( consensus ) in the local neighborhood conforming to a prespecified interagent communication topology	mechanism
Under rather weak conditions on the connectivity of the interagent communication and a global observability criterion Furthermore	method
under standard smoothness assumptions on the local observation functions	method
To benchmark the performance of the $ \mathcal { CIWNLS } $ estimator with that of the centralized nonlinear least-squares estimator	method
the asymptotic normality of the estimate sequence is established	method
and the asymptotic covariance of the distributed estimator is evaluated	method
Curriculum learning ( CL ) or self-paced learning ( SPL ) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training	background
The two methods share a similar conceptual learning paradigm	background
but differ in specific learning schemes	background
In CL the curriculum is predetermined by prior knowledge	background
and remain fixed thereafter	background
In SPL the curriculum is dynamically determined to adjust to the learning pace of the leaner and propose a unified framework named self-paced curriculum leaning ( SPCL )	mechanism
SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training	mechanism
In comparison to human education	method
SPCL is analogous to `` instructor-student-collaborative '' learning mode	method
as opposed to `` instructor-driven '' in CL or `` student-driven '' in SPL Empirically	method
we show that the advantage of SPCL on two tasks	method
Image and video classification research has made great progress through the development of handcrafted local features and learning based features	background
show that by focusing on the structure of CNNs	finding
rather than end-to-end training methods	finding
we are able to design an efficient and powerful video feature learning algorithm	finding
In this paper	mechanism
we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness	mechanism
We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks ( CNNs ) share the same convolution-pooling network structure	mechanism
We then propose a two-stream Convolutional ISA ( ConvISA ) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm	mechanism
Through custom designed network structures for pixels and optical flow	mechanism
our method also reflects distinctive characteristics of these two data sources	mechanism
As an example	method
we study the problem of designing efficient video feature learning algorithms for action recognition	method
Our experimental results on standard action recognition benchmarks	method
Early detection and precise characterization of emerging topics in text streams can be highly useful in applications such as timely and targeted public health interventions and discovering evolving regional business trends	background
Many methods have been proposed for detecting emerging events in text streams using topic modeling	background
On both tasks	finding
we find that Semantic Scan provides significantly better event detection and characterization accuracy than competing approaches	finding
while providing up to an order of magnitude speedup	finding
In this paper	mechanism
we describe Semantic Scan ( SS ) that has been developed specifically Semantic Scan integrates novel contrastive topic modeling with online document assignment and principled likelihood ratio-based spatial scanning to identify emerging events with unexpected patterns of keywords hidden in text streams	mechanism
This enables more timely and accurate detection and characterization of anomalous	mechanism
spatially localized emerging events Semantic Scan does not require manual intervention or labeled training data	mechanism
and is robust to noise in real-world text data since it identifies anomalous text patterns that occur in a cluster of new documents rather than an anomaly in a single new document	mechanism
We compare Semantic Scan to alternative state-of-the-art methods such as Topics over Time	method
Online LDA and Labeled LDA on two real-world tasks : ( i ) a disease surveillance task monitoring free-text Emergency Department chief complaints in Allegheny County	method
and ( ii ) an emerging business trend detection task based on Yelp reviews	method
Conditional Gaussian graphical models generalize the well-known Gaussian graphical models to conditional distributions to model the output network influenced by conditioning input variables	background
we show that our methods can solve one million dimensional problems to high accuracy in a little over a day on a single machine	finding
In this paper	mechanism
we propose a new optimization procedure based on a Newton method leading to drastic improvement in computation time compared to the previous methods We then extend our method to scale to large problems under memory constraints	mechanism
using block coordinate descent to limit memory usage while achieving fast convergence	mechanism
Using synthetic and genomic data	method
Unease over data privacy will retard consumer acceptance of IoT deployments	background
The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud	background
that interposes a locally-controlled software component called a privacy mediator on every raw sensor stream Each mediator is in the same administrative domain as the sensors whose data is being collected	mechanism
and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain This solution necessitates a logical point of presence for mediators within the administrative boundaries of each organization	mechanism
Such points of presence are provided by cloudlets	mechanism
which are small locally-administered data centers at the edge of the Internet that can support code mobility The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility	mechanism
and demonstrate our method	finding
We develop a parallel variational inference ( VI ) procedure we make use of the recently proposed nonparametric VI to facilitate an embarrassingly parallel VI procedure that can be applied to a wider scope of models	mechanism
including to nonconjugate models	mechanism
We derive our embarrassingly parallel VI algorithm	mechanism
analyze our method theoretically empirically on a few nonconjugate models	method
Conclusions : A solution meeting the specification of the use case described above could improve human monitoring efficiency with expedited warning of events requiring follow-up	background
including otherwise overlooked events with no syndromic indicators	background
This approach can remove obstacles to collaboration with efficient	background
minimal data-sharing and without costly overhead	background
Results : Direct communication between public health problem owners and analytic developers was informative to both groups and constructive for the solution development process	finding
The consultancy achieved refinement of the asyndromic detection challenge and of solution requirements	finding
Participants summarized and evaluated solution approaches and discussed dissemination and collaboration strategies	finding
Materials and Methods : Supported by the Defense Threat Reduction Agency Biosurveillance Ecosystem project	mechanism
the International Society for Disease Surveillance formed an advisory group to select tractable use case problems and convene inter-disciplinary consultancies to translate analytic needs into well-defined problems and to promote development of applicable solution methods	mechanism
The initial consultancys focus was a problem originated by the North Carolina Department of Health and its NC DETECT surveillance system : Derive a method for detection of patient record clusters worthy of follow-up based on free-text chief complaints and without syndromic classification	mechanism
Component tasks are the collection of epidemiologists use case problems	method
multidisciplinary consultancies to refine them	method
and dissemination of problem requirements and shareable datasets We describe an initial use case and consultancy as a concrete example and challenge to developers	method
we show that with significantly lower number of pairwise judgments and feature-engineering effort	finding
we can achieve competitive coreference performance	finding
In this paper	mechanism
we define the problem of coreference resolution in text as one of clustering with pairwise constraints where human experts are asked to provide pairwise constraints ( pairwise judgments of coreferentiality ) Further	mechanism
we describe an active learning strategy by asking the most informative questions to human experts at each step of coreference resolution	mechanism
Positing that these pairwise judgments are easy to obtain from humans given the right context	method
We evaluate this hypothesis and our algorithms on both entity and event coreference tasks and on two languages	method
The Restricted Isometric Property ( R	background
I	background
P	background
) is a very important condition for recovering sparse vectors from high dimensional space	background
Traditional methods often rely on R	background
I	background
P or its relaxed variants	background
The proposed algorithm converges geometrically	finding
achieves nearly optimal recovery bound O ( s2 log ( d ) ) where s is the sparsity and d is the nominal dimension	finding
We prove that when features exhibit cluster structures	mechanism
which often happens in real applications	mechanism
we are able to recover the sparse vector consistently	mechanism
The consistency comes from our proposed density correction algorithm	mechanism
which removes the variance of estimated cluster centers using cluster density	mechanism
In this paper	method
we study the sparse recovery problem in which the feature matrix is strictly non-R	method
I	method
P	method
We demonstrate that our methods significantly improve the performance of state-of-the-art motion features	finding
We propose two well-motivated ranking-based methods First	mechanism
as an improvement over the classic power normalization method	mechanism
we propose a parameter-free ranking technique called rank normalization ( RaN )	mechanism
RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD	mechanism
Second inspired by curriculum learning	mechanism
we introduce a training-free re-ranking technique called multi-class iterative re-ranking ( MIR )	mechanism
MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly	mechanism
on six real-world datasets	method
Propagation of contagion in networks depends on the graph topology The only known analytical characterization of the equilibrium distribution of this process is for complete networks	background
For large networks with arbitrary topology	background
it is infeasible to numerically solve for the equilibrium distribution since it requires solving the eigenvalue-eigenvector problem of a matrix that is exponential in N	background
the size of the network	background
We confirm this result	finding
on static undirected	mechanism
finite-size networks	mechanism
This is a contact process with nonzero exogenous infection rate ( also known as the { \epsilon } -SIS	mechanism
{ \epsilon } susceptible-infected-susceptible	mechanism
model [ 1 ] ) We show that	mechanism
for a certain range of the network process parameters	mechanism
the equilibrium distribution of the extended contact process on arbitrary	mechanism
finite-size networks is well approximated by the equilibrium distribution of the scaled SIS process	mechanism
which we derived in closed-form in prior work	mechanism
We use this approximation to decide	mechanism
in polynomial-time which agents and network substructures are more susceptible to infection by the extended contact process	mechanism
with numerical simulations comparing the equilibrium distribution of the extended contact process with that of a scaled SIS process	method
The widespread use of social networks enables the rapid diffusion of information	background
e	background
g	background
news among users in very large communities	background
It is a substantial challenge to be able to observe and understand such diffusion processes	background
which may be modeled as networks that are both large and dynamic A key tool in this regard is data summarization	background
However few existing studies aim to summarize graphs/networks for dynamics	background
Dynamic networks raise new challenges not found in static settings	background
including time sensitivity and the needs for online interestingness evaluation and summary traceability	background
which render existing techniques inapplicable	background
The study offers insight into the effectiveness and design properties of OSNet	background
Based on the concepts of diffusion radius and scope	mechanism
we define interestingness measures for dynamic networks	mechanism
and we propose OSNet	mechanism
an online summarization framework	mechanism
We report on extensive experiments with both synthetic and real-life data	method
Session types provide a means to prescribe the communication behavior between concurrent message-passing processes However	background
in a distributed setting	background
some processes may be written in languages that do not support static typing of sessions or may be compromised by a malicious intruder	background
violating invariants of the session types	background
We prove that dynamic monitoring does not change system behavior for welltyped processes	finding
and that one of an indicated set of possible culprits must have been compromised in case of an alarm	finding
We present a system of in the case when the monitor detects an undesirable action and an alarm is raised	mechanism
which has guaranteed convergence and great scalability : close to 6 times faster on instance of ImageNet data set when run with 6 machines	finding
shown	finding
We propose a distributed approach The proposed scheme is close to optimally scalable in terms of number of machines	mechanism
and guaranteed to converge to the same optima as the undistributed setting	mechanism
The convergence and scalability of the distributed setting is The convergence analysis provides novel insights into this complex learning scheme	mechanism
including : 1 ) layerwise convergence	mechanism
and 2 ) convergence of the weights in probability	mechanism
theoretically empirically empirically across di ? erent datasets ( TIMIT and ImageNet ) and machine learning tasks ( image classi ? cation and phoneme extraction )	method
results of consistency under orthogonality and appropriate handling of redundant features	finding
we demonstrate on synthetic data that the Lass-0 solutions are closer to the true sparse support than L1 regularization models	finding
Lass-0 finds more parsimonious solutions that L1 regularization while maintaining similar predictive accuracy	finding
using convex relaxation of L1 regularization	mechanism
also known as the Lasso	mechanism
as an initialization step Our algorithm	mechanism
the Lass-0 ( `` Lass-zero '' )	mechanism
uses a computationally efficient stepwise search to determine a locally optimal L0 solution given any L1 regularization solution	mechanism
We present theoretical Empirically Additionally	method
in real-world data	method
Overall this technique extends the capabilities of 3D printing in a new and interesting way	background
without requiring any new hardware	background
demonstrating the immediate feasibility of our approach using a low cost	finding
commodity printer	finding
We introduce a technique by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling Our approach offers a range of design parameters for controlling the properties of single strands and also of hair bundles	mechanism
We further detail a list of post-processing techniques for refining the behavior and appearance of printed strands	mechanism
We provide several examples of output	method
One typically proves infeasibility in satisfiability/ constraint satisfaction ( or optimality in integer programming ) by constructing a tree certificate	background
We explore the power of a simple paradigm	mechanism
that of throwing random darts into the assignment space and then This method seems to work well when the number of short certificates of infeasibility is moderate	mechanism
suggesting that the overhead of throwing darts is more than paid for by the information gained by these darts	mechanism
Smartwatches are becoming increasingly powerful	background
but limited input makes completing complex tasks impractical	background
and found it was effective at producing reasonable drafts	finding
Our WearWrite system introduces a new paradigm not through new hardware or input methods	mechanism
but by directing a crowd to work on their behalf from their wearable device WearWrite lets authors give writing instructions and provide bits of expertise and big picture directions from their smartwatch	mechanism
while crowd workers actually write the document on more powerful devices	mechanism
We used this approach to write three academic papers	method
In applied fields	background
practitioners hoping to apply causal structure learning or causal orientation algorithms face an important question : which independence test is appropriate for my data ? In the case of real-valued iid data	background
linear dependencies and Gaussian error terms	background
partial correlation is sufficient	background
But once any of these assumptions is modified	background
the situation becomes more complex	background
We show how properly accounting for spatial and temporal variation can lead to more reasonable causal graphs	finding
We also show how highly structured data	finding
like images and text	finding
can be used in a causal inference framework using a novel structured input/output Gaussian process formulation	finding
Inspired by the success of Gaussian process regression for handling non-iid observations in a wide variety of areas and by the usefulness of the Hilbert-Schmidt Independence Criterion ( HSIC )	mechanism
a kernel-based independence test	mechanism
we propose a simple framework to address all of these issues : first	mechanism
use Gaussian process regression to control Second	mechanism
use HSIC	mechanism
We illustrate this on two classic datasets	method
one spatial the other temporal	method
that are usually treated as iid	method
We demonstrate this idea on a dataset of translated sentences	method
trying to predict the source language	method
An indoor ultrasonic location tracking system that can utilize standard audio speakers The method uses a communication scheme based on linearly increasing frequency modulated chirps in the audio bandwidth just above the human hearing frequency range where mobile devices are still sensitive The method uses gradual frequency and amplitude changes that minimize human perceivable ( psychoacoustic ) artifacts derived from the non-ideal impulse response of audio speakers	mechanism
Chirps also benefit from Pulse Compression	mechanism
which improves ranging resolution and resilience to both Doppler shifts and multi-path propagation that plague indoor environments	mechanism
The method supports the decoding of multiple unique identifier packets simultaneously	mechanism
A Time-Difference-of-Arrival pseudo-ranging technique allows for localization without explicit synchronization with the broadcasting infrastructure	mechanism
An alternate received signal strength indicator based localization technique allows less accurate localization at the benefit of sparser transmission infrastructure	mechanism
Online discussion forums are complex webs of overlapping subcommunities ( macrolevel structure	background
across threads ) in which users enact different roles depending on which subcommunity they are participating in within a particular time point ( microlevel structure	background
within threads )	background
we demonstrate that our model can provide useful explanations of microlevel and macrolevel user presentation characteristics in different communities using the topics discovered from posts	finding
we show that our model does better than MMSB and LDA in predicting user reply structure within threads that the proposed active sub-network discovery model is stable and recovers the original parameters of the experimental setup with high probability	finding
we develop a scalable algorithm based on stochastic variational inference and leverage topic models ( LDA ) along with mixed membership stochastic block ( MMSB ) models	mechanism
We evaluate our model on three large-scale datasets	method
Cancer-ThreadStarter ( 22K users and 14	method
4K threads )	method
Cancer-NameMention ( 15	method
1K users and 12	method
4K threads ) and StackOverFlow ( 1	method
19 million users and 4	method
55 million threads ) Qualitatively	method
Quantitatively In addition	method
we demonstrate via synthetic data experiments	method
showing that rising 5th and 6th graders can understand the lawfulness of Kodu programs	finding
We also discuss some misconceptions students may develop about Kodu	finding
their causes and potential remedies	finding
We present an analysis of assessment data	method
While the version of LegionTools discussed here focuses on Amazon 's Mechanical Turk platform	background
it can be easily extended to other platforms as APIs become available	background
We introduce LegionTools	mechanism
a toolkit and interface for managing large	mechanism
synchronous crowds of online workers for experiments This poster contributes the design and implementation of a state-of-the-art crowd management tool	mechanism
along with a publicly-available	mechanism
open-source toolkit that future system builders can use We describe the toolkit itself	mechanism
along with the underlying design rationale	mechanism
in order to make it clear to the community of system builders at UIST when and how this tool may be beneficial to their project	mechanism
We also describe initial deployments of the system in which workers were synchronously recruited to support real-time crowdsourcing systems	method
including the largest synchronous recruitment and routing of workers from Mechanical Turk that we are aware of	method
Obtaining per-device energy consumption estimates in Non-Intrusive Load Monitoring ( NILM ) has proven to be a challenging task	background
We show that reliable energy estimates can be obtained by crowdsourcing the results from using 1	finding
456 event detectors applied to the publicly available BLUED dataset	finding
We present Power Consumption Clustered Non-Intrusive Load Monitoring ( PCC-NILM )	mechanism
a relaxation of the NILM problem The Approximate Power Trace Decomposition Algorithm ( APTDA ) is presented as an unsupervised	mechanism
Recently diversity-inducing regularization methods for latent variable models ( LVMs )	background
which encourage the components in LVMs to be diverse	background
have been studied	background
which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis	finding
We use neural network ( NN ) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error	mechanism
In addition to theoretical analysis	method
we also present empirical study	method
where we identify previously unknown heterogeneous changes in space and time	finding
We present a scalable Gaussian process model We use Random Kitchen Sink features to flexibly define a change surface in combination with expressive spectral mixture kernels to capture the complex statistical structure Finally	mechanism
through the use of novel methods for additive non-separable kernels	mechanism
we can scale the model to large datasets	mechanism
We demonstrate the model on numerical and real world data	method
including a large spatio-temporal disease dataset	method
User-generated online reviews can play a significant role in the success of retail products	background
hotels restaurants etc	background
where FRAUDEAGLE successfully reveals fraud-bots in a large online app review database	finding
We propose a fast and effective framework	mechanism
FRAUDEAGLE Our method has several advantages : ( 1 ) it exploits the network effect among reviewers and products	mechanism
unlike the vast majority of existing methods that focus on review text or behavioral analysis	mechanism
( 2 ) it consists of two complementary steps ; scoring users and reviews for fraud detection	mechanism
and grouping for visualization and sensemaking	mechanism
( 3 ) it operates in a completely unsupervised fashion requiring no labeled data	mechanism
while still incorporating side information if available	mechanism
and ( 4 ) it is scalable to large datasets as its run time grows linearly with network size	mechanism
We demonstrate the effectiveness of our framework on syntheticand real datasets ;	method
Networked or telematic music performances take many forms	background
ranging from small laptop ensembles using local area networks to long-distance musical collaborations using audio and video links	background
Two important concerns for any networked performance are :	background
which achieved a coordinated performance involving 68 computer musicians	finding
each with their own connection to the network	finding
are described	finding
A recent project	mechanism
the Global Net Orchestra	mechanism
is described	mechanism
the technical aspects of the project	mechanism
In this demo we present Perseus	mechanism
a large-scale system by supporting the coupled summarization of graph properties and structures	mechanism
guiding attention to outliers	mechanism
and allowing the user to interactively explore normal and anomalous node behaviors Specifically	mechanism
Perseus provides for the following operations : 1 ) ( e	mechanism
g	mechanism
degree PageRank real eigenvectors ) by performing scalable	mechanism
offline batch processing on Hadoop ; 2 ) ; 3 ) ; 4 )	mechanism
by incrementally revealing its neighbors	mechanism
In our demonstration	method
we invite the audience to interact with Perseus to explore a variety of multi-million-edge social networks including a Wikipedia vote network	method
a friendship/foeship network in Slashdot	method
and a trust network based on the consumer review website Epinions	method
com	method
The long-term goal of connecting scales in biological simulation can be facilitated by scale-agnostic methods	background
show that although WE has important limitations	finding
it can achieve performance significantly exceeding standard parallel simulationby orders of magnitude for some observables	finding
We demonstrate that the weighted ensemble ( WE ) strategy	mechanism
initially developed for molecular simulations The WE approach runs an ensemble of parallel trajectories with assigned weights and uses a statistical resampling strategy of replicating and pruning trajectories to focus computational effort on difficult-to-sample regions	mechanism
The method can also generate unbiased estimates of non-equilibrium and equilibrium observables	mechanism
sometimes with significantly less aggregate computing time than would be possible using standard parallelization Here	mechanism
we use WE to orchestrate particle-based kinetic Monte Carlo simulations	mechanism
which include spatial geometry ( e	mechanism
g	mechanism
of organelles plasma membrane ) and biochemical interactions among mobile molecular species	mechanism
We study a series of models exhibiting spatial	method
temporal and biochemical complexity and	method
Besides the application to password generation	background
our proposed Human Usability Model ( HUM ) will have other applications	background
We show that our password generation methods are humanly computable and	finding
to a well-defined extent	finding
machine uncrackable	finding
Then motivated by the special case of password creation	mechanism
we propose a collection of well-defined password-generation methods	mechanism
For the proof of security	mechanism
we posit that password generation methods are public	mechanism
but that the humans privately chosen seed is not	mechanism
and that the adversary will have observed only a few input-output pairs	mechanism
The spatial pyramid and its variants have been very popular feature models due to their success in balancing spatial location encoding and spatial invariance	background
results show that	finding
despite its simplicity	finding
this method achieves comparable or better results than spatio-temporal pyramid	finding
This paper introduces the space-time extended descriptor	mechanism
a simple but efficient alternative way Instead of only coding motion information and leaving the spatio-temporal location to be represented at the pooling stage	mechanism
location information is used as part of the encoding step	mechanism
This method is a much more effective and efficient location encoding method as compared to the fixed grid model because it avoids the danger of over committing to artificial boundaries and its dimension is relatively low	mechanism
Experimental on several benchmark datasets	method
Suppose you are a teacher	background
and have to convey a set of object-property pairs ( 'lions eat meat '	background
or 'aspirin is a blood-thinner ' )	background
A good teacher will convey a lot of information	background
with little effort on the student side	background
Specifically given a list of objects ( like animals or medical drugs ) and their associated properties	background
what is the best and most intuitive way to convey this information to the student	background
without the student being overwhelmed ? A related	background
harder problem is : how can we assign a numerical score to each lesson plan ( i	background
e	background
way of conveying information ) ?	background
it is effective achieving excellent results on real data	finding
both with respect to our proposed metric	finding
but also with respect to encoding length	finding
demonstrate the effectiveness of HYTRA	finding
and we provide a metric for comparing different approaches based on information theory	mechanism
We also design a multi-pronged algorithm	mechanism
HYTRA for this problem	mechanism
Our proposed HYTRA is scalable ( near-linear in the dataset size )	mechanism
and it is intuitive	mechanism
conforming to well-known educational principles	mechanism
such as grouping related concepts	mechanism
and `` comparing '' and `` contrasting ''	mechanism
Experiments on real and synthetic datasets	method
The engineering analysis for determining the remaining seismic capacity of buildings following earthquakes requires performing structural calculations	background
observations of the actual damage	background
and applying extensive engineering judgment	background
Additionally the analysis should often be performed under stringent time requirements	background
The results of the study can be used to develop formal representation of damage information in information models and potentially allow better allocation of data collection time in the field	background
The study showed that the information required to represent the damaged conditions can be grouped under five broad categories and using seventeen damage parameters	finding
showed that the damage parameters have varying degrees of importance	finding
The damage descriptions for seven common damage modes of structural walls were studied by employing the affinity diagramming method	method
A sensitivity analysis	method
Differential dynamic logic is a logic for specifying and verifying safety	background
liveness and other properties about models of cyber-physical systems	background
Theorem provers based on differential dynamic logic have been used to verify safety properties for models of self-driving cars and collision avoidance protocols for aircraft	background
Examples include : an unambiguous separation between proof checking and proof search	background
the ability to extract program traces corresponding to counter-examples	background
and synthesis of surely-live deterministic programs from liveness proofs for nondeterministic programs	background
This paper presents a differential dynamic logic The resulting logic extends both the syntax and semantics of differential dynamic logic with proof terms -- syntactic representations of logical deductions the logic allows equivalence rewriting deep within formulas and supports both uniform renaming and uniform substitutions	mechanism
and observe significant speedups over competing state-of-the-art ( and synchronous ) methods	finding
the former on shared memory machines with mini-batching	mechanism
and the latter in a delayed update framework	mechanism
In both cases	mechanism
we perform computations asynchronously whenever possible	mechanism
We assume block-separable constraints as in Block-Coordinate Frank-Wolfe ( BCFW ) method ( Lacoste-Julien et al	mechanism
2013 ) but our analysis subsumes BCFW and reveals problemdependent quantities that govern the speedups of our methods over BCFW	mechanism
A notable feature of our algorithms is that they do not depend on worst-case bounded delays	mechanism
but only ( mildly ) on expected delays	mechanism
making them robust to stragglers and faulty worker threads	mechanism
We present experiments on structural SVM and Group Fused Lasso	method
demonstrate that our approach significantly outperforms the compared baselines	finding
In this work	mechanism
we introduce Video Question Answering in temporal domain We present an encoder-decoder approach using Recurrent Neural Networks to learn temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions	mechanism
We explore approaches for finer understanding of video content using question form of `` fill-in-the-blank ''	method
and managed to collect 109	method
895 video clips with duration over 1	method
000 hours from TACoS	method
MPII-MD MEDTest 14 datasets	method
while the corresponding 390	method
744 questions are generated from annotations Extensive experiments	method
Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet	background
present a framework where image annotation and image retrieval are considered as the special cases Specifically	mechanism
the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables	mechanism
we propose a new max margin structure learning approach called Enhanced Max Margin Learning ( EMML ) framework	mechanism
which is much more efficient with a much faster convergence rate than the existing max margin learning methods	mechanism
as verified through empirical evaluations	mechanism
Furthermore we apply EMML framework that is highly scalable in the sense that the query response time is independent of the database scale The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database	mechanism
and excels many existing multimodal data mining methods in the literature that do not scale up at all	mechanism
The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases	method
and show its use- fulness for nontrivial temporal properties of hybrid systems solving an open problem formulated in previous work	finding
The differential temporal dynamic logic dTL 2 is a logic It combines differential dynamic logic with temporal logic The logic dTL 2 supports some linear time temporal properties of LTL	mechanism
It extends differential temporal dynamic logic dTL with nested temporalities	mechanism
We take particular care to handle the case of alternating universal dynamic and existential temporal modalities and its dual	mechanism
We provide a semantics and a proof system for the logic dTL 2	method
The interest in distributed control methods for power systems is motivated by the need for scalable solutions to handle the coordination of an increasing number of distributed resources	background
This paper presents a fully distributed multilevel method Our proposed approach constitutes a distributed iterative mechanism to solve the first order optimality conditions of the DC-OPF problem using the fact that optimality conditions involve local variable couplings The proposed distributed structure requires each bus to update a few local variables and exchange information with neighboring buses Our multilevel distributed approach distributes the computation at several levels	mechanism
i	mechanism
e	mechanism
nodes subareas and areas	mechanism
It allows for synchronous information exchanges	mechanism
i	mechanism
e	mechanism
after each iteration	mechanism
at the nodal level and asynchronous communication	mechanism
i	mechanism
e	mechanism
after multiple iterations	mechanism
between subareas and areas	mechanism
To define meaningful subareas	mechanism
we are using a graph theoretic partitioning method derived from an epidemics model	mechanism
We compare the performance of the proposed partitioning method over a random partitioning method using the IEEE 118-bus system	method
Preliminary results show that our method allows us to pinpoint locations of co-behavior for traffic in the Manhattan road network	finding
We present a spectral analysis of taxi movement based on the graph Fourier transform	mechanism
which necessitates the spectral decomposition of a large directed	mechanism
sparse matrix Important considerations toward handling this matrix are discussed	mechanism
Plug-meters benefit many grid and building-level energy management applications like automated load control and load scheduling However	background
installing and maintaining large and/or long term deployments of such meters requires assignment and updating of the identity ( labels ) of electrical loads connected to them	background
Specifically we carry out tests on PLAID	method
a publicly available high-frequency dataset of hundreds of residential appliances By examining how the classification accuracy changes with sampling frequency	method
we also explore the computational complexity of these techniques	method
Many big data applications collect large numbers of time series A first task in analyzing such data is to find a low- dimensional representation	background
a graph which faithfully describes relations among the measured processes and through time	background
The processes are often affected by a relatively small number of unmeasured trends	background
This paper presents a computationally tractable algorithm from the collected data	mechanism
The algorithm is demonstrated on simulated time series datasets	method
Metering of electricity consumption	background
both at the building-level and appliance-level	background
provides stakeholders like residents	background
facility managers building owners	background
etc	background
with information requisite to engage in energy efficient practices	background
find that the inferred energy values have an average error of 10	finding
9 %	finding
In this paper	mechanism
we utilize a framework that performs the step of energy estimation for load disaggregation The framework combines data from these contactless sensors and aggregate metering	mechanism
in order to virtually meter the electricity consumption of specific appliances The solution requires minimal calibration	mechanism
and is easily performed using commercially available sensors	mechanism
We test it in a commercial building with 6 appliances that are monitored using magnetic field sensors and	method
This paper presents the first reported in-situ reconfiguration of a narrowband CMOS low noise amplifier ( LNA ) using a GeTe phase-change ( PC ) switch In this work	mechanism
we present a robust realization of a reconfigurable 3/5 GHz LNA designed and fabricated in a 0	mechanism
13 m CMOS process and flip-chip integrated with a four-terminal PC switch fabricated using an in-house process	mechanism
Devices can be made more intelligent if they have the ability to sense their surroundings and physical configuration	background
and demonstrates high accuracy	finding
Instead we use speakers and microphones already present in a wide variety of devices Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment	mechanism
chiefly the materials and geometry of proximate surfaces	mechanism
We offer several example uses	method
two of which we implemented as self-contained demos	method
and conclude with an evaluation that quantifies their performance	method
People are more creative at solving difficult design problems when they use relevant examples from outside of the problem s domain as inspirations	background
Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own	finding
or drew inspiration from domains closer to or not sharing structural correspondence with the original problem	finding
In this paper	mechanism
we demonstrate an approach in which non-experts identify domains that have the potential	mechanism
We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains	method
To study signals on networks	background
to detect epidemics	background
or to predict blackouts	background
we need to understand network topology and its impact on the behavior of network processes	background
The high dimensionality of large networks presents significant analytical and computational challenges ; only specific network structures have been studied without approximation	background
We introduce the network effect ratio	mechanism
3D-stacked integration of DRAM and logic layers using through-silicon via ( TSV ) technology has given rise to a new interpretation of near-data processing ( NDP ) concepts that were proposed decades ago	background
However processing capability within the stack is limited by stringent power and thermal constraints	background
Simple processing mechanisms with intensive memory accesses	background
such as data reorganization	background
are an effective means of exploiting 3D stacking-based NDP	background
Data reorganization handled completely in memory improves the host processor 's memory access performance	background
This article details data reorganization performed in parallel with host memory accesses	mechanism
providing mechanisms	mechanism
How do people interact with their Facebook wall ? At a high level	background
this question captures the essence of our work	background
While most prior efforts focus on Twitter	background
the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study	background
Our work provides a solid step towards a systematic and quantitative wall-centric profiling of Facebook user activity	background
Our key results can be summarized in the following points	finding
First we find that many wall activities	finding
including number of posts	finding
number of likes	finding
number of posts of type photo	finding
etc	finding
can be described by the PowerWall distribution	finding
What is more surprising is that most of these distributions have similar slope	finding
with a value close to 1 ! Second	finding
we show how our patterns and metrics can help us spot surprising behaviors and anomalies	finding
For example we find a user posting every two days	finding
exactly the same count of posts ; another user posting at midnight	finding
with no other activity before or after	finding
We propose PowerWall	mechanism
a lesser known heavy-tailed distribution	mechanism
We conduct an extensive study of roughly 7K users over three years during four month intervals each year	method
Biomedical scientists have invested significant effort into making it easy to perform lots of experiments quickly and cheaply	background
These high throughput methods are the workhorses of modern systems biology efforts	background
However we simply can not perform an experiment for every possible combination of different cell type	background
genetic mutation and other conditions	background
In practice this has led researchers to either exhaustively test a few conditions or targets	background
or to try to pick the experiments that best allow a particular problem to be explored	background
But which experiments should we pick ? The ones we think we can predict the outcome of accurately	background
the ones for which we are uncertain what the results will be	background
or a combination of the two ? Humans are not particularly well suited for this task because it requires reasoning about many possible outcomes at the same time	background
However computers are much better at handling statistics for many experiments	background
and machine learning algorithms allow computers to learn how to make predictions and decisions based on the data theyve previously processed The next challenge is to apply these methods to reduce the cost of achieving the goals of large projects	background
such as The Cancer Genome Atlas	background
showed that the active learning approach outperforms strategies a human might use	finding
even when the potential outcomes of individual experiments are not known beforehand	finding
Now Naik et al	mechanism
have performed cell biology experiments in which experiments were chosen by an active learning algorithm and then performed using liquid handling robots and an automated microscope	mechanism
The key idea behind the approach is that you learn more from an experiment you cant predict ( or that you predicted incorrectly ) than from just confirming your confident predictions	mechanism
The results of the robot-driven experiments	method
In large-scale complex networks	background
the underlying nonlinear dynamical system is high-dimensional and performing qualitative analysis of the differential equation becomes prohibitive	background
The study of such systems is often deferred to numerical simulations or local analysis about equilibrium points of the system	background
in a network modeled by the classical logistic ordinary differential equations In this paper	mechanism
we extend the work developed in [ 1 ]	mechanism
the weaker strain dies out regardless of the initial conditions if its maximum in-flow rate of infection across nodes is smaller than the minimum in-flow rate of the stronger strain	mechanism
We bound any solution of the logistic ODE by one- dimensional solutions over certain homogeneous networks	mechanism
for which the system is well understood Our global stability approach via bounds readily applies to the discrete-time logistic model counterpart	mechanism
The preferred treatment for kidney failure is a transplant ; however	background
demand for donor kidneys far outstrips supply	background
Kidney exchange an innovation where willing but incompatible patient-donor pairs can exchange organsvia barter cycles and altruist-initiated chainsprovides a life-saving alternative	background
Typically fielded exchanges act myopically	background
considering only the current pool of pairs when planning the cycles and chains	background
It results in higher values of the objective it yields better solutions for the efficient objective ( which does not incorporate equity ) than traditional myopic matching that uses the efficiency objective	finding
Motivated by our experience running the computational side of a large nationwide kidney exchange	mechanism
we present FUTURE-MATCH	mechanism
a framework FUTUREMATCH takes as input a high-level objective ( e	mechanism
g	mechanism
`` maximize graft survival of transplants over time '' ) decided on by experts	mechanism
then automatically ( i ) learns based on data how to make this objective concrete and ( ii ) learns the `` means '' to accomplish this goala task	mechanism
in our experience	mechanism
that humans handle poorly It uses data from all live kidney transplants in the US since 1987 to learn the quality of each possible match ; it then learns the potentials of elements of the current input graph offline ( e	mechanism
g	mechanism
potentials of pairs based on features such as donor and patient blood types )	mechanism
translates these to weights	mechanism
and performs a computationally feasible batch matching that incorporates dynamic	mechanism
failure-aware considerations through the weights	mechanism
We validate FUTUREMATCH on real fielded exchange data Furthermore	method
even under economically inefficient objectives that enforce equity	method
Humans rely on eye gaze and hand manipulations extensively in their everyday activities	background
Most often users gaze at an object to perceive it and then use their hands to manipulate it	background
results show that gaze+gesture can outperform systems using gaze or gesture alone	finding
and in general	finding
approach the performance of `` gold standard '' input systems	finding
such as the mouse and trackpad	finding
We propose applying a multimodal	mechanism
gaze plus free-space gesture approach We show the input methods are highly complementary	mechanism
mitigating issues of imprecision and limited expressivity in gaze-alone systems	mechanism
and issues of targeting speed in gesture-alone systems	mechanism
We extend an existing interaction taxonomy that naturally divides the gaze+gesture interaction space	mechanism
which we then populate with a series of example interaction techniques to illustrate the character and utility of each method	mechanism
We contextualize these interaction techniques in three example scenarios	method
In our user study	method
we pit our approach against five contemporary approaches ;	method
There is often a large disparity between the size of a game we wish to solve and the size of the largest instances solvable by the best algorithms ; for example	background
a popular variant of poker has about 10165 nodes in its game tree	background
while the currently best approximate equilibrium-finding algorithms scale to games with around 1012 nodes	background
In order to approximate equilibrium strategies in these games	background
the leading approach is to create a sufficiently small strategic approximation of the full game	background
called an abstraction	background
and to solve that smaller game instead	background
The leading abstraction algorithm for imperfect-information games generates abstractions that have imperfect recall and are distribution aware	background
using k-means with the earth mover 's distance metric to cluster similar states together	background
A distribution-aware abstraction groups states together at a given round if their full distributions over future strength are similar ( as opposed to	background
for example just the expectation of their strength )	background
The leading algorithm considers distributions over future strength at the final round of the game	background
show that our algorithm improves performance over the previously best approach	finding
We present the first algorithm using earth mover 's distance	mechanism
Experiments on no-limit Texas Hold'em	method
Non-technical loss ( NTL ) represents a major challenge when providing reliable electrical service in developing countries	background
where it often accounts for 11-15 % of total generation capacity [ 1 ]	background
NTL is caused by a variety of factors such as theft	background
unmetered homes and inability to pay which at volume can lead to system instability	background
grid failure and major financial losses for providers	background
We show that the model can be used to determine uncertainty bounds that can help in separating NTL from total losses	finding
Our approach models the primary sources of state uncertainty including line losses	mechanism
transformer losses meter calibration error	mechanism
packet loss and sample synchronization error	mechanism
We conduct an extensive data-driven simulation on 72 days of wireless meter data from a 430-home microgrid deployed in Les Anglais	method
Haiti	method
For the important task of binocular depth perception from complex natural-image stimuli	background
the neurophysiological basis for disambiguating multiple matches between the eyes across similar features has remained a long-standing problem	background
Recurrent interactions among binocular disparity-tuned neurons in the primary visual cortex ( V1 ) could play a role in stereoscopic computationsbyalteringresponsesto favorthemost likelydepthinterpretation fora givenimagepair	background
Psychophysicalresearch has shown that binocular disparity stimuli displayed in 1 region of the visualfield can be extrapolated into neighboring regions that contain ambiguous depth information	background
by cooperative algorithms play an important role in solving the stereo correspondence problem	background
and found that unambiguous binocular disparity stimuli displayed in the surrounding visualfields of disparity-selective V1 neurons indeed modified their responses when either bistable stereoscopic or uniform featureless stimuli were presented within their receptivefield centers	finding
The delayed timing of the response behavior compared with the timing of classical surround suppression and multiple control experiments suggests that these modulations are carried out by slower disparity-specific recurrentconnectionsamongV1neurons	finding
Theseresultsprovideexplicitevidencethatthespatialinteractionsthatarepredicted	finding
Regret matching is a widely-used algorithm for learning how to	background
We prove how this can be done by carefully discounting the prior regrets	mechanism
This provides to our knowledge	mechanism
the first principled warm-starting method It also extends to warm-starting the widely-adopted counterfactual regret minimization ( CFR ) algorithm We then study optimizing a parameter vector for a player in a two-player zero-sum game ( e	mechanism
g	mechanism
optimizing bet sizes to use in poker )	mechanism
We propose a custom gradient descent algorithm that provably finds a locally optimal parameter vector while leveraging our warm-start theory to significantly save regret-matching iterations at each step	mechanism
It optimizes the parameter vector while simultaneously finding an equilibrium This amounts to the first action abstraction algorithm ( algorithm for selecting a small number of discrete actions to use from a continuum of actions -- a key preprocessing step for solving large games using current equilibrium-finding algorithms ) with convergence guarantees for extensive-form games	mechanism
we show this experimentally as well We present experiments in no-limit Leduc Hold'em and nolimit Texas Hold'em to optimize bet sizing	method
For many reasons one might consider mechanisms	background
or social choice functions	background
that only have access to the ordinal rankings of alternatives by the individual agents rather than their utility functions	background
sample complexity results for the class of scoring functions	finding
under three different models	mechanism
In our worst-case model	mechanism
no assumptions are made about the underlying distribution and we analyze the worst-case distortion-or degree to which the selected alternative does not maximize social welfare-of optimal ( randomized ) social choice functions	mechanism
In our average-case model	mechanism
we derive optimal functions under neutral ( or impartial culture ) probabilistic models Finally	mechanism
a very general learning-theoretic model allows for the computation of optimal social choice functions ( i	mechanism
e	mechanism
ones that maximize expected social welfare ) under arbitrary	mechanism
sampleable distributions In the latter case	mechanism
we provide both algorithms and	mechanism
and further validate the approach empirically	method
shows that our framework outperforms several strong baselines	finding
We provide a solution using instructional materials	mechanism
We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures ( given a corpus of question-answer pairs and instructional materials )	mechanism
and uses what it learns to answer novel elementary science questions	mechanism
Our evaluation	method
In Massively Open Online Courses ( MOOCs ) TA resources are limited ; most MOOCs use peer assessments to grade assignments	background
Students have to divide up their time between working on their own homework and grading others	background
If there is no risk of being caught and penalized	background
students have no reason to spend any time grading others	background
Course staff want to incentivize students to balance their time between course work and peer grading	background
We present the first model	mechanism
modeling the student 's choice of effort in response to a grader 's audit levels as a Stackelberg game with multiple followers We demonstrate that computing the equilibrium for this game is computationally hard	mechanism
We then provide a PTAS in order to compute an approximate solution to the problem of allocating audit levels	mechanism
However we show that this allocation does not necessarily maximize social welfare ; in fact	mechanism
there exist settings where course auditor utility is arbitrarily far from optimal under an approximately optimal allocation To circumvent this issue	mechanism
we present a natural condition that guarantees that approximately optimal TA allocations guarantee approximately optimal welfare for the course auditors	mechanism
For decades researchers have struggled with the problem of envy-free cake cutting : how to divide a divisible good between multiple agents so that each agent likes his own allocation best	background
Our main result is an envy-free cake cutting protocol for agents with piecewise linear valuations	mechanism
which requires a number of operations that is polynomial in natural parameters of the given instance	mechanism
Achieving high performance for compute bounded numerical kernels typically requires an expert to hand select an appropriate set of Single-instruction multiple-data ( SIMD ) instructions	background
then statically scheduling them in order to hide their latency while avoiding register spilling in the process	background
Unfortunately this level of control over the code forces the expert to trade programming abstraction for performance which is why many performance critical kernels are written in assembly language	background
An alternative is to either resort to auto-vectorization ( see Figure 1 ) or to use intrinsic functions	background
both features offered by compilers However	background
in both scenarios the expert loses control over which instructions are selected	background
which optimizations are applied to the code and moreover how the instructions are scheduled for a target architecture	background
In this paper through the use of custom macro intrinsics that provide the programmer control over the instruction selection	mechanism
and scheduling while leveraging the compiler to manage the registers This provides the best of both assembly and vector intrinsics programming so that a programmer can obtain high performance implementations within the C programming language	mechanism
The resulting axiomatization of differential dynamic logic is proved to be sound and relatively complete	finding
This article introduces a relatively complete proof calculus ( dL ) that is entirely based on uniform substitution	mechanism
a proof rule that substitutes a formula for a predicate symbol everywhere Uniform substitutions make it possible to use axioms instead of axiom schemata	mechanism
thereby substantially simplifying implementations Instead of subtle schema variables and soundness-critical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones	mechanism
the resulting calculus adopts only a finite number of ordinary dLformulas as axioms	mechanism
which uniform substitutions instantiate soundly	mechanism
The static semantics of differential dynamic logic and the soundness-critical restrictions it imposes on proof steps is captured exclusively in uniform substitutions and variable renamings as opposed to being spread in delicate ways across the prover implementation In addition to sound uniform substitutions	mechanism
this article introduces differential forms for differential dynamic logic that make it possible to internalize differential invariants	mechanism
differential substitutions and derivatives as first-class axioms to reason about differential equations axiomatically	mechanism
Complex event detection is a retrieval task with the goal of finding videos of a particular event in a large-scale unconstrained internet video archive	background
given example videos and text descriptions	background
demonstrate the efficacy of our proposed method	finding
In this paper	mechanism
we propose two novel strategies based on both the events-kit text descriptions and the concepts high-level feature descriptions	mechanism
Moreover we introduce a novel event oriented dictionary representation based on the selected semantic concepts Towards this goal	mechanism
we leverage training samples of selected concepts from the Semantic Indexing ( SIN ) dataset with a pool of 346 concepts	mechanism
into a novel supervised multitask dictionary learning framework	mechanism
Extensive experimental results on TRECVID Multimedia Event Detection ( MED ) dataset	method
CPS security though well studied	background
suffers from fragmentation	background
we demonstrate an ability to investigate and extend existing results through the proposed information flow analyses	finding
This paper considers the development of information flow analyses Here	mechanism
we use information flow analysis	mechanism
a well established set of methods developed in software security	mechanism
Specifically we propose the Kullback Liebler ( KL ) divergence as a causal measure of information flow	mechanism
which quantifies the effect of adversarial inputs on sensor outputs	mechanism
We show that the proposed measure characterizes the resilience of control systems to specific attack strategies by relating the KL divergence to optimal detection	mechanism
We then relate information flows to stealthy attack scenarios where an adversary can bypass detection	mechanism
Finally this article examines active detection mechanisms where a defender intelligently manipulates control inputs or the system itself to elicit information flows from an attacker 's malicious behavior	method
In all previous cases	method
The system and method of the present invention enables high-speed	background
high precision and low-cost motion tracking for a wide range of applications	background
According to embodiments of the present invention are a system and method that use projected structured patterns of light and linear optical sensors Sensors are capable of recovering two-dimensional location within the projection area	mechanism
while several sensors can be combined for up to six degrees of freedom tracking	mechanism
The structure patterns are based on m-sequences	mechanism
in which any consecutive subsequence of m bits is unique	mechanism
Both digital and static light sources can be used	mechanism
With a few highly intuitive rules	finding
we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems	finding
We propose a general framework Specifically	mechanism
we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks	mechanism
We deploy the framework on a CNN for sentiment analysis	method
and an RNN for named entity recognition	method
we show that an AlN barrier decreases the switch parasitic capacitance with minimal increases in the switching power	finding
results in switches with an improvement in cutoff frequency	finding
$ f_ { \mathrm { CO } } $ from 5	finding
3 to 8 THz	finding
a $ C_ { \mathrm { \scriptscriptstyle OFF } } $ improvement from 15 to 10 fF	finding
while maintaining the $ R_ { \mathrm { \scriptscriptstyle ON } } $ at 2 $ \Omega $ This improvement was accompanied by normalized minimum power to amorphize increases of only 14 % ( from 1	finding
5 to 1	finding
7 W ) for a 100-ns heater pulse	finding
We demonstrate four-terminal GeTe-based RF switches with These devices incorporate an AlN-based dielectric separating high-conductivity W micro-heaters from the RF signal path	mechanism
Decoupling these design variables	mechanism
with the high thermal conductivity of the AlN	mechanism
makes it possible to increase the electrical separation with a thicker AlN film for lower parasitic capacitance with minimal decrease in desirable thermal coupling Increasing the AlN thickness from 105 to 170 nm	mechanism
With dc pulsed	method
and RF testing	method
The LD results ans wer a fundamental question on how to quantify the rate at which the distributed scheme approaches the centralized performance as the inter-sensor communication rate increases	background
it is shown that the network achieves weak consensus	finding
i	finding
e	finding
the conditional estimation error covariance at a randomly selected sensor converges weakly ( in distribution ) to a unique invariant measure	finding
Further it is proved that as ! 1 this invariant measure satisfies the Large Deviation ( LD ) upper and lower bounds	finding
implying that this measure converges exponentially fast ( in probability ) to the Dirac measureP	finding
where Pis the stable error covariance of the centralized ( Kalman ) filtering setup	finding
in distributed Kalman filtering for poten tially unstable and large linear dynamic systems A gossip network protocol termed Modified Gossip Interactive Kalman Filteri ng ( M-GIKF ) is proposed	mechanism
where sensors exchange their filtered states ( estimates and error covariances ) and propagate their observations via inter-sensor communications of rate ; is defined as the averaged number of inter-sensor message passa ges per signal evolution epoch The filtered states are interpre ted as stochastic particles swapped through local interaction	mechanism
The paper shows that the conditional estimation error covariance sequence at each sensor under M-GIKF evolves as a random Riccati equa- tion ( RRE ) with Markov modulated switching	mechanism
By formulating the RRE as a random dynamical system	method
How can we analyze large-scale real-world data with various attributes ? Many real-world data ( e	background
g	background
network traffic logs	background
web data social networks	background
knowledge bases and sensor streams ) with multiple attributes are represented as multi-dimensional arrays	background
called tensors	background
For analyzing a tensor	background
tensor decompositions are widely used in many data mining applications : detecting malicious attackers in network traffic logs ( with source IP	background
destination IP port-number	background
timestamp ) finding telemarketers in a phone call history ( with sender	background
receiver date )	background
and identifying interesting concepts in a knowledge base ( with subject	background
object relation )	background
and discover hidden concepts	finding
In this paper	mechanism
we propose HaTen2	mechanism
a distributed method that runs on the MapReduce framework	mechanism
Our careful design and implementation of HaTen2 dramatically reduce the size of intermediate data and the number of jobs leading to achieve high scalability compared with the state-of-the-art method	mechanism
Thanks to HaTen2	method
we analyze big real-world sparse tensors that can not be handled by the current state of the art	method
Mental simulation is an important skill for program understanding and prediction of program behavior Finally	background
we present recommendations for question prompt design to foster better student simulation of program execution	background
Analysis of student responses suggest that this type of question can be used to identify misconceptions and misinterpretation of instructions	finding
This poster presents the iterative design and refinement process using a novel introductory computational thinking curriculum for Microsoft 's Kodu Game Lab	mechanism
We present an analysis of question prompts and student responses from data collected from three rising 3rd - 6th graders where the curriculum was implemented	method
We show that	finding
using our approximate maximum flow algorithm	finding
we can efficiently determine whether a given directed graph is -balanced	finding
We introduce the notion of balance for directed graphs : aweighted directed graph is -balanced if for every cut S V	mechanism
the total weight of edges going from S to V S is within factor of the total weight of edges going from V S to S	mechanism
We first revisit oblivious routings in directed graphs	mechanism
Our main algorithmic result is an oblivious routing scheme for single-source instances that achieve an O ( log 3 n / loglog n ) competitive ratio	mechanism
In the process	mechanism
we make several technical contributions which may be of independent interest	mechanism
In particular we give an efficient algorithm We also define and construct low-stretch arborescences	mechanism
a generalization of low-stretch spanning trees to directed graphs	mechanism
On the negative side	mechanism
we present new lower bounds for oblivious routing problems on directed graphs We show that the competitive ratio of oblivious routing algorithms for directed graphs is ( n ) in general ; this result improves upon the long-standing best known lower bound of ( n ) by Hajiaghayi et al	mechanism
We also show that our restriction to single-source instances is necessary by showing an ( n ) lower bound for multiple-source oblivious routing in Eulerian graphs	mechanism
We also study the maximum flow problem in balanced directed graphs with arbitrary capacities We develop an efficient algorithm that finds an ( 1+ ) -approximate maximum flows in -balanced graphs in time O ( m 2 / 2 )	mechanism
Additionally we give an application to the directed sparsest cut problem	method
Virtual machine ( VM ) migration demands distinct properties under resource oversubscription and workload surges	background
We show that our implementation	finding
resolves VM contention up to several times faster than live migration	finding
We present enlightened post-copy	mechanism
a new mechanism that evicts the target VM with fast execution transfer and short total duration This design contrasts with common live migration	mechanism
which uses the down time of the migrated VM as its primary metric ; it instead focuses on recovering the aggregate performance of the VMs being affected	mechanism
In enlightened post-copy	mechanism
the guest OS identifies memory state that is expected to encompass the VM 's working set The hypervisor accordingly transfers its state	mechanism
mitigating the performance impact on the migrated VM resulting from post-copy transfer	mechanism
with modest instrumentation in guest Linux	method
The context of consumer search is often unobserved and the prediction of it can be nontrivial	background
Consumers arrive at search engines with diverse interests	background
and their search context may vary even when they are searching using the same keyword Our study has the potential to help advertisers design keyword portfolios and bidding strategy by extracting contextual ambiguity and other semantic characteristics of keywords based on large-scale analytics from unstructured data	background
It can also help search engines improve the quality of displayed ads in response to a consumer search query	background
We find that consumer click behavior varies significantly across keywords	finding
and such variation can be partially explained by keyword category and the contextual ambiguity of keywords	finding
Specifically higher contextual ambiguity is associated with higher CTR on top-positioned ads	finding
but also a faster decay in CTR with screen position	finding
Therefore the overall effect of contextual ambiguity on CTR varies across positions	finding
In our study	mechanism
we propose based on probabilistic topic models from machine learning and computational linguistics using a hierarchical Bayesian approach that allows for topic-specific effects and nonlinear position effects	mechanism
and jointly models click-through rate ( CTR ) and ad position ( rank )	mechanism
We validate our study using a novel data set from a major search engine that contains information on consumer click activities for 2	method
625 distinct keywords across multiple product categories from 10	method
000 impressions	method
Pipes carrying pressurized fluids are an important part of the civil infrastructure	background
and structural health monitoring ( SHM ) could ensure structural integrity by predicting and preventing structural failures Guided wave ultrasonics is a good candidate for use in pipe SHM because guided waves can propagate long distances and are sensitive to structural damage such as cracks and corrosion losses	background
We introduce a damage detector based on singular value decomposition ( SVD )	mechanism
caused by a mass scatterer that simulates subtle damage	mechanism
under realistic environmental variations	mechanism
We show the effectiveness and robustness of this method on experimental data collected on a pipe segment under realistic environmental and operational variations over a time period of several months	method
Multimedia event detection has been one of the major endeavors in video event analysis	background
A variety of approaches have been proposed recently to tackle this problem	background
Among others using semantic representation has been accredited for its promising performance and desirable ability for human-understandable reasoning	background
To generate semantic representation	background
we usually utilize several external image/video archives and apply the concept detectors trained on them to the event videos	background
Due to the intrinsic difference of these archives	background
the resulted representation is presumable to have different predicting capabilities for a certain event	background
with encouraging results that validate the efficacy of our proposed approach	finding
Motivated by these two shortcomings	mechanism
we propose a bi-level semantic representation analyzing method	mechanism
Regarding source-level our method learns weights of semantic representation attained from different multimedia archives	mechanism
Meanwhile it restrains the negative influence of noisy or irrelevant concepts in the overall concept-level	mechanism
In addition we particularly focus on efficient multimedia event detection with few positive examples	mechanism
which is highly appreciated in the real-world scenario	mechanism
We perform extensive experiments on the challenging TRECVID MED 2013 and 2014 datasets	method
In multi-core systems	background
main memory is a major shared resource among processor cores A task running on one core can be delayed by other tasks running simultaneously on other cores due to interference in the shared main memory system	background
We find that memory interference can be significantly reduced by ( i ) partitioning DRAM banks	finding
and ( ii ) co-locating memory-intensive tasks on the same processing core Experimental results show that the predictions made by our approach are close to the measured worst-case interference under workloads with both high and low memory contention In addition	finding
our memory interference-aware task allocation algorithm provides a significant improvement in task schedulability over previous work	finding
with as much as 96 % more tasksets being schedulable	finding
In this paper	mechanism
we present techniques on a multi-core platform that uses a commercial-off-the-shelf ( COTS ) DRAM system	mechanism
We explicitly model the major resources in the DRAM system	mechanism
including banks buses	mechanism
and the memory controller Based on these observations	mechanism
we develop a memory interference-aware task allocation algorithm	mechanism
By considering their timing characteristics	method
we analyze the worst-case memory interference delay imposed on a task by other tasks running in parallel We evaluate our approach on a COTS-based multi-core platform running Linux/RK	method
Sustainable building system design techniques aim to find an optimal balance between occupant comfort and the energy performance of HVAC systems	background
Design and implementation of effective heating ventilating and air conditioning ( HVAC ) controls is the key to achieve these optimal design conditions	background
Any anomalies in the functioning of a system component or a control system would result in occupant discomfort and/or energy wastage	background
While occupant discomfort can be directly sensed by occupants	background
measurement of waste in energy use would require additional sensing and analysis infrastructure One way of identifying such a waste is to compare asdesigned system requirements with the actual performance of the systems The findings in this paper substantiate the need to formally define the sequence of operations and also point to the need to verify the implemented controls in a given project to detect any deviations from the actual design intent	background
Any deviation in the sensor data as compared to the expected operation pattern of the design intent indicated incorrect operation of the system with incorrectly implemented controls	finding
One year sensor data for the AHU parameters was analyzed to assess the correctness of the implementation of the design intent	method
The design intent was interpreted from the sequence of operations ( SOOs ) and confirmed with a commissioning engineer	method
who worked on the project	method
The design intent was then graphically represented as a pattern that the sensor data corresponding to the controls is expected to follow if it follows the design intent	method
Elucidating assembly pathways of complex macromolecular structures	background
such as virus capsids	background
is an important problem for understanding the many cellular processes dependent on self-assembly but also challenging given limited experimental technologies for observing such systems	background
We have previously addressed this problem through simulation-based data fitting	background
learning rate parameters of coarse-grained stochastic simulation models to match light scattering data from bulk assembly of purified coat protein in vitro providing an unprecedented view of the fine-scale reaction pathways that might have produced those data	background
These simulation results help us understand how RNA viral coat and genome may interact in assembly to promote rapid growth while avoiding kinetic traps expected from much prior theory	background
bringing us a step closer to the goal of understanding how viral assembly in the cell may differ from our current conception based largely on in vitro models	background
We find a surprising complexity and synergy of interaction effects	finding
Energetic effects that gain or lower free energy tend to disrupt successful assembly relative to the in vitro model individually	finding
while the full combination of positive and negative effects collectively promotes greatly accelerated assembly without loss of yield	finding
Furthermore it accomplishes this change in kinetics while substantially altering the ensemble of assembly pathways open to the system	finding
using analytical models of various contributions of RNA folding to assembly	mechanism
Machine learning ( ML ) algorithms are commonly applied to big data	background
using distributed systems that partition the data across machines and allow each machine to read and update all ML model parameters -- - a strategy known as data parallelism	background
An alternative and complimentary strategy	background
model parallelism partitions the model parameters for non-shared parallel access and updates	background
and may periodically repartition the parameters to facilitate communication	background
we show that SchMP programs running on STRADS outperform non-model-parallel ML implementations : for example	finding
SchMP LDA and SchMP Lasso respectively achieve 10x and 5x faster convergence than recent	finding
well-established baselines	finding
We propose scheduled model parallelism ( SchMP )	mechanism
a programming approach by efficiently scheduling parameter updates	mechanism
taking into account parameter dependencies and uneven convergence	mechanism
To support SchMP at scale	mechanism
we develop a distributed framework STRADS which optimizes the throughput of SchMP programs	mechanism
and benchmark four common ML applications written as SchMP programs : LDA topic modeling	mechanism
matrix factorization sparse least-squares ( Lasso ) regression and sparse logistic regression By improving ML progress per iteration through SchMP programming whilst improving iteration throughput through STRADS	mechanism
Sequential games of perfect information can be solved by backward induction	background
where solutions to endgames are propagated up the game tree	background
show that our approach leads to significant performance improvements in practice	finding
Nonetheless we show that endgame solving can have significant benefits in imperfectinformation games with large state and action spaces : computation of exact ( rather than approximate ) equilibrium strategies	mechanism
computation of relevant equilibrium refinements	mechanism
significantly finer-grained action and information abstraction	mechanism
new information abstraction algorithms that take into account the relevant distribution of players types entering the endgame	mechanism
being able to select the coarseness of the action abstraction dynamically	mechanism
additional abstraction techniques for speeding up endgame solving	mechanism
a solution to the off-tree problem	mechanism
and using different degrees of probability thresholding in modeling versus playing	mechanism
We discuss each of these topics in detail	mechanism
and introduce techniques that enable one even when the number of states and actions in the game is large	mechanism
Our experiments on two-player no-limit Texas Holdem poker	method
Privacy decision making has been examined from various perspectives	background
A dominant normative perspective has focused on rational processes by which consumers with stable preferences for privacy weigh the expected benefits of privacy choices against their potential costs More recently	background
an alternate behavioral perspective has leveraged theories from behavioral decision research to construe privacy decision making as a process in which cognitive heuristics and biases predictably occur Our results suggest a way to integrate diverse streams of IS literature on privacy decision making : consumers may both over-estimate their response to normative factors and under-estimate their response to behavioral factors in hypothetical choice contexts relative to actual choice contexts	background
We find that both relative and objective risks can	finding
in fact impact consumer privacy decisions	finding
However and surprisingly	finding
the impact of objective changes in risk diminishes between hypothetical and actual choice settings	finding
Vice versa the impact of relative risk is more pronounced going from hypothetical to actual choice settings	finding
In a series of experiments by evaluating the impact of changes in objective risk of disclosure and the impact of changes in relative perceptions of risk of disclosure on both hypothetical and actual consumer privacy choices	method
Communication and coordination play a major role in the ability of bacterial cells to adapt to ever changing environments and conditions	background
prove that the method we propose leads to convergence even when using a dynamically changing interaction network	finding
illustrate the ability of the method to explain and further predict several aspects of bacterial swarm food search	finding
Here we develop a new distributed gradient descent method This method can also be used for computational tasks when agents are facing similarly restricted conditions We formalize the communication and computation assumptions re- quired for successful coordination and The proposed method improves upon prior models suggested for bacterial foraging despite making fewer assumptions	mechanism
Simulation studies and analysis of experimental data	method
ABSTRACT Clad steel refers to a thick carbon steel structural plate bonded to a corrosion resistant alloy ( CRA ) plate	background
such as stainless steel or titanium	background
and is widely used in industry to construct pressure vessels	background
The CRA resists the chemically aggressive environment on the interior	background
but can not prevent the development of corrosion losses and cracks that limit the continued safe operation of such vessels	background
In previous resear ch	background
sponsored by industry to detect and localize damage in pressurized piping systems under operational and environmental changes	background
we investigated a number of data-driven signal processing methods to extract damage information from ultrasonic guided wave pitch-catch records ; we also discuss observations of plate-like mode properties implied by these results	background
We discuss conditions under which localization is achieved by relatively simple first-arrival methods	finding
and other conditions for which data-driven methods are needed	finding
We now apply those methods to relatively large clad steel plate specimens	mechanism
We study a sparse array of wafer-type ultrasonic transducers adhered to the carbon steel surface	method
attempting to localize mass scatterers grease-coupled to the stainless steel surface	method
Many commercial products and academic research activities are embracing behavior analysis as a technique for improving detection of attacks of many sortsfrom retweet boosting	background
hashtag hijacking to link advertising	background
Traditional approaches focus on detecting dense blocks in the adjacency matrix of graph data	background
and recently the tensors of multimodal data	background
where it improves the F1 score over previous techniques by 68percent and finds suspicious behavioral patterns in social datasets spanning 0	finding
3 billion posts	finding
In this paper	mechanism
we first give a list of axioms that any metric of suspiciousness should satisfy ; we propose an intuitive	mechanism
principled metric that satisfies the axioms	mechanism
and is fast to compute ; moreover	mechanism
we propose CrossSpot	mechanism
an algorithm typically indicating fraud or some other noteworthy deviation from the usual	mechanism
and sort them in the order of importance ( suspiciousness )	mechanism
Finally we apply CrossSpot to the real data	method
A traditional goal of neural recording with extracellular electrodes is to isolate action potential waveforms of an individual neuron	background
Recently in braincomputer interfaces ( BCIs )	background
it has been recognized that threshold crossing events of the voltage waveform also convey rich information	background
To date the threshold for detecting threshold crossings has been selected to preserve single-neuron isolation	background
Significance	background
How neural signals are processed impacts the information that can be extracted from them	background
Both the type and quality of information contained in threshold crossings depend on the threshold setting	background
There is more information available in these signals than is typically extracted	background
Adjusting the detection threshold to the parameter of interest in a BCI context should improve our ability to decode motor intent	background
and thus enhance BCI control	background
Further by sweeping the detection threshold	background
one can gain insights into the topographic organization of the nearby neural tissue	background
Main Results	finding
The optimal threshold depends on the desired information	finding
In M1 velocity is optimally encoded at higher thresholds than speed ; in both cases the optimal thresholds are lower than are typically used in BCI applications	finding
In V1 information about the orientation of a visual stimulus is optimally encoded at higher thresholds than is visual contrast	finding
Here we introduce a procedure We apply this procedure in two distinct contexts : the encoding of kinematic parameters from neural activity in primary motor cortex ( M1 )	mechanism
and visual stimulus parameters from neural activity in primary visual cortex ( V1 Approach We record extracellularly from multi-electrode arrays implanted in M1 or V1 in monkeys	mechanism
Then we systematically sweep the voltage detection threshold and quantify the information conveyed by the corresponding threshold crossings A conceptual model explains these results as a consequence of cortical topography	mechanism
We show that CSMA method can achieve good results and is very efficient in the inpainting problem as compared to [ 1 ]	finding
[ 2 ] Our method also achieves higher face recognition rates	finding
This paper proposes a novel approach named Compressed Submanifold Multifactor Analysis ( CSMA ) Our approach can deal with the problem of missing values and outliers via SVD-L1 The Random Projection method is used to obtain the fast low-rank approximation of a given multifactor dataset	mechanism
In addition it is able to preserve the geometry of the original data Our CSMA method can be used efficiently for multiple purposes	mechanism
e	mechanism
g	mechanism
noise and outlier removal	mechanism
estimation of missing values	mechanism
biometric applications	mechanism
compared to LRTC	method
SPMA MPCA and some other methods	method
i	method
e	method
PCA LDA and LPP	method
on three challenging face databases	method
i	method
e	method
CMU-MPIE CMU-PIE and Extended YALE-B	method
Consumer privacy decision making is often layered : different interrelated decisions determine	background
together a final privacy outcome and its associated benefits and costs Layered privacy choices are particularly common online	background
where consumers are frequently tasked with multiple	background
sequential choices ( such as first selecting a services privacy settings	background
and then engaging in privacy-sensitive behaviors ) that will ultimately impact their privacy trade-offs Implications for privacy decision research as well as policy makers are discussed	background
We find that various manipulations of decision frames	finding
common to privacy contexts	finding
can significantly alter individual choice of privacy protective options	finding
Further and importantly	finding
we find that participants subsequent disclosure behavior stays constant despite the shifts in chosen privacy protections induced by choice framing	finding
Specifically in a series of experiments	method
we investigate the impact of framing on participants initial privacy choices	method
and whether participants subsequent behaviors take account of	method
and neutralize that impact	method
Motivation : As cancer researchers have come to appreciate the importance of intratumor heterogeneity	background
much attention has focused on the challenges of accurately profiling heterogeneity in individual patients Experimental technologies for directly profiling genomes of single cells are rapidly improving	background
but they are still impractical for large-scale sampling	background
Bulk genomic assays remain the standard for population-scale studies	background
but conflate the influences of mixtures of genetically distinct tumor	background
stromal and infiltrating immune cells	background
Many computational approaches have been developed to deconvolute these mixed samples and reconstruct the genomics of genetically homogeneous clonal subpopulations	background
All such methods	background
however are limited to reconstructing only coarse approximations to a few major subpopulations	background
In prior work	background
we showed that one can improve deconvolution of genomic data by leveraging substructure in cellular mixtures through a strategy called simplicial complex inference	background
Results We show that these improvements lead to more accurate inference of cell populations and mixture proportions We further demonstrate their effectiveness in identifying mixture substructure Availability : Source code is available at this http URL	finding
by introducing enhancements to automate learning of substructured genomic mixtures	mechanism
with specific emphasis on genome-wide copy number variation ( CNV ) data	mechanism
We introduce methods for dimensionality estimation fuzzy clustering and automated model inference methods for other key model parameters	mechanism
in simulated scenarios	method
in real tumor CNV data	method
The pervasiveness of mobile technologies today have facilitated the creation of massive crowdsourced and geotagged data from individual users in real time and at different locations in the city	background
Such ubiquitous user-generated data allow us to infer various patterns of human behavior	background
which help us understand the interactions between humans and cities	background
Our study demonstrates the potential of how to best make use of the large volumes and diverse sources of crowdsourced and geotagged user-generated data to create matrices to predict local economic demand in a manner that is fast	background
cheap accurate and meaningful	background
Our results suggest that foot traffic can increase local popularity and business performance	finding
while mobility and traffic from automobiles may hurt local businesses	finding
especially the well-established chains and high-end restaurants We also find that on average one more street closure nearby leads to a 4	finding
7 % decrease in the probability of a restaurant being fully booked during the dinner peak	finding
Our study is instantiated on a unique dataset of restaurant bookings from OpenTable for 3	mechanism
187 restaurants in New York City from November 2013 to March 2014	mechanism
Specifically we extract multiple traffic and human mobility features from publicly available data sources using NLP and geo-mapping techniques	method
and examine the effects of both static and dynamic features on economic outcome of local businesses	method
How much has a network changed since yesterday ? How different is the wiring of Bobs brain ( a left-handed male ) and Alices brain ( a right-handed female )	background
and how is it different ? Graph similarity with given node correspondence	background
i	background
e	background
the detection of changes in the connectivity of graphs	background
arises in numerous settings	background
showcase the advantages of our method over existing similarity measures	finding
We propose D elta C on	mechanism
a principled intuitive	mechanism
and scalable algorithm ( e	mechanism
g	mechanism
employees of a company	mechanism
customers of a mobile carrier In conjunction	mechanism
we propose D elta C on -A ttr	mechanism
a related approach	mechanism
and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs	method
Experiments on various synthetic and real graphs Finally	method
we employ D elta C on and D elta C on -A ttr on real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs	method
( b ) do temporal anomaly detection in the who-emails-whom Enron graph and find the top culprits for the changes in the temporal corporate email graph	method
and ( c ) recover pairs of test-retest large brain scans ( 17M edges	method
up to 90M edges ) for 21 subjects	method
Effective enforcement of laws and policies requires expending resources to prevent and detect offenders	background
as well as appropriate punishment schemes to deter violators	background
In particular enforcement of privacy laws and policies in modern organizations that hold large volumes of personal information ( e	background
g	background
hospitals banks ) relies heavily on internal audit mechanisms	background
We present an audit game model that is a natural generalization of a standard security game model Computing the Stackelberg equilibrium for this game is challenging because it involves solving an optimization problem with non-convex quadratic constraints We present an additive FPTAS that efficiently computes the solution	mechanism
Admixture-introduced linkage disequilibrium ( LD ) has recently been introduced into the inference of the histories of complex admixtures	background
Our method is a considerable improvement over other current methods and further facilitates the inference of the histories of complex population admixtures	background
and it was shown to be more accurate than MALDER	finding
a state-of-the-art method that was recently developed for similar purposes	finding
under various admixture models	finding
Interestingly we were able to identify more than one admixture events in several populations	finding
which have yet to be reported	finding
For example two major admixture events were identified in the Xinjiang Uyghur	finding
occurring around 27 ? ? ? 30 generations ago and 182 ? ? ? 195 generations ago	finding
respectively	finding
In an African population ( MKK )	finding
three recent major admixtures occurring 13 ? ? ? 16	finding
50 ? ? ? 67	finding
and 107 ? ? ? 139 generations ago were detected	finding
We first illustrated the dynamic changes of LD in admixed populations and mathematically formulated the LD under a generalized admixture model with finite population size	mechanism
We next developed a new method	mechanism
MALDmef by fitting LD with multiple exponential functions for inferring and dating multiple-wave admixtures MALDmef takes into account the effects of source populations which substantially affect modeling LD in admixed population	mechanism
which renders it capable of efficiently detecting and dating multiple-wave admixture events	mechanism
The performance of MALDmef was evaluated by simulation We further applied MALDmef to analyzing genome-wide data from the Human Genome Diversity Project ( HGDP ) and the HapMap Project	method
Large-scale deep learning requires huge computational resources to train a multi-layer neural network	background
Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections	background
We show that GeePS enables a state-of-the-art single-node GPU implementation to scale well	finding
such as to 13 times the number of training images processed per second on 16 machines ( relative to the original optimized single-node code ) Moreover	finding
GeePS achieves a higher training throughput with just four GPU machines than that a state-of-the-art CPU-only system achieves with 108 machines	finding
This paper describes a new parameter server	mechanism
called GeePS that supports scalable deep learning across GPUs distributed among multiple machines	mechanism
overcoming these obstacles	mechanism
Often Big Data applications collect a large number of time series	background
for example the financial data of companies quoted in a stock exchange	background
the health care data of all patients that visit the emergency room of a hospital	background
or the temperature sequences continuously measured by weather stations across the US	background
A first task in the analytics of these data is to derive a low dimensional representation	background
a graph or discrete manifold	background
that describes well the interrelations among the time series and their intrarelations across time	background
The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested	finding
This paper presents a computationally tractable algorithm This graph is directed and weighted	mechanism
possibly representing causal relations	mechanism
not just reciprocal correlations as in many existing approaches in the literature	mechanism
A detailed convergence analysis is carried out	mechanism
The algorithm is demonstrated on random graph and real network time series datasets	method
and its performance is compared to that of related methods	method
Given a large collection of time-evolving activities	background
such as Google search queries	background
which consist of d keywords/activities for m locations of duration n	background
how can we analyze temporal patterns and relationships among all these activities and find location-specific trends ? How do we go about capturing non-linear evolutions of local activities and forecasting future patterns ? For example	background
assume that we have the online search volume for multiple keywords	background
e	background
g	background
`` Nokia/Nexus/Kindle '' or `` CNN/BBC '' for 236 countries/territories	background
from 2004 to 2015	background
demonstrate that COMPCUBE consistently outperforms the best state-of- the-art methods in terms of both accuracy and execution speed	finding
We present COMPCUBE	mechanism
a unifying non-linear model	mechanism
which provides a compact and powerful representation of co-evolving activities ; and also a novel fitting algorithm	mechanism
COMPCUBE-FIT which is parameter-free and scalable Our method captures the following important patterns : ( B ) asic trends	mechanism
i	mechanism
e	mechanism
non-linear dynamics of co-evolving activities	mechanism
signs of ( C ) ompetition and latent interaction	mechanism
e	mechanism
g	mechanism
Nokia vs	mechanism
Nexus ( S ) easonality	mechanism
e	mechanism
g	mechanism
a Christmas spike for iPod in the U	mechanism
S	mechanism
and Europe and ( D ) eltas	mechanism
e	mechanism
g	mechanism
unrepeated local events such as the U	mechanism
S	mechanism
election in 2008 Thanks to its concise but effective summarization	mechanism
COMPCUBE can also forecast long-range future activities	mechanism
Extensive experiments on real datasets	method
Given a large collection of co-evolving online activities	background
such as searches for the keywords `` Xbox ''	background
`` PlayStation '' and `` Wii ''	background
how can we find patterns and rules ? Are these keywords related ? If so	background
are they competing against each other ?	background
show that EcoWeb is effective	finding
in that it can capture long-range dynamics and meaningful patterns such as seasonalities	finding
and practical in that it can provide accurate long-range forecasts	finding
EcoWeb consistently outperforms existing methods in terms of both accuracy and execution speed	finding
We present EcoWeb	mechanism
( i	mechanism
e	mechanism
Ecosystem on the Web )	mechanism
which is an intuitive model designed as a non-linear dynamical system Our second contribution is a novel	mechanism
parameter-free and scalable fitting algorithm	mechanism
EcoWeb-Fit that estimates the parameters of EcoWeb	mechanism
Extensive experiments on real data	method
T cells must receive signals through the T cell receptor ( TCR ) and the costimulatory receptor CD28 to become fully activated	background
This combination of imaging and computational analysis could be applied to other systems to determine the spatiotemporal dynamics of signaling molecules	background
The regulatory proteins WAVE2 and cofilin were efficiently recruited to the immunological synapse only when both TCR and CD28 signaled Constitutive activation of either protein in TCR-stimulated T cells enabled normal actin reorganization even when CD28 signaling was blocked	finding
Roybal et al	mechanism
imaged actin and fluorescently tagged actin regulatory proteins in T cells activated through the TCR in the absence or presence of CD28 signaling	mechanism
Computational image processing to normalize differences in cell shape enabled tracking of the fluorescent proteins	mechanism
results show that the proposed data-driven approach works well in a smart grid setting with increasing uncertainties and it produces an online state estimate excelling current industrial approach	finding
we propose a data-driven state estimation approach based on recent targeted investment on sensors	mechanism
data storage and computing devices	mechanism
An architecture is proposed to use power system physics and pattern to systematically clean historical data and conduct supervised learning	mechanism
where historical similar measurements and their states are used to learn the relationship between the current measurement and the state	mechanism
In order to deal with nonlinearity	mechanism
kernel trick is used to produce linear mapping in a carefully selected higher dimensional space To speed up the data-driven approach for online services	mechanism
we analyze power system data set and discover its clustering property due to the periodic pattern of power systems	mechanism
This leads to significant dimension reduction and the idea of preorganizing data points in a tree structure for inquiry	mechanism
leading to 1000 times speedup	mechanism
Numerical	method
Multi-person tracking plays a critical role in the analysis of surveillance video thus potentially opening the door to automatic summarization of the vast amount of surveillance video generated every day	background
and we were able to localize a person 53	finding
2 % of the time with 69	finding
8 % precision	finding
Results showed that we were able to generate a reasonable visual diary ( i	finding
e	finding
a summary of what a person did ) for different people	finding
Therefore we propose a multi-person tracking algorithm for very long-term ( e	mechanism
g	mechanism
month-long ) multi-camera surveillance scenarios	mechanism
Long-term tracking is challenging because 1 ) the apparel/appearance of the same person will vary greatly over multiple days and 2 ) a person will leave and re-enter the scene numerous times	mechanism
To tackle these challenges	mechanism
we leverage face recognition information	mechanism
which is robust to apparel change	mechanism
to automatically reinitialize our tracker over multiple days of recordings	mechanism
Unfortunately recognized faces are unavailable oftentimes Therefore	mechanism
our tracker propagates identity information to frames without recognized faces by uncovering the appearance and spatial manifold formed by person detections	mechanism
We tested our algorithm on a 23-day 15-camera data set ( 4	method
935 hours total )	method
We further performed video summarization experiments based on our tracking output	method
on 116	method
25 hours of video	method
Cyber-physical systems ( CPS ) are heterogeneous	background
be- cause they tightly couple computation	background
communication and control along with physical dynamics	background
which are traditionally considered separately	background
Without a comprehensive modeling formalism	background
model- based development of CPS involves using a multitude of models in a variety of formalisms that capture various aspects of the system design	background
such as software design	background
networking design physical mod- els	background
and protocol design	background
In this paper	mechanism
we propose a multi-view architecture framework that treats models as views of the under- lying system structure and uses structural and semantic mappings Index TermsControl design	mechanism
control engineering formal veri- fication	mechanism
software architecture	mechanism
Throughout the paper	method
the theoretical concepts are illustrated using two examples : a quad- rotor and an automotive intersection collision avoidance system	method
In this paper	mechanism
we introduce InstructableCrowd	mechanism
a system We create a framework which enables users to converse with the crowd using their phone and describe a problem which they might have	mechanism
We create an interface for a crowd worker to both chat with the user and compose a rule with an `` IF '' part connected to the user 's phone sensors ( e	mechanism
g	mechanism
incoming emails GPS location	mechanism
meeting calendar weather information etc	mechanism
) and a `` THEN '' part connected to user 's phone effectors ( e	mechanism
g	mechanism
sending an email	mechanism
creating an alarm	mechanism
posting a tweet	mechanism
etc	mechanism
)	mechanism
The system then sends the rules created by the crowd to the user 's phone in order to help the user solve his problem	mechanism
The maximum Nash welfare ( MNW ) solution -- - which selects an allocation that maximizes the product of utilities -- - is known to provide outstanding fairness guarantees when allocating divisible goods	background
These results lead us to believe that MNW is the ultimate solution for allocating indivisible goods	background
and underlie its deployment on a popular fair division website	background
demonstrate that it scales well	finding
And while it seems to lose its luster when applied to indivisible goods	mechanism
we show that	mechanism
in fact the MNW solution is unexpectedly	mechanism
strikingly fair even in that setting	mechanism
We also establish that the MNW solution provides a good approximation to another popular ( yet possibly infeasible ) fairness property	mechanism
the maximin share guarantee	mechanism
in theory and -- - even more so -- - in practice	mechanism
While finding the MNW solution is computationally hard	mechanism
we develop a nontrivial implementation	mechanism
and	mechanism
on real data	method
The safety of mobile robots in dynamic environments is predicated on making sure that they do not collide with obstacles	background
Our verification results are generic in the sense that they are not limited to the particul ar choices of one specific control algorithm but identify conditions that make them simultaneously apply to a broad class of control algorithms	background
we prove that provably safe motion is flexible enough to let the r obot still navigate waypoints and pass intersections Moreover	finding
we formally prove that safety can still be guaranteed despite s ensor uncertainty and actuator perturbation	finding
and when control choices for more aggressive maneuvers are introduced	finding
and formally verify a series of increasingly powerful safety properties of controllers ( i ) static safety	mechanism
which ensures that no collisions can happen with stationary obstacles	mechanism
( ii ) passive safety	mechanism
which ensures that no collisions can happen with stationary or moving obstacles while the robot moves	mechanism
( iii ) the stronger passive friendly safety in which the robot further maintains sufficient maneuvering distance for obstacles to avoid collision as well	mechanism
and ( iv ) passive orientation safety	mechanism
which allows for imperfect sensor coverage of the robot	mechanism
i	mechanism
e	mechanism
the robot is aw are that not everything in its environment will be visible	mechanism
We complement these provably correct safety properties with liveness properties : We use hybrid system models and theorem proving techniques	mechanism
Given a bipartite graph of users and the products that they review	background
or followers and followees	background
how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral	background
etc	background
) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph	background
Fraudsters can evade these methods using camouflage	background
by adding reviews or follows with honest targets so that they look `` normal ''	background
Even worse some fraudsters use hijacked accounts from honest users	background
and then the camouflage is indeed organic	background
( c ) is effective in real-world data	finding
show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud	finding
FRAUDAR successfully detected a subgraph of more than 4000 detected accounts	finding
of which a majority had tweets showing that they used follower-buying services	finding
We propose FRAUDAR	mechanism
an algorithm that ( a ) is camouflage-resistant	mechanism
( b ) provides upper bounds on the effectiveness of fraudsters	mechanism
and	mechanism
Experimental results under various attacks Additionally	method
in real-world experiments with a Twitter follower-followee graph of 1	method
47 billion edges	method
The underlying motivating application is epidemics like computer virus spreading	background
for example in wide campus local networks	background
We consider multiple classes of viruses	mechanism
each type bearing their own statistical characterization -- exogenous contamination	mechanism
contagious propagation and healing	mechanism
The network state ( distribution of nodes infected by each class in the network ) is a jump Markov process	mechanism
not necessarily reversible	mechanism
making it a challenge to obtain its invariant distribution	mechanism
By suitable renormalization	mechanism
in the limit of a large network ( number of nodes )	mechanism
we describe the macroscopic or emergent behavior of the network by the solution of a set of deterministic nonlinear differential equations These nonlinear differential equations are obtained by mean field analysis of the microscopic random dynamics	mechanism
We study the qualitative behavior of the nonlinear differential equations describing the mean field dynamics	method
The ubiquitous deployment of mobile and sensor technologies has led to both the capacity to observe human behavior in physical ( offline ) settings as well as to record it	background
Finally our study has important welfare implications in that efficient information sharing leads to an income increase among all drivers	background
instead of a redistribution of income between different types of drivers Our work allows us not only to explain driver decision making behavior using these detailed behavioral traces	background
but also to prescribe information sharing strategy for the firm in order to improve the overall market efficiency	background
We find strong heterogeneity in individual learning behavior and driving decisions	finding
which is significantly associated with individual economic outcomes	finding
Drivers with higher incomes benefit significantly from their ability to learn from not only demand information directly observable in the local market	finding
but also aggregate information on demand flows across markets	finding
Interestingly our policy simulations indicate information that is noisy at the individual level becomes valuable after being aggregated across various spatial and temporal dimensions Moreover	finding
the value of information does not increase monotonically with the scale and frequency of information sharing	finding
This capacity to use data where occupancy of the taxi is known is a distinctive feature of our data set and sets this work apart from prior work which has attempted to study driver behavior We conduct our study using a heterogeneous Bayesian learning model	mechanism
In this paper	method
we study decision making behavior of 11	method
196 taxi drivers in a large Asian city using a rich data set consisting of 10	method
6 million fine-grained GPS trip records These records include detailed taxi GPS trajectories	method
taxi occupancy data ( i	method
e	method
whether a taxi was occupied with a passenger or was vacant ) and taxi drivers daily incomes The specific decision we focus on pertains to actions drivers take to find new passengers after they have dropped off their current passengers	method
In particular we study the role of information derivable from the GPS trace data ( e	method
g	method
where passengers are dropped off	method
where passengers are picked up	method
longitudinal taxicab travel history with fine-grained time stamps ) observable by or made available to drivers in enabling them to learn the distribution of demand for their services over space and time	method
show that AD3 compares favorably with the state-of-the-art	finding
We present AD3	mechanism
a new algorithm	mechanism
based on the alternating directions method of multipliers	mechanism
Like other dual decomposition algorithms	mechanism
AD3 has a modular architecture	mechanism
where local subproblems are solved independently	mechanism
and their solutions are gathered to compute a global update	mechanism
The key characteristic of AD3 is that each local subproblem has a quadratic regularizer	mechanism
leading to faster convergence	mechanism
both theoretically and in practice	mechanism
We provide closed-form solutions for these AD3 subproblems for binary pairwise factors and factors imposing first-order logic constraints	mechanism
For arbitrary factors ( large or combinatorial )	mechanism
we introduce an active set method which requires only an oracle for computing a local MAP configuration	mechanism
making AD3 applicable to a wide range of problems	mechanism
Experiments on synthetic and real-world problems	method
We describe a system called Olive that freezes and precisely reproduces the environment necessary It uses virtual machine ( VM ) technology	mechanism
complete with all its software dependencies	mechanism
This legacy world can be completely closed-source : there is no requirement for availability of source code	mechanism
nor a requirement for recompilation or relinking	mechanism
The entire VM is streamed over the Internet from a web server	mechanism
much as video is streamed today	mechanism
; and we observe power law growth for both nodes and links	finding
a fact that completely breaks the sigmoid models ( like SI	finding
and Bass )	finding
where NETTIDE gives good fitting accuracy	finding
and more importantly	finding
applied on the WeChat data	finding
our NETTIDE forecasted more than 730 days into the future	finding
with 3 % error	finding
In its place	mechanism
we propose NETTIDE	mechanism
along with differential equations for the growth of the count of nodes	mechanism
as well as links	mechanism
Our model accurately fits the growth patterns of real graphs ; it is general	mechanism
encompassing as special cases all the known	mechanism
traditional models ( including Bass	mechanism
SI log-logistic growth ) ; while still remaining parsimonious	mechanism
requiring only a handful of parameters Moreover	mechanism
our NETTIDE for link growth is the first one of its kind	mechanism
accurately fitting real data	mechanism
and naturally leading to the densification phenomenon	mechanism
We examine the growth of several real networks	method
including one of the world 's largest online social network	method
`` WeChat ''	method
with 300 million nodes and 4	method
75 billion links by 2013 We validate our model with four real	method
time-evolving social networks	method
We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR For example	finding
we observe both diffusion and non-diffusion mechanism in the group joining process	finding
and power-law decay in group quitting process	finding
rather than exponential decay as expected in SIR model	finding
Therefore we propose a new model comeNgo	mechanism
a concise yet flexible dynamic model Our model has the following advantages : ( a ) unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation	mechanism
( b ) succinctness and interpretability : it contains only six parameters with clear physical meanings	mechanism
( c ) accuracy : it can capture various kinds of group evolution patterns preciously and the goodness of fit increase by 58 % over baseline	mechanism
( d ) usefulness : it can be used in multiple application scenarios such as forecasting and pattern discovery	mechanism
In this paper	method
we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users	method
System management includes the selection of maintenance actions depending on the available observations : when a system is made up by components known to be similar	background
data collected on one is also relevant for the management of others	background
This is typically the case of wind farms	background
which are made up by similar turbines	background
Optimal management of wind farms is an important task due to high cost of turbines operation and maintenance : in this context	background
we recently proposed a method for planning and learning at system-level	background
called PLUS built upon the Partially Observable Markov Decision Process ( POMDP ) framework	background
which treats transition and emission probabilities as random variables	background
and is therefore suitable for including model uncertainty	background
and discuss its potential and computational complexity	finding
The proposed approach	mechanism
called Multiple Uncertain POMDP ( MU-POMDP )	mechanism
models the components as POMDPs	mechanism
and assumes the corresponding parameters as dependent random variables Through this framework	mechanism
we can calibrate specific degradation and emission models for each component while	mechanism
at the same time	mechanism
process observations at system-level	mechanism
We compare the performance of the proposed MU-POMDP with PLUS	method
Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type	background
and the tensor-based summaries look for high-order dense blocks by clustering the values ( including timestamps ) in each dimension	background
CatchTartan outperforms the baselines on both the accuracy and speed	finding
providing comprehensive summaries for the events	finding
human life and scientific development	finding
In this paper	mechanism
as a two-level matrix ( temporal-behaviors by dimensional-values ) and propose a novel representation called Tartan that includes a set of dimensions	mechanism
the values in each dimension	mechanism
a list of consecutive time slices and the behaviors in each slice	mechanism
We further develop a propagation method CatchTartan it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner	mechanism
We apply CatchTartan to four Twitter datasets up to 10 million tweets and the DBLP data	method
We prove that the mean-squared error of the estimator asymptotically converges if the degree of instability of the field dynamics is within a prespecified threshold defined as tracking capacity of the estimator The tracking capacity is a function of the local observation models and the agent communication network yielding distributed estimates with minimized mean-squared error	finding
we show that the distributed estimator with optimal gains converges faster and with approximately 3dB better mean-squared error performance than previous distributed estimators	finding
We develop a Kalman filter type consensus + innovations distributed linear estimator of the dynamic field termed as Consensus+Innovations Kalman Filter We design the optimal consensus and innovation gain matrices	mechanism
The field is observed by a sparsely connected network of agents/sensors collaborating among themselves We analyze the convergence properties of this distributed estimator Through numerical evaluations	method
In this report	background
we describe CMU-SMUs participation in the Video Hyperlinking task of TRECVID 2015	background
results show that ( 1 ) the context does not generally improve results	finding
( 2 ) the search performance mainly rely on textual features	finding
and the combination of audio and visual feature can not provide improvements ; ( 3 ) due to the lack of training examples	finding
machine learning techniques can not provide contributions	finding
We treat video hyperlinking as ad-hoc retrieval scenario and use a variety of retrieval methods	mechanism
Different combination strategies are used to combine those features	mechanism
Besides we also attempt to categorize the queries and use different search strategies for different categories	mechanism
Experiments	method
The physical constraints of smartwatches limit the range and complexity of tasks that can be completed	background
WearWrite represents a new approach to getting work done from wearables using the crowd	background
we validate that it is possible to manage the crowd writing process from a watch	finding
This paper presents WearWrite	mechanism
a system that enables by leveraging a crowd to help translate their ideas into text	mechanism
WearWrite users dictate tasks	mechanism
respond to questions	mechanism
and receive notifications of major edits on their watch	mechanism
Using a dynamic task queue	mechanism
the crowd receives tasks issued by the watch user and generic tasks from the system Watch users captured new ideas as they came to mind and managed a crowd during spare moments while going about their daily routine	mechanism
In a week-long study with seven smartwatch users supported by approximately 29 crowd workers each	method
We show that on these problems the proposed method performs very well	finding
solving the problems faster than state-of-the-art methods and to higher accuracy	finding
We present a new algorithmic approach This model has found many applications in multiple change point detection	mechanism
signal compression and total variation denoising	mechanism
though existing algorithms typically using first-order or alternating minimization schemes	mechanism
In this paper we instead develop a specialized projected Newton method	mechanism
combined with a primal active set approach	mechanism
which we show to be substantially faster that existing methods	mechanism
Furthermore we present two applications that use this algorithm as a fast subroutine for a more complex outer loop : segmenting linear regression models for time series data	mechanism
and color image denoising	mechanism
Crowdsourced clustering approaches present a promising way to harness deep semantic knowledge for clustering complex information	background
We introduce Alloy	mechanism
a hybrid approach that combines the richness of human judgments with the power of machine algorithms	mechanism
Alloy supports greater global context through a new `` sample and search '' crowd pattern which changes the crowd 's task from classifying a fixed subset of items to actively sampling and querying the entire dataset It also improves efficiency through a two phase process in which crowds provide examples to help a machine cluster the head of the distribution	mechanism
then classify low-confidence examples in the tail To accomplish this	mechanism
Alloy introduces a modular `` cast and gather '' approach which leverages a machine learning backbone to stitch together different types of judgment tasks	mechanism
As our approach is compact	background
non-invasive low-cost and low-powered	background
we envision the technology being integrated into future smartwatches	background
supporting rich touch interactions beyond the confines of the small touchscreen	background
Our approach can segment touch events at 99 % accuracy	finding
and resolve the 2D location of touches with a mean error of 7	finding
6mm	finding
SkinTrack is a wearable system It consists of a ring	mechanism
which emits a continuous high frequency AC signal	mechanism
and a sensing wristband with multiple electrodes	mechanism
Due to the phase delay inherent in a high-frequency AC signal propagating through the body	mechanism
a phase difference can be observed between pairs of electrodes	mechanism
SkinTrack measures these phase differences to compute a 2D finger touch coordinate	mechanism
Strong Nash equilibrium ( SNE ) is an appealing solution concept when rational agents can form coalitions	background
A strategy profile is an SNE if no coalition of agents can benefit by deviating	background
validate the overall approach and show that the new conditions significantly reduce search tree size compared to using NE conditions alone	finding
We present the first general-purpose algorithms An SNE must simultaneously be a Nash equilibrium ( NE ) and the optimal solution of multiple non-convex optimization problems	mechanism
This makes even the derivation of necessary and sufficient mathematical equilibrium constraints difficult	mechanism
We show that forcing an SNE to be resilient only to pure-strategy deviations by coalitions	mechanism
unlike for NEs	mechanism
is only a necessary condition here	mechanism
Second we show that the application of Karush-Kuhn-Tucker conditions leads to another set of necessary conditions that are not sufficient	mechanism
Third we show that forcing the Pareto efficiency of an SNE for each coalition with respect to coalition correlated strategies is sufficient but not necessary We then develop a tree search algorithm for SNE finding At each node	mechanism
it calls an oracle to suggest a candidate SNE and then verifies the candidate	mechanism
We show that our new necessary conditions can be leveraged to make the oracle more powerful	mechanism
Experiments	method
How can we predict Smith 's main hobby if we know the main hobby of Smith 's friends ? Can we measure the confidence in our predic- tion if we are given the main hobby of only a few of Smith 's friends ?	background
results demonstrate that our algorithm outperforms other algorithms on graphs with less smoothness and low label density	finding
Providing a confidence level for the classification prob- lem is important because most nodes in real world networks tend to have few neighbors	mechanism
and thus a small amount of evidence	mechanism
Our contributions are three-fold : ( a ) novel algorithm ; we propose a semi-supervised learning algorithm that converges fast	mechanism
and provides the confidence estimate	mechanism
( b ) theoretical analysis ; we show the solid theoretical foundation of our algo- rithm and the connections to label propagation and Bayesian inference ( c ) empirical analysis ; we perform extensive experiments on three dif- ferent real networks Specifically	method
the experimental	method
Given their large energy footprints and the availability of building energy management systems	background
airports are uniquely positioned to take advantage of demand response ( DR ) programs	background
Therefore further studies should be carried out to conclude the potential of flight schedules in improving accuracies of energy prediction baselines	background
test results reveals that a model	finding
which has trained over specific seasonal data with only time-of-week and temperature as inputs	finding
has the best prediction performance The number of passengers of departure flight schedules is shown to have a positive relationship to the load	finding
but does not improve the model accuracy significantly	finding
However since this study is done for the spring season	finding
when heating ventilating	finding
and air conditioning ( HVAC ) systems run the least	finding
the results may not represent other seasons with high cooling or heating demand	finding
Specifically the authors propose piece-wise linear regression models	mechanism
For the given period of April and May	method
The fair division of indivisible goods has long been an important topic in economics and	background
more recently computer science	background
we show that even when the number of goods is larger than the number of agents by a linear fraction	finding
envy-free allocations are unlikely to exist	finding
We then show that when the number of goods is larger by a logarithmic factor	finding
such allocations exist with high probability show that the asymptotic behavior of the theory holds even when the number of goods and agents is quite small	finding
We demonstrate that there is a sharp phase transition from nonexistence to existence of envy-free allocations	finding
and that on average the computational problem is hardest at that transition	finding
We investigate the existence of envyfree allocations of indivisible goods	mechanism
Under additive valuations	method
We support these results experimentally and	method
Non-Technical Loss ( NTL ) represents a major challenge when providing reliable electrical service in developing countries	background
where it often accounts for 11-15 % of total generation capacity [ 1 ]	background
NTL is caused by a variety of factors such as theft	background
unmetered homes and inability to pay	background
which at volume can lead to system instability	background
grid failure and major financial losses for providers	background
Both classes of approaches can provide a confidence interval based on the amount of detected NTL	finding
We see that both are quite effective	finding
but that the data-driven class is significantly easier to implement	finding
In this paper	mechanism
we investigate error sources and techniques The model-driven class considers the primary sources of state uncertainty including line losses	mechanism
meter consumption meter calibration error	mechanism
packet loss and sample synchronization error	mechanism
In the data-driven class	mechanism
we use two approaches that learn grid state based on training data The first approach uses a regression technique on an NTL-free period of grid operation to capture the relationship between state error and total consumption The second approach uses an SVM trained on synthetic NTL data	mechanism
We adopt and compare two classes of approaches for detecting NTL : ( 1 ) model- driven and ( 2 ) data- driven	method
We experimentally evaluate and compare the approaches on wireless meter data collected from a 525-home microgrid deployed in Les Anglais	method
Haiti	method
In both cases	method
we are able to experimentally evaluate to what degree we can reliably separate NTL from total losses	method
Kidney exchanges are organized markets where patients swap willing but incompatible donors	background
In the last decade	background
kidney exchanges grew from small and regional to large and national -- -and soon	background
international	background
This growth results in more lives saved	background
but exacerbates the empirical hardness of the $ \mathcal { NP } $ -complete problem of optimally matching patients to donors	background
show that indeed	finding
small numbers of attributes suffice	finding
In this paper	mechanism
we observe that if the kidney exchange compatibility graph can be encoded by a constant number of patient and donor attributes	mechanism
We give necessary and sufficient conditions for losslessly shrinking the representation of an arbitrary compatibility graph	mechanism
Then using real compatibility graphs from the UNOS nationwide kidney exchange	mechanism
we show how many attributes are needed to encode real compatibility graphs	mechanism
The experiments	method
Friendsourcing consists of broadcasting questions and help requests to friends on social networking sites	background
Results indicate that large extrinsic rewards increase friends ' response rates without reducing the relationship strength between friends	finding
Additionally the extrinsic rewards allow requesters to explain away the failure of friendsourcing requests and thus preserve their perceptions of relationship ties with friends	finding
we conducted an experiment on a new friendsourcing platform - Mobilyzr	method
Crowdsourcing offers a powerful new paradigm for online work	background
We also contribute a set of design patterns that may be informative for other systems aimed at supporting big picture thinking in small pieces	background
In this paper	mechanism
we explore the idea that a computational system can scaffold an emerging interdependent	mechanism
big picture view entirely through the small contributions of individuals	mechanism
each of whom sees only a part of the whole	mechanism
To investigate the viability	method
strengths and weaknesses of this approach we instantiate the idea in a prototype system for accomplishing distributed information synthesis and evaluate its output across a variety of topics	method
Our main theoretical result is that any graph specifying synergistic and antagonistic pairs can arise even from a restricted class of cooperative games	finding
We think of a pair of agents as synergistic ( resp	mechanism
antagonistic ) if the Shapley value of one agent when the other agent participates in a joint effort is higher ( resp	mechanism
lower ) than when the other agent does not participate We also study the computational complexity of determining whether a given pair of agents is synergistic	mechanism
Finally we use the concepts developed in the paper to uncover the structure of synergies in two real-world organizations	method
the European Union and the International Monetary Fund	method
How do users behave if they can tag each other in social networks ? Twitter lists can be regarded as the tagging process ; a user ( i	background
e	background
tagger ) creates a list with a name ( i	background
e	background
tag ) and adds other users ( i	background
e	background
tagged users ) into the list	background
This tagging network is by nature different from the resource tagging networks ( e	background
g	background
Flickr and Delicious ) because users on this network can tag each other	background
This study sheds light on the underlying characteristics of the interactive tagging network	background
which is relevant to the social scientists and the system designers of the tagging systems	background
we found the pervasive patterns across the different tagging networks	finding
and the interactive patterns within the interactive tagging network	finding
By quantitatively studying million-scale networks	method
Social media is an increasingly important part of modern life We propose changes that Twitter and other social platforms should make to promote fuller access to users with visual impairments	background
Our findings illuminate the importance of the ability to use social media for people who are blind	finding
while also highlighting the many challenges such media currently present this user base	finding
including difficulty in creating profiles	finding
in awareness of available features and settings	finding
in controlling revelations of one 's disability status	finding
and in dealing with the increasing pervasiveness of image-based content	finding
via a combination of surveys of blind Twitter users	method
large-scale analysis of tweets from and Twitter profiles of blind and sighted users	method
and analysis of tweets containing embedded imagery	method
verify the superiority of the proposed approach	finding
We first pre-train a number of concept classifiers using data from other sources	mechanism
Then we evaluate the semantic correlation of each concept w	mechanism
r	mechanism
t	mechanism
the event of interest	mechanism
After further refinement to take prediction inaccuracy and discriminative power into account	mechanism
we apply the discovered concept classifiers on all test videos and obtain multiple score vectors	mechanism
These distinct score vectors are converted into pairwise comparison matrices and the nuclear norm rank aggregation framework is adopted to seek consensus	mechanism
we propose an efficient	mechanism
highly scalable algorithm that is an order of magnitude faster than existing alternatives	mechanism
Experiments on recent TRECVID datasets	method
We introduce disciplined convex stochastic programming ( DCSP )	mechanism
a modeling framework	mechanism
by allowing modelers to naturally express a wide variety of convex stochastic programs in a manner that reflects their underlying mathematical representation DCSP allows modelers to express expectations of arbitrary expressions	mechanism
partial optimizations and chance constraints across a wide variety of convex optimization problem families ( e	mechanism
g	mechanism
linear quadratic second order cone	mechanism
and semidefinite programs	mechanism
We illustrate DCSP 's expressivity through a number of sample implementations of problems drawn from the operations research	method
finance and machine learning literatures	method
Nudging behaviors through user interface design is a practice that is well-studied in HCI research	background
Corporations often use this knowledge to modify online interfaces to influence user information disclosure	background
We show that ( 1 ) a set of images	finding
biased toward more revealing figures	finding
change subjects ' personal views of appropriate information to share ; ( 2 ) that shifts in perceptions significantly increases the probability that a subject divulges personal information ; and ( 3 ) that these shift also increases the probability that the subject advises others to do so	finding
Our main contribution is a key mechanism by which norm-shaping designs can change beliefs and subsequent disclosure behaviors	finding
In this paper	method
we experimentally test empirically identifying	method
A class of models using directed	mechanism
weighted graphs is introduced	mechanism
A computationally tractable algorithm from observed time series data is presented The performance guarantees of this algorithm for prediction are outlined under several assumptions on the properties of the dynamics of the system of agents and on the true values of the parameters	mechanism
These guarantees are tested empirically through simulation studies using several random graph models	method
Finding densely connected subgraphs	background
also called communities	background
in networks are of interest for many applications In previous work	background
we showed an optimization method for efficiently finding subgraphs denser than the overall network [ 1 ]	background
This result is derived from our studies of network processes	background
dynamical processes that model interactions between individual agents in networks ( i	background
e	background
spread of infection or cascading failures )	background
that these subgraphs in the sense that there are no other subgraphs in the network isomorphic to these subgraphs	mechanism
This is one of the oldest non-trivial problems in computational geometry yet despite a long history of research the previous fastest running times for computing a ( 1+ ) -approximate geometric median were O ( d n 4/3 8/3 ) by Chin et	background
al O ( d exp 4 log 1 ) by Badoiu et	background
al O ( nd + poly ( d	background
1 ) ) by Feldman and Langberg	background
and the polynomial running time of O ( ( nd ) O ( 1 ) log1/ ) by Parrilo and Sturmfels and Xue and Ye	background
In this paper we provide faster algorithms While our O ( d 2 ) is a fairly straightforward application of stochastic subgradient descent	mechanism
our O ( nd log 3 n / ) time algorithm is a novel long step interior point method	mechanism
We start with a simple O ( ( nd ) O ( 1 ) log1/ ) time interior point method and show how to improve it	mechanism
ultimately building an algorithm that is quite non-standard from the perspective of interior point literature	mechanism
Our result is one of few cases of outperforming standard interior point theory	mechanism
Furthermore it is the only case we know of where interior point methods yield a nearly linear time algorithm for a canonical optimization problem that traditionally requires superlinear time	mechanism
Computer security problems often occur when there are disconnects between users understanding of their role in computer security and what is expected of them	background
that inform future directions for better design and research into security interventions	background
Our findings emphasize the need for better understanding of how users computers get infected	background
so that we can more effectively design user-centered mitigations	background
produced engagement as the overarching theme	finding
whereby participants with greater engagement in computer security and maintenance did not necessarily have more secure computer states Thus	finding
user engagement alone may not be predictive of computer security	finding
We identify several other themes	finding
We built and deployed the Security Behavior Observatory ( SBO ) from participants home computers	mechanism
Combining SBO data with user interviews	method
this paper presents a qualitative study comparing users attitudes	method
behaviors and understanding of computer security to the actual states of their computers Qualitative inductive thematic analysis of the interviews	method
The environment of a living cell is vastly different from that of an in vitro reaction system	background
an issue that presents great challenges to the use of in vitro models	background
or computer simulations based on them	background
for understanding biochemistry in vivo	background
Virus capsids make an excellent model system for such questions because they typically have few distinct components	background
making them amenable to in vitro and modeling studies	background
yet their assembly can involve complex networks of possible reactions that can not be resolved in detail by any current experimental technology	background
We previously fit kinetic simulation parameters to bulk in vitro assembly data to yield a close match between simulated and real data	background
and then used the simulations to study features of assembly that can not be monitored experimentally The work demonstrates how computer simulations can help us understand how assembly might differ between the in vitro and in vivo environments and what features of the cellular environment account for these differences	background
exhibit surprising behavioral complexity	finding
with distinct effects often acting synergistically to drive efficient assembly and alter pathways relative to the in vitro model	finding
We bypass that limitation by applying analytical models of nucleic acid effects to adjust kinetic rate parameters learned from in vitro data to see how these adjustments	mechanism
singly or in combination	mechanism
might affect fine-scale assembly progress	mechanism
The resulting simulations	method
Our main result is that biased games satisfying certain mild conditions always admit an equilibrium	finding
We present that we call biased games	mechanism
In these games	mechanism
a player 's utility is influenced by the distance between his mixed strategy and a given base strategy	mechanism
We argue that biased games capture important aspects of the interaction between software agents We also tackle the computation of equilibria in biased games	mechanism
A kidney exchange is an organized barter market where patients in need of a kidney swap willing but incompatible donors	background
Determining an optimal set of exchanges is theoretically and empirically hard Traditionally	background
exchanges took place in cycles	background
with each participating patient-donor pair both giving and receiving a kidney	background
The recent introduction of chains	background
where a donor without a paired patient triggers a sequence of donations without requiring a kidney in return	background
increased the efficacy of fielded kidney exchanges -- -while also dramatically raising the empirical computational hardness of clearing the market in practice Finally	background
we note that our position-indexed chain-edge formulation can be modified in a straightforward way to take post-match edge failure into account	background
under the restriction that edges have equal probabilities of failure	background
Post-match edge failure is a primary source of inefficiency in presently-fielded kidney exchanges	background
we show that our new models are competitive with all existing solvers -- -in many cases outperforming all other solvers by orders of magnitude	finding
In this paper	mechanism
we address the tractable clearing of kidney exchanges with short cycles and chains that are long but bounded	mechanism
This corresponds to the practice at most modern fielded kidney exchanges	mechanism
We introduce three new integer programming formulations	mechanism
two of which are compact Furthermore	mechanism
one of these models has a linear programming relaxation that is exactly as tight as the previous tightest formulation ( which was not compact ) for instances in which each donor has a paired patient	mechanism
We show how to implement such failure-aware matching in our model	mechanism
and also extend the state-of-the-art general branch-and-price-based non-compact formulation for the failure-aware problem to run its pricing problem in polynomial time	mechanism
On real data from the UNOS nationwide exchange in the United States and the NLDKSS nationwide exchange in the United Kingdom	method
as well as on generated realistic large-scale data	method
Kidney exchange is a barter market where patients trade willing but medically incompatible donors	background
These trades occur via cycles	background
where each patient-donor pair both gives and receives a kidney	background
and via chains	background
which begin with an altruistic donor who does not require a kidney in return For logistical reasons	background
the maximum length of a cycle is typically limited to a small constant	background
while chains can be much longer	background
Given a compatibility graph of patient-donor pairs	background
altruists and feasible potential transplants between them	background
finding even a maximum-cardinality set of vertex-disjoint cycles and chains is NP-hard	background
There has been much work on developing provably optimal solvers that are efficient in practice	background
One of the leading techniques has been branch and price	background
where column generation is used to incrementally bring cycles and chains into the optimization model on an as-needed basis	background
This shows incorrectness of two leading branch-and-price solvers that suggested polynomial-time chain pricing algorithms	background
In particular only positive-price columns need to be brought into the model	mechanism
An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy	background
In a system with redundant requests	background
each job that arrives to the system is copied and dispatched to multiple servers	background
As soon as the first copy completes service	background
the job is considered complete	background
and all remaining copies are deleted	background
We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity	finding
We propose a theoretical model of redundancy	mechanism
the Redundancy-d system	mechanism
in which each job sends redundant copies to d servers chosen uniformly at random	mechanism
We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers	mechanism
Complex networks have been shown to exhibit universal properties	background
with one of the most consistent patterns being the scale-free degree distribution	background
and we show the pervasiveness of the power-hop	finding
by identifying another power-law pattern that describes the relationship between the fractions of node pairs C ( r ) within r hops and the hop count r	mechanism
This scale-free distribution is pervasive and describes a large variety of networks	mechanism
ranging from social and urban to technological and biological networks In particular	mechanism
inspired by the definition of the fractal correlation dimension D2 on a point-set	mechanism
we consider the hop-count r to be the underlying distance metric between two vertices of the network	mechanism
and we examine the scaling of C ( r ) with r	mechanism
We find that this relationship follows a power-law in real networks within the range 2 r d	mechanism
where d is the effective diameter of the network	mechanism
that is the 90-th percentile distance We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h	mechanism
We provide theoretical justification for this pattern under successful existing network models	method
while we analyze a large set of real and synthetic network datasets	method
Modernsmartphoneplatformshavemillionsofapps manyofwhich request permissions to access private data and resources	background
like user accounts or location	background
Prior research has shown that users are often unaware of	background
if not uncomfortable with	background
many of their permission settings	background
Prior work also suggests that it is theoretically possible to predict many of the privacy settings a user would want by asking the user a small number of questions	background
We discuss the implications of our results for mobile permission management and the design of personalized privacy assistant solutions	background
The results of our study are encouraging	finding
We find that 78	finding
7 % of the recommendations made by the PPA were adopted by users	finding
Following initial recommendations on permission settings	finding
participants were motivated to further review and modify their settings with daily privacy nudges	finding
Despite showing substantial engagement with these nudges	finding
participants only changed 5	finding
1 % of the settings previously adopted based on the PPAs recommendations	finding
The PPA and its recommendations were perceived as useful and usable	finding
in which we implemented and evaluated a Personalized Privacy Assistant ( PPA )	mechanism
We report on a field study ( n=72 ) with participants using their own Android devices	method
The rise of Internet-scale networks	background
such as web graphs and social media with hundreds of millions to billions of nodes	background
presents new scientific opportunities	background
such as overlapping community detection to discover the structure of the Internet	background
or to analyze trends in online social behavior	background
we demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours	finding
our method is several orders of magnitude faster	finding
with competitive or improved accuracy at overlapping community detection	finding
We propose a scalable approach By applying a succinct representation of networks as a bag of triangular motifs	mechanism
developing a parsimonious statistical model	mechanism
deriving an efficient stochastic variational inference algorithm	mechanism
and implementing it as a distributed cluster program via the Petuum parameter server system	method
Compared to other state-of-the-art probabilistic network approaches	method
Different propagation characteristics of the wave modes	background
their distinctive sensitivities to different types and ranges of EOCs	background
and to different damage scenarios	background
make the interpretation of diffuse-field guided-wave signals a challenging task	background
We show that such a subset is less affected by EOCs compared to the complete time-traces of the signals	finding
Moreover it is shown that the effects of damage on the energy of this subset suppress those of EOCs	finding
This paper proposes an unsupervised feature-extraction method for online damage detection of pipelines under varying EOCs	mechanism
A set of signals from the undamaged state of a pipe are used as reference records	mechanism
The reference dataset is used to extract the aforementioned sparse representation	mechanism
During the monitoring stage	mechanism
the sparse subset	mechanism
representing the undamaged pipe	mechanism
will not accurately reconstruct the energy of a signal from a damaged pipe In other words	mechanism
such a sparse representation of guided-waves is sensitive to occurrence of damage	mechanism
Therefore the energy estimation errors are used as damage-sensitive features for damage detection purposes	mechanism
A diverse set of experimental analyses are conducted to verify the hypotheses of the proposed feature-extraction approach	method
and to validate the detection performance of the damage-sensitive features	method
The empirical validation of the proposed method includes ( 1 ) detecting a structural abnormality in an aluminum pipe	method
under varying temperature at different ranges	method
( 2 ) detecting multiple small damages of different types	method
at different locations	method
in a steel pipe	method
under varying temperature	method
( 3 ) detecting a structural abnormality in an operating hot-water piping system	method
under multiple varying EOCs	method
such as temperature	method
water flow rate	method
and inner pressure ; and ( 4 ) detecting a structural abnormality as the ratio of the damaged pipe 's signals in the reference dataset increases	method
While Bayesian methods are praised for their ability to incorporate useful prior knowledge	background
in practice priors that allow for computationally convenient or tractable inference are more commonly used	background
We prove that our method can generate asymptotically exact samples	finding
and demonstrate it	finding
We present a procedure : given an inferred false posterior and true prior	mechanism
our algorithm generates samples from the true posterior	mechanism
This transformation procedure	mechanism
which we call `` prior swapping '' works for arbitrary priors	mechanism
Notably its cost is independent of data size	mechanism
It therefore allows us	mechanism
in some cases	mechanism
to apply significantly less-costly inference procedures to more-sophisticated models than previously possible	mechanism
It also lets us quickly perform any additional inferences	mechanism
such as with updated priors or for many different hyperparameter settings	mechanism
without touching the data	mechanism
empirically on a number of models and priors	method
results that capture	finding
e	finding
g	finding
the conspicuous phenomenon of emergence and downfall of leaders in social networks	finding
We propose a family of models by reinforcement and penalization of their connections according to certain local laws of interaction	mechanism
The family of stochastic dynamical systems	mechanism
on the edges of a graph	mechanism
exhibits \emph { good } convergence properties	mechanism
in particular we prove a strong-stability result : a subset of binary matrices or graphs -- characterized by certain compatibility properties -- is a global almost sure attractor of the family of stochastic dynamical systems	mechanism
To illustrate finer properties of the corresponding strong attractor	method
we present some simulation	method
An age-old problem in the design of server farms is the choice of the task assignment policy	background
This is the algorithm that determines how to assign incoming jobs to servers	background
Popular policies include Round-Robin assignment	background
Join-the-Shortest-Queue Join-Queue-with-Least-Work and so on	background
We show that when server-side variability dominates runtime	finding
replication of jobs can be very beneficial	finding
We introduce the Replication-d algorithm	mechanism
where the job is considered `` done '' as soon as the first replica completes	mechanism
We provide an exact closed-form analysis of Replication-d	mechanism
We next introduce a much more general model	mechanism
one which takes both the inherent job size distribution and the server-side variability into account	mechanism
This is a departure from traditional queueing models which only allow for one `` size '' distribution We propose and analyze a new	mechanism
Replicate-Idle-Queue ( RIQ )	mechanism
which is designed to perform well given these dual sources of variability	mechanism
Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled a myriad of new applications in many different domains from personal entertainment to process and infrastructure online monitoring in large industrial sites	background
among other	background
We show that this platform is non-omnidirectional in the flight plane and that UAV-to-UAV communication ceases around 75m transmitting payloads up to 200m ( over 802	finding
11g @ 54MBps )	finding
Our work focuses on how one can use several small UAVs collaboratively We demonstrate how a TDMA overlay using 802	mechanism
11 radios on low-cost commercial-off-the-shelf ( COTS ) UAVs can be used to enable high channel utilization in multi-hop networks	mechanism
by avoiding mutual interference This paper presents an extensive network characterisation and modelling of the quality of the UAV-to-UAV link	mechanism
in terms of packet delivery ratio as a function of distance	mechanism
packet size and orientation	mechanism
Then we solve the mathematical problem of finding the optimal link length and number of hops that maximize the end-to-end throughput	mechanism
as we extend the network	mechanism
We validate our mathematical model with extensive experimental campaigns	method
More than 10 % of the population has dyslexia	background
and most are diagnosed only after they fail in school	background
Currently we are working with schools to put our approach into practice at scale to reduce school failure as a primary way dyslexia is diagnosed	background
revealed differences in how people with dyslexia read and write	finding
with 83 % accuracy in a held-out test set with 100 participants	finding
detection via machine learning models by watching how people interact with a linguistic web-based game : Dytective The design of Dytective is based on ( i ) the empirical linguistic analysis of the errors that people with dyslexia make	mechanism
( ii ) principles of language acquisition	mechanism
and ( iii ) specific linguistic skills related to dyslexia We trained a machine learning model that was able to predict dyslexia	mechanism
Experiments with 243 children and adults ( 95 with diagnosed dyslexia )	method
Results led to accurate reconstruction of several known regulatory and signaling pathways and to novel mechanistic insights highlighting the usefulness of temporal models	finding
We present TimePath	mechanism
a new method that integrates time series and static datasets TimePath uses an Integer Programming formulation to select a subset of pathways that	mechanism
together explain the observed dynamic responses	mechanism
Applying TimePath to study human response to HIV-1 We experimentally validated several of TimePaths predictions	method
results extend the reach of a recent theory of invariance to discriminative and kernelized features based on unitary kernels and outperform previous work in almost all cases on off-angle face matching while we are on par with the previous state-of-the-art on the LFW unsupervised and image-restricted protocols	finding
without any low-level image descriptors other than raw-pixels	finding
We propose an explicitly discriminative and 'simple ' approach	mechanism
the approach works well As a special case	mechanism
a single common framework can be used for face recognition and vice-versa for pose estimation We show that our main proposed method ( DIKF ) can perform well under very challenging large-scale semisynthetic face matching and pose estimation protocols with unaligned faces using no landmarking whatsoever	mechanism
In practice Our theoretical We additionally benchmark on CMU MPIE	method
Large graphs are prevalent in many applications and enable a variety of information dissemination processes	background
e	background
g	background
meme virus and influence propagation	background
How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way ( e	background
g	background
stop a virus propagation	background
facilitate the propagation of a piece of good idea	background
etc ) ? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models	background
In addition we reveal the intrinsic relationship between edge deletion and node deletion problems	finding
results validate the effectiveness and efficiency of the proposed algorithms	finding
We propose effective	mechanism
scalable algorithms	mechanism
Experimental	method
Robust face detection in the wild is one of the ultimate components to support various facial related problems	background
i	background
e	background
unconstrained face recognition	background
facial periocular recognition	background
facial landmarking and pose estimation	background
facial expression recognition	background
3D facial model construction	background
etc	background
results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin	finding
and consistently achieves competitive results on FDDB against the recent state-of-the-art face detection methods	finding
In this paper	mechanism
we present a face detection approach named Contextual Multi-Scale Region-based Convolution Neural Network ( CMS-RCNN ) Similar to the region-based CNNs	mechanism
our proposed network consists of the region proposal component and the region-of-interest ( RoI ) detection component	mechanism
However far apart of that network	mechanism
there are two main contributions in our proposed network that play a significant role to achieve the state-of-the-art performance in face detection Firstly	mechanism
the multi-scale information is grouped both in region proposal and RoI detection to deal with tiny face regions	mechanism
Secondly our proposed network allows explicit body contextual reasoning in the network inspired from the intuition of human vision system	mechanism
The proposed approach is benchmarked on two recent challenging face detection databases	method
i	method
e	method
the WIDER FACE Dataset which contains high degree of variability	method
as well as the Face Detection Dataset and Benchmark ( FDDB ) The experimental	method
Recently fair division theory has emerged as a promising approach for allocation of multiple computational resources among agents While in reality agents are not all present in the system simultaneously	background
previous work has studied static settings where all relevant information is known upfront	background
We believe that our work informs the design of superior multiagent systems	background
and at the same time expands the scope of fair division theory by initiating the study of dynamic and fair resource allocation mechanisms	background
On the conceptual level	mechanism
we develop a dynamic model of fair division	mechanism
and propose desirable axiomatic properties On the technical level	mechanism
we construct two novel mechanisms that provably satisfy some of these properties	mechanism
and analyze their performance using real data	method
Our main result is a characterization of worst-case optimal truthful estimators	finding
which provably outperform the median	finding
for possibly asymmetric distributions with bounded support	finding
taking a game-theoretic viewpoint In our setting	mechanism
samples are supplied by strategic agents	mechanism
who wish to pull the estimate as close as possible to their own value In this setting	mechanism
the sample mean gives rise to manipulation opportunities	mechanism
whereas the sample median does not We show that when the underlying distribution is symmetric	mechanism
there are truthful estimators that dominate the median	mechanism
and show it often achieves order of magnitude speedups over existing general-purpose optimization solvers	finding
This paper develops an approach a common general-purpose modeling framework	mechanism
Specifically we develop an algorithm based upon fast epigraph projections	mechanism
projections onto the epigraph of a convex function	mechanism
an approach closely linked to proximal operator methods	mechanism
We show that by using these operators	mechanism
we can solve any disciplined convex program without transforming the problem to a standard cone form	mechanism
as is done by current DCP libraries We then develop a large library of efficient epigraph projection operators	mechanism
mirroring and extending work on fast proximal algorithms	mechanism
for many common convex functions	mechanism
Finally we evaluate the performance of the algorithm	method
Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines	background
which could be of interest to studies on website profiling	background
Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models	background
First we find that legitimate but compromised websites constitute 33	finding
1 % of the malicious websites in our dataset	finding
with an accuracy of 95	finding
3 % Second	finding
we find that malicious URLs can be surprisingly long-lived	finding
with 10 % of malicious sites staying active for three months or more Third	finding
we observe that a significant number of URLs exhibit the same temporal pattern that suggests a flush-crowd behavior	finding
inflicting most of their damage during the first few days of appearance	finding
Finally the distribution of the visits to malicious sites per user is skewed	finding
with 1	finding
4 % of users visiting more than 10 malicious sites in 8 months	finding
we develop a classifier	mechanism
We conduct an extensive study and follow a website-centric and user-centric point of view We collect data from four online databases	method
including Symantec 's WINE Project	method
for a total of more than 600K malicious URLs and over 500K users	method
The design of revenue-maximizing combinatorial auctions	background
i	background
e	background
multi item auctions over bundles of goods	background
is one of the most fundamental problems in computational economics	background
unsolved even for two bidders and two items for sale	background
In the traditional economic models	background
it is assumed that the bidders ' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution Despite this strong and oftentimes unrealistic assumption	background
it is remarkable that the revenue-maximizing combinatorial auction remains unknown The most scalable automated mechanism design algorithms take as input samples from the bidders ' valuation distribution and then search for a high-revenue auction in a rich auction class	background
In this work	mechanism
we provide the first sample complexity analysis In particular	mechanism
we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying	mechanism
unknown distribution over bidder valuations	mechanism
for each of the auction classes in the hierarchy In addition to helping set automated mechanism design on firm foundations	mechanism
our results also push the boundaries of learning theory	mechanism
In particular the hypothesis functions used in our contexts are defined through multi stage combinatorial optimization procedures	mechanism
rather than simple decision boundaries	mechanism
as are common in machine learning	mechanism
Online content have become an important medium to disseminate information and express opinions	background
This paper is an extended abstract of the 2012 ACM SIGKDD best doctoral dissertation award of Ahmed [ 2011 ]	background
and provide algorithms that create a structured representation of the otherwise unstructured content	mechanism
We leverage the expressiveness of latent probabilistic models ( e	mechanism
g	mechanism
topic models ) and non-parametric Bayes techniques ( e	mechanism
g	mechanism
Dirichlet processes )	mechanism
and give online and distributed inference algorithms that scale to terabyte datasets and adapt the inferred representation with the arrival of new documents	mechanism
It remains a challenge to detect associations between genotypes and phenotypes because of insufficient sample sizes and complex underlying mechanisms involved in associations	background
Availability and implementation : Software is available at http : //www	background
sailing	background
cs	background
cmu	background
edu/	background
Contact : ude	background
umc	background
sc @ gnixpe	background
Results we show that NETAM finds significantly more phenotype-associated SNPs than traditional genotypephenotype association analysis under false positive control	finding
taking advantage of gene expression data and identified 477 significant path associations	finding
among which we analyzed paths related to beta-amyloid	finding
estrogen and nicotine pathways	finding
In this article	mechanism
we propose a novel method	mechanism
NETAM We take a network-driven approach : NETAM first constructs an association network	mechanism
where nodes represent SNPs	mechanism
gene traits or phenotypes	mechanism
and edges represent the strength of association between two nodes	mechanism
NETAM assigns a score to each path from an SNP to a phenotype	mechanism
and then identifies significant paths based on the scores	mechanism
In our simulation study	method
Furthermore we applied NETAM on late-onset Alzheimers disease data We also provide hypothetical biological pathways to explain our findings	method
that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data	finding
We introduce the Multiple Quantile Graphical Model ( MQGM )	mechanism
which extends the neighborhood selection approach of Meinshausen and Buhlmann The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others	mechanism
Our approach models a set of conditional quantiles of one variable as a sparse function of all others	mechanism
and hence offers a much richer	mechanism
more expressive class of conditional distribution estimates We establish that	mechanism
under suitable regularity conditions	mechanism
the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows	mechanism
even outside of the usual homoskedastic Gaussian data model	mechanism
We develop an efficient algorithm using the alternating direction method of multipliers	mechanism
We also describe a strategy	mechanism
Lastly we present detailed experiments	method
For each user	finding
we discover and explain a surprising	finding
bi-modal pattern of the inter-arrival time ( IAT ) of landed queries ( queries with user click-through ) we then notice the correlations among its parameters at the group level	finding
In this paper	mechanism
we present a novel	mechanism
user-and group-level framework	mechanism
M3A : Model	mechanism
MetaModel and Anomaly detection Specifically	mechanism
the model Camel-Log is proposed Thus	mechanism
we further propose the metamodel Meta-Click	mechanism
Combining Camel-Log and Meta-Click	mechanism
the proposed M3A has the following strong points : ( 1 ) the accurate modeling of marginal IAT distribution	mechanism
( 2 ) quantitative interpretations	mechanism
and ( 3 ) anomaly detection	mechanism
We studied what is probably the largest	method
publicly available query log that contains more than 30 million queries from 0	method
6 million users	method
The large number of user-generated videos uploaded on to the Internet everyday has led to many commercial video search engines	background
which mainly rely on text metadata for search	background
where our system outperformed other submissions in both text queries and video example queries	finding
thus demonstrating the effectiveness of our proposed approaches	finding
We present novel strategies in these topics and under different query inputs	mechanism
including pure textual queries and query by video examples	mechanism
Our proposed strategies have been incorporated into our submission for the TRECVID 2014 Multimedia Event Detection evaluation	method
Recent computer systems research has proposed using redundant requests to reduce latency	background
The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request	background
We find some surprising results First	finding
the response time of a fully redundant class follows a simple exponential distribution and that of the non-redundant class follows a generalized hyperexponential	finding
Second fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant We find that	finding
in many cases	finding
redundancy outperforms JSQ and Opt-Split with respect to overall response time	finding
making it an attractive solution	finding
This paper presents the first exact analysis of systems with redundancy We allow for any number of classes of redundant requests	mechanism
any number of classes of non-redundant requests	mechanism
any degree of redundancy	mechanism
and any number of heterogeneous servers	mechanism
In all cases we derive the limiting distribution of the state of the system	mechanism
In small ( two or three server ) systems	mechanism
we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes	mechanism
and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy	mechanism
We also compare redundancy with other approaches for reducing latency	method
such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and join-the-shortest-queue ( JSQ ) routing of a class	method
Work in human-computer interaction has generally assumed either a single user or a group of users working together in a shared virtual space Recent crowd-powered systems use a different model in which a dynamic group of individuals ( the crowd ) collectively form a single actor that responds to real-time performance tasks	background
e	background
g	background
controlling an on-screen character	background
driving a robot	background
or operating an existing desktop interface	background
Nowhere is the focus on the individual performer more finely resolved than in the study of the human psychomotor system	background
a mainstay topic in psychology that	background
largely owing to Fitts law	background
also has a legacy in HCI	background
This work contributes to the beginning of a predictive science for the general crowd actor model	background
In this paper	mechanism
we introduce the idea of the crowd actor as a way by modeling the crowd as a individual motor system performing pointing tasks	mechanism
We combined the input of 200 participants in a controlled offline experiment to demonstrate the inherent trade-offs between speed and errors based on personality	method
the number of constituent individuals	method
and the mechanism used to distribute work across the group	method
Finally 10 workers participated in a synchronous experiment to explore how the crowd actor responds in a real online setting	method
show that not only is our proposed tracker effective	finding
but also the solution path enables automatic pinpointing of potential tracking failures	finding
which can be readily utilized in an active learning framework to improve identity-aware multi-object tracking	finding
We propose an identity-aware multi-object tracker based on the solution path algorithm	mechanism
Our tracker not only produces based on cues such as recognition	mechanism
but also has the ability The tracker is formulated as a quadratic optimization problem with l0 norm constraints	mechanism
which we propose to solve with the solution path algorithm	mechanism
The algorithm successively solves the same optimization problem but under different lp norm constraints	mechanism
where p gradually decreases from 1 to 0	mechanism
Inspired by the success of the solution path algorithm in various machine learning tasks	mechanism
this strategy is expected to converge to a better local minimum than directly minimizing the hardly solvable l0 norm or the roughly approximated l1 norm constraints	mechanism
Furthermore the acquired solution path complies with the `` decision making process '' of the tracker	mechanism
which provides more insight to locating potential tracking errors	mechanism
Experiments	method
With the progressively trained CNN models	finding
we have achieved better gender classification results on the large-scale PCSO mugshot database with 400K images under occlusion and low-resolution settings	finding
compared to the one undergone traditional training	finding
Inspired by the trainable attention model via deep architecture	mechanism
and the fact that the periocular region is proven to be the most salient region for gender classification purposes	mechanism
we are able to design a progressive convolutional neural network training paradigm The network benefits from this attention shift and becomes more robust towards occlusions and low-resolution degradations	mechanism
In addition our progressively trained network is sufficiently generalized so that it can be robust to occlusions of arbitrary types and at arbitrary locations	mechanism
as well as low resolution	mechanism
Advances in fluorescence in situ hybridization ( FISH ) make it feasible to detect multiple copy-number changes in hundreds of cells of solid tumors	background
Studies using FISH	background
sequencing and other technologies have revealed substantial intra-tumor heterogeneity The evolution of subclones in tumors may be modeled by phylogenies	background
Tumors often harbor aneuploid or polyploid cell populations	background
Tests on simulated data show improved accuracy of the ploidy-based approach relative to prior ploidyless methods Tests on real data further demonstrate novel insights these methods offer into tumor progression processes	finding
Trees for DCIS samples are significantly less complex than trees for paired IDC samples Consensus graphs show substantial divergence among most paired samples from both sets	finding
Low consensus between DCIS and IDC trees may help explain the difficulty in finding biomarkers that predict which DCIS cases are at most risk to progress to IDC	finding
The FISHtrees software is available at ftp : //ftp	finding
ncbi	finding
nih	finding
gov/pub/FISHtrees	finding
We present FISHtrees 3	mechanism
0 which implements a ploidy-based tree building method based on mixed integer linear programming ( MILP ) The ploidy-based modeling in FISHtrees includes a new formulation of the problem of merging trees for changes of a single gene into trees modeling changes in multiple genes and the ploidy	mechanism
When multiple samples are collected from each patient	mechanism
varying over time or tumor regions	mechanism
it is useful to evaluate similarities in tumor progression among the samples	mechanism
Therefore we further implemented in FISHtrees 3	mechanism
0 a new method to build consensus graphs for multiple samples	mechanism
We validate FISHtrees 3	method
0 on a simulated data and on FISH data from paired cases of cervical primary and metastatic tumors and on paired breast ductal carcinoma in situ ( DCIS ) and invasive ductal carcinoma ( IDC )	method
In learning latent variable models ( LVMs )	background
it is important to effectively capture infrequent patterns and shrink model size without sacrificing modeling power	background
Various studies have been done to `` diversify '' a LVM	background
which aim to learn a diverse set of latent components in LVMs	background
and experimental results demonstrate the effectiveness and efficiency of our methods	finding
We propose two approaches that have complementary advantages	mechanism
One is to define diversity-promoting mutual angular priors which assign larger density to components with larger mutual angles based on Bayesian network and von Mises-Fisher distribution and use these priors to affect the posterior via Bayes rule	mechanism
We develop two efficient approximate posterior inference algorithms based on variational inference and Markov chain Monte Carlo sampling	mechanism
The other approach is to impose diversity-promoting regularization directly over the post-data distribution of components	mechanism
These two methods are applied to the Bayesian mixture of experts model to encourage the `` experts '' to be diverse	method
where our system outperforms previous approaches by a large gap	finding
Instead we propose a probabilistic model by jointly leveraging text and images	mechanism
To avoid hand-crafted feature engineering	mechanism
we design end-to-end features based on distributed representations of images and words The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images	mechanism
We evaluate our model and features on the WordNet hierarchies	method
Besides appearance information	background
the video contains temporal evolution	background
which represents an important and useful source of information about its content	background
Many video representation approaches are based on the motion information within the video	background
The common approach to extract the motion information is to compute the optical flow from the vertical and the horizontal temporal evolution of two consecutive frames	background
Our HMG pipeline with several additional speed-ups is able to achieve real-time video processing and outperforms several well-known descriptors including descriptors based on the costly optical flow	finding
In this work we propose a very efficient approach Our method is based on a simple temporal and spatial derivation	mechanism
which captures the changes between two consecutive frames	mechanism
The proposed descriptor	method
Histograms of Motion Gradients ( HMG )	method
is validated on the UCF50 human action recognition dataset	method
The applications of laser scanning technology are rapidly expanding in the civil engineering domain	background
LiDAR technology is now commonly used in the surveying and monitoring of large infrastructures In particular	background
tunnels have become key transport infrastructures	background
subjected to maintenance processes that allow quality checks for tunnel modifications or tunnel clearance and profile checks	background
demonstrating that tunnel management activities can definitely benefit from using mobile LiDAR by minimizing survey time and increasing productivity in dangerous environments	background
An accuracy of 100 % in detection of cross sections is achieved	finding
Only one of the cross sections shows a relative error in vertical clearance measurement higher than 1 %	finding
The results demonstrated the effectiveness of the developed approach for computing vertical clearances and	finding
The research described in this paper targets developing an approach based on ground based mobile LiDAR data	mechanism
The steps of this approach include extraction of cross sections orthogonal to the vehicle trajectory and road markings based on radiometric information	mechanism
and conversion of cross section to a two-dimensional profile to estimate the vertical clearance	mechanism
The validation of the developed approach is done using real-life case study	method
a road tunnel in southern Galicia	method
Spain	method
Complex event detection on unconstrained Internet videos has seen much progress in recent years	background
and achieve state-of-the-art performances	finding
In this paper	mechanism
we present a state-of-the-art event search system without any example videos	mechanism
Relying on the key observation that events ( e	mechanism
g	mechanism
dog show ) are usually compositions of multiple mid-level concepts ( e	mechanism
g	mechanism
`` dog '' `` theater	mechanism
'' and `` dog jumping '' )	mechanism
we first train a skip-gram model to measure the relevance of each concept with the event of interest The relevant concept classifiers then cast votes on the test videos but their reliability	mechanism
due to lack of labeled training videos	mechanism
has been largely unaddressed We propose to combine the concept classifiers based on a principled estimate of their accuracy on the unlabeled test videos A novel warping technique is proposed to improve the performance and an efficient highly-scalable algorithm is provided to quickly solve the resulting optimization	mechanism
We conduct extensive experiments on the latest TRECVID MEDTest 2014	method
MEDTest 2013 and CCV datasets	method
Kidney exchange where candidates with organ failure trade incompatible but willing donors	background
is a lifesaving alternative to the deceased donor waitlist	background
which has inadequate supply to meet demand While fielded kidney exchanges see huge benefit from altruistic kidney donors ( who give an organ without a paired needy candidate )	background
a significantly higher medical risk to the donor deters similar altruism with livers	background
We conclude with thoughts regarding the fielding of a nationwide liver or joint liverkidney exchange from a legal and computational point of view	background
In this paper	mechanism
we begin by proposing the idea of liver exchange	mechanism
and show on demographically accurate data that We then explore crossorgan donation where kidneys and livers can be bartered for each other	mechanism
We show theoretically that this multiorgan exchange provides linearly more transplants than running separate kidney and liver exchanges ; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool	mechanism
We support this result experimentally on demographically accurate multi-organ exchanges	method
Suspicious graph patterns show up in many applications	background
from Twitter users who buy fake followers	background
manipulating the social network	background
to botnet members performing distributed denial of service attacks	background
disturbing the network traffic graph	background
C atch S ync consistently outperforms existing competitors	finding
both in detection accuracy by 36p on Twitter and 20p on Tencent Weibo	finding
as well as in speed	finding
We propose a fast and effective method	mechanism
C atch S ync	mechanism
which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority We introduce novel measures and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots	mechanism
Thanks to careful design	mechanism
C atch S ync has the following desirable properties : ( a ) it is scalable to large datasets	mechanism
being linear in the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology	mechanism
without needing labeled data	mechanism
nor timing information	mechanism
and the like	mechanism
while still capable of using side information if available	mechanism
We applied C atch S ync on three large	method
real datasets 1-billion-edge Twitter social graph	method
3-billion-edge and 12-billion-edge Tencent Weibo social graphs	method
and several synthetic ones ;	method
Action recognition ( AR ) is one of the most important tasks in video analysis and computer vision	background
Recently a large number of related methods have been proposed	background
leaving a reasonable space for further exploring the insights underlying such type of infrared AR problem and accordingly designing proper techniques to further promote the performance on this specifically constructed InfAR dataset	background
Our results reveal : ( 1 ) In all	finding
dense trajectory feature can achieve the best performance while the appearance features	finding
e	finding
g	finding
HOG have relatively poorer performance ; ( 2 ) the encoding method of vector of locally aggregated descriptors is evidently better than that of the widely-used Fisher Vector ; ( 3 ) the late fusion facilitates a better performance than early fusion ; ( 4 ) action videos captured in winter is more discriminable than in summer ; ( 5 ) compared to appearance information	finding
the motion information is more essential for infrared action recognition and utilizing this information through deep CNN can improve greatly the performance	finding
The best performance achieved on our dataset is 76	finding
66 % ( Average Precision )	finding
Specifically we construct a new Infrared Action Recognition ( InfAR ) dataset captured at different times	mechanism
including in summer and winter	mechanism
and explore how discriminable actions in our InfAR dataset are with the state-of-the-art pipelines based on low-level features and deep convolutional neural network ( CNN )	mechanism
respectively	mechanism
Suppose you are a teacher	background
and have to convey a set of object-property pairs 'lions eat meat '	background
A good teacher will convey a lot of information	background
with little effort on the student side	background
What is the best and most intuitive way to convey this information to the student	background
without the student being overwhelmed ?	background
it is effective	finding
achieving excellent results on real data	finding
both with respect to our proposed metric	finding
but also with respect to encoding length demonstrate the effectiveness of groupNteach	finding
we provide a metric based on information theory	mechanism
We also design an algorithm	mechanism
groupNteach Our proposed groupNteach is scalable near-linear in the dataset size ; and it is intuitive	mechanism
conforming to well-known educational principles	mechanism
Experiments on real and synthetic datasets	method
It is common that users are interested in finding video segments	background
which contain further information about the video contents in a segment of interest	background
Results show that ( 1 ) text features play a crucial role in search performance	finding
and the combination of audio and visual features can not provide improvements ; ( 2 ) the consideration of contexts can not obtain better results ; and ( 3 ) due to the lack of training examples	finding
machine learning techniques can not improve the performance	finding
In this study	method
we explore the effectiveness of various video features on the performance of video hyperlinking	method
including subtitle metadata	method
content features ( i	method
e	method
audio and visual )	method
surrounding context as well as the combinations of those features	method
Besides we also test different search strategies over different types of queries	method
which are categorized according to their video contents	method
Comprehensive experimental studies have been conducted on the dataset of TRECVID 2015 video hyperlinking task	method
Clustering is the task of grouping a set of objects so that objects in the same cluster are more similar to each other than to those in other clusters	background
The crucial step in most clustering algorithms is to find an appropriate similarity metric	background
which is both challenging and problem-dependent	background
confirm several orders of magnitude speedup while still achieving state-of-the-art performance	finding
In this paper	mechanism
we propose a new structured Mahalanobis Distance Metric Learning method We formulate our problem as an instance of large margin structured prediction and prove that it can be solved very efficiently in closed-form	mechanism
The complexity of our method is ( in most cases ) linear in the size of the training dataset We further reveal a striking similarity between our approach and multivariate linear regression	mechanism
Experiments on both synthetic and real datasets	method
Matrix sketching is aimed at finding close approximations of a matrix by factors of much smaller dimensions	background
which has important applications in optimization and machine learning	background
Given a matrix A of size m by n	background
state-of-the-art randomized algorithms take O ( m * n ) time and space to obtain its low-rank decomposition	background
fully demonstrate the potential of our methods in large scale matrix sketching and related areas	finding
In this paper	mechanism
we propose the cascaded bilateral sampling ( CABS ) framework We start from demonstrating how the approximation quality of bilateral matrix sketching depends on the encoding powers of sampling	mechanism
In particular the sampled rows and columns should correspond to the code-vectors in the ground truth decompositions Motivated by this analysis	mechanism
we propose to first generate a pilot-sketch using simple random sampling	mechanism
and then pursue more advanced	mechanism
`` follow-up '' sampling on the pilot-sketch factors seeking maximal encoding powers In this cascading process	mechanism
the rise of approximation quality is shown to be lower-bounded by the improvement of encoding powers in the follow-up sampling step	mechanism
thus theoretically guarantees the algorithmic boosting property	mechanism
Computationally our framework only takes linear time and space	mechanism
and at the same time its performance rivals the quality of state-of-the-art algorithms consuming a quadratic amount of resources	mechanism
Empirical evaluations on benchmark data	method
The system is equipped with its own controller and attack detector	mechanism
and the goal of the attacker is to move the system to a target state while altering the system 's actuator input and sensor output to avoid detection	mechanism
We formulate a cost function that reflects the attacker 's goals	mechanism
and using dynamic programming	mechanism
we show that the optimal attack strategy reduces to a linear feedback of the attacker 's state estimate By changing the parameters of the cost function	mechanism
we show how an attacker can design optimal attacks to balance the control objective and the detection avoidance objective	mechanism
Finally we provide a numerical illustration based on a remotely-controlled helicopter under attack	method
At least 10 % of the global population has dyslexia	background
In the United States and Spain	background
dyslexia is associated with a large percentage of school drop out Our results suggest that Dytective is able to differentiate school age children with and without dyslexia in both English and Spanish speakers	background
We found children with and without dyslexia differed significantly in their performance on the game	finding
we designed a browser-based game	mechanism
Dytective to detect risk of dyslexia across the English and Spanish languages Dytective consists of linguistic tasks informed by analysis of common errors made by persons with dyslexia	mechanism
To evaluate Dytective	method
we conducted a user study with 60 English and Spanish speaking children between 7 and 12 years old	method
Acute hypotensive episodes ( AHEs ) are serious clinical events in intensive care units ( ICUs )	background
and require immediate treatment to prevent patient injury	background
HeartCast was found to outperform other state-of-the-art methods found in the literature with a 13	finding
7 % improvement in classification accuracy	finding
We propose HeartCast	mechanism
a model that extracts essential features from such data HeartCast combines a non-linear support vector machine with best-feature extraction via analysis of the baseline threshold	mechanism
quartile parameters and window size of the physiological signals	mechanism
Our approach has the following benefits : ( a ) it extracts the most relevant features ; ( b ) it provides the best results for identification of an AHE event ; ( c ) it is fast and scales with linear complexity over the length of the window ; and ( d ) it can manage missing values and noise/outliers by using a best-feature extraction method	mechanism
We performed experiments on data continuously captured from physiological time series of ICU patients ( roughly 3 GB of processed data )	method
Summary An important experimental design question for high-throughput time series studies is the number of replicates required for accurate reconstruction of the profiles	background
Due to budget and sample availability constraints	background
more replicates imply fewer time points and vice versa These results provide theoretical support to the large number of high-throughput time series experiments that do not use replicates	background
we observe that	finding
under reasonable noise levels	finding
autocorrelations in the time series data allow dense sampling to better determine the correct levels of non-sampled points when compared to replicate sampling	finding
by developing a theoretical framework that focuses on a restricted yet expressive set of possible curves over a wide range of noise levels and by analyzing real expression data	mechanism
A Java implementation of our framework can be used	mechanism
For both the theoretical analysis and experimental data	method
results show that our method archives better performance than Faster R-CNN on both hands on wheel detection and cell-phone usage detection while remaining at similar testing cost our approach obtains higher accuracy	finding
is less time consuming and is independent to landmarking	finding
The groundtruth database will be publicly available	finding
In this paper	mechanism
we present an advanced deep learning based approach we propose Multiple Scale Faster-RCNN ( MSFRCNN ) approach that uses a standard Region Proposal Network ( RPN ) generation and incorporates feature maps from shallower convolution feature maps	mechanism
i	mechanism
e	mechanism
conv3 and conv4	mechanism
for ROI pooling	mechanism
In our driver distraction detection framework	mechanism
we first make use of the proposed MS-FRCNN to detect individual objects	mechanism
namely a hand	mechanism
a cell-phone and a steering wheel Then	mechanism
the geometric information is extracted to determine if a cell-phone is being used or how many hands are on the wheel	mechanism
The proposed approach is demonstrated and evaluated on the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge database and the challenging Strategic Highway Research Program ( SHRP-2 ) face view videos that was acquired to monitor drivers under naturalistic driving conditions The experimental Compare to the state-of-the-art cell-phone usage detection	method
What is a fair way to assign rooms to several housemates	background
and divide the rent between them ? This is not just a theoretical question : many people have used the Spliddit website to obtain envy-free solutions to rent division instances	background
But envy freeness	background
in and of itself	background
is insufficient to guarantee outcomes that people view as intuitive and acceptable	background
Based on these results	background
the maximin solution has been deployed on Spliddit since April 2015	background
and identify the maximin solution	finding
which maximizes the minimum utility subject to envy freeness	finding
as the most attractive that the maximin solution gives rise to significant gains in terms of our optimization objectives demonstrates that people find the maximin solution to be significantly fairer than arbitrary envy-free solutions ; this user study is unprecedented in that it asks people about their real-world rent division instances	finding
We develop a general algorithmic framework	mechanism
We then study the relations between natural optimization objectives	method
We demonstrate in theory and using experiments on real data from Spliddit	method
Finally a user study with Spliddit users as subjects	method
Weakly supervised methods have recently become one of the most popular machine learning methods since they are able to be used on large-scale datasets without the critical requirement of richly annotated data	background
Our uniform method is able to achieve competitive results in various face analysis applications	finding
such as occlusion detection	finding
face recognition gender classification	finding
twins verification and facial attractiveness analysis	finding
In this paper	mechanism
we present a novel	mechanism
self-taught discriminative approach in the weakly supervised framework	mechanism
Our method can find regions which are discriminative across classes yet consistent within a class and can solve many face related problems The proposed method first trains a deep face model with high discriminative capability to extract facial features The hypercolumn features are then used to give pixel level representation for better classification performance along with discriminative region detection	mechanism
In addition calibration approaches are proposed to enable the system to deal with multi-class and mixed-class problems The system is also able to detect multiple discriminative regions from one image	mechanism
Component-based modeling can be used to split large models into partial models to reduce modeling complexity	background
In this paper	mechanism
we propose a component-based hybrid system verification approach that combines the advantages of component-based modeling e	mechanism
g	mechanism
reduced model complexity with the advantages of formal verification e	mechanism
g	mechanism
guaranteed contract compliance Our strategy is to decompose the system into components	mechanism
verify their local safety individually and compose them to form an overall system that provably satisfies a global contract	mechanism
without proving the whole system We introduce the necessary formalism and a technique such that safety properties provably emerge from component safety	mechanism
Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community	background
The efficacy and the scalability of WELL have been extensively demonstrated The comprehensive results demonstrate that WELL outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data	finding
In addition the results also verify that WELL is robust to the level of noisiness in the video data	finding
Notably WELL trained on sufficient noisy web labels is able to achieve a comparable accuracy to supervised learning methods trained on the clean manually-labeled data	finding
this paper proposes a novel method called WEbly-Labeled Learning ( WELL )	mechanism
which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human WELL introduces a number of novel multi-modal approaches to incorporate meaningful prior knowledge called curriculum from the noisy web videos	mechanism
To investigate this problem	method
we empirically study the curriculum constructed from the multi-modal features of the videos collected from YouTube and Flickr	method
on two public benchmarks	method
including the largest multimedia dataset and the largest manually-labeled video set experimental	method
Brain-computer interfaces ( BCIs ) have the potential to restore motor abilities to paralyzed individuals	background
These systems act by reading motor intent signals directly from the brain and using them to control	background
for example the movement of a cursor on a computer screen or the motion of a robotic limb	background
To construct a BCI	background
a mapping must be specified that dictates how neural activity will actuate the device	background
Here we forward an alternate approach to the BCI design problem	mechanism
using ideas from optimal control theory	mechanism
We first argue that the brain can be considered as an optimal controller	mechanism
We then introduce a mathematical definition of BCI usability	mechanism
and formulate the BCI design problem as a constrained optimization problem that maximizes this usability	mechanism
Teaching chess to students with learning disabilities has been shown to benefit their school performance in unrelated domains At the same time	background
chess involves skills that are highly correlated with dyslexia	background
such as visuospatial and calculation abilities Therefore	background
dyslexia might have an impact on how people learn and play chess using a computer	background
suggesting that chess may be useful as a fun way to help people with dyslexia improve their abilities	background
We could not find significant differences on four dependent measures out of the twelve measures we collected	finding
In this paper	mechanism
we created a online chess game	mechanism
we carried out a within-subject experiment with 62 participants	method
31 of them with diagnosed dyslexia	method
Participants used an instrumented web-based chess learning platform that we developed to ( i ) complete lessons on how to play chess and about chess theory	method
( ii ) work through exercises designed to test and reaffirm their skills	method
and ( iii ) play chess against a computer opponent	method
The attacker performs an integrity attack in order to move the system to a target state while evading detection over a finite time window	background
Finally we demonstrate our proposed attack strategy	finding
We formulate and solve an optimal control problem We provide a sufficient condition for the existence of an optimal attack sequence	mechanism
in a numerical example	method
we prove that the procedure is faithful in the population setting	finding
yielding no false negatives The approach leads to computational and statistical advantages over fitting a full model	finding
and provides an effective	finding
practical approach to variable screening in convex regression	finding
Under the assumption that the true regression function is convex and sparse	mechanism
we develop a screening procedure Our approach is a two-stage quadratic programming method that estimates a sum of one-dimensional convex functions	mechanism
followed by one-dimensional concave regression fits on the residuals	mechanism
In contrast to previous methods for sparse additive models	mechanism
the optimization is finite dimensional and requires no tuning parameters for smoothness	mechanism
and introduce algorithms	mechanism
Under appropriate assumptions We give a finite sample statistical analysis	method
Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in imperfect-information games	background
They show that running counterfactual regret minimization on such abstractions leads to good strategies in the original games	finding
We develop the first general	mechanism
algorithm-agnostic solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions	mechanism
when implemented in the original ( perfect-recall ) game	mechanism
Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions for which any such results have been obtained Further	mechanism
our analysis is tighter in two ways	mechanism
each of which can lead to an exponential reduction in the solution quality error bound	mechanism
We then show that for extensive-form games that satisfy certain properties	mechanism
the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem	mechanism
where the increase in our bound is the distance function This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds	mechanism
We proceed to show a divide in the class of abstraction problems	mechanism
If payoffs are at the same scale at all information sets considered for abstraction	mechanism
the input forms a metric space	mechanism
and this immediately yields a $ 2 $ -approximation algorithm for abstraction Conversely	mechanism
if this condition is not satisfied	mechanism
we show that the input does not form a metric space	mechanism
Finally we provide computational experiments to evaluate the practical usefulness of the abstraction techniques	method
With the availability of high resolution digital technology	background
there has been increased interest in developing statistical and image processing techniques that can enhance the existing capabilities of analyzing works of art for authenticity This method is also valuable in determining whether an original painting has undergone any modifications	background
given that a representation of the initial version is available	background
we are not only able to distinguish between a low-quality digitized representation of a painting and its forgery	finding
but also specifically indicate where the differences occur and where the replica is particularly faithful to the original	finding
This work explores the merits of using advanced correlation filters in supplementing art experts efforts We show that by training the optimal trade-off synthetic discriminant function ( OTSDF ) filter on each section of a coarsely parceled image of an original painting	mechanism
Abstract SummaryWith the rapid advances in technologies of microarray and massively parallel sequencing	background
data of multiple omics sources from a large patient cohort are now frequently seen in many consortium studies Effective multi-level omics data integration has brought new statistical challenges	background
One important biological objective of such integrative analysis is to cluster patients in order to identify clinically relevant disease subtypes	background
which will form basis for tailored treatment and personalized medicine	background
Several methods have been proposed in the literature for this purpose	background
including the popular iCluster method used in many cancer applications	background
When clustering high-dimensional omics data	background
effective feature selection is critical for better clustering accuracy and biological interpretation	background
It is also common that a portion of `` scattered samples '' has patterns distinct from all major clusters and should not be assigned into any cluster as they may represent a rare disease subcategory or be in transition between disease subtypes	background
of the iCluster factor model by an overlapping sparse group lasso penalty on the omics features using prior knowledge of inter-omics regulatory flows	mechanism
We then perform regularization over samples to allow clustering with scattered samples and generate tight clusters	mechanism
The proposed group structured tight iCluster method will be evaluated by two real breast cancer examples and simulations to demonstrate its improved clustering accuracy	method
biological interpretation and ability to generate coherent tight clusters	method
Computational offloading services at the edge of the Internet for mobile devices are becoming a reality	background
We present experimental results that confirm substantial wins from edge computing for highly interactive mobile applications	finding
Using a wide range of mobile applications	mechanism
from WiFi and 4G LTE networks	method
Demand response is seeing increased popularity worldwide and industrial loads are actively taking part in this trend	background
As a host of energy-intensive industrial processes	background
steel plants have both the motivation and potential to provide demand response	background
and propose methods such as adding cuts and implementing an application-specific branch and bound algorithm	mechanism
Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society	background
ranging from online personalization to insurance and credit decisions to predictive policing	background
demonstrates that QII measures are a useful transparency mechanism when black box access to the learning system is available	finding
In particular they provide better explanations than standard associative measures for a host of scenarios that we consider Further	finding
we show that in the situations we consider	finding
QII is efficiently approximable and can be made differentially private while preserving accuracy	finding
We develop a formal foundation Specifically	mechanism
we introduce a family of Quantitative Input Influence ( QII ) measures that capture the degree of influence of inputs on outputs of systems	mechanism
These measures provide a foundation for the design of transparency reports that accompany system decisions ( e	mechanism
g	mechanism
explaining a specific credit decision ) and for testing tools useful for internal and external oversight ( e	mechanism
g	mechanism
to detect algorithmic discrimination ) Distinctively	mechanism
our causal QII measures carefully account for correlated inputs while measuring influence They support a general class of transparency queries and can	mechanism
in particular explain decisions about individuals ( e	mechanism
g	mechanism
a loan decision ) and groups ( e	mechanism
g	mechanism
disparate impact based on gender ) Finally	mechanism
since single inputs may not always have high influence	mechanism
the QII measures also quantify the joint influence of a set of inputs ( e	mechanism
g	mechanism
age and income ) on outcomes ( e	mechanism
g	mechanism
loan decisions ) and the marginal influence of individual inputs within such a set ( e	mechanism
g	mechanism
income )	mechanism
Since a single input may be part of multiple influential sets	mechanism
the average marginal influence of the input is computed using principled aggregation measures	mechanism
such as the Shapley value	mechanism
previously applied to measure influence in voting Further	mechanism
since transparency reports could compromise privacy	mechanism
we explore the transparency-privacy tradeoff and prove that a number of useful transparency reports can be made differentially private with very little addition of noise	mechanism
Our empirical validation with standard machine learning algorithms	method
Cities are increasingly equipped with low-resolution cameras	background
Video from some of these cameras is publicly accessible in real time	background
In particular the end goal is to build a model Models learn different appearance of vehicles as seen from different viewpoints	mechanism
A major difficulty with any type of analysis like this is the need for large amounts of training data	mechanism
In our case	mechanism
it is easy to collect unlabeled data from publicly available low-resolution low-framerate cameras in Pittsburgh or NYC	mechanism
Multimedia event detection has been receiving increasing attention in recent years	background
Besides recognizing an event	background
the discovery of evidences ( which is refered to as `` recounting '' ) is also crucial for user to better understand the searching result	background
and demonstrate the promising results obtained by our method	finding
we propose a weakly supervised evidence discovery method based on self-paced learning framework	mechanism
which follows a learning process from easy `` evidences '' to gradually more complex ones	mechanism
and simultaneously exploit more and more positive evidence samples from numerous weakly annotated video segments	mechanism
Moreover to evaluate our method quantitatively	method
we also propose two metrics	method
\textit { PctOverlap } and \textit { F1-score }	method
for measuring the performance of evidence localization specifically	method
The experiments are conducted on a subset of TRECVID MED dataset	method
Given a large-scale and high-order tensor	background
how can we find dense blocks in it ? Can we find them in near-linear time but with a quality guarantee ? Extensive previous work has shown that dense blocks in tensors as well as graphs indicate anomalous or fraudulent behavior e	background
g	background
lockstep behavior in social networks	background
upito 114 $ $ \times $ $ faster than state-of-the-art methods with similar accuracy 4 Effective : M-Zoom successfully detected edit wars and bot activities and spotted network attacks with near-perfect accuracy AUCi=i0	finding
98	finding
The data and software related to this paper are available at http : //www	finding
cs	finding
cmu	finding
edu/~kijungs/codes/mzoom/	finding
In this work	mechanism
we propose M-Zoom	mechanism
a flexible framework which works with a broad class of density measures	mechanism
M-Zoom has the following properties : 1 Scalable : M-Zoom scales linearly with all aspects of tensors and is 2 Provably accurate : M-Zoom provides a guarantee on the lowest density of the blocks it finds	mechanism
3 Flexible : M-Zoom supports multi-block detection and size bounds as well as diverse density measures	mechanism
in Wikipedia from a TCP dump	method
The LBC layer affords significant parameter savings	finding
9x to 169x in the number of learnable parameters compared to a standard convolutional layer results in up to 9x to 169x savings in model size compared to a standard convolutional layer	finding
that our local binary convolution layer is a good approximation of a standard convolutional layer	finding
CNNs with LBC layers	finding
called local binary convolutional neural networks ( LBCNN )	finding
reach state-of-the-art performance on a range of visual datasets ( MNIST	finding
SVHN CIFAR-10 and a subset of ImageNet ) while enjoying significant computational savings	finding
We propose local binary convolution ( LBC )	mechanism
The design principles of LBC are motivated by local binary patterns ( LBP )	mechanism
The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process	mechanism
a non-linear activation function and a set of learnable linear weights	mechanism
The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer	mechanism
Furthermore due to lower model complexity and sparse and binary nature of the weights also	mechanism
We demonstrate both theoretically and experimentally Empirically	method
We show that this approach leads to state of the art results on the task	finding
we propose an approach using the Abstract Meaning Representation ( AMR ) formalism	mechanism
We construct meaning representation graphs for the given text and for each question-answer pair by merging the AMRs of comprising sentences using cross-sentential phenomena such as coreference and rhetorical structures	mechanism
Then we reduce machine comprehension to a graph containment problem We posit that there is a latent mapping of the question-answer meaning representation graph onto the text meaning representation graph that explains the answer	mechanism
We present a unified max-margin framework that learns to find this mapping ( given a corpus of texts and question-answer pairs )	mechanism
and uses what it learns to answer questions on novel texts	mechanism
Rapid advances in biology demand new tools for more active research dissemination and engaged teaching	background
While existing views communicate the same information	finding
study participants found the interactive	finding
karyogram-based views much easier and likable to use We additionally discuss feedback from biology and genomics faculty	finding
This paper presents Synteny Explorer	mechanism
an interactive visualization application designed The tool visualizes synteny blocks : segments of homologous DNA shared between various extant species that can be traced back or reconstructed in extinct	mechanism
ancestral species	mechanism
We take a karyogram-based approach to create an interactive synteny visualization	mechanism
leading to a more appealing and engaging design for undergraduate-level genome evolution education	mechanism
For validation we conduct three user studies : two focused studies on color and animation design choices and a larger study that performs overall system usability testing while comparing our karyogram-based designs with two more common genome mapping representations in an educational context who judge Synteny Explorer 's fitness for use in classrooms	method
we discover that the most recent users ' actions can better reflect users ' current intentions and preferences	finding
TDAP achieves good accuracy : it improves at least 5	finding
6 % in terms of prediction accuracy	finding
and TDAP scales well : it runs 4 times faster when the number of machines increases from 2 to 10	finding
Under this observation	mechanism
we thereby propose a novel time-decaying online learning algorithm derived from the state-of-the-art FTRL-proximal algorithm	mechanism
called Time-Decaying Adaptive Prediction ( TDAP ) algorithm	mechanism
To scale Big Data	mechanism
we further parallelize our algorithm following the data parallel scheme under both BSP and SSP consistency model	mechanism
Based on the analysis of users ' behaviors in Video-On-Demand ( VoD ) recommender systems	method
We experimentally evaluate our TDAP algorithm on real IPTV VoD datasets using two state-of-the-art distributed computing platforms	method
compared to FTRL-proximal algorithm ;	method
An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer	background
For these schemes	finding
we prove that forging passwords is equivalent to recovering the secret mapping	finding
Thus our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts	finding
We propose several candidate authentication protocols -- - a computer that stores information and performs computations correctly but does not provide confidentiality Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $	mechanism
The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate	mechanism
We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $	mechanism
for a security parameter $ s ( f ) $ that depends on two key properties of $ f $	mechanism
To obtain our results	mechanism
we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $	mechanism
Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate )	mechanism
and generalize recent results of Feldman et al	mechanism
show the superiority of our approach	finding
To achieve this	mechanism
videos are represented in terms of detected visual concepts	mechanism
which are then scored as relevant or irrelevant according to their similarity with a given textual query	mechanism
In this paper	mechanism
we propose a more robust approach in order to alleviate many of the brittleness and low precision problems of previous work	mechanism
Not only do we jointly consider semantic relatedness	mechanism
visual reliability and discriminative power	mechanism
To handle noise and non-linearities in the ranking scores of the selected concepts	mechanism
we propose a novel pairwise order matrix approach for score aggregation	mechanism
Extensive experiments on the large-scale TRECVID Multimedia Event Detection data	method
Counterfactual Regret Minimization ( CFR ) is the most popular iterative algorithm for solving zero-sum imperfect-information games	background
Regret-Based Pruning ( RBP ) is an improvement that allows poorly-performing actions to be temporarily pruned	background
thus speeding up CFR	background
We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium	finding
This leads to provably faster convergence and lower space requirements	finding
show that Total RBP results in an order of magnitude reduction in space	finding
and the reduction factor increases with game size	finding
We introduce Total RBP	mechanism
a new form of RBP as actions are pruned	mechanism
Experiments	method
Many applications collect a large number of time series	background
for example the financial data of companies quoted in a stock exchange	background
the health care data of all patients that visit the emergency room of a hospital	background
or the temperature sequences continuously measured by weather stations across the US	background
These data are often referred to as un structured The first task in its analytics is to derive a low dimensional representation	background
a graph or discrete manifold	background
that describes well the inter relations among the time series and their intra relations across time	background
The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested	finding
This paper presents a computationally tractable algorithm The resulting graph is directed and weighted	mechanism
possibly capturing causal relations	mechanism
not just reciprocal correlations as in many existing approaches in the literature	mechanism
A convergence analysis is carried out	method
The algorithm is demonstrated on random graph datasets and real network time series datasets	method
and its performance is compared to that of related methods	method
A method is presented The invention provides correctness guarantees for CPS executions at runtime Offline verification of CPS models are combined with runtime validation of system executions for compliance with the model	mechanism
The invention ensures that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model	mechanism
assuming the system dynamics deviation is bounded If	mechanism
at some point	mechanism
the observed behavior no longer complies with the model	mechanism
such that offline verification results no longer apply	mechanism
provably safe fallback actions are initiated	mechanism
The invention includes a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic	mechanism
Pooling plays an important role in generating a discriminative video representation	background
and we prove new and closed-form proximal steps	finding
and achieve promising improvements	finding
In this paper	mechanism
we propose a new semantic pooling approach especially when only a few shots/segments are relevant to the event of interest while many other shots are irrelevant or even misleading	mechanism
The commonly adopted pooling strategies aggregate the shots indifferently in one way or another	mechanism
resulting in a great loss of information	mechanism
Instead in this work we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event analysis	mechanism
Next we propose a new isotonic regularizer that is able to exploit the constructed semantic ordering information The resulting nearly-isotonic support vector machine classifier exhibits higher discriminative power in event analysis tasks	mechanism
Computationally we develop an efficient implementation using the proximal gradient algorithm	method
We conduct extensive experiments on three real-world video datasets	method
Recently there has been a surge of interest in using spectral methods for estimating latent variable models	background
Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient	finding
By leveraging some recent advances in continuous linear algebra and numerical analysis	mechanism
we develop a computationally efficient spectral algorithm for learning nonparametric HMMs Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as \emph { continuous matrices }	mechanism
We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices	mechanism
In this paper	method
we study the estimation of an $ m $ -state hidden Markov model ( HMM ) with only smoothness assumptions	method
such as H\ '' olderian conditions	method
on the emission densities	method
We implement our method using Chebyshev polynomial approximations	method
Meeting tail latency Service Level Objectives ( SLOs ) in shared cloud networks is both important and challenging	background
One primary challenge is determining limits on the multi-tenancy such that SLOs are met	background
Doing so involves estimating latency	background
which is difficult	background
especially when tenants exhibit bursty behavior as is common in production environments	background
Nevertheless recent papers in the past two years ( Silo	background
QJump and PriorityMeister ) show techniques for calculating latency based on a branch of mathematical modeling called Deterministic Network Calculus ( DNC )	background
The DNC theory is designed for adversarial worst-case conditions	background
which is sometimes necessary	background
but is often overly conservative	background
SNC-Meister supports 75 % more tenants than the state-of-the-art	finding
This paper describes SNC-Meister	mechanism
a new admission control system SNC-Meister improves upon the state-of-the-art DNC-based systems by using a new theory	mechanism
Stochastic Network Calculus ( SNC )	mechanism
which is designed for tail latency percentiles Focusing on tail latency percentiles	mechanism
rather than the adversarial worst-case DNC latency	mechanism
allows SNC-Meister to pack together many more tenants	mechanism
: in experiments with production traces	method
the robustness and effectiveness of the proposed system	finding
This paper presents a robust	mechanism
fully automatic and semi self-training system Based on the observation that some certain facial areas	mechanism
e	mechanism
g	mechanism
cheeks do not typically contain any facial hair whereas the others	mechanism
e	mechanism
g	mechanism
brows often contain facial hair	mechanism
a self-trained model is first built using a testing image itself	mechanism
To overcome the limitation of that facial hairs in brows regions and beard/moustache regions are different in length	mechanism
density color etc	mechanism
a pre-trained model is also constructed using training data The pre-trained model is only pursued when the self-trained model produces low confident classification results	mechanism
In the proposed system	mechanism
we employ the superpixel together a combination of two classifiers	mechanism
i	mechanism
e	mechanism
Random Ferns ( rFerns ) and Support Vector Machines ( SVM ) to obtain good classification performance as well as improve time efficiency	mechanism
A feature vector	mechanism
consisting of Histogram of Gabor ( HoG ) and Histogram of Oriented Gradient of Gabor ( HOGG ) at different directions and frequencies	mechanism
is generated from both the bounding box of the superpixel and the super pixel foreground	mechanism
The segmentation result is then refined by our proposed aggregately searching strategy in order to deal with inaccurate landmarking points	mechanism
Detect and segment beard/moustache simultaneouslyUse advantages of both pre-trained model and self-trained modelWork on superpixelPropose an aggregate searching strategy to overcome the limits of landmarkerPropose a new feature that is able to emphasize high frequency information of facial hair	mechanism
Experimental results have demonstrated It is evaluated in images drawn from three entire databases i	method
e	method
the Multiple Biometric Grand Challenge ( MBGC ) still face database	method
the NIST color Facial Recognition Technology FERET database and a large subset from Pinellas County database	method
The best prior complete algorithm has significantly worse complexity and has	background
to our knowledge	background
never been implemented	background
We present a complete algorithm in games with more than two players	mechanism
The main components of our tree-search-based method are a node-selection strategy	mechanism
an exclusion oracle	mechanism
and a subdivision scheme	mechanism
The node-selection strategy determines the next region to be explored -- -based on the region 's size and an estimate of whether the region contains an equilibrium The exclusion oracle provides a provably correct sufficient condition for there not to exist an equilibrium in the region	mechanism
The subdivision scheme determines how the region is split if it can not be excluded	mechanism
Unlike well-known incomplete methods	mechanism
our method does not need to proceed locally	mechanism
which avoids it getting stuck in a local minimum that may be far from any actual equilibrium	mechanism
The run time grows rapidly with the game size	mechanism
and this suggests a hybrid scheme where one of the relatively fast prior incomplete algorithms is run	mechanism
and if it fails to find an equilibrium	mechanism
then our method is used	mechanism
Multimodal sentiment analysis is drawing an increasing amount of attention these days	background
It enables mining of opinions in video reviews and surveys which are now available aplenty on online platforms like YouTube	background
we show how SAL improves the generalizability of state-of-the-art models	finding
We increase prediction accuracy significantly in all three modalities ( text	finding
audio video )	finding
as well as in their fusion	finding
We show how SAL	finding
achieves good accuracy across test datasets	finding
Then we propose a Select-Additive Learning ( SAL ) procedure SAL is a two-phase learning method	mechanism
In Selection phase	mechanism
it selects the confounding learned representation	mechanism
In Addition phase	mechanism
it forces the classifier to discard confounded representations by adding Gaussian noise	mechanism
In this paper	method
we first examine the data and verify the existence of this dependence problem	method
In our experiments	method
even when trained on one dataset	method
We discuss the implications of our results for market design in general	background
and kidney exchange in particular	background
Our main result asserts that	finding
any fixed optimal matching is likely to be individually rational up to lower-order terms We also show that a simple and practical mechanism is ( fully ) individually rational	finding
and likely to be optimal up to lower-order terms	finding
by considering an arbitrary graph	mechanism
but assuming that vertices are associated with players at random	mechanism
under certain conditions	method
A descending clock auction ( DCA ) is for buying items from multiple sellers	background
The literature has focused on the case where each bidder has two options : to accept or reject the offered price	background
show that the optimization-based approach dramatically outperforms the percentile-based approach -- because it takes feasibility into account in pricing	finding
Both pricing techniques scale to the large	finding
We present a multi-option DCA ( MDCA ) framework where at each round	mechanism
the auctioneer offers each bidder different prices for different options	mechanism
and a bidder may find multiple options still acceptable Setting prices during a MDCA is trickier than in a DCA	mechanism
We develop a Markov chain model ( which options are still acceptable )	mechanism
We leverage it This is unlike most auctions which only compute the next price vector	mechanism
Computing the trajectory enables better planning	mechanism
We reoptimize the trajectory after each round	mechanism
Each optimization minimizes total payment while ensuring feasibility in a stochastic sense We also introduce percentile-based approaches to decrementing prices	mechanism
Experiments with real FCC incentive auction interference constraint data	method
confirm the significance of the problem and the effectiveness of FlexRR 's solution	finding
Using FlexRR we consistently observe near-ideal run-times ( relative to no performance jitter ) across all real and injected straggler behaviors tested	finding
FlexRR FlexRR combines a more flexible synchronization model with dynamic peer-to-peer re-assignment of work among workers	mechanism
Experiments with real straggler behavior observed on Amazon EC2 and Microsoft Azure	method
as well as injected straggler behavior stress tests	method
A method and apparatus A touch image is received	mechanism
and this touch image has at least a first area that corresponds to an area of the touchscreen that has an elongated interface object positioned at least proximate to it	mechanism
The elongated interface object has a pitch and a yaw with respect to the touchscreen surface	mechanism
A first transformation is performed to obtain a first transformation image of the touch image	mechanism
and a second transformation is performed to obtain a second transformation image of the touch image	mechanism
The first transformation differs from the second transformation The yaw is determined for the elongated interface object based on both the first and second transformation images	mechanism
The pitch is determined based on at least one of the first and second transformation images	mechanism
Which team is the best in the league ? How does my team fare with respect to the rest of the league ? These are questions that every sports fan is interested in knowing the answers to	background
In other cases	background
such as in college sports	background
knowing the answer to these questions is crucial for shaping the picture of spe- cific contests	background
In professional sports	background
sports networks provide power rankings regularly - typically every week or month de- pending on the season length of the league - based on their experts opinion	background
We finally propose an ad- vanced ranking technique based on tensor decomposition	background
we show that the cycles in the network are significantly correlated with the performance	finding
In this work we propose an alternative	mechanism
ob- jective and network-based In brief	mechanism
our method is based on analyzing a directed network formed between the teams of the corresponding leagues that captures their win-lose relationships Using data from the National Football League and the National Basketball As- sociation	mechanism
we show that even simple network theory metrics ( e	mechanism
g	mechanism
Page Rank ) can provide a ranking that has the same ac- curacy in predicting winners of upcoming match-ups as more complicated systems ( e	mechanism
g	mechanism
Cortana	mechanism
We further explore the impact of the network structure on the prediction accuracy and	method
Describing videos with natural language is one of the ultimate goals of video understanding	background
Video records multi-modal information including image	background
motion aural speech and so on	background
results show the effectiveness of multi-modal fusion encoder trained in the end-to-end framework	finding
which achieved top performance in both common metrics evaluation and human evaluation	finding
In this paper	mechanism
we propose the multi-modal fusion encoder and integrate it with text sequence decoder into an end-to-end video caption framework	mechanism
Features from visual	mechanism
aural speech and meta modalities are fused together to represent the video contents	mechanism
Long Short-Term Memory Recurrent Neural Networks ( LSTM-RNNs ) are then used as the decoder to generate natural language sentences	mechanism
Experimental	method
We envision that by casting accountability theories in computing and control systems in terms of causal information flow	background
we can provide a common foundation to develop a theory for CPS that compose elements from both domains	background
we summarize our results	finding
We envision that a unified theory of accountability in CPS can be built on a foundation of causal information flow analysis	mechanism
This theory will support design and analysis of mechanisms at various stages of the accountability regime : attack detection	mechanism
responsibility-assignment ( e	mechanism
g	mechanism
attack identification or localization )	mechanism
and corrective measures ( e	mechanism
g	mechanism
via resilient control ) As an initial step in this direction	mechanism
We use the Kullback-Liebler ( KL ) divergence as a causal information flow measure	mechanism
We then recover	mechanism
using information flow analyses	mechanism
a set of existing results in the literature that were previously proved using different techniques	mechanism
These results cover passive detection	mechanism
stealthy attack characterization	mechanism
and active detection	mechanism
This research direction is related to recent work on accountability in computational systems [ 1 ]	mechanism
[ 2 ]	mechanism
[ 3 ]	mechanism
[ 4 ]	mechanism
on attack detection in control systems	method
Electric vehicles ( EVs )	background
specifically Battery EVs ( BEVs )	background
can offer significant energy and emission savings over internal combustion engine based vehicles	background
Norway has a long history of research and government incentives for BEVs	background
The results suggest significant positive effects of BEV technology improvement	finding
toll waivers and charging station density on BEV sales for both personal consumers and business buyers	finding
except that bus lanes access may have a negative impact for personal consumers	finding
possibly due to consumers ' concern regarding bus lane congestion The effects on business buyers are generally less pronounced than on personal consumers	finding
In addition we find significant heterogeneity in consumer preferences over BEV price and car specifications In particular	finding
a 9 500 NOK increases in consumer income can lead to approximately 10 % decrease in price sensitivity on average	finding
In other words	finding
individual consumers with higher income would be less price-sensitive than those with lower income	finding
Significant heterogeneity in incentive policy impacts on different brands are also found	finding
especially for Renault	finding
Ford Nissan ( all three being a good compromise of prices and ranges ) and Tesla ( with an exceptionally long range )	finding
we use Random-Coefficient Discrete Choice Model ( referred to BLP model )	mechanism
The BEV market and ample data sets in Norway Our study is instantiated on the entire BEV sales data in Norway from 2011 to 2013	method
as well as demographics information at municipality level	method
Green and Laffonti ? [ 13 ] proved that one can not generically achieve both	background
show that the inefficiency for a simple randomized mechanism is 5 -- -100 times smaller than the worst case	finding
This relative difference increases with the number ofi ? agents	finding
We consider strategyproof budget-balanced mechanisms that are approximately efficient For deterministic mechanisms	mechanism
we show that a strategyproof and budget-balanced mechanism must have a sink agent whose valuation function is ignored in selecting an alternative	mechanism
and she is compensated with the payments made by the other agents	mechanism
We assume the valuations of the agents come from a bounded open interval	mechanism
This result strengthens Green and Laffont 's impossibility result by showing that even in a restricted domain of valuations	mechanism
there does not exist a mechanism that is strategyproof	mechanism
budget balanced and takes every agent 's valuation into consideration -- a corollary of which is that it can not be efficient	mechanism
Using this result	mechanism
we find a tight lower bound on the inefficiencies of strategyproof	mechanism
budget-balanced mechanisms in this domain	mechanism
The bound shows that the inefficiency asymptotically disappears when the number of agents is large -- a result close in spirit to Green and Laffonti ? [ 13	mechanism
Theorem 9	mechanism
4 ]	mechanism
However our results provide worst-case bounds and the best possible rate of convergence Next	mechanism
we consider minimizing any convex combination of inefficiency and budget imbalance	mechanism
We show that if the valuations are unrestricted	mechanism
no deterministic mechanism can do asymptotically better than minimizing inefficiency alone	mechanism
Finally we investigate randomized mechanisms and provide improved lower bounds on expected inefficiency We give a tight lower bound for an interesting class of strategyproof	mechanism
budget-balanced randomized mechanisms	mechanism
We also use an optimization-based approach -- in the spirit of automated mechanism design -- to provide a lower bound on the minimum achievable inefficiency of any randomized mechanism	mechanism
Experiments with real data from two applications	method
The Next-Generation Airborne Collision Avoidance System ( ACAS X ) is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft	background
It is currently being developed by the Federal Aviation Administration ( FAA ) Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety	background
In this paper	mechanism
we determine the geometric configurations under a precise set of assumptions and using hybrid systems theorem proving techniques We consider subsequent advisories and show how to adapt our formal verification to take them into account	mechanism
We examine the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version	mechanism
demonstrating how formal hybrid systems proving approaches are helping to ensure the safety of ACAS X	mechanism
Low engagement rates and high attrition rates have been formidable challenges for mobile apps and their long-term success	background
especially for those whose revenues come mainly from in-app purchases	background
To date still little is known about how companies can comprehensively identify user engagement stages so as to improve business revenues	background
Our structural-model- and field-experimentation-based findings are nontrivial and suggest	background
with respect to the crucial role of modeling user engagement	background
potential overall welfare improvements in the mobile app market	background
Interestingly we found that such an engagement-specific pricing strategy leads	finding
simultaneously to lower average prices for consumers and higher overall business revenues for the app	finding
Our experimental results provide more causal evidence that a personalized promotion strategy targeting user engagement stages can both decrease costs to app users and enhance overall business performance	finding
This paper proposes a structural econometric framework that accounts for both the time-varying nature of engagement and consumer forward-looking consumption behavior Our policy simulation enabled us to tailor	mechanism
based on the model-detected engagement stages	mechanism
an optimal pricing strategy to each consumer	mechanism
The present study analyzed a fine-grained mobile tapstream dataset on mobile users ' continuous content consumption behavior in a popular mobile reading app	method
To further evaluate the effectiveness of our method	method
we conducted a randomized field experiment on a mobile app platform	method
An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy	background
In a system with redundant requests	background
each job that arrives to the system is copied and dispatched to multiple servers	background
As soon as the first copy completes service	background
the job is considered complete	background
and all remaining copies are deleted	background
We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity	finding
We propose a theoretical model of redundancy	mechanism
the Redundancy- d system	mechanism
in which each job sends redundant copies to d servers chosen uniformly at random	mechanism
We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers	mechanism
Practical implications As many marketers are interested in hoarding consumers personal information	background
privacy advocates call for methods that would ensure careful and well-informed disclosure Offering reversibility to a decision to disclose personal information	background
or merely pointing out the irreversibility of that decision	background
can make consumers reevaluate the sensitivity of the situation	background
leading to more careful disclosures Originality/value Although previous research on reversibility in consumer behavior focused on product return policies and showed that reversibility increases purchases	background
none have studied how reversibility affects self-disclosure and how it can decrease it	background
Findings showed that consumers disclose less in both the reversible and irreversible conditions	finding
compared to the control condition showed that this is because consumers treat reversibility as a cue to the sensitivity of the information they are asked to divulge	finding
and that leads them to disclose less when reversibility or irreversibility is made explicitly salient beforehand	finding
Design/methodology/approach Three studies examined how informing consumers they may ( reversible condition ) or may not ( irreversible condition ) revise their personal information in the future affected their propensity to disclose personal information	method
compared to a control condition Study 1 ( which included three experiments with different time intervals between initial and revised disclosure Studies 2 and 3	method
The approach finds a dual flow solution to this linear system through a sequence of flow adjustments along cycles	background
Our methods demonstrate significant speedups over previous implementations	finding
and are competitive with standard numerical routines	finding
We study both data structure oriented and recursive methods The primary difficulty faced by this approach	mechanism
updating and querying long cycles	mechanism
motivated us to study an important special case : instances where all cycles are formed by fundamental cycles on a length $ n $ path	mechanism
This paper studies an attacker against a cyber-physical system ( CPS ) The attacker 's probability of being detected is related to the nonnegative bias induced by his or her attack on the CPS ' detection statistic We formulate a linear quadratic cost function that captures the attacker 's control goal and establish constraints on the induced bias that reflect the attacker 's detection-avoidance objectives When the attacker is constrained to be detected at the false-alarm rate of the detector	mechanism
we show that the optimal attack strategy reduces to a linear feedback of the attacker 's state estimate In the case that the attacker 's bias is upper bounded by a positive constant	mechanism
we provide two algorithms -- an optimal algorithm and a sub-optimal	mechanism
less computationally intensive algorithm --	mechanism
Finally we illustrate our attack strategies in numerical examples based on a remotely-controlled helicopter under attack	method
Real-time virtualization techniques have been investigated with the primary goal of consolidating multiple real-time systems onto a single hardware platform while ensuring timing predictability However	background
a shared last-level cache ( LLC ) on recent multi-core platforms can easily hamper timing predictability due to the resulting temporal interference among consolidated workloads	background
results show that our techniques can effectively control the cache allocation of tasks in VMs	finding
Our cache management scheme yields a significant utilization benefit compared to other approaches	finding
In this paper	mechanism
we propose a real-time cache management framework Our framework introduces two hypervisor-level techniques	mechanism
vLLC and vColoring	mechanism
that enable the cache allocation of individual tasks running in a virtual machine ( VM )	mechanism
which is not achievable by the current state of the art Our framework also provides a cache management scheme that determines cache allocation to tasks	mechanism
designs VMs in a cache-aware manner	mechanism
and minimizes the aggregated utilization of VMs to be consolidated	mechanism
As a proof of concept	method
we implemented vLLC and vColoring in the KVM hypervisor running on x86 and ARM multi-core platforms	method
Experimental with three different guest OSs	method
namely Linux/RK vanilla Linux and MS Windows Embedded	method
When navigating indoors	background
blind people are often unaware of key visual information	background
such as posters	background
signs and exit doors	background
With VizMap we move towards integrating the strengths of the end user	background
on-site crowd online crowd	background
and computer vision to solve a long-standing challenge in indoor blind exploration	background
Our VizMap system uses computer vision and crowdsourcing VizMap starts with videos taken by on-site sighted volunteers and uses these to create a 3D spatial model	mechanism
These video frames are semantically labeled by remote crowd workers with key visual information	mechanism
These semantic labels are located within and embedded into the reconstructed 3D model	mechanism
forming a query-able spatial representation of the environment	mechanism
VizMap can then localize the user with a photo from their smartphone	mechanism
and enable them to explore the visual elements that are nearby	mechanism
We explore a range of example applications enabled by our reconstructed spatial representation	method
Electrical Impedance Tomography ( EIT ) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch	background
This prior work demonstrated great promise for non-invasive	background
high accuracy recognition of gestures for interactive control shed light on the future feasibility of EIT for sensing human input	background
We introduce a new system In turn	mechanism
this enables superior interior reconstruction and gesture recognition	mechanism
More importantly we use our new system as a vehicle for experimentation ' we compare two EIT sensing methods and three different electrode resolutions	method
Results from in-depth empirical evaluations and a user study	method
demonstrate that our approach achieves high accuracy in multiclass classification and outperforms other classification approaches	finding
We present that is based on the regularization of graph signals	mechanism
Our approach is based on the theory of discrete signal processing on graphs where the graph represents similarities between data and we interpret labels for the dataset elements as a signal indexed by the nodes of the graph	mechanism
We postulate that true labels form a low-frequency graph signal and the classifier finds the smoothest graph signal that satisfies constraints given by known data labels	mechanism
Our experiments	method
suggesting that AuraSense can be low latency and robust across users and environments	finding
In this work	mechanism
we introduce AuraSense	mechanism
using electric field sensing as an adapted device	mechanism
We identified four configurations that can support six well-known modalities of particular interest and utility	mechanism
including gestures above or in close proximity to watches	mechanism
and touchscreen-like finger tracking on the skin	mechanism
To explore how this sensing approach could enhance smartwatch interactions	method
we considered different antenna configurations and how they could enable useful interaction modalities We quantify the feasibility of these input modalities	method
Longitudinally the geodesic distance was found to be proportional to the elapsed time separating the two scans in question we found that each structures annualized rate of change in the geodesic distance followed the order of AD > MCI > HC	finding
with statistical significance being reached in every case In addition	finding
for each of the six structures of interest	finding
within the same time interval ( e	finding
g	finding
from baseline to the 6th month )	finding
we observed significant correlations between the geodesic distance and the cognitive deterioration as quantified by the ADAS-cog increase and the MMSE decrease Furthermore	finding
as the disease progresses over time	finding
this linkage between the inter-shape geodesic distance and the cognitive decline becomes considerably stronger and more significant	finding
We propose a geodesic distance on a Grassmannian manifold Cross-sectionally	mechanism
utilizing a linear mixed-effects statistical model	mechanism
Longitudinal magnetic resonance imaging ( MRI ) scans of 754 subjects ( 3092 scans in total ) were used in this study	method
The world is full of physical interfaces that are inaccessible to blind people	background
from microwaves and information kiosks to thermostats and checkout terminals	background
Blind people can not independently use such devices without at least first learning their layout	background
and usually only after labeling them with sighted assistance and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone	background
We show that VizLens provides accurate and usable real-time feedback and our crowdsourcing labeling workflow was fast ( 8 minutes )	finding
accurate ( 99	finding
7 % )	finding
and cheap ( $ 1	finding
15 )	finding
We introduce VizLens - an accessible mobile application and supporting backend that can robustly and interactively VizLens users capture a photo of an inaccessible interface and send it to multiple crowd workers	mechanism
who work in parallel to quickly label and describe elements of the interface to make subsequent computer vision easier	mechanism
The VizLens application helps users recapture the interface in the field of the camera	mechanism
and uses computer vision to interactively describe the part of the interface beneath their finger ( updating 8 times per second )	mechanism
We then explore extensions of VizLens that allow it to ( i ) adapt to state changes in dynamic interfaces	mechanism
( ii ) combine crowd labeling with OCR technology to handle dynamic displays	mechanism
and ( iii ) benefit from head-mounted cameras VizLens robustly solves a long-standing challenge in accessibility by deeply integrating crowdsourcing and computer vision	mechanism
in a study with 10 blind participants	method
The empirical results show that the patterns of parameters as a seizure approach and the method is efficient in analyzing nonlinear epilepsy electroencephalogram data	finding
The accuracy of estimating the optimal parameters is improved by using the nonlinear dynamic model	finding
We propose a nonlinear dynamic model for an invasive electroencephalogram analysis via the LevenbergMarquardt algorithm	mechanism
We introduce the crucial windows where the estimated parameters present patterns before seizure onset The optimal parameters minimizes the error between the observed signal and the generated signal by the model	mechanism
The proposed approach effectively discriminates between healthy signals and epileptic seizure signals	mechanism
We evaluate the proposed method using an electroencephalogram dataset with normal and epileptic seizure sequences	method
Smartwatches and wearables are unique in that they reside on the body	background
presenting great potential for always-available input and interaction	background
Their position on the wrist makes them ideal for capturing bio-acoustic signals	background
Overall our contributions unlock user interface techniques that previously relied on special-purpose and/or cumbersome instrumentation	background
making such interactions considerably more feasible for inclusion in future consumer devices	background
We developed a custom smartwatch kernel For example	mechanism
we can use bio-acoustic data to classify hand gestures such as flicks	mechanism
claps scratches and taps	mechanism
which combine with on-device motion tracking to create a wide range of expressive input modalities	mechanism
Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects	mechanism
enabling passive object recognition that can augment everyday experiences with context-aware functionality Finally	mechanism
we can generate structured vibrations using a transducer	mechanism
and show that data can be transmitted through the human body	mechanism
Using this new source of high-fidelity data	method
we uncovered a wide range of applications	method
Given `` who-trusts/distrusts-whom '' information	background
how can we propagate the trust and distrust ? With the appearance of fraudsters in social network sites	background
the importance of trust prediction has increased	background
confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy	finding
achieving up to 50	finding
4 percentage relative improvement	finding
In this paper	mechanism
we propose PIN -TRUST	mechanism
a novel method The novelties of our method are the following : ( a ) it is carefully designed	mechanism
to take into account positive	mechanism
implicit and negative information	mechanism
( b ) it is scalable ( i	mechanism
e	mechanism
linear on the input size )	mechanism
( c ) most importantly	mechanism
it is effective and accurate	mechanism
Our extensive experiments with a real dataset	method
Epinions	method
com data of 100K nodes and 1M edges	method
Many applications in speech	background
robotics finance and biology deal with sequential data	background
where ordering matters and recurrent structures are common	background
where the predictive uncertainties provided by GP-LSTM are uniquely valuable	finding
we propose expressive closed-form kernel functions for Gaussian processes	mechanism
The resulting model	mechanism
GP-LSTM fully encapsulates the inductive biases of long short-term memory ( LSTM ) recurrent networks	mechanism
while retaining the non-parametric probabilistic advantages of Gaussian processes We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic procedure and exploit the structure of these kernels for fast and scalable training and prediction	mechanism
We demonstrate state-of-the-art performance on several benchmarks	method
and thoroughly investigate a consequential autonomous driving application	method
As mobile computing and cloud computing converge	background
the sensing and interaction capabilities of mobile devices can be seamlessly fused with compute-intensive and data-intensive processing in the cloud	background
Cloudlets are important architectural components in this convergence	background
representing the middle tier of a mobile device cloudlet cloud hierarchy	background
We describe a plug-and-play architecture	mechanism
and a proof of concept using Google Glass	method
In the human genome	background
distal enhancers are involved in regulating target genes through proximal promoters by forming enhancer-promoter interactions This work shows for the first time that sequence-based features alone can reliably predict enhancer-promoter interactions genome-wide	background
which provides important insights into the sequence determinants for long-range gene regulation	background
demonstrate that SPEID is effective in predicting enhancer-promoter interactions as compared to state-of-the-art methods that use non-sequence features from functional genomic signals	finding
Here we report a new computational method ( named `` SPEID '' ) using deep learning models based on sequence-based features only	mechanism
when the locations of putative enhancers and promoters in a particular cell type are given	mechanism
Our results across six different cell types	method
Sensorized commercial buildings are a rich target for building a new class of applications that improve operational and energy efficiency of building operations that take into account human activities The attendees would be able to create example buildings and write their own queries	background
show application queries that extracts relevant metadata from these buildings	finding
Our demo presents Brick [ 4 ]	mechanism
a uniform schema Our schema defines a concrete ontology for sensors	mechanism
subsystems and relationships among them	mechanism
which enables portable applications	mechanism
Using a web application	method
we will demonstrate real buildings that have been mapped to the Brick schema	method
and	method
demonstrate that our algorithm can continuously execute using energy harvested from indoor solar panels we see that we can detect motions with an average of 85 % recall rate and perform occupancy counting with an average error of 10 % in terms of maximum occupancy	finding
In this paper	mechanism
we present a platform The system transmits a wide-band ultrasonic signal into a room and then processes the superposition of the reflections recorded by a microphone The system has two modes of operation	mechanism
one for presence detection and one for estimating the number of occupants in a region	mechanism
The presence detection uses the difference between multiple transmissions in succession with a set of general classifiers that make a binary decision about if the room contains occupants	mechanism
We then use a semi-supervised learning approach based on Weighted Principal Component Analysis ( WPCA ) that requires minimal training data to estimate the number of occupants We also present the design of an energy harvesting embedded platform and The platform has a dual Bluetooth Low-Energy and 802	mechanism
15	mechanism
4 interface to communicate with a gateway or nearby mobile phone that runs an interface that aids in collecting training data	mechanism
We evaluate our algorithm on a wide-variety of indoor spaces as well as benchmark the hardware in terms of sampling rate given an energy budget On more than three weeks of data	method
Commercial buildings have long since been a primary target for applications from a number of areas : from cyber-physical systems to building energy use to improved human interactions in built environments	background
While technological advances have been made in these areas	background
such solutions rarely experience widespread adoption due to the lack of a common descriptive schema which would reduce the now-prohibitive cost of porting these applications and systems to different buildings	background
We demonstrate the completeness and effectiveness of Brick	finding
this paper describes Brick	mechanism
a uniform schema Our schema defines a concrete ontology for sensors	mechanism
subsystems and relationships among them	mechanism
which enables portable applications	mechanism
by using it to represent the entire vendor-specific sensor metadata of six diverse buildings across different campuses	method
comprising 17 700 data points	method
and running eight complex unmodified applications on these buildings	method
Several generations of inexpensive depth cameras have opened the possibility for new kinds of interaction on everyday surfaces	background
A number of research systems have demonstrated that depth cameras	background
combined with projectors for output	background
can turn nearly any reasonably flat surface into a touch-sensitive display	background
Results show that our technique boosts touch detection accuracy by 15 % and reduces positional error by 55 % compared to the next best-performing technique	finding
In this paper we present DIRECT	mechanism
that merges depth and infrared imagery captured by a commodity sensor This yields significantly better touch tracking than from depth data alone	mechanism
as well as any prior system	mechanism
Further extending prior work	mechanism
DIRECT supports arbitrary user orientation and requires no prior calibration or background capture	mechanism
We describe the implementation of our system and	mechanism
quantify its accuracy through a comparison study of previously published	method
depth-based touch-tracking algorithms	method
generating a tightly synchronized PPS output that is able to adjust for the distance between the nodes	finding
Even without communication	finding
the devices maintain synchronization over multiple seconds	finding
In this demonstration we present Pulsar	mechanism
a speed-of-light propagation-aware time synchronization platform	mechanism
Pulsar uses ultra-wideband ( UWB ) radios for time transfer with each node backed by a chip scale atomic clock ( CSAC ) that in combination are able due to a stable CSAC clocking the system	mechanism
The demonstration will show two Pulsar boards	method
Cohesion and structural equivalence are two competing network models to explain diffusion of innovation	background
We found subpopulation size in such million-node network only falls in two levels	finding
200 and 500	finding
in the extraction step The results show CRBT adoption is affected by both cohesion and structural equivalence The size and direction of network influence both change with the size of group	finding
Structural equivalence has a negative effect on adoption when group size is at about 200	finding
and has a positive effect when group size is at about 500	finding
The effect of cohesion	finding
on the other hand	finding
is consistent	finding
Since this societal scale network is very large	mechanism
we use a novel technique Using a new auto-probit model with network terms	mechanism
we then compare the competing influences of cohesion and structural equivalence on each of the subpopulation extracted	method
Finally we use meta-analysis to summarize the estimated parameters from all subpopulations	method
To respond to environmental changes	background
such as drought	background
plants must regulate numerous cellular processes	background
The work provides a framework for understanding and modulating plant responses to stress	background
Working in the model plant Arabidopsis	mechanism
Song et al	mechanism
profiled the binding of 21 transcription factors to chromatin and	mechanism
Furthermore we show that	finding
once the model is trained	finding
the algorithm can perform inference	finding
In this paper we introduce BOLT	mechanism
a novel approach that performs online binary matrix factorization on a sequence of high frequency current cycles collected in a building to infer additive subcomponents of the current signal	mechanism
The system learns these constituent current waveforms in an unsupervised fashion and	mechanism
in a subsequent step	mechanism
seeks to find combinations of these subcomponents that constitute appliances By doing so	mechanism
points in time when appliances are active and	mechanism
to some degree	mechanism
their power consumption can be estimated by BOLT	mechanism
Our system treats energy disaggregation as a binary matrix factorization problem and uses a neural network	mechanism
with binary activations in the one but last layer and a linear output layer	mechanism
to solve it	mechanism
which allows leveraging high-frequency information without having to explicitly transmit and store large amounts of data to a centralized repository	mechanism
The algorithmic performance of the proposed method is evaluated on a publicly available dataset in real-time on inexpensive off-the-shelf and general purpose hardware	method
Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures	background
We show improved performance	finding
We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches Specifically	mechanism
we apply additive base kernels to subsets of output features from deep neural architectures	mechanism
and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective	mechanism
Within this framework	mechanism
we derive an efficient form of stochastic variational inference which leverages local kernel interpolation	mechanism
inducing points and structure exploiting algebra	mechanism
over stand alone deep networks	method
SVMs and state of the art scalable Gaussian processes on several classification benchmarks	method
including an airline delay dataset containing 6 million training points	method
CIFAR and ImageNet	method
In graph signal processing	background
the graph adjacency matrix or the graph Laplacian commonly define the shift operator	background
The spectral decomposition of the shift operator plays an important role in that the eigenvalues represent frequencies and the eigenvectors provide a spectral basis	background
This is useful	background
for example in the design of filters	background
The main results characterize the form of the solution to an important system of equations that leads to this deterministic distribution function and significantly reduce the number of equations that must be solved to find the solution for a given set of model parameters are provided for sample parameters	finding
formed by including each link of a D-dimensional lattice supergraph independently with identical probability	mechanism
a percolation model	mechanism
Using the stochastic canonical equation methods developed by Girko for symmetric matrices with independent upper triangular entries	mechanism
a deterministic distribution is found that asymptotically approximates the empirical spectral distribution of the scaled adjacency matrix for a model with arbitrary parameters	mechanism
Simulations comparing the expected empirical spectral distributions and the computed deterministic distributions	method
Existing approaches for trilateration require three or more beacons to determine a unique position solution	background
our approach is able to reduce the number of beacons between 22 % and 60 % ( 33 % on an average ) as compared to standard trilateration	finding
We show that with prior knowledge of the map and a model of beacon coverage	mechanism
it is possible This not only reduces installation cost by requiring fewer nodes	mechanism
but can also improve robustness One of the main challenges with respect to beacon placement algorithms is defining a metric for estimating performance We propose augmenting the commonly used Geometric Dilution of Precision ( GDOP ) metric	mechanism
We then use this enhanced GDOP metric as part of a toolchain to compare various beacon placement algorithms in terms of coverage and expected accuracy	method
When applied to a set of real floor plans	method
With the potential to enhance the power system 's operational flexibility in a cost-effective way	background
demand response is gaining increased attention worldwide	background
Industrial loads such as cement crushing plants consume large amounts of electric energy and therefore are prime candidates for the provision of significant amounts of demand response	background
They have the capability to turn on/off an arbitrary number of their crushers thereby adjusting their electric power consumption	background
In this paper	mechanism
we propose a coordination method based on model predictive control	mechanism
and demonstrate data transmission rates up to four times faster than prior camera-based techniques	finding
highlighting different interaction techniques CapCam enables	finding
We present CapCam	mechanism
a novel technique	mechanism
simply by pressing a device to the screen 's surface Pairing data	mechanism
used to bootstrap a conventional wireless connection	mechanism
is transmitted optically to the phone 's rear camera	mechanism
This approach utilizes the near-ubiquitous rear camera on smart devices	mechanism
making it applicable to a wide range of devices	mechanism
both new and old	mechanism
CapCam also tracks phones ' physical positions on the host capacitive touchscreen without any instrumentation	mechanism
enabling a wide range of targeted interactions	mechanism
We quantify the communication performance of our pairing approach To demonstrate the unique capability and utility of our system	method
we built a series of example applications	method
Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications	background
These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods	background
We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics This is complemented by impressive empirical results obtained	finding
In this paper	mechanism
we present techniques by reducing the variance in the stochastic gradient	mechanism
on a variety of real world datasets	method
and on four different machine learning tasks ( regression	method
classification independent component analysis and mixture modeling )	method
The core number of a node is the highest k-core in which the node participates	background
Core numbers are useful in many graph mining tasks	background
especially ones that involve finding communities of nodes	background
influential spreaders and dense subgraphs	background
Large graphs often do not fit on the memory of a single machine	background
demonstrate that NimbleCore gives space savings up to 60X	finding
while accurately estimating core numbers with average relative error less than 2	finding
3 %	finding
We propose NimbleCore	mechanism
an iterative external-memory algorithm	mechanism
using O ( n log d max ) space	mechanism
where n is the number of nodes and d max is the maximum node-degree in the graph	mechanism
We also show that NimbleCore requires O ( n ) space for graphs with power-law degree distributions	mechanism
Experiments on forty-eight large graphs from various domains	method
it is shown that the proposed algorithms lead to consistent parameter estimates at each agent	finding
the distributed estimators are shown to yield order-optimal convergence rates	finding
i	finding
e	finding
as far as the order of pathwise convergence is concerned	finding
the local agent estimates are as good as the optimal centralized nonlinear least squares estimator having access to the entire network observation data at all times	finding
Conforming to a given inter-agent communication or interaction topology	mechanism
distributed recursive estimators of the consensus + innovations type are presented in which at every observation sampling epoch the network agents exchange a single round of messages with their communication neighbors and recursively update their local parameter estimates by simultaneously processing the received neighborhood data and the new information ( innovation ) embedded in the observation sample	mechanism
Under rather weak conditions on the connectivity of the inter-agent communication and a global observability criterion	method
Furthermore under standard smoothness assumptions on the sensing nonlinearities	method
How can we design a product or movie that will attract	background
for example the interest of Pennsylvania adolescents or liberal newspaper critics ? What should be the genre of that movie and who should be in the cast	background
and show that it is highly scalable and effectively provides movie designs oriented towards different groups of users	finding
including men women	finding
and adolescents	finding
We formulate the movie design as an optimization problem over the inference of user-feature scores and selection of the features that maximize the number of attracted users Our approach	mechanism
PNP is based on a heterogeneous	mechanism
tripartite graph of users	mechanism
movies and features ( e	mechanism
g	mechanism
actors directors genres )	mechanism
where users rate movies and features contribute to movies	mechanism
We learn the preferences by leveraging user similarities defined through different types of relations	mechanism
and show that our method outperforms state-of-the-art approaches	mechanism
including matrix factorization and other heterogeneous graph-based analysis	mechanism
We evaluate PNP on publicly available real-world data	method
The World Wide Web ( WWW ) has become a rapidly growing platform consisting of numerous sources which provide supporting or contradictory information about claims ( e	background
g	background
`` Chicken meat is healthy '' )	background
In order to decide whether a claim is true or false	background
one needs to analyze content of different sources of information on the Web	background
measure credibility of information sources	background
and aggregate all these information	background
we demonstrate ClaimEval 's capability in determining validity of a set of claims	finding
resulting in improved accuracy compared to state-of-the-art baselines	finding
In this paper	mechanism
we present ClaimEval	mechanism
a novel and integrated approach which given a set of claims to validate	mechanism
extracts a set of pro and con arguments from the Web information sources	mechanism
and jointly ClaimEval uses Probabilistic Soft Logic ( PSL )	mechanism
resulting in a flexible and principled framework which makes it easy to state and incorporate different forms of prior-knowledge	mechanism
Through extensive experiments on real-world datasets	method
Human-Computer Music Performance for popular music - where musical structure is important	background
but where musicians often decide on the spur of the moment exactly what the musical form will be - presents many challenges to make computer systems that are flexible and adaptable to human musicians	background
We present new formalisms and representations	mechanism
and a corresponding implementation	mechanism
In networks such as the smart grid	background
communication networks and social networks	background
local measurements/observations are scattered over a wide geographical area	background
Centralized inference algorithm are based on gathering all the observations at a central processing unit	background
We discover and show that the message information matrix converges exponentially fast to a unique positive definite limit matrix for arbitrary positive semidefinite initialization	finding
using factor graphs and a distributed inference algorithm based on Gaussian belief propagation The distributed inference involves only local computation of the information matrix and of the mean vector and message passing between neighbors We provide the necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator	mechanism
An easily verifiable sufficient convergence condition on the topology of a factor graph is further provided	mechanism
analytically	method
High performance dense linear algebra ( DLA ) libraries often rely on a general matrix multiply ( Gemm ) kernel that is implemented using assembly or with vector intrinsics	background
In particular the real-valued Gemm kernels provide the overwhelming fraction of performance for the complex-valued Gemm kernels	background
along with the entire level-3 BLAS and many of the real and complex LAPACK routines	background
Thus achieving high performance for the Gemm kernel translates into a high performance linear algebra stack above this kernel	background
However it is a monumental task for a domain expert to manually implement the kernel for every library-supported architecture	background
This leads to the belief that the craft of a Gemm kernel is more dark art than science	background
It is this premise that drives the popularity of autotuning with code generation in the domain of DLA	background
results demonstrate that our approach yields generated kernels with performance that is competitive with kernels implemented manually or using empirical search	finding
This paper instead	mechanism
focuses on an analytical approach to code generation of the Gemm kernel for different architecture	mechanism
We distill the implementation of the kernel into an even smaller kernel	mechanism
an outer-product and analytically determine how available SIMD instructions can be used to compute the outer-product efficiently We codify this approach into a system to automatically generate a high performance SIMD implementation of the Gemm kernel	mechanism
Experimental	method
Many real-world graphs	background
such as those that arise from the web	background
biology and transportation	background
appear random and without a structure that can be exploited for performance on modern computer architectures	background
They focus primarily on reducing storage requirements and improving the cost of certain matrix operations for these large data sets	background
we outperform the state of the art for graphs with up to 10 7 non-zero edges	finding
Therefore we propose a data structure in a sparse and hierarchical fashion By maintaining the structure of the graph	mechanism
we preserve locality in the graph and in the cache	mechanism
For synthetic scale-free graph data	method
We also suggest several useful extensions of this method for increasing interpretability of predictive models and prediction performance	background
We present a novel subset scan method This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias and poor classifier fit	mechanism
not just in one or two dimensions of features of a priori interest	mechanism
but in the space of all possible feature subgroups	mechanism
We use subset scan and parametric bootstrap methods to efficiently address the difficulty of assessing the exponentially many possible subgroups	mechanism
Understanding how brain functions has been an intriguing topic for years With the recent progress on collecting massive data and developing advanced technology	background
people have become interested in addressing the challenge of decoding brain wave data into meaningful mind states	background
with many machine learning models and algorithms being revisited and developed	background
especially the ones that handle time series data because of the nature of brain waves	background
and reach a significant better results compared to traditional methods	finding
In this paper	mechanism
we propose an extension of State Space Model to work with different sources of information together with its learning and inference algorithms	mechanism
We apply this model to decode the mind state of students during lectures based on their brain waves	method
Abstract This paper presents a Grammar-aware Driver Parsing ( GDP ) algorithm	mechanism
with deep features	mechanism
A deep model is first trained to extract highly discriminative features of the driver	mechanism
Then a grammatical structure on the deep features is defined to be used as prior knowledge for a semi-supervised proposal candidate generation The Region with Convolutional Neural Networks ( R-CNN ) method is ultimately utilized to precisely segment parts of the driver	mechanism
The proposed method not only aims to automatically find parts of the driver in challenging drivers in the wild databases	mechanism
i	mechanism
e	mechanism
the standardized Strategic Highway Research Program ( SHRP-2 ) and the challenging Vision for Intelligent Vehicles and Application ( VIVA )	mechanism
but is also able to investigate seat belt usage and the position of the driver 's hands ( on a phone vs on a steering wheel )	mechanism
We conduct experiments on various applications and compare our GDP method against other state-of-the-art detection and segmentation approaches	method
i	method
e	method
SDS [ 1 ]	method
CRF-RNN [ 2 ]	method
DJTL [ 3 ]	method
and R-CNN [ 4 ] on SHRP-2 and VIVA databases	method
Mobile botnets have proliferated with the popularization of mobile and portable devices	background
being a simple and powerful method to launch Distributed Denial of Service ( DDoS ) attacks	background
This letter presents a stochastic adaptive model to generate DDoS attacks	mechanism
The bots collaborations combine reinforcement and fading rules based upon the level of servers activity and map to a time-varying weighted directed graph This model can explain the natural emergence of two distinct time-scales when bots massively attack a server	mechanism
Recent computer systems research has proposed using redundant requests to reduce latency	background
The idea is to replicate a request so that it joins the queue at multiple servers	background
The request is considered complete as soon as any one copy of the request completes	background
Redundancy is beneficial because it allows us to overcome server-side variability the fact that the server we choose might be temporarily slow due to factors such as background load	background
network interrupts and garbage collection	background
When there is significant server-side variability	background
replicating requests can greatly reduce response times	background
In the past few years	background
queueing theorists have begun to study redundancy	background
first via approximations	background
and more recently	background
via exact analysis	background
Unfortunately for analytical tractability	background
most existing theoretical analysis has assumed an Independent Runtimes ( IR ) model	background
wherein the replicas of a job each experience independent runtimes ( service times ) at different servers	background
and has provably excellent performance	finding
This paper introduces a much more realistic model of redundancy	mechanism
Our model allows us where we track both S and X for each job	mechanism
Analysis within the S & X model is	mechanism
of course much more difficult	mechanism
Nevertheless we design a policy	mechanism
Redundant-to-Idle-Queue ( RIQ ) which is both analytically tractable within the S & X model	mechanism
The kernel trick becomes a burden for some machine learning tasks such as dictionary learning	background
where a huge amount of training samples are needed	background
making the kernel matrix gigantic and infeasible to store or process	background
yields much better results than its image space counterparts	finding
We have shown	mechanism
in the context of missing data recovery through joint dictionary learning i	mechanism
e	mechanism
periocular-based full face hallucination	mechanism
that the approximated kernel expansion using Fastfood transform for joint dictionary learning Also	mechanism
explicit kernel expansion through Fastfood allows us to de-kernelize the reconstructed image in the feature space back to the image space	mechanism
enabling applications that require reconstructive dictionaries such as cross-domain reconstruction	mechanism
image super-resolution missing data recovery	mechanism
etc	mechanism
Policy approaches for addressing emerging consumer privacy concerns increasingly rely on providing consumers with more information and control over the usage of their personal data	background
Our results suggest that choice mechanisms alone may not reliably serve policy maker goals of protecting consumers privacy in the face of emerging data practices by firms	background
We find that consumers decision frames and thus	finding
their propensity to select privacy protective alternatives can be subtly but powerfully influenced by commonplace heterogeneity in the presentation of privacy choices	finding
In three experiments	method
Counterfactual Regret Minimization ( CFR ) is a popular iterative algorithm for approximating Nash equilibria in imperfect-information multi-step two-player zero-sum games	background
demonstrate that one can improve overall convergence in a game by first running CFR on a smaller	finding
coarser abstraction of the game and then using the strategy in the abstract game to warm start CFR in the full game	finding
We introduce the first general	mechanism
principled method Our approach requires only a strategy for each player	mechanism
and accomplishes the warm start at the cost of a single traversal of the game tree	mechanism
The method provably warm starts CFR to as many iterations as it would have taken to reach a strategy profile of the same quality as the input strategies	mechanism
and does not alter the convergence bounds of the algorithms	mechanism
Unlike prior approaches to warm starting	mechanism
ours can be applied in all cases Our method is agnostic to the origins of the input strategies	mechanism
For example they can be based on human domain knowledge	mechanism
the observed strategy of a strong agent	mechanism
the solution of a coarser abstraction	mechanism
or the output of some algorithm that converges rapidly at first but slowly as it gets closer to an equilibrium	mechanism
Experiments	method
Long-standing policy approaches to privacy protection are centered on consumer notice and control and assume that privacy decision making is a deliberative process of comparison between costs and benefits from information disclosure	background
An emerging body of work	background
however documents the powerful effects of factors unrelated to objective trade-offs in privacy settings	background
Our results confirm that understanding how differences in privacy choice emerge can help harmonize disparate perspectives on privacy decision making	background
We find that effects of rational and behavioral factors are associated with differences in the order and valence of queries considered in privacy settings	finding
In an online experiment	method
we borrow from query-theory literature and measure individuals ' considerations ( that is	method
queries ) across manipulations of rational and behavioral factors	method
Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled a myriad of new applications many of which provide aerial vision-based sensing In intrusion detection or target tracking applications	background
it is important to reach a given area of interest in the shortest time	background
and create an online data streaming connection to a monitoring station for immediate delivery of content to the operator	background
we validate our approach We show a seven-fold increase in the refresh rate of the AOI coverage	finding
we can cooperatively use multiple UAVs creating an array of moving cameras	mechanism
that always remain connected without breaks in communication	mechanism
We propose an optimal solution	mechanism
In this work	method
with a simulation that captures physical models and the application layer of each UAV	method
as well as the wireless network when comparing to a solution without the optimal sweeping and decentralised formation control	method
Robust face detection is one of the most important preprocessing steps to support facial expression analysis	background
facial landmarking face recognition	background
pose estimation building of 3D facial models	background
etc	background
results show that our proposed approach consistently achieves highly competitive results with the state-of-the-art performance against other recent face detection methods	finding
In this paper	mechanism
we present a novel approach named Multiple Scale Faster Region-based Convolutional Neural Network ( MS-FRCNN )	mechanism
The proposed approach is benchmarked on two challenging face detection databases	method
i	method
e	method
the Wider Face database and the Face Detection Dataset and Benchmark ( FDDB )	method
and compared against recent other face detection methods	method
e	method
g	method
Two-stage CNN Multi-scale Cascade CNN	method
Faceness Aggregate Chanel Features	method
HeadHunter Multi-view Face Detection	method
Cascade CNN etc	method
The experimental	method
5aural speech by converting it into visual text with less than a five second delay	background
Keeping the delay short 6 allows end-users to follow and participate in conversations	background
These results show the potential to 17 reliably capture speech even during sudden bursts of speed	background
as well as for generating enhanced captions	background
18 unlike other human-powered captioning approaches	background
We show that both hearing and DHH participants preferred 15 and followed collaborative captions better than those generated by automatic speech recognition ( ASR ) or 16 professionals due to the more consistent flow of the resulting captions	finding
We 8 first surveyed the audio characteristics of 240 one-hour-long captioned lectures on YouTube	method
such as speed 9 and duration of speaking bursts We then analyzed how these characteristics impact caption generation and 10 readability	method
considering specifically our human-powered collaborative captioning approach	method
We note that 11 most of these characteristics are also present in more general domains	method
For our caption comparison evalu12 ation	method
we transcribed a classroom lecture in real-time using all three captioning approaches	method
We recruited 13 48 participants ( 24 DHH ) to watch these classroom transcripts in an eye-tracking laboratory	method
We presented 14 these captions in a randomized	method
balanced order	method
Kidney exchange is a type of barter market where patients exchange willing but incompatible donors	background
These exchanges are conducted via cycleswhere each incompatible patient-donor pair in the cycle both gives and receives a kidneyand chains	background
which are started by an altruist donor who does not need a kidney in return	background
Algorithms from our group autonomously make the transplant plans for that exchange	finding
our new solver scales significantly better than the prior leading approaches	finding
We develop a provably correct which also necessarily changes the algorithm 's complexity	mechanism
as well as other improvements to the search algorithm A cap is desirable in practice since if even one edge in the chain fails	mechanism
the rest of the chain fails : the cap precludes very long chains that are extremely unlikely to execute and instead causes the solution to have more parallel chains and cycles that are more likely to succeed We work with the UNOS nationwide kidney exchange	mechanism
which uses a chain cap	mechanism
Next we compare our solver to the leading constraint-generation-based solver and to the best prior correct branch-and-price-based solver	method
We focus on the setting where chains have a length cap	method
On that real data and demographically-accurate generated data	method
that shows that in this model	finding
it is not possible to locally converge to an MIS in sub-polynomial time	finding
which allow us to circumvent the lower bound and find an MIS in polylogarithmic time	finding
it is possible to find an MIS in \ ( \mathcal O ( \log ^3 n ) \ ) time	finding
then we can also find an MIS in \ ( \mathcal O ( \log ^3 n ) \ ) time	finding
we can find an MIS in \ ( \mathcal O ( \log ^2 n ) \ ) time	finding
it is also possible to find an MIS in \ ( \mathcal O ( \log ^2 n ) \ ) time	finding
in an extremely harsh broadcast model that relies only on carrier sensing The model consists of an anonymous broadcast network in which nodes have no knowledge about the topology of the network or even an upper bound on its size	mechanism
Furthermore it is assumed that an adversary chooses at which time slot each node wakes up	mechanism
At each time slot a node can either beep	mechanism
that is emit a signal	mechanism
or be silent	mechanism
At a particular time slot	mechanism
beeping nodes receive no feedback	mechanism
while silent nodes can only differentiate between none of its neighbors beeping	mechanism
or at least one of its neighbors beeping	mechanism
We start by proving a lower bound We then study four different relaxations of the model First	method
we show that if a polynomial upper bound on the network size is known	method
Second if we assume sleeping nodes are awoken by neighboring beeps Third	method
if in addition to this wakeup assumption we allow sender-side collision detection	method
that is beeping nodes can distinguish whether at least one neighboring node is beeping concurrently or not	method
Finally if instead we endow nodes with synchronous clocks	method
and demonstrate our attack strategy	finding
The cyber-physical system is equipped with a Kalman filter and an attack detector that uses the innovations process of the Kalman filter	mechanism
The attacker performs an integrity attack on the actuators and sensors of the system with the aim of moving the system to a target state under the constraint that the probability of him or her being detected is equal to the false alarm probability of the attack detector	mechanism
We formulate and solve a constrained optimization problem	mechanism
in a numerical example	method
Having a shared and accurate sense of time is critical to distributed Cyber-Physical Systems ( CPS ) and the Internet of Things ( IoT )	background
Thanks to decades of research in clock technologies and synchronization protocols	background
it is now possible to measure and synchronize time across distributed systems with unprecedented accuracy However	background
applications have not benefited to the same extent due to limitations of the system services that help manage time	background
and hardware-OS and OS-application interfaces through which timing information flows to the application	background
Results from its evaluation are also presented	finding
We advocate the adoption of a holistic notion of Quality of Time ( QoT ) that captures metrics such as resolution	mechanism
accuracy and stability	mechanism
Building on this notion we propose an architecture in which the local perception of time is a controllable operating system primitive with observable uncertainty	mechanism
and where time synchronization balances applications ' timing demands with system resources such as energy and bandwidth Our architecture features an expressive application programming interface that is centered around the abstraction of a timeline a virtual temporal coordinate frame that is defined by an application to provide its components with a shared sense of time	mechanism
with a desired accuracy and resolution	mechanism
The timeline abstraction enables developers	mechanism
Leveraging open source hardware and software components	method
we have implemented an initial Linux realization of the proposed timeline-driven QoT stack on a standard embedded computing platform	method
indicating potentials of the grounded modeling for semantic extraction and language understanding applications	background
show significant superiority of our approach in topic perplexity and key entity identification	finding
In this paper	mechanism
we propose a structured topic representation based on an entity taxonomy from a knowledge base A probabilistic model is developed Each topic is equipped with a random walk over the entity hierarchy to extract semantically grounded and coherent themes	mechanism
Accurate entity modeling is achieved by leveraging rich textual features from the knowledge base	mechanism
Experiments	method
Formative assessments allow learners to quickly identify knowledge gaps Our results suggest Questimator may be useful for assessing learning in topics for which there is not an existing quiz	background
we found that participants ' scores on Questimator-generated quizzes correlated well with their scores on existing online quizzes on topics ranging from philosophy to economics	finding
Also Questimator generates questions with comparable discriminatory power as existing online quizzes	finding
This paper introduces Questimator	mechanism
an automated system that generates multiple-choice assessment questions for any topic contained within Wikipedia	mechanism
Given a topic	mechanism
Questimator traverses the Wikipedia graph to find and rank related topics	mechanism
and uses article text to form questions	mechanism
answers and distractor options	mechanism
In a study with 833 participants from Mechanical Turk	method
Concurrent C0 is an imperative programming language in the C family with session-typed message-passing concurrency	background
and show the results obtained	finding
While the abstract measure of span always decreases ( or remains unchanged )	finding
only a few of the examples reap a practical benefit	finding
A key idea is to postpone message reception as much as possible by interpreting receive commands as a request for a message	mechanism
We implemented our ideas as a translation from a blocking intermediate language to a non-blocking language	mechanism
Finally we evaluated our techniques with several benchmark programs	method
that outperforms traditional message passing techniques	finding
We describe Concurrent C0 with contracts and session-typed communication over channels	mechanism
Concurrent C0 supports an operation called forwarding which allows channels to be combined in a well-defined way The language 's type system enables elegant expression of session types and message-passing concurrent programs	mechanism
We provide a Go-based implementation with language based optimizations	method
Matrix-parametrized models ( MPMs ) are widely used in machine learning ( ML ) applications	background
to show that SFB guarantees convergence of algorithms ( under full broadcasting ) without requiring a centralized synchronization mechanism	finding
corroborate SFB 's efficiency	finding
we offer two contributions : first	mechanism
we develop a computation model for a large family of MPMs	mechanism
which share the following property : the parameter update computed on each data sample is a rank-1 matrix	mechanism
i	mechanism
e	mechanism
the outer product of two `` sufficient factors '' ( SFs ) Second	mechanism
we implement a decentralized	mechanism
peer-to-peer system Sufficient Factor Broadcasting ( SFB )	mechanism
which broadcasts the SFs among worker machines	mechanism
and reconstructs the update matrices locally at each worker	mechanism
SFB takes advantage of small rank-1 matrix updates and efficient partial broadcasting strategies to dramatically improve communication efficiency We propose a graph optimization based partial broadcasting scheme	mechanism
which minimizes the delay of information dissemination under the constraint that each machine only communicates with a subset rather than all of machines	mechanism
Furthermore we provide theoretical analysis Experiments on four MPMs	method
Regular SGVB estimators rely on sampling of parameters once per minibatch of data	mechanism
and have variance that is constant w	mechanism
r	mechanism
t	mechanism
the minibatch size	mechanism
The efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch	mechanism
Such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size	mechanism
generally leading to much faster convergence	mechanism
We find an important connection with regularization by dropout : the original Gaussian dropout objective corresponds to SGVB with local noise	mechanism
a scale-invariant prior and proportionally fixed posterior variance	mechanism
Our method allows ; specifically	mechanism
we propose \emph { variational dropout }	mechanism
a generalization of Gaussian dropout	mechanism
but with a more flexibly parameterized posterior	mechanism
The method is demonstrated through several experiments	method
A well-established approach -- which we refer to as implicit utilitarian voting -- assumes that voters have latent utility functions that induce the reported rankings	background
and seeks voting rules that approximately maximize utilitarian social welfare	background
Our methods underlie the design and implementation of an upcoming social choice website	background
results show that regret-based rules are more compelling than distortion-based rules	finding
leading us to focus on developing a scalable implementation for the optimal ( deterministic ) regret-based rule	finding
We extend this approach to the design of rules that select a subset of alternatives	mechanism
We derive analytical bounds on the performance of optimal ( deterministic as well as randomized ) rules in terms of two measures	mechanism
distortion and regret	mechanism
Empirical	method
The GFT is the mapping from the signal set into its representation by a direct sum of irreducible shift invariant subspaces : 1 ) this decomposition may not be unique ; and 2 ) there is freedom in the choice of basis for each component subspace	background
These issues are particularly relevant when the graph shift has repeated eigenvalues as is the case in many real-world applications ; by ignoring them	background
there is no way of knowing if different researchers are using the same definition of the GFT and whether their results are comparable or not	background
An illustrative example	finding
We develop a quasi -coordinate free definition of the GFT and graph spectral decomposition of graph signals that we implement through oblique spectral projectors	mechanism
We present properties of the GFT and of the spectral projectors and discuss a generalized Parseval 's inequality	mechanism
for a large real-world urban traffic dataset is provided	method
Optical music recognition ( OMR ) is the task of recognizing images of musical scores	background
In this paper	mechanism
improved algorithms were developed	mechanism
which facilitated bulk annotation of scanned scores for use in an interactive score display system	mechanism
Creating an initial annotation by OMR and verifying by hand substantially reduced the manual effort required to process scanned scores to be used in a live performance setting	mechanism
Computer music systems can interact with humans at different levels	background
including scores phrases	background
notes beats and gestures	background
However most current systems lack basic musicianship skills	background
and claim that a more human-like interaction is achieved	finding
We have built an artificial pianist that can automatically improve its ability to sense and coordinate with a human pianist	mechanism
learning from rehearsal experience	mechanism
We describe different machine learning algorithms to learn musicianship for duet interaction	mechanism
explore the properties of the learned models	method
such as dominant features	method
limits of validity	method
and minimal training size	method
Processes such as disease propagation and information diffusion often spread over some latent network structure which must be learned from observation	background
we show that our method learns a structure similar to the true underlying graph	finding
but enables faster and more accurate detection	finding
Motivated by new theoretical results on the consistency of constrained and unconstrained subset scans	mechanism
we propose a novel framework by comparing the most anomalous subsets detected with and without the graph constraints	mechanism
Our framework uses the mean normalized log-likelihood ratio score to measure the quality of a graph structure	mechanism
and efficiently searches for the highest-scoring graph structure	mechanism
Using simulated disease outbreaks injected into real-world Emergency Department data from Allegheny County	method
Our results show that identical highly expressed geolocations can be identified with the inexact method and the method based on eigenvector projections	finding
while reducing computation time by a factor of 26	finding
000 and reducing energy dispersal among the spectral components corresponding to the multiple zero eigenvalue	finding
We propose an inexact method as defined by the signal decomposition over the Jordan subspaces of the graph adjacency matrix	mechanism
This method projects the signal over the generalized eigenspaces of the adjacency matrix	mechanism
which accelerates the transform computation over large	mechanism
sparse and directed adjacency matrices The trade-off between execution time and fidelity to the original graph structure is discussed	mechanism
In addition properties such as a generalized Parseval 's identity and total variation ordering of the generalized eigenspaces are discussed	mechanism
The method is applied to 2010-2013 NYC taxi trip data to identify traffic hotspots on the Manhattan grid	method
Design of filters for graph signal processing benefits from knowledge of the spectral decomposition of matrices that encode graphs	background
such as the adjacency matrix and the Laplacian matrix	background
used to define the shift operator	background
For shift matrices with real eigenvalues	background
which arise for symmetric graphs	background
the empirical spectral distribution captures the eigenvalue locations	background
Under realistic circumstances	background
stochastic influences often affect the network structure and	background
consequently the shift matrix empirical spectral distribution	background
Nevertheless deterministic functions may often be found to approximate the asymptotic behavior of empirical spectral distributions of random matrices	background
demonstrate the results for sample parameters	finding
This paper uses stochastic canonical equation methods developed by Girko	mechanism
Included simulations	method
Robust principal component analysis PCA is one of the most important dimension-reduction techniques for handling high-dimensional data with outliers	background
illustrate the effectiveness and superiority of the proposed method	finding
In this letter	mechanism
we equivalently reformulate the objective of conventional PCA and learn the optimal projection directions by maximizing the sum of projected difference between each pair of instances based on -norm	mechanism
The proposed method is robust to outliers and also invariant to rotation	mechanism
More important the reformulated objective not only automatically avoids the calculation of optimal mean and makes the assumption of centered data unnecessary	mechanism
but also theoretically connects to the minimization of reconstruction error	mechanism
To solve the proposed nonsmooth problem	mechanism
we exploit an efficient optimization algorithm to soften the contributions from outliers by reweighting each data point iteratively	mechanism
We theoretically analyze the convergence and computational complexity of the proposed algorithm	mechanism
Extensive experimental results on several benchmark data sets	method
Biological adaptation is a powerful mechanism that makes many disorders hard to combat	background
We show that for the development of regulatory cells	finding
sequential plans yield significantly higher utility than the best static therapy In contrast	finding
for developing effector cells	finding
we find that ( at least for the given simulator	finding
objective function action possibilities	finding
and measurement possibilities ) single-step plans suffice for optimal treatment	finding
We propose a general approach where we leverage Monte Carlo tree search and the biological entity is modeled by a black-box simulator that the planner calls during planning We show that the framework can be used to steer a biological entity modeled via a complex signaling pathway network that has numerous feedback loops that operate at different rates and have hard-to-understand aggregate behavior We apply the framework to steering the adaptation of a patient 's immune system	mechanism
In particular we apply it to a leading T cell simulator ( available in the biological modeling package BioNetGen	mechanism
We run experiments with two alternate goals : developing regulatory T cells or developing effector T cells	method
The former is important for preventing autoimmune diseases while the latter is associated with better survival rates in cancer patients We are especially interested in the effect of sequential plans	method
an approach that has not been explored extensively in the biological literature	method
As publishers gather more information about their users	background
they can use that information to enable advertisers to create increasingly targeted campaigns	background
This enables better usage of advertising inventory	background
it yields two orders of magnitude improvement in run time and significant improvement in abstraction quality These benefits hold both for guaranteed and non-guaranteed campaigns	finding
We develop an optimal anytime algorithm The performance stems from three improvements : 1 ) a quadratic-time ( as opposed to doubly exponential or heuristic ) algorithm for finding an optimal split of an abstract segment	mechanism
2 ) a better scoring function for evaluating splits	mechanism
and 3 ) splitting time lossily like any other targeting attribute ( instead of losslessly segmenting time first )	mechanism
Compared to the segment abstraction algorithm by Walsh et al	method
[ 2010 ] for the same problem	method
Learning detectors that can recognize concepts	background
such as people actions	background
objects etc	background
in video content is an interesting but challenging problem	background
To the best of our knowledge	background
WELL achieves by far the best reported performance on these two webly-labeled big video datasets	background
The efficacy and the scalability of WELL have been extensively demonstrated Experimental results show that WELL significantly outperforms the state-of-the-art methods	finding
we propose a novel method called WEbly-Labeled Learning ( WELL )	mechanism
It is established on two theories called curriculum learning and self-paced learning and exhibits useful properties that can be theoretically verified	mechanism
We provide compelling insights on the latent non-convex robust loss that is being minimized on the noisy data	mechanism
In addition we propose two novel techniques that not only enable WELL to be applied to big data but also lead to more accurate results	mechanism
on two public benchmarks	method
including the largest multimedia dataset and the largest manually-labeled video set	method
State-of-the-art applications of Stackelberg security games -- including wildlife protection -- offer a wealth of data	background
which can be used to learn the behavior of the adversary	background
We also validate our approach	finding
We develop a new approach	mechanism
by observing how the attacker responds to only three defender strategies	mechanism
using experiments on real and synthetic data	method
Robust principal component analysis ( PCA ) is one of the most important dimension reduction techniques to handle high-dimensional data with outliers	background
Some experimental results demonstrate the effectiveness and superiority of the proposed approaches on image reconstruction and recognition	finding
In this paper	mechanism
we equivalently reformulate the maximization of variances for robust PCA	mechanism
such that the optimal projection directions are learned by maximizing the sum of the projected difference between each pair of instances	mechanism
rather than the difference between each instance and the mean of the Based on this reformulation	mechanism
we propose a novel robust PCA to automatically avoid the calculation of the optimal mean based on l1-norm distance This strategy also makes the assumption of centered data unnecessary	mechanism
Additionally we intuitively extend the proposed robust PCA to its 2D version for image recognition Efficient non-greedy algorithms are exploited to solve the proposed robust PCA and 2D robust PCA with fast convergence and low computational complexity	mechanism
on benchmark data sets	method
End-to-end learning of CNN/RNNs is currently not possible for whole videos due to GPU memory limitations and so a common practice is to use sampled frames as inputs along with the video labels as supervision	background
show that a simple maximum pooling on the sparsely sampled local features leads to significant performance improvement	finding
We therefore propose to instead treat the deep networks trained on local inputs as local feature extractors	mechanism
The local features are then aggregated to form global features which are used to assign video-level labels through a second classification stage	mechanism
We investigate a number of design choices for this local feature approach	mechanism
Experimental results on the HMDB51 and UCF101 datasets	method
Hybrid systems verification is quite important for developing correct controllers for physical systems	background
but is also challenging	background
Verification engineers thus	background
need to be empowered with ways of guiding hybrid systems verification while receiving as much help from automation as possible	background
We also share thoughts how the success of such a user interface design could be evaluated and anecdotal observations about it	background
Unsurprisingly the most difficult user interface challenges come from the desire to integrate automation and human guidance	finding
This paper presents the design ideas behind the user interface for the hybrid systems theorem prover KeYmaera X	mechanism
In human-robot teams	background
humans often start with an inaccurate model of the robot capabilities	background
As they interact with the robot	background
they infer the robot 's capabilities and partially adapt to the robot	background
i	background
e	background
they might change their actions based on the observed outcomes and the robot 's actions	background
without replicating the robot 's policy	background
We prove that the optimal policy can be computed efficiently	finding
that the proposed model significantly improves human-robot team performance	finding
We present a game-theoretic model where the human responds to the robot 's actions by maximizing a reward function that changes stochastically over time	mechanism
The robot can then use this model to decide optimally between taking actions that reveal its capabilities to the human and taking the best action given the information that the human currently has	mechanism
under certain observability assumptions We demonstrate through a human subject experiment compared to policies that assume complete adaptation of the human to the robot	method
Biological systems are increasingly being studied by high throughput profiling of molecular data over time	background
TPS can thus serve as a key design strategy for high throughput time series experiments	background
the points selected by TPS can be used to reconstruct an accurate representation for the expression values of the non selected points	finding
Here we present the Time Point Selection ( TPS ) method in a principled and practical way TPS utilizes expression data from a small set of genes sampled at a high rate	mechanism
Further even though the selection is only based on gene expression	mechanism
these points are also appropriate for representing a much larger set of protein	mechanism
miRNA and DNA methylation changes over time	mechanism
As we show by applying TPS to study mouse lung development	method
Identifying a masked suspect is one of the toughest challenges in biometrics that exist	background
This is an important problem faced in many law-enforcement applications on almost a daily basis	background
Herein a practical method is presented	mechanism
This approach reconstructs the entire frontal face based on an image of an individual 's periocular region	mechanism
By using an approach based on a modified sparsifying dictionary learning algorithm	mechanism
faces can be effectively reconstructed more accurately than with conventional methods	mechanism
Further various methods presented herein are open set	mechanism
and thus can reconstruct faces even if the algorithms are not specifically trained using those faces	mechanism
Complex event detection has been progressively researched in recent years for the broad interest of video indexing and retrieval	background
To fulfill the purpose of event detection	background
one needs to train a classifier using both positive and negative examples	background
Current classifier training treats the negative videos as equally negative	background
have validated the efficacy of our proposed approach	finding
we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event	mechanism
Based on these decisive attributes	mechanism
we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation	mechanism
The resulting fine-grained labels may be not optimal to capture the discriminative cues from the negative videos	mechanism
Hence we propose to jointly optimize the fine-grained labels with the classifier learning	mechanism
which brings mutual reciprocality	mechanism
Meanwhile the labels of positive examples are supposed to remain unchanged	mechanism
We thus additionally introduce a constraint for this purpose On the other hand	mechanism
the state-of-the-art deep convolutional neural network features are leveraged in our approach for event detection to further boost the performance	mechanism
Extensive experiments on the challenging TRECVID MED 2014 dataset	method
Generalized canonical correlation analysis ( GCCA ) aims at extracting common structure from multiple 'views '	background
i	background
e	background
high-dimensional matrices representing the same objects in different feature domains an extension of classical two-view CCA	background
further reduce the runtime significantly ( by 30 % ) if multiple cores are available	finding
to showcase the effectiveness of the proposed algorithms	finding
we propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements	mechanism
respectively Consequently the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100	mechanism
000 while the current approaches can only handle thousands of features / samples	mechanism
Our second contribution is a distributed algorithm for GCCA	mechanism
which computes the canonical components of different views in parallel and thus can	mechanism
in experiments Judiciously designed synthetic and real-data experiments using a multilingual dataset are employed	method
Display appropriation provides a means by which mobile users can cyber-forage local display hardware to provide them with access to a high-quality output device	background
In this demonstration we show a system that presents an alternative vision in which users are able to cyber-forage for both display and compute resources in their local area enabling them The demonstration leverages a cohesive suite of existing systems	mechanism
i	mechanism
e	mechanism
cloudlets Internet Suspend/Resume ( ISR )	mechanism
Yarely and Tacita	mechanism
to deliver this vision	mechanism
that reduces execution time from 3	finding
000 days to less than a day	finding
This paper presents a two-part solution space and time design considerations with Dijkstra 's algorithm	mechanism
with HTCondor Our contribution is to present a solution	mechanism
with detailed analysis of the necessary design decisions	method
Our first contribution is two discoveries : ( i ) the number of comments grows as a power-law on the number of votes and ( ii ) the time between a submission creation and a user 's reaction obeys a log-logistic distribution	finding
VnC outperformed state-of-the-art baselines on accuracy Additionally	finding
we illustrate VnC usefulness for forecasting and outlier detection	finding
Based on these patterns	mechanism
we propose VnC ( Vote-and-Comment )	mechanism
a parsimonious but accurate and scalable model	mechanism
We analyzed over 20	method
000 submissions corresponding to more than 100 million user interactions from three social voting Web sites : Reddit	method
Imgur and Digg In our experiments on real data	method
Given a heterogeneous network	background
with nodes of different types - e	background
g	background
products users and sellers from an online recommendation site like Amazon - and labels for a few nodes ( 'honest '	background
'suspicious ' etc )	background
can we find a closed formula for Belief Propagation ( BP )	background
exact or approximate ? Can we say whether it will converge ?	background
( 4 ) Effectiveness ZooBP identifies fraudulent users with a near-perfect precision of 92	finding
3 % over the top 300 results	finding
We propose ZooBP	mechanism
a method with provable convergence guarantees ZooBP has the following advantages : ( 1 ) Generality : It works on heterogeneous graphs with multiple types of nodes and edges ; ( 2 ) Closed-form solution : ZooBP gives a closed-form solution as well as convergence guarantees ; ( 3 ) Scalability : ZooBP is linear on the graph size and is up to 600 faster than BP	mechanism
running on graphs with 3	mechanism
3 million edges in a few seconds	mechanism
Applied on real data ( a Flipkart e-commerce network with users	method
products and sellers )	method
Many theories have emerged which investigate how in- variance is generated in hierarchical networks through sim- ple schemes such as max and mean pooling	background
The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations	background
We utilize a novel pooling layer called adaptive pooling These networks with the learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks In- terestingly	mechanism
adaptive pooling can converge to mean pooling ( when initialized with random pooling weights )	mechanism
find more general linear pooling schemes or even decide not to pool at all	mechanism
We illustrate the general notion of selective invari- ance through object categorization experiments on large- scale datasets such as SVHN and ILSVRC 2012	method
A k-core is the maximal subgraph where all vertices have degree at least k	background
This concept has been applied to such diverse areas as hierarchical structure analysis	background
graph visualization and graph clustering	background
Our discoveries are as follows : ( 1 ) Mirror Pattern : coreness of vertices ( i	finding
e	finding
maximum k such that each vertex belongs to the k-core ) is strongly correlated to their degree	finding
( 2 ) Core-Triangle Pattern : degeneracy of a graph ( i	finding
e	finding
maximum k such that the k-core exists in the graph ) obeys a 3-to-1 power law with respect to the count of triangles	finding
( 3 ) Structured Core Pattern : degeneracy-cores are not cliques but have non-trivial structures such as core-periphery and communities	finding
Our algorithmic contributions show the usefulness of these patterns	mechanism
( 1 ) Core-A	mechanism
which measures the deviation from Mirror Pattern	mechanism
successfully finds anomalies in real-world graphs complementing densest-subgraph based anomaly detection methods	mechanism
( 2 ) Core-D	mechanism
a single-pass streaming algorithm based on Core-Triangle Pattern	mechanism
accurately estimates the degeneracy of billion-scale graphs up to 7 faster than a recent multipass algorithm	mechanism
( 3 ) Core-S	mechanism
inspired by Structured Core Pattern	mechanism
identifies influential spreaders up to 17 faster than top competitors with comparable accuracy	mechanism
Methods and apparatuses are provided In one aspect	mechanism
a proximity image is received having proximity image data from which it can be determined which areas of the proximity sensitive surface sensed the elongated interface object during a period of time	mechanism
A proximity blob is identified in the proximity image and the proximity image is transformed using a plurality of different transformations to obtain a plurality of differently transformed proximity images	mechanism
A plurality of features is determined for the identified blob in the transformed proximity images and the pitch of the elongated interface object relative to the proximity sensitive surface is determined based upon the determined features and a multi dimensional heuristic regression model of the proximity sensitive surface ; and a yaw is determined based upon the pitch	mechanism
There are many cases where collections of subgraphs may be contrasted against each other	background
For example they may be as- signed ground truth labels ( spam/not-spam )	background
or it may be desired to directly compare the biological networks of different species or compound networks of different chemicals	background
show findings that agree with human intuition on datasets from Amazon co-purchases	finding
Congressional bill sponsorships and DBLP co-authorships	finding
We also show that our approach of characterizing subgraphs is better suited for sense-making than discriminating classification approaches	finding
We define this characterization problem as one of partitioning the attributes into as many groups as the number of classes	mechanism
while maximizing the total attributed quality score of all the given subgraphs We show that our attribute-to-class assignment problem is NP-hard and an optimal ( 1 -- 1/e ) -approximation algorithm exists	mechanism
We also propose two different faster heuristics that are linear-time in the number of attributes and subgraphs Unlike previous work where only attributes were taken into account for characterization	mechanism
here we exploit both attributes and social ties ( i	mechanism
e	mechanism
graph structure )	mechanism
Through extensive experiments	method
we compare our proposed algorithms	method
The ubiquity of mobile devices and cloud services has led to an unprecedented growth of online personal photo and video collections	background
Due to the scarcity of personal media search log data	background
research to date has mainly focused on searching images and videos on the web	background
To the best of our knowledge	background
this paper is the first The insightful observations will not only be instrumental in guiding future personal media search methods	background
but also benefit related tasks such as personal photo browsing and recommendation	background
Our findings suggest there is a significant gap between personal queries and automatically detected concepts	finding
which is responsible for the low accuracy of many personal media search queries verify the efficacy of the proposed method in improving personal media search	finding
where the proposed method consistently outperforms baseline methods	finding
we propose the deep query understanding model to learn a mapping from the personal queries to the concepts in the clicked photos	mechanism
using large-scale real-world search logs	method
We analyze different types of search sessions mined from Flickr search logs and discover a number of interesting characteristics of personal media search in terms of information needs and click behaviors	method
Experimental results	method
With the ubiquitous development of mobile technologies	background
many cities today have installed mobile-enabled bike sharing systems - both publicly and privately owned - in an effort to nudge dwellers towards a more sustainable mode of transportation However	background
there is little evidence - apart from anecdote stories - for the success of these systems	background
This can have significant implications that shared bike systems can shift transportation modes	background
which consequently can have rippling effects for the economy and environment	background
Our findings provide evidence that even when controlling for the lost parking space ( used to build the parking stations ) the parking demand in the nearby areas was reduced by approximately 2 %	finding
In particular our follow-up analyses indicate that the new bike share system could lead to a monthly reduction of 0	finding
82 metric tones CO2 emissions per square mile	finding
or approximately 4	finding
381 metric tones of CO2 in the metro area of Pittsburgh	finding
and using the difference-in-differences framework	mechanism
The latter can be thought of as a lower bound for the car trips generated towards a specific area and has implications towards potential substitution effects between driving and biking	method
In particular we use data from Healthy Ride	method
the newly installed shared bike system in the city of Pittsburgh	method
combined with data we obtained from the Pittsburgh Parking Authority	method
Multi-aspect data appear frequently in many web-related applications	background
For example product reviews are quadruplets of ( user	background
product keyword timestamp )	background
How can we analyze such web-scale multi-aspect data ? Can we analyze them on an off-the-shelf workstation with limited amount of memory ? Tucker decomposition has been widely used for discovering patterns in relationships among entities in multi-aspect data	background
naturally expressed as high-order tensors	background
S-HOT showed better scalability not only with the order but also with the dimensionality and the rank than baseline methods In particular	finding
S-HOT decomposed tensors 1000 larger than baseline methods in terms dimensionality S- HOT also successfully analyzed real-world tensors that are both large-scale and high-order on an off-the-shelf workstation with limited amount of memory	finding
while baseline methods failed The source code of S-HOT is publicly available at http : //dm	finding
postech	finding
ac	finding
kr/shot to encourage reproducibility	finding
we propose S-HOT	mechanism
a scalable high-order tucker decomposition method that employs the on-the-fly computation Moreover	mechanism
S-HOT is designed for handling disk-resident tensors	mechanism
too large to fit in memory	mechanism
without loading them all in memory at once	mechanism
We provide theoretical analysis on the amount of memory space and the number of scans of data required by S-HOT	method
In our experiments	method
How can we detect fraudulent lockstep behavior in large-scale multi-aspect data ( i	background
e	background
tensors ) ? Can we detect it when data are too large to fit in memory or even on a disk ? Past studies have shown that dense blocks in real-world tensors ( e	background
g	background
social media Wikipedia	background
TCP dumps etc	background
) signal anomalous or fraudulent behavior such as retweet boosting	background
bot activities and network attacks	background
Thus various approaches	background
including tensor decomposition and search	background
have been used for rapid and accurate dense-block detection in tensors	background
D-Cube is ( 1 ) Memory Efficient : requires up to 1	finding
600 times less memory and handles 1	finding
000 times larger data ( 2	finding
6TB ) ( 2 ) Fast : up to 5 times faster due to its near-linear scalability with all aspects of data	finding
( 3 ) Provably Accurate : gives a guarantee on the densities of the blocks it finds	finding
and ( 4 ) Effective : successfully spotted network attacks from TCP dumps and synchronized behavior in rating data with the highest accuracy	finding
we propose D-Cube	mechanism
a disk-based dense-block detection method	mechanism
which also can be run in a distributed manner across multiple machines	mechanism
Compared with state-of-the-art methods	method
Supervised CNNs due to their immense learning capacity	background
have shown superior performance on a range of computer vision problems including optical flow prediction	background
Our guided learning approach is competitive with or superior to state-of-the-art approaches	finding
We therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used The models are further refined in an unsupervised fashion using an image reconstruction loss yet is completely unsupervised and can run in real time	mechanism
on three standard benchmark datasets	method
This generalizes a prior decomposition result for an M/M/k/staggeredsetup	background
We show the response time of an M/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/k and the setup time	finding
where the approximation is nearly exact	finding
that for exponentially distributed setup times	method
A group of agents makes linear measurements of the unknown parameter	background
The agent measurements are locally unobservable	background
and the agents exchange information over a communication network in order to compute an estimate	background
A subset of the agents is adversarial and exchanges false information in order to prevent the remaining	background
normally-behaving agents from correctly estimating the parameter	background
Finally we provide examples of the performance of the FRDE algorithm	finding
We present Flag Raising Distributed Estimation ( FRDE ) algorithm The FRDE algorithm is a consensus+innovations type estimator in which agents combine estimates of neighboring agents ( consensus ) with local sensing information ( innovations )	mechanism
Under the FRDE algorithm	mechanism
global observability for connected normally-behaving agents is a necessary and sufficient condition to either correctly estimate the parameter or correctly detect the presence of an adversary	mechanism
If FRDE detects an adversary	mechanism
we show how existing methods for attack identification in cyber-physical systems can be used to identify the adversarial agents	mechanism
and analyze numerical	method
Unmanned aerial vehicles ( UAVs ) recently enabled a myriad of new applications spanning domains from personal entertainment to surveillance	background
We show that this platform is not omnidirectional in the horizontal plane and that UAV-to-UAV communication ceases around 75m the paper derives the optimal number of hops that maximize the end-to-end throughput	finding
as well as the corresponding hop lengths	finding
transmitting payloads up to 200m ( over 802	finding
11g at 54MBps )	finding
In this paper	mechanism
we focus on using several small UAVs collaboratively We make use of 802	mechanism
11 radios on low-cost commercial-off-the-shelf UAVs	mechanism
set up a time-division multiple access overlay protocol to avoid mutual interference	mechanism
and enable high channel utilization in multihop networks In particular	mechanism
we provide a model for the quality of the UAV-to-UAV link	mechanism
in terms of packet delivery ratio as a function of distance	mechanism
packet size and orientation	mechanism
based on an extensive measurement campaign	mechanism
Concerning the operation in a multihop mode to allow extending the network We validate our mathematical model with extensive experimental measurements	method
Summary Successful application of two-photon imaging withgenetic tools in awake macaque monkeys will enable fundamental advances in our understanding of higher cognitive function at the level of molecular and neuronal circuits	background
By providing two-photon imaging access to cortical neuronal populations at single-cell or single dendritic spine resolution in awake monkeys	background
the techniques reported can help bridge the use of modern genetic and molecular tools and the study of higher cognitive function	background
confirm that fluorescence activity is linearly proportional to neuronal spiking activity across a wide range of firing rates ( 10Hz to 150Hz )	finding
Here we report techniques Using genetically encoded indicators including GCaMP5 and GCaMP6s delivered by AAV2/1 into the visual cortex	mechanism
we demonstrate that high-quality two-photon imaging of large neuronal populations can be achieved and maintained in awake monkeys for months	mechanism
Simultaneous intracellular recording and two-photon calcium imaging	method
Our work provides a solid step toward a systematic and quantitative wall-centric profiling of Facebook user activity	background
Our key results can be summarized in the following points	finding
First we find that many wall activities	finding
including number of posts	finding
number of likes	finding
number of posts of type photo	finding
can be described by the PowerWall distribution	finding
What is more surprising is that most of these distributions have similar slope	finding
with a value close to 1 ! Second	finding
we show how our patterns and metrics can help us spot surprising behaviors and anomalies	finding
For example we find a user posting every two days	finding
exactly the same count of posts ; another user posting at midnight	finding
with no other activity before or after	finding
In this work	mechanism
we model Facebook user behavior : We propose PowerWall	mechanism
a lesser known heavy-tailed distribution to fit our data	mechanism
we analyze the wall activities of users focusing on identifying common patterns and surprising phenomena	method
We conduct an extensive study of roughly 7k users over 3 years during 4-month intervals each year	method
To infer the histories of population admixture	background
one important challenge with methods based on the admixture linkage disequilibrium ( ALD ) is to get rid of the effect of source LD ( SLD ) which is directly inherited from source populations	background
In previous methods	background
only the decay curve of weighted LD between pairs of sites whose genetic distance were larger than a certain starting distance was fitted by single or multiple exponential functions	background
for the inference of recent single- or multiple-wave of admixture	background
We showed that iMAAPs is a considerable improvement over other current methods and further facilitates the inference of the histories of complex population admixtures	finding
We further developed a method	mechanism
iMAAPs by fitting ALD using Polynomial spectrum	mechanism
In this study	method
we defined the SLD in the formularized weighted LD statistic under the two-way admixture model	method
and proposed polynomial spectrum ( p-spectrum ) We also found reference populations could be used to reduce the SLD in weighted LD statistic	method
We evaluated the performance of iMAAPs under various admixture models in simulated data and applied iMAAPs into analysis of genome-wide single nucleotide polymorphism data from the Human Genome Diversity Project ( HGDP ) and the HapMap Project	method
Moreover the family of models adapts well to capture the phenomenon of emergence and downfall of leaders in social networks	finding
We formulate a set of time-varying stochastic networked dynamical systems The dynamics of the strength of connections abide by local laws of reinforcement and penalization due to interactions among the agents	mechanism
The proposed stochastic dynamical systems exhibit a strong-attractor as a certain subset of the set of binary matrices	mechanism
as it will be illustrated via numerical simulations	method
Abstract Rapid advances in high-throughput sequencing and a growing realization of the importance of evolutionary theory to cancer genomics have led to a proliferation of phylogenetic studies of tumour progression These studies have yielded not only new insights but also a plethora of experimental approaches	background
sometimes reaching conflicting or poorly supported conclusions	background
closing with a perspective on the prospects and broader implications of this field	background
We survey the range of methods and tools available to the researcher	method
their key applications	method
and the various unsolved problems	method
Sparse iterative methods	background
in particular first-order methods	background
are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games	background
The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on	background
we show that	finding
for the first time	finding
the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm	finding
CFRP in practice	finding
We investigate the acceleration of first-order methods through better design of the dilated entropy function -- -a class of distance-generating functions related to the domains associated with the extensive-form games	mechanism
By introducing a new weighting scheme for the dilated entropy function	mechanism
we develop the first distance-generating function that only a logarithmic dependence on the branching factor of the player	mechanism
This result improves the convergence rate of several first-order methods by a factor of ( b dd )	mechanism
where b is the branching factor of the player	mechanism
and d is the depth of the game tree	mechanism
Thus far counterfactual regret minimization methods have been faster in practice	mechanism
and more popular	mechanism
than first-order methods despite their theoretically inferior convergence rates	mechanism
Using our new weighting scheme and practical tuning	method
These graph-based problems are related to many real-world applications	background
such as localizing stimulus in brain connectivity networks	background
and mining traffic events in city street networks	background
where the key issue is to find the supports of localized activated patterns	background
Counterparts of these problems in classical signal/image processing	background
such as impulse detection and foreground detection	background
have been studied over the past few decades	background
The analysis validates the effectiveness of the approach and suggests that graph signal processing tools may aid in urban planning and traffic forecasting	finding
tuning any thresholds	mechanism
We use piecewise-constant graph signals where each piece indicates a localized pattern that exhibits homogeneous internal behavior and the number of pieces indicates the number of localized patterns For such signals	mechanism
we show that decomposition and dictionary learning are natural extensions of localization	mechanism
the goal of which is not only to efficiently approximate graph signals	mechanism
but also to accurately find supports of localized patterns	mechanism
we propose a specific graph signal model	mechanism
an optimization problem	mechanism
and a computationally efficient solver The proposed solvers directly find the supports of arbitrary localized activated patterns without	mechanism
We then conduct an extensive empirical study to validate the proposed methods on both simulated and real data including the analysis of a large volume of spatio-temporal Manhattan urban data	method
Video semantic recognition usually suffers from the curse of dimensionality and the absence of enough high-quality labeled instances	background
thus semisupervised feature selection gains increasing attentions for its efficiency and comprehensibility	background
Most of the previous methods assume that videos with close distance ( neighbors ) have similar labels and characterize the intrinsic local structure through a predetermined graph of both labeled and unlabeled data	background
illustrate the effectiveness and superiority of the proposed approach on video semantic recognition related tasks	finding
In this paper	mechanism
we exploit a novel semisupervised feature selection method from a new perspective	mechanism
The primary assumption underlying our model is that the instances with similar labels should have a larger probability of being neighbors	mechanism
Instead of using a predetermined similarity graph	mechanism
we incorporate the exploration of the local structure into the procedure of joint feature selection so as to learn the optimal graph simultaneously	mechanism
Moreover an adaptive loss function is exploited to measure the label fitness	mechanism
which significantly enhances model 's robustness to videos with a small or substantial loss	mechanism
We propose an efficient alternating optimization algorithm to solve the proposed challenging problem	mechanism
together with analyses on its convergence and computational complexity in theory	mechanism
Finally extensive experimental results on benchmark datasets	method
: Cellular Electron CryoTomography ( CECT ) enables 3D visualization of cellular organization at near-native state and in sub-molecular resolution	background
making it a powerful tool for analyzing structures of macromolecular complexes and their spatial organizations inside single cells	background
However high degree of structural complexity together with practical imaging limitations make the systematic de novo discovery of structures within cells challenging	background
It would likely require averaging and classifying millions of subtomograms potentially containing hundreds of highly heterogeneous structural classes	background
Results show that our new approach achieves significant improvements in both discrimination ability and scalability	finding
More importantly our new approach is able to discover new structural classes and recover structures that do not exist in training data	finding
in this paper we propose a new approach for subdividing subtomograms into smaller but relatively homogeneous subsets	mechanism
The structures in these subsets can then be separately recovered using existing computation intensive methods	mechanism
Our approach is based on supervised structural feature extraction using deep learning	mechanism
in combination with unsupervised clustering and reference-free classification	mechanism
Our experiments compared to existing unsupervised rotation invariant feature and pose-normalization based approaches	method
The techniques developed in the paper for establishing weak convergence might be of independent interest	background
A method is disclosed Initially	mechanism
a set of modulated ultrasound signals and a set of radio signals are separately broadcast from a group of transmitters	mechanism
The ultrasound signals include at least one symbol configured for pulse compression	mechanism
After the receipt of a demodulated ultrasound signal from a mobile device	mechanism
wherein the demodulated ultrasound signal is derived from the modulated ultrasound signals	mechanism
transmitter identifier and timing information are extracted from the demodulated ultrasound signal Timing information include	mechanism
for example the arrival time of the demodulated ultrasound signal in relation to the start time of its transmission	mechanism
After the locations of the transmitters have been ascertained from the transmitter identifier information	mechanism
the location of the mobile device can be determined based on the timing information and the locations of the transmitters	mechanism
Researchers and educators have designed curricula and resources for introductory programming environments such as Scratch	background
App Inventor and Kodu to foster computational thinking in K-12	background
We found that the students who used physical manipulatives performed well in rule construction	finding
whereas the students who engaged more with the rule editor of the programming environment had better mental simulation of the rules and understanding of the concepts	finding
In particular we investigated the impact of physical manipulatives on 3rd -- 5th grade students ' ability to understand	method
recognize construct and use game programming design patterns	method
Our result essentially states that under an appropriate dynamics of the underlying network of contacts	background
the macroprocess ( Y N ( t ) ) becomes asymptotically ( in N ) Markov	background
Abstract The heterogeneity-gap between different modalities brings a significant challenge to multimedia information retrieval	background
Some studies formalize the cross-modal retrieval tasks as a ranking problem and learn a shared multi-modal embedding space to measure the cross-modality similarity	background
indicate that the proposed method achieves significant improvements over the state-of-the-arts in this literature	finding
In this paper	mechanism
we involve the self-paced learning theory with diversity into the cross-modal learning This strategy enhances the models robustness to outliers and achieves better generalization via training the model gradually from easy rankings by diverse queries to more complex ones	mechanism
An efficient alternative algorithm is exploited to solve the proposed challenging problem with fast convergence in practice	mechanism
Extensive experimental results on several benchmark datasets	method
In addition we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision	finding
demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory	finding
we propose a sparsity constrained maximum likelihood estimator based on matrix factorization	mechanism
and an efficient alternating gradient descent algorithm with hard thresholding to solve it	mechanism
Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM	mechanism
Experiments on both synthetic and genomic data	method
Despite progress in visual perception tasks such as image classification and detection	background
computers still struggle to understand the interdependency of objects in the scene as a whole	background
e	background
g	background
relations between objects or their attributes	background
validate the superiority of VRL	finding
which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types	finding
We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes	finding
we propose a deep Variation-structured Reinforcement Learning ( VRL ) framework to sequentially discover object relationships and attributes in the whole image	mechanism
First a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories	mechanism
predicates and attributes	mechanism
Next we use a variation-structured traversal over the action graph to construct a small	mechanism
adaptive action set for each step based on the current state and historical actions	mechanism
In particular an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish	mechanism
We then make sequential predictions using a deep RL framework	mechanism
incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector	mechanism
Our experiments on the Visual Relationship Detection ( VRD ) dataset and the large-scale Visual Genome dataset	method
14 The precision and recall of RBCs detection are 98	finding
43 % and 94	finding
99 % respectively	finding
whereas those of WBCs detection are 99	finding
12 % and 99	finding
12 %	finding
The F-measure of our proposed WBCs segmentation gets up to 95	finding
8 %	finding
This paper presents an end-to-end framework Our proposed system contains several components to solve different problems regarding RBCs and WBCs We first design a novel blood cell color representation which is able to emphasize the RBCs and WBCs in separate channels	mechanism
Template matching technique is then employed to individually detect RBCs and WBCs in our proposed representation	mechanism
In order to automatically segment the RBCs and nuclei from WBCs	mechanism
we develop an adaptive level set-based segmentation method which makes use of both local and global information The detected and segmented RBCs	mechanism
however can be a single RBC	mechanism
a connected RBC or an abnormal RBC Therefore	mechanism
we first separate and reconstruct RBCs from the connected RBCs by our suggested modified template matching	mechanism
Shape matching by inner distance is later used to classify the abnormal RBCs from the normal RBCs	mechanism
Our proposed method has been tested and evaluated on different images from ALL-IDB	method
10 WebPath 24 UPMC	method
23 Flicker datasets	method
and the one used by Mohamed et al	method
As smartphones and tablets have been widely adopted and mobile banking apps have come into ubiquitous use	background
mobile devices have increasingly become new tools that customers use for banking	background
payments budgeting and shopping	background
This study has implications for banks managers related to the design and management of service delivery channels	background
and for financial regulators related to the inclusiveness of financial system	background
Our findings suggest that : ( 1 ) the use of the mobile channel increases customer demand for digital services ; ( 2 ) lower ATM density and higher branch channel density in the customers vicinity is associated with higher digital service demand ; ( 3 ) the mobile phone channel serves as a complement to the PC channel	finding
the tablet channel substitutes for the PC channel	finding
and the mobile phone channel and the tablet channel complement one another ; ( 4 ) customers acquire more information for financial decision-making following the use of the mobile channel	finding
and mobile phone and tablet users are less likely to incur overdraft and credit card penalty fees	finding
Net benefit of the mobile channel to the bank is $ 0	finding
07 USD per month per ( average ) customer	finding
based on a novel large-scale dataset that contains 43 million individual transactions from 190	mechanism
000 customers during April to June 2013 from a financial institution in the United States	mechanism
Our analysis is validated	method
Intelligent conversational assistants	background
such as Apple 's Siri	background
Microsoft 's Cortana	background
and Amazon 's Echo	background
have quickly become a part of our digital life	background
Our observations could assist the deployment of crowd-powered conversation systems and crowd-powered systems in general	background
Up to the first month of our deployment	finding
59 users have held conversations with Chorus during 320 conversational sessions	finding
we developed a crowd-powered conversational assistant	mechanism
Chorus and deployed it to see how users and workers would interact together when mediated by the system Chorus sophisticatedly converses with end users over time by recruiting workers on demand	mechanism
which in turn decide what might be the best response for each user sentence	mechanism
Voting systems typically treat all voters equally	background
We derive possibility and impossibility results for the existence of such weighting schemes	finding
depending on whether the voting rule and the weighting scheme are deterministic or randomized	finding
as well as on the social choice axioms satisfied by the voting rule	finding
we draw on no-regret learning	mechanism
Specifically given a voting rule	mechanism
we wish to design a weighting scheme such that applying the voting rule	mechanism
with voters weighted by the scheme	mechanism
leads to choices that are almost as good as those endorsed by the best voter in hindsight	mechanism
Understanding traffic density from large-scale web camera ( webcam ) videos is a challenging problem because such videos have low spatial and temporal resolution	background
high occlusion and large perspective	background
and get insights from optimization based method to improve deep model FCN based method significantly reduces the mean absolute error from 10	finding
99 to 5	finding
31 on the public dataset TRANCOS compared with the state-of-the-art baseline	finding
we explore both deep learning based and optimization based methods	mechanism
both methods map the image into vehicle density map	mechanism
one based on rank constrained regression and the other one based on fully convolution networks ( FCN ) The regression based method learns different weights for different blocks in the image to increase freedom degrees of weights and embed perspective information	mechanism
The FCN based method jointly estimates vehicle density map and vehicle count with a residual learning framework to perform end-to-end dense prediction	mechanism
allowing arbitrary image resolution	mechanism
and adapting to different vehicle scales and perspectives	mechanism
We analyze and compare both methods Since existing datasets do not cover all the challenges in our work	method
we collected and labelled a large-scale traffic video dataset	method
containing 60 million frames from 212 webcams Both methods are extensively evaluated and compared on different counting tasks and datasets	method
In this paper	mechanism
we present reasoning techniques comprising discrete dynamics as well as continuous dynamics	mechanism
in which the components have local responsibilities	mechanism
Our approach supports component contracts i	mechanism
e	mechanism
input assumptions and output guarantees of interfaces that are more general than previous component-based hybrid systems verification techniques in the following ways : We introduce change contracts	mechanism
which characterize how current values exchanged between components along ports relate to previous values	mechanism
We also introduce delay contracts	mechanism
which describe the change relative to the time that has passed since the last value was exchanged Together	mechanism
these contracts can take into account what has changed between two components in a given amount of time since the last exchange of information	mechanism
Most crucially we prove that the safety of compatible components implies safety of the composite	mechanism
The proof steps of the theorem are also implemented as a tactic in KeYmaerai ? X	method
allowing automatic generation of a KeYmaerai ? X proof for the composite system from proofs of the concrete components	method
Processes such as disease propagation and information diffusion often spread over some latent network structure that must be learned from observation	background
the authors show that their method learns a structure similar to the true underlying graph	finding
but enables faster and more accurate detection	finding
They propose a novel framework by comparing the most anomalous subsets detected with and without the graph constraints	mechanism
Their framework uses the mean normalized log-likelihood ratio score to measure the quality of a graph structure	mechanism
and it efficiently searches for the highest-scoring graph structure	mechanism
Using simulated disease outbreaks injected into real-world Emergency Department data from Allegheny County	method
we show results on a synthetic world	finding
where the agents communicate in ungrounded vocabulary	finding
i	finding
e	finding
symbols with no pre-specified meanings ( X	finding
Y Z ) We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes ( shape/color/style ) Thus	finding
we demonstrate the emergence of grounded language and communication among 'visual ' dialog agents with no human supervision	finding
and show that the RL 'fine-tuned ' agents significantly outperform SL agents	finding
Interestingly the RL Qbot learns to ask questions that Abot is good at	finding
ultimately resulting in more informative dialog and a better team	finding
We introduce the first goal-driven training Specifically	mechanism
we pose a cooperative 'image guessing ' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images	mechanism
We use deep reinforcement learning ( RL ) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward	mechanism
We demonstrate two experimental results First	method
as a 'sanity check ' demonstration of pure RL ( from scratch )	method
Second we conduct large-scale real-image experiments on the VisDial dataset	method
where we pretrain with supervised dialog data	method
Chest X-ray ( CXR ) is one of the most commonly prescribed medical imaging procedures	background
often with over 2-10x more scans than other imaging modalities such as MRI	background
CT scan and PET scans	background
These voluminous CXR scans place significant workloads on radiologists and medical practitioners	background
surpassing the current state-of-the-art	background
show that our method produces highly accurate and natural segmentation our model reaches human-level performance without relying on any existing trained model or dataset	finding
Our method also generalizes well to CXR images from a different patient population and disease profiles	finding
In this work	mechanism
we propose Structure Correcting Adversarial Network ( SCAN ) SCAN incorporates a critic network to impose on the convolutional segmentation network the structural regularities emerging from human physiology	mechanism
During training the critic network learns to discriminate between the ground truth organ annotations from the masks synthesized by the segmentation network Through this adversarial process the critic network learns the higher order structures and guides the segmentation model to achieve realistic segmentation outcomes	mechanism
Extensive experiments Using only very limited training data available	method
The disclosure describes a sensor system	mechanism
and embodies both crowd sourcing and machine learning together Further	mechanism
a sporadic crowd assessment is used to ensure continued sensor accuracy when the system is relying on machine learning analysis This sensor approach requires minimal and non-permanent sensor installation by utilizing any device with a camera as a sensor host	mechanism
and provides human- centered and actionable sensor output	mechanism
Modeling the long-term facial aging process is extremely challenging due to the presence of large and non-linear variations during the face development stages	background
to further show the advantages of our proposed approach	finding
this work first decomposes the aging process into multiple short-term stages	mechanism
Then a novel generative probabilistic model	mechanism
named Temporal Non-Volume Preserving ( TNVP ) transformation	mechanism
is presented Unlike Generative Adversarial Networks ( GANs )	mechanism
which requires an empirical balance threshold	mechanism
and Restricted Boltzmann Machines ( RBM )	mechanism
an intractable model	mechanism
our proposed TNVP approach guarantees a tractable density function	mechanism
exact inference and evaluation for embedding the feature transformations between faces in consecutive stages	mechanism
Our model shows its advantages not only in capturing the non-linear age related variance in each stage but also producing a smooth synthesis in age progression across faces	mechanism
Our approach can model any face in the wild provided with only four basic landmark points	mechanism
Moreover the structure can be transformed into a deep convolutional network while keeping the advantages of probabilistic models with tractable log-likelihood density estimation	mechanism
Our method is evaluated in both terms of synthesizing age-progressed faces and cross-age face verification and consistently shows the state-of-the-art results in various face aging databases	method
i	method
e	method
FG-NET MORPH AginG Faces in the Wild ( AGFW )	method
and Cross-Age Celebrity Dataset ( CACD ) A large-scale face verification on Megaface challenge 1 is also performed	method
Analyzing videos of human actions involves understanding the temporal relationships among video frames	background
CNNs are the current state-of-the-art methods for action recognition in videos	background
show that it achieves competitive accuracy with the two-stage approaches	finding
In this paper	mechanism
we present a novel CNN architecture that implicitly captures motion information	mechanism
Our method is 10x faster than a two-stage approach	mechanism
does not need to cache flow information	mechanism
and is end-to-end trainable	mechanism
Experimental results on UCF101 and HMDB51	method
This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract	background
likely at the levels of parts	background
and independent of the specific objects or categories experienced during training	background
suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks	background
Interestingly the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet	finding
We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network ( DCNN ) that learns to associate different views of each 3D object The re-training process effectively performs distance metric learning under the object persistency constraints	mechanism
to modify the view-manifold of object representations	mechanism
It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects	mechanism
resulting in the untangling of the view-manifolds between individual objects within the same category and across categories	mechanism
This untangling enables the model to discriminate and recognize objects within the same category	mechanism
independent of viewpoints	mechanism
We found that this ability is not limited to the trained objects	mechanism
but transfers to novel objects in both trained and untrained categories	mechanism
as well as to a variety of completely novel artificial synthetic objects	mechanism
Given a collection of seasonal time-series	background
how can we find regular ( cyclic ) patterns and outliers ( i	background
e	background
rare events ) ? These two types of patterns are hidden and mixed in the time-varying activities	background
demonstrate the benefits of the proposed model and algorithm	finding
in that the model can capture latent cyclic patterns	finding
trends and rare events	finding
and the algorithm outperforms the existing state-of-the-art approaches	finding
CycloneFact was up to 5 times more accurate and 20 times faster than top competitors	finding
We present CycloneM	mechanism
a unifying model and CycloneFact	mechanism
a novel algorithm We also present an automatic mining framework AutoCyclone	mechanism
based on CycloneM and CycloneFact Our method has the following properties ; ( a ) effective : it captures important cyclic features such as trend and seasonality	mechanism
and distinguishes regular patterns and rare events clearly ; ( b ) robust and accurate : it detects the above features and patterns accurately against outliers ; ( c ) fast : CycloneFact takes linear time in the data size and typically converges in a few iterations ; ( d ) parameter free : our modeling framework frees the user from having to provide parameter values	mechanism
Extensive experiments on 4 real datasets	method
The recently developed variational autoencoders ( VAEs ) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods	background
Our method is able to discover highly interpretable activity hierarchies	finding
and obtain improved clustering accuracy and generalization capacity based on the learned rich representations	finding
In this work	mechanism
we propose hierarchical nonparametric variational autoencoders	mechanism
which combines tree-structured Bayesian nonparametric priors with VAEs	mechanism
Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference	mechanism
The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus	mechanism
and infers accurate representations of data instances	mechanism
We apply our model in video representation learning	method
Many problems in image processing and computer vision ( e	background
g	background
colorization style transfer ) can be posed as 'manipulating ' an input image into a corresponding output image given a user-specified guiding signal	background
A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals ( even signals unseen during training )	background
such as diverse paintings and arbitrary descriptive attributes	background
show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals ( e	finding
g	finding
style images and attributes ) in real-time ( tens of milliseconds per image ) even for unseen signals	finding
We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal ( even unseen ones ) To this end	mechanism
we propose the Zero-shot Manipulation Net ( ZM-Net )	mechanism
a fully-differentiable architecture that jointly optimizes an image-transformation network ( TNet ) and a parameter network ( PNet ) The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself Moreover	mechanism
a large-scale style dataset with over 20	mechanism
000 style images is also constructed	mechanism
Extensive experiments	method
Reading tracing and explaining the behavior of code are strongly correlated with the ability to write code effectively	background
Kodu reasoning problems appear to be a promising tool for assessing computational thinking in young programmers	background
Explicitly teaching semantics proved helpful with one type of misconception but not with others We found different styles of student reasoning ( analytical and analogical ) that may correspond to distinct neo-Piagetian stages of development as described by Teague and Lister ( 2014 )	finding
we introduced two groups of third graders to Microsoft 's Kodu Game Lab ; the second group was also given four semantic `` Laws of Kodu '' to better scaffold their reasoning and discourage some common misconceptions During each session	method
students were asked to predict the behavior of short Kodu programs	method
We find that the collection and use of consumer data for targeting purposes affect consumer welfare through three distinct	finding
and possibly countervailing	finding
effects : match improvement	finding
offer discrimination and supply expansion	finding
Furthermore we find that the economic interests of the three agents can be misaligned	finding
depending on the degree of heterogeneity in consumer preferences	finding
Finally we find that a strategic intermediary may choose to share with advertising firms only a subset of consumer data	finding
maximizing its profits at their cost	finding
overlooking the other agents interests	finding
regulation of data collection and sharing may increase consumers welfare	finding
In situations where the intermediary has an incentive to reveal the information that maximizes its payoff	method
When tasked to find fraudulent social network users	background
what is a practitioner to do ?	background
We report the signs of such behaviors	finding
including oddities in local network connectivity	finding
account attributes and similarities and differences across fraud providers We discover several types of fraud behaviors	finding
with the possibility of even more	finding
which give exceptionally strong ( > 0	finding
95 precision/recall ) discriminative power on ground-truth data	finding
and which reduces misclassification rate by > 18 % over baselines and routes practitioner attention to samples at high-risk of misclassification	finding
and building algorithms First	mechanism
we set up honeypots	mechanism
or `` dummy '' social network accounts on which we solicit fake followers ( after careful IRB approval )	mechanism
We discuss how to leverage these insights in practice	mechanism
build strongly performing entropy-based features	mechanism
and propose OEC ( Open-ended Classification )	mechanism
an approach for `` future-proofing '' existing algorithms to account for the complexities of link fraud	mechanism
Our contributions are	mechanism
( b ) features : we engineer features ( c ) algorithm : we motivate and discuss OEC	mechanism
by analyzing fraudulent behavioral patterns	method
featurizing users to yield strong discriminative performance	method
( a ) observations : we analyze our honeypot fraudster ecosystem and give insights regarding various fraud behaviors	method
Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled countless new applications in the domain of aerial sensing	background
In scenarios such as intrusion detection	background
target tracking and facility monitoring it is important to reach a given area of interest ( AOI )	background
and create an online data streaming connection to a monitoring ground station ( GS ) for immediate delivery of content to the operator	background
In previous work	background
we showed that a multi-hop line network can increase the range of the mission by finding the optimal number of relay UAVs	background
and their optimal placement	background
We will also discuss how changing slot width online can overcome typical and less known TDMA in-efficiencies	mechanism
and therefore reach maximum end-to-end throughput and low delay	mechanism
Most cameras are equipped with an auto-contrast feature that enables them to take high quality pictures in a wide range of lighting conditions	background
Auto-contrast works by increasing the sensitivity of the camera to light in dimly lit surroundings	background
but reducing it in bright conditions to ensure that images do not become saturated	background
Our visual system is equipped with a similar feature	background
Neurons in the visual system increase or decrease their sensitivity to light as appropriate to enable us to see in both dimly lit rooms and dazzling sunshine	background
This process which is known as dynamic range adaptation	background
also occurs in neurons that are sensitive to sound or touch	background
This makes sense because in a 3D task	background
which also features depth	background
the neurons have a greater range of possible movement directions to encode	background
These results presented by Rasmussen et al	background
raise several additional questions	background
Are the mechanisms that support dynamic range adaptation the same in sensory and motor neurons ? If these neurons also encode other aspects of movement	background
such as speed	background
would these also be included in the same range as direction or is the adaptation process segregated by specific parameter categories ? And how do these changes in sensitivity affect the movements that animals produce	background
showed that neurons became less sensitive to the cursors direction of movement when the task switched from 2D to 3D	finding
Conversely the neurons became more sensitive to the direction of movement when the task switched from 3D to 2D	finding
Under these circumstances the neurons can use activity that was previously dedicated to encoding depth to instead represent the 2D space in finer detail	finding
Rasmussen et al	method
trained two rhesus macaque monkeys to use their brain activity to move a cursor on a virtual reality screen in either 2D or 3D Studying this brain activity	method
we prove that every limit point of the sequence generated by m-PAPG is a critical point of the objective function we prove that the function value decays linearly for every $ s $ steps ; we prove that the sequences generated by m-PAPG converge to the same critical point	finding
provided that a proximal Lipschitz condition is satisfied	finding
In this work we propose m-PAPG	mechanism
an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol The worker machines communicate asynchronously with a controlled staleness bound $ s $ and operate at different frequencies We characterize various convergence properties of m-PAPG :	mechanism
1 ) Under a general non-smooth and non-convex setting	method
; 2 ) Under an error bound condition	method
3 ) Under the Kurdyka- $ { \L } $ ojasiewicz inequality	method
Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation	background
One of the main challenges for self-driving vehicles on public roads is the safe cooperation and collaboration among multiple vehicles using sensor-based perception and inter-vehicle communications	background
When self-driving vehicles try to occupy the same spatial area simultaneously	background
they might collide with one another	background
might become deadlocked	background
or might slam on the brakes making it uncomfortable or unsafe for passengers in a self-driving vehicle	background
results show that our traffic protocol has higher traffic throughput	finding
compared to simple traffic protocols	finding
while ensuring safety	finding
We present a safe protocol for merge points named Autonomous Vehicle Protocol for Merge Points	mechanism
where self-driving vehicles use both vehicular communications and their own perception systems	mechanism
Our simulation	method
Gaussian belief propagation ( BP ) has been widely used for distributed estimation in large-scale networks such as the smart grid	background
communication networks and social networks	background
where local meansurements/observations are scattered over a wide geographical area	background
that the exchanged message information matrix converges for arbitrary positive semidefinite initial value	finding
and its distance to the unique positive definite limit matrix decreases exponentially fast	finding
focusing in particular on the convergence of the information matrix	mechanism
We show analytically	method
Intelligent personalization systems are becoming increasingly reliant on contextually-relevant devices and services	background
such as those available within modern IoT deployments	background
An IoT context may emerge -- -or become pervasive -- -when the intelligent system generates knowledge from dialogue-based interactions with the end-user ; the context is strengthened even further by incorporating state representations about the environment ( e	background
g	background
generated from wireless sensor data ) into the knowledge graph	background
This is crucial for pervasive applications like digital assistance in IoT	background
where context-aware systems need to adapt quickly : activities like leaving work home-bound	background
driving to the grocery store	background
arriving at home	background
and walking the dog	background
for example can occur in a relatively short period of time -- - during which an intelligent assistant must be able to support user requests in a consistent and coherent manner	background
Given that computational ontologies can serve as semantic models for heterogeneous data	background
they are becoming increasingly viable for reasoning across different IoT contexts	background
This involves : ( a ) federation and dynamic pruning of multiple modular ontologies	background
ideally to comprehensively capture only the knowledge that will facilitate execution of a multi-context task ; ( b ) fast consistency-checking and ontology-based inferences	background
aided by rules-based execution environments that can evaluate/transform ambient wireless sensor network ( WSN ) data	background
in real-time ; and ( c ) run-time execution of ontology-based control procedures	background
through rule-engine actuation commands sent across the WSN	background
Only by realizing these functionalities may intelligent systems be capable of reasoning over device properties	background
system states and user activities	background
while appropriately delegating commands to other intelligent agents or other relevant IoT services	background
Preliminary results are also discussed	finding
In this poster	mechanism
we illustrate how a multi-context knowledge base can be structured on the basis of modular ontologies and integrated with a distributed rules-based inference engine in multiple smart-building environments	mechanism
The approach we describe is also partially based on the Ubiquitous Personal Assistant ( UPA ) project	mechanism
Bosch Research 's largest research initiative worldwide	mechanism
This work is conducted through the partnership of Bosch Research Pittsburgh and Carnegie Mellon University ( CMU )	method
and is in partial satisfaction of CMU 's Bosch Energy Research Network ( BERN ) grant	method
awarded for developments in intelligent building solutions	method
Traditional generative adversarial networks ( GAN ) and many of its variants are trained by minimizing the KL or JS-divergence loss that measures how close the generated data distribution is from the true data distribution A recent advance called the WGAN based on Wasserstein distance can improve on the KL and JS-divergence based GANs	background
and alleviate the gradient vanishing	background
instability and mode collapse issues that are common in the GAN training	background
that the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN	finding
and have seen both visual and quantitative improvement over baseline WGAN	finding
by first generalizing its discriminator loss to a margin-based one	mechanism
which leads to a better discriminator	mechanism
and in turn a better generator	mechanism
and then carrying out a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later stages will improve upon early stages We call this method Gang of GANs ( GoGAN ) We have also proposed a new way of measuring GAN quality which is based on image completion tasks	mechanism
We have shown theoretically We have evaluated our method on four visual datasets : CelebA	method
LSUN Bedroom CIFAR-10	method
and 50K-SSFF	method
The problems of hand detection have been widely addressed in many areas	background
e	background
g	background
human computer interaction environment	background
driver behaviors monitoring	background
etc	background
Our proposed method achieves the state-of-the-art results with 20 % of the detection accuracy higher than the second best one in the VIVA challenge	finding
This paper presents the Multiple Scale Faster Region-based Convolutional Neural Network ( MS-FRCNN ) Our proposed method introduces a multiple scale deep feature extraction approach in order to handle the challenging factors to provide a robust hand detection algorithm	mechanism
The method is evaluated on the challenging hand database	method
i	method
e	method
the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge	method
and compared against various recent hand detection methods	method
While only recently developed	background
the ability to profile expression data in single cells ( scRNA-Seq ) has already led to several important studies and findings	background
Such database queries ( which can be performed using our web server ) will enable researchers to better characterize cells when analyzing heterogeneous scRNA-Seq samples	background
We show that the NN method improves upon prior methods in both	finding
the ability to correctly group cells in experiments not used in the training and the ability to correctly infer cell type or state by querying a database of tens of thousands of single cell profiles	finding
we develop and test a method based on neural networks ( NN ) We tested various NN architectures	mechanism
some biologically motivated	mechanism
and used these to obtain a reduced dimension representation of the single cell expression data	mechanism
High Assurance SPIRAL ( HA-SPIRAL ) is a tool that At the heart of HA-SPIRAL is a mathematical identity rewrite engine based on a computer algebra system	mechanism
The rewrite engine refines the mathematical expression provided by a control engineer	mechanism
through mathematical identities	mechanism
into an equivalent mathematical expression that can be implemented in code	mechanism
The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors	background
demonstrate that S pike M accurately and succinctly describes all patterns of the rise and fall spikes in social networks	finding
In this article	mechanism
we propose S pike M	mechanism
a concise yet flexible analytical model of Our model has the following advantages First	mechanism
unification power : it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models We provide the threshold of the take-off versus die-out conditions for S pike M and discuss the generality of our model by applying it to an arbitrary graph topology Second	mechanism
practicality : it matches the observed behavior of diverse sets of real data Third	mechanism
parsimony : it requires only a handful of parameters	mechanism
Fourth usefulness : it makes it possible to perform analytic tasks such as forecasting	mechanism
spotting anomalies and interpretation by reverse engineering the system parameters of interest ( quality of news	mechanism
number of interested bloggers	mechanism
etc	mechanism
) We also introduce an efficient and effective algorithm namely S pike S tream	mechanism
which identifies multiple diffusion patterns in a large collection of online event streams	mechanism
Extensive experiments on real datasets	method
In face recognition tasks	background
the changing pose of the face can cause enough information to be lost to cause the recognition to fail so being able to determine the pose of the face beforehand can allow for some better recognition performance	background
We show this method can perform pose estimation with a high accuracy of 85	finding
21 % and an accuracy of 98	finding
42 % when allowing a 15 tolerance on the pose estimate on the CUbiC FacePix dataset	finding
with our methods achieving 77	finding
01 % accuracy on yaw estimation	finding
We propose method in which the training data itself is the underlying structure of a classifier	mechanism
This is accomplished through the use of matrix decomposition equations However	mechanism
instead of decomposing a matrix	mechanism
one is created by carefully selecting the terms in the decomposition equation such that the resulting matrix has the desired properties for classification	mechanism
We show two recomposition methods using the Spectral Decomposition and Singular Value Decomposition equations	mechanism
We also show results on both yaw and pitch estimation on the Pointing'04 dataset	method
The mechanism classes we study are significantly different from well-understood function classes typically found in machine learning	background
so bounding their complexity requires a sharp understanding of the interplay between mechanism parameters and buyer valuations	background
We present a single	mechanism
overarching theorem that uses empirical Rademacher complexity	mechanism
including affine maximizer auctions	mechanism
mixed-bundling auctions and second-price item auctions Despite the extensive applicability of our main theorem	mechanism
we match and improve over the best-known generalization guarantees for many auction classes	mechanism
This all-encompassing theorem also applies to multi- and single-item pricing mechanisms in both multi- and single-unit settings	mechanism
such as linear and non-linear pricing mechanisms	mechanism
Finally our central theorem allows us to easily derive generalization guarantees for every class in several finely grained hierarchies of auction and pricing mechanism classes	mechanism
Instead the mechanism designer receives a set of samples from this distribution and his goal is to use the sample to design a pricing mechanism or auction with high expected profit	method
We provide generalization guarantees which bound the difference between average profit on the sample and expected profit over the distribution	method
These bounds are directly proportional to the intrinsic complexity of the mechanism class the designer is optimizing over	method
Rapid improvements in the precision of mobile technologies make it possible for advertisers to go beyond using the real-time static location and contextual information about consumers Our finding suggests that highly targeted mobile promotions can have the inadvertent impact of reducing impulse purchase behavior by customers who are in an exploratory shopping stage	background
On a broader note	background
our work can be viewed as a first step towards studying the large-scale	background
fine-grained digital trace of individual physical behavior	background
and how it can be used to predict and market to individual anticipated future behavior	background
We find that trajectory-based mobile targeting can lead to higher redemption probability	finding
faster redemption behavior	finding
and higher transaction amount from customers compared to other baselines	finding
It also facilitates higher revenues for the focal store as well as the overall shopping mall	finding
Moreover the effect of trajectory-based targeting comes not only from improvements in the efficiency of customers current shopping process	finding
but also from its ability to nudge customers towards changing their future shopping patterns and generate additional revenues	finding
Finally we find significant heterogeneity in the impact of trajectory-based targeting	finding
It is especially effective in influencing high-income consumers Interestingly	finding
it becomes less effective in boosting the revenues of the shopping mall during the weekends and for those shoppers who like to explore across products categories	finding
In this study	mechanism
we propose a novel trajectory-based targeting strategy that leverages full information on consumers physical movement trajectories using granular behavioral information from different mobility dimensions	mechanism
To analyze the effectiveness of this new strategy	method
we design a large-scale randomized field experiment in a large shopping mall that involved 83	method
370 unique user responses for a 14-day period in June 2014	method
Common appliances have shifted toward flat interface panels	background
making them inaccessible to blind people	background
Although blind people can label appliances with Braille stickers	background
doing so generally requires sighted assistance to identify the original functions and apply the labels	background
We demonstrate the viability of Facade	finding
We introduce Facade - a crowdsourced fabrication pipeline by adding a 3D printed augmentation of tactile buttons overlaying the original panel Facade users capture a photo of the appliance with a readily available fiducial marker ( a dollar bill ) for recovering size information This image is sent to multiple crowd workers	mechanism
who work in parallel to quickly label and describe elements of the interface	mechanism
Facade then generates a 3D model for a layer of tactile and pressable buttons that fits over the original controls Finally	mechanism
a home 3D printer or commercial service fabricates the layer	mechanism
which is then aligned and attached to the interface by the blind person	mechanism
in a study with 11 blind participants	method
The same techniques can be applied to monitor other types of traffic data	background
We are able to approximately recover the taxi-pick activities in Manhattan by sampling at only 5 selected intersections	finding
This paper proposes a series of sampling	mechanism
recovery and representation techniques based on graph signal processing	mechanism
We validate our proposed techniques on Manhattan 's taxi pickups during the years of 2014 and 2015	method
Our method effectively transfers discriminability of connectives to the implicit features	finding
and achieves state-of-the-art performance on the PDTB benchmark	finding
We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives	mechanism
and thus encouraged We develop an adversarial model through competition between the implicit network and a rival feature discriminator	mechanism
Hearing-impaired people and non-native speakers rely on captions for access to video content Based on our results	background
we outline opportunities for future research and provide design suggestions to deliver cost-efficient captioning solutions	background
Our findings show that BandCaption enables crowd workers who have different needs and strengths to accomplish micro-tasks and make complementary contributions	finding
In this paper	mechanism
we present the design	mechanism
implementation and evaluation of BandCaption	mechanism
a system that combines automatic speech recognition with input from crowd workers Each group has different abilities and incentives	mechanism
which our workflow leverages	mechanism
We consider four stakeholder groups as our source of crowd workers : ( i ) individuals with hearing impairments	method
( ii ) second-language speakers with low proficiency	method
( iii ) second-language speakers with high proficiency	method
and ( iv ) native speakers	method
The promise of smart environments and the Internet of Things ( IoT ) relies on robust sensing of diverse environmental facets	background
the results of which show the versatility	finding
accuracy and potential utility of our approach	finding
Further through what we call Synthetic Sensors	mechanism
we can virtualize raw sensor data into actionable feeds	mechanism
whilst simultaneously mitigating immediate privacy issues	mechanism
A series of structured	method
formative studies informed the development of our new sensor hardware and accompanying information architecture	method
We deployed our system across many months and environments	method
Introduction Drug overdoses are an increasingly serious problem in the United States and worldwide	background
The CDC estimates that 47	background
055 drug overdose deaths occurred in the United States in 2014	background
61 % of which involved opioids ( including heroin	background
pain relievers such as oxycodone	background
and synthetics )	background
1 Overdose deaths involving opioids increased 3-fold from 2000 to 2014	background
1 These statistics motivate public health to identify emerging trends in overdoses	background
including geographic demographic	background
and behavioral patterns ( e	background
g	background
which combinations of drugs are involved )	background
Early detection can inform prevention and response efforts	background
as well as quantifying the effects of drug legislation and other policy changes	background
The fast subset scan 2 detects significant spatial patterns of disease by efficiently maximizing a log-likelihood ratio statistic over subsets of data points	background
and has recently been extended to multidimensional data ( MD-Scan )	background
Conclusions Retrospective analysis of Allegheny County overdose data suggests high potential utility for a prospective overdose surveillance system	background
which would enable public health users to identify emerging patterns of overdoses in their early stages and facilitate targeted and effective health interventions	background
The MDTS approach can also be used for other multidimensional public health surveillance tasks	background
such as STI surveillance	background
where the patterns or outbreaks of interest may have demographic	background
geographic and behavioral components	background
and demonstrate the utility of this approach for discovering emerging geographic	finding
demographic and behavioral trends in fatal drug overdoses	finding
Results The highest-scoring clusters discovered by MDTS were shared with Allegheny Countys Dept	finding
of Human Services and their feedback obtained	finding
One set of potentially relevant findings from our analysis involved fentanyl	finding
a dangerous and potent opioid which has been a serious problem in western PA	finding
In addition to identifying two well- known	finding
large clusters of overdoses14 deaths in January 2014 and 26 deaths in March-April 2015MDTS was able to provide additional information about each cluster	finding
For example the first cluster was likely due to fentanyl-laced heroin	finding
while the second was more likely due to fentanyl disguised as heroin ( only 11 victims had heroin in their system )	finding
Moreover the second cluster was initially confined to the Pittsburgh suburb of McKeesport and a typical demographic ( white males ages 20-49 )	finding
before spreading across the county	finding
Our analysis demonstrated that prospective surveillance using MDTS would have identified the cluster as early as March 29th	finding
enabling targeted prevention efforts	finding
MDTS also discovered a previously unidentified	finding
highly localized cluster of fentanyl-related overdoses affecting an unusual and underserved demographic ( elderly black males near downtown Pittsburgh )	finding
This cluster occurred in January- February 2015	finding
and may have been related to the larger cluster of fentanyl-related overdoses that occurred two months later Finally	finding
we identified multiple overdose clusters involving combinations of methadone and Xanax between 2008 and 2012	finding
and observed dramatic reductions in these clusters corresponding to the passage of the Methadone Death and Incident Review Act ( October 2012 )	finding
which increased state oversight of methadone clinics and prescribing physicians	finding
We present the multidimensional tensor scan ( MDTS )	mechanism
a new method Methods The multidimensional tensor scan ( MDTS ) is a new approach In addition to detecting the spatial area ( subset of locations ) and time window affected by an emerging outbreak	mechanism
MDTS can also identify the affected subset of values for each observed attribute	mechanism
For example given the drug overdose surveillance data described below	mechanism
MDTS can identify the affected genders	mechanism
races age ranges	mechanism
and which drugs were involved	mechanism
MDTS finds subsets of the attribute space with higher than expected case counts	mechanism
first using a novel tensor decomposition approach to estimate the expected counts MDTS then iteratively applies a conditional optimization step	mechanism
optimizing over all subsets of values for each attribute conditional on the current subsets of values for all other attributes 3	mechanism
and using the linear-time subset scanning property 2 to make each conditional optimization step computationally efficient The resulting approach has high power to detect and characterize emerging trends which may only affect a subset of the monitored population ( e	mechanism
g	mechanism
specific ages genders	mechanism
neighborhoods or users of particular combinations of drugs	mechanism
We used MDTS to analyze publicly available data from the Allegheny County	method
PA medical examiners office and to detect emerging overdose patterns and trends The dataset consists of ~2000 fatal accidental drug overdoses between 2008 and 2015	method
For each overdose victim	method
we have date	method
location ( zip code )	method
age decile gender race	method
and the presence/absence of 27 commonly abused drugs in their system	method
Homes offices and many other environments will be increasingly saturated with connected	background
computational appliances forming the `` Internet of Things '' ( IoT )	background
At present most of these devices rely on mechanical inputs	background
webpages or smartphone apps for control	background
suggests high accuracy 98	finding
8 % recognition accuracy among 17 appliances	finding
We propose an approach where users simply tap a smartphone to an appliance To achieve this	mechanism
our prototype smartphone recognizes physical contact with uninstrumented appliances	mechanism
and summons appliance-specific interfaces	mechanism
Our user study Finally	method
to underscore the immediate feasibility and utility of our system	method
we built twelve example applications	method
including six fully functional end-to-end demonstrations	method
Small local groups who share protected resources ( e	background
g	background
families work teams	background
student organizations ) have unmet authentication needs	background
Our results suggest that ( 1 ) individuals who enter the same shared thumprint are distinguishable from one another	finding
( 2 ) that people can enter thumprints consistently over time	finding
and ( 3 ) that thumprints are resilient to casual adversaries	finding
we designed Thumprint : inclusive group authentication with a shared secret knock	mechanism
All group members share one secret knock	mechanism
but individual expressions of the secret are discernible	mechanism
We evaluated the usability and security of our concept through two user studies with 30 participants	method
Infrastructure monitoring applications currently lack a cost-effective and reliable solution for supporting the last communication hop for low-power devices	background
The use of cellular infrastructure requires contracts and complex radios that are often too power hungry and cost prohibitive for sensing applications that require just a few bits of data each day	background
New low-power sub-GHz	background
long-range radios are an ideal technology to help fill this communication void by providing access points that are able to cover multiple kilometers of urban space with thousands of end-point devices	background
These new Low-Power Wide-Area Networking ( LPWAN ) platforms provide a cost-effective and highly deployable option that could piggyback off of existing public and private wireless networks ( WiFi	background
Cellular etc )	background
In this paper	mechanism
we present OpenChirp	mechanism
a prototype end-to-end LPWAN architecture built using LoRa Wide-Area Network ( LoRaWAN ) We present a software architecture that exposes an application layer We define a service model on top of LoRaWAN that acts as a session layer At the device-level	mechanism
we introduce and benchmark an open-source hardware platform that uses Bluetooth Low-Energy ( BLE )	mechanism
We evaluate the system in terms of end-node energy consumption	method
radio penetration into buildings as well as coverage provided by a network currently deployed at Carnegie Mellon University	method
whereas previous models	background
assuming no exogenous infection	background
showed dependency only on the infection and healing rates	background
We show that the sufficient condition for infection to become extinct not only depends on the ratio of infection and healing rates but also on N	finding
the size of the network	finding
We relate the time-limiting behavior of a network epidemics process to the spectral radius of the underlying network Our analysis differs from previous work in that the scaled SIS process accounts for the possibility that a healthy individual has a nonzero probability of becoming infected even when all of its neighbors are healthy	mechanism
For example the source of infection may be outside a human only contact network for diseases with animal to human transmissions such as Ebola	mechanism
The proliferation of mobile and sensor technologies has contributed to the rise of location-based mobile targeting	background
Beyond the location	background
time and spatial context of individuals	background
the social context wherein they are embedded can reveal rich information about their behavior	background
Such real-time social dynamics can help mobile advertisers to more fully understand consumer contextual preferences and	background
thereby provide better digital experiences Overall	background
our study demonstrates the potential of inferring individuals social contexts in real time from their movement trajectories as well as the value of leveraging such real-time social dynamics for improved mobile-targeting effectiveness	background
Our analyses indicated significant heterogeneity in consumer behavior under different real-time social contexts	finding
We found for example	finding
that a customer in a group with others is on average 1	finding
97 times more responsive to mobile promotions than is a solo shopper	finding
and that this impact increases with increased group size ( from dyad to triad )	finding
Interestingly we also found that couples seemed to have an attention deficit with respect to mobile promotions and were the least responsive compared with the other social groups	finding
Meanwhile high-income customers and male customers were more likely to respond to mobile promotions when shopping alone than when shopping with social groups	finding
Our analyses also revealed significant heterogeneity in the interaction effect between mobile promotion design and real-time social contexts	finding
we automatically detected based on their detailed GPS trajectories using state-of-the-art machine-learning methods	mechanism
To evaluate the effectiveness of mobile targeting under different social contexts	method
we designed a randomized field experiment for a large shopping mall in Asia based on 52	method
500 unique user responses for 252 stores over the course of a 21-day period in April 2015	method
We show that PSNR=29	finding
90 dB is recovered for graph signals reconstructed from 70 % of the graph frequency components We illustrate that graph frequency components reveal taxi behaviors that are not obvious from the raw signal	finding
We apply graph signal processing based on 20102013 New York City taxi data	mechanism
Such analysis requires a signal extraction method that involves computing shortest paths between the start and end locations for each of the 700 million trip records	mechanism
We perform spectral analysis on these graph signals	method
for which it is necessary to address the challenge of finding the eigendecomposition of the 6K-node directed Manhattan road network	method
Current touch input technologies are best suited for small and flat applications	background
such as smartphones	background
tablets and kiosks	background
we show that Electrick can enable new interactive opportunities on a diverse set of objects and surfaces that were previously static	finding
We introduce Electrick	mechanism
a low-cost and versatile sensing technique This is achieved by using electric field tomography in concert with an electrically conductive material	mechanism
which can be easily and cheaply added to objects and surfaces We show that our technique is compatible with commonplace manufacturing methods	mechanism
such as spray/brush coating	mechanism
vacuum forming and casting/molding enabling a wide range of possible uses and outputs Our technique can also bring touch interactivity to rapidly fabricated objects	mechanism
including those that are laser cut or 3D printed	mechanism
Through a series of studies and illustrative example uses	method
Blind people often need to identify objects around them	background
from packages of food to items of clothing	background
Automatic object recognition continues to provide limited assistance in such tasks because models tend to be trained on images taken by sighted people with different background clutter	background
scale viewpoints occlusion	background
and image quality than in photos taken by blind users	background
demonstrate the feasibility of our approach	finding
which reaches accuracies over 90 % for some participants	finding
We adopt transfer learning with a deep learning system for user-defined multi-label k-instance classification	mechanism
Experiments with blind participants We analyze user data and feedback to explore effects of sample size	method
photo-quality variance and object shape ; and contrast models trained on photos by blind participants to those by sighted participants and generic recognizers	method
Current tools for screening dyslexia use linguistic elements	background
since most dyslexia manifestations are related to difficulties in reading and writing	background
In this paper	mechanism
we propose a method and present DysMusic	mechanism
a prototype The advantages of DysMusic are that the approach is language independent and could be used with younger children	mechanism
i	mechanism
e	mechanism
pre-readers	mechanism
The prototype was designed with the help of five children and five parents who tested the game using the think aloud protocol and being observed while playing	method
In this paper	mechanism
we present an end-to-end zero-slack rate-monotonic scheme ( ZSRM ) based on real-time pipelines	mechanism
called the ZSRM pipeline scheduler	mechanism
Under ZSRM each task is associated with a parameter called zero-slack instant	mechanism
and whenever a higher-criticality job has not finished at its zero-slack instant relative to its arrival time	mechanism
all jobs of lower criticality are suspended to meet the deadline of the higher-criticality job	mechanism
We develop a new schedulability test and algorithm	mechanism
Information cascades are ubiquitous in both physical society and online social media	background
taking on large variations in structures	background
dynamics and semantics potentially providing insights into intrinsic mechanisms governing information spreading in nature and new models to forecast as well as to impose good control over information cascades in real applications	background
We find that the structural complexity of information cascades is far beyond the previous conjectures	finding
finding some brand new structure patterns of information cascades	finding
In this paper	mechanism
we explore a large-scale dataset including 432 million information cascades with explicit records of spreading traces We first propose seven-dimensional metrics	mechanism
which reflect size and spreading orientation aspects	mechanism
Further we analyze the correlations of these metrics	method
This paper continues the program initiated in [ 5 ]	background
The applicability of the method is demonstrated	finding
towards a derivation system The general idea is that complex protocols can be formally derived	mechanism
starting from basic security components	mechanism
using a sequence of refinements and transformations	mechanism
just like logical proofs are derived starting from axioms	mechanism
using proof rules and transformations	mechanism
The claim is that in practice	mechanism
many protocols are already derived in such a way	mechanism
but informally Capturing this practice in a suitable formalism turns out to be a considerable task	mechanism
The present paper proposes rules In general	mechanism
security protocols are	mechanism
of course not compositional : information revealed by one may interfere with the security of the other	mechanism
However annotating protocol steps by pre- and post-conditions	mechanism
allows secure sequential composition	mechanism
Establishing that protocol components satisfy each other 's invariants allows more general forms of composition	mechanism
ensuring that the individually secure sub-protocols will not interact insecurely in the composite protocol	mechanism
on modular derivations of two standard protocols	method
together with their simple security properties	method
Feature extraction and encoding represent two of the most crucial steps in an action recognition system	background
This work proposes a new approach that allows us to obtain real-time frame rate processing for an action recognition system	mechanism
The motion information represents an important source of information within the video	mechanism
The common approach to extract the motion information is to compute the optical flow	mechanism
However the estimation of optical flow is very demanding in terms of computational cost	mechanism
in many cases being the most significant processing step within the overall pipeline of the target video analysis application	mechanism
In this work we propose an efficient approach to capture the motion information within the video	mechanism
Our proposed descriptor	mechanism
Histograms of Motion Gradients ( HMG )	mechanism
is based on a simple temporal and spatial derivation	mechanism
which captures the changes between two consecutive frames For the encoding step a widely adopted method is the Vector of Locally Aggregated Descriptors ( VLAD )	mechanism
which is an efficient encoding method	mechanism
however it considers only the difference between local descriptors and their centroids	mechanism
In this work we propose Shape Difference VLAD ( SD-VLAD )	mechanism
an encoding method which brings complementary information by using the shape information within the encoding process	mechanism
and we propose also a real-time framework for action recognition	mechanism
We validated our proposed pipeline for action recognition on three challenging datasets UCF50	method
UCF101 and HMDB51	method
As online fraudsters invest more resources	background
including purchasing large pools of fake user accounts and dedicated IPs	background
fraudulent attacks become less obvious and their detection becomes increasingly challenging	background
showed that HoloScope achieved significant accuracy improvements on synthetic and real data	finding
compared with state-of-the-art fraud detection methods	finding
Hence we propose HoloScope	mechanism
which uses information from graph topology and temporal spikes In terms of graph topology	mechanism
we introduce `` contrast suspiciousness	mechanism
'' a dynamic weighting approach	mechanism
which allows us to more accurately detect fraudulent blocks	mechanism
particularly low-density blocks	mechanism
In terms of temporal spikes	mechanism
HoloScope takes into account the sudden bursts and drops of fraudsters ' attacking patterns In addition	mechanism
we provide theoretical bounds for how much this increases the time cost needed for fraudsters to conduct adversarial attacks Additionally	mechanism
from the perspective of ratings	mechanism
HoloScope incorporates the deviation of rating scores in order to catch fraudsters more accurately	mechanism
Moreover HoloScope has a concise framework and sub-quadratic time complexity	mechanism
making the algorithm reproducible and scalable	mechanism
Extensive experiments	method
In comparison-shopping services ( CSS )	background
there exist frauds who perform excessive clicks on a target item in order to boost the popularity of it	background
propose three anomaly scores designed based on click behaviors of users in CSS	mechanism
As one of the featured initiatives in smart grids	background
demand response is enabling active participation of electricity consumers in the supply/demand balancing process	background
thereby enhancing the power systems operational flexibility in a costeffective way	background
Industrial load plays an important role in demand response because of its intense power consumption	background
already existing advanced monitoring and control infrastructure	background
and its strong economic incentive due to the high energy costs	background
As typical industrial loads	background
cement plants are able to quickly adjust their power consumption rate by switching on/off the crushers	background
by proposing methods that enable these loads to provide regulation or load following with the support of an on-site energy storage system	mechanism
Large graph datasets have caused renewed interest for graph partitioning	background
Towards this we introduce the idea of skew-resistant graph partitioning	mechanism
where Skewresistant graph partitioning tries to mitigate skewness by taking the characteristics of both the target workload and the graph structure into consideration	mechanism
our evaluation shows that these algorithms are able to detect proxy use instances that would be difficult to find using existing techniques	finding
and subsequently remove them while maintaining acceptable classification performance	finding
This paper presents an approach In contrast to prior work	mechanism
we focus on use restrictions on proxies ( i	mechanism
e	mechanism
strong predictors ) of protected information types Our definition relates proxy use to intermediate computations that occur in a program	mechanism
and identify two essential properties that characterize this behavior : 1 ) its result is strongly associated with the protected information type in question	mechanism
and 2 ) it is likely to causally affect the final output of the program	mechanism
For a specific instantiation of this definition	mechanism
we present a program analysis technique that detects instances of proxy use in a model	mechanism
and provides a witness that identifies which parts of the corresponding program exhibit the behavior Recognizing that not all instances of proxy use of a protected information type are inappropriate	mechanism
we make use of a normative judgment oracle that makes this inappropriateness determination for a given witness	mechanism
Our repair algorithm uses the witness of an inappropriate proxy use to transform the model into one that provably does not exhibit proxy use	mechanism
while avoiding changes that unduly affect classification accuracy	mechanism
Using a corpus of social datasets	method
Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community	background
results demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data	finding
In addition the results also verify that WELL-MM is robust to the level of noisiness in the video data	finding
Notably WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data	finding
We propose a novel method called Multi-modal WEbly-Labeled Learning ( WELL-MM )	mechanism
which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human	mechanism
WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos	mechanism
We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images The comprehensive experimental on FCVID and YFCC100M	method
Summary Cryo-electron tomography ( cryo-ET ) captures the 3Delectron density distribution of macromolecular complexes in close to native state	background
With the rapid advance of cryo-ET acquisition technologies	background
it is possible to generate large numbers ( > 100	background
000 ) of subtomograms	background
each containing a macromolecular complex	background
Often these subtomograms represent a heterogeneous sample due to variations in the structure and composition of a complex insitu form or because particles are a mixture of different complexes	background
In this case subtomograms must be classified	background
This paper introduces an open source software platform	mechanism
TomoMiner Its scalable and robust parallel processing allows efficient classification of tens to hundreds of thousands of subtomograms	mechanism
In addition TomoMiner provides a pre-configured TomoMinerCloud computing service permitting users without sufficient computing resources instant access to TomoMiners high-performance features	mechanism
The Kinect sensing devices have been widely used in current Human-Computer Interaction entertainment	background
The results show that our method has its advantage for motion detection in a real-time Kinect entertaining environment	finding
In this paper	mechanism
we tackle it by proposing a linear algorithm	mechanism
which is augmented by feature interaction	mechanism
The linear property guarantees its speed whereas feature interaction captures the higher order effect from the data to enhance its accuracy The Schatten-p norm is leveraged to integrate the main linear effect and the higher order nonlinear effect by mining the correlation between them	mechanism
The resulted classification model is a desirable combination of speed and accuracy We propose a novel solution to solve our objective function	mechanism
Experiments are performed on three public Kinect-based entertainment data sets related to fitness and gaming	method
Different from general image localization task through matching	background
the appearance of an environment during significant events varies greatly from its daily appearance	background
since there are usually crowds	background
decorations or even destruction when a major event happens	background
Experimental results show that our solution significantly improves over matching on whole images and the automatically learned saliency is a strong predictor of distinctive building areas	finding
Based on this observation	mechanism
we formulate the problem as joint saliency estimation and matching at the image region level	mechanism
as opposed to the key point or whole-image level	mechanism
As image-level labels of daily environment are easily generated with GPS information	mechanism
we treat region based saliency estimation and matching as a weakly labeled learning problem over the training data	mechanism
Our solution is to iteratively optimize saliency and the region-matching model	mechanism
For saliency optimization	mechanism
we derive a closed form solution	mechanism
which has an intuitive explanation	mechanism
For region matching model optimization	mechanism
we use self-paced learning to learn from the pseudo labels generated by ( sub-optimal ) saliency values	mechanism
We conduct extensive experiments on two challenging public datasets : Boston Marathon 2013 and Tokyo Time Machine	method
Utility maximization under a budget constraint is a classical problem in economics and management science	background
It is commonly assumed that the utility is a `` nice '' known analytic function	background
for example continuous and concave	background
In many domains	background
such as marketing	background
increased availability of computational resources and data has enabled the development of sophisticated simulations to evaluate the impact of allocating a fixed budget among alternatives ( e	background
g	background
marketing channels ) on outcomes	background
such as demand	background
While simulations enable high resolution evaluation of alternative budget allocation strategies	background
they significantly complicate the associated budget optimization problem	background
demonstrates the effectiveness of our approach	finding
by first converting the problem into a multi-choice knapsack optimization problem with unknown weights We show that if weights ( corresponding to marginal impact thresholds for each channel ) are well approximated	mechanism
we can achieve a solution within a factor of 2 of optimal	mechanism
and this bound is tight	mechanism
We then develop several parsimonious query algorithms	mechanism
Experimental evaluation	method
Deep generative models have achieved impressive success in recent years	background
show generality and effectiveness of the imported extensions	finding
through a new formulation of GANs and VAEs	mechanism
We show that GANs and VAEs are essentially minimizing KL divergences with opposite directions and reversed latent/visible treatments	mechanism
extending the two learning phases of classic wake-sleep algorithm	mechanism
respectively	mechanism
The unified view provides a powerful tool to analyze a diverse set of existing model variants	mechanism
and enables to exchange ideas across research lines in a principled way For example	mechanism
we transfer the importance weighting method in VAE literatures for improved GAN learning	mechanism
and enhance VAEs with an adversarial mechanism	mechanism
Quantitative experiments	method
In total this work substantially expands the scope and scale of problems that can be solved using semidefinite programming methods	background
We show that for certain problems	finding
the method is strictly decreasing and guaranteed to converge to a critical point	finding
In all settings	finding
we demonstrate improvement over the existing state of the art along various dimensions	finding
In this paper	mechanism
we propose a coordinate descent approach The approach	mechanism
which we call the Mixing method	mechanism
is extremely simple to implement	mechanism
has no free parameters	mechanism
and typically attains an order of magnitude or better improvement in optimization performance over the current state of the art	mechanism
We then apply the algorithm to three separate domains : solving the maximum cut semidefinite relaxation	method
solving a ( novel ) maximum satisfiability relaxation	method
and solving the GloVe word embedding optimization problem	method
demonstrates that H-FUSE reconstructs the original data 30 81 % better than the least squares method	finding
We propose H-FUSE	mechanism
a novel method that solves above problems by allowing injection of domain knowl- edge in a principled way	mechanism
and turning the task into a well- defined optimization problem H-FUSE has the following desirable properties : ( a ) Effectiveness	mechanism
recovering histori- cal data from aggregated reports with high accuracy ; ( b ) Self-awareness	mechanism
providing an assessment of when the re- covery is not reliable ; ( c ) Scalability	mechanism
computationally lin- ear on the size of the input data	mechanism
Experiments on the real data ( epidemiology counts from the Tycho project [ 13 ]	method
Occupancy estimation is an important primitive for a wide range of applications including building energy efficiency	background
safety and security	background
we observe that FORK achieves over 99 % accuracy in real-time ( 4-9 FPS ) in occupancy estimation	finding
In this work	mechanism
we develop a prototype system called FORK using off-the-shelf components that performs the entire depth data processing on a cheaper and low power ARM processor in real-time	mechanism
As ARM processors are extremely weak in running computer vision algorithms	mechanism
FORK is designed to detect humans and track them in a very efficient way by leveraging a novel lightweight model based approach instead of traditional approaches based on histogram of oriented gradients ( HOG ) features Unlike other camera based approaches	mechanism
FORK is much less privacy invasive ( even if the sensor is compromised )	mechanism
Based on a complete implementation	method
real-world deployment and extensive evaluation at realistic scenarios	method
We present OpenFace	mechanism
our new that approaches state-of-the-art accuracy	mechanism
Integrating OpenFace with inter-frame tracking	mechanism
we build RTFace	mechanism
a mechanism that selectively blurs faces according to specified policies at full frame rates	mechanism
This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions	mechanism
Finally we present a scalable	mechanism
privacy-aware architecture	mechanism
As video cameras proliferate	background
the ability to scalably capture and search their data becomes important	background
In this setting	mechanism
we describe interactive data exploration ( IDE )	mechanism
which refers using predicates that may not have been part of any prior indexing We also describe a new technique called just-in-time indexing ( JITI ) that improves response times in IDE	mechanism
Due to the advent of active safety features and automated driving capabilities	background
the complexity of embedded computing systems within automobiles continues to increase	background
Such advanced driver assistance systems ( ADAS ) are inherently safetycritical and must tolerate failures in any subsystem	background
Recent work has studied the use of software-based faulttolerance techniques that utilize task-level hot and cold standbys to tolerate fail-stop processor and task failures	background
The benefit of using standbys is maximal when a task and any of its standbys obey the placement constraint of not being co-located on the same processor	background
that saves at least one processor up to 40 % of the time relative to the best known heuristic to date	finding
finds that our heuristic uses no more than one additional processor in most cases relative to an optimal allocation that we construct for evaluation purposes using a creative technique	finding
We propose based on a `` tiered '' placement constraint	mechanism
and show that our heuristic produces a better task assignment We then introduce a task allocation algorithm that	mechanism
for the first time to our knowledge	mechanism
leverages the run-time attributes of cold standbys We have designed and implemented our software fault-tolerance framework in AUTOSAR	mechanism
an automotive industry standard	mechanism
Our empirical study We use this implementation to provide an experimental evaluation of our task-level fault-tolerance features	method
Finally we present an analysis of the worst-case behavior of our task recovery features	method
Audience Participation Games challenge traditional assumptions about gameplay by blurring the line between audience and player	background
allowing audience members to impact gameplay in a meaningful way	background
Their recent rise in popularity has created new opportunities for game research and development Our results show the breadth of opportunities and challenges that designers face in creating engaging Audience Participation Games	background
we developed several versions of two prototype games as design probes We livestreamed them to an online audience in order	mechanism
Nanosecond-level clock synchronization is a missing capability for many real-time applications like next-generation wireless systems that leverage spatial multiplexing to improve channel capacity and provide services like time-of-flight localization	background
With finegrained synchronization	background
both clock stability and propagation delays introduce significant sources of error	background
to show a clock synchronization of better than five nanoseconds per hop with an average of 2	finding
12 ns and a standard deviation of 0	finding
84 ns	finding
The platform is able to identify and avoid clock error in cases where there is heavy multi-path or non-Line-of-Sight signals	finding
In this paper	mechanism
we introduce Pulsar	mechanism
a wireless time transfer platform to better than five nanosecond between indoor or GPS-denied devices	mechanism
Pulsar leverages a stable clock source derived from a Chip-Scale Atomic Clock ( CSAC ) along with an Ultra-WideBand ( UWB ) radio able to perform sub-nanosecond packet timestamping to estimate and correct for clock offsets	mechanism
We design and evaluate a proof-ofconcept network-wide synchronization protocol for Pulsar that selects low-jitter links to both estimate the location of nodes and reduce cumulative synchronization error across multiple hops	mechanism
The Pulsar platform and protocol together provide a phase synchronized one pulse per second ( 1PPS ) signal and 10 MHz reference clock that can be easily integrated with typical enduser applications like software-defined radios and communication systems	mechanism
We experimentally evaluate the Pulsar platform in terms of clock synchronization accuracy	method
Allan deviation between pairwise clocks and ranging accuracy	method
We show that even though GAN optimization does not correspond to a convex-concave game	finding
even for simple parameterizations	finding
under proper conditions	finding
equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation	finding
On the other hand	finding
we show that the recently-proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium	finding
Motivated by this stability analysis	mechanism
we propose an additional regularization term for gradient descent GAN updates	mechanism
In this paper	method
we analyze the `` gradient descent '' form of GAN optimization ( i	method
e	method
the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters )	method
Consider a stream of retweet events - how can we spot fraudulent lock-step behavior in such multi-aspect data ( i	background
e	background
tensors ) evolving over time ? Can we detect it in real time	background
with an accuracy guarantee ? Past studies have shown that dense subtensors tend to indicate anomalous or even fraudulent behavior in many tensor data	background
including social media	background
Wikipedia and TCP dumps	background
updates by our algorithms are up to a million times faster than the fastest batch algorithms Effective : our DENSESALERT successfully spots anomalies especially those overlooked by existing algorithms	finding
We propose DENSESTREAM	mechanism
an incremental algorithm that maintains and updates a dense subtensor in a tensor stream ( i	mechanism
e	mechanism
a sequence of changes in a tensor )	mechanism
and DENSESALERT an incremental algorithm spotting the sudden appearances of dense subtensors	mechanism
Our algorithms are : ( 1 ) Fast and `` any time '' :	mechanism
( 2 ) Provably accurate : our algorithms guarantee a lower bound on the density of the subtensor they maintain	mechanism
and ( 3 )	mechanism
in real-world tensors	method
showed that depth continuity is a prerequisite for facilitation of Gabor target detection in the context of flanking Gabors	finding
and that similarly	finding
surface continuity in purely disparity-defined slanted surfaces was strongly enhanced in distributed patch detections as a function of stimulus duration in this dual discrimination task	finding
showing that the perceptual processing of disparity and integration of 3D surface information across depth cues has time courses of several seconds	finding
attesting to complexity of the neural processing hardware showed how surface reconstruction could be accomplished across the typically sparse depth information available	finding
and integrated among sparse	finding
incommensurate cue modalities	finding
in a computational model based on a novel Leaky Drift Diffusion Theory that we developed for the underlying neural signals	mechanism
which can serve as an analytic basis for the time course of all neural decision processes	mechanism
Three complementary computational modeling projects from three collaborating laboratories	mechanism
Psychophysical studies The time course of depth surface perception was studied in a coordinated trio of psychophysical	method
neurophysiological and functional imaging studies	method
Deep learning models can take weeks to train on a single GPU-equipped machine	background
necessitating scaling out DL training to a GPU-cluster	background
We show that Poseidon is applicable to different DL frameworks We show that Poseidon enables Caffe and TensorFlow to achieve 15	finding
5x speed-up on 16 single-GPU machines	finding
even with limited bandwidth ( 10GbE ) and the challenging VGG19-22K network for image classification Moreover	finding
Poseidon-enabled TensorFlow achieves 31	finding
5x speed-up with 32 single-GPU machines on Inception-V3	finding
a 50 % improvement over the open-source TensorFlow ( 20x speed-up	finding
We present Poseidon	mechanism
an efficient communication architecture Poseidon exploits the layered model structures in DL programs to overlap communication and computation	mechanism
reducing bursty network communication Moreover	mechanism
Poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer	mechanism
according to layer properties and the number of machines	mechanism
by plugging Poseidon into Caffe and TensorFlow	method
Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled countless new applications in the domain of aerial sensing	background
In scenarios such as intrusion detection	background
target tracking and facility monitoring it is important to reach a given area of interest ( AOI )	background
and create an online data streaming connection to a monitoring ground station ( GS ) for immediate delivery of content to the operator	background
In previous work	background
we showed that a multi-hop line network can increase the range of the mission by finding the optimal number of relay UAVs	background
and their optimal placement	background
In this demo	mechanism
we show that CSMA ( typical 802	mechanism
11 's medium access protocol ) and that TDMA is a better alternative	mechanism
We will also discuss how changing slot width online can overcome typical and less known TDMA inefficiencies	mechanism
and therefore reach maximum end-to-end throughput and low delay	mechanism
Gaussian belief propagation ( BP ) has been widely used for distributed inference in large-scale networks such as the smart grid	background
sensor networks and social networks	background
where local measurements/observations are scattered over a wide geographical area One particular case is when two neighboring agents share a common observation	background
For example to estimate voltage in the direct current ( DC ) power flow model	background
the current measurement over a power line is proportional to the voltage difference between two neighboring buses	background
that the updating information matrix converges at a geometric rate to a unique positive definite matrix with arbitrary positive semidefinite initial value and further provide the necessary and sufficient convergence condition for the belief mean vector to the optimal estimate	finding
of Gaussian BP for this pairwise linear Gaussian model	mechanism
We show analytically	method
Developing a remote exploit is not easy	background
It requires a comprehensive understanding of a vulnerability and delicate techniques to bypass defense mechanisms	background
As a result	background
attackers may prefer to reuse an existing exploit and make necessary changes over developing a new exploit from scratch	background
One such adaptation is the replacement of the original shellcode ( i	background
e	background
the attacker-injected code that is executed as the final step of the exploit ) in the original exploit with a replacement shellcode	background
resulting in a modified exploit that carries out the actions desired by the attacker as opposed to the original exploit author	background
We call this a shellcode transplant	background
Among the 100 test cases	finding
our system successfully generated 88 % of the exploits	finding
we present ShellSwap	mechanism
a system that uses symbolic tracing	mechanism
with a combination of shellcode layout remediation and path kneading	mechanism
We evaluated the ShellSwap system on a combination of 20 exploits and 5 pieces of shellcode that are independently developed and different from the original exploit	method
The availability of large idea repositories ( e	background
g	background
the U	background
S	background
patent database ) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems Our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations	background
We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods	finding
analogies retrieved by our models significantly increased people 's likelihood of generating creative ideas compared to analogies retrieved by traditional methods	finding
specifically `` problem schemas ''	mechanism
which specify the purpose of a product and the mechanisms by which it achieves that purpose	mechanism
Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions	mechanism
In an ideation experiment	method
Our results significantly expand knowledge of eutherian genome evolution and will facilitate greater understanding of the role of chromosome rearrangements in adaptation	background
speciation and the etiology of inherited and spontaneously occurring diseases	background
Orangutan was found to have eight chromosomes that were completely conserved in homologous sequence order and orientation with the eutherian ancestor	finding
the largest number for any species	finding
Ruminant artiodactyls had the highest frequency of intrachromosomal rearrangements	finding
and interchromosomal rearrangements dominated in murid rodents	finding
A total of 162 chromosomal breakpoints in evolution of the eutherian ancestral genome to the human genome were identified ; however	finding
the rate of rearrangements was significantly lower ( 0	finding
80/My ) during the first 60 My of eutherian evolution	finding
then increased to greater than 2	finding
0/My along the five primate lineages studied	finding
we developed an algorithm ( DESCHRAMBLER ) that probabilistically determines the adjacencies of syntenic fragments using chromosome-scale and fragmented genome assemblies	mechanism
The reconstructed chromosomes of the eutherian	mechanism
boreoeutherian and euarchontoglires ancestor each included > 80 % of the entire length of the human genome	mechanism
whereas reconstructed chromosomes of the most recent common ancestor of simians	mechanism
catarrhini great apes	mechanism
and humans and chimpanzees included > 90 % of human genome sequence These high-coverage reconstructions permitted reliable identification of chromosomal rearrangements over 105 My of eutherian evolution	mechanism
With the explosion in the availability of user-generated videos documenting any conflicts and human rights abuses around the world	background
analysts and researchers increasingly find themselves overwhelmed with massive amounts of video data to acquire and analyze useful information	background
We show our framework 's efficacy on localizing intense audio event like gunshot	finding
and further experiments also indicate that our methods can be generalized to localizing other audio events in noisy videos	finding
In this paper	mechanism
we develop a temporal localization framework The proposed method utilizes Localized Self-Paced Reranking ( LSPaR ) to refine the localization results LSPaR utilizes samples from easy to noisier ones so that it can overcome the noisiness of the initial retrieval results from user-generated videos	mechanism
Previous work has replaced structural assumptions on the noise with a worst-case approach that aims to choose an outcome that minimizes the maximum error with respect to any feasible true ranking	background
This approach underlies algorithms that have recently been deployed on the social choice website RoboVote	background
org	background
We derive ( mostly sharp ) analytical bounds on the expected error and establish the practical benefits of our approach	finding
We take a less conservative viewpoint by minimizing the average error with respect to the set of feasible ground truth rankings	mechanism
through experiments	method
Low-income families pay substantial portions of their total expenditure on household energy bills	background
making them vulnerable to rising energy costs Habitat for Humanity houses are built for low-income families and made affordable with volunteer work and construction material donations While specific enclosure suggestions apply to this case-study	background
the utilized approach on exploring different options to identify opportunities to save energy can be used to understand impact on the lives of low-income families	background
The results show that it is possible to reduce the energy cost of these houses without significantly increasing the construction costs through exploration of different wall and window options	finding
In collaboration with Habitat for Humanity of Westchester	mechanism
we created an energy simulation model of an existing low-income house and calculated the homeOs annual energy usage with different design alternatives for windows and walls	mechanism
The resulting estimated annual energy savings are then evaluated alongside their initial investment costs	method
which were retrieved from RS Means standard construction cost data and quotations from industry	method
It is known that such allocations can be computed using O ( n ln ( 1/e ) ) operations in the standard Robertson-Webb Model	background
implies that allocations that are exactly equitable can not be computed	background
Importantly our result	finding
We establish a lower bound of ( ln ( 1/e ) /lnln ( 1/e ) ) on the complexity of this problem	mechanism
which is almost tight for a constant number of players	mechanism
In the era of social media	background
a large number of user-generated videos are uploaded to the Internet every day	background
capturing events all over the world	background
show that the proposed method achieves excellent precision and robustness	finding
In this paper	mechanism
we propose a hierarchical approach Our system utilizes clustered audio-signatures to align video pairs	mechanism
Global alignment for all videos is then achieved via forming alignable video groups with self-paced learning	mechanism
Experiments on the Boston Marathon dataset	method
Mainstream crowdwork platforms treat microtasks as indivisible units We reflect on the implications of these findings for the design of future crowd work platforms that effectively harness the potential of subcontracting workflows	background
Finally we describe the outcome of two tasks on Mechanical Turk meant to simulate aspects of subcontracting	finding
: real-time assistance	mechanism
task management and task improvement	mechanism
and reflect on potential use cases and implementation considerations associated with each	mechanism
After describing the value proposition of subcontracting	method
we then define three models for microtask subcontracting	method
The world is full of physical interfaces that are inaccessible to blind people	background
from microwaves and information kiosks to thermostats and checkout terminals	background
Blind people can not independently use such devices without at least first learning their layout	background
and usually only after labeling them with sighted assistance	background
we introduce VizLens -- -a robust and interactive screen reader for interfaces in the real world	mechanism
VizLens users take a picture of an interface they would like to use	mechanism
it is interpreted quickly and robustly by multiple crowd workers in parallel	mechanism
and then computer vision is able to give interactive feedback and guidance to users to help them use the interface in real time Built on top of VizLens	mechanism
we developed automatically generating tactile overlays to physical interfaces to provide blind people with a permanent static solution	mechanism
We introduce Facade -- -a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel	method
Crowd work is an increasingly prevalent and important kind of work	background
Because of its flexible nature	background
crowd work may offer benefits for people with disabilities Given ongoing and upcoming changes to the world economy and technological progress	background
we believe it is important to find a way to make sure people with disabilities are able to equally participate in this kind of work	background
In this paper	method
we first characterize the accessibility of the tasks posted to a popular crowd marketplace	method
Amazon Mechanical Turk	method
by performing manual and automatic checks on 120 tasks from several common types	method
We then outline research directions that could have positive impact on this problem	method
Providing navigation assistance to people with visual impairments often requires augmenting the environment with after-market technology	background
LuzDeploy is a computational method We use LuzDeploy to orchestrate volunteers to install physical infrastructure for the navigation assistance of people with visual impairments Our system provides on-the-go enrollment so that volunteers can participate to the collective action whenever they have time	mechanism
coming and leaving as needed	mechanism
Providing automated instructions also allows to avoid instructing participants directly	mechanism
so experts do not need to be available on-site	mechanism
What can humans compute in their heads ? We are thinking of a variety of Crypto Protocols	background
games like Sudoku	background
Crossword Puzzles Speed Chess	background
and so on	background
we propose a rigorous model of human computation and associated measures of complexity We apply the model and measures first and foremost to the problem of ( 1 ) humanly computable password generation	mechanism
and then consider related problems of ( 2 ) humanly computable `` one-way functions '' and ( 3 ) humanly computable `` pseudorandom generators '' The theory of Human Computability developed here plays by different rules than standard computability	mechanism
and this takes some getting used to For reasons to be made clear	mechanism
the polynomial versus exponential time divide of modern computability theory is irrelevant to human computation In human computability	mechanism
the step-counts for both humans and computers must be more concrete	mechanism
Specifically we restrict the adversary to at most 10^24 ( Avogadro number of ) steps	mechanism
An alternate view of this work is that it deals with the analysis of algorithms and counting steps for the case that inputs are small as opposed to the usual case of inputs large-in-the-limit	mechanism
How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we exploit them for applications ? A k-core is the maximal subgraph in which all vertices have degree at least k	background
This concept has been applied to such diverse areas as hierarchical structure analysis	background
graph visualization and graph clustering	background
Our discoveries are : ( 1 ) Mirror Pattern : coreness ( i	finding
e	finding
maximum k such that each vertex belongs to the k-core ) is strongly correlated with degree	finding
( 2 ) Core-Triangle Pattern : degeneracy ( i	finding
e	finding
maximum k such that the k-core exists ) obeys a 3-to-1 power-law with respect to the count of triangles	finding
( 3 ) Structured Core Pattern : degeneracycores are not cliques but have non-trivial structures such as coreperiphery and communities	finding
Our algorithmic contributions show the usefulness of these patterns	mechanism
( 1 ) Core-A	mechanism
which measures the deviation from Mirror Pattern	mechanism
successfully spots anomalies in real-world graphs	mechanism
( 2 ) Core-D	mechanism
a single-pass streaming algorithm based on Core-Triangle Pattern	mechanism
accurately estimates degeneracy up to 12\ ( \times \ ) faster than its competitor ( 3 ) Core-S	mechanism
inspired by Structured Core Pattern	mechanism
identifies influential spreaders up to 17\ ( \times \ ) faster than its competitors with comparable accuracy	mechanism
Systems for providing mixed physical-virtual interaction on desktop surfaces have been proposed for decades	background
though no such systems have achieved widespread use	background
demonstrating their imminent feasibility	finding
In this paper	method
we use an elicitation study and interviews to synthesize a list of ten interactive behaviors that desk-bound	method
digital interfaces should implement to support responsive cohabitation with physical objects As a proof of concept	method
we implemented these interactive behaviors in a working augmented desk system	method
Given a bipartite graph of users and the products that they review	background
or followers and followees	background
how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral	background
etc	background
) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph	background
Fraudsters can evade these methods using camouflage	background
by adding reviews or follows with honest targets so that they look normal	background
Even worse some fraudsters use hijacked accounts from honest users	background
and then the camouflage is indeed organic	background
show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud FRAUDAR successfully detected a subgraph of more than 4	finding
000 detected accounts	finding
of which a majority had tweets showing that they used follower-buying services	finding
We propose FRAUDAR	mechanism
an algorithm that ( a ) is camouflage resistant	mechanism
( b ) provides upper bounds on the effectiveness of fraudsters	mechanism
and ( c ) is effective in real-world data	mechanism
Experimental results under various attacks Additionally	method
in real-world experiments with a Twitter follower -- followee graph of 1	method
47 billion edges	method
show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results	finding
without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval	finding
In this paper	mechanism
we propose a new model through the closeness between the topic vectors	mechanism
Our method enables efficient inference in the low-dimensional embedding space	mechanism
reducing previous cubic or quadratic time complexity to linear w	mechanism
r	mechanism
t the topic size	mechanism
We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence	mechanism
Extensive experiments	method
Intelligent personalization systems are becoming increasingly reliant on contextually-relevant devices and services	background
such as those available within modern IoT deployments	background
An IoT context may emerge -- -or become pervasive -- -when the intelligent system generates knowledge from dialogue-based interactions with the end-user ; the context is strengthened even further by incorporating state representations about the environment ( e	background
g	background
generated from wireless sensor data ) into the knowledge graph	background
This is crucial for pervasive applications like digital assistance in IoT	background
where context-aware systems need to adapt quickly : activities like leaving work home-bound	background
driving to the grocery store	background
arriving at home	background
and walking the dog	background
for example can occur in a relatively short period of time -- -during which an intelligent assistant must be able to support user requests in a consistent and coherent manner	background
Given that computational ontologies can serve as semantic models for heterogeneous data	background
they are becoming increasingly viable for reasoning across different IoT contexts	background
This involves : ( a ) federation and dynamic pruning of multiple modular ontologies	background
ideally to comprehensively capture only the knowledge that will facilitate execution of a multi-context task ; ( b ) fast consistency-checking and ontology-based inferences	background
aided by rules-based execution environments that can evaluate/transform ambient wireless sensor network ( WSN ) data	background
in real-time ; and ( c ) run-time execution of ontology-based control procedures	background
through rule-engine actuation commands sent across the WSN The approach we describe is also partially based on the Ubiquitous Personal Assistant ( UPA ) project	background
Bosch Research 's largest research initiative worldwide	background
Preliminary results are also discussed	finding
In this poster	mechanism
we illustrate how a multi-context knowledge base can be structured on the basis of modular ontologies and integrated with a distributed rules-based inference engine in multiple smart-building environments	mechanism
This work is conducted through the partnership of Bosch Research Pittsburgh and Carnegie Mellon University ( CMU )	method
and is in partial satisfaction of CMU 's Bosch Energy Research Network ( BERN ) grant	method
awarded for developments in intelligent building solutions	method
Audio transcription is an important task for making content accessible to people who are deaf or hard of hearing	background
Much of the transcription work is increasingly done by crowd workers	background
people online who pick up the work as it becomes available often in small bits at a time Whereas work typically provides a ladder for skill development -- a series of opportunities to acquire new skills that lead to advancement -- crowd transcription work generally does not	background
Our research demonstrates a new way for workers on crowd platforms to align their work and skill development with the accessibility domain while they work	background
We show that Scopist can distinguish touch-typing from stenotyping with 94 % accuracy	finding
we created Scopist	mechanism
a JavaScript application for learning an efficient text-entry method known as stenotype while doing audio transcription tasks	mechanism
Scopist facilitates on-the-job learning to prepare crowd workers for remote	mechanism
real-time captioning by supporting both touch-typing and chording Real-time captioning is a difficult skill to master but is important for making live events accessible	mechanism
We conducted 3 crowd studies of Scopist focusing on Scopist 's performance and support for learning	method
Situational awareness involves the timely acquisition of knowledge about real-world events	background
distillation of those events into higher-level conceptual constructs	background
and their synthesis into a coherent context-sensitive view	background
We explore how convergent trends in video sensing	mechanism
crowd sourcing and edge computing can be harnessed to create a shared real-time information system	mechanism
Social media has become a popular and important tool for human communication	background
However due to this popularity	background
spam and the distribution of malicious content by computer-controlled users	background
known as bots	background
has become a widespread problem	background
At the same time	background
when users use social media	background
they generate valuable data that can be used to understand the patterns of human communication	background
is characterized by following four patterns : ( i ) heavy-tails	finding
( ii ) periodic-spikes	finding
( iii ) correlation between consecutive values	finding
and ( iv ) bimodallity	finding
Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics	finding
Additionally when detecting bots	finding
Act-M provided a precision higher than 93 % and 77 % with a sensitivity of 70 % for the Twitter and Reddit datasets	finding
respectively	finding
As our second contribution	mechanism
we propose a mathematical model named Act-M ( Activity Model ) We show that Act-M can accurately from social media users Finally	mechanism
we use Act-M to develop a method	mechanism
The first contribution of this article is showing that the distribution of inter-arrival times ( IATs ) between postings We validate Act-M using data from over 55 million postings from four social media services : Reddit	method
Twitter Stack-Overflow and Hacker-News	method
demonstrate that our approach significantly outperforms the compared baselines	finding
In this work	mechanism
we introduce Video Question Answering in the temporal domain We present an encoderdecoder approach using Recurrent Neural Networks to learn the temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions	mechanism
In addition 390	mechanism
744 corresponding questions are generated from annotations	mechanism
We explore approaches for finer understanding of video content using the question form of fill-in-the-blank	method
and collect our Video Context QA dataset consisting of 109	method
895 video clips with a total duration of more than 1000 h from existing TACoS	method
MPII-MD and MEDTest 14 datasets	method
Extensive experiments	method
validates the accuracy of sentence and attribute generation	finding
We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures With differentiable approximation to discrete text samples	mechanism
explicit constraints on independent attribute controls	mechanism
and efficient collaborative learning of generator and discriminators	mechanism
our model learns highly interpretable representations from even only word annotations	mechanism
and produces realistic sentences with desired attributes	mechanism
Quantitative evaluation	method
Nuclear organization has an important role in determining genome function ; however	background
it is not clear how spatiotemporal organization of the genome relates to functionality	background
We anticipate the general applicability and scalability of our method will enhance causative analyses between gene function and compartmentalization in a high-throughput manner	background
was successful at labeling 9 different loci in HCT116 cells with up to 20 % efficiency These loci included both nuclear speckle-associated	finding
euchromatin regions and nuclear lamina-associated	finding
heterochromatin regions	finding
Here we report an efficient and scalable method named SHACKTeR ( Short Homology and CRISPR/Cas9-mediated Knock-in of a TetO Repeat Compared to alternatives	mechanism
our method does not require a nearby repetitive sequence and it requires only two modifications to the genome : CRISPR/Cas9-mediated knock-in of an optimized TetO repeat and its visualization by TetR-EGFP expression	mechanism
Our simplified knock-in protocol	mechanism
utilizing short homology arms integrated by PCR	mechanism
Robust hand detection and classification is one of the most crucial pre-processing steps to support human computer interaction	background
driver behavior monitoring	background
virtual reality etc	background
The experimental results show that our proposed MS-FRCN approach consistently achieves the state-of-the-art hand detection results	finding
i	finding
e	finding
Average Precision ( AP ) / Average Recall ( AR ) of 95	finding
1 % / 94	finding
5 % at level 1 and 86	finding
0 % / 83	finding
4 % at level 2	finding
on the VIVA challenge In addition	finding
the proposed method achieves the state-of-the-art results for left/right hand and driver/passenger classification tasks on the VIVA database with a significant improvement on AP/AR of ~7 % and ~13 % for both classification tasks	finding
respectively The hand detection performance of MS-RFCN reaches to 75	finding
1 % of AP and 77	finding
8 % of AR on Oxford database	finding
This work presents a novel approach named Multiple Scale Region-based Fully Convolutional Networks ( MSRFCN ) In this approach	mechanism
the whole image is passed through the proposed fully convolutional network to compute score maps Those score maps with their position-sensitive properties can help to efficiently address a dilemma between translation-invariance in classification and detection	mechanism
The method is evaluated on the challenging hand databases	method
i	method
e	method
the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge	method
Oxford hand dataset and compared against various recent hand detection methods	method
Abstract Server-side variability the idea that the same job can take longer to run on one server than another due to server-dependent factors isan increasingly important concern in many queueing systems	background
One strategy for overcoming server-side variability to achieve low response time is redundancy	background
under which jobs create copies of themselves and send these copies to multiple different servers	background
waiting for only one copy to complete service	background
Most of the existing theoretical work on redundancy has focused on developing bounds	background
approximations and exact analysis to study the response time gains offered by redundancy	background
shows that FCFS can be unfair in that it can hurt non-redundant jobs	finding
which is provably fair and also achieves excellent overall mean response time	finding
We develop new exact analysis under First-Come First-Served ( FCFS ) scheduling for a general type of system structure ; We then introduce the Least Redundant First ( LRF ) scheduling policy	mechanism
which we prove is optimal with respect to overall system response time	mechanism
but which can be unfair in that it can hurt the jobs that become redundant	mechanism
Finally we introduce the Primaries First ( PF ) scheduling policy	mechanism
our analysis	method
Generative Adversarial Networks ( GANs ) have recently achieved significant improvement on paired/unpaired image-to-image translation	background
such as photo $ \rightarrow $ sketch and artist painting style transfer	background
show considerable performance gain by our contrast-GAN over other conditional GANs Quantitative results further demonstrate the superiority of our model on generating manipulated results with high visual fidelity and reasonable object semantics	finding
we introduce a contrasting GAN ( contrast-GAN ) with a novel adversarial contrasting objective	mechanism
Instead of directly making the synthesized samples close to target data as previous GANs did	mechanism
our adversarial contrasting objective optimizes over the distance comparisons between samples	mechanism
that is enforcing the manipulated data be semantically closer to the real data with target category than the input data Equipped with the new contrasting objective	mechanism
a novel mask-conditional contrast-GAN architecture is proposed to enable disentangle image background with object semantic changes	mechanism
Experiments on several semantic manipulation tasks on ImageNet and MSCOCO dataset	method
The influence of motor cortex on muscles during different behaviors is incompletely understood	background
Future frame prediction in videos is a promising avenue for unsupervised video representation learning	background
Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video	background
demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows	finding
Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning	finding
In this paper	mechanism
we develop a dual motion Generative Adversarial Net ( GAN ) architecture	mechanism
which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism The primal future-frame prediction and dual future-flow prediction form a closed loop	mechanism
generating informative feedback signals to each other for better video prediction	mechanism
To make both synthesized future frames and flows indistinguishable from reality	mechanism
a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames	mechanism
while the future-frame prediction in turn leads to realistic optical flows Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder	mechanism
which is based on variational	mechanism
Extensive experiments	method
We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR	finding
For example we observe both diffusion and non-diffusion mechanism in the group joining process	finding
and power-law decay in group quitting process	finding
rather than exponential decay as expected in SIR model	finding
Therefore we propose a new model come N go	mechanism
a concise yet flexible dynamic model Our model has the following advantages : ( a ) Unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation	mechanism
( b ) Succinctness and interpretability : it contains only six parameters with clear physical meanings ( c ) Accuracy : it can capture various kinds of group evolution patterns preciously	mechanism
and the goodness of fit increases by 58 % over baseline	mechanism
( d ) Usefulness : it can be used in multiple application scenarios	mechanism
such as forecasting and pattern discovery Furthermore	mechanism
our model can provide insights about different evolution patterns of social groups	mechanism
and we also find that group structure and its evolution has notable relations with temporal patterns of group evolution	mechanism
In this article	method
we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users	method
Computer vision based technologies have seen widespread adoption over the recent years	background
This use is not limited to the rapid adoption of facial recognition technology but extends to facial expression recognition	background
scene recognition and more	background
In this paper we introduce a novel distributed privacy infrastructure for the Internet-of-Things and discuss in particular how it can help The infrastructure	mechanism
supports the automated discovery of IoT resources and the selective notification of users This includes the presence of computer vision applications that collect data about users	mechanism
In particular we describe an implementation of functionality that helps users discover nearby cameras and choose whether or not they want their faces to be denatured in the video streams	mechanism
which has undergone early deployment and evaluation on two campuses	method
Information cascades are ubiquitous in both physical society and online social media	background
taking on large variations in structures	background
dynamics and semantics	background
Our discoveries also provide a foundation for the microscopic mechanisms for information spreading	background
potentially leading to implications for cascade prediction and outlier detection	background
We find that the structural complexity of information cascades is far beyond the previous conjectures We find that bimodal law governs majority of the metrics	finding
information flows in cascades have four directions	finding
and the self-loop number and average activity of cascades follows power law and finally uncover some notable implications of structural patterns in information cascades	finding
We first propose a ten-dimensional metric	mechanism
reflecting cascade size	mechanism
silhouette direction and activity aspects	mechanism
Here we explore a large-scale dataset including $ 432 $ million information cascades with explicit records of spreading traces	method
spreading behaviors information content as well as user profiles We then analyze the high-order structural patterns of information cascades Finally	method
we evaluate to what extent the structural features of information cascades can explain its dynamic patterns and semantics	method
Extreme Classification comprises multi-class or multi-label prediction where there is a large number of classes	background
and is increasingly relevant to many real-world applications such as text and image tagging	background
our proposed method achieves accuracy competitive to the state-of-the-art while reducing the training time from days to tens of minutes compared with existing parallel or sparse methods on a cluster of 100 cores	finding
In this work	mechanism
we extend PD-Sparse	mechanism
By introducing separable loss functions	mechanism
we can scale out the training	mechanism
with network communication and space efficiency comparable to those in one-versus-all approaches while maintaining an overall complexity sub-linear in the number of classes	mechanism
On several large-scale benchmarks	method
Time-limited promotions that exploit consumers ' sense of urgency to boost sales account for billions of dollars in consumer spending each year	background
and show that RUSH ! provides higher expected value than various baselines that do not jointly model time and category information	finding
Specifically we use large-scale anonymized transaction records	mechanism
based on which we generate data-driven	mechanism
personalized coupons	mechanism
Our proposed model RUSH ( 1 ) predicts { both the time and category } of the next event ; ( 2 ) captures correlations between purchases in different categories ( such as shopping triggering dining purchases ) ; ( 3 ) incorporates temporal dynamics of purchase behavior ( such as increased spending on weekends ) ; ( 4 ) is composed of additive factors that are easily interpretable ; and finally ( 5 ) scales linearly to millions of transactions	mechanism
We design a cost-benefit framework that facilitates systematic evaluation in terms of our application	method
How do people make friends dynamically in social networks ? What are the temporal patterns for an individual increasing its social connectivity ? What are the basic mechanisms governing the formation of these temporal patterns ? No matter cyber or physical social systems	background
their structure and dynamics are mainly driven by the connectivity dynamics of each individual	background
Our model and discoveries provide a foundation for the microscopic mechanisms of network growth dynamics	background
potentially leading to implications for prediction	background
clustering and outlier detection on human dynamics	background
We uncover a wide range of long-term power law growth and short-term bursty growth for the social connectivity of different users	finding
We propose three key ingredients	finding
namely average-effect multiscale-effect and correlation-effect	finding
which govern the observed growth patterns at microscopic level	finding
we discover statistical regularities underlying the empirical growth dynamics	finding
As a result	mechanism
we propose the long short memory process incorporating these ingredients	mechanism
demonstrating that it successfully reproduces the complex growth patterns observed in the empirical data	mechanism
We examine the detailed growth process of `` WeChat ''	method
the largest online social network in China	method
with 300 million users and 4	method
75 billion links spanning two years	method
By analyzing modeling parameters	method
With the pervasiveness of mobile technology and location-based computing	background
new forms of smart urban transportation	background
such as Uber & Lyft	background
have become increasingly popular	background
These new forms of urban infrastructure can influence individuals ' movement frictions and patterns	background
in turn influencing local consumption patterns and the economic performance of local businesses	background
in this paper	mechanism
we utilize a novel dataset and econometric analysis methods	mechanism
a quasi-experimental examination	method
This generalizes a prior decomposition result for an M/M/k/staggeredM/M/k/staggered-setup	background
We show that	finding
the response time of an M/G/k/staggeredM/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/kM/G/k and the setup time	finding
where the approximation is nearly exact	finding
for exponentially distributed setup times	method
Quickly converting speech to text allows deaf and hard of hearing people to interactively follow along with live speech Doing so reliably requires a combination of perception	background
understanding and speed that neither humans nor machines possess alone Scribe illustrates the broad potential for deeply interleaving human labor and machine intelligence to provide intelligent interactive services that neither can currently achieve alone	background
In this article	mechanism
we discuss how our Scribe system combines human labor and machine intelligence in real time	mechanism
Scribe integrates automated assistance in two ways	mechanism
First its user interface directs workers to different portions of the audio stream	mechanism
slows down the portion they are asked to type	mechanism
and adaptively determines segment length based on typing speed	mechanism
Second it automatically merges the partial input of multiple workers into a single transcript using a custom version of multiple-sequence alignment	mechanism
The results demonstrate the effectiveness of our proposed model	finding
M & M TGM not only outperforms prior state-of-the-art methods but also achieves better generalization ability	finding
In this paper	mechanism
we propose an unified caption framework	mechanism
M & M TGM	mechanism
which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics	mechanism
Compared to pre-defined topics	mechanism
the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better	mechanism
We formulate the topic-aware caption generation as a multi-task learning problem	mechanism
in which we add a parallel task	mechanism
topic prediction in addition to the caption task	mechanism
For the topic prediction task	mechanism
we use the mined topics as the teacher to train a student topic prediction model	mechanism
which learns to predict the latent topics from multimodal contents of videos	mechanism
The topic prediction provides intermediate supervision to the learning process	mechanism
As for the caption task	mechanism
we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics	mechanism
The entire learning procedure is end-to-end and it optimizes both tasks simultaneously	mechanism
from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets on multiple evaluation metrics and on both benchmark datasets	method
Data association which could be categorized into offline approaches and the online counterparts	background
is a crucial part of a multi-object tracker in the tracking-by-detection framework	background
results show that our approach achieves the state-of-the-art performance on challenging datasets	finding
In this paper	mechanism
we propose a mixed style tracker	mechanism
which is not only as efficient as the online tracker but also aware of future observations in offline setting	mechanism
We start from a Markov Decision Process ( MDP ) online tracker and design a parallelized apprenticeship learning algorithm to learn both the reward function and transition policy in MDP	mechanism
By proposing a rewind to track strategy to generate backward tracklets	mechanism
future detections in offline data are efficiently utilized to obtain a more stable similarity measurement for association	mechanism
Experiment	method
demonstrate the effectiveness of proposed framework	finding
we propose a unified SED detection framework which divides events into two categories	mechanism
i	mechanism
e	mechanism
short-term events and long-duration events The former can be represented as a kind of snapshots of static key-poses and embodies an inner-dependencies	mechanism
while the latter contains complex interactions between pedestrians	mechanism
and shows obvious inter-dependencies and temporal context	mechanism
For short-term event	mechanism
a novel cascade Convolutional Neural Network ( CNN ) -HsNet is first constructed to detect the pedestrian	mechanism
and then the corresponding events are classified	mechanism
For long-duration event	mechanism
Dense Trajectory ( DT ) and Improved Dense Trajectory ( IDT ) are first applied to explore the temporal features of the events respectively	mechanism
and subsequently Fisher Vector ( FV ) coding is adopted to encode raw features and linear SVM classifiers are learned to predict	mechanism
Finally a heuristic fusion scheme is used to obtain the results In addition	method
a new large-scale pedestrian dataset	method
named SED-PD is built for evaluation	method
Comprehensive experiments on TRECVID SEDtest datasets	method
Genome-wide association studies have discovered a large number of genetic variants associated with complex diseases such as Alzheimers disease	background
we demonstrate that BTAM significantly improves the statistical power over forward three-way association mapping that finds genotypes associated with both transcripts and phenotypes and genotype-phenotype association mapping and report top 10 genotype-transcript-phenotype associations	finding
In this paper	mechanism
we present a novel approach called Backward Three-way Association Mapping ( BTAM ) Assuming that genotypes affect transcript levels	mechanism
which in turn affect phenotypes	mechanism
we first find transcripts associated with the phenotypes	mechanism
and then find genotypes associated with the chosen transcripts	mechanism
The backward ordering of association mappings allows us to avoid a large number of association testings between all genotypes and all transcripts	mechanism
making it possible to identify three-way associations with a small computational cost	mechanism
In our simulation study	method
Furthermore we apply BTAM on an Alzheimers disease dataset	method
The Next-Generation Airborne Collision Avoidance System ACASi ? X is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft	background
It is currently being developed by the Federal Aviation Administration FAA	background
Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety	background
In this paper we determine the geometric configurations under a precise set of assumptions and formally verify these configurations using hybrid systems theorem proving techniques	mechanism
We conduct an initial examination of the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version	mechanism
demonstrating how formal	mechanism
hybrid approaches are helping ensure the safety of ACAS X	mechanism
Recently to solve large-scale lasso and group lasso problems	background
screening rules have been developed	background
the goal of which is to reduce the problem size by efficiently discarding zero coefficients using simple rules independently of the others	background
we demonstrate the efficiency of our screening rules	finding
In this paper	mechanism
we develop screening rules for overlapping group lasso we take into account overlapping groups only if they are inclusive of the group being tested	mechanism
and then we derive screening rules	mechanism
adopting the dual polytope projection approach This strategy allows us to screen each group independently of each other	mechanism
In our experiments	method
on various datasets	method
Facility location and committee selection are classic embodiments of this problem	background
demonstrate the viability of this approach and the value of such optimized mechanisms vis-a-vis mechanisms derived through worst-case analysis	finding
We propose a class of percentile mechanisms	mechanism
a form of generalized median mechanisms	mechanism
that are strategy-proof	mechanism
and for L1 and L2 cost models More importantly	mechanism
we propose a sample-based framework	mechanism
while maintaining strategy-proofness	mechanism
Our empirical investigations	method
using social cost and maximum load as objectives	method
using first and second authentication processes and a handoff from the first process to the second process	mechanism
In one embodiment	mechanism
the first authentication process is a stronger process performed at the outset of a session	mechanism
and the second authentication process is a weaker process iteratively performed during the session	mechanism
The stronger authentication process may require cooperation from the user	mechanism
while the weaker authentication process is preferably one that requires little or no user cooperation	mechanism
In other embodiments	mechanism
a strong authentication process may be iteratively performed during the session	mechanism
Social choice theory provides insights into a variety of collective decision making settings	background
In this paper we model the problem via Markov decision processes ( MDP )	mechanism
where the states of the MDP coincide with preference profiles and a ( deterministic	mechanism
stationary ) policy corresponds to a social choice function We can therefore employ the axioms studied in the social choice literature as guidelines in the design of socially desirable policies	mechanism
We present tractable algorithms that compute optimal policies under different prominent social choice constraints	mechanism
Our machinery relies on techniques for exploiting symmetries and isomorphisms between MDPs	mechanism
While the analysis of unlabeled networks has been studied extensively in the past	background
finding patterns in different kinds of labeled graphs is still an open challenge	background
We show that Com $ $ ^2 $ $ 2 spots intuitive patterns regarding edge labels that carry temporal or other discrete information	finding
Our findings include large `` star '' -like patterns	finding
near-bipartite cores as well as tiny groups ( five users )	finding
calling each other hundreds of times within a few days	finding
We also show that we are able to automatically identify competing airline companies	finding
We propose Com $ $ ^2 $ $ 2	mechanism
a novel fast and incremental tensor analysis approach The method is ( a ) scalable	mechanism
being linear on the input size	mechanism
( b ) general	mechanism
( c ) needs no user-defined parameters and ( d ) effective	mechanism
returning results that agree with intuition	mechanism
We apply our method to real datasets	method
including a phone call network	method
a computer-traffic network and a flight information network	method
The phone call network consists of 4 million mobile users	method
with 51 million edges ( phone calls )	method
over 14 days	method
while the flights dataset consists of 7733 airports and 5995 airline companies flying 67	method
663 different routes	method
The Pitman-Yor process provides an elegant way to cluster data that exhibit power law behavior	background
where the number of clusters is unknown or unbounded	background
We show that our method scales well with increasing data while avoiding any degradation in estimate quality	finding
In this paper we present new auxiliary-variable representations for the Pitman-Yor process and a special case of the hierarchical Pitman-Yor process that allows us	mechanism
We show that OpenEval is able to respond to the queries within a limited amount of time while also achieving high F1 score In addition	finding
we show that the accuracy of responses provided by OpenEval is increased as more time is given for evaluation	finding
that illustrate the effectiveness of our approach compared to related techniques	finding
We introduce OpenEval	mechanism
a which uses information on the web that are stated as multiargument predicate instances ( e	mechanism
g	mechanism
DrugHasSideEffect ( Aspirin	mechanism
GI Bleeding ) ) )	mechanism
OpenEval gets a small number of instances of a predicate as seed positive examples and automatically learns how to evaluate the truth of a new predicate instance by querying the web and processing the retrieved unstructured web pages	mechanism
We have extensively tested our model and shown empirical results	method
How can we correlate the neural activity in the human brain as it responds to typed words	background
with properties of these terms ( like edible	background
fits in hand ) ? In short	background
we want to find latent variables	background
that jointly explain both the brain activity	background
as well as the behavioral responses	background
This is one of many settings of the Coupled Matrix-Tensor Factorization ( CMTF ) problem	background
by up to 200	finding
along with an up to 65 fold increase in sparsity	finding
with comparable accuracy to the baseline	finding
TURBO-SMT is able to find meaningful latent variables	finding
as well as to predict brain activity with competitive accuracy	finding
We introduce TURBO-SMT	mechanism
a meta-method : it boosts the performance of any CMTF algorithm	mechanism
We apply TURBO-SMT to BRAINQ	method
a dataset consisting of a ( nouns	method
brain voxels human subjects ) tensor and a ( nouns	method
properties ) matrix	method
with coupling along the nouns dimension	method
The typical approach thus far is to use tensors or dynamical systems	background
EEG-MINE ( a ) can successfully reconstruct the signals with high accuracy ; ( b ) can spot surprising patterns within seizure EEG signals ; and ( c ) may provide early warning of epileptic seizures	finding
Here we present EEG-MINE	mechanism
a nonlinear chaos-based `` gray box model ''	mechanism
that blends domain knowledge with data observations	mechanism
When applied to numerous	method
real EEG sequences	method
Live music performance with computers has motivated many research projects in science	background
engineering and the arts	background
We conclude with directions for future work	background
outline our efforts to establish a new direction	mechanism
Human-Computer Music Performance ( HCMP )	mechanism
as a framework for a variety of coordinated studies	mechanism
Our work in this area spans performance analysis	mechanism
synchronization techniques and interactive performance systems	mechanism
We review the development of techniques for live music performance and	method
Matching function binaries -- the process of identifying similar functions among binary executables -- is a challenge that underlies many security applications such as malware analysis and patch-based exploit generation	background
Recent work tries to establish semantic similarity based on static analysis methods	background
BLEX outperforms BinDiff by up to 3	finding
5 times in correctly identifying similar functions BLEX also outperforms BinDiff if the binaries have been compiled by different compilers Averaged over all indexed functions	finding
our search engine ranks the correct matches among the top ten results 77 % of the time	finding
In this work	mechanism
we propose blanket execution	mechanism
a novel that achieves complete coverage by overriding the intended program logic	mechanism
Blanket execution collects the side effects of functions during execution under a controlled randomized environment	mechanism
Two functions are deemed similar	mechanism
if their corresponding side effects	mechanism
as observed under the same environment	mechanism
are similar too	mechanism
We implement our blanket execution technique in a system called BLEX	mechanism
Using the functionality in BLEX	mechanism
we have also built a binary search engine	mechanism
We evaluate BLEX rigorously against the state of the art binary comparison tool BinDiff	method
When comparing optimized and un-optimized executables from the popular GNU coreutils package	method
Complex systems are designed using the model-based design paradigm in which mathematical models of systems are created and checked against specifications	background
Cyber-physical systems ( CPS ) are complex systems in which the physical environment is sensed and controlled by computational or cyber elements possibly distributed over communication networks	background
Various aspects of CPS design such as physical dynamics	background
software control and communication networking must interoperate correctly for correct functioning of the systems	background
Modeling formalisms analysis techniques and tools for designing these different aspects have evolved ind ependently	background
and remain dissimilar and disparate	background
There is no unifying formalism in which one can model all these aspects equally well	background
In current practice	background
there is no principled approach that deals with this modeling heterogeneity within a formal framework	background
Composition of analysis results	finding
this thesis develops a framework based on behavioral semantics Heterogeneity arising from the different interacting aspects of CPS design must be addressed in order to enable system-level verification We develop behavioral semantics to address heterogeneity in a general yet formal manner	mechanism
Our framework makes no assumptions about the specifics of any particular formalism	mechanism
therefore it readily supports various formalisms	mechanism
techniques and tools	mechanism
Models can be analyzed independently in isolation	mechanism
supporting separation of concerns	mechanism
Mappings across heterogeneous semantic domains enable associations between analysis results	mechanism
Interdependencies across different models and specifications can be formally represented as constraints over parameters and verification can be carried out in a semantically consistent manner	mechanism
is supported both hierarchically across different levels of abstraction and structurally into interacting component models at a given level of abstraction	method
The theoretical concepts developed in the thesis are illustrated using a case study on the hierarchical heterogeneous verification of an automotive intersection collision avoidance system	method
Randomly mutating well-formed program inputs or simply fuzzing	background
is a highly effective and widely used strategy to find bugs in software	background
Other than showing fuzzers find bugs	background
there has been little systematic effort in understanding the science of how to fuzz properly	background
Overall we find 240 bugs in 8 applications and show that the choice of algorithm can greatly increase the number of bugs found We also show that current seed selection strategies as found in Peach may fare no better than picking seeds at random	finding
We make our data set and code publicly available	finding
We design six different algorithms using over 650 CPU days on Amazon Elastic Compute Cloud ( EC2 )	mechanism
and evaluate to provide ground truth data	method
A reliable and accurate biometric identification system must be able to distinguish individuals even in situations where their biometric signatures are very similar	background
indicate that both our proposed approaches achieve high identification rates and are hence quite promising at distinguishing twins	finding
proposes two novel methods using ( 1 ) facial aging features and ( 2 ) asymmetry decomposition features	mechanism
Facial aging features are extracted using Gabor filters from regions of the face that typically exhibit wrinkles and laugh lines	mechanism
while Facial asymmetry decomposition based features are obtained by projecting the difference between the two left sides ( consisting of the left half of the face and its mirror ) and two right sides ( consisting of the right half of the face and its mirror ) of a face onto a subspace	mechanism
Feature vectors obtained using these methods were used for classification HighlightsThe proposal of two novel approaches to distinguishing identical twins	mechanism
Facial aging and intrinsic facial symmetry features are sued	mechanism
A thorough evaluation on a challenging database	mechanism
The summarizing of existing techniques and the results obtained by them	mechanism
Experiments conducted on images of five types of twins from the University of Notre Dame ND-Twins database	method
Function identification is a fundamental challenge in reverse engineering and binary program analysis	background
For instance binary rewriting and control flow integrity rely on accurate function detection and identification in binaries	background
we found that BYTE-WEIGHT missed 44	finding
621 functions in comparison with the 266	finding
672 functions missed by the industry-leading tool IDA	finding
Furthermore while IDA misidentified 459	finding
247 functions BYTEWEIGHT misidentified only 43	finding
992 functions	finding
In this paper	mechanism
we propose BYTEWEIGHT	mechanism
a new algorithm Our approach automatically learns key features for recognizing functions and can therefore easily be adapted to different platforms	mechanism
new compilers and new optimizations	mechanism
We evaluated our tool against three well-known tools that feature function identification : IDA	method
BAP and Dyninst	method
Our data set consists of 2	method
200 binaries created with three different compilers	method
with four different optimization levels	method
and across two different operating systems	method
In our experiments with 2	method
200 binaries	method
we demonstrate that contextual supervision improves significantly over a reasonable baseline and existing unsupervised methods for source separation and show that recovery of the signal components depends only on cross-correlation between features for different signals	finding
not on correlations between features for the same signal	finding
We propose a new framework that lies between the fully supervised and unsupervised setting	mechanism
Instead of supervision	mechanism
we provide input features for each source signal and use convex methods Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision ; our motivating application is energy disaggregation of hourly smart meter data ( the separation of whole-home power signals into different energy uses )	mechanism
Here contextual supervision allows us for thousands homes	mechanism
a task previously impossible due to the need for specialized data collection hardware	mechanism
On smaller datasets which include labels	method
Finally we analyze the case of l2 loss theoretically	method
Computing equilibria of games is a central task in computer science	background
A large number of results are known for Nash equilibrium ( NE )	background
showing that the problem is in P	mechanism
We then design a spatial branch -- and -- bound algorithm to find an SNE	mechanism
and we experimentally evaluate the algorithm	method
where we show performance comparable to state of the art detector-based methods	finding
We present a model that localizes objects via unsupervised tracking while learning a representation of each object	mechanism
avoiding the need for pre-built detectors	mechanism
Our model uses a dependent Dirichlet process mixture to capture the uncertainty in the number and appearance of objects and requires only spatial and color video data that can be efficiently extracted via frame differencing	mechanism
We give two inference algorithms for use in both online and offline settings	mechanism
and use them to perform accurate detection-free tracking on multiple real videos	mechanism
We demonstrate our method in difficult detection scenarios involving occlusions and appearance shifts	method
on videos containing a large number of objects	method
and on a recent human-tracking benchmark	method
Given the re-broadcasts ( i	background
e	background
retweets ) of posts in Twitter	background
how can we spot fake from genuine user reactions ? What will be the tell-tale sign the connectivity of retweeters	background
their relative timing	background
or something else ? High retweet activity indicates influential users	background
and can be monetized	background
Hence there are strong incentives for fraudulent users to artificially boost their retweets ' volume	background
Our main contribu- tions are : ( a ) the discovery of patterns that fraudulent activity seems to follow ( the `` triangles `` a nd `` homogeneity '' patterns	finding
the formation of micro-clusters in appropriate feature spaces ) ; and	finding
( b ) `` RTGen ''	mechanism
a realistic generator that mimics the behaviors of both honest and fraud- ulent users	mechanism
We present experiments on a dataset of more than 6 million retweets crawled from Twitter	method
Safety critical systems often have shutdown mechanisms to bring the system to a safe state in the event of a malfunction	background
shows that using a rate-limited ride-through bound permits a tighter safety limit on speed than a xed threshold without creating false alarm shutdowns resulted in im- proved detection of speed limit violations and shorter shutdown stopping distances without needing to increase the false alarm shutdown rate	finding
We ex- amine the use of ride-through	mechanism
a technique by allowing small transient violations of safety rules Adding state machines to select speci c safety bounds based on vehicle state accommodates expected control system transients	mechanism
An illustrative example of enforcing a speed limit for an autonomous vehicle Testing these principles on an autonomous utility vehicle	method
Empirical results demonstrate the effectiveness of our proposed approach	finding
In this paper	mechanism
we propose sparse output coding	mechanism
a principled way by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem	mechanism
Specifically sparse output coding is composed of two steps : efficient coding matrix learning with scalability to thousands of classes	mechanism
and probabilistic decoding	mechanism
on object recognition and scene classification	method
Given a multimillion-node social network	background
how can we sum- marize connectivity pattern from the data	background
and how can we find unex- pected user behavior ?	background
We discover that ( a ) the lockstep behavior on the graph shapes dense `` block '' in its adjacency matrix and creates `` ray '' in spectral subspaces	finding
and ( b ) partially overlapping of the behavior shapes `` staircase '' in the matrix and creates `` pearl '' in the subspaces	finding
We demonstrate that our approach is effective	finding
The second contribution is that we provide a fast algorithm	mechanism
using the discovery as a guide for practi- tioners	mechanism
Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep	method
on both synthetic and real data	method
our model can be used to effectively explore a citation network and provide meaningful explanations for links while still maintaining competitive citation prediction performance	finding
In this paper	mechanism
we present a novel model that integrates the merits of content and citation analyses into a single probabilistic framework	mechanism
We demonstrate our model on three real-world citation networks	method
Compared with existing baselines	method
Automatically recognizing a large number of action categories from videos is of significant importance for video understanding	background
validate the superiority of our method over fully-supervised approaches using few positive exemplars	finding
we propose to perform action recognition when no positive exemplars of that class are provided	mechanism
which is often known as the zero-shot learning	mechanism
Different from other zero-shot learning approaches	mechanism
which exploit attributes as the intermediate layer for the knowledge transfer	mechanism
our main contribution is SIR	mechanism
which directly leverages the semantic inter-class relationships between the known and unknown actions followed by label transfer learning	mechanism
The inter-class semantic relationships are automatically measured by continuous word vectors	mechanism
which learned by the skip-gram model using the large-scale text corpus	mechanism
Extensive experiments on the UCF101 dataset	method
With the goal of improving the quality of life for people suffering from various motor control disorders	background
brain-machine interfaces provide direct neural control of prosthetic devices by translating neural signals into control signals	background
These systems act by reading motor intent signals directly from the brain and using them to control	background
for example the movement of a cursor on a computer screen	background
Over the past two decades	background
much attention has been devoted to the decoding problem : how should recorded neural activity be translated into the movement of the cursor ? These results have implications for understanding how motor neurons are recruited to perform various tasks	background
and may lend insight into the brain 's ability to conceptualize artificial systems	background
This framework leads to new interpretations of why certain types of decoders have been shown to perform better than others	finding
Here we recast the decoder design problem from a physical control system perspective	mechanism
and	mechanism
We propose a novel method The proposed method considers positive	mechanism
implicit and negative information of all users in a network based on belief propagation to predict trust relationships of a target user	mechanism
Given the retweeting activity for the posts of several Twitter users	background
how can we distinguish organic activity from spammy retweets by paid followers to boost a post 's appearance of popularity ? More gen- erally	background
given groups of observations	background
can we spot strange groups ?	background
Our method achieves a 97 % accuracy on a real dataset of 12 million retweets crawled from Twitter	finding
Here we propose : ( A ) ND-Sync	mechanism
an efficient method	mechanism
and ( B ) a set of carefully designed features ND-Sync is effec- tive in spotting retweet fraudsters	mechanism
robust to different types of abnormal activity	mechanism
and adaptable as it can easily incorporate additional features	mechanism
is represented in image data In one embodiment	mechanism
age-estimation techniques involves combining a Contourlet Appearance Model ( CAM ) for facial-age feature extraction and Support Vector Regression ( SVR ) for learning aging rules	mechanism
In a particular example	method
characteristics of input facial images are converted to feature vectors by CAM	method
then these feature vectors are analyzed by an aging-mechanism-based classifier to estimate whether the images represent faces of younger or older people prior to age-estimation	method
the aging-mechanism-based classifier being generated in one embodiment by running Support Vector Machines ( SVM ) on training images	method
In an exemplary binary youth/adult classifier	method
faces classified as adults are passed to an adult age-estimation function and the others are passed to a youth age-estimation function	method
Refactoring of code is a common device in software engineering	background
As cyber-physical systems CPS become ever more complex	background
similar engineering practices become more common in CPS development	background
Proper safe developments of CPS designs are accompanied by a proof of correctness	background
For some of these we can give strong results that they are correct	finding
we develop proof-aware refactorings for CPS	mechanism
That is we study model transformations on CPS and show how they correspond to relations on correctness proofs	mechanism
As the main technical device	mechanism
we show how the impact of model transformations on correctness can be characterized by different notions of refinement in differential dynamic logic Furthermore	mechanism
we demonstrate the application of refinements on a series of safety-preserving and liveness-preserving refactorings	mechanism
by proving on a meta-level Where this is impossible	method
we construct proof obligations for showing that the refactoring respects the refinement relation	method
Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender 's randomized assignment of resources to targets	background
implies that in some realistic situations	background
limited surveillance may not need to be explicitly addressed	background
that in zero-sum security games	finding
lazy defenders who simply keep optimizing against perfectly informed attackers	finding
are almost optimal against diligent attackers	finding
who go to the effort of gathering a reasonable number of observations	finding
This result	finding
We analytically demonstrate	method
Motivated by the success of CNNs in object recognition on images	background
researchers are striving to develop CNN equivalents for learning video features	background
results show competitive performance	finding
Therefore we propose to leverage effective techniques from both data-driven and data-independent approaches to improve action recognition system	mechanism
Our contribution is three-fold	mechanism
First we explicitly show that local handcrafted features and CNNs share the same convolution-pooling network structure	mechanism
Second we propose to use independent subspace analysis ( ISA ) to learn descriptors for state-of-the-art handcrafted features	mechanism
Third we enhance ISA with two new improvements	mechanism
which make our learned descriptors significantly outperform the handcrafted ones	mechanism
Experimental on standard action recognition benchmarks	method
Methods and software A supply and demand balancing scheme is used across a network of agents to facilitate agreement on the optimal incremental price for energy provision in an electric power network subject to the constraint that the total generation in the electric power network matches the total network demand A multi-step optimization approach is provided that incorporates inter-temporal constraints	mechanism
allowing for optimal integration of flexible power loads and power storage entities and taking into account power generation ramp rate constraints at individual generation entities The approach is extended to cope with line flow constraints imposed by the physical system topology and transmission line limits/capacities	mechanism
Designers of human computation systms often face the need to aggregate noisy information provided by multiple people	background
Our short-term goal is to motivate the design of better human computation systems ; our long-term goal is to spark an interaction between researchers in ( computational ) social choice and human computation	background
Our empirical conclusions show that noisy human voting can differ from what popular theoretical models would predict	finding
We conduct extensive experiments on Amazon Mechanical Turk	method
The Poisson distribution has been widely studied and used for modeling univariate count-valued data	background
Multivariate generalizations of the Poisson distribution that permit dependencies	background
however have been far less popular	background
Finally we suggest new research directions as explored in the subsequent discussion section	background
These empirical experiments develop intuition about the comparative advantages and disadvantages of each class of multivariate distribution that was derived from the Poisson	finding
compare the models in terms of interpretability and theory Then	method
we empirically compare multiple models from each class on three real-world datasets that have varying data characteristics from different domains	method
namely traffic accident data	method
biological next generation sequencing data	method
and text data	method
More generally the approach could be used to identify malleable components of cognitive functions	background
such as spatial reasoning or executive functions	background
we find that the component models provide both better predictions and better explanations than the faculty models	finding
Weak model variations tend to improve generalization across students	finding
but hurt generalization across items and make a sacrifice to explanatory power	finding
we develop statistical models of them These models use latent variables Strong versions of these models provide a common explanation for the variance in task difficulty and transfer	mechanism
Weak versions decouple difficulty and transfer explanations by describing task difficulty with parameters for each unique task	mechanism
We analyze naturally occurring datasets from student use of educational technologies We contrast a faculty theory of broad transfer with a component theory of more constrained transfer We evaluate these models in terms of both their prediction accuracy on held-out data and their power in explaining task difficulty and learning transfer	method
In comparisons across eight datasets	method
Current methods for depression assessment depend almost entirely on clinical interview or self-report ratings	background
These findings suggest that automatic detection of depression from behavioral indicators is feasible and that multimodal measures afford most powerful detection	background
The core premise of evidence-based medicine is that clinical decisions are informed by the peer-reviewed literature and discuss the implication of this form of bias as it pertains to evidence-based medicine	background
We performed an exhaustive search that identified articles exploring the question of whether survival benefit was associated with maximal high-grade glioma ( HGG ) resection	method
Fluorescence microscopy is one of the most important tools in cell biology research because it provides spatial and temporal information to investigate regulatory systems inside cells This technique can generate data in the form of signal intensities at thousands of positions resolved inside individual live cells	background
However given extensive cell-to-cell variation	background
these data can not be readily assembled into three- or four-dimensional maps of protein concentration that can be compared across different cells and conditions	background
We have developed a method and applied it Antigen recognition in T cells by the T cell receptor ( TCR ) is amplified by engagement of the costimulatory receptor CD28	mechanism
We imaged actin and eight core actin regulators to generate over a thousand movies of T cells under conditions in which CD28 was either engaged or blocked in the context of a strong TCR signal	mechanism
Our computational analysis	method
One goal of human genetics is to understand the genetic basis of disease	background
a challenge for diseases of complex inheritance because risk alleles are few relative to the vast set of benign variants	background
Risk variants are often sought by association studies in which allele frequencies in case subjects are contrasted with those from population-based samples used as control subjects	background
If such a resource were to exist	background
it would yield ample savings and would facilitate the effective use of data repositories by removing administrative and technical barriers	background
These results highlight how UNICORN can enable reliable	background
powerful and convenient genetic association analyses without access to the individual-level data	background
We call this concept the Universal Control Repository Network ( UNICORN )	mechanism
a means Our approach to UNICORN uses existing genetic resources and various statistical tools to analyze these data	mechanism
including hierarchical clustering with spectral analysis of ancestry ; and empirical Bayesian analysis along with Gaussian spatial processes to estimate ancestry-specific allele frequencies	mechanism
We demonstrate our approach using tens of thousands of control subjects from studies of Crohn disease	method
indicate that SMs reveal more PTSD symptoms to the VH than they report on the Post Deployment Health Assessment Pre/Post deployment facial expression analysis indicated more sad expressions and few happy expressions at post deployment	finding
SimSensei is a Virtual Human ( VH ) interviewing platform that uses off-the-shelf sensors ( i	mechanism
e	mechanism
webcams Microsoft Kinect and a microphone ) to capture and interpret real-time audiovisual behavioral signals from users interacting with the VH system	mechanism
The system was specifically designed for clinical interviewing and health care support by providing a face-to-face interaction between a user and a VH that can automatically react to the inferred state of the user through analysis of behavioral signals gleaned from the user 's facial expressions	mechanism
body gestures and vocal parameters	mechanism
Akin to how non-verbal behavioral signals have an impact on human-to-human interaction and communication	mechanism
Results from of sample of service members ( SMs ) who were interviewed before and after a deployment to Afghanistan	method
Multifunctional polymer-based composites have been widely used in various research and industrial applications	background
such as flexible and stretchable electronics and sensors and sensor-integrated smart structures This work may provide rational methods for the fabrication of aligned composites	background
by embedding liquid-metal inclusions in an elastomer matrix	mechanism
The elasticity electrostatics	method
and electromechanical coupling of the composite are investigated	method
Maximum-a-Posteriori ( MAP ) inference lies at the heart of Graphical Models and Structured Prediction	background
Despite the intractability of exact MAP inference	background
approximate methods based on LP relaxations have exhibited superior performance across a wide range of applications	background
In this paper	mechanism
we introduce an effective MAP inference method for problems with large output domains	mechanism
The method builds upon alternating minimization of an Augmented Lagrangian that exploits the sparsity of messages through greedy optimization techniques	mechanism
A key feature of our greedy approach is to introduce variables in an on-demand manner with a pre-built data structure over local factors	mechanism
In addition we introduce a variant of GDMM for binary MAP inference problems with a large number of factors	mechanism
Empirically	method
Recent studies have shown that brain-machine interfaces ( BMIs ) offer great potential for restoring upper limb function	background
We describe a method of shared control where the user controls a prosthetic arm using a BMI and receives assistance with positioning the hand when it approaches an object	mechanism
Complex networks have been shown to exhibit universal properties	background
with one of the most consistent patterns being the scale-free degree distribution	background
by identifying another power-law pattern that describes the relationship between the fractions of node pairs C ( r ) within r hops and the hop count r	mechanism
This scale-free distribution is pervasive and describes a large variety of networks	mechanism
ranging from social and urban to technological and biological networks	mechanism
In particular inspired by the definition of the fractal correlation dimension D2 on a point-set	mechanism
we consider the hop-count r to be the underlying distance metric between two vertices of the network	mechanism
and we examine the scaling of C ( r ) with r	mechanism
We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h	mechanism
under successful existing network models	method
while we analyze a large set of real and synthetic network datasets and	method
Eumelanins are extended heterogeneous biopolymers composed of molecular subunits with ambiguous macromolecular topology	background
which suggests that natural eumelanin pigments contain indole-based tetramers that are arranged into porphyrin-like domains	finding
suggest that sodium ions undergo occupancy-dependent stepwise insertion into the core of porphyrin-like tetramers in natural eumelanins at discrete potentials	finding
Here an electrochemical fingerprinting technique is described	mechanism
Spectroscopy and density functional theory calculations	method
High throughput screening determines the effects of many conditions on a given biological target	background
Currently to estimate the effects of those conditions on other targets requires either strong modeling assumptions ( e	background
g	background
similarities among targets ) or separate screens	background
The results represent the first practical demonstration of the utility of active learning-driven biological experimentation in which the set of possible phenotypes is unknown in advance	background
We have previously described an active machine learning algorithm that can iteratively choose small sets of experiments to learn models of multiple effects	mechanism
We now show that	method
with no prior knowledge and with liquid handling robotics and automated microscopy under its control	method
Despite the enormous medical impact of cancers and intensive study of their biology	background
detailed characterization of tumor growth and development remains elusive	background
This difficulty occurs in large part because of enormous heterogeneity in the molecular mechanisms of cancer progression	background
both tumor-to-tumor and cell-to-cell in single tumors Advances in genomic technologies	background
especially at the single-cell level	background
are improving the situation	background
but these approaches are held back by limitations of the biotechnologies for gathering genomic data from heterogeneous cell populations and the computational methods for making sense of those data	background
One popular way to gain the advantages of whole-genome methods without the cost of single-cell genomics has been the use of computational deconvolution ( unmixing ) methods to reconstruct clonal heterogeneity from bulk genomic data	background
a key step in the process of accurately deconvolving tumor genomic data and inferring clonal heterogeneity from bulk data	background
that this new method substantially improves our ability to resolve discrete tumor subgroups	finding
Here we present a new method by better identifying subspaces corresponding to tumors produced from mixtures of distinct combinations of clonal subpopulations We develop a nonparametric clustering method based on medoidshift clustering for identifying subgroups of tumors expected to correspond to distinct trajectories of evolutionary progression	mechanism
We show on synthetic and real tumor copy-number data	method
In eukaryotic cells	background
mitochondria form a dynamic interconnected network to respond to changing needs at different subcellular locations	background
we developed high-resolution computational image analysis techniques	mechanism
to examine the relations between mitochondrial fusion/fission and spatial distribution within the axon of Drosophila larval neurons	method
Bayesian theory has provided a compelling conceptualization for perceptual inference in the brain	background
Central to Bayesian inference is the notion of statistical priors	background
To understand the neural mechanisms of Bayesian inference	background
we need to understand the neural representation of statistical regularities in the natural environment	background
They also suggest that the Boltzmann machine can be a viable model for conceptualizing computations in the visual cortex and	background
as such can be used to predict neural circuits in the visual cortex from natural scene statistics	background
and found that the units in the model exhibited cooperative and competitive interactions	finding
forming a `` disparity association field ''	finding
analogous to the contour association field	finding
The cooperative and competitive interactions in the disparity association field are consistent with constraints of computational models for stereo matching	finding
and found the results to be consistent with neurophysiological data in terms of the functional connectivity measurements between disparity-tuned neurons in the macaque primary visual cortex	finding
These findings demonstrate that there is a relationship between the functional connectivity observed in the visual cortex and the statistics of natural scenes	finding
We applied a Boltzmann machine model	mechanism
In addition we simulated neurophysiological experiments on the model	method
Here we adapt an electron microscopy technique that selectively labels synapses	mechanism
in combination with a machine-learning algorithm for semiautomated synapse detection	mechanism
Synapse density and length were compared across development and during whisker-evoked plasticity	method
Targeted electrophysiological analysis of changes in miniature EPSC and IPSC properties in L2 pyramidal neurons	method
In male but not female infants	finding
angular displacement increased from Play to Still-Face and decreased from Still Face to Reunion	finding
Infant angular velocity was higher during Still-Face than Reunion with no differences between male and female infants Windowed cross-correlation suggested changes in how infant and mother head movements are associated	finding
revealing dramatic changes in direction of association	finding
Coordination between mother and infant head movement velocity was greater during Play compared with Reunion	finding
Characterizing the spatial distribution of proteins directly from microscopy images is a difficult problem with numerous applications in cell biology ( e	background
g	background
identifying motor-related proteins ) and clinical research ( e	background
g	background
identification of cancer biomarkers )	background
Such models are expected to be valuable for representing and summarizing each pattern and for constructing systems biology simulations of cell behaviors	background
We were able to show that these patterns could be distinguished from each other with high accuracy	finding
and we were able to assign to one of these subclasses hundreds of proteins whose subcellular localization had not previously been well defined	finding
These films are composed of polydimethylsiloxane ( PDMS ) embedded with vertically aligned columns of ferromagnetic Ag-Ni microparticles	mechanism
The microparticles are magnetically aligned and support electrical conductivity only through the thickness ( z-axis ) of the elastomer film	mechanism
This lack of consistency will affect future research on the clinical significance of snoring	background
Ten subjects reporting habitual snoring were included in the study	method
performed at Landspitali-University Hospital	method
Iceland	method
Snoring was assessed by listening to the air medium microphone located on a patient 's chest	method
compared to listening to two overhead air medium microphones ( stereo ) and manual scoring of a piezoelectric sensor and nasal cannula vibrations The sensitivity and positive predictive value of scoring snore events from the different sensors was compared to the chest audio	method
Games for health ( G4H ) aim to improve health outcomes and encourage behavior change While existing theoretical frameworks describe features of both games and health interventions	background
We discuss how this work can be applied to provide conceptual tools	background
improve the G4H design process	background
and guide approaches to encoding G4H-related data for large-scale empirical analysis	background
We found evidence of conceptual differences suggesting that a G4H perspective is not simply the sum of game and health perspectives At the same time	finding
we found evidence of convergence in stakeholder views	finding
including areas where game experts provided insights about health and vice versa	finding
Both the occurrence and intensity of facial expressions are critical to what the face reveals	background
While much progress has been made towards the automatic detection of facial expression occurrence	background
controversy exists about how to estimate expression intensity	background
The most straight-forward approach is to train multiclass or regression models using intensity ground truth	background
However collecting intensity ground truth is even more time consuming and expensive than collecting binary ground truth	background
However if they do so	finding
high reliability with expert human coders can be achieved	finding
Intensity-trained multiclass and regression models outperformed binary-trained classifier decision values on smile intensity estimation Multiclass models even outperformed binary-trained classifiers	finding
across multiple databases and methods for feature extraction and dimensionality reduction	method
on smile occurrence detection	method
Laser photocoagulation is a mainstay or adjuvant treatment for a variety of common retinal diseases	background
The authors introduce an automated laser photocoagulation system for intraocular surgery	mechanism
based on a novel handheld instrument	mechanism
As sampling-based motion planners become faster	background
they can be re-executed more frequently by a robot during task execution to react to uncertainty in robot motion	background
obstacle motion sensing noise	background
and uncertainty in the robot 's kinematic model	background
We show that	finding
as parallel computation power increases	finding
HFR offers asymptotic optimality for these objectives during each period for goal-oriented problems	finding
We then demonstrate the effectiveness of HFR	finding
where during each period	mechanism
fast sampling-based motion planners are executed in parallel as the robot simultaneously executes the first action of the best motion plan from the previous period	mechanism
We consider discrete-time systems with stochastic nonlinear ( but linearizable ) dynamics and observation models with noise drawn from zero mean Gaussian distributions	mechanism
for holonomic and nonholonomic robots including car-like vehicles and steerable medical needles	method
Nationally sponsored cancer-care quality-improvement efforts have been deployed in community health centers to increase breast	background
cervical and colorectal cancer-screening rates among vulnerable populations Despite several immediate and short-term gains	background
screening rates remain below national benchmark objectives	background
Overall improvement has been both difficult to sustain over time in some organizational settings and/or challenging to diffuse to other settings as repeatable best practices	background
Reasons for this include facility-level changes	background
which typically occur in dynamic organizational environments that are complex	background
adaptive and unpredictable	background
This study applies a computational-modeling approach	method
combining principles of health-services research	method
health informatics network theory	method
and systems science	method
Management decisions underpinning availability of ecosystem services and the organisms that provide them in agroecosystems	background
such as pollinators and pollination services	background
have emerged as a foremost consideration for both conservation and crop production goals	background
There is growing evidence that innovative management practices can support diverse pollinators and increase crop pollination	background
Robust efficient and low-cost networks are advantageous in both biological and engineered systems	background
During neural network development in the brain	background
synapses are massively over-produced and then pruned-back over time	background
This strategy is not commonly used when designing engineered networks	background
since adding connections that will soon be removed is considered wasteful	background
and found that the rate is decreasing over time	finding
and show that decreasing rates lead to more robust and efficient networks compared to other rates	finding
Thus inspiration from neural network formation suggests effective ways to design distributed networks across several domains	finding
We also present an application of this strategy	mechanism
We first used high-throughput image analysis techniques to quantify the rate of pruning in the mammalian neocortex across a broad developmental time window Based on these results	method
we analyzed a model of computational routing networks using both theoretical analysis and simulations	method
There are likely marked differences in endotracheal intubation ( ETI ) techniques between novice and experienced providers	background
We performed a proof of concept study	method
offered positive indications towards this concept	finding
In our research	mechanism
we propose a portfolio of serious games Our system provides a collection of mini games based on rehabilitation exercises used in conventional physical therapy	mechanism
monitors the patient 's performance while exercising and provides clinicians with an interface to personalize the training	mechanism
The clinician can set the current state of rehabilitation and change the playable games over time to drive diversification	mechanism
While the system still has to be evaluated	method
an early stage case study with one patient	method
Analogous to genomic sequence alignment	background
biological network alignment identifies conserved regions between networks of different species	background
Then function can be transferred from well- to poorly-annotated species between aligned network regions	background
Network alignment typically encompasses two algorithmic components : node cost function ( NCF )	background
which measures similarities between nodes in different networks	background
and alignment strategy ( AS )	background
which uses these similarities to rapidly identify high-scoring alignments	background
Different methods use both different NCFs and different ASs	background
Thus it is unclear whether the superiority of a method comes from its NCF	background
its AS or both	background
We already showed on state-of-the-art methods	background
MI-GRAAL and IsoRankN	background
that combining NCF of one method and AS of another method can give a new superior method	background
Existing methods determine this parameter more-less arbitrarily	background
which could affect alignment quality	background
Existing methods assume that the larger the neighborhood size	background
the better	background
GHOST by mixing-and-matching the methods ' NCFs and ASs	mechanism
Platinum ( Pt ) drugs are the most potent and commonly used anti-cancer chemotherapeutics	background
Nanoformulation of Pt drugs has the potential to improve the delivery to tumors and reduce toxic side effects	background
In studying the strength and specificity of interaction between members of two protein families	background
key questions center on which pairs of possible partners actually interact	background
how well they interact	background
and why they interact while others do not	background
We develop here a method	mechanism
DgSpi ( data-driven graphical models of specificity in protein : protein interactions )	mechanism
based on data from MacBeath and colleagues	method
It is often assumed that central pattern generators	background
which generate rhythmic patterns without rhythmic inputs	background
play a key role in the spinal control of human locomotion suggest feedback integration to be functionally more important than central pattern generation in human locomotion across behaviours	background
In addition the proposed control architecture may serve as a guide in the search for the neurophysiological origin and circuitry of spinal control in humans	background
The results	finding
We propose a neural control model in which the spinal control generates muscle stimulations mainly through integrated reflex pathways with no central pattern generator Using a physics-based neuromuscular human model	mechanism
we show that this control network is sufficient including walking and running	mechanism
acceleration and deceleration	mechanism
slope and stair negotiation	mechanism
turning and deliberate obstacle avoidance	mechanism
We compared the performance of the automated scanning using various control thresholds	method
in order to find the most effective threshold in terms of accuracy and speed we conducted the handheld operation above a fixed target surface	method
We tested these mechanisms using a multimodule snake robot as a physical model	mechanism
successfully generating differential and reversal turning with performance comparable to that of the organisms	mechanism
The assessment of jaundice in outpatient neonates is problematic	background
Visual assessment is inaccurate	background
and more exact methodologies are cumbersome and/or expensive	background
using a smartphone application called BiliCam	method
Gene therapies have emerged as a promising treatment for congestive heart failure Prior work on controlling the movement of Cerberus required accurate knowledge of device geometry	background
we developed Cerberus	mechanism
a minimally invasive parallel wire robot	mechanism
this paper presents work on developing an auto-calibration procedure using force sensors to move injector	mechanism
Recent studies implicate chromatin modifiers in autism spectrum disorder ( ASD ) through the identification of recurrent de novo loss of function mutations in affected individuals	background
CHD8 targets are strongly enriched for other ASD risk genes in both human and mouse neurodevelopment	finding
and converge in ASD-associated co-expression networks in human midfetal cortex CHD8 knockdown in hNSCs results in dysregulation of ASD risk genes directly targeted by CHD8	finding
Integration of CHD8-binding data into ASD risk models improves detection of risk genes These results suggest loss of CHD8 contributes to ASD by perturbing an ancient gene regulatory network during human brain development	finding
we identify genes targeted by CHD8	method
a chromodomain helicase strongly associated with ASD	method
in human midfetal brain	method
human neural stem cells ( hNSCs ) and embryonic mouse cortex	method
Seven subjects participated in laboratory-based usability sessions to evaluate the physical design	method
appearance functionality and perceived ease of use of a multi-user telehealth kiosk prototype	method
During usability testing	method
The quantitative relationship between presynaptic calcium influx and transmitter release critically depends on the spatial coupling of presynaptic calcium channels to synaptic vesicles	background
When there is a close association between calcium channels and synaptic vesicles	background
the flux through a single open calcium channel may be sufficient to trigger transmitter release	background
Furthermore calcium ion flux through this channel has a low probability of triggering synaptic vesicle fusion ( _6 % )	finding
even when multiple channels open in a single active zone	finding
These mechanisms work to control the rare triggering of vesicle fusion in the frog neuromuscular junction from each of the tens of thousands of individual release sites at this large model synapse	finding
Here we used a combination of pharmacological calcium channel block	method
high-resolution calcium imaging	method
postsynaptic recording and 3D Monte Carlo reaction-diffusion simulations in the adult frog neuromuscular junction	method
Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data	background
For example they are often used to explore complex systems where connections between entities are not well understood	background
such as in functional brain networks or genetic networks	background
Extensive simulation studies from gene and protein profiles positron emission tomography data	method
Individuals who exhibit large-magnitude blood pressure ( BP ) reactions to acute psychological stressors are at risk for hypertension and premature death by cardiovascular disease	background
providing novel evidence for a candidate neurophysiological source of stress-related cardiovascular risk	finding
Of course for testing the significance of an additional variable between two nested linear models	background
one typically uses the chi-squared test	background
comparing the drop in residual sum of squares ( RSS ) to a [ Formula : see text ] distribution	background
Recently there has been substantial interest in spectral methods for learning dynamical systems	background
These methods are popular since they often offer a good tradeoff between computational and statistical efficiency	background
the correctness of these instances follows directly from our general analysis	finding
we present a new view of dynamical system learning : we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems	mechanism
thereby allowing users to incorporate prior knowledge via standard techniques such as L1 regularization Many existing spectral methods are special cases of this new framework	mechanism
using linear regression as the supervised learner	mechanism
We demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does ;	method
While studies show that autism is highly heritable	background
the nature of the genetic basis of this disorder remains illusive	background
The proposed modeling framework can be naturally extended to incorporate additional structural information concerning the dependence between genes	background
Based on the idea that highly correlated genes are functionally interrelated and more likely to affect risk	mechanism
we develop a novel statistical tool by combining the genetic association scores with gene co-expression in specific brain regions and periods of development	mechanism
The gene dependence network is estimated using a novel partial neighborhood selection ( PNS ) algorithm	mechanism
where node specific properties are incorporated into network estimation for improved statistical and computational efficiency Then we adopt a hidden Markov random field ( HMRF ) model to combine the estimated network and the genetic association scores in a systematic manner	mechanism
Using currently available genetic association data from whole exome sequencing studies and brain gene expression levels	method
Reconstructing regulatory and signaling response networks is one of the major goals of systems biology	background
Our multi-task learning method was able to identify known and novel factors and genes	finding
improving upon prior methods that model each condition independently	finding
The MT-SDREM networks were also better at identifying proteins whose removal affects viral load indicating that joint learning can still lead to accurate	finding
condition-specific networks	finding
Supporting website with MT-SDREM implementation : http : //sb	finding
cs	finding
cmu	finding
edu/mtsdrem	finding
we developed MT-SDREM	mechanism
a multi-task learning method which jointly models networks for several related conditions	mechanism
In MT-SDREM parameters are jointly constrained across the networks while still allowing for condition-specific pathways and regulation	mechanism
We applied MT-SDREM to reconstruct dynamic human response networks for three flu strains : H1N1	method
H5N1 and H3N2	method
Methods to assess individual facial actions have potential to shed light on important behavioral phenomena ranging from emotion and social interaction to psychological disorders and health	background
However manual coding of such actions is labor intensive and requires extensive training	background
To date establishing reliable automated coding of unscripted facial actions has been a daunting challenge impeding development of psychological theories and applications requiring facial expression assessment These findings suggest automated FACS coding has progressed sufficiently to be applied to observational research in emotion and related areas of study	background
Autism is a psychiatric/neurological condition in which alterations in social interaction ( among other symptoms ) are diagnosed by behavioral psychiatric methods	background
The approach is based on previous advances in fMRI analysis methods that permit ( a ) the identification of a concept	background
such as the thought of a physical object	background
from its fMRI pattern	background
and ( b ) the ability to assess the semantic content of a concept from its fMRI pattern The findings suggest that psychiatric alterations of thought can begin to be biologically understood by assessing the form and content of the altered thought 's underlying brain activation patterns	background
and machine learning methods	mechanism
Given a simple noun such as apple	background
and a question such as `` Is it edible ?	background
'' what processes take place in the human brain ?	background
even though originating from the field of neuroscience to provide neuroscientific insights toward a better understanding of the way that neurons interact with each other	background
GeBM produces brain activity patterns that are strikingly similar to the real ones	finding
where the inferred functional connectivity is able as well as detect regularities and outliers in multisubject brain activity measurements	finding
In this work	mechanism
we present a simple	mechanism
novel good-enough brain model	mechanism
or GeBM in short	mechanism
and a novel algorithm Sparse-SysId	mechanism
which are Moreover	mechanism
GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text )	mechanism
We evaluate GeBM by using real brain data	method
Story understanding involves many perceptual and cognitive subprocesses	background
from perceiving individual words	background
to parsing sentences	background
to understanding the relationships among the story characters Additionally	background
this approach is promising for studying individual differences : it can be used to create single subject maps that may potentially be used to measure reading comprehension and diagnose reading disorders	background
We present an integrated computational model of reading that incorporates these and additional subprocesses	mechanism
simultaneously discovering their fMRI signatures Our model predicts the fMRI activity associated with reading arbitrary text passages	mechanism
well enough This approach is the first to ranging from visual word properties to the mention of different story characters and different actions they perform We construct brain representation maps that replicate many results from a wide range of classical studies that focus each on one aspect of language processing and offer new insights on which type of information is processed by different areas involved in language processing	mechanism
The manipulation performance was investigated in both clamped and handheld conditions In positioning experiments with varying side loads	method
In studying the strength and specificity of interaction between members of two protein families	background
The advent of large-scale experimental studies of interactions between members of a target family and a diverse set of possible interaction partners offers the opportunity to address these questions	background
We develop here a method	mechanism
DgSpi ( Data-driven Graphical models of Specificity in Protein : protein Interactions )	mechanism
based on data from MacBeath and colleagues	method
Computer-mediated communication is driving fundamental changes in the nature of written language	background
The results of this analysis offer support for prior arguments that focus on geographical proximity and population size	finding
However demographic similarity - especially with regard to race - plays an even more central role	finding
as cities with similar racial demographics are far more likely to share linguistic influence Rather than moving towards a single unified `` netspeak '' dialect	finding
language evolution in computer-mediated communication reproduces existing fault lines in spoken American English	finding
Using a latent vector autoregressive model to aggregate across thousands of words Our model is robust to unpredictable changes in Twitter 's sampling rate	mechanism
and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors	mechanism
Many statistical methods gain robustness and flexibility by sacrificing convenient computational structures	background
Though this paper focuses on the problem of graph estimation	background
the proposed methodology is widely applicable to other problems with similar structures	background
We also report results	finding
We explain how novel computational techniques In particular	mechanism
we propose a nonparanormal neighborhood pursuit algorithm with theoretical guarantees	mechanism
Moreover we provide an alternative view under a smoothing optimization framework	mechanism
thorough experimental on text	method
stock and genomic datasets	method
While only recently developed	background
the ability to profile expression data in single cells ( scRNA-Seq ) has already led to several important studies and findings	background
However this technology has also raised several new computational challenges	background
Such database queries ( which can be performed using our web server ) will enable researchers to better characterize cells when analyzing heterogeneous scRNA-Seq samples	background
We show that the NN method improves upon prior methods in both	finding
the ability to correctly group cells in experiments not used in the training and the ability to correctly infer cell type or state by querying a database of tens of thousands of single cell profiles	finding
we develop and test a method based on neural networks ( NN )	mechanism
and used these to obtain a reduced dimension representation of the single cell expression data	mechanism
We tested various NN architectures	method
some of which incorporate prior biological knowledge	method
Limbless organisms such as snakes can navigate nearly all terrain	background
In particular desert-dwelling sidewinder rattlesnakes ( Crotalus cerastes ) operate effectively on inclined granular media ( such as sand dunes ) that induce failure in field-tested limbless robots through slipping and pitching	background
Together these three approaches demonstrate how sidewinding with contact-length control mitigates failure on granular media	background
reveal that as granular incline angle increases	finding
sidewinder rattlesnakes increase the length of their body in contact with the sand demonstrate that granular yield stresses decrease with increasing incline angle	finding
Implementing this strategy in a physical robot model of the snake	mechanism
Our laboratory experiments Plate drag experiments	method
Whole-exome sequencing ( WES ) studies have demonstrated the contribution of de novo loss-of-function single-nucleotide variants ( SNVs ) to autism spectrum disorder ( ASD )	background
Fluctuations in the growth rate of a bacterial culture during unbalanced growth are generally considered undesirable in quantitative studies of bacterial Our method has implications for both basic understanding of bacterial physiology and for the classification of bacterial strains	background
Here we present a method '' by time-frequency analysis of unbalanced growth curves measured with high temporal resolution	mechanism
The signatures are then applied to differentiate amongst different bacterial strains or the same strain under different growth conditions	mechanism
and to identify the essential architecture of the gene network underlying the observed growth dynamics	mechanism
Spontaneously arising ( de novo ) mutations have an important role in medical genetics	background
For diseases with extensive locus heterogeneity	background
such as autism spectrum disorders ( ASDs )	background
the signal from de novo mutations is distributed across many genes	background
suggesting that the role of de novo mutations in ASDs might reside in fundamental neurodevelopmental processes	background
Here we provide a statistical framework for the analysis of excesses in de novo mutation per gene and gene set by calibrating a model of de novo mutation	mechanism
Influence maximization in social networks has been widely studied motivated by applications like spread of ideas or innovations in a network and viral marketing of products	background
Current studies focus almost exclusively on unsigned social networks containing only positive relationships ( e	background
g	background
friend or trust ) between users	background
We prove that the influence function of the PRIM problem under the IC-P model is monotonic and submodular Thus	finding
a greedy algorithm can be used to achieve an approximation ratio of 1-1/e for solving the PRIM problem in signed social networks	finding
validate that our approximation algorithm for solving the PRIM problem outperforms state-of-the-art methods	finding
Thus in this paper	mechanism
we propose the polarity-related influence maximization ( PRIM ) problem	mechanism
we first extend the standard Independent Cascade ( IC ) model to the signed social networks and propose a Polarity-related Independent Cascade ( named IC-P ) diffusion model	mechanism
Experimental results on two signed social network datasets	method
Epinions and Slashdot	method
The HMT3522 progression series of human breast cells have been used to discover how tissue architecture	background
microenvironment and signaling molecules affect breast cell growth and behaviors	background
We found that different breast cell states contain distinct gene networks	finding
The network specific to non-malignant HMT3522-S1 cells is dominated by genes involved in normal processes	finding
whereas the T4-2-specific network is enriched with cancer-related genes The networks specific to various conditions of the reverted T4-2 cells are enriched with pathways suggestive of compensatory effects	finding
showed that aberrant expression values of certain hubs in the identified networks are associated with poor clinical outcomes	finding
Thus analysis of various reversion conditions ( including non-reverted ) of HMT3522 cells using Treegl can be a good model system to study drug effects on breast cancer	finding
We employed a `` pan-cell-state '' strategy	mechanism
and analyzed jointly microarray profiles obtained from different state-specific cell populations from this progression and reversion model of the breast cells using a tree-lineage multi-network inference algorithm	mechanism
Treegl	mechanism
consistent with clinical data showing patient resistance to anticancer drugs	method
We validated the findings using an external dataset	method
and	method
A key component of genetic architecture is the allelic spectrum influencing trait variability	background
For autism spectrum disorder ( herein termed autism )	background
the nature of the allelic spectrum is uncertain	background
Individual risk-associated genes have been identified from rare variation	background
especially de novo mutations From this evidence	background
one might conclude that rare variation dominates the allelic spectrum in autism	background
new methods that distinguish total narrow-sense heritability from that due to common variation	mechanism
Using a unique epidemiological sample from Sweden and synthesis of results from other studies	method
In effort to improve thermal control in minimally invasive cryosurgery	background
the concept of a miniature	background
wireless implantable sensing unit has been developed recently The sensing unit integrates a wireless power delivery mechanism	background
wireless communication means	background
and a sensing core-the subject matter of the current study	background
Stochastic models are increasingly used to study the behaviour of biochemical systems	background
While the structure of such models is often readily available from first principles	background
unknown quantitative features of the model are incorporated into the model as parameters	background
demonstrate its effectiveness	finding
We present a new parameter discovery algorithm that uses simulated annealing	mechanism
sequential hypothesis testing	mechanism
and statistical model checking We apply our technique to a model of glucose and insulin metabolism used for in-silico validation of artificial pancreata and	mechanism
by developing parallel CUDA-based implementation for parameter synthesis in this model	method
Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism	background
The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks	background
around which transcriptional regulation is focused owing to a disease or a genetic perturbation	background
Using a z-score-based scoring scheme	background
RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network	background
The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes	background
We show that MIRA 's results are biologically sound	finding
empirically significant and more reliable than RA	finding
In this article	mechanism
we propose a mutual information-based multivariate reporter algorithm MIRA MIRA is a multivariate and combinatorial algorithm that calculates the aggregate transcriptional response around a metabolite using mutual information	mechanism
Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism	background
The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks	background
around which transcriptional regulation is focused owing to a disease or a genetic perturbation Using a z-score-based scoring scheme	background
RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network	background
The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes	background
We show that MIRA 's results are biologically sound	finding
empirically significant and more reliable than RA	finding
In this article	mechanism
we propose a mutual information-based multivariate reporter algorithm ( MIRA ) with the goal of eliminating the following problems in detecting reporter metabolites : MIRA is a multivariate and combinatorial algorithm that calculates the aggregate transcriptional response around a metabolite using mutual information	mechanism
These phages include representatives of all three virion morphologies	method
The phages also span considerable sequence diversity	method
With the continuous improvement in genotyping and molecular phenotyping technology and the decreasing typing cost	background
it is expected that in a few years	background
more and more clinical studies of complex diseases will recruit thousands of individuals for pan-omic genetic association analyses	background
report some interesting findings	finding
GenAMap is available from http : //sailing	finding
cs	finding
cmu	finding
edu/genamap	finding
We demonstrate the function of GenAMap via a case study of the Brem and Kruglyak yeast dataset	method
and then apply it on a comprehensive eQTL analysis of the NIH heterogeneous stock mice dataset and	method
we present the implementation of hybrid position/force control operating in the sub-tactile force range for a handheld robotic system	mechanism
Results	finding
Compared to the general population and patients with other chronic diseases reviewed here	finding
patients with CLD reported significantly lower health quality status	finding
more bad mental and physical health days	finding
a significant symptom disease burden	finding
and greater activity limitations	finding
They also reported impairment in their ability to work	finding
increased utilization of healthcare services	finding
and greater out of pocket medical costs	finding
Conclusions	finding
CLD patients have significantly impaired HRQoL and greater healthcare utilization compared to the general population and patients with other chronic diseases	finding
Performing micromanipulation and delicate operations in submillimeter workspaces is difficult because of destabilizing tremor and imprecise targeting	background
Accurate micromanipulation is especially important for microsurgical procedures	background
such as vitreoretinal surgery	background
to maximize successful outcomes and minimize collateral damage	background
In this paper	mechanism
we derive a virtual fixture framework for active handheld micromanipulators that is based on high-bandwidth position measurements rather than forces applied to a robot handle	mechanism
For applicability in surgical environments	mechanism
the fixtures are generated in real-time from microscope video during the procedure	mechanism
Additionally we develop motion scaling behavior around virtual fixtures as a simple and direct extension to the proposed framework	mechanism
In more medically relevant experiments of vein tracing and membrane peeling in eye phantoms	method
Population stratification is an important task in genetic analyses	background
It provides information about the ancestry of individuals and can be an important confounder in genome-wide association studies	background
Public genotyping projects have made a large number of datasets available for study	background
However practical constraints dictate that of a geographical/ethnic population	background
only a small number of individuals are genotyped	background
The resulting data are a sample from the entire population	background
found that the accuracy of recovery of population structure is affected to a large extent by the sample used for analysis and how representative it is of the underlying populations	finding
we show that sample selection bias can affect the results of population structure analyses	finding
We demonstrate that such a correction is effective in practice	finding
We develop a mathematical framework in models for population structure and also proposed a correction for sample selection bias using auxiliary information about the sample	mechanism
We examined two commonly used methods for analyses of such datasets	method
ADMIXTURE and EIGENSOFT	method
and Using simulated data and real genotype data from cattle using simulated and real data	method
Other identified temporal structures that were not highlighted in this paper may also be used to gain insights to possible novel mechanisms	background
Importantly the Temp-O workflow is generic ; while we applied it on NSCLC	background
it can be applied in other cancers and diseases	background
Importantly the identified temporal structures are meaningful in the tumor progression of NSCLC as	background
Validating our findings in independent datasets Further	method
on a large independent dataset	method
Matched field processing is a model-based framework for localizing targets in complex propagation environments	background
In underwater acoustics	background
it has been extensively studied for improving localization performance in multimodal and multipath media	background
For guided wave structural health monitoring problems	background
matched field processing has not been widely applied but is an attractive option for damage localization due to equally complex propagation environments	background
and demonstrates its localization performance by distinguishing two nearby scatterers the data-driven matched field processing framework is shown to successfully localize two nearby scatterers with significantly smaller localization errors and finer resolutions	finding
this paper introduces data-driven matched field processing	mechanism
a framework and then use these models This paper presents the data-driven framework	mechanism
analyzes its behavior under unmodeled multipath interference from experimental measurements of an aluminum plate	method
Compared with delay-based models that are commonly used in structural health monitoring	method
Recent technological advances coupled with large sample sets have uncovered many factors underlying the genetic basis of traits and the predisposition to complex disease	background
but much is left to discover	background
A common thread to most genetic investigations is familial relationships	background
Close relatives can be identified from family records	background
and more distant relatives can be inferred from large panels of genetic markers Finally	background
while our methods have been developed for refining genetic relationship matrices and improving estimates of heritability	background
they have much broader potential application in statistics Most notably	background
for error-in-variables random effects models and settings that require regularization of matrices with block or hierarchical structure	background
we show that smoothing leads to better estimates of the relatedness amongst distantly related individuals	finding
We illustrate our method We show that by using smoothed relationship matrices we can estimate heritability using population-based samples	finding
We propose a new method by exploiting the underlying structure due to hierarchical groupings of correlated individuals The approach	mechanism
which we call Treelet Covariance Smoothing	mechanism
employs a multiscale decomposition of covariance matrices to improve estimates of pairwise relationships	mechanism
On both simulated and real data with a large genome-wide association study and estimate the `` heritability '' of body mass index quite accurately	method
Traditionally heritability defined as the fraction of the total trait variance attributable to additive genetic effects	method
is estimated from samples of closely related individuals using random effects models	method
Good feature design is important to achieve effective image classification	background
The feature design is applicable to different application domains	background
Both experiments show promising performance improvements over the state-of-the-art	finding
This paper presents a novel feature design with two main contributions	mechanism
First prior to computing the feature descriptors	mechanism
with learning-based filters Second	mechanism
we propose with another set of learning-based filters In this way	mechanism
while generic feature descriptors are used	mechanism
data-adaptive information is integrated into the feature extraction process based on the optimization objective to enhance the discriminative power of feature descriptors	mechanism
and is evaluated on both lung tissue classification in high-resolution computed tomography ( HRCT ) images and apoptosis detection in time-lapse phase contrast microscopy image sequences	method
The experimental results show that the proposed method is faster and requires smaller memory than the existing methods with little or no loss of accuracy	finding
Exploiting the redundancy in a tensor representing the affinity between feature points	mechanism
we approximate the affinity tensor with the linear combination of Kronecker products between bases and index tensors	mechanism
The bases and index tensors are highly compressed representations of the approximated affinity tensor	mechanism
requiring much smaller memory than in previous methods	mechanism
which store the full affinity tensor	mechanism
We compute the principal eigenvector of the approximated affinity tensor using the small bases and index tensors without explicitly storing the approximated tensor	mechanism
To compensate for the loss of matching accuracy by the approximation	mechanism
we also adopt and incorporate a marginalization scheme that maps a higher order tensor to matrix as well as a one-to-one mapping constraint into the eigenvector computation process	mechanism
We describe the design and control of a wearable robotic device powered by pneumatic artificial muscle actuators The design is inspired by the biological musculoskeletal system of the human foot and lower leg	mechanism
mimicking the morphology and the functionality of the biological muscle-tendon-ligament structure A key feature of the device is its soft structure that provides active assistance without restricting natural degrees of freedom at the ankle joint	mechanism
Four pneumatic artificial muscles assist dorsiflexion and plantarflexion as well as inversion and eversion	mechanism
The prototype is also equipped with various embedded sensors for gait pattern analysis	mechanism
experimentally demonstrated using a linear time-invariant ( LTI ) controller	method
The controller is found using an identified LTI model of the system	method
resulting from the interaction of the soft orthotic device with a human leg	method
and model-based classical control design techniques demonstrated with several angle-reference following experiments	method
Virus capsid assembly has been widely studied as a biophysical system	background
both for its biological and medical significance and as an important model for complex self-assembly processes No current technology can monitor assembly in detail and what information we have on assembly kinetics comes exclusively from in vitro studies	background
There are many differences between the intracellular environment and that of an in vitro assembly assay	background
however that might be expected to alter assembly pathways These models may help us understand how complicated assembly systems may have evolved to function with high efficiency and fidelity in the densely crowded environment of the cell	background
suggest a striking difference depending on whether or not a system uses nucleation-limited assembly	finding
with crowding tending to promote off-pathway growth in a nonnucleation-limited model but often enhancing assembly efficiency at high crowding levels even while impeding it at lower crowding levels in a nucleation-limited model	finding
We combine prior particle simulation methods for estimating crowding effects with coarse-grained stochastic models of capsid assembly	mechanism
using the crowding models to adjust kinetics of capsid simulations	mechanism
Simulations	method
Computational cancer phylogenetics seeks to enumerate the temporal sequences of aberrations in tumor evolution	background
thereby delineating the evolution of possible tumor progression pathways	background
molecular subtypes and mechanisms of action	background
We previously developed a pipeline for constructing phylogenies describing evolution between major recurring cell types computationally inferred from whole-genome tumor profiles	background
We show which confirms its effectiveness for tumor phylogeny inference and suggests avenues for future advances	finding
Here we present a novel hidden Markov model ( HMM ) scheme through joint segmentation and calling of multisample tumor data	mechanism
Our method classifies sets of genome-wide DNA copy number measurements into a partitioning of samples into normal ( diploid ) or amplified at each probe It differs from other similar HMM methods in its design specifically for the needs of tumor phylogenetics	mechanism
by seeking to identify robust markers of progression conserved across a set of copy number profiles	mechanism
an analysis of our method in comparison to other methods on both synthetic and real tumor data	method
In many behavioral domains	background
such as facial expression and gesture	background
sparse structure is prevalent As a consequence	background
high-dimensional representations such as SIFT and Gabor features have been favored despite their much greater computational cost and potential loss of information	background
We propose a Kernel Structured Sparsity ( KSS ) method We characterize spatio-temporal events as time-series of motion patterns and by utilizing time-series kernels we apply standard structured-sparse coding techniques to tackle this important problem	mechanism
We evaluated the KSS method using both gesture and facial expression datasets that include spontaneous behavior and differ in degree of difficulty and type of ground truth coding	method
In contrast to previous systems in which the user interrogates an intermediate representation of visual information	background
such as a tactile display representing a camera generated image	background
a potentially useful feature for higher levels analysis of the visual scene	background
have quantified the user 's ability to discriminate the angle of the edge	finding
We present a novel device mounted on the fingertip our device uses a fingertip-mounted camera and haptic stimulator to allow the user to feel visual features directly from the environment	mechanism
Visual features ranging from simple intensity or oriented edges to more complex information identified automatically about objects in the environment may be translated in this manner into haptic stimulation of the finger	mechanism
Experiments using an initial prototype to trace a continuous straight edge	method
How can we correlate the neural activity in the human brain as it responds to typed words	background
with properties of these terms ( like 'edible '	background
'fits in hand ' ) ? In short	background
we want to find latent variables	background
that jointly explain both the brain activity	background
as well as the behavioral responses	background
This is one of many settings of the Coupled Matrix-Tensor Factorization ( CMTF ) problem	background
TURBO-SMT is able to find meaningful latent variables	finding
as well as to predict brain activity with competitive accuracy	finding
We apply TURBO-SMT to BRAINQ	method
a dataset consisting of a ( nouns	method
brain voxels human subjects ) tensor and a ( nouns	method
properties ) matrix	method
with coupling along the nouns dimension	method
Our results significantly expand knowledge of eutherian genome evolution and will facilitate greater understanding of the role of chromosome rearrangements in adaptation	background
speciation and the etiology of inherited and spontaneously occurring diseases	background
With the introduction of next-generation sequencing ( NGS ) technologies	background
we are facing an exponential increase in the amount of genomic sequence data	background
The success of all medical and genetic applications of next-generation sequencing critically depends on the existence of computational techniques that can process and analyze the enormous amount of sequence data quickly and accurately	background
NGS	mechanism
We propose a new algorithm	mechanism
FastHASH which drastically improves the performance of the seed-and-extend type hash table based read mapping algorithms	mechanism
while maintaining the high sensitivity and comprehensiveness of such methods FastHASH is a generic algorithm compatible with all seed-and-extend class read mapping algorithms	mechanism
It introduces two main techniques	mechanism
namely Adjacency Filtering	mechanism
and Cheap K-mer Selection	mechanism
We implemented FastHASH and merged it into the codebase of the popular read mapping program	mechanism
mrFAST	mechanism
Depending on the edit distance cutoffs	method
Most of the relevant prior works involve supervised learning frameworks ( e	background
g	background
Support Vector Machines )	background
However in-home monitoring provides only coarse ground truth information about symptom occurrences	background
making it very hard to adapt and train supervised learning classifiers for symptom detection	background
We were able to detect subject specific symptoms ( e	finding
g	finding
dyskinesia ) that conformed with a daily log maintained by the patients	finding
to use a weakly supervised machine learning framework We address this challenge by formulating symptom detection under incomplete ground truth information as a multiple instance learning ( MIL ) problem	mechanism
MIL is a weakly supervised learning framework that does not require exact instances of symptom occurrences for training ; rather	mechanism
it learns from approximate time intervals within which a symptom might or might not have occurred on a given day Once trained	mechanism
the MIL detector was able to spot symptom-prone time windows on other days and approximately localize the symptom instances	mechanism
We monitored two Parkinson 's disease ( PD ) patients	method
each for four days with a set of five triaxial accelerometers and utilized a MIL algorithm based on axis parallel rectangle ( APR ) fitting in the feature space	method
The proposed soft capsules could be used as minimally invasive tetherless medical devices with therapeutic capability for the next generation capsule endoscopy	background
show that the drug release can be controlled by the frequency of the external magnetic pulse	finding
to evaluate the magnetically actuated multimodal drug release capability	finding
The experimental results This paper presents simulations and various experiments	method
Recognizing facial action units ( AUs ) is important for situation analysis and automated video annotation	background
Previous work has emphasized face tracking and registration and the choice of features classifiers	background
Relatively neglected is the effect of imbalanced data for action unit detection	background
Our findings suggest that skew is a critical factor in evaluating performance metrics	background
With exception of area under the ROC curve	finding
all were attenuated by skewed distributions	finding
in many cases	finding
dramatically so	finding
While ROC was unaffected by skew	finding
precision-recall curves suggest that ROC may mask poor performance	finding
To avoid or minimize skew-biased estimates of performance	finding
we recommend reporting skew-normalized scores along with the obtained ones	finding
we conducted experiments using both simulated classifiers and three major databases that differ in size	method
type of FACS coding	method
and degree of skew We evaluated influence of skew on both threshold metrics ( Accuracy	method
F-score Cohen 's kappa	method
and Krippendorf 's alpha ) and rank metrics ( area under the receiver operating characteristic ( ROC ) curve and precision-recall curve )	method
demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems	finding
compared to fully-synchronous and asynchronous schemes	finding
We propose a parameter server system for distributed ML	mechanism
which follows a Stale Synchronous Parallel ( SSP ) model of computation that The parameter server provides an easy-to-use shared interface for read/write access to an ML model 's values ( parameters and variables )	mechanism
and the SSP model allows distributed workers to read older	mechanism
stale versions of these values from a local cache	mechanism
instead of waiting to get them from a central storage	mechanism
This significantly increases the proportion of time workers spend computing	mechanism
as opposed to waiting	mechanism
Furthermore the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values	mechanism
We provide a proof of correctness under SSP	method
as well as empirical results	method
our method is several orders of magnitude faster	finding
with competitive or improved accuracy for latent space recovery and link prediction	finding
We propose a scalable approach With a succinct representation of networks as a bag of triangular motifs	mechanism
a parsimonious statistical model	mechanism
and an efficient stochastic variational inference algorithm	mechanism
we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours	mechanism
a setting that is out of reach for many existing methods	mechanism
When compared to the state-of-the-art probabilistic approaches	method
The performance of our method is investigated and illustrated	finding
by bringing together two different nonparametric ideas : distribution free inference and nonparametric smoothing We start from the general conformal prediction approach and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution	mechanism
The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cut-off values	mechanism
Under standard smoothness conditions	mechanism
we get an asymptotic efficiency result that is near optimal for a wide range of function classes But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size	mechanism
through simulation studies in a real data example	method
Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D	background
demonstrate the performance of the proposed controller results also show the feasibility of this technique in 2D and 3D environments	finding
This paper presents a control law A look-ahead proportional controller for position and orientation is presented	mechanism
The look-ahead distance is a linear function of insertion speed	mechanism
Simulations in a 3D brain-like environment Experimental	method
Previous studies have examined the characteristics of physiological tremor under laboratory settings as well as different operating conditions	background
However different test methods make the comparison of results across trials and conditions difficult	background
Two vitroretinal microsurgeons were evaluated while performing a pointing task with no entry-point constraint	method
constrained by an artificial eye model	method
and constrained by a rabbit eye in vivo	method
A spectral analysis was also performed	method
According to this hypothesis	background
when symptoms are severe	background
depressed participants withdraw from other people in order to protect themselves from anticipated rejection	background
scorn and social exclusion	background
As their symptoms fade	background
participants send more signals indicating a willingness to affiliate	background
suggests that automatic facial expression analysis may be ready for use in behavioral and clinical science	background
Automatic and manual coding were highly consistent for FACS action units	finding
and showed similar effects for change over time in depression severity	finding
For both systems	finding
when symptom severity was high	finding
participants made more facial expressions associated with contempt	finding
smiled less and those smiles that occurred were more likely to be accompanied by facial actions associated with contempt	finding
These results are consistent with the `` social risk hypothesis '' of depression	finding
The finding that automatic facial expression analysis was both consistent with manual coding and produced the same pattern of depression effects	finding
Depressed participants were followed over the course of treatment and video recorded during a series of clinical interviews	method
Facial expressions were analyzed from the video using both manual and automatic systems	method
G protein coupled receptors ( GPCRs ) are seven helical transmembrane proteins that function as signal transducers	background
They bind ligands in their extracellular and transmembrane regions and activate cognate G proteins at their intracellular surface at the other side of the membrane	background
The relay of allosteric communication between the ligand binding site and the distant G protein binding site is poorly understood	background
The GREMLIN-predicted long-range interactions between amino acids were analyzed with respect to the seven GPCR structures that have been crystallized at the time this study was undertaken	finding
Previous works either study the group sparsity in the parametric setting ( e	background
g	background
group lasso )	background
or address the problem in the nonparametric setting without exploiting the structural information ( e	background
g	background
sparse additive models )	background
that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models	finding
and also conduct a comparative experiment on a real breast cancer dataset	finding
In this paper	mechanism
we present a new method	mechanism
called group sparse additive models ( GroupSpAM )	mechanism
which can handle group sparsity in additive models	mechanism
We generalize the _1/_2 norm to Hilbert spaces as the sparsity-inducing penalty in GroupSpAM	mechanism
Moreover we derive a novel thresholding condition for identifying the functional sparsity at the group level	mechanism
and propose an efficient block coordinate descent algorithm for constructing the estimate	mechanism
We demonstrate by simulation	method
an inexpensive pico-projector-based augmented reality ( AR ) display The system is designed for use with Micron	mechanism
an active handheld surgical tool that cancels hand tremor of surgeons to improve microsurgical accuracy	mechanism
Using the AR display	mechanism
virtual cues can be injected into the microscope view Cues can be used to maintain high performance by helping the surgeon to avoid drifting out of the workspace of the instrument	mechanism
Also boundary information such as the view range of the cameras that record surgical procedures can be displayed to tell surgeons the operation area	mechanism
Furthermore numerical textual	mechanism
or graphical information can be displayed	mechanism
showing such things as tool tip depth in the work space and on/off status of the canceling function of Micron	mechanism
We propose a procedure that estimates the structure of a graphical model by minimizing the temporally smoothed L1 penalized regression	mechanism
which allows jointly estimating the partition boundaries of the VCVS model and the coefficient of the sparse precision matrix on each block of the partition A highly scalable proximal gradient method is proposed to solve the resultant convex optimization problem ; and the conditions for sparsistent estimation and the convergence rate of both the partition boundaries and the network structure are established for the first time for such estimators	mechanism
Semantic grounding is the process of relating meaning to symbols ( e	background
g	background
words )	background
It is the foundation for creating a representational symbolic system such as language	background
Motor and a portion of somatosensory neurons were found to be involved in primarily sensorimotor mapping	finding
while parietal and some somatosensory neurons were found to be involved in both sensorimotor and verb-category mapping	finding
The time course of the spike activities and the selective tuning pattern of these neurons indicate that they belong to a large neural network used for semantic processing	finding
These two mechanisms were investigated by examining neuronal-level spike ( i	method
e	method
neuronal action potential ) activities from the motor	method
somatosensory and parietal areas in two human participants	method
Present treatments for ventricular tachycardia have significant drawbacks	background
To ameliorate these drawbacks	mechanism
it may be advantageous to employ an epicardial robotic walker with precise control of needle insertion depth	mechanism
This paper introduces an open source software platform	mechanism
TomoMiner Its scalable and robust parallel processing allows efficient classification of tens to hundreds of thousands of subtomograms	mechanism
In addition TomoMiner provides a pre-configured TomoMinerCloud computing service permitting users without sufficient computing resources instant access to TomoMiners high-performance features	mechanism
Our experiments demonstrate that our synthesis method is precise and efficient The implicit specification helped us find one concurrency bug previously missed when model-checking using an explicit	finding
user-provided specification	finding
observed that different synchronization placements are produced for our experiments	finding
favoring a minimal number of synchronization operations or maximum concurrency	finding
respectively	finding
We present a computer-aided programming approach The approach allows programmers to program assuming a friendly	mechanism
non-preemptive scheduler and our synthesis procedure inserts synchronization The correctness specification is implicit	mechanism
inferred from the non-preemptive behavior	mechanism
Let us consider sequences of calls that the program makes to an external interface	mechanism
The specification requires that any such sequence produced under a preemptive scheduler should be included in the set of sequences produced under a non-preemptive scheduler	mechanism
We guarantee that our synthesis does not introduce deadlocks and that the synchronization inserted is optimal w	mechanism
r	mechanism
t	mechanism
a given objective function	mechanism
The solution is based on a finitary abstraction	mechanism
an algorithm for bounded language inclusion moduloan independence relation	mechanism
and generation of a set of global constraints over synchronization placements	mechanism
Each model of the global constraints set corresponds to a correctness-ensuring synchronization placement	mechanism
The placement that is optimal w	mechanism
r	mechanism
t	mechanism
the given objective function is chosen as the synchronization solution	mechanism
We apply the approach to device-driver programming	method
where the driver threads call the software interface of the device and the API provided by the operating system	method
We implemented objective functions for coarse-grained and fine-grained locking and	method
Macroautophagy is regarded as a nonspecific bulk degradation process of cytoplasmic material within the lysosome	background
indicating stimulus-specific pathways in stress-induced macroautophagy	background
Protein dynamics are linked to image-based models of autophagosome turnover	finding
Depending on the inducing stimulus	finding
protein as well as organelle turnover differ	finding
Amino acid starvation-induced macroautophagy leads to selective degradation of proteins important for protein translation	finding
Thus protein dynamics reflect cellular conditions in the respective treatment	finding
In the present study we monitor protein turnover and degradation by global	method
unbiased approaches relying on quantitative mass spectrometry-based proteomics	method
Macroautophagy is induced by rapamycin treatment	method
and by amino acid and glucose starvation in differentially	method
metabolically labeled cells	method
to show the higher accuracy of the surface reconstruction as compared to standard stereo reconstruction	finding
to show the increased surgical accuracy due to motion scaling are also carried out	finding
using an actively stabilized handheld robot	mechanism
guided by monocular vision We employ a previously developed monocular camera based surface reconstruction method using automated laser beam scanning over the retina	mechanism
We use the reconstructed plane to find a coordinate transform between the 2D image plane coordinate system and the global 3D frame Within a hemispherical region around the target	mechanism
we use motion scaling for higher precision	mechanism
The contribution of this work is the homography matrix estimation using monocular vision and application of the previously developed laser surface reconstruction to Micron guided vein cannulation	mechanism
Experiments are conducted in a wet eye phantom Further	method
experiments	method
Neuromechanical simulations have been used to study the spinal control of human locomotion which involves complex mechanical dynamics So far	background
most neuromechanical simulation studies have focused on demonstrating the capability of a proposed control model in generating normal walking	background
As many of these models with competing control hypotheses can generate human-like normal walking behaviors	background
a more in-depth evaluation is required	background
A model that captures these selective amplifications would be able to explain both steady and reactive spinal control of human locomotion	background
Neuromechanical simulations that investigate hypothesized control models are complementary to gait experiments in better understanding the control of human locomotion	background
Remarkably similar response trends for the majority of investigated muscles and experimental conditions reinforce the plausibility of the reflex circuits of the model	finding
However the model 's responses lack in amplitude suggesting that in these cases the proposed reflex circuits need to be amplified by additional control structures such as location-specific cutaneous reflexes	finding
The immediate changes in muscle activations of the model are compared to those of humans across different gait phases and disturbance magnitudes	method
for two experiments with whole body disturbances	method
Palau has unique features advantageous for this study : due to its population history	background
Palauans are substantially interrelated ; affected individuals often	background
but not always	background
cluster in families ; and we have essentially complete ascertainment of affected individuals	background
This extensive sharing	finding
typically identical by descent	finding
was significantly greater in cases than population controls	finding
even after controlling for relatedness	finding
Several regions of the genome exhibited substantial excess of shared haplotypes for affected individuals	finding
including 3p21 3p12	finding
4q28 and 5q23-q31	finding
Two of these regions	finding
4q28 and 5q23-q31	finding
showed significant linkage by traditional LOD score analysis and could harbor variants of more sizeable risk for psychosis or a multiplicity of risk variants	finding
The pattern of haplotype sharing in 4q28 highlights PCDH10	finding
encoding a cadherin-related neuronal receptor	finding
as possibly involved in risk	finding
This work develops a learning algorithm only using data	mechanism
The algorithm uses sparse optimization The features are data driven in the sense that they are constructed using nonlinear algebraic equations on the spatial derivatives of the data	mechanism
Several numerical experiments	method
Three-dimensional live cell imaging of the interaction of T cells with antigen-presenting cells ( APCs ) visualizes the subcellular distributions of signaling intermediates during T cell activation at thousands of resolved positions within a cell	background
These information-rich maps of local protein concentrations are a valuable resource in understanding T cell signaling	background
Here we describe a protocol This protocol allows quantitative analysis of T cell signaling as it occurs inside live cells with resolution in time and space across thousands of cells	mechanism
Quantitative image analysis procedures are necessary for the automated discovery of effects of drug treatment in large collections of fluorescent micrographs	background
These results can function as a baseline for comparison to other protein organization modeling approaches in plant cells	background
we report the dose dependent drug effects in the first high-content Arabidopsis thaliana drug screen of its kind	finding
we generated a large collection of images of single plant cells after various drug treatments	mechanism
For this protoplasts were isolated from six transgenic lines of A	mechanism
thaliana expressing fluorescently tagged proteins	mechanism
Eight drugs at three concentrations were applied to protoplast cultures followed by automated image acquisition For image analysis	mechanism
we developed a cell segmentation protocol for detecting drug effects using a Hough transform-based region of interest detector and a novel cross-channel texture feature descriptor	mechanism
In order to determine treatment effects	mechanism
we summarized differences between treated and untreated experiments with an L1 Cramr-von Mises statistic	mechanism
The distribution of these statistics across all pairs of treated and untreated replicates was compared to the variation within control replicates to determine the statistical significance of observed effects	method
Using this pipeline	method
Event discovery aims to discover a temporal segment of interest	background
such as human behavior	background
actions or activities	background
Most approaches to event discovery within or between time series use supervised learning	background
A potential solution to CED is searching over all possible pairs of segments	background
which would incur a prohibitive quartic cost	background
We consider extensions to video search and supervised event detection	background
In this paper	mechanism
we propose an efficient branch-and-bound ( B & B ) framework that avoids exhaustive search while guaranteeing a globally optimal solution	mechanism
To this end	mechanism
we derive novel bounding functions for various commonality measures and provide extensions to multiple commonality discovery and accelerated search The B & B framework takes as input any multidimensional signal that can be quantified into histograms A generalization of the framework can be readily applied to discover events at the same or different times ( synchrony and event commonality	mechanism
respectively )	mechanism
The effectiveness of the B & B framework is evaluated in motion capture of deliberate behavior and in video of spontaneous facial behavior in diverse interpersonal contexts : interviews	method
small groups of young adults	method
and parent-infant face-to-face interaction	method
Recent research has uncovered an important These findings illustrate the importance of population-based reference cohorts for the interpretation of candidate pathogenic variants	background
even for analyses of complex diseases and de novo variation	background
Robust principal component analysis ( PCA ) is one of the most important dimension-reduction techniques for handling high-dimensional data with outliers	background
illustrate the effectiveness and superiority of the proposed method	finding
Extensive experimental results on several benchmark data sets	method
Experimental results show the effectiveness of the proposed state estimation system	finding
especially for the aggressive	finding
intermittent GPS and high-altitude MAV flight	finding
In this paper	mechanism
we present by fusing long-range stereo visual odometry	mechanism
GPS barometric and IMU ( Inertial Measurement Unit ) measurements The new estimation system has two main parts	mechanism
a stochastic cloning EKF ( Extended Kalman Filter ) estimator that loosely	mechanism
and is derived and discussed in detail	mechanism
A long-range stereo visual odometry is proposed by using both multi-view stereo triangulation and a multi-view stereo inverse depth filter	mechanism
The odometry takes the EKF information ( IMU integral ) for robust camera pose tracking and image feature matching	mechanism
and the stereo odometry output serves as the relative measurements for the update of the state estimation	mechanism
on a benchmark dataset and our real flight dataset	method
Autonomous robots often rely on models of their sensing and actions for intelligent decision making	background
shows that this approach significantly enhances the detection power of existing RIM-detection algorithms in high-dimensional spaces	finding
To find inaccuracies tractably	mechanism
the robot conducts an informed search through low-dimensional projections of execution data to find parametric Regions of Inaccurate Modeling ( RIMs )	mechanism
Empirical evidence from two robot domains	method
Quantifying differences or similarities in connectomes has been a challenge due to the immense complexity of global brain networks	background
This novel approach opens a new door for probing the influence of pathological	background
genetic social or environmental factors on the unique configuration of the human connectome	background
Here we introduce a noninvasive method that uses diffusion MRI to characterize whole-brain white matter architecture as a single local connectome fingerprint	mechanism
Clinical decision support tools ( DSTs ) are computational systems that aid healthcare decision-making	background
While effective in labs	background
almost all these systems failed when they moved into clinical practice	background
Healthcare researchers speculated it is most likely due to a lack of user-centered HCI considerations in the design of these systems These findings suggest an alternative perspective to the traditional use models	background
in which clinicians engage with DSTs at the point of making a decision	background
We identify situations across patients ' healthcare trajectories when decision supports would help	background
and we discuss new forms it might take in these situations	background
Our findings reveal a lack of perceived need for and trust of machine intelligence	finding
as well as many barriers to computer use at the point of clinical decision-making	finding
a field study investigating how clinicians make a heart pump implant decision	method
Death by suicide demonstrates profound personal suffering and societal failure	background
The results provide insight into how advanced technology can be used for suicide assessment and prevention	background
In this novel prospective	mechanism
multimodal multicenter mixed demographic study	mechanism
we used machine learning	mechanism
How neural stem cells generate the correct number and type of differentiated neurons in appropriate places remains an important question Although nervous systems are diverse across phyla	background
in many taxa the larva forms an anterior concentration of serotonergic neurons	background
or apical organ	background
which are observed in a great diversity of metazoans This work explains how spatial patterning in the ectoderm controls progression of neurogenesis in addition to providing spatial cues for neuron location	background
We show that neurogenesis in sea star larvae begins with soxc-expressing multipotent progenitors	finding
These give rise to restricted progenitors that express lhx2/9 soxc- and lhx2/9-expressing cells can undergo both asymmetric divisions	finding
allowing for progression towards a particular neural fate	finding
and symmetric proliferative divisions We show that nested concentric domains of gene expression along the anterior-posterior ( AP ) axis	finding
control neurogenesis in the sea star larva by promoting particular division modes and progression towards becoming a neuron	finding
Modification to the sizes of these AP territories provides a simple mechanism to explain the diversity of neuron number among apical organs	finding
A fundamental problem in comparative genomics is to compute the distance between two genomes in terms of its higher level organization ( given by genes or syntenic blocks )	background
For two genomes without duplicate genes	background
we can easily define ( and almost always efficiently compute ) a variety of distance measures	background
Of the many distance measures	background
the breakpoint distance ( the number of nonconserved adjacencies ) was the first one to be studied and remains of significant interest because of its simplicity and model-free property	background
The three breakpoint distance problems corresponding to the three formulations have been widely studied Although we provided last year a solution for the exemplar problem that runs very fast on full genomes	background
we show that our algorithms run very fast ( in seconds ) on mammalian genomes and scale well beyond	finding
We find that our algorithm for the `` any matching '' formulation significantly outperforms other methods in terms of accuracy while achieving nearly maximum coverage	finding
three formulations ( exemplar	mechanism
maximum matching and any matching ) have been proposed	mechanism
In this article	mechanism
we describe very fast	mechanism
exact algorithms for these two problems Our algorithms rely on a compact integer-linear program that we further simplify by developing an algorithm to remove variables	mechanism
based on new results on the structure of adjacencies and matchings	mechanism
Through extensive experiments using both simulations and biological data sets	method
We also apply these algorithms ( as well as the classic orthology tool MSOAR ) to create orthology assignment	method
then compare their quality in terms of both accuracy and coverage	method
Using this resource Altering expression of FURIN	method
TSNARE1 or CNTN4 knockdown of FURIN in human neural progenitor cells	method
In this paper	mechanism
we present a novel approach called `` Backward Three-way Association Mapping '' ( BTAM Assuming that genotypes affect transcript levels	mechanism
which in turn affect phenotypes	mechanism
we first find transcripts associated with the phenotypes	mechanism
and then find genotypes associated with the chosen transcripts	mechanism
The backward ordering of association mappings allows us to avoid a large number of association testings between all genotypes and all transcripts	mechanism
making it possible	mechanism
In our simulation study	method
Furthermore we apply BTAM on an Alzheimer 's disease dataset	method
Stable chronic functionality of intracortical probes is of utmost importance toward realizing clinical application of brain-machine interfaces	background
Sustained immune response from the brain tissue to the neural probes is one of the major challenges that hinder stable chronic functionality	background
There is a growing body of evidence in the literature that highly compliant neural probes with sub-cellular dimensions may significantly reduce the foreign-body response	background
thereby enhancing long term stability of intracortical recordings	background
thereby showing promise toward chronic applications	background
To demonstrate the versatility of the process to show the co-delivery potential of the needles	finding
Insertion of the needles without mechanical failure	finding
and their subsequent dissolution are demonstrated	finding
It is concluded that ultra-miniature	finding
ultra-compliant probes and associated biodissolvable delivery needles can be successfully fabricated	finding
and the use of the ultra-compliant meandered probes results in drastic reduction in strains imposed in the tissue as compared to stiff probes	finding
needles from different biodissolvable materials	method
as well as two-dimensional needle arrays with different geometries and dimensions	method
are fabricated	method
Further needles incorporating anti-inflammatory drugs are created	method
Automated machine-reading biocuration systems typically use sentence-by-sentence information extraction to construct meaning representations for use by curators	background
This does not directly reflect the typical discourse structure used by scientists to construct an argument from the experimental data available within a article	background
and is therefore less likely to correspond to representations typically used in biomedical informatics systems ( let alone to the mental models that scientists have ) Although preliminary	background
these results support the notion that targeting information extraction methods to experimental results could provide accurate	background
automated methods for biocuration	background
We also suggest the need for finer-grained curation of experimental methods used when constructing molecular biology databases	background
We evaluate our system on text passages from articles that were curated in molecular biology databases ( the Pathway Logic Datum repository	method
the Molecular Interaction MINT and INTACT databases ) linking individual experiments in articles to the type of assay used ( coprecipitation	method
phosphorylation translocation etc	method
)	method
We use supervised machine learning techniques on text passages containing unambiguous references to experiments	method
We show that the stable phase is the one with the lower defect line tension	finding
The strong and opposite monolayer curvatures present in junctions and edges enhance the mole fraction of negatively curved lipids in junctions and deplete it in edges	finding
This lipid sorting affects the two line tensions and in turn the relative stability of the two phases	finding
It also leads to a subtle entropic barrier for the transition between junction and edge that is absent in uniform membranes	finding
We use a combination of coarse-grained molecular dynamics simulations and theoretical modeling These junctions are localized defect lines in which three bilayers merge in such a way that each bilayer shares one monolayer with one of the other two bilayers	mechanism
The resulting local morphology is non-lamellar	mechanism
resembling the threefold symmetric defect lines in inverse hexagonal phases	mechanism
but it regularly occurs during membrane fission and fusion events	mechanism
We realize a system of junctions by setting up a honeycomb lattice	mechanism
which in its primitive cell contains two hexagons and four three-line junctions	mechanism
permitting us to study their stability as well as their line tension We specifically consider the effects of lipid composition and intrinsic curvature in binary mixtures	mechanism
which contain a fraction of negatively curved lipids in a curvature-neutral background phase	mechanism
Three-junction stability results from a competition between the junction and an open edge	mechanism
which arises if one of the three bilayers detaches from the other two	mechanism
The paradigm of evidence-based medicine dictates that clinical practice should reflect the shifting landscape of the peer-reviewed literature	background
Aneuploidy and structural variations ( SVs ) generate cancer genomes containing a mixture of rearranged genomic segments with extensive somatic copy number alterations	background
We demonstrate the accuracy of Weaver findingsfrom Our approach provides a more complete assessment of the complex genomic architectures inherent to many cancer genomes	finding
Here we introduce Weaver	mechanism
an algorithm Weaver uses a Markov random field to estimate joint probabilities of allele-specific copy numbers of SVs and their inter-connectivity based on paired-end whole-genome sequencing data	mechanism
Weaver also predicts the timing of SVs relative to chromosome amplifications	mechanism
Wearable activity trackers have become a viable business opportunity	background
Nevertheless research has raised concerns over their potentially detrimental effects on wellbeing	background
For example a recent study found that while counting steps with a pedometer increased steps taken throughout the day	background
at the same time it decreased the enjoyment people derived from walking	background
This poses a serious threat to the incorporation of healthy routines into everyday life	background
Advances in fluorescence in situ hybridization ( FISH ) make it feasible to detect multiple copy-number changes in hundreds of cells of solid tumors	background
Studies using FISH	background
sequencing and other technologies have revealed substantial intra-tumor heterogeneity	background
The evolution of subclones in tumors may be modeled by phylogenies Tumors often harbor aneuploid or polyploid cell populations	background
Tests on simulated data show improved accuracy of the ploidy-based approach relative to prior ploidyless methods Tests on real data further demonstrate novel insights these methods offer into tumor progression processes Trees for DCIS samples are significantly less complex than trees for paired IDC samples Consensus graphs show substantial divergence among most paired samples from both sets	finding
Low consensus between DCIS and IDC trees may help explain the difficulty in finding biomarkers that predict which DCIS cases are at most risk to progress to IDC	finding
The FISHtrees software is available at ftp : //ftp	finding
ncbi	finding
nih	finding
gov/pub/FISHtrees	finding
The recent proliferation of cryptomarkets and the associated emergence of a sub-field of research on the anonymous web have outpaced the development of an ethical consensus regarding research methods and dissemination amongst scholars working in this unique online space	background
The peculiar characteristics of cryptomarket research	background
which often involves encryption	background
illegal activity large-scale data collection	background
and geographical separation from research participants	background
challenge conventional ethical frameworks	background
A further complicating factor for reaching ethical consensus is the confluence of scholars drawn from a variety of academic disciplines	background
each with their own particular norms	background
practices and perspectives	background
Efforts to model how signaling and regulatory networks work in cells have largely either not considered spatial organization or have used compartmental models with minimal spatial resolution Fluorescence microscopy provides the ability to monitor the spatiotemporal distribution of many molecules during signaling events	background
Here we present and methods	mechanism
evaluate	method
IF achieved state-of-the-art results for emotion expression and action unit detection in three databases	finding
FERA CK+ and RU-FACS ; measured audience reaction to a talk given by one of the authors ; and discovered synchrony for smiling in videos of parent-infant interaction IF is free of charge for academic use at http : //www	finding
humansensing	finding
cs	finding
cmu	finding
edu/intraface/	finding
This paper presents IntraFace ( IF )	mechanism
a publicly-available software package In addition	mechanism
IFincludes a newly develop technique for unsupervised synchrony detection a	mechanism
In tests	method
The generativity and complexity of human thought stem in large part from the ability to represent relations among concepts and form propositions	background
the classifiers were able to reliably identify the thematic role of an object from its associated fMRI activation pattern	finding
classifiers reliably identified the thematic roles in the data of a left-out participant ( mean accuracy_=_	finding
66 ) indicating that the neural representations of thematic roles were common across individuals	finding
Machine-learning classifiers were trained on functional magnetic resonance imaging ( fMRI ) data evoked by a set of short videos that conveyed agent-verb-patient propositions	method
When tested on a held-out video Moreover	method
when trained on one subset of the study participants	method
Despite the widespread popularity of genome-wide association studies ( GWAS ) for genetic mapping of complex traits	background
In this work	mechanism
we propose a new method that considers dynamic phenotypes measured at a sequence of time points	mechanism
Our approach relies on the use of Time-Varying Group Sparse Additive Models ( TV-GroupSpAM ) for high-dimensional	mechanism
functional regression	mechanism
Because no assumptions are required about illumination or surface properties	background
the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video	background
The software is available online at http : //zface	background
org	background
The method has been validated Experimental findings strongly support the validity of real-time	finding
3D registration and reconstruction from 2D video	finding
in a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction	method
The environment of a living cell is vastly different from that of an in vitro reaction system	background
an issue that presents great challenges to the use of in vitro models	background
or computer simulations based on them	background
for understanding biochemistry in vivo Virus capsids make an excellent model system for such questions because they typically have few distinct components	background
making them amenable to in vitro and modeling studies	background
yet their assembly can involve complex networks of possible reactions that can not be resolved in detail by any current experimental technology	background
We previously fit kinetic simulation parameters to bulk in vitro assembly data to yield a close match between simulated and real data	background
and then used the simulations to study features of assembly that can not be monitored experimentally	background
The work demonstrates how computer simulations can help us understand how assembly might differ between the in vitro and in vivo environments and what features of the cellular environment account for these differences	background
The resulting simulations exhibit surprising behavioral complexity	finding
with distinct effects often acting synergistically to drive efficient assembly and alter pathways relative to the in vitro model	finding
computationally adding features of the cellular environment to the system	mechanism
specifically the presence of nucleic acid about which many capsids assemble	mechanism
The major challenge of such work is computational : simulating fine-scale assembly pathways on the scale and in the parameter domains of real viruses is far too computationally costly to allow for explicit models of nucleic acid interaction	mechanism
We bypass that limitation by applying analytical models of nucleic acid effects to adjust kinetic rate parameters learned from in vitro data to see how these adjustments	mechanism
singly or in combination	mechanism
might affect fine-scale assembly progress	mechanism
Establishing quantitative bounds on the execution cost of programs is essential in many areas of computer science such as complexity analysis	background
compiler optimizations security and privacy	background
Techniques based on program analysis	background
type systems and abstract interpretation are well-studied	background
We prove our type system sound We demonstrate the precision and generality of our technique	finding
In this work	mechanism
we propose a relational cost analysis technique by making use of relational properties of programs and inputs	mechanism
We develop Rel Cost	mechanism
a refinement type and effect system The key novelty of our technique is the combination of relational refinements with two modes of typing-relational typing for reasoning about similar computations/inputs and unary typing for reasoning about unrelated computations/inputs	mechanism
This combination allows us to analyze the execution cost difference of two programs more precisely than a naive non-relational approach	mechanism
using a semantic model based on step-indexed unary and binary logical relations accounting for non-relational and relational reasoning principles with their respective costs	method
through examples	method
During human-robot collaboration	background
a robot and a user must often complete a disjoint set of tasks that use an overlapping set of objects	background
without using the same object simultaneously	background
A key challenge is deciding what task the robot should perform next in order to facilitate fluent and efficient collaboration	background
Most prior work does so by first predicting the human 's intended goal	background
and then selecting actions given that goal	background
However it is often difficult	background
and sometimes impossible	background
to infer the human 's exact goal in real time	background
and this serial predict-then-act method is not adaptive to changes in human goals	background
show that our POMDP model outperforms state of the art predict-then-act models by producing fewer human-robot collisions and less human idling time	finding
In this paper	mechanism
we present a system We extend recent work utilizing Partially Observable Markov Decision Processes ( POMDPs ) for shared autonomy in order	mechanism
demonstrate successful data association results	finding
We evaluate our algorithm in simulation and on real sonar images	method
showing significantly longer flight distances over the current state of the art	finding
We demonstrate this method on two flight data sets from a full-sized helicopter	method
As robots aspire for long-term autonomous operations in complex dynamic environments	background
the ability to reliably take mission-critical decisions in ambiguous situations becomes critical	background
and show that it effectively handles uncertain situations	finding
propose a generic framework	mechanism
We present this in the context of vision-based autonomous MAV flight in outdoor natural environments	method
The design of legged robots is often inspired by animals evolved to excel at different tasks	background
However while mimicking morphological features seen in nature can be very powerful	background
robots may need to perform motor tasks that their living counterparts do not	background
In the absence of designs that can be mimicked	background
an alternative is to resort to mathematical models that allow the relationship between a robot 's form and function to be explored	background
Our model was successfully used to find optimized designs for legged robots performing tasks that include jumping	finding
walking and climbing up a step	finding
Although our results are preliminary and our analysis makes a number of simplifying assumptions	finding
our findings indicate that the cost function	finding
the sum of squared joint torques over the duration of a task	finding
varies substantially as the design parameters change	finding
In this paper	mechanism
we propose such a model such that a measure of performance is optimized The framework begins by planning trajectories for a simplified model consisting of the center of mass and feet	mechanism
The framework then optimizes the length of each leg link while solving for associated full-body motions	mechanism
We use the Bayes tree factorization thereby minimizing computation We use kernel density estimation which naturally encapsulates multi-hypothesis and non-Gaussian inference	mechanism
A variety of new uncertainty models can now be directly applied in the factor graph	mechanism
and have the solver recover a potentially multi modal posterior	mechanism
For example data association for loop closure proposals can be incorporated at inference time without further modifications to the factor graph	mechanism
Our implementation of the presented algorithm is written entirely in the Julia language	mechanism
exploiting high performance parallel computing	mechanism
We show a larger scale use case with the well known Victoria park mapping and localization data set inferring over uncertain loop closures	method
This is the first amortized analysis	background
that automatically derives polynomial bounds for higher-order functions and polynomial bounds that depend on user-defined inductive types	background
The practicality of the analysis system is the system infers bounds on the number of queries that are sent by OCaml programs to DynamoDB	finding
a commercial NoSQL cloud database service	finding
The system automatically derives worst-case resource bounds for higher-order polymorphic programs with user-defined inductive types	mechanism
The technique is parametric in the resource and can derive bounds for time	mechanism
memory allocations and energy usage The derived bounds are multivariate resource polynomials which are functions of different size parameters that depend on the standard OCaml types	mechanism
Bound inference is fully automatic and reduced to a linear optimization problem that is passed to an off-the-shelf LP solver Technically	mechanism
the analysis system is based on a novel multivariate automatic amortized resource analysis ( AARA ) It builds on existing work on linear AARA for higher-order programs with user-defined inductive types and on multivariate AARA for first-order programs with built-in lists and binary trees Moreover	mechanism
the analysis handles a limited form of side effects and even outperforms the linear bound inference of previous systems At the same time	mechanism
it preserves the expressivity and efficiency of existing AARA techniques	mechanism
For robots with several degrees of freedom	background
collision checks are computationally expensive and often dominate planning time	background
demonstrate that POMP performs comparably with RRTConnect and LazyPRM for the first feasible path	finding
and BIT { * } for anytime performance	finding
both in terms of collision checks and total planning time	finding
We present POMP ( Pareto Optimal Motion Planner )	mechanism
an anytime algorithm We assume that the roadmaps we search over are embedded in a continuous ambient space	mechanism
where nearby points tend to share the same collision state	mechanism
This enables us to formulate a probabilistic model that computes the probability of unevaluated configurations being collision-free	mechanism
We update the model over time as more checks are performed This model lets us define a weighting function for roadmap edges that is related to the probability of the edge being in collision Our approach is to trade off between these two weights	mechanism
gradually prioritizing edge length over collision likelihood	mechanism
We also show that this tradeoff is approximately equivalent to minimizing the expected path length	mechanism
with a penalty of being in collision	mechanism
Our experiments	method
results obtained	finding
We propose a submap-based technique Our approach relies on the use of probabilistic volumetric techniques to create submaps from multibeam sonar scans	mechanism
as these offer increased outlier robustness	mechanism
Special attention is paid to the problem of denoising/enhancing sonar data	mechanism
Pairwise submap alignment constraints are used in a factor graph framework to correct for navigation drift and improve map accuracy	mechanism
We provide experimental from the inspection of the running gear and bulbous bow of a 600-foot	method
Wright-class supply ship	method
Inertial reorientation of airborne articulated bodies has been an active area of research in the robotics community	background
as this behavior can help guide dynamic robots to a safe landing with minimal damage	background
show that the DiverBot can execute one somersault without drift and multiple somersaults with minimal drift	finding
To this end	mechanism
a planar three link robot	mechanism
called DiverBot is proposed By considering a gravity-free scenario	mechanism
a local connection is obtained between joint angles and the body orientation	mechanism
resulting in a reduction in the system dynamics	mechanism
An optimal control policy applied on this reduced configuration space yielded diving maneuvers that are dynamically feasible	mechanism
Numerical results	method
The challenge here is to have a signal and detection system that works from long range ( > 1000m ) amongst ground clutter during various seasonal conditions on passive imagery	background
We present a long-range visual signal detection system We use a smoke-grenade as a ground signal	mechanism
which has the advantageous properties of being easy to carry by ground crews because of its light weight and small size	mechanism
but when released has a long visual signaling range	mechanism
We employ a camera system on the UAV with a visual texture feature extraction approach in a machine learning framework to classify image patches as `signal ' or `background '	mechanism
We study conventional approaches and develop a visual feature descriptor that can better differentiate the appearance of the visual signal under varying conditions and	mechanism
when used to train a random-forest classifier	mechanism
Further we develop a method by assessing the shape of the smoke signal	mechanism
The system was rigorously and quantitatively evaluated on data collected from a camera mounted on a helicopter and flown towards a plume of signal smoke over a variety of seasons	method
ground conditions weather conditions	method
and environments We present a preliminary evaluation of the wind estimation in conditions with different wind intensities and orientations relative to the approach direction	method
In a hierarchical motion planning system for urban autonomous driving	background
it is a common practice to separate tactical reasoning from the lower-level trajectory planning	background
The results demonstrate enhanced planning feasibility	finding
coherency and scalability	finding
We therefore propose a planning method that automatically discovers tactical maneuver patterns	mechanism
and fuses pattern reasoning and sampling-based trajectory planning	mechanism
We validate the performance of the proposed approach	finding
We extend the existing assignment and planning approaches for quadrotor teams to find minimal-time trajectories to enable team transition between non-rest initial and ending states while ensuring dynamic feasibility with respect to predefined kinematic	mechanism
dynamic and collision constraints This work also presents a method for safe splitting and merging of robot formations according to input specification	mechanism
The proposed methodology is capable of generating dynamically feasible and safe plans for teams of quadrotors in real time	mechanism
through various trials and scenarios conducted in simulation	method
In many multi-robot applications such as target search	background
environmental monitoring and reconnaissance	background
the multi-robot system operates semi-autonomously	background
but under the supervision of a remote human who monitors task progress	background
In these applications	background
each robot collects a large amount of task-specific data that must be sent to the human periodically to keep the human aware of task progress It is often the case that the human-robot communication links are extremely bandwidth constrained and/or have significantly higher latency than inter-robot communication links	background
so it is impossible for all robots to send their task-specific data together Thus	background
only a subset of robots	background
which we call the knowledge leaders	background
can send their data at a time	background
We prove that the knowledge leader selection is a submodular function maximization problem under explicit conditions The effectiveness of our approach is demonstrated	finding
and present a novel distributed submodular optimization algorithm that has the same approximation guarantees as the centralized greedy algorithm	mechanism
using numerical simulations	method
Formal constructive type theory has proved to be an effective language for mechanized proof	background
By avoiding non-constructive principles	background
such as the law of the excluded middle	background
type theory admits sharper proofs and broader interpretations of results	background
From a computer science perspective	background
interest in type theory arises from its applications to programming languages	background
Standard constructive type theories used in mechanization admit computational interpretations based on meta-mathematical normalization theorems	background
These proofs are notoriously brittle ; any change to the theory potentially invalidates its computational meaning	background
As a case in point	background
Voevodsky 's univalence axiom raises questions about the computational meaning of proofs	background
by providing a direct	mechanism
deterministic operational interpretation for a representative higher-dimensional dependent type theory with higher inductive types and an instance of univalence	mechanism
Rather than being a formal type theory defined by rules	mechanism
it is instead a computational type theory in the sense of Martin-Lof 's meaning explanations and of the NuPRL semantics	mechanism
The definition of the type theory starts with programs ; types are specifications of program behavior	mechanism
Planning in CPSs requires temporal reasoning to handle the dynamics of the environment	background
including human behavior	background
as well as temporal constraints on system goals and durations of actions that systems and human actors may take	background
The discrete abstraction of time in a state space planning should have a time sampling parameter value that satisfies some relation to achieve a certain precision	background
In particular the sampling period should be small enough to allow the dynamics of the problem domain to be modeled with sufficient precision	background
Meanwhile in many cases	background
events in the far future ( relative to the sampling period ) may be relevant to the decision making earlier in the planning timeline ; therefore	background
a longer planning look-ahead horizon can yield a closer-to optimal plan	background
We illustrate our approach	finding
In this paper	mechanism
we propose a multiscale temporal planning approach formulated as MDP planning	mechanism
in a middleware used to monitor large sensor networks	method
Modern frameworks are required to be extendable as well as secure	background
In this poster we describe an approach that uses a combination of static analysis and run-time management	mechanism
based on software architecture models	mechanism
Static analysis identifies the architecture and communication patterns among the collection of apps on an Android device and which communications might be vulnerable to attack	mechanism
Run-time mechanisms monitor these potentially vulnerable communication patterns	mechanism
and adapt the system to either deny them	mechanism
request explicit approval from the user	mechanism
or allow then	mechanism
We implement a prototype of the approach for the Android platform	method
The Android platform is designed to support mutually un-trusted third-party apps	background
which run as isolated processes but may interact via platform-controlled mechanisms	background
called Intents	background
Interactions among third-party apps are intended and can contribute to a rich user experience	background
for example the ability to share pictures from one app with another	background
The Android platform presents an interesting point in a design space of module systems that is biased toward isolation	background
extensibility and untrusted contributions	background
The Intent mechanism essentially provides message channels among modules	background
in which the set of message types is extensible However	background
the module system has design limitations including the lack of consistent mechanisms to document message types	background
very limited checking that a message conforms to its specifications	background
the inability to explicitly declare dependencies on other modules	background
and the lack of checks for backward compatibility as message types evolve over time	background
Based on our results	background
we outline further research questions and propose possible mitigation strategies	background
Our findings suggest that design limitations do indeed cause development problems	finding
we studied a broad corpus of apps and cross-validated our results against app documentation and Android support forums	method
Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed	background
Some of these issues might be addressed by future tools which autocomplete more complete handlers	background
We found that programmers handle exceptions locally in catch blocks much of the time	finding
rather than propagating by throwing an Exception	finding
Programmers make heavy use of actions like Log	finding
Print Return or Throw in catch blocks	finding
and also frequently copy code between handlers We found bad practices like empty catch blocks or catching Exception are indeed widespread	finding
We discuss evidence that programmers may misjudge risk when catching Exception	finding
and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception	finding
We used the Boa tool	method
The theorems are not specific to sequences and can be applied to other data types with different costs for operating on interior and leaf versions	background
The key advantages of the present approach compared to current approaches is that our implementation requires no changes to existing programming languages	finding
supports nested parallelism	finding
and has well defined cost semantics	finding
At the same time	finding
it allows for functional implementations of algorithms such as depth-first search with the same asymptotic complexity as imperative implementations	finding
to develop a form of functional arrays ( sequences The key idea is to consider sequences with functional value semantics but nonfunctional cost semantics	mechanism
Because the value semantics is functional	mechanism
`` updating { '' } a sequence returns a new sequence	mechanism
We allow operations on `` older { '' } sequences ( called interior sequences ) to be more expensive than operations on the `` most recent { '' } sequences ( called leaf sequences )	mechanism
We embed sequences in a language supporting fork-join parallelism	mechanism
Due to the parallelism	mechanism
operations can be interleaved non-deterministically	mechanism
and in conjunction with the different cost for interior and leaf sequences	mechanism
this can lead to non-deterministic costs for a program	mechanism
Consequently the costs of programs can be difficult to analyze	mechanism
The main result is the derivation of a deterministic cost dynamics which makes analyzing the costs easier	mechanism
We present a wait-free concurrent implementation of sequences that requires constant work for accessing and updating leaf sequences	mechanism
and logarithmic work for accessing and linear work for updating interior sequences	mechanism
We sketch a proof of correctness for the sequence implementation	method
We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility	mechanism
then reconstructs face shape by fitting a part-based 3D model	mechanism
Next the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation	mechanism
The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze	mechanism
To evaluate the algorithm in comparison with alternative approaches	method
three publicly-available databases were used	method
Boston University Head Tracking	method
Multi-View Gaze and CAVE Gaze datasets	method
Massive Open Online Courses ( MOOCs ) have been promoted as a means to revolutionize access to education	background
We describe their motivations for participating as well as the challenges they encountered We also describe the importance of the face-to-face learning environment provided by the iLab as a source of community support	finding
Massive online classes can benefit from peer interactions such as discussion	background
critique or tutoring	background
This paper introduces an imprecise yet simple browser-based conversational turn detector for video conversations	mechanism
Turns are detected without accessing video or audio data	mechanism
Although xMOOCs are not designed to directly engage students via social media platforms	background
some students in these courses join MOOC-associated Facebook groups	background
These findings have implications for how MOOCs and social media platforms can support learners from non-English speaking contexts	background
Results suggests that a non-trivial number of MOOC students engage in Facebook groups	finding
that learners from a number of non-U	finding
S	finding
locations are disproportionately likely to participate in such groups	finding
and that the groups display both location and language homophily	finding
the geographic distribution of students in such groups as compared to the courses at large	method
and the extent to which such groups are location and/or language homophilous	method
Massive Open Online Courses ( MOOCs ) provide an effective learning platform with various high-quality educational materials accessible to learners from all over the However	background
current MOOCs lack personalized learning guidance and intelligent assessment for individuals	background
show our approach significantly improves over previous vanilla BKT models on predicting students ' quiz performance	finding
This paper proposes to model both the hierarchical and the temporal properties of the knowledge states in order to improve the modeling accuracy	mechanism
Based on the content organization characteristics on the Coursera MOOC platform	mechanism
we provide a well-defined KC model	mechanism
and develop Multi-Grained-BKT and Historical-BKT to capture the above features effectively	mechanism
Experiments on a Coursera course dataset	method
determine whether a ( T ) X and b ( T ) Y are uncorrelated for every a is an element of R-p	background
b is an element of R-q or not	background
Linear independence testing is a fundamental information-theoretic and statistical problem that can be posed as follows : given n points \ { ( X ( i ) ; Y-i ) \ } ( n ) ( i=1 ) from a p + q dimensional multivariate distribution where X-i is an element of R-p and Y-i is an element of R-q	background
With the appearance of fraudsters in social network sites	background
the importance of trust prediction has increased	background
Most such methods use only explicit and implicit trust information ( e	background
g	background
if Smith likes several of Johnson 's reviews	background
then Smith implicitly trusts Johnson )	background
but they do not consider distrust	background
In this paper	mechanism
we propose PIN-TRUST	mechanism
a novel method The novelties of our method are the following : ( a ) it is carefully designed	mechanism
to take into account positive	mechanism
implicit and negative information	mechanism
( b ) it is scalable ( i	mechanism
e	mechanism
linear on the input size )	mechanism
( c ) most importantly	mechanism
it is effective and accurate	mechanism
Our extensive experiments with a real dataset	method
Epinions	method
corn data of 100K nodes and 1M edges	method
Recent work in dense monocular 3D reconstruction relies on dense pixel correspondences and assumes brightness constancy and saliency	background
and thus are fundamentally unable to reconstruct low-textured or non-lambertian objects such as glass or metal	background
Occlusion boundaries differ from texture in that each unique view generates a unique set of occlusions	background
By detecting and solving for the depths of occlusion boundaries by compensating with an increasing number of unique views	mechanism
Change introduces conflict into software ecosystems : breaking changes may ripple through the ecosystem and trigger rework for users of a package	background
but often developers can invest additional effort or accept opportunity costs to alleviate or delay downstream costs Our results illustrate that there is a large design space in how to build an ecosystem	background
its policies and its supporting infrastructure ; and there is value in making community values and accepted tradeoffs explicit and transparent in order to resolve conflicts and negotiate change-related costs	background
We found that all three ecosystems differ substantially in their practices and expectations toward change and that those differences can be explained largely by different community values in each ecosystem	finding
We performed a multiple case study of three software ecosystems with different tooling and philosophies toward change	method
Eclipse R/CRAN and Node	method
js/npm	method
Our results show that These policies can generalize across interactions with different numbers of people and can handle various levels of sensing noise	finding
a new state representation that we designed for this problem	mechanism
In this scenario	method
we assume that the correct behavior for the robot should convey attentiveness to the focus of attention of the conversation	method
Thus the robot should turn towards the speaker	method
from tests in a simulated environment	method
showed good performance in comparison to an existing people detection approach	finding
The projected polygon step captures significantly more people in the scene ( 77\ % vs	finding
80\ % ) and supports group clustering in dense	finding
complex scenarios Examples are provided for group splitting and merging	finding
dense crowds with obstructions	finding
and cases where other approaches typically encounter difficulty	finding
We describe a method It incorporates social expectations and is inspired by human perceptual processes	mechanism
The approach uses a single Kinect to cluster all moving objects into groups	mechanism
applies a 2D polygon projection in obscured regions	mechanism
and a group personal space modeled using asymmetric Gaussians in order to inhibit certain socially inappropriate robot paths	mechanism
This approach trades off detection of individual people for higher coverage and lower cost	mechanism
while preserving high speed processing	mechanism
A real-world evaluation of this approach	method
Trajectory planning methods for on-road autonomous driving are commonly formulated to optimize a Single Objective calculated by accumulating Multiple Weighted Feature terms ( SOMWF )	background
This paper addresses this issue by proposing a framework with multiple tunable phases of planning	mechanism
along with two novel techniques : Optimization-free trajectory smoothing/nudging	mechanism
Sampling-based trajectory search with cascaded ranking	mechanism
Proactive latency-aware adaptation is an approach for self-adaptive systems that improves over reactive adaptation by considering both the current and anticipated adaptation needs of the system	background
and taking into account the latency of adaptation tactics so that they can be started with the necessary lead time	background
Making an adaptation decision with these characteristics requires solving an optimization problem to select the adaptation path that maximizes an objective function over a finite look-ahead horizon Since this is a problem of selecting adaptation actions in the context of the probabilistic behavior of the environment	background
Markov decision processes ( MDP ) are a suitable approach	background
However given all the possible interactions between the different and possibly concurrent adaptation tactics	background
the system and the environment	background
constructing the MDP is a complex task	background
Probabilistic model checking can be used to deal with this problem since it takes as input a formal specification of the stochastic system	background
which is internally translated into an MDP	background
and solved	background
results show that this approach reduces the adaptation decision time by an order of magnitude while producing the same results	finding
In this paper we present an approach by constructing most of the MDP offline	mechanism
also using formal specification	mechanism
At run time	mechanism
the adaptation decision is made by solving the MDP through stochastic dynamic programming	mechanism
weaving in the stochastic environment model as the solution is computed	mechanism
Our experimental compared to the probabilistic model checking approach	method
Such incentives are likely aligned with benefits to utilities and grid operators	background
which might take the form of peak-shaving or ancillary services However	background
private cost savings are not strictly aligned with public benefits related to the avoidance of health and environmental damages from power plant emissions	background
We find that feasible strategies exist to simultaneously realize public and private benefits and that load shifting can result in substantial cost savings and avoided damages in some circumstances	finding
Concerns over increased latency and bandwidth costs can be mitigated with modifications to the model	finding
However the level of realized savings is dependent upon the specifics of a particular network operator and electricity rate schedule	finding
so we compare private cost minimization with a strategy that minimizes these externalities	method
Personal informatics systems are becoming increasing prevalent as their price	background
form and ease of use improves	background
Though these systems offer great potential value to users	background
many systems are hampered by issues that limit their ability to foster engagement	background
and people often abandon use of these systems without garnering meaningful outcomes	background
While continued use of these systems is not necessary for all people	background
there is an opportunity to better support people working towards achievement-based goals and discuss how these strategies could be used to foster engagement with PI systems	background
We then propose seven strategies for the design community to explore	mechanism
Machine learning improves mobile user experience	background
Interestingly envisioning apps with adaptive interfaces that reduce navigation and selection effort is not standard UX practice	background
When implementing an adaptive UI for our mobile transit app	background
we encountered a number of problems	background
extracted six design patterns where UI adaptation can improve in-app navigation	finding
Next we designed an exemplar set of wireframes	mechanism
illustrating how UX designers might annotate their interaction flows	mechanism
we reviewed the interfaces of popular apps and	method
The results of both analyses support the attribution of the effect to the behavioral interpretation	finding
suggests that more social oriented topics triggered richer discussion than more biopsychology oriented topics	finding
we adopted two approaches	method
First we used propensity score matching to pair students who exhibit a similar level of involvement in other course activities	method
Second we explored individual variation in engagement in higher-order thinking behaviors across weeks	method
A further analysis using LDA applied to course materials	method
we found that students ' progress through the first zone of the game seemed to encounter a `` roadblock { '' } during gameplay	finding
dropping out when they can not ( or do not want to ) progress further	finding
These results demonstrate that modeling player behavior can be useful for both assessing learning and for designing complex problem solving content for learning environments	finding
In this study of a Science learning game called Quantum Spectre	method
Using this prior analysis	method
alongside Survival Analysis techniques for analyzing time-series data and drop-out rates	method
Compressed sensing is a simple and efficient technique that has a number of applications in signal processing and machine learning	background
In machine learning it provides answers to questions such as : `` under what conditions is the sparse representation of data efficient ? { '' } ; `` when is learning a large margin classifier directly on the compressed domain possible ? { '' } ; and `` why does a large margin classifier learn more effectively if the data is sparse ? { '' }	background
and show for the high dimensional sparse signals	finding
when the bounds are tight	finding
directly learning in the compressed domain is possible	finding
by leveraging compressed sensing from the learning perspective We show	mechanism
for a full-rank signal	mechanism
the high dimensional sparse representation of data is efficient because from the classifiers viewpoint such a representation is in fact a low dimensional problem	mechanism
We provide practical bounds on the linear classifier to investigate the relationship between the SVM classifier in the high dimensional and compressed domains	method
An important research problem in learning analytics is	background
which suggests ways in which we might foster these social benefits through intervention	finding
we propose a pipeline that includes data infrastructure	mechanism
learning analytics and intervention	mechanism
along with computational models for individual components	mechanism
Next we describe an example of applying this pipeline to real data in a case study	method
whose goal is to investigate the positive effects that goal-setting students have on their peers	method
Various trends are reshaping content delivery on the Internet : the explosive growth of traffic due to video	background
users ' increasing expectations for higher quality of experience ( QoE )	background
and the proliferation of server capacity from a variety of sources ( e	background
g	background
cloud computing services	background
content provider-owned datacenters	background
CDNs and ISP-owned CDNs )	background
In order to meet the scale and quality demands imposed by users	background
content providers have started to spread demand across multiple CDNs using a broker	background
Brokers break many traditional CDN assumptions ( e	background
g	background
unexpected traffic skew and significant variance in demand over short timescales )	background
we show the potential challenges and opportunities that brokers impart on content delivery	finding
through a redesigned broker-CDN interface	mechanism
Through an analysis of data from a leading broker and a leading CDN	method
The core number of a node is the highest k-core in which the node participates	background
Core numbers are useful in many graph mining tasks	background
especially ones that involve finding communities of nodes	background
influential spreaders and dense subgraphs	background
Large graphs often do not fit on the memory of a single machine	background
Existing external memory solutions do not give bounds on the required space	background
demonstrate that Nimble Core gives space savings up to 60X	finding
while accurately estimating core numbers with average relative error less than 2	finding
3\ %	finding
We propose Nimble Core	mechanism
an iterative external-memory algorithm	mechanism
which using O ( n log d ( max ) ) space	mechanism
where n is the number of nodes and d ( max ) is the maximum node-degree in the graph	mechanism
We also show that Core requires O ( n ) space for graphs with power-law degree distributions	mechanism
Nimble Experiments on forty-eight large graphs from various domains	method
Making effective problem selection decisions is a challenging Self-Regulated Learning skill Students need to learn effective problem-selection strategies but also develop the motivation to use them	background
A mastery-approach orientation is generally associated with positive problem selection behaviors such as willingness to work on new materials Our experiment contributes to prior literature by demonstrating that with tutor features to foster a mastery-approach orientation	background
shared control over problem selection can lead to significantly better learning outcomes than full system control	background
The results show that shared control over problem selection accompanied by mastery-oriented features leads to significantly better learning outcomes	finding
as compared to fully system-controlled problem selection	finding
as well as better declarative knowledge of a key problem-selection strategy Nevertheless	finding
there was no effect on future problem selection and future learning	finding
The results show that the metacognitive scaffolding facilitated tutor learning ( regardless of the presence of the cognitive scaffolding )	finding
whereas cognitive scaffolding had virtually no effect The same pattern was confirmed	finding
We conducted a classroom study to test these hypotheses in the context of learning to solve equations by teaching a synthetic peer	method
SimStudent	method
by two additional datasets collected from two previous school studies we conducted	method
The results show that the proposed adaptive online course technology is robust enough to be used in actual classroom with mixed effect for learning	finding
we have developed an adaptive online course on the Open Learning Initiative ( OLI ) platform by integrating four new instances of cognitive tutors into an existing OLI course	mechanism
Cognitive tutors were created with an innovative cognitive tutor authoring system called WATSON	mechanism
To evaluate the effectiveness of the adaptive online course	method
a quasi-experiment was conducted in a gateway course at Carnegie Mellon University	method
Ambiguity arises in requirements when a statement is unintentionally or otherwise incomplete	background
missing information or when a word or phrase has more than one possible meaning	background
to yield a rank order of vague terms in both isolation and composition	finding
to show how increases in vagueness will decrease users ' acceptance of privacy risk and thus decrease users ' willingness to share personal information	finding
The taxonomy was evaluated in a paired comparison experiment and results were analyzed using the Bradley-Terry model We further provide empirical evidence based on factorial vignette surveys	method
confidentiality of training data induced by releasing machine-learning models	background
and has recently received increasing attention	background
which to the best of our knowledge	background
were not previously known	background
Interestingly we also discovered an intriguing phenomenon	finding
which we call `` invertibility interference	finding
{ '' } where a highly invertible model quickly becomes highly non-invertible by adding little noise	finding
We show that even very restricted communication between layers could leak a significant amount of information Perhaps more importantly	finding
our study also unveils unexpected computational power of these restricted communication channels	finding
Motivated by existing MI attacks and other previous attacks that turn out to be MI `` in disguise	mechanism
{ '' } by presenting a game-based methodology Our methodology uncovers a number of subtle issues	mechanism
and devising a rigorous game-based definition	mechanism
analogous to those in cryptography	mechanism
is an interesting avenue for future work	mechanism
We describe methodologies for two types of attacks The first is for black-box attacks	mechanism
which consider an adversary who infers sensitive values with only oracle access to a model	mechanism
The second methodology targets the white-box scenario where an adversary has some additional knowledge about the structure of a model	mechanism
For the restricted class of Boolean models and black-box attacks	mechanism
we characterize model invertibility using the concept of influence from Boolean analysis in the noiseless case	mechanism
and connect model invertibility with stable influence in the noisy case	mechanism
For the white-box case	mechanism
we consider a common phenomenon in machine-learning models where the model is a sequential composition of several sub-models	mechanism
quantitatively	method
Social networking sites ( SNSs ) offer users a platform to build and maintain social connections	background
Results show that women self-disclose more than men	finding
People with a stronger desire to manage impressions self-disclose less Network size is negatively associated with self-disclosure	finding
while tie strength and network density are positively associated	finding
This observational research develops a machine learning model and uses it Features include emotional valence	mechanism
social distance between the poster and people mentioned in the post	mechanism
the language similarity between the post and the community and post topic	mechanism
To validate the model and advance our understanding about online self-disclosure	method
we applied it to de-identified	method
aggregated status updates from Facebook users	method
Anonymity online is important to people at times in their lives	background
Anonymous communication applications such as Whisper and YikYak enable people to communicate with strangers anonymously through their smartphones Our results provide implications for how anonymity in mobile apps can encourage expressiveness and interaction among users	background
People share various types of content that range from deep confessions and secrets to lighthearted jokes and momentary feelings	finding
An important driver for participation and posting is to get social validation from others	finding
even though they are anonymous strangers We also find that participants believe these anonymous apps allow more honesty	finding
openness and diversity of opinion than they can find elsewhere	finding
We present a typology of the content people share	mechanism
and their motivations for participation in anonymous apps	mechanism
When health services involve long-term treatment over months or years	background
providers have the ability	background
not present in acute emergency care	background
to collaboratively reflect on clients ' changing health data and adjust interventions	background
Current literature shows a bias toward standardized records and routines in the implementation of health information technology	background
a policy that may not be appropriate for long-term health services	background
We discuss how the design of information systems should vary based on temporal factors	background
Our fieldwork in this context complements and provides contrasts to previous CSCW studies performed in time-critical hospital settings	finding
We define a temporal spectrum ranging from time-critical services that benefit from standardization to long-term services that require more flexibility	method
We provide empirical evidence from fieldwork that we performed in organizations providing long-term behavioral and mental health services for children	method
Hackathons are events where people who are not normally collocated converge for a few days to write code together	background
Hackathons it seems	background
are everywhere Our findings have implications for technology support that needs to be in place for hackathons and for understanding the role of brief interludes of collocation in loosely-coupled	background
geographically distributed work	background
We present results that suggest the way that hackathon-style collocation advances technical work varies across technical domain	finding
community structure and expertise of participants	finding
Building social ties	finding
in contrast seems relatively constant across hackathons	finding
Results from different hackathon team formation strategies suggest a tradeoff between advancing technical work and building social ties	finding
from a multiple-case study	method
People are more creative at solving difficult design problems when they use relevant examples from outside of the problem 's domain as inspirations	background
Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own	finding
or drew inspiration from domains closer to or not sharing structural correspondence with the original problem	finding
In this paper	mechanism
we demonstrate an approach in which non-experts identify domains that have the potential to yield useful and non-obvious inspirations for solutions	mechanism
We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains	method
Large graph datasets have caused renewed interest for graph partitioning	background
we introduce the idea of skew-resistant graph partitioning Skewresistant graph partitioning tries to mitigate skewness by taking the characteristics of both the target workload and the graph structure into consideration	mechanism
Current dialogue systems typically lack a variation of audio-visual feedback tokens	background
Either they do not encompass feedback tokens at all	background
or only support a limited set of stereotypical functions	background
However this does not mirror the subtleties of spontaneous conversations	background
In this study	mechanism
we devised an array of monomodal and multimodal binary comparison perception tests and experiments	mechanism
This allowed us to investigate i ) which features ( amplitude	method
frequency duration	method
) of the visual feedback influences attentiveness perception ; ii ) whether visual or verbal backchannels are perceived to be more attentive iii ) whether the fusion of unimodal tokens with low perceived attentiveness increases the degree of perceived attentiveness compared to unimodal tokens with high perceived attentiveness taken alone ; iv ) the automatic ranking of audio-visual feedback token in terms of conveyed degree of attentiveness	method
A previous approach utilizes gaze duration and word rarity features to perform this detection	background
However while this system can be used by trained users	background
its performance is not sufficient during natural reading by untrained users	background
The experimental results demonstrate that learning using SVMs and proposed eye movement features improves detection performance and that personalization further improves results	finding
) examine the effect of personalization	method
as measured by F-measure	method
Automatic emotion recognition plays a central role in the technologies underlying social robots	background
affect-sensitive human computer interaction design and affect-aware tutors	background
Finally we present a detailed analysis of the most indicative behavioral cues for emotion recognition in children	finding
Our experiments compare unimodal and multimodal emotion recognition baseline models to enable future research on this topic	method
Cyber-attacks aimed at breaking into networks and bringing websites down appear to have become an every-day phenomenon	background
using which we summarize the major players and trends in DDoS cyber-attacks	finding
We take a high-level view of attacks mostly considering aggregate country-to-country attacks	method
Cyber-attacks are cheap	background
easy to conduct and often pose little risk in terms of attribution	background
but their impact could be lasting	background
The low attribution is because tracing cyber-attacks is primitive in the current network architecture	background
Moreover even when attribution is known	background
the absence of enforcement provisions in international law makes cyber attacks tough to litigate	background
and hence attribution is hardly a deterrent	background
Rather than attributing attacks	background
we can re-look at cyber-attacks as societal events associated with social	background
political economic and cultural ( SPEC ) motivations	background
Because it is possible to observe SPEC motives on the internet	background
social media data could be valuable in understanding cyber attacks	background
We find that	finding
for some countries	finding
the probability of attacks increases by up to 27\ % while experiencing negative sentiments from other nations	finding
To verify our model	finding
b ) Using cyber-attacks trend and sentiments trend	mechanism
we build a decision tree model to find attacks that could be related to extreme sentiments	mechanism
In this research	method
we use sentiment in Twitter posts	method
and Arbor Networks data we describe three examples in which cyber-attacks follow increased tension between nations	method
as perceived in social media	method
Increasing proliferation of mobile and online social networking platforms have given us unprecedented opportunity to observe and study social interactions at a fine temporal scale	background
A collection of all such social interactions among a group of individuals ( or agents ) observed over an interval of time is referred to as a temporally-detailed ( TD ) social network	background
A TD social network opens up the opportunity to explore TD questions on the underlying social system	background
e	background
g	background
`` How is the betweenness centrality of an individual changing with time ? { '' } To this end	background
related work has proposed temporal extensions of centrality metrics ( e	background
g	background
betweenness and closeness )	background
We prove the correctness and completeness of our algorithm	finding
shows that the proposed algorithm out performs the alternatives by a wide margin	finding
To this end	mechanism
we propose a novel computational paradigm called epoch-point based techniques Using the concept of epoch-points	mechanism
we develop a novel algorithm for computing shortest path based centrality metric such as betweenness on a TD social network	mechanism
Our experimental analysis	method
Software architects inhabit a complex	background
rapidly evolving technological landscape	background
These must be overcome by future research in order to make our vision of curated knowledge bases a reality	background
We report in this paper on the initial results of using supervised machine learning to assist with knowledge base curation	finding
Our results show immense promise in recommending Web pages that are highly relevant to curators	finding
We also describe the major obstacles	finding
both practical and scientific	finding
that our work has uncovered	finding
we envisage an ecosystem of curated	mechanism
automatically updated knowledge bases These knowledge bases would emulate engineering handbooks that are commonly found in other engineering disciplines As a first step towards this vision	mechanism
we have built a curated knowledge base for comparing distributed databases based on a semantically defined feature taxonomy	mechanism
Large-scale deep learning requires huge computational resources to train a multi-layer neural network	background
Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections	background
This paper describes a new parameter server	mechanism
called GeePS overcoming these obstacles	mechanism
we show that SchMP programs running on STRADS outperform non-model-parallel ML implementations : for example	finding
SchMP LDA and SchMP Lasso respectively achieve 10x and 5x faster convergence than recent	finding
well-established baselines	finding
We propose scheduled model parallelism ( SchMP )	mechanism
a programming approach by efficiently scheduling parameter updates	mechanism
taking into account parameter dependencies and uneven convergence	mechanism
To support SchMP at scale	mechanism
we develop a distributed framework STRADS which optimizes the throughput of SchMP programs	mechanism
and benchmark four common ML applications written as SchMP programs : LDA topic modeling	mechanism
matrix factorization sparse least-squares ( Lasso ) regression and sparse logistic regression By improving ML progress per iteration through SchMP programming whilst improving iteration throughput through STRADS	mechanism
Today 's cellular core	background
which connects the radio access network to the Internet	background
relies on fixed hardware appliances placed at a few dedicated locations and uses relatively static routing policies	background
As such today 's core design has key limitations-it induces inefficient provisioning tradeoffs and is poorly equipped to handle overload	background
failure scenarios and diverse application requirements	background
To address these limitations	background
ongoing efforts envision `` clean slate { '' } solutions that depart from cellular standards and routing protocols ; e	background
g	background
via programmable switches at base stations and per-flow SDN-like orchestration	background
show that KLEIN can scale to billions of devices and is close to optimal for wide variety of traffic and deployment parameters	finding
We propose KLEIN	mechanism
a design that stays within the confines of current cellular standards and addresses the above limitations by combining network functions virtualization with smart resource management	mechanism
We address key challenges w	mechanism
r	mechanism
t	mechanism
scalability and responsiveness in realizing KLEIN via backwards-compatible orchestration mechanisms	mechanism
Our evaluations through data-driven simulations and real prototype experiments using OpenAirInterface	method
Mutual adaptation is critical for effective team collaboration	background
indicate that the proposed formalism can significantly improve the effectiveness of human-robot teams	finding
while human subject ratings on the robot performance and trust are comparable to those achieved by cross training	finding
a state-ofthe-art human-robot team training practice	finding
This paper presents a formalism We propose the bounded-memory adaptation model ( BAM )	mechanism
based on a bounded memory assumption	mechanism
We integrate BAM into a partially observable stochastic model	mechanism
When the human is adaptive	mechanism
the robot will guide the human towards a new	mechanism
optimal collaborative strategy unknown to the human in advance	mechanism
When the human is not willing to change their strategy	mechanism
the robot adapts to the human in order to retain human trust	mechanism
Human subject experiments	method
This is the first demonstration that robot appearance affects people 's moral judgments about robots	background
we found further evidence for a previously discovered Human-Robot ( HR ) asymmetry in moral judgments : Importantly	finding
we found that people 's representation of the `` robot { '' } making these moral decisions appears to be one of a mechanical robot	finding
people showed the HR asymmetry only when making judgments about a mechanical-looking robot	finding
not a humanoid robot	finding
In three studies For when we manipulated the pictorial display of a verbally described robot	method
When interacting with robots deployed in the open world	background
people may often attempt to engage with them in a playful manner or test their competencies	background
and frame research directions and design implications for robots deployed in the wild	background
We report on a pilot field-study We discuss early results from this initial study	finding
We have been studying the intentions of everyday users in their engagement with a long-lived robot system that provides directions within an office building	method
exploring the use of direct queries to elicit the sincerity of user requests	method
in terms of their actual need for directions	method
2D alignment of face images works well provided images are frontal or nearly so and pitch and yaw remain modest	background
In spontaneous facial behavior	background
these constraints often are violated by moderate to large head rotation	background
3D alignment from 2D video has been proposed as a solution	background
The results suggest that 3D alignment from 2D video is feasible on a wide range of face orientations	background
Differences among methods are considered and suggest directions for further research	background
We report results for four that provided necessary technical descriptions of their methods	finding
The leading approach achieved prediction consistency error of 3	finding
48\ %	finding
Corresponding result for the lowest ranked approach was 5	finding
9\ %	finding
made training and validation sets available to investigators	method
and invited them to test their algorithms on an independent test-set	method
Eight teams accepted the challenge and submitted test results	method
For each participating tracker	background
a short description is provided in the Appendix	background
The VOT2016 goes beyond its predecessors by ( i ) introducing a new semi-automatic ground truth bounding box annotation methodology and ( ii ) extending the evaluation system with the no-reset experiment	mechanism
Much robotics research has focused on intent-expressive ( legible ) motion	background
that the produced motions are significantly more legible compared to those generated assuming an omniscient observer	finding
show through large-scale user studies	method
Nonverbal behaviors play an important role in communication for both humans and social robots	background
However adding contextually appropriate animations by hand is time consuming and does not scale well	background
Previous researchers have developed automated systems for inserting animations based on utterance text	background
yet these systems lack human understanding of social context and are still being improved	background
Results showed untrained workers are capable of providing reasonable labeling of semantic information and that emotional expressions derived from the labels were rated more highly than control videos	finding
More study is needed to determine the effects of emphasis labels	finding
To test this approach	method
untrained workers from Mechanical Turk labeled semantic information	method
specifically emotion and emphasis	method
for each utterance	method
which was used to automatically add animations	method
Videos of a robot performing the animated dialogue were rated by a second set of participants	method
Morphable face models are a powerful tool	background
We present a new multi-part model of the eye that includes a morphable model of the facial eye region	mechanism
as well as an anatomy-based eyeball model	mechanism
It is the first morphable model that accurately	mechanism
since it was built from high-quality head scans	mechanism
It is also the first	mechanism
since we treat it as a separate part	mechanism
To showcase our model we present a new method for illumination-and head-pose invariant gaze estimation from a single RGB image	mechanism
We fit our model to an image through analysis-by-synthesis	mechanism
solving for eye region shape	mechanism
texture eyeball pose	mechanism
and illumination simultaneously	mechanism
The fitted eyeball pose parameters are then used to estimate gaze direction	mechanism
Through evaluation on two standard datasets	method
Computer vision has a great potential to help our daily lives by searching for lost keys	background
watering flowers or reminding us to take a pill To succeed with such tasks	background
computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes	background
While most of such scenes are not particularly exciting	background
they typically do not appear on YouTube	background
in movies or TV broadcasts We believe that the realism	background
diversity and casual nature of this dataset will present unique challenges and new opportunities for computer vision community	background
provide baseline results for several tasks including action recognition and automatic description generation	finding
Using this rich data	method
we evaluate and	method
Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets	background
For example babies push objects	background
poke them put them in their mouth and throw them to learn representations	background
Towards this goal	mechanism
we build one of the first systems on a Baxter platform that pushes	mechanism
pokes grasps and observes objects in a tabletop environment It uses four different types of physical interactions to collect more than 130K datapoints	mechanism
with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation	mechanism
Quantitatively we evaluate our learned ConvNet on image classification tasks	method
Discrete energy minimization is widely-used in computer vision and machine learning for problems such as MAP inference in graphical models The problem	background
in general is notoriously intractable	background
and finding the global optimal solution is known to be NP-hard This paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms	background
Specifically we show that general energy minimization	finding
even in the 2-label pairwise case	finding
and planar energy minimization with three or more labels are exp-APX-complete	finding
This finding rules out the existence of any approximation algorithm with a sub-exponential approximation ratio in the input size for these two problems	finding
including constant factor approximations	finding
Moreover we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes - PO	mechanism
APX and exp-APX	mechanism
corresponding to problems that are solvable	mechanism
approximable and inapproximable in polynomial time	mechanism
Problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones	mechanism
What happens if one pushes a cup sitting on a table toward the edge of the table ? How about pushing a desk against a wall ?	background
show that the challenging task of predicting long-term movements of objects as their reaction to external forces is possible from a single image The code and dataset are available at : http : //allenai	finding
org/plato/forces	finding
Our experimental evaluations	method
New IT functions have greatly increased the amount of in-car information delivered to drivers	background
Although valuable that information can distract drivers when delivered during vehicle operation	background
By inferring driver state from sensor data	background
prior research has shown that it can accurately identify opportune moments to deliver information	background
With these results	background
researchers can then build information delivery systems that can deliver information to drivers both when they are interruptible and when they find the information valuable	background
we identified driving situations when each of the in-car information items is highly valuable	finding
and verified these situations Results from our study offer important insights for understanding the diversity of drivers ' experiences about the value of in-car information and the ability to determine situations in which this information is valuable to drivers	finding
To answer this question	method
we conducted a series of surveys and interviews and compiled a list of representative in-car information items and context factors that affect the importance of these items By combining and exploring those context factors through a large online survey of drivers	method
Lastly we examined what technology is available for detecting these driving situations	method
and which situations require further advanced technologies for detection	method
In a given scene	background
humans can easily predict a set of immediate future events that might happen	background
However pixel-level anticipation in computer vision is difficult because machine learning struggles with the ambiguity in predicting the future	background
We show that our method predicts events in a variety of scenes and can produce multiple different predictions for an ambiguous future	finding
We also find that our method learns a representation that is applicable to semantic vision tasks	finding
We propose a conditional variational autoencoder as a solution to this problem In this framework	mechanism
direct inference from the image shapes the distribution of possible trajectories while latent variables encode information that is not available in the image	mechanism
There is a growing interest in behavior based biometrics	background
Although biometric data has considerable variations for an individual and may be faked	background
yet the combination of such `weak experts ' can be rather strong	background
A remotely detectable component is gaze direction estimation and thus	background
eye movement patterns	background
We show that it improves the precision of gaze direction estimation algorithms considerably	finding
Here we present a novel personalization method which does not require a precise calibration setup	mechanism
can be non-obtrusive	mechanism
is fast and easy to use The method is convenient ; we exploit 3D face model reconstruction for the enrichment of a small number of collected data artificially	mechanism
Software architecture modeling is important for analyzing system quality attributes	background
particularly security	background
However such analyses often assume that the architecture is completely known in advance	background
In many modern domains	background
especially those that use plugin-based frameworks	background
it is not possible to have such a complete model because the software system continuously changes	background
The Android mobile operating system is one such framework	background
where users can install and uninstall apps at run time	background
that indicates that the architecture can be amenable for use throughout the system 's lifetime	finding
In this paper	mechanism
we describe a formal architecture style We illustrate the use of the style with two security analyses : a predicate-based approach defined over architectural structure that can detect some common security vulnerabilities	mechanism
and inter-app permission leakage determined by model checking We also show how the evolving architecture of an Android device can be obtained by analysis of the apps on a device	mechanism
and	mechanism
provide some performance evaluation	method
Language usage behavior of users evolves over time	background
as they interact on social media such as Twitter	background
Our work is applicable in predicting activity and influence	finding
interest evolution job change and community change expected to happen to a user	finding
in future	finding
We propose Man-O-Meter	mechanism
a framework We model the evolution using a combination of three dimensions : ( a ) time	mechanism
( b ) content ( topics ) and ( c ) influence flow over social relationships We assert the goodness of our approach	mechanism
by predicting ranks of experts	mechanism
with respect to their influence in their respective expertise category	mechanism
using the change in language used in time	mechanism
Wicks et al	background
conducted a number of ablation experiments on a population of worms in which one or more of the neurons in the TW circuit are surgically ablated ( removed )	background
of a ( nonlinear ODE ) model of a neural circuit in Caeorhabditis elegans ( C	mechanism
elegans ) the common roundworm	mechanism
Our approach to this problem rests on encoding each TW response as a hybrid automaton with parametric uncertainty	mechanism
In contrast our technique allow us to more thoroughly explore the models parameter space using statistical sampling theory	mechanism
identifying in the process the distribution of TW responses	mechanism
Problems of this nature arise in formal verification of continuous and hybrid dynamical systems	background
where there is an increasing need for methods to expedite formal proofs	background
we discuss and illustrate certain classes of problems where this relationship is interesting	background
The relationship between increased deductive power and running time performance of the proof rules is far from obvious	finding
We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of benchmarks	method
Suppose you are a teacher	background
and have to convey a set of object-property pairs ( 'lions eat meat ' )	background
A good teacher will convey a lot of information	background
with little effort on the student side	background
What is the best and most intuitive way to convey this information to the student	background
without the student being overwhelmed ?	background
it is effective	finding
achieving excellent results both with respect to our proposed metric	finding
but also with respect to encoding length demonstrate the effectiveness of GROUPNTEACH	finding
We also design an algorithm	mechanism
GROUPNTEACH for this problem	mechanism
Our proposed GROUPNTEACH is scalable ( near-linear in the dataset size ) ; and it is intuitive	mechanism
conforming to wellknown educational principles	mechanism
on real data	method
Experiments on real and synthetic datasets	method
Recent research has improved our understanding of how to create strong	background
memorable text passwords	background
and suggest ways to ease password entry for mobile users	background
We compare the strength and usability of passwords created and used on mobile devices with those created and used on desktops and laptops	method
while varying password policy requirements and input methods	method
Friendsourcing consists of broadcasting questions and help requests to friends on social networking sites	background
Despite its potential value	background
friendsourcing requests often fall on deaf ears	background
One way to improve response rates and motivate friends to undertake more effortful tasks may be to offer extrinsic rewards	background
such as money or a gift	background
for responding to friendsourcing requests	background
However past research suggests that these extrinsic rewards can have unintended consequences	background
including undermining intrinsic motivations and undercutting the relationship between people	background
Results indicate that large extrinsic rewards increase friends ' response rates without reducing the relationship strength between friends Additionally	finding
the extrinsic rewards allow requesters to explain away the failure of friendsourcing requests and thus preserve their perceptions of relationship ties with friends	finding
we conducted an experiment on a new friendsourcing platform - Mobilyzr	method
People accumulate huge assortments of a possessions	background
but it is not yet clear how systems and system designers can help people make meaning from these large archives	background
Early research in HCl has suggested that people generally appear to value their virtual things less than their material things	background
but theory on material possessions does not entirely explain this difference	background
We conclude with implication and strategies for aimed at supporting people in having more meaningful interactions and experiences with their virtual possessions	background
Our study revealed insights about how materializing virtual possessions influences factors shaping how people draw on	finding
understand and value those possessions	finding
we designed a technology probe that selected snippets from old emails and mailed them as physical postcards to participating households The probe uncovered features of emails that trigger meaningful reflection	mechanism
and how contextual information can help people engage in reminiscence	mechanism
Timebanking is a growing type of peer-to-peer service exchange	background
but is hampered by the effort of finding good transaction partners	background
and shows that such an algorithm can retrieve matches that are subjectively better than matches based on matching the category of people 's historical offers or requests to the category of a current transaction request	finding
by using a Matching Algorithm for Service Transactions ( MAST )	mechanism
MAST matches transaction partners in terms of similarity of interests and complementarity of abilities and needs	mechanism
We present an experiment involving data and participants from a real timebanking network	method
that evaluates the acceptability of MAST	method
From these findings we argue for the importance of extensions in supporting modularity	background
community engagement and relatable prototyping materials in the iterative design of prosthetics	background
We discuss materials that support on-the-spot design and iteration	finding
dimensions along which in-person iteration is most important ( such as length and angle ) and the value of a supportive social network for users who prototype their own assistive technology	finding
a case study of three participants with upper-limb amputations working with researchers Our study made use of 3D printing and other playful and practical prototyping materials	method
Crowdsourcing offers a powerful new paradigm for online work However	background
real world tasks are often interdependent	background
requiring a big picture view of the difference pieces involved	background
Existing crowdsourcing approaches that support such tasks - ranging fromWikipedia to flash teams-are bottlenecked by relying on a small number of individuals to maintain the big picture	background
that may be informative for other systems aimed at supporting big picture thinking in small pieces	background
we instantiate the idea in a prototype system for accomplishing distributed information synthesis and We also contribute a set of design patterns	mechanism
evaluate its output across a variety of topics	method
More and more data nowadays exist in hierarchical formats such as JSON due to the increasing popularity of web applications and web services	background
showed that our tool helped spreadsheet users complete data exploration tasks nearly two times faster than using Excel and even outperform programmers in most tasks	finding
In this paper	mechanism
we present a spreadsheet tool We introduce novel interaction techniques and algorithms using the data 's relative hierarchical relationships with the data in its adjacent columns Our tool leverages the data 's structural information	mechanism
Our lab study	method
Two initial randomized experiments A pair of final studies compared to ( equivalent ) performance levels exhibited by participants who had either completed no prior activity or completed an activity activating a concrete mindset	method
Crowdsourced clustering approaches present a promising way to harness deep semantic knowledge for clustering complex information	background
We introduce Alloy	mechanism
a hybrid approach that combines the richness of human judgments with the power of machine algorithms	mechanism
Alloy through a new `` sample and search { '' } crowd pattern which changes the crowd 's task from classifying a fixed subset of items to actively sampling and querying the entire dataset	mechanism
It also through a two phase process in which crowds provide examples To accomplish this	mechanism
Alloy introduces a modular `` cast and gather { '' } approach which leverages a machine learning backbone	mechanism
Although many users create predictable passwords	background
the extent to which users realize these passwords are predictable is not well understood	background
We conclude with design directions for helping users make better passwords	background
Participants had serious misconceptions about the impact of basing passwords on common phrases and including digits and keyboard patterns in passwords	finding
However in most other cases	finding
participants ' perceptions of what characteristics make a password secure were consistent with the performance of current password-cracking tools	finding
We find large variance in participants ' understanding of how passwords may be attacked	finding
potentially explaining why users nonetheless make predictable passwords	finding
In this 165-participant online study	method
we ask participants to rate the comparative security of carefully juxtaposed pairs of passwords	method
as well as the security and memorability of both existing passwords and common password-creation strategies	method
Stateless model checking is a powerful technique for testing concurrent programs	background
but suffers from exponential state space explosion when the test input parameters are too large	background
Several reduction techniques can mitigate this explosion	background
but even after pruning equivalent interleavings	background
the state space size is often intractable Most prior tools are limited to pre-empting only on synchronization APIs	background
which reduces the space further	background
but can miss unsynchronized thread communication bugs	background
QUICKSAND found 1	finding
25x as many bugs and verified 4	finding
3x as many tests compared to prior model checking approaches	finding
We present QUICKSAND	mechanism
a new stateless model checking framework using different preemption points It uses state space estimation most likely to complete in a fixed CPU budget	mechanism
and it incorporates data-race analysis Preempting threads during a data race 's instructions can automatically classify the race as buggy or benign	mechanism
and uncovers new bugs not reachable by prior model checkers	mechanism
It also enables full verification of all possible schedules when every data race is verified as benign within the CPU budget	mechanism
In our evaluation	method
Previous work on muscle activity sensing has leveraged specialized sensors such as electromyography and force sensitive resistors Our work is the first to explore the feasibility of using solely motion sensors on everyday wearable devices to detect fine-grained gestures	background
This promising technology can be deployed today on current smartwatches and has the potential to be applied to cross-device interactions	background
or as a tool for research in fields involving finger and hand motion	background
a new technique using integrated motion sensors ( accelerometer and gyroscope ) in off-the-shelf smartwatches	mechanism
Clinical decision support tools ( DSTs ) are computational systems that aid healthcare decision-making While effective in labs	background
almost all these systems failed when they moved into clinical practice Healthcare researchers speculated it is most likely due to a lack of user-centered HCI considerations in the design of these systems	background
and we discuss new forms it might take in these situations	background
Our findings reveal a lack of perceived need for and trust of machine intelligence	finding
as well as many barriers to computer use at the point of clinical decision-making	finding
These findings suggest an alternative perspective to the traditional use models	finding
in which clinicians engage with DSTs at the point of making a decision	finding
We identify situations across patients ' healthcare trajectories when decision supports would help	finding
a field study	method
There is a significant gap in the body of research on cross-device interfaces	background
with XDBrowser a cross-device web browser we are developing We describe the design space in this context	mechanism
the usage scenarios targeted by users the strategies used for designing cross-device interfaces	mechanism
and seven concrete mobile multi-device design patterns that emerged	mechanism
We discuss the method	mechanism
Social media is an increasingly important part of modern life While Twitter has traditionally been thought of as the most accessible social media platform for blind users	background
Twitter 's increasing integration of image content and users ' diverse uses for images have presented emergent accessibility challenges	background
Our findings illuminate the importance of the ability to use social media for people who are blind	finding
while also highlighting the many challenges such media currently present this user base	finding
including difficulty in creating profiles	finding
in awareness of available features and settings	finding
in controlling revelations of one 's disability status	finding
and in dealing with the increasing pervasiveness of image based content	finding
via a combination of surveys of blind Twitter users	method
large-scale analysis of tweets from and Twitter profiles of blind and sighted users	method
and analysis of tweets containing embedded imagery	method
Due to the rapid deployability and low cost of the tags used	background
we can create a new class of interactive paper devices that are drawn on demand for simple tasks	background
These capabilities allow new interactive possibilities for pop-up books and other papercraft objects	background
We describe techniques We use sensing and signal processing techniques that determine how a tag is being manipulated by the user via an RFID reader and show how tags may be enhanced with a simple set of conductive traces that can be printed on paper	mechanism
stencil-traced or even hand-drawn	mechanism
These traces modify the behavior of contiguous tags to serve as input devices Our techniques provide the capability to use off-the-shelf RFID tags to sense touch	mechanism
cover overlap of tags by conductive or dielectric ( insulating ) materials	mechanism
and tag movement trajectories	mechanism
Paper prototypes can be made functional in seconds	mechanism
which has been successfully used for this purpose	background
AutoSLEX finds the best basis in a library of smoothed localized exponentials ( SLEX ) basis functions that are orthogonal and localized in both time and frequency	background
We demonstrate the utility of the proposed improvements	finding
on synthetic and real data	method
Current symbol-based dictionaries providing vocabulary support for persons with the language disorder	background
aphasia are housed on smartphones or other portable devices	background
To employ the support on these external devices requires the user to divert their attention away from their conversation partner	background
to the neglect of conversation dynamics like eye contact or verbal inflection	background
A prior study investigated head-worn displays ( HWDs ) as an alternative form factor for supporting glanceable	background
unobtrusive and always-available conversation support	background
Our findings should motivate further work on head-worn conversation support for persons with aphasia	background
we compared vocabulary support on a HWD to equivalent support on a smartphone in terms of overall experience	method
perceived focus and conversational success	method
Lastly we elicited critical discussion of how each device might be better designed for conversation support	method
When navigating indoors	background
blind people are often unaware of key visual information	background
such as posters	background
signs and exit doors	background
With VizMap we move towards integrating the strengths of the end user	background
on-site crowd online crowd	background
and computer vision to solve a long-standing challenge in indoor blind exploration	background
Our VizMap system uses computer vision and crowdsourcing VizMap starts with videos taken by on-site sighted volunteers and uses these to create a 3D spatial model	mechanism
These video frames are semantically labeled by remote crowd workers with key visual information	mechanism
These semantic labels are located within and embedded into the reconstructed 3D model	mechanism
forming a query-able spatial representation of the environment	mechanism
VizMap can then localize the user with a photo from their smartphone	mechanism
and enable them to explore the visual elements that are nearby	mechanism
We explore a range of example applications enabled by our reconstructed spatial representation	method
Playtesting or using play to guide game design	background
gives designers feedback about whether their game is meeting their goals and the player 's expectations We conclude with lessons learned and next steps in our research on playtesting	background
novice game designers leveraged playtest methods and tools	finding
employed playtesting and data collection methods appropriate for their goals	finding
and effectively applied playtest data in iterative design	finding
case study We identify common missteps made by novice designers and address these missteps through the concept of purposefulness	method
understanding why you are playtesting as well as how to playtest	method
We ground our workshops in the development of rich player experience goals	method
which inform playtest design	method
data collection and iteration	method
We show that by applying methods taught in our workshops	method
Unease over data privacy will retard consumer acceptance of IoT deployments	background
We propose a solution that interposes a locally-controlled software component called a privacy mediator on every raw sensor stream	mechanism
Each mediator is in the same administrative domain as the sensors whose data is being collected	mechanism
and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain	mechanism
This solution necessitates a logical point of presence for mediators within the administrative boundaries of each organization	mechanism
Such points of presence are provided by cloudlets	mechanism
which are small locally-administered data centers at the edge of the Internet that can support code mobility	mechanism
The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility	mechanism
We demonstrate its usefulness results in easier and better-structured proofs	finding
We introduce differential refinement logic ( dRL )	mechanism
a logic with first-class support and a proof calculus dRL simultaneously solves several seemingly different challenges common in theorem proving for hybrid systems : By using a refinement relation to arrange proofs hierarchically according to the structure of natural subsystems	mechanism
we can increase the readability and modularity of the resulting proof	mechanism
dRL extends an existing specification and verification language for hybrid systems ( differential dynamic logic	mechanism
dL ) by adding a refinement relation to directly compare hybrid systems	mechanism
with examples where using refinement	method
Self-adaptive systems have the ability to adapt their behavior to dynamic operating conditions	background
In reaction to changes in the environment	background
these systems determine the appropriate corrective actions based in part on information about which action will have the best impact on the system	background
can improve the accuracy of predictions used for decision-making	finding
by describing an approach based on architectural system descriptions	mechanism
which at the same time	mechanism
hence improving the selection of the best corrective action	mechanism
The core of our approach is a language equipped with a formal semantics defined in terms of Discrete Time Markov Chains that enables us to describe both the impact of adaptation tactics	mechanism
as well as the assumptions about the environment	mechanism
To validate our approach	method
we show how employing our language in the Rainbow framework for architecture-based self-adaptation	method
Everyday tools and objects often need to be customized for an unplanned use or adapted for specific user	background
such as adding a bigger pull to a zipper or a larger grip for a pen The advent of low-cost 3D printing offers the possibility to rapidly construct a wide range of such adaptations	background
We believe this work would benefit makers and designers for prototyping lifehacking solutions and assistive technologies	background
In this paper	mechanism
we describe Reprise-a design tool for specifying	mechanism
generating customizing and fitting adaptations onto existing household objects	mechanism
Reprise allows users to express at a high level what type of action is applied to an object Based on this high level specification	mechanism
Reprise automatically generates adaptations	mechanism
Users can use simple sliders to customize the adaptations to better suit their particular needs and preferences	mechanism
such as increasing the tightness for gripping	mechanism
enhancing torque for rotation	mechanism
or making a larger base for stability	mechanism
Finally Reprise provides a toolkit of fastening methods and support structures for fitting the adaptations onto existing objects	mechanism
Patients researching medical diagnoses	background
scientist exploring new fields of literature	background
and students learning about new domains are all faced with the challenge of capturing information they find for later use	background
However saving information is challenging on mobile devices	background
where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use Furthermore	background
beyond the challenge of simply selecting a region of bounded text on a mobile device	background
in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user	background
In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster	background
we find that this approach reduced selection time and was preferred by participants over the default system text selection method	finding
We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context	mechanism
In a two part user study	method
The world is full of physical interfaces that are inaccessible to blind people	background
from microwaves and information kiosks to thermostats and checkout terminals	background
Blind people can not independently use such devices without at least first learning their layout	background
and usually only after labeling them with sighted assistance	background
and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone	background
The recent advances in image captioning stimulate the research in generating natural language description for visual content	background
which can be widely applied in many applications such as assisting blind people	background
Video description generation is a more complex task than image caption	background
Most works of video description generation focus on visual information in the video	background
prove that fusing audio information greatly improves the video description performance	finding
In this paper	mechanism
we propose using both audio and visual cues	mechanism
We use unified deep neural networks with both convolutional and recurrent structure	mechanism
Experimental results on the Microsoft Research Video Description ( MSVD ) corpus	method
An important feature of functional programs is that they are parallel by default	background
We prove the safety of this collector In addition	finding
we describe how the proposed techniques can be implemented on modern shared-memory machines	finding
In this paper	mechanism
we present a technique At the highest level of abstraction	mechanism
the approach consists of a technique to organize memory as a hierarchy of heaps	mechanism
and an algorithm for performing automatic memory reclamation by taking advantage of a disentanglement property of parallel functional programs	mechanism
More specifically the idea is to assign to each parallel task its own heap in memory and organize the heaps in a hierarchy/tree that mirrors the hierarchy of tasks	mechanism
We present a nested-parallel calculus that specifies hierarchical heaps and prove in this calculus a disentanglement property	mechanism
which prohibits a task from accessing objects allocated by another task that might execute in parallel	mechanism
Leveraging the disentanglement property	mechanism
we present a garbage collection technique that can operate on any subtree in the memory hierarchy concurrently as other tasks ( and/or other collections ) proceed in parallel	mechanism
and present a prototype implementation as an extension to MLton	mechanism
a high-performance compiler for the Standard ML language	mechanism
by formalizing it in the context of our parallel calculus	method
Finally we evaluate the performance of this implementation on a number of parallel benchmarks	method
Micro-clones are small pieces of redundant code	background
such as repeated subexpressions or statements	background
Our results suggest that the detection and removal of micro-clones is valued by developers	background
can be automated at scale	background
and may be fixed with rapid turnaround times	background
Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in imperfect-information games	background
They show that running counterfactual regret minimization on such abstractions leads to good strategies in the original games	finding
We develop the first general	mechanism
algorithm-agnostic solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions	mechanism
when implemented in the original ( perfect-recall ) game	mechanism
Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions for which any such results have been obtained	mechanism
Further our analysis is tighter in two ways	mechanism
each of which can lead to an exponential reduction in the solution quality error bound	mechanism
We then show that for extensive-form games that satisfy certain properties	mechanism
the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem	mechanism
where the increase in our bound is the distance function	mechanism
This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds	mechanism
We proceed to show a divide in the class of abstraction problems	mechanism
If payoffs are at the same scale at all information sets considered for abstraction	mechanism
the input forms a metric space	mechanism
and this immediately yields a 2-approximation algorithm for abstraction	mechanism
Conversely if this condition is not satisfied	mechanism
we show that the input does not form a metric space	mechanism
Finally we provide computational experiments to evaluate the practical usefulness of the abstraction techniques	method
results demonstrate that SwitchKV can achieve up to 5x throughput and 3x latency improvements over traditional system designs	finding
SwitchKV is a new key-value store system design that combines high-performance cache nodes with resource-constrained backend nodes The cache nodes absorb the hottest queries so that no individual backend node is over-burdened	mechanism
Compared with previous designs	mechanism
SwitchKV exploits SDN techniques and deeply optimized switch hardware Programmable network switches keep track of cached keys and route requests to the appropriate nodes at line speed	mechanism
based on keys encoded in packet headers	mechanism
A new hybrid caching strategy keeps cache and switch forwarding rules updated with low overhead and ensures that system load is always well-balanced under rapidly changing workloads	mechanism
Our evaluation	method
Multi-stage log-structured ( MSLS ) designs	background
such as LevelDB	background
RocksDB HBase and Cassandra	background
are a family of storage system designs that exploit the high sequential write speeds of hard disks and flash drives by using multiple append-only data structures	background
find optimized system parameters that decrease LevelDB 's insert cost by up to 9	finding
4-26	finding
2\ % ; our analytic primitives and model also suggest changes to RocksDB that reduce its insert cost by up to 32	finding
0\ % without reducing query performance or requiring extra memory	finding
As a first step	mechanism
we propose new analytic primitives and MSLS design models Our model can almost perfectly estimate the cost of inserts in LevelDB	mechanism
whereas the conventional worst-case analysis gives 1	mechanism
83	mechanism
5X higher estimates than the actual cost	mechanism
A few minutes of offline analysis using our model can	method
Motivation : Reconstructing regulatory networks from expression and interaction data is a major goal of systems biology	background
While much work has focused on trying to experimentally and computationally determine the set of transcription-factors ( TFs ) and microRNAs ( miRNAs ) that regulate genes in these networks	background
relatively little work has focused on inferring the regulation of miRNAs by TFs	background
Such regulation can play an important role in several biological processes including development and disease	background
The main challenge for predicting such interactions is the very small positive training set currently available	background
Another challenge is the fact that a large fraction of miRNAs are encoded within genes making it hard to determine the specific way in which they are regulated	background
and can be used by any method that combines miRNAs	background
genes and TFs	background
Results As we show	finding
the methods we develop achieve good performance demonstrating the advantage of using the predicted set of interactions for identifying more coherent and relevant modules	finding
genes and miRNAs The complete set of predictions is available on the supporting website	finding
we extended semisupervised machine-learning approaches to integrate a large set of different types of data including sequence	mechanism
expression ChIP-seq and epigenetic data	mechanism
on both a labeled test set	method
and when analyzing general co-expression networks	method
We next analyze mRNA and miRNA cancer expression data	method
We validate uSpark and demonstrating only minor performance overhead with low verification costs	finding
We present uberSpark ( uSpark )	mechanism
an innovative architecture uSpark comprises two key ideas : ( i ) endowing low-level system software with abstractions found in higher-level languages ( e	mechanism
g	mechanism
objects interfaces function-call semantics for implementations of interfaces	mechanism
access control on interfaces	mechanism
concurrency and serialization )	mechanism
enforced using a combination of commodity hardware mechanisms and lightweight static analysis ; and ( ii ) interfacing with platform hardware by programming in Assembly using an idiomatic style ( called CASM ) that is verifiable via tools aimed at C	mechanism
while retaining its performance and low-level access to hardware After verification	mechanism
the C code is compiled using a certified compiler while the CASM code is translated into its corresponding Assembly instructions	mechanism
Collectively these innovations enable compositional verification of security invariants without sacrificing performance	mechanism
by building and verifying security invariants of an existing open-source commodity x86 micro-hypervisor and several of its extensions	method
Human-chosen text passwords	background
today 's dominant form of authentication	background
are vulnerable to guessing attacks	background
We show that neural networks can often guess passwords more effectively than state-of-the-art approaches	finding
such as probabilistic context-free grammars and Markov models We also show that our neural networks can be highly compressed-to as little as hundreds of kilobytes-without substantially worsening guessing effectiveness	finding
Together our contributions enable more accurate and practical password checking than was previously possible	finding
We propose using artificial neural networks to model text passwords ' resistance to guessing attacks and explore how different architectures and training methods impact neural networks ' guessing effectiveness	mechanism
Building on these results	mechanism
we implement in JavaScript the first principled client-side model of password guessing	mechanism
which analyzes a password 's resistance to a guessing attack of arbitrary duration with sub-second latency	mechanism
Modern RDMA hardware offers the potential for exceptional performance	background
that outperforms an existing design by 50x	finding
and improve the CPU efficiency of a prior high-performance key-value store by 83\ %	finding
This paper lays out guidelines that can be used by system designers Our guidelines emphasize paying attention to low-level details such as individual PCIe transactions and NIC architecture	mechanism
We empirically demonstrate how these guidelines can be used to improve the performance of RDMA-based systems we design a networked sequencer	method
We present results which indicate that starting with the ASR output is worse unless it is sufficiently accurate ( Word Error Rate of under 30\ % )	finding
Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines	background
which could be of interest to studies on website profiling	background
Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models	background
In order to conduct this study	mechanism
we develop a classifier to distinguish between compromised vs	mechanism
malicious websites	mechanism
We conduct an extensive study and follow a website-centric and user-centric point of view	method
We collect data from four online databases	method
including Symantec 's WINE Project	method
for a total of more than 600K malicious URLs and over 500K users	method
Modern Internet applications are being disaggregated into a microservice-based architecture	background
with services being updated and deployed hundreds of times a day	background
The accelerated software life cycle and heterogeneity of language runtimes in a single application necessitates a new approach for testing the resiliency of these applications in production infrastructures	background
We present Gremlin	mechanism
a framework Gremlin is based on the observation that microservices are loosely coupled and thus rely on standard message-exchange patterns over the network	mechanism
Gremlin allows the operator to easily design tests and executes them by manipulating inter-service messages at the network layer	mechanism
We show how to use Gremlin to express common failure scenarios and how developers of an enterprise application were able to discover previously unknown bugs in their failure-handling code without modifying the application	mechanism
We revisit this question and provide an efficient coding scheme with a constant rate for the interesting case of fully connected networks	mechanism
that if a ( d-regular ) network has mixing time m	method
Concurrency bugs that stem from schedule-dependent branches are hard to understand and debug	background
because	background
shows that Cortex is able to expose failing schedules with only a few perturbations to non-failing executions	finding
and takes a practical amount of time	finding
We present Cortex : a system without relying on information from failing executions Cortex preemptively exposes failing executions by perturbing the order of events and control-flow behavior in non-failing schedules from production runs of a program	mechanism
By leveraging this information from production runs	mechanism
Cortex synthesizes executions to guide the search for failing schedules	mechanism
Production-guided search helps cope with the large execution search space by targeting failing executions that are similar to observed non-failing	mechanism
Evaluation on popular benchmarks	method
The widespread availability of high-quality motion capture data and the maturity of solutions to animate virtual characters has paved the way for the next generation of interactive virtual worlds exhibiting intricate interactions between characters and the environments they inhabit	background
This paper presents an automated approach for analyzing both motions and environments We extract the salient features that characterize the contact-rich motion repertoire of a character and detect valid transitions in the environment where each of these motions may be possible	mechanism
along with additional semantics that inform which surfaces of the environment the character may use for support during the motion	mechanism
The precomputed motion semantics can be easily integrated into standard navigation and animation pipelines in order to greatly enhance the motion capabilities of virtual characters	mechanism
The computational efficiency of our approach enables two additional applications	mechanism
Environment designers can interactively design new environments and get instant feedback on how characters may potentially interact	mechanism
which can be used for iterative modeling and refinement	mechanism
End users can dynamically edit virtual worlds and characters will automatically accommodate the changes in the environment in their movement strategies	mechanism
Data compression can be an effective method to achieve higher system performance and energy efficiency in modern data-intensive applications by exploiting redundancy and data similarity Prior works have studied a variety of data compression techniques to improve both capacity ( e	background
g	background
of caches and main memory ) and bandwidth utilization ( e	background
g	background
of the on-chip and off-chip interconnects )	background
we propose two new toggle-aware compression techniques : Energy Control and Metadata Consolidation	mechanism
Factorization Machines offer good performance and useful embeddings of data	background
demonstrate its efficiency	finding
In this paper we describe DiFacto	mechanism
which uses a refined Factorization Machine model with sparse memory adaptive constraints and frequency adaptive regularization	mechanism
We show how to distribute DiFacto over multiple machines using the Parameter Server framework by computing distributed sub gradients on minibatches asynchronously	mechanism
We analyze its convergence and in computational advertising datasets with billions examples and features	method
Social community detection is a growing field of interest in the area of social network applications	background
and many approaches have been developed	background
including graph partitioning	background
latent space model	background
block model and spectral clustering	background
show that UCGT achieves better performance than existing state-of-the-art comparison methods	finding
we propose by incorporating their spatiotemporal data and semantic information	mechanism
Technically we propose a unified probabilistic generative model	mechanism
User-Community-Geo-Topic ( UCGT ) With a well-designed multi-component model structure and a parallel inference implementation to leverage the power of multicores and clusters	mechanism
our UCGT model is expressive while remaining efficient and scalable to growing large-scale geo-social networking data	mechanism
We deploy UCGT to two application scenarios of user behavior predictions : check-in prediction and social interaction prediction	method
Extensive experiments on two large-scale geo-social networking datasets	method
Given such geometric alignments	background
the natural approach for recognition might extract pose-normalized appearance features from a canonically-aligned coordinate frame	background
Though such approaches are extraordinarily common	background
that synthesis is a surprisingly simple but effective strategy that allows for state-of-the-art categorization and automatic 3D alignment	finding
To do so	mechanism
it makes use of recent methods for automatic alignment of cuboidal objects in images we demonstrate that they are not optimal	mechanism
both theoretically and empirically	mechanism
One reason is that such approaches require accurate shape alignment	mechanism
However even with ground-truth alignment	mechanism
posenormalized representations may still be sub-optimal	mechanism
Instead we introduce methods based on pose-synthesis	mechanism
a somewhat simple approach of augmenting training data with geometrically perturbed training samples we introduce a novel dataset for cuboidal object categorization	mechanism
We demonstrate both theoretically and empirically	method
We demonstrate use of Spire to author complex shaders that are portable across different rendering pipelines and to rapidly explore shader optimization decisions that span multiple compute and graphics passes and even offline asset preprocessing	finding
and demonstrate rapid	finding
automatic re-optimization of shaders for different target hardware platforms	finding
We present Spire	mechanism
a shading language and compiler framework that facilitates rapid exploration of shader optimization choices ( such as frequency reduction and algorithmic approximation ) afforded by modern real-time graphics engines	mechanism
Our design combines ideas from rate-based shader programming with new language features that expand the scope of shader execution beyond traditional GPU hardware pipelines	mechanism
overloading shader terms at various spatio-temporal computation rates provided by the pipeline	mechanism
In contrast to prior work	mechanism
neither the shading language 's design	mechanism
nor our compiler framework 's implementation	mechanism
is specific to the capabilities of any one rendering pipeline	mechanism
thus Spire establishes architectural separation between the shading system and the implementation of modern rendering engines ( allowing different rendering pipelines to utilize its services	mechanism
We further demonstrate the utility of Spire by developing a shader level-of-detail library and shader auto-tuning system on top of its abstractions	method
were it got previously discording participants	finding
talking to each other and agreeing on the issues	finding
This paper describes a group interview technique designed The technique borrows from agile software development the concept of user stories to cast CMMI 's specific practices in concrete terms and the Planning Poker technique	mechanism
instead of document reviews and audit like interviews	mechanism
for fact finding and corroboration	mechanism
The method was successfully used in one consulting assignment	method
Initial results show that the proposed method does afford better segmentation compared to one of the present state of the art algorithms	finding
a Bayesian baseline approach that segments the documents individually	finding
This paper proposes the use of lexical similarity across different documents Given a set of topically related documents	mechanism
the segmentation process is carried out using a Bayesian framework By using similar sentences from different documents more accurate segment likelihood estimations are obtained	mechanism
The proposed approach was tested in an educational domain where a set of learning materials from different media sources needed to be segmented so that students could browse through them more efficiently	method
The resulting CP approach outperforms a Branch-and-Bound approach derived from two closely related problems In addition	finding
the CP approach presented in this paper resulted in a first place position in the competition	finding
In order to achieve smooth autonomous driving in real-life urban and highway environments	background
a motion planner must generate trajectories that are locally smooth and responsive ( reactive )	background
and at the same time	background
far-sighted and intelligent ( deliberative )	background
Prior approaches achieved both planning qualities for full-speed-range operations at a high computational cost	background
In this paper	mechanism
a pipelined ( phased ) framework with tunable planning modules is proposed	mechanism
The topic of kinematics of laser rangefinders has received little attention in the robotics literature	background
even though such sensors have been the perception sensors of choice on commercial AGVs	background
field robots and aerial robots for some time	background
In recognition of the uniqueness of optical reflection mechanisms	mechanism
this paper presents a formulation based on a matrix reflection operator	mechanism
Many applications for robotic systems require the systems to traverse diverse	background
unstructured environments State estimation with Visual Odometry ( VO ) in these applications is challenging because there is no single algorithm that performs well across all environments and situations	background
The unique trade-offs inherent to each algorithm mean different algorithms excel in different environments	background
Our method reduces the mean translational relative pose error by 3	finding
5\ % and the angular error by 4	finding
3\ % compared to the single best odometry algorithm	finding
Compared to the poorest performing odometry algorithm	finding
our method reduces the mean translational error by 39	finding
4\ % and the angular error by 20	finding
1\ %	finding
We develop a method by using an ensemble of VO algorithms The method combines the estimates by dynamically switching to the best algorithm for the current context	mechanism
according to a statistical model of VO estimate errors The model is a Random Forest regressor that is trained to predict the accuracy of each algorithm as a function of different features extracted from the sensory input	mechanism
We evaluate our method in a dataset of consisting of four unique environments and eight runs	method
totaling over 25min of data	method
One of the challenges of field testing planetary rovers on Earth is the difference in gravity between the test and the intended operating conditions	background
This not only changes the weight exerted by the robot on the surface but also affects the behaviour of the granular surface itself	background
and unfortunatly no field test can fully address this shortcoming	background
Excavating with gravity offload underestimates the detrimental effects of gravity on traction	background
but overestimates the detrimental effects on excavation resistance ; though not ideal	background
this is a more balanced test than excavating in Earth gravity	background
which underestimates detrimental effects on both traction and resistance	background
Experiments demonstrate that continuous excavation ( e	finding
g	finding
bucket-wheel ) fares better than discrete excavation ( e	finding
g	finding
front-loader ) when subjected to gravity offload	finding
and is better suited for planetary excavation	finding
Lessons learned from the prototype development also address ways to mitigate suspension lift-off for lightweight skid-steer robots	finding
a problem encountered during mobility field testing	finding
This key result is incorporated into the development of a novel planetary excavator prototype	mechanism
The resulting invariant generation method is observed to be much more scalable and efficient than the na `` ive approach	finding
exhibiting orders of magnitude performance improvement on many of the problems	finding
Based on the notion of discrete abstraction	mechanism
our method eliminates unsoundness and unnecessary coarseness found in existing approaches for computing abstractions for non-linear continuous systems and is able to construct invariants with intricate boolean structure	mechanism
in contrast to invariants typically generated using template-based methods	mechanism
In order to tackle the state explosion problem associated with discrete abstraction	mechanism
we present invariant generation algorithms that exploit sound proof rules for safety verification	mechanism
such as differential cut ( DC )	mechanism
and a new proof rule that we call differential divide-and-conquer ( DDC )	mechanism
which splits the verification problem into smaller sub-problems	mechanism
Reduced frequency range in vowel production is a well documented speech characteristic of individuals with psychological and neurological disorders	background
Affective disorders such as depression and post-traumatic stress disorder ( PTSD ) are known to influence motor control and in particular speech production	background
These findings could potentially support treatment of affective disorders	background
like depression and PTSD in the future	background
The experiments show a significantly reduced vowel space in subjects that scored positively on the questionnaires	finding
We show the measure 's statistical robustness against varying demographics of individuals and articulation rate	finding
The reduced vowel space for subjects with symptoms of depression can be explained by the common condition of psychomotor retardation influencing articulation and motor control	finding
Within this work	mechanism
we investigate an automatic unsupervised machine learning based approach	mechanism
Level-of-detail ( LOD ) rendering is a key optimization used by modern video game engines to achieve high-quality rendering with fast performance	background
These LOD systems require simplified shaders	background
but generating simplified shaders remains largely a manual optimization task for game developers	background
We present an end-to-end system The system operates on shaders used in both forward and deferred rendering pipelines	mechanism
requires no additional semantic information beyond input shader source code	mechanism
and in only seconds to minutes generates LOD policies ( consisting of simplified shader	mechanism
the desired LOD distance set	mechanism
and transition generation ) with performance and quality characteristics comparable to custom hand-authored solutions	mechanism
Our design contributes new shader simplification transforms such as approximate common subexpression elimination and movement of GPU logic to parameter bind-time processing on the CPU	mechanism
and it uses a greedy search algorithm that employs extensive caching and up-front collection of input shader statistics to rapidly identify simplified shaders with desirable performance-quality trade-offs	mechanism
validated	finding
We present a computational tool with user-controlled aesthetics In contrast to approaches that leverage texture synthesis for creating decorative surface patterns	mechanism
our method relies on user-defined spline curves as central design primitives More specifically	mechanism
we build on the physically-inspired metaphor of an embedded elastic curve that can move on a smooth surface	mechanism
deform and connect with other curves We formalize this idea as a globally coupled energy-minimization problem	mechanism
discretized with piece-wise linear curves that are optimized in the parametric space of a smooth surface	mechanism
Building on this technical core	mechanism
we propose a set of interactive design and editing tools that we demonstrate on manually-created layouts and semi-automated deformable packings In order to prevent excessive compliance	mechanism
we furthermore propose a structural analysis tool that uses eigenanalysis	mechanism
We used our approach to create a variety of designs in simulation with a set of 3D-printed physical prototypes	method
We demonstrate the versatility of our method	finding
We present an interactive tool made from flexible interlocking quadrilateral elements of a single size and shape	mechanism
With the element shape fixed	mechanism
the design task becomes one of finding a discrete structure-i	mechanism
e	mechanism
element connectivity and binary orientations-that leads to a desired geometry	mechanism
we propose a forward modeling tool that Paralleling principles from conventional modeling software	mechanism
our approach leverages a library of base shapes that can be instantiated	mechanism
combined and extended using two fundamental operations : merging and extrusion	mechanism
we furthermore propose a method	mechanism
by creating a diverse set of digital and physical examples that can serve as personalized lamps or decorative items	method
The proposed framework generalizes moderately well from textbook graphics to hand-drawn sketches	background
and user effort ratio results demonstrate the potential power of an interactive system in which simple user interactions complement computer recognition for fast kinematic modeling	background
Current state-of-the-art performance is achieved	finding
A rigorous set of experiments was conducted to systematically evaluate the performance of each phase in our framework	method
comparing various combinations of joint and body detection schemes and feasibility constraints	method
Precision-recall curves are used to assess object detection performance	method
Data races complicate programming language semantics	background
and a data race is often a bug	background
Existing techniques detect data races and define their semantics by detecting conflicts between synchronization-free regions ( SFRs )	background
Valor is the first region conflict detector to provide strong semantic guarantees for racy program executions with under 2X slowdown	background
Overall Valor advances the state of the art in always-on support for strong behavioral guarantees for data races	background
showing that Valor dramatically outperforms FastRCD and FastTrack	finding
This paper describes Valor that achieves high performance by eliminating the costly analysis on each read operation that prior approaches require Valor instead logs a region 's reads and lazily detects conflicts for logged reads when the region ends	mechanism
we have also developed FastRCD that leverages the epoch optimization strategy of the FastTrack data race detector	mechanism
As a comparison	method
We evaluate Valor	method
FastRCD and FastTrack	method
and the effects of iterated deception	finding
moving to a mathematical model	mechanism
and ending with a studies on the implications of deceptive motion for human-robot interactions	method
Communication constraints dictated by hardware often require a multi-robot system to make decisions and take actions locally Unfortunately	background
local knowledge may impose limits that ultimately impede global optimality in a decentralized optimization problem	background
to show that the convergence of local searching processes is related to a shortest path routing problem on a graph subject to the network topology	finding
results show that this fully decentralized method converges quickly while sacrificing little optimality	finding
This paper enhances a recent anytime optimal assignment method based on a task-swap mechanism	mechanism
redesigning the algorithm We propose a fully decentralized approach that allows local search processes to execute concurrently while minimizing interactions amongst the processes	mechanism
needing neither global broadcast nor a multi-hop communication protocol	mechanism
The formulation is analyzed in a novel way using tools from group theory and optimization duality theory Simulation	method
Autonomous landing is an essential function for micro air vehicles ( MAVs ) for many scenarios	background
in order to establish the efficacy and robustness of the proposed approach	finding
Simulation and experimental evaluation of the performance of the perception and trajectory generation methodologies are analyzed independently and jointly	method
We propose a new generative model of network evolution in dynamic and harsh environments	mechanism
Our model can reproduce the range of topologies observed across known robust and fragile biological networks	mechanism
as well as several additional transport	mechanism
communication and social networks	mechanism
We also develop a new optimization measure based on preserving high connectivity following random or adversarial bursty node loss	mechanism
propose a new distributed algorithm	mechanism
Using this measure	method
we evaluate the robustness of several real-world networks and	method
illustrating the potential to adapt and personalize the structure and motion of existing linkages	finding
We present a method Given a working linkage as input	mechanism
the user can make targeted edits to the shape or motion of selected parts while preserving other	mechanism
e	mechanism
g	mechanism
functionally-important aspects	mechanism
In order to make this process intuitive and efficient	mechanism
we provide a number of editing tools at different levels of abstraction	mechanism
For instance the user can directly change the structure of a linkage by displacing joints	mechanism
edit the motion of selected points on the linkage	mechanism
or impose limits on the size of its enclosure	mechanism
Our method safeguards against degenerate configurations during these edits	mechanism
thus ensuring the correct functioning of the mechanism at all times	mechanism
Linkage editing poses strict requirements on performance that standard approaches fail to provide In order to enable interactive and robust editing	mechanism
we build on a symbolic kinematics approach that uses closed-form expressions instead of numerical methods to compute the motion of a linkage and its derivatives	mechanism
We demonstrate our system on a diverse set of examples To validate the feasibility of our edited designs	method
we fabricated two physical prototypes	method
Motivation : It remains a challenge to detect associations between genotypes and phenotypes because of insufficient sample sizes and complex underlying mechanisms involved in associations	background
Fortunately it is becoming more feasible to obtain gene expression data in addition to genotypes and phenotypes	background
giving us new opportunities	background
In this article	mechanism
we propose a novel method	mechanism
NETAM We take a network-driven approach : NETAM first constructs an association network	mechanism
where nodes represent SNPs	mechanism
gene traits or phenotypes	mechanism
and edges represent the strength of association between two nodes NETAM assigns a score to each path from an SNP to a phenotype	mechanism
and then identifies significant paths based on the scores	mechanism
In our simulation study Furthermore	method
we applied NETAM on late-onset Alzheimer 's disease data	method
We demonstrate the power of our approach Our method achieves superior performance	finding
This paper presents a novel solution that automatically transforms unlabeled	mechanism
heterogeneous motion data into new styles	mechanism
The key idea of our approach is an online learning algorithm that automatically constructs a series of local mixtures of autoregressive models ( MAR ) We construct local MAR models on the fly by searching for the closest examples of each input pose in the database	mechanism
Once the model parameters are estimated from the training data	mechanism
the model adapts the current pose with simple linear transformations	mechanism
In addition we introduce an efficient local regression model	mechanism
by transferring stylistic human motion for a wide variety of actions	method
including walking running	method
punching kicking jumping and transitions between those behaviors in a comparison against alternative methods	method
We have also performed experiments to evaluate the generalization ability of our data-driven model as well as the key components of our system	method
shows that in practical time	finding
Symbiosis generates DSPs that both isolate the small fraction of event orders and data-flows responsible for the failure	finding
and show which event reorderings prevent failing DSPs contain 81\ % fewer events and 96\ % fewer data-flows than the full failure-inducing schedules	finding
Moreover by allowing developers to focus on only a few events	finding
DSPs reduce the amount of time required to find a valid fix	finding
We present Symbiosis based on novel differential schedule projections ( DSPs )	mechanism
A DSP shows the small set of memory operations and data-flows responsible for a failure	mechanism
as well as a reordering of those elements that avoids the failure	mechanism
To build a DSP	mechanism
Symbiosis first generates a full	mechanism
failing multithreaded schedule via thread path profiling and symbolic constraint solving	mechanism
Symbiosis selectively reorders events in the failing schedule to produce a non-failing	mechanism
alternate schedule	mechanism
A DSP reports the ordering and data-flow differences between the failing and non-failing schedules	mechanism
Our evaluation on buggy real-world software and benchmarks In our experiments	method
Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval	background
The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index	mechanism
The proposed adjustment model relies on a concise optimization framework with interpretations	mechanism
The proposed index leverages the text-based inverted index for video retrieval	mechanism
Experimental	method
Multimedia event detection ( MED ) and multimedia event recounting ( MER ) are fundamental tasks in managing large amounts of unconstrained web videos	background
and have attracted a lot of attention in recent years	background
and obtain very promising results for both MED and MER	finding
we propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events Coupled in a joint optimization framework	mechanism
recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences To better utilize the powerful and interpretable semantic video representation	mechanism
we segment each video into several shots and exploit the rich temporal structures at shot level	mechanism
The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm	mechanism
which after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps	mechanism
enables us to efficiently process extremely large video corpora	mechanism
Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features	background
State-of-the-art results are shown for the task of generating novel image descriptions our automatically generated captions are equal to or preferred by humans 21	finding
0\ % of the time	finding
Critical to our approach is a recurrent neural network that attempts to dynamically build a visual representation of the scene as a caption is being generated or read	mechanism
The representation automatically learns to remember long-term visual concepts	mechanism
Our model is capable of both generating novel captions given an image	mechanism
and reconstructing visual features given an image description	mechanism
We evaluate our approach on several tasks	method
These include sentence generation	method
sentence retrieval and image retrieval When compared to human generated captions	method
Convolutional Neural Networks ( CNNs ) have achieved promising performance in image classification and action recognition tasks	background
demonstrate the promising performance of our method	finding
both for event detection and evidence recounting	finding
In this work	mechanism
we propose a flexible deep CNN infrastructure	mechanism
namely Deep Event Network ( DevNet )	mechanism
Taking key frames of videos as input	mechanism
we first detect the event of interest at the video level by aggregating the CNN features of the key frames	mechanism
The pieces of evidences which recount the detection results	mechanism
are also automatically localized	mechanism
both temporally and spatially	mechanism
The challenge is that we only have video level labels	mechanism
while the key evidences usually take place at the frame levels	mechanism
Based on the intrinsic property of CNNs	mechanism
we first generate a spatial-temporal saliency map by back passing through DevNet	mechanism
which then can be used to find the key frames which are most indicative to the event	mechanism
as well as to localize the specific spatial position	mechanism
usually an object	mechanism
in the frame of the highly indicative area	mechanism
show that our method compares competitively with the state of the art on both 2D and 3D measures	finding
while yielding a richer interpretation of the 3D scene behind the image	finding
In this paper	mechanism
we propose a novel algorithm that infers the 3D layout of building facades from a single 2D image of an urban scene	mechanism
Different from existing methods that only yield coarse orientation labels or qualitative block approximations	mechanism
our algorithm quantitatively using a set of planes mutually related by 3D geometric constraints	mechanism
Each plane is characterized by a continuous orientation vector and a depth distribution	mechanism
An optimal solution is reached through inter-planar interactions Due to the quantitative and plane-based nature of our geometric reasoning	mechanism
our model is more expressive and informative than existing approaches	mechanism
Experiments	method
two of the most prevalent sources of data on the Web	background
Blogs consist of sequences of images and associated text : they portray events and experiences with concise sentences and representative images	background
In the opposite direction	background
blog posts can be enhanced with sets of photo streams by showing interpolations between consecutive images in the blogs	background
we demonstrate that blog posts and photo streams are mutually beneficial for summarization	finding
exploration semantic knowledge transfer	finding
and photo interpolation	finding
We propose an approach that utilize large collections of photo streams and blog posts	mechanism
We leverage Hogs to help achieve story-based semantic summarization of collections of photo streams	mechanism
We formulate the problem of joint alignment from blogs to photo streams and photo stream summarization in a unified latent ranking SVM framework	mechanism
We alternate between solving the two coupled latent SVM problems	mechanism
by first fixing the summarization and solving for the alignment from blog images to photo streams and vice versa	mechanism
On a newly collected large-scale Disneyland dataset of 10K blogs ( 120K associated images ) and OK photo streams ( 540K images )	method
In contrast existing approaches either do not consider multiple object instances per video	background
or rely heavily on the motion of the objects present	background
demonstrate the effectiveness of our approach	finding
We present a semi-supervised approach that We start with a handful of labeled boxes and iteratively learn and label hundreds of thousands of object instances	mechanism
We propose criteria for for constraining the semi-supervised learning process and minimizing semantic drift	mechanism
Our approach does not assume exhaustive labeling of each object instance in any single frame	mechanism
or any explicit annotation of negative data	mechanism
Working in such a generic setting allow us to tackle multiple object instances in video	mechanism
many of which are static	mechanism
The experiments by evaluating the automatically labeled data on a variety of metrics like quality	method
coverage ( recall )	method
diversity and relevance to training an object detector	method
Contact behaviors in physics simulations are important for real-time interactive applications	background
especially in virtual reality applications where user 's body parts are tracked and interact with the environment via contact	background
For these contact simulations	background
it is ideal to have small changes in initial condition yield predictable changes in the output	background
Predictable simulation is key for success in iterative learning processes as well	background
such as learning controllers for manipulations or locomotion tasks	background
Our results confirmed that parameter settings do matter a great deal and suggest that there may be a trade-off between accuracy and predictability	background
We found that in the commonly available physics engines	finding
small changes in initial condition can sometimes induce different sequences of contact events to occur and ultimately lead to a vastly different result	finding
We first tune each engine to match an analytical solution as closely as possible and then compare the results for a more complex simulation	method
We present a co-clustering framework Unlike traditional clustering approaches which assume a one-to-one mapping between the clusters in the text-based feature space and the visual space	mechanism
we adopt a one-to-many mapping between the two spaces	mechanism
This is primarily because each semantic sense ( concept ) can correspond to different visual senses due to viewpoint and appearance variations Our structure-EM style optimization not only	mechanism
We envision a future time when wearable cameras ( e	background
g	background
small cameras in glasses or pinned on a shirt collar ) are worn by the masses and record first-person point-of-view ( POV ) videos of everyday life	background
Furthermore we show how our approach can enable several practical applications such as privacy filtering	background
automated video collection and social group discovery	background
Our proposed approach significantly improves self-search performance over several well-known face detectors and recognizers	finding
Motivated by these benefits and risks	mechanism
we develop a self-search technique tailored to first-person POV videos	mechanism
The key observation of our work is that the egocentric head motions of a target person ( i	mechanism
e	mechanism
the self ) are observed both in the POV video of the target and observer The motion correlation between the target person 's video and the observer 's video can then be used to uniquely identify instances of the self	mechanism
We incorporate this feature into our proposed approach that computes the motion correlation over supervoxel hierarchies to localize target instances in observer videos	mechanism
Semantic search in video is a novel and challenging problem in information and multimedia retrieval Existing solutions are mainly limited to text matching	background
in which the query words are matched against the textual metadata generated by users	background
We share our observations and lessons in building such a state-of-the-art system	background
which may be instrumental in guiding the design of the future system for semantic search in video	background
The novelty and practicality is demonstrated where the proposed system achieves the best performance	finding
This paper presents a state-of-the-art system The system relies on substantial video content understanding and allows for semantic search over a large collection of videos	mechanism
Many recent works propose mechanisms demonstrating the potential advantages of managing memory at a fine ( e	background
g	background
cache line ) granularity-e	background
g	background
fine-grained deduplication and fine-grained memory protection	background
We show that our framework can enable simple and efficient implementations of seven memory management techniques	finding
each of which has a wide variety of applications	finding
Our evaluations show that overlay-on-write	finding
when applied to fork	finding
can improve performance by 15\ % and reduce memory capacity requirements by 53\ % on average compared to traditional copy-on-write	finding
For sparse data computation	finding
our framework can outperform a state-of-the-art software-based sparse representation on a number of real-world sparse matrices	finding
Our framework is general	finding
powerful and effective in enabling fine-grained memory management at low cost	finding
We propose a new virtual memory framework In our framework	mechanism
each virtual page can be mapped to a structure called a page overlay	mechanism
in addition to a regular physical page	mechanism
An overlay contains a subset of cache lines from the virtual page	mechanism
Cache lines that are present in the overlay are accessed from there and all other cache lines are accessed from the regular physical page	mechanism
Our page-overlay framework enables cache-line-granularity memory management without significantly altering the existing virtual memory framework or introducing high overheads	mechanism
We quantitatively evaluate the potential benefits of two of these techniques : overlay-on-write and sparse-data-structure computation	method
Distributed in-memory key-value stores ( KVSs )	background
such as memcached	background
have become a critical data serving layer in modern Internet-oriented datacenter infrastructure	background
Their performance and efficiency directly affect the QoS of web services and the efficiency of datacenters Traditionally	background
these systems have had significant overheads from inefficient network processing	background
OS kernel involvement	background
and concurrency control	background
and start with a rigorous architectural characterization across system stacks over a collection of representative KVS implementations	mechanism
Our detailed full-system characterization not only identifies the critical hardware/software ingredients for high-petformance KVS systems	mechanism
but also leads to guided optimizations atop a recent design We craft a set of design principles for future platform architectures	mechanism
and via detailed simulations	method
Recovering the motion of a non-rigid body from a set of monocular images permits the analysis of dynamic scenes in uncontrolled environments However	background
the extension of factorisation algorithms for rigid structure from motion to the low-rank non-rigid case has proved challenging	background
We therefore make the recommendation that 3D reconstruction error always be measured relative to a trivial reconstruction such as a planar one	background
We elucidate that this greater difficulty is due to the need to find multiple solutions to a non-trivial problem	mechanism
casting a number of previous approaches as alleviating this issue by either a ) introducing constraints on the basis	mechanism
making the problems nonidentical	mechanism
or b ) incorporating heuristics to encourage a diverse set of solutions	mechanism
making the problems inter-dependent	mechanism
However we acknowledge that our method minimises an algebraic error and is thus inherently sensitive to deviation from the low-rank model	mechanism
We compare our closed-form solution for non-rigid structure with known cameras to the closed-form solution of Dai et al	method
These two types of images are captured from orthogonal viewpoints and have different resolutions	background
thus conveying very different types of information that can be used in a complementary way Moreover	background
their integration is necessary to enable an accurate understanding of changes in natural phenomena over massive city-scale landscapes	background
our proposed method is capable of generating detailed estimates of land surface conditions over an entire city	finding
The strategy proposed in this work uses macro-level imaging to learn the extent to which the land condition corresponds between land regions that share similar visual characteristics ( e	mechanism
g	mechanism
mountains streets buildings	mechanism
rivers ) whereas micro-level images are used to acquire high resolution statistics of land conditions ( e	mechanism
g	mechanism
the amount of debris on the ground )	mechanism
By combining macro- and micro-level information about regional correspondences and surface conditions	mechanism
A popular approach in this regard is to represent a sequence using a bag of words ( BOW ) representation due to its : ( i ) fixed dimensionality irrespective of the sequence length	background
and ( ii ) its ability to compactly model the statistics in A drawback to the BOW representation	background
however is the intrinsic destruction of the temporal ordering information	background
show significant performance improvements across both isolated and continuous event detection tasks	finding
In this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering Our representation	mechanism
like BOW is of a fixed dimensionality making it easily integrated with a linear detection function	mechanism
Extensive experiments on CK+	method
6DMG and UvA-NEMO databases	method
Occupancy count in rooms is valuable for applications such as room utilization	background
opportunistic meeting support	background
and efficient heating-cooling operations	background
In this paper we present the PerCCS algorithm that explores the possibility from CO2 sensors already integrated in everyday room airconditioning infrastructure	mechanism
PerCSS uses task-driven Sparse Non-negative Matrix Factorization ( SNMF ) to learn a nonnegative low-dimensional representation of the CO2 data in the preprocessing stage This denoised CO2 acts as the predictor variable for estimating occupancy count using Ensemble Least Square Regression	mechanism
Understanding the purpose of why sensitive data is used could help improve privacy as well as enable new kinds of access control	background
and achieved an accuracy of about 85\ % and 94\ % respectively in inferring purposes	finding
We have also found that text-based features alone are highly effective in inferring purposes	finding
In this paper	mechanism
we introduce a new technique We extract multiple kinds of features from decompiled code	mechanism
focusing on app-specific features and text-based features These features are then used to train a machine learning classifier	mechanism
We have evaluated our approach in the context of two sensitive permissions	method
namely ACCESS FINE LOCATION and READ CONTACT LIST	method
Some languages have very consistent mappings between graphemes and phonemes	background
while in other languages	background
this mapping is more ambiguous	background
Consonantal writing systems prove to be a challenge for Text to Speech Systems ( TTS ) because they do not indicate short vowels	background
which creates an ambiguity in pronunciation Special letter-to-sound rules may be needed for some cases in languages that otherwise have a good correspondence between graphemes and phonemes Our methods can be generalized to other languages that exhibit similar phenomena	background
and show significant improvements for dialects of Arabic	finding
We propose a technique during ITS training and predict pronunciations from text during synthesis time	mechanism
We conduct experiments on dialects of Arabic for disambiguating homographs and Hindi for discovering the schwa-deletion rules	mechanism
We evaluate our systems using objective and subjective metrics of TTS	method
Distant speech recognition ( DSR ) remains to be an open challenge	background
even for the state-of-the-art deep neural network ( DNN ) models	background
Previous work has attempted to improve DNNs under constantly distant speech	background
Our experiments show that in the simplest case	finding
incorporating the SMD descriptors improves word error rates of DNNs by 5	finding
6\ % relative	finding
Further optimizing SMD extraction and integration results in more gains	finding
Our solution is to incorporate the frame-level SMD information into DNN training	mechanism
Generation of the SMD information relies on a universal extractor that is learned on a meeting corpus	mechanism
We study the utility of different architectures in instantiating the SMD extractor	method
On our target acoustic modeling task	method
two approaches are proposed to build distance-aware DNN models using the SMD information : simple concatenation and distance adaptive training ( DAT )	method
Spoken Term Detection ( STD ) or Keyword Search ( KWS ) techniques can locate keyword instances but do not differentiate between meanings	background
We show that the distributed representation approach outperforms all other approaches	finding
regardless of the WER Although LDA-based approaches do well on clean data	finding
they degrade significantly with WER	finding
Paradoxically lower WER does not guarantee better SWSI performance	finding
due to the influence of common locutions	finding
In this paper we present a fully unsupervised SWSI approach based on distributed representations of spoken utterances To determine how ASR performance affects SWSI	mechanism
we used three different levels of Word Error Rate ( WER )	mechanism
40\ % 20\ % and 0\ % ; 40\ % WER is representative of online video	mechanism
0\ % of text	mechanism
We compare this approach to several others	method
including the state-of-the-art Hierarchical Dirichlet Process ( HDP )	method
Ensuring language coverage in dialog systems can be a challenge	background
since the language in a domain may drift over time	background
creating a mismatch between the original training data and current input	background
This in turn degrades performance by increasing misunderstanding and eventually leading to task failure	background
Without the capability of adapting the vocabulary and the language model based on certain domains or users	background
recognition errors may degrade the understanding performance	background
and even lead to a task failure	background
which incurs more time and effort to recover	background
show that both recognition and semantic parsing accuracy can thereby be improved	finding
by leveraging different types of relatedness between vocabulary items and words retrieved from web-based resources	mechanism
Our experiments	method
The experiments show that high-level semantic information can accurately estimate the prominence of slots	finding
significantly improving the slot induction performance ; furthermore	finding
a semantic decoder trained on the data with automatically extracted slots achieves about 68\ % F-measure	finding
which is close to the one from hand-crafted grammars	finding
Given unlabelled conversations	method
we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots ( e	method
g	method
both slots `` direction { '' } and `` locale { '' } convey location-related information ) for building a coherent semantic hierarchy	method
and then estimate the slot importance at different levels	method
The high-level semantic estimation involves not only within-slot but also cross slot relations	method
Self-adaptive systems overcome many of the limitations of human supervision in complex software-intensive systems by endowing them with the ability to automatically adapt their structure and behavior in the presence of runtime changes	background
However adaptation in some classes of systems ( e	background
g	background
safety-critical ) can benefit by receiving information from humans ( e	background
g	background
acting as sophisticated sensors	background
decision-makers ) or by involving them as system-level effectors to execute adaptations ( e	background
g	background
when automation is not possible	background
or as a fallback mechanism ) However	background
human participants are influenced by factors external to the system ( e	background
g	background
training level fatigue ) that affect the likelihood of success when they perform a task	background
its duration or even if they are willing to perform it in the first place	background
We illustrate our approach	finding
We contribute a formal framework	mechanism
focusing on the role of human participants as actors ( i	mechanism
e	mechanism
effectors ) during the execution stage of adaptation	mechanism
The approach consists of : ( i ) a language to express adaptation models that capture factors affecting human behavior and its interactions with the system	mechanism
and ( ii ) a formalization of these adaptation models as stochastic multiplayer games ( SMGs ) that can be used to analyze human-system-environment interactions	mechanism
in an adaptive industrial middleware used to monitor and manage sensor networks in renewable energy production plants	method
Our experiments demonstrate that our synthesis method is precise and efficient	finding
The implicit specification helped us find one concurrency bug previously missed when model-checking using an explicit	finding
user-provided specification and observed that different synchronization placements are produced for our experiments	finding
favoring a minimal number of synchronization operations or maximum concurrency	finding
respectively	finding
We present a computer-aided programming approach to concurrency	mechanism
and our synthesis procedure inserts synchronization The correctness specification is implicit	mechanism
inferred from the non-preemptive behavior	mechanism
Let us consider sequences of calls that the program makes to an external interface	mechanism
The specification requires that any such sequence produced under a preemptive scheduler should be included in the set of sequences produced under a non-preemptive scheduler	mechanism
We guarantee that our synthesis does not introduce deadlocks and that the synchronization inserted is optimal w	mechanism
r	mechanism
t	mechanism
a given objective function	mechanism
The solution is based on a finitary abstraction	mechanism
an algorithm for bounded language inclusion modulo an independence relation	mechanism
and generation of a set of global constraints over synchronization placements	mechanism
Each model of the global constraints set corresponds to a correctness-ensuring synchronization placement	mechanism
The placement that is optimal w	mechanism
r	mechanism
t	mechanism
the given objective function is chosen as the synchronization solution	mechanism
We apply the approach to device-driver programming	method
where the driver threads call the software interface of the device and the API provided by the operating system	method
We implemented objective functions for coarse-grained and fine-grained locking	method
Self-adaptive systems tend to be reactive and myopic	background
adapting in response to changes without anticipating what the subsequent adaptation needs will be	background
Adapting reactively can result in inefficiencies due to the system performing a suboptimal sequence of adaptations	background
Furthermore when adaptations have latency	background
and take some time to produce their effect	background
they have to be started with sufficient lead time so that they complete by the time their effect is needed	background
Proactive latency-aware adaptation addresses these issues by making adaptation decisions with a look-ahead horizon and taking adaptation latency into account	background
Our results show that the decision based on a look-ahead horizon	finding
and the factoring of both tactic latency and environment uncertainty	finding
considerably improve the effectiveness of adaptation decisions	finding
In this paper we present an approach that uses probabilistic model checking for adaptation decisions	mechanism
The key idea is to use a formal model of the adaptive system in which the adaptation decision is left underspecified through nondeterminism	mechanism
and have the model checker resolve the nondeterministic choices so that the accumulated utility over the horizon is maximized	mechanism
The adaptation decision is optimal over the horizon	mechanism
and takes into account the inherent uncertainty of the environment predictions needed for looking ahead	mechanism
Almost every complex software system today is configurable	background
While configurability has many benefits	background
it challenges performance prediction	background
optimization and debugging	background
Worse configuration options may interact	background
giving rise to a configuration space of possibly exponential size	background
demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them	finding
A series of experiments	method
Column subset selection ( CSS ) is the problem of selecting a small portion of columns from a large data matrix as one form of interpretable data summarization	background
Leverage score sampling	background
which enjoys both sound theoretical guarantee and superior empirical performance	background
is widely recognized as the state-of-the-art algorithm for column subset selection	background
and demonstrate its competitive performance show its superior performance in terms of both approximation accuracy and computational efficiency	finding
We conclude that further theoretical investigation and practical consideration should be devoted to iterative norm sampling in column subset selection	finding
under a wide range of experimental settings	method
We also compare iterative norm sampling with several of its other competitors and	method
In geometrically complex situations	background
where many surfaces intersect a pixel	background
current rendering systems shade each contributing surface at least once per pixel	background
As the sample density and geometric complexity increase	background
the shading cost becomes prohibitive for real-time rendering	background
Under deferred shading	background
so does the required framebuffer memory	background
We present Aggregate G-Buffer Anti-Aliasing ( AGAA )	mechanism
a new technique using modern graphics hardware AGAA uses the rasterization pipeline to generate a compact	mechanism
pre-filtered geometric representation inside each pixel	mechanism
We then shade this at a fixed rate	mechanism
independent of geometric complexity	mechanism
By decoupling shading rate from geometric sampling rate	mechanism
the algorithm reduces the storage and bandwidth costs of a geometry buffer	mechanism
and allows scaling to high visibility sampling rates for anti-aliasing	mechanism
Although substantial progress has been achieved in speech-to-speech translation systems over the last few years	background
from the other end and expects that speech alone is available in the target language	mechanism
and that no ( standard or nonstandard ) orthography exists It	mechanism
therefore treats the acoustic representation of the language as primary and uses language-independent methods that is then used in the translation system Thus	mechanism
the speech translation system is created for the target language as defined by the recording of that language rather than some body of orthographic transcripts In this work	mechanism
we are creating an application called APT ( Acoustic Patient Translator ) which uses a novel scheme of speech recognition and translation within a targeted domain	mechanism
By working with a set of predefined sentences appropriately chosen to fit a scenario	mechanism
we use utterance classification as a speech recognition algorithm The utterance classification is achieved using cross-lingual	mechanism
language-independent phonetic labeling	mechanism
Since we are working with a set of select phrases	mechanism
the translation part is trivial	mechanism
We are concentrating on communication with hospital staff	method
such as scheduling a doctor 's appointment	method
as our domain	method
In addition to English	method
we also run experiments on Tamil	method
Traditional automated response grading approaches use manually engineered time-aggregated features ( such as mean length of pauses	background
We find such models reach the best performance in terms of correlation with human raters We also find that when there are limited time-aggregated features available	finding
our model that incorporates time-sequence features improves performance drastically	finding
We introduce a new method We propose to incorporate general time-sequence features ( such as pitch ) which preserve more information than time-aggregated features and do not require human effort to design	mechanism
We use a type of recurrent neural network to jointly optimize the learning of high level abstractions from time-sequence features with the time-aggregated features	mechanism
We first automatically learn high level abstractions from time-sequence features with a Bidirectional Long Short Term Memory ( BLSTM ) and then combine the high level abstractions with time-aggregated features in a Multilayer Perceptron ( MLP ) /Linear Regression ( LR )	mechanism
We optimize the BLSTM and the MLP/LR jointly	mechanism
Programmers often need to revert some code to an earlier state	background
or restore a block of code that was deleted a while ago	background
However support for this backtracking in modern programming environments is limited	background
showed that programmers can successfully use AZURITE	finding
and were twice as fast as when limited to conventional features	finding
In this paper	mechanism
we present AZURITE	mechanism
an Eclipse plug-in With AZURITE	mechanism
programmers can easily perform backtracking tasks	mechanism
even when the desired code is not in the undo stack or a version control system	mechanism
AZURITE also provides novel user interfaces specifically designed for selective undo	mechanism
which were iteratively improved through user feedback gathered from actual users in a preliminary field trial	mechanism
A formal lab study	method
We prove the conjecture	finding
We suggest attempting the conjecture in the case that X-1	mechanism
X-n are the leaves of an information flow tree	mechanism
in the case that the information flow tree is a caterpillar graph ( similar to a two-state hidden Markov model )	method
Vehicular networks are inherently unstable networks with high mobility and intermittent connectivity	background
These networks can greatly benefit from Delay Tolerant Networking ( DTN ) solutions for opportunistic connectivity in the transmission of delaytolerant data	background
experimental insight and extract lessons for DTN routing protocol design	finding
Syntax extension mechanisms are powerful	background
but reasoning about syntax extensions can be difficult	background
Recent work on type-specific languages ( TSLs ) addressed reasoning about composition	background
hygiene and typing for extensions introducing new literal forms	background
and show interesting where the two mechanisms operate in concert	finding
We supplement TSLs with typed syntax macros ( TSMs )	mechanism
which unlike TSLs	mechanism
are explicitly invoked we describe two flavors of term-level TSMs : synthetic TSMs specify the type of term that they generate	mechanism
while analytic TSMs can generate terms of arbitrary type	mechanism
but can only be used in positions where the type is otherwise known	mechanism
At the level of types	mechanism
we describe a third flavor of TSM	mechanism
use cases	method
Multimodal analysis has long been an integral part of studying learning	background
Historically multimodal analyses of learning have been extremely laborious and time intensive and the implications that this work may have in non-education-related contexts	background
we find that affect-and pose-based segmentation are more effective	finding
than traditional approaches	finding
for drawing correlations between learning-relevant constructs	finding
and multimodal behaviors We also find that pose-based segmentation outperforms the two more traditional segmentation strategies for predicting student success on the hands-on task	finding
our results	finding
In particular we propose affect-and pose-based data segmentation	mechanism
as alternatives to human-based segmentation	mechanism
and fixed-window segmentation	mechanism
we present a comparative analysis of four different data segmentation techniques	method
In a study of ten dyads working on an open-ended engineering design task	method
One powerful aspect of 3D printing is its ability to extend	background
repair or more generally modify everyday objects	background
Our validation helps to illustrate the strengths and weaknesses of each technique	finding
For example we characterize how surface curvature and roughness affect print-over 's strength compared to the conventional print-in-one-piece	method
Do we really need 3D labels in order to learn how to predict 3D ?	background
Despite never seeing a 3D label	finding
our method produces competitive results	finding
Rather than use explicit supervision	mechanism
we use the regularity of indoor scenes	mechanism
We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D is unavailable	method
precluding supervised learning	method
Contemporary approaches extract features from a single output layer	background
While finetuning such models helps performance	finding
we show that even `` off-the-self { '' } multi-scale features perform quite well	finding
and demonstrate state-of-the-art classification performance our results reduce the lowest previously-reported error by 23	finding
9\ % and 9	finding
5\ % respectively	finding
By extracting features from multiple layers	mechanism
one can simultaneously reason about high	mechanism
mid and low-level features during classification	mechanism
The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph ( DAG-CNNs )	mechanism
We use DAG-CNNs	mechanism
We present extensive analysis on three standard scene benchmarks ( SUN397	method
MIT67 and Scene15 ) In terms of the heavily benchmarked MIT67 and Scene15 datasets	method
Given only a large	mechanism
unlabeled image collection	mechanism
we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first	mechanism
Mobile applications frequently access sensitive personal information to meet user or business requirements	background
Because such information is sensitive in general	background
regulators increasingly require mobile-app developers to publish privacy policies that describe what information is collected	background
Furthermore regulators have fined companies when these policies are inconsistent with the actual data practices of mobile apps	background
we propose a semi-automated framework that consists of a policy terminology-API method map that links policy phrases to API methods that produce sensitive information	mechanism
and information flow analysis to detect misalignments	mechanism
We present an implementation of our framework based on a privacy-policy-phrase ontology and a collection of mappings from API methods to policy phrases	method
Specifically inspired by curriculum learning	mechanism
we present a two-step approach for CNN training	mechanism
First we use easy images to train an initial visual representation	mechanism
We then use this initial CNN and adapt it to harder	mechanism
more realistic images by leveraging the structure of data and categories	mechanism
While feedforward deep convolutional neural networks ( CNNs ) have been a great success in computer vision	background
it is important to note that the human visual cortex generally contains more feedback than feedforward connections	background
demonstrate its effectiveness in solving tasks such as image classification and object localization	finding
a computational feedback mechanism in deep neural networks In addition to the feedforward inference in traditional neural networks	mechanism
a feedback loop is introduced according to the `` goal { '' } of the network	mechanism
e	mechanism
g	mechanism
high-level semantic labels	mechanism
We analogize this mechanism as `` Look and Think Twice	mechanism
{ '' } The feedback networks help better visualize and understand how deep neural networks work	mechanism
and capture visual attention on expected objects	mechanism
even in images with cluttered background and multiple	mechanism
Experiments on ImageNet dataset	method
Varied sources of error contribute to the challenge of facial action unit detection	background
Previous approaches address specific and known sources	background
However many sources are unknown	background
With few exceptions	finding
CPM outperformed baseline and state-of-the art methods	finding
we propose a Confident Preserving Machine ( CPM that follows an easy-to-hard classification strategy	mechanism
During training CPM learns two confident classifiers	mechanism
A confident positive classifier separates easily identified positive samples from all else ; a confident negative classifier does same for negative samples	mechanism
During testing CPM then learns a person-specific classifier using `` virtual labels { '' } provided by confident classifiers	mechanism
This step is achieved using a quasi-semi-supervised ( QSS ) approach	mechanism
Hard samples are typically close to the decision boundary	mechanism
and the QSS approach disambiguates them using spatio-temporal constraints	mechanism
To evaluate CPM	method
we compared it with a baseline single-margin classifier and state-of-the-art semi-supervised learning	method
transfer learning and boosting methods in three datasets of spontaneous facial behavior	method
Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent	background
semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ	background
LDA classifiers have two distinct benefits : ( i ) they exhibit higher average precision than similarity metrics typically used in correspondence problems	mechanism
and ( ii ) unlike exemplar SVM	mechanism
can output globally interpretable posterior probabilities without calibration	mechanism
whilst also being significantly faster to train	mechanism
We pose the correspondence problem as a graphical model	mechanism
where the unary potentials are computed via convolution with the set of exemplar classifiers	mechanism
and the joint potentials enforce smoothly varying correspondence assignment	mechanism
Starting with the seminal work by Kempe et al	background
a broad variety of problems	background
such as targeted marketing and the spread of viruses and malware	background
have been modeled as selecting a subset of nodes to maximize diffusion through a network	background
In cyber-security applications	background
however a key consideration largely ignored in this literature is stealth	background
In particular an attacker often has a specific target in mind	background
but succeeds only if the target is reached ( e	background
g	background
by malware ) before the malicious payload is detected and corresponding countermeasures deployed	background
The dual side of this problem is deployment of a limited number of monitoring units	background
such as cyber-forensics specialists	background
so as to limit the likelihood of such targeted and stealthy diffusion processes reaching their intended targets	background
show that a number of natural variants of this problem are NP-hard to approximate	finding
On the positive side	finding
we show that if stealthy diffusion starts from randomly selected nodes	finding
the defender 's objective is submodular	finding
and a fast greedy algorithm has provable approximation guarantees	finding
show that the proposed algorithms are highly effective and scalable	finding
In addition we present approximation algorithms by adaptively selecting the starting nodes for the diffusion process	mechanism
Our experimental results	method
Poor spelling is a challenge faced by people with dyslexia throughout their lives	background
Spellcheckers are therefore a crucial tool for people with dyslexia	background
but current spellcheckers do not detect real-word errors	background
which are a common type of errors made by people with dyslexia	background
Real-word errors are spelling mistakes that result in an unintended but real word	background
for instance form instead of from	background
Nearly 20\ % of the errors that people with dyslexia make are real-word	background
and showed that it detects more of these errors than widely used spellcheckers	finding
people with dyslexia corrected sentences more accurately and in less time with Real Check	finding
In this paper	mechanism
we introduce a system called Real Check that uses a probabilistic language model	mechanism
a statistical dependency parser and Google n-grams	mechanism
Today many data-driven web pages present information in a way that is difficult for blind and low vision users to navigate and to understand	background
EnTable as accessible tables	mechanism
EnTable allows blind and low vision users to submit requests for pages they wish to access	mechanism
The system then employs sighted informants to markup the desired page with semantic information	mechanism
allowing the page to be re-written using straightforward < table > tags	mechanism
Screen reader users who browse the web using the EnTable browser extension can report data sets that are confusing	mechanism
and utilize data sets re-written with the < table > tag based on their own requests or on the requests of other users	mechanism
Large scale deployment of sensors is essential to practical applications in cyber physical systems	background
For instance instrumenting a commercial building for `smart energy ' management requires deployment and operation of thousands of measurement and metering sensors and actuators that direct operation of the HVAC system	background
Each of these sensors need to be named consistently and constantly calibrated	background
we show that Zodiac can successfully classify sensors with an average accuracy of 98\ % with 28\ % fewer training examples when compared to a regular expression based method	finding
we propose Zodiac-a framework based on active learning from sensor metadata	mechanism
In contrast to prior work	mechanism
Zodiac requires minimal user input in terms of labelling examples while being more accurate	mechanism
To evaluate Zodiac	method
we deploy it across four real buildings on our campus and label the ground truth metadata for all the sensors in these buildings manually	method
Using a combination of hierarchical clustering and random forest classifiers	method
Modern cyber-physical systems interact closely with continuous physical processes like kinematic movement	background
Software component frameworks do not provide an explicit way to represent or reason about these processes	background
Meanwhile hybrid program models have been successful in proving critical properties of discrete-continuous systems	background
These programs deal with diverse aspects of a cyber-physical system such as controller decisions	background
component communication protocols	background
and mechanical dynamics	background
requiring several programs to address the variation	background
However currently these aspects are often intertwined in mostly monolithic hybrid programs	background
which are difficult to understand	background
change and organize	background
architectural models We build formal architectural abstractions of hybrid programs and formulas	mechanism
enabling analysis of hybrid programs at the component level	mechanism
reusing parts of hybrid programs	mechanism
and automatic transformation from views into hybrid programs and formulas	mechanism
Our approach is evaluated in the context of a robotic collision avoidance case study	method
Dependencies among software projects and libraries are an indicator of the often implicit collaboration among many developers in software ecosystems	background
Negotiating change can be tricky : changes to one module may cause ripple effects to many other modules that depend on it	background
yet insisting on only backward-compatible changes may incur significant opportunity cost and stifle change	background
we are finding that developers in fact struggle with change	finding
that they often use adhoc mechanisms to negotiate change	finding
and that existing awareness mechanisms like Github notification feeds are rarely used due to information overload	finding
outline a vision toward a change-based awareness system	mechanism
In ongoing interviews with developers in two software ecosystems ( CRAN and Node	method
js ) We study the state of the art and current information needs and	method
Research has shown that understanding conversational structure between students is paramount to evaluating the productivity of the collaboration and estimating outcomes	background
However previous methods often rely on human supplied dialogue act labels or discourse parsing algorithms requiring large labeled datasets	background
In this paper we present a new method In particular	mechanism
we introduce a machine learning method	mechanism
for example when one student provides the answer to a question or comments on something another student previously said Our method	mechanism
which utilizes a fast	mechanism
exact optimization process known as spectral optimization	mechanism
does not require manually annotated training data and is highly scalable and generalizable	mechanism
Empirical using real world datasets consisting of conversations between students participating in Coursera courses	method
Mobile and web applications increasingly leverage service-oriented architectures in which developers integrate third-party services into end user applications	background
This includes identity management	background
mapping and navigation	background
cloud storage and advertising services	background
among others	background
The study results include detected conflicts and violations of the principles as well as two patterns for balancing privacy and data use flexibility in requirements specifications	finding
show that reasoning over complex compositions of multi-party systems is feasible within exponential asymptotic timeframes proportional to the policy size	finding
the number of expressed data	finding
and orthogonal to the number of conflicts found	finding
we propose new techniques based on Description Logic ( DL ) which are prominent privacy properties found in international standards and guidelines	mechanism
We evaluate our techniques in an empirical case study that examines the data practices of the Waze mobile application and three of their service providers : Facebook Login	method
Amazon Web Services ( a cloud storage provider )	method
and Flurry	method
com ( a popular mobile analytics and advertising platform )	method
Analysis of automation reasoning over the DL models	method
In software development	background
IDE services such as syntax highlighting	background
code completion and `` jump to declaration { '' } are used to assist developers in programming	background
is available at http : //www	finding
youtube	finding
com/watch ? v=w1TECeRXGrg	finding
In this work	mechanism
we introduce Varis Technically	mechanism
we first perform symbolic execution on a PHP program to approximate all possible variations of the generated client-side code and subsequently parse this client code into a VarDOM that compactly represents all its variations	mechanism
Finally using the VarDOM	mechanism
we implement various types of IDE services for embedded client code including syntax highlighting	mechanism
code completion and `` jump to declaration { '' }	mechanism
The video demonstration for Varis	method
To ensure quality and trustworthiness of mobile apps	background
Google Play store imposes various developer policies	background
Once an app is reported for exhibiting policy-violating behaviors	background
it is removed from the store to protect users	background
Currently Google Play store relies on mobile users ' feedbacks to identify policy violations	background
The distance-keeping target can either be used for lane following for a standalone ACC system or an autonomous vehicle	background
Our object tracking algorithm can also be extended to find the target of interest for lane changing or ramp merging for an autonomous vehicle	background
We demonstrate that the overall performance of the proposed algorithm is better than that of a commercial ACC system	finding
We propose a robust object tracking algorithm Taking advantage of a context-based region of interest	mechanism
we are able to maximize the performance of each sensor	mechanism
and reduce the computation time since we only focus on the targets inside the region Tracking targets in road coordinates enables finding the distance-keeping target on any curved road	mechanism
while a commercial Adaptive Cruise Control ( ACC ) system works best on straight roads	mechanism
Suppose you are a teacher	background
and have to convey a set of object-property pairs ( 'lions eat meat ' ; or `aspirin is a blood-thinner ' )	background
A good teacher will convey a lot of information	background
with little effort on the student side	background
Specifically given a list of objects ( like animals or medical drugs ) and their associated properties	background
what is the best and most intuitive way to convey this information to the student	background
without the student being overwhelmed	background
it is effective achieving excellent results on real data	finding
both with respect to our proposed metric	finding
but also with respect to encoding length demonstrate the effectiveness of HYTRA	finding
We also design a multi-pronged algorithm	mechanism
HYTRA Our proposed HYTRA is scalable ( near-linear in the dataset size ) ; and it is intuitive	mechanism
conforming to well-known educational principles	mechanism
such as grouping related concepts	mechanism
and `` comparing { '' } and `` contrasting { '' }	mechanism
Experiments on real and synthetic datasets	method
This approach is complementary to other efforts in the literature on speeding up computation through GPU implementation	background
fast matrix operations	background
or quantization in that any of these optimizations can be incorporated	background
we are able to dramatically reduce the rate of growth of computation as the number of models increases	finding
show that we are able to maintain	finding
or even exceed	finding
the level of performance compared to the default approach of using all the models directly	finding
in both detection and classification tasks	finding
We show that by formulating the visual task as a large matrix multiplication problem	mechanism
something that is possible for a broad set of modern detectors and classifiers The approach	mechanism
based on a bilinear separation model	mechanism
combines standard matrix factorization with a task-dependent term which ensures that the resulting smaller size problem maintains performance on the original task	mechanism
Experiments	method
The rapid growth of cloud storage systems calls for fast and scalable namespace processing	background
In this paper we explore explicit replication of directory lookup state in all servers Both eliminate most repeated RPCs to different servers in order Our realization for server replicated directory lookup state	mechanism
ShardFS employs a novel file system specific hybrid optimistic and pessimistic concurrency control favoring single object transactions over distributed transactions	mechanism
Our experimentation	method
Humans play an active role in the execution of certain kinds of programs	background
such as spreadsheets	background
workflows and interactive notebooks	background
Interacting closely with execution is especially useful when end-users are learning from examples while doing their to derive implications for the design of interactive and mixed-initiative programming languages	background
These protocols present a linear	finding
idealized process despite the complex contingencies of the lab work they describe	finding
However they employ a variety of techniques for limiting or expanding the semantic interpretation of individual steps and for integrating outside protocols	finding
We use these observations	finding
we investigated a particularly rigid and formalized category of `` program { '' } people write for each other : lab protocols	method
Requirements analysts can model regulated data practices to identify and reason about risks of noncompliance	background
If terminology is inconsistent or ambiguous	background
however these models and their conclusions will be unreliable	background
Tregex is a utility to match regular expressions against constituency parse trees	background
which are hierarchical expressions of natural language clauses	background
including noun and verb phrases	background
Cilk Plus and OpenMP are parallel language extensions for the C and C++ programming languages	background
The CPLEX Study Group of the ISO/IEC C Standards Committee is developing a proposal for a parallel programming extension to C that combines ideas from Cilk Plus and OpenMP	background
We found several usability problems worthy of further investigation based on student performance	finding
including declaring and using reductions	finding
multi-line compiler directives	finding
and the understandability of task assignment to threads	finding
We conducted a preliminary comparison of Cilk Plus and OpenMP in a master 's level course on security	method
The widespread presence of motion sensors on users ' personal mobile devices has spawned a growing research interest in human activity recognition ( HAR )	background
Our results indicate that on-device sensor and sensor handling heterogeneities impair HAR performances significantly	finding
Moreover the impairments vary significantly across devices and depends on the type of recognition technique used	finding
propose a novel clustering-based mitigation technique suitable	mechanism
where heterogeneity of devices and their usage scenarios are intrinsic	mechanism
The method has been validated Experimental findings strongly support the validity of real-time	finding
3D registration and reconstruction from 2D video	finding
The software is available online at http : //zface	finding
org	finding
in a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction	method
Cultural events are kinds of typical events closely related to history and nationality	background
which play an important role in cultural heritage through generations	background
by combining both ideas of object / scene contents mining and strong image representation via CNN into a whole framework	mechanism
Specifically  we employ selective search to extract a batch of bottom-up region proposals	mechanism
which are served as key object / scene candidates in each event image ; while	mechanism
we investigate two state-of-the-art deep architectures	mechanism
VGGNet and GoogLeNet	mechanism
and adapt them to our task by performing domain-specific ( i	mechanism
e	mechanism
event ) fine-tuning on both global image and hierarchical region proposals	mechanism
These two models can complementarily exploit feature hierarchies spatially	mechanism
which simultaneously capture the global context and local evidences within the image	mechanism
to develop a method Our method helps reduce the number of comparison between definitions across multiple jurisdictions as well as allows software designers keep track of several inter-related definitions in a systematic way	mechanism
Automated program repair ( APR ) is a challenging process of detecting bugs	background
localizing buggy code	background
generating fix candidates and validating the fixes	background
Effectiveness of program repair methods relies on the generated fix candidates	background
and the methods used to traverse the space of generated candidates to search for the best ones Existing approaches generate fix candidates based on either syntactic searches over source code or semantic analysis of specification	background
e	background
g	background
test cases	background
achieving promising results	finding
In this paper	mechanism
we propose to combine both syntactic and semantic fix candidates We present an automated repair method based on structured specifications	mechanism
deductive verification and genetic programming	mechanism
Given a function with its specification	mechanism
we utilize a modular verifier to detect bugs and localize both program statements and sub-formulas in the specification that relate to those bugs While the former are identified as buggy code	mechanism
the latter are transformed as semantic fix candidates	mechanism
We additionally generate syntactic fix candidates via various mutation operators	mechanism
Best candidates which receives fewer warnings via a static verification	mechanism
are selected for evolution though genetic programming until we find one satisfying the specification	mechanism
Another interesting feature of our proposed approach is that we efficiently ensure the soundness of repaired code through modular ( or compositional ) verification	mechanism
We implemented our proposal and tested it on C programs taken from the SIR benchmark that are seeded with bugs	method
Our behavioral data supports In fact	finding
one third of our detection instances occurred during robot transit	finding
i	finding
e	finding
while the robots were making no verbal offer	finding
We find that candy accessibility dominates any social influence of robot orientation and that robot speed influences both whether people will interrupt a robot in transit ( slow more interruptible ) and whether they will respond to its verbal offer ( fast more salient )	finding
The advent of multi-core systems set off a race to get concurrent programming to the masses	background
and we show its application to the construction of concurrent software	finding
In this paper we propose an exception-handling model for concurrent systems Its main quality attributes are simplicity and expressiveness	mechanism
allowing programmers to deal with exceptional situations in a concurrent setting in a familiar way	mechanism
The proposal is centered on a new kind of exception type that defines new paths for exception propagation among concurrent threads of execution In our model	mechanism
beyond being able to control where exceptions are raised	mechanism
the developer can define in which thread	mechanism
and when during its execution	mechanism
a particular exception will be handled	mechanism
The proposed model has been implemented in Scala	method
The noise model of deletions poses significant challenges in coding theory	background
with basic questions like the capacity of the binary deletion channel still being open	background
The abovementioned results bring our understanding of deletion code constructions in these regimes to a similar level as worst case errors	background
Interface-confinement is a common mechanism that secures untrusted code by executing it inside a sandbox	background
The sandbox limits ( confines ) the code 's interaction with key system resources to a restricted set of interfaces	background
This practice is seen in web browsers	background
hypervisors and other security-critical systems	background
System M is the first program logic that allows proofs of safety for programs that execute adversary-supplied code without forcing the adversarial code to be available for deep static analysis	background
System M can be used to model and verify protocols as well as system designs	background
and prove the soundness of System M relative to the model	finding
We demonstrate the reasoning principles of System M	finding
Motivated by these systems	mechanism
we present a program logic	mechanism
called System M In addition to using computation types to specify effects of computations	mechanism
System M includes a novel invariant type to specify the properties of interface-confined code	mechanism
The interpretation of invariant type includes terms whose effects satisfy an invariant	mechanism
We construct a step-indexed model built over traces by verifying the state integrity property of the design of Memoir	method
a previously proposed trusted computing system	method
Information flow analysis has largely focused on methods that require access to the program in question or total control over an analyzed system	background
We reduce these problems to ones of causal inference Our systematic study leads to practical advice for detecting web data usage	finding
a previously unformalized area	finding
Leveraging this connection	mechanism
we provide a systematic black-box methodology based on experimental science and statistical analysis	mechanism
We formalize such limited information flow analyses and study an instance of it : detecting the usage of data by websites by proving a connection between noninterference and causation	method
We illustrate these concepts with a series of experiments collecting data on the use of information by websites	method
In today 's ubiquitous computing environment where the number of devices	background
applications and web services are ever increasing	background
human attention is the new bottleneck in computing	background
proved the effectiveness of Attelia showed that notifications at detected breakpoint timing resulted in 46\ % lower cognitive load compared to randomly-timed notifications	finding
further validated Attelia 's value	finding
with a 33\ % decrease in cognitive load compared to randomly-timed notifications	finding
we propose Attelia	mechanism
a novel middleware that identifies breakpoints in user interaction and delivers notifications at these moments	mechanism
Attelia works in real-time and uses only the mobile devices that users naturally use and wear	mechanism
without any modifications to applications	mechanism
and without any dedicated psycho-physiological sensors	mechanism
Applications such as construction monitoring and planning for renovations	background
require the accurate recovery of existing conditions of structures	background
We demonstrate the capability and robustness of our approach	finding
To address this issue	mechanism
this paper presents an approach from a 3D point cloud containing a complex network of thin structures	mechanism
and In our approach	mechanism
each beam is evolved from a seed by matching and aligning the cross section images	mechanism
This growing algorithm can model beams with arbitrary cross sections By performing the algorithm on a point connectivity graph	mechanism
we distinguish beams from joints and improve the algorithm 's robustness to closely spaced objects	mechanism
In parallel planes and joints are also extracted and modeled	mechanism
The connectivity graph of these primitives allows for a compact	mechanism
object-level understanding of the entire structure	mechanism
on both synthetic and real datasets	method
Many learning-based computer vision algorithms perform poorly when faced with examples that are dissimilar to those on which they were trained	background
show that the proposed approach outperforms existing single-step methods on a dataset of nine building styles	finding
We propose a two-step approach The first step uses a small number of labeled examples	mechanism
while the second step uses traditional domain adaptation methods	mechanism
Registration of Point Cloud Data ( PCD ) forms a core component of many 3D vision algorithms such as object matching and environment reconstruction	background
showing better convergence for a wider range of initial conditions and higher speeds than previous state of the art methods	finding
In this paper	mechanism
we introduce a PCD registration algorithm that utilizes Gaussian Mixture Models ( GMM ) and a novel dual-mode parameter optimization technique which we call mixture decoupling	mechanism
by first optimizing over the mixture parameters ( decoupling the mixture weights	mechanism
means and covariances from the points ) before optimizing over the 6DOF registration parameters	mechanism
Furthermore we frame both the decoupling and registration process inside a unified	mechanism
dual-mode Expectation Maximization ( EM ) framework	mechanism
for which we derive a Maximum Likelihood Estimation ( MLE ) solution along with a parallel implementation on the GPU	mechanism
We evaluate our MLE-based mixture decoupling ( MLMD ) registration method over both synthetic and real data	method
Formal verification of industrial systems is very challenging	background
due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams	background
More importantly industrial systems are rarely designed for verification	background
but rather for operational needs	background
The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team	background
Human swarm interaction ( HSI ) involves operators gathering information about a swarm 's state as it evolves	background
and using it to make informed decisions on how to influence the collective behavior of the swarm	background
In order to determine the proper input	background
an operator must have an accurate representation and understanding of the current swarm state	background
including what emergent behavior is currently happening	background
Our results show that	finding
while participants were good at recognizing all behaviors	finding
there are indeed differences between the three	finding
with rendezvous being easier to recognize than flocking or dispersion Furthermore	finding
differences in recognition are also affected by viewing time for flocking	finding
was also especially insightful for understanding how participants went about recognizing behaviors-allowing for potential avenues of research in future studies	finding
Feedback from participants	method
A server has constantly received various reference time series Q of length X and seeks the exact kNN over a collection of time series distributed across a set of M local sites	background
When X and M are large	background
and when the amount of query increases	background
simply sending each Q to all M sites incurs high communication bandwidth costs	background
which we would like to avoid	background
Prior work has presented a communication-efficient kNN algorithm for the Euclidean distance similarity measure	background
show that our method reduces communication bandwidth by up to 92\ %	finding
In this paper	mechanism
we present the first kNN algorithm	mechanism
which is generally believed a better measure for time series	mechanism
we design a new multi-resolution structure for the reference time series	mechanism
and multi-resolution lower bounds that can effectively prune the search space	mechanism
We present a new protocol between the server and the local sites that leverages multi-resolution pruning and cascading lower bounds	mechanism
Empirical studies on both real-world and synthetic data sets	method
A multi-faceted graph defines several facets on a set of nodes	background
Each facet is a set of edges that represent the relationships between the nodes in a specific context	background
Mining multi-faceted graphs have several applications	background
including finding fraudster rings that launch advertising traffic fraud attacks	background
tracking IP addresses of botnets over time	background
analyzing interactions on social networks and co-authorship of scientific papers	background
where NeSim is shown to be superior to MCL	finding
JP and AP	finding
the well-established clustering algorithms	finding
We also report the success stories of MuFace in finding advertisement click rings	finding
We propose NeSim	mechanism
a distributed efficient clustering algorithm that does We also propose optimizations to further improve the scalability	mechanism
the efficiency and the clusters quality	mechanism
We employ generalpurpose graph-clustering algorithms in a novel way Due to the qualities of NeSim	mechanism
we employ it as a backbone in the distributed MuFace algorithm	mechanism
which discovers multi-faceted communities	mechanism
We evaluate the proposed algorithms on several real and synthetic datasets	method
Dataflow analysis-based dynamic parallel monitoring ( DADPM ) is a recent approach for identifying bugs in parallel software as it executes	background
based on the key insight of explicitly modeling a sliding window of uncertainty across parallel threads	background
First by explicitly tracking new `` uncertain { '' } states in the metadata lattice	mechanism
Second as the analysis tool runs dynamically	mechanism
it can use the existence ( or absence ) of observed uncertain states For example	mechanism
we demonstrate how the epoch size parameter can be adjusted dynamically in response to uncertainty in order	mechanism
This paper shows how to adapt a canonical dataflow analysis problem ( reaching definitions ) and a popular security monitoring tool ( TAINTCHECK ) to our new uncertainty-tracking framework	method
and	method
Retinal vein cannulation is a demanding procedure proposed to treat retinal vein occlusion by direct therapeutic agent delivery methods	background
demonstrates a significant improvement in the total time the needle could be maintained stably inside of the vein	finding
This was especially evident in smaller veins and is attributed to decreased movement of the positioned cannula following venous cannulation	finding
In this study	mechanism
with an assistive system combining a handheld micromanipulator	mechanism
Micron with a force-sensing microneedle	mechanism
The integrated system senses the instant of vein puncture based on measured forces and the position of the needle tip	mechanism
The system actively holds the cannulation device securely in the vein following cannulation and during drug delivery	mechanism
Preliminary testing of the system in a dry phantom	method
stretched vinyl membranes	method
Public speaking has become an integral part of many professions and is central to career building opportunities Yet	background
public speaking anxiety is often referred to as the most common fear in everyday life and can hinder one 's ability to speak in public severely	background
While virtual and real audiences have been successfully utilized to treat public speaking anxiety in the past	background
little work has been done on identifying behavioral characteristics of speakers suffering from anxiety Complementary to automatic measures of anxiety	background
we are also interested in speakers ' perceptual differences when interacting with a virtual audience based on their level of anxiety in order to improve and further the development of virtual audiences for the training of public speaking and the reduction of anxiety	background
achieves a high correlation between ground truth and our estimation ( r=0	finding
825 )	finding
We identify several indicators for public speaking anxiety	method
among them are less eye contact with the audience	method
reduced variability in the voice	method
and more pauses	method
We automatically assess the public speaking anxiety as reported by the speakers through a self-assessment questionnaire using a speaker independent paradigm	method
Our approach using ensemble trees	method
Action Unit ( AU ) detection from facial images is an important classification task in affective computing	background
However most existing approaches use carefully engineered feature extractors along with off-the-shelf classifiers	background
There has also been less focus on how well classifiers generalize when tested on different datasets	background
indicate that our approach obtains competitive results on all datasets	finding
also indicate that the network generalizes well to other datasets	finding
even when under different training and testing conditions	finding
In our paper	mechanism
we propose a multi-label convolutional neural network approach	mechanism
Experiments on three AU datasets-CK+	method
DISFA and BP4D Cross-dataset experiments	method
Large teams of robots that operate collectively	background
whose behavior emerges from local interactions with neighbors	background
are known as swarms This research is necessary if real world swarms are to be deployed in the future	background
With these results	background
and with participant feedback about the helpfulness of the four display types	background
we hope future studies can make more informed decision about interface design when it comes to the control of swarms	background
Results show that summarizing the swarm 's current state to just an average position and bounding ellipse allowed predictions as accurate as those made when full state information was shown However	finding
such display methods were inferior for prediction than either the summary center and ellipse or full information methods	finding
In the study	method
participants are shown swarms performing one of three different behaviors	method
and are asked to use the information available from the display to make their predictions	method
Furthermore two leader-based methods were used	method
whereby the operators were shown only a small subset of the swarm	method
Planning for multirobot manipulation in dense clutter becomes particularly challenging as the motion of the manipulated object causes the connectivity of the robots ' free space to change and discuss future adaptations to general environments	background
Finally we show how to construct the FTG	finding
This paper introduces a data structure	mechanism
the Feasible Transition Graph ( FTG )	mechanism
and algorithms that We define an equivalence relation over object configurations based on the robots ' free space connectivity Within an equivalence class	mechanism
the homogeneous multirobot motion planning problem is straightforward	mechanism
which allows us The FTG captures transitions among the equivalence classes and encodes constraints that must be satisfied for the robots to manipulate the object	mechanism
From this data structure	mechanism
we readily derive a complete planner to coordinate such motion	mechanism
in some sample environments	method
Sharing scientific data	background
software and instruments is becoming increasingly common as science moves toward large-scale	background
distributed collaborations	background
Sharing these resources requires extra work to make them generally useful	background
Our results have important implications for future empirical studies as well as funding policy	background
Our findings indicate that they conduct a rich set of extra work around community management	finding
code maintenance education and training	finding
developer-user interaction and foreseeing user needs	finding
We identify several conditions under which they are likely to do this work	finding
as well as design principles that can facilitate it	finding
a qualitative interview-based study	method
Eye tracking is a compelling tool for revealing people 's spatial-temporal distribution of visual attention Such an approach will allow designers to evaluate and refine their visual design without requiring the use of limited/expensive eye trackers	background
which demonstrated good accuracy when compared to a real eye tracker	finding
and showed that it accurately generated gaze heatmaps and trajectory maps	finding
we introduce a new approach that harnesses the crowd In our approach	mechanism
crowdsourcing participants use mouse clicks to self-report the positions and trajectory for the following valuable eye tracking measures : first gaze	mechanism
last gaze and all gazes	mechanism
We validate our crowdsourcing approach with a user study	method
We then deployed our prototype	method
GazeCrowd in a crowdsourcing setting	method
In a variety of peer production settings	background
from Wikipedia to open source software development to crowdsourcing	background
individuals may encounter	background
edit or review the work of unknown others Typically this is done without much context to the person 's past behavior or performance	background
This work provides insight into the impact of activity history design factors on psychological and behavioral outcomes that can be of use in other related settings	background
Surprisingly negative work history did not lead to negative outcomes	finding
but in contrast	finding
a positive work history led to positive initial impressions that persisted in the face of contrary information	finding
we conducted an online experiment on Mechanical Turk varying the content	method
quality and presentation of information about another Turker 's work history	method
Telepresence means business people can make deals in other countries	background
doctors can give remote medical advice	background
and soldiers can rescue someone from thousands of miles away	background
When interaction is mediated	background
people are removed from and lack context about the person they are making decisions about	background
We discuss implications of our results for theory and future research	background
The results suggest that technological mediation influences decision making	finding
but its influence depends on an individual 's self-construal : participants who saw themselves as defined through their relationships ( interdependent self-construal ) recommended riskier and more painful treatments in video conferencing than when face-to-face	finding
We conducted a laboratory experiment involving medical treatment decisions	method
and discuss possible incentives and safeguards to context sharing from a user standpoint	background
that show how GCF 's ability to form groups increases users ' access to relevant and timely information	finding
In this paper	mechanism
we present the Group Context Framework ( GCF )	mechanism
a general-purpose toolkit GCF provides a standardized The framework then intelligently groups with other devices to satisfy these requirements Through two prototypes	mechanism
we demonstrate how GCF can be used to support a broad range of collaborative and cooperative tasks	mechanism
We then show how our framework 's architecture allows devices to opportunistically detect and collaborate with one another	mechanism
even when running different applications	mechanism
Finally we present two real-world domains	method
In many scenarios involving human interaction with a remote swarm	background
the human operator needs to be periodically updated with state information from the robotic swarm	background
A complete representation of swarm state is high dimensional and perceptually inaccessible to the human Thus	background
a summary representation is often required	background
In addition it is often the case that the human-swarm communication channel is extremely bandwidth constrained and may have high latency	background
This motivates the need for the swarm itself to compute a summary representation of its own state for transmission to the human operator	background
The summary representation may be generated by selecting a subset of robots	background
known as the information leaders	background
whose own states suffice to give a bounded approximation of the entire swarm	background
even in the presence of uncertainty	background
and proof of convergence for the algorithms demonstrating the performance and effectiveness of the proposed algorithms	finding
In this paper	mechanism
we propose two fully distributed asynchronous algorithms that only rely on inter-robot local communication	mechanism
In particular by representing noisy robot states as error ellipsoids with tunable confidence level	mechanism
the information leaders are selected such that the Minimum-Volume Covering Ellipsoid ( MVCE ) summarizes the noisy swarm state boundary	mechanism
We provide bounded optimality analysis We present simulation results	method
High-mobility walking robots offer unique capabilities in complex off-road environments where wheeled vehicles are not able to travel	background
Key steps in planning a safe path for the robot autonomously include estimating the height of the support ground surface - which is often occluded by vegetation - and classifying the terrain and obstacles above the ground surface	background
This paper describes the development and experimental evaluation of a terrain classification and ground surface height estimation system	mechanism
We provide experimental evaluation on an extensive	method
manually-labeled dataset collected from geographically diverse sites over a 28-month period	method
We present the design	mechanism
fabrication and characterization of a fiber optically sensorized robotic hand The robotic hand has three fingers that enable both pinch and power grips	mechanism
The main bone structure was made of a rigid plastic material and covered by soft skin Both bone and skin contain embedded fiber optics for force and tactile sensing	mechanism
respectively	mechanism
Eight fiber optic strain sensors were used for rigid bone force sensing	mechanism
and six fiber optic strain sensors were used for soft skin tactile sensing	mechanism
For characterization different loads were applied in two orthogonal axes at the fingertip and the sensor signals were measured from the bone structure The skin was also characterized by applying a light load on different places for contact localization	mechanism
The actuation of the hand was achieved by a tendon-driven under-actuated system Gripping motions are implemented using an active tendon located on the volar side of each finger and connected to a motor	mechanism
Opening motions of the hand were enabled by passive elastic tendons located on the dorsal side of each finger	mechanism
Our key insight is we can formalize the selection problem as the `` best arm { '' } variant of the multi-armed bandit problem	finding
We show that the successive rejects algorithm identifies the best candidate using fewer rollouts than a baseline algorithm in simulation	finding
We also show that selecting a good candidate increases the likelihood of successful execution on a real robot	finding
We present an algorithm We frame this as a selection problem where the goal is to choose the most robust trajectory from a finite set of candidates	mechanism
We generate each candidate using a kinodynamic state space planner and evaluate it using noisy rollouts	mechanism
We use the successive rejects algorithm to efficiently allocate rollouts between candidate trajectories given a rollout budget	mechanism
Because Simple Hand gives limited space for links	background
current iteration of links is not obviously nonlinear	background
This paper presents a novel nonlinear compliant link It has two major properties : bi-directionality and stiffening compliance	mechanism
Bi-directionality means it can be stretched and compressed	mechanism
and is realized by antagonistic arrangement of an extension spring and a compression spring	mechanism
Stiffening compliance means it becomes stiffer as it is stretched	mechanism
and is realized by asymmetric geometry	mechanism
The links are parts of Simple Hand	mechanism
A key challenge of developing robots that work closely with people is creating a user interface that allows a user to communicate complex instructions to a robot quickly and easily	background
to show that the proposed system is able to detect and track a leader through unconstrained and cluttered off-road environments under a wide variety of illumination and motion conditions	finding
This paper presents a marker tracking system that uses near infrared cameras	mechanism
retro-reflective markers and LIDAR	mechanism
In this application the robot is carrying equipment and supplies for a group of pedestrians	method
and the primary task for the user interface is to keep the robot traveling with the overall group in the right formation	method
We provide an extensive quantitative evaluation	method
Exploration of unknown environments is an important aspect to fielding teams of robots Without the ability to determine on their own where to go in the environment	background
the full potential of robotic teams is limited to the abilities of human operators to deploy them for search and rescue	background
mapping or other tasks that are predicated on gaining knowledge from the environment	background
This is of particular importance in real-world 3-Dimensional ( 3-D ) environments where simple planar assumptions can lead to incomplete exploration	background
for example real-world environments have areas underneath overhangs or inside caves	background
demonstrate its applicability	finding
In this paper	mechanism
we present a combined air-ground system We first describe the hardware and software components of the system We then present our algorithm for planning 3-D goal locations for a heterogeneous team of robots to efficiently explore a previously unknown environment and	mechanism
in real-world experiments	method
Navigating in a previously unknown environment and recognizing naturally occurring text in a scene are two important autonomous capabilities that are typically treated as distinct	background
We show that we are able to improve SLAM illustrating how location priors enable improved loop closure with text features	finding
In this work	mechanism
we propose a novel high-level feature descriptor	mechanism
the `` junction { '' }	mechanism
which is particularly well-suited to and is also	mechanism
through text spotting on datasets collected with a Google Tango	method
Robot perception is generally viewed as the interpretation of data from various types of sensors such as cameras suggesting further investigation on the use of non-visual perception in human-robot team operations	background
We use a special type of the Noisy-OR model known as BN2O model of Bayesian inference network	mechanism
As a proof-of-concept study	method
we specifically focus on a door detection problem in a stealth mission setting where a team operation must not be exposed to the visibility of the team 's opponents	method
on both synthetic data and real person tracking data	method
Using a combination of formal reasoning and physical intuition	method
we analyze and test successively more capable leaping behaviors through successively more complicated body mechanics	method
Theoretical contributions Conceptual contributions	method
Robotic swarms are distributed systems whose members interact via local control laws to achieve a variety of behaviors	background
such as flocking	background
In many practical applications	background
human operators may need to change the current behavior of a swarm from the goal that the swarm was going towards into a new goal due to dynamic changes in mission objectives	background
There are two related but distinct capabilities needed to supervise a robotic swarm The first is comprehension of the swarm 's state and the second is prediction of the effects of human inputs on the swarm 's behavior	background
Both of them are very challenging	background
Prior work in the literature has shown that inserting the human input as soon as possible to divert the swarm from its original goal towards the new goal does not always result in optimal performance ( measured by some criterion such as the total time required by the swarm to reach the second goal )	background
This phenomenon has been called Neglect Benevolence	background
conveying the idea that in many cases it is preferable to neglect the swarm for some time before inserting human input	background
Our results show that humans can learn to approximate optimal timing and that displays which make consensus variables perceptually accessible can enhance performance	finding
We developed the swarm configuration shape-changing Neglect Benevolence Task as a Human Swarm Interaction ( HSI ) reference task	mechanism
In the future	background
we envision domestic robots to play a large role in our everyday lives	background
This requires robots able to anticipate our needs and preferences and adapt their behavior	background
qualitative insights towards robot behavior during kitchen organization	finding
an open source dataset of real life kitchens	mechanism
through a home study	method
Our analysis includes and a proof-of-concept application of this dataset	method
We demonstrate that our design works in practice	finding
We develop a new paradigm By focusing on avoiding redundant computation we achieve a reduction of one to two orders of magnitude reduction in design area utilization as compared to previous implementations	mechanism
Experience Graphs have been shown to accelerate motion planning using parts of previous paths in an A { * } framework	background
Experience Graphs work by computing a new heuristic for weighted A { * } search on top of the domain 's original heuristic and the edges in an Experience Graph	background
The new heuristic biases the search toward relevant prior experience and uses the original heuristic for guidance otherwise	background
In previous work	background
Experience Graphs were always built on top of domain heuristics which were computed by dynamic programming ( a lower dimensional version of the original planning problem )	background
When the original heuristic is computed this way the Experience Graph heuristic can be computed very efficiently However	background
there are many commonly used heuristics in planning that are not computed in this fashion	background
such as euclidean distance	background
by making use of popular nearest neighbor algorithms	mechanism
Experimentally	method
Many robot applications involve lifelong planning in relatively static environments e	background
g	background
assembling objects or sorting mail in an office building In these types of scenarios	background
the robot performs many tasks over a long period of time	background
Thus the time required for computing a motion plan becomes a significant concern	background
prompting the need for a fast and efficient motion planner	background
Since these environments remain similar in between planning requests	background
planning from scratch is wasteful	background
We show the improvements with our method	finding
This work describes a method given changes in the environment by lazily evaluating the validity of past experiences during the planning process	mechanism
in a single-arm manipulation domain with simulations on the PR2 robot	method
The results produced by our algorithm	finding
In this paper	mechanism
we present an algorithm Here	mechanism
we seek to compute a schedule that will allow a fleet of agents to visit all targets of a given set while maximizing the frequency of visitation and maintaining a sufficient fuel capacity by refueling at depots	mechanism
We also present a heuristic method to allow us to compute bounded suboptimal results in real time will allow a team of robots to efficiently cover a given set of targets or tasks persistently over long periods of time	mechanism
even when the cost to transition between tasks is dynamic	mechanism
our approach produces 3D models that are more structurally representative of the environment being surveyed	finding
We show that the method produces reasonably accurate surface reconstruction and blending consistency	finding
with and without the use of a prior mesh	finding
using data collected from an autonomous underwater vehicle performing simultaneous localization and mapping ( SLAM ) Compared to other methods that generate a 2D-only mosaic We experimentally evaluate our approach with a Hovering Autonomous Underwater Vehicle ( HAUV ) performing inspection of a large underwater ship hull	method
Wearable cameras provide a first-person perspective which enables continuous visual hand grasp analysis of everyday activities	background
In contrast to previous work focused on manual analysis of first-person videos of hand grasps	background
we propose a fully automatic vision-based approach for grasp analysis A set of grasp classifiers are trained for discriminating between different grasp types based on large margin visual predictors Building on the output of these grasp classifiers	mechanism
visual structures among hand grasps are learned based on an iterative discriminative clustering procedure	mechanism
We first evaluated our classifiers on a controlled indoor grasp dataset and then validated the analytic power of our approach on real-world data taken from a machinist Analysis of real-world video	method
Assembly of large structures requires large fixtures	background
often referred to as monuments Their cost and massive size limit flexibility and scalability of the manufacturing process	background
Numerous small mobile robots can replace these large structures and	background
therefore replicate the efficiency of the assembly line with far more flexibility	background
An assembly line made up of mobile manipulators can easily and rapidly be reconfigured to support scalability and a varied product mix	background
while allowing for near optimal resource assignment	background
demonstrate these techniques	finding
In this paper	mechanism
we describe a set of techniques that we combine We describe and	mechanism
in the context of a testbed we implemented for assembling a wing ladder	method
is mapping each noun in the command into a physical object in the environment	background
Our experiments clearly show that the proposed approach is efficient for commanding outdoor robots	finding
We propose a language-driven navigation approach We consider unknown environments that contain previously unseen objects The proposed approach Robots receive from human teammates commands in natural language	mechanism
such as `` Navigate around the building to the car left of the fire hydrant and near the tree { '' }	mechanism
A robot needs first to classify its surrounding objects into categories	mechanism
using images obtained from its sensors	mechanism
The result of this classification is a map of the environment	mechanism
where each object is given a list of semantic labels	mechanism
such as `` tree { '' } and `` car { '' }	mechanism
with varying degrees of confidence	mechanism
Then the robot needs to ground the nouns in the command We use a probabilistic model	mechanism
such as `` left of { '' } and `` near { '' } The model is learned from examples provided by humans	mechanism
For each noun in the command	mechanism
a distribution on the objects in the environment is computed by combining spatial constraints with a prior given as the semantic classifier 's confidence values The robot needs also to ground the navigation mode specified in the command	mechanism
such as `` navigate quickly { '' } and `` navigate covertly { '' }	mechanism
as a cost map	mechanism
The cost map is also learned from examples	mechanism
using Inverse Optimal Control ( IOC )	mechanism
The cost map and the grounded goal are used to generate a path for the robot	mechanism
This approach is evaluated on a robot in a real-world environment	method
Here we present a general framework in a fundamental and first principle method The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity	mechanism
Then scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously	mechanism
with datasets collected in our own experiments as well as using the KITTI odometry benchmark In addition to comparison of the motion estimation accuracy	method
we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes	method
show that this can significantly improve the robot 's ability to accurately generalize the demonstration	finding
We formalize as an optimization problem over a Hilbert space of trajectories : minimize the distance between the demonstration and the new trajectory subject to the new end point constraints	mechanism
We show that the commonly used version of Dynamic Movement Primitives ( DMPs ) implement this minimization in the way they adapt demonstrations	mechanism
for a particular choice of the Hilbert space norm	mechanism
The generalization to arbitrary norms	mechanism
Our experiments	method
Many motion planning problems in robotics are high dimensional planning problems	background
While sampling-based motion planning algorithms handle the high dimensionality very well	background
the solution qualities are often hard to control due to the inherent randomization	background
In addition they suffer severely when the configuration space has several `narrow passages '	background
Search-based planners on the other hand typically provide good solution qualities and are not affected by narrow passages	background
However in the absence of a good heuristic or when there are deep local minima in the heuristic	background
they suffer from the curse of dimensionality	background
and show its benefits over these approaches	finding
In this work	mechanism
our primary contribution is a method in addition to the original heuristic ( s ) used	mechanism
With the ability to escape local minima easily	mechanism
the effect of dimensionality becomes less pronounced	mechanism
We compare our proposed method with the recently published Multi-Heuristic A { * } search	method
and the popular RRT-Connect in a full-body mobile manipulation domain for the PR2 robot	method
Autonomous systems that navigate in unknown environments encounter a variety of planning problems	background
This work opens the door on the more general problem of adaptive motion planning	background
We have developed a planning system that does this by running competing planners in parallel	mechanism
In this paper	mechanism
we present an approach that constructs a planner ensemble - a set of complementary planners that leverage a diverse set of assumptions	mechanism
Our approach optimizes the submodular selection criteria with a greedy approach and lazy evaluation	mechanism
We seed our selection with learnt priors on planner performance	mechanism
thus allowing us to solve new applications without evaluating every planner on that application	mechanism
in simulation from an autonomous helicopter	method
We demonstrate the ability to solve more rearrangement by pushing tasks than existing primitive based solutions	finding
Finally we show the plans we generate are feasible for execution on a real robot	finding
We present a randomized kinodynamic planner We embed a physics model into the planner to allow reasoning about interaction with objects in the environment By carefully selecting this model	mechanism
we are able to reduce our state and action space	mechanism
gaining tractability in the search The result is a planner capable of generating trajectories for full arm manipulation and simultaneous object interaction	mechanism
and show that on a variety of environments we can achieve a higher planning success rate given a restricted time budget for planning	finding
In this work we present a fast kinodynamic RRT-planner that uses dynamic nonprehensile actions In contrast to many previous works	mechanism
the presented planner is not restricted to quasi-static interactions and monotonicity	mechanism
Instead the results of dynamic robot actions are predicted using a black box physics model	mechanism
Given a general set of primitive actions and a physics model	mechanism
the planner randomly explores the configuration space of the environment to find a sequence of actions that transform the environment into some goal configuration	mechanism
In contrast to a naive kinodynamic RRT-planner we show that we can exploit the physical fact that in an environment with friction any object eventually comes to rest	mechanism
This allows a search on the configuration space rather than the state space	mechanism
reducing the dimension of the search space by a factor of two without restricting us to non-dynamic interactions	mechanism
We compare our algorithm against a naive kinodynamic RRT-planner	method
We present an execution monitoring framework In our approach	mechanism
plans are initially based on a model of the world that is only as faithful as computational and algorithmic limitations allow Through experience	mechanism
the monitor discovers previously unmodeled modes of the world	mechanism
defined as regions of a feature space in which the experienced outcome of a plan deviates significantly from the predicted outcome	mechanism
The monitor may then make suggestions to change the model to match the real world more accurately	mechanism
We demonstrate this approach on the adversarial domain of robot soccer : we monitor pass interception performance of potentially unknown opponents to try to find unforeseen modes of behavior that affect their interception performance	method
We demonstrate the advantages of our algorithm Our results show that spare work surfaces are beneficial to assembly	finding
Tilted work surfaces are only sometimes beneficial	finding
depending on the objects	finding
The goal of this paper is to develop a regrasp planning algorithm general enough We focus on pick-and-place regrasp which reorients an object from one placement to another by using a sequence of pickups and place-downs	mechanism
We improve the pick-and-place regrasp approach developed in 1990s and analyze its performance in robotic assembly with different work surfaces in the workcell Our algorithm will automatically compute the stable placements of an object	mechanism
find several force-closure grasps	mechanism
generate a graph of regrasp actions	mechanism
and search for regrasp sequences	mechanism
with various mesh models and use the algorithm to evaluate the completeness	method
the cost and the length of regrasp sequences with different mesh models and different assembly tasks in the presence of different work surfaces	method
Simultaneous localization and mapping with infinite planes is attractive because of the reduced complexity with respect to both sparse point-based and dense volumetric methods	background
to show its advantages over alternative solutions results	finding
using a homogeneous plane parametrization with a corresponding minimal representation for the optimization	mechanism
Because it is a minimal representation	mechanism
it is suitable for use with Gauss-Newton	mechanism
Powell 's Dog Leg and incremental solvers such as iSAM	mechanism
We also introduce a relative plane formulation We also introduce a simple mapping system and present	mechanism
We evaluate our proposed approach on simulated data experimental	method
showing real-time mapping of select indoor environments with a hand-held RGBD sensor	method
demonstrates applications	finding
We develop a computationally efficient control policy for active perception that incorporates explicit models of sensing and mobility Like previous work	mechanism
our policy maximizes an information-theoretic objective function between the discrete occupancy belief distribution ( e	mechanism
g	mechanism
voxel grid ) and future measurements that can be made by mobile sensors	mechanism
However our work is unique in three ways	mechanism
First we show that by using Cauchy-Schwarz Quadratic Mutual Information ( CSQMI )	mechanism
we get significant gains in efficiency	mechanism
Second while most previous methods adopt a myopic	mechanism
gradient-following approach that yields poor convergence properties	mechanism
our algorithm searches over a set of paths and is less susceptible to local minima	mechanism
In doing so	mechanism
we explicitly incorporate models of sensors	mechanism
and model the dependence ( and independence ) of measurements over multiple time steps in a path	mechanism
Third because we consider models of sensing and mobility	mechanism
our method naturally applies to both ground and aerial vehicles	mechanism
via simulation and experimentation	method
With the prevalence of social media	background
such as Twitter	background
short-length text like microblogs have become an important mode of text on the Internet	background
In contrast to other forms of media	background
such as newspaper	background
the text in these social media posts usually contains fewer words	background
and is concentrated on a much narrower selection of topics	background
For these reasons	background
traditional LDA-based sentiment and topic modeling techniques generally do not work well in case of social media data	background
Another characteristic feature of this data is the use of special meta tokens	background
such as hashtags	background
which contain unique semantic meanings that are not captured by other ordinary words	background
In this paper	mechanism
we propose probabilistic graphical models We first propose MTM ( Microblog Topic Model )	mechanism
a generative model that assumes each social media post generates from a single topic	mechanism
and models both words and hashtags separately	mechanism
We then propose MSTM ( Microblog Sentiment Topic Model )	mechanism
an extension of MTM	mechanism
which also embodies the sentiment associated with the topics	mechanism
We evaluated our models using Twitter dataset	method
and experimental	method
There have been increasing interests in the robotics community in building smaller and more agile autonomous micro aerial vehicles ( MAVs )	background
We present extensive statistical analysis to verify the performance of our approach in different environments with varying flight speeds	method
Learning from demonstration ( LfD ) is a common technique applied to many problems in robotics	background
such as populating grasp databases	background
training for reinforcement learning of high-level skill sets and bootstrapping motion planners	background
The data set collected has been made available to the robotics community	background
to teach a robot how to grasp	finding
to teach a robot how to perform dexterous manipulation tasks such as scooping and to accelerate motion planning for full-body manipulation tasks	finding
We also present experiments in which we apply demonstrations collected through our infrastructure	method
that our method can efficiently build maps of large indoor and outdoor environments in a distributed	finding
online and real-time setting	finding
We present a novel Expectation Maximization ( EM ) based approach by incorporating robot pose uncertainty An EM and hypothesis based method is used to determine a common reference frame	mechanism
We detail a 2D laser scan correspondence method to form robust correspondences between laser scans shared amongst robots	mechanism
The implementation is experimentally validated using teams of aerial vehicles	method
and analyzed to determine its accuracy	method
computational efficiency scalability to many robots	method
and robustness to varying environments	method
We demonstrate through multiple experiments	method
Our work is motivated by the potential impact of realistic simulators on the development cycle of software for real robots	background
Unlike calibration where the goal is to identify and remove error from a signal	background
The case is made for building models from approximate state information	mechanism
relieving the burden of ground truth	mechanism
Instead of physically modeling sensor behavior	mechanism
a data-driven approach is taken	mechanism
The implementation of our approach to simulate a simple but noisy laser rangefinder is described	method
Finally approaches to validate the simulator are discussed	method
We compare not only raw sensor predictions	method
but also overall performance of algorithms on simulated versus real data	method
Our first contribution is two discoveries : ( i ) the number of comments grows as a power-law on the number of votes and ( ii ) the time between a submission creation and a user 's reaction obeys a log-logistic distribution VNC outperformed state-of-the-art baselines on accuracy	finding
Additionally we illustrate VNC usefulness for forecasting and outlier detection	finding
Based on these patterns	mechanism
we propose VNC ( VOTE-AND-COMMENT )	mechanism
a parsimonious but accurate and scalable model that	mechanism
Autonomous mobile robots are required to operate in partially known and unstructured environments	background
It is imperative to guarantee safety of such systems for their successful deployment	background
Current state of the art does not fully exploit the sensor and dynamic capabilities of a robot	background
Also given the non-holonomic systems with non-linear dynamic constraints	background
it becomes computationally infeasible to find an optimal solution if the full dynamics are to be exploited online	background
We present results	finding
In this paper we present an online algorithm through an emergency maneuver library	mechanism
The maneuvers in the emergency maneuver library are optimized such that the probability of finding an emergency maneuver that lies in the known obstacle free space is maximized	mechanism
We prove that the related trajectory set diversity problem is monotonic and submodular which enables one to develop an efficient trajectory set generation algorithm with bounded sub-optimality	mechanism
We generate an off-line computed trajectory set that exploits the full dynamics of the robot and the known obstacle-free region	mechanism
One example is server-side scheduling for video service	background
where clients request flows of content from a server with limited capacity	background
and any content not delivered by its deadline is lost State-of-the-art policies	background
like Discriminatory Processor Sharing and Weighted Fair Queueing	background
use a fixed static proportional allocation of service rate and fail to achieve both goals	background
The well-known Earliest Deadline First policy minimizes overall loss	background
but fails to provide proportional loss across flows	background
because it treats packets as independent jobs	background
We prove that all policies in this broad class minimize overall loss Furthermore	finding
we demonstrate that many EPDF policies accurately differentiate loss fractions in proportion to class weights	finding
satisfying the second goal	finding
This paper introduces the Earliest Progressive Deadline First ( EPDF ) class of policies	mechanism
Theoretical and practical implications for collaboration across culture are discussed	background
we predicted and found that overall	finding
Chinese individuals were less helpful than Canadians	finding
This effect was stronger for males than females Interestingly	finding
more helping behavior was observed among Canadians with high levels of internal moral identity Yet	finding
this effect was not observed among Chinese individuals	finding
Human activity recognition is an important and challenging task for video content analysis and understanding	background
Individual activity recognition has been well studied recently	background
demonstrate effectiveness of the proposed method	finding
In this paper	mechanism
a novel human group activity recognition method is proposed this paper proposes three types of group-activity descriptor using motion trajectory and appearance information of people	mechanism
Experimental results on a public human group activity dataset	method
In many markets	background
products are highly complex with an extremely large set of features	background
In advertising auctions	background
for example an impression	background
i	background
e	background
a viewer on a web page	background
has numerous features describing the viewer 's demographics	background
browsing history temporal aspects	background
etc	background
In these markets	background
an auctioneer must select a few key features to signal to bidders	background
We present an efficient algorithmic in a setting where the product 's features are drawn independently from a known distribution	mechanism
the bidders ' values for a product are additive over their known values for the features of the product	mechanism
and the number of features is exponentially larger than the number of bidders and the number of signals	mechanism
Our approach involves solving a novel optimization problem regarding the expectation of a sum of independent random vectors that may be of independent interest	mechanism
We complement our positive result with a hardness result for the problem when features are arbitrarily correlated This result is based on the conjectured hardness of learning k-juntas	mechanism
a central open problem in learning theory	mechanism
Standard approaches to reachability problems for linear hybrid systems require numerical solutions for large optimization problems	background
and become infeasible for systems involving both nonlinear dynamics over the reals and stochasticity	background
We demonstrate SReach 's applicability	finding
In this paper	mechanism
we present a new tool SReach The first one is ( nonlinear ) hybrid automata with parametric uncertainty	mechanism
The second one is probabilistic hybrid automata with additional randomness for both transition probabilities and variable resets	mechanism
SReach encodes stochastic information by using a set of introduced random variables	mechanism
and combines delta-complete decision procedures and statistical tests to solve delta-reachability problems in a sound manner Compared to standard simulation-based methods	mechanism
it supports non-deterministic branching	mechanism
increases the coverage of simulation	mechanism
and avoids the zero-crossing problem	mechanism
by discussing three representative biological models and additional benchmarks for nonlinear hybrid systems with multiple probabilistic system parameters	method
Weighted signed networks ( WSNs ) are networks in which edges are labeled with positive and negative weights	background
WSNs can capture like/dislike	background
trust/distrust and other social relationships between people	background
We propose two novel measures of node behavior : the goodness of a node intuitively captures how much this node is liked/trusted by other nodes	mechanism
while the fairness of a node captures how fair the node is in rating other nodes ' likeability or trust level We provide axioms that these two notions need to satisfy and show that past work does not meet these requirements for WSNs	mechanism
We provide a mutually recursive definition of these two concepts and prove that they converge to a unique solution in linear time	mechanism
We use the two measures to predict the edge weight in WSNs	mechanism
that when compared against several individual algorithms from both the signed and unsigned social network literature We then use these as features in different multiple regression models	method
The collective buys energy as a group through a central coordinator who also decides about the storage and usage of renewable energy produced by the collective	background
Minimizing the cost is not only of interest to the consumers but is also socially desirable because it reduces the consumption at times of peak demand	background
We prove that our algorithm converges	finding
and it achieves the optimal solution We also present simulation results to quantify the performance of our algorithm	finding
We develop an iterative coordination algorithm in which the coordinator makes the storage decision and shapes the demands of the consumers by designing a virtual price signal for the agents	mechanism
under realistic conditions	method
based on real world consumption data	method
A key challenge in ITS research and development is to support tutoring at scale	background
for example by embedding tutors in MOOCs	background
The feasibility of this general approach to ITS/MOOC integration was demonstrated	finding
a widely used ITS authoring tool suite	mechanism
CTAT/TutorShop was modified Specifically	mechanism
the inner loop ( the example-tracing tutor engine ) was moved to the client by reimplementing it in JavaScript	mechanism
and the tutors were made compatible with the LTI e-learning standard	mechanism
a case study in which with simple tutors in an edX MOOC `` Data Analytics and Learning	method
{ '' }	method
To be able to provide better support for collaborative learning in Intelligent Tutoring Systems	background
it is important to understand how collaboration patterns change Although interactive talk is often held as a gold standard in collaboration	background
as students become more proficient	background
it may not be as important	background
We found that	finding
over time the frequency of interactive talk and errors both decrease in dyads working together on conceptual problems	finding
Better conversational alignment can lead to shared understanding	background
changed beliefs and increased rapport	background
provide guidelines for development of peer tutoring agents that can increase learning gains through subtle changes to improve tutor-tutee alignment	background
Our results which illustrate that rapport as well as convergence are significantly correlated with learning gains	finding
We develop an approach by accounting for the horizontal richness and time-based dependencies that arise in non-stationary and noisy longitudinal interaction streams	mechanism
Collaborative and individual learning appear to have complementary strengths ; however	background
the best way to combine these learning methods is still unclear	background
While previous work has demonstrated the effectiveness of Intelligent Tutoring Systems ( ITSs ) for individual learning	background
collaborative learning with ITSs is much less frequent - especially for young students	background
In addition we propose future research to understand how to best combine individual and collaborative learning within an ITS	background
Our previous findings demonstrate that ITSs are able to support collaboration	finding
as well as individual learning	finding
for this population	finding
Internet of Things ( IoT ) allows for cyber-physical applications to be created and composed to provide intelligent support or automation of end-user tasks For many of such tasks	background
human participation is crucial to the success and the quality of the tasks	background
The cyber systems should proactively request help from the humans to accomplish the tasks when needed	background
We illustrate our approach	finding
through an example of indoor air quality control in smart homes	method
we found MindMiner was easy to learn and use	finding
and could capture users ' implicit knowledge about writing performance and cluster target entities into groups that match subjects ' mental models We also found that MindMiner 's constraint suggestions and uncertainty polling functions could improve both efficiency and the quality of clustering	finding
We present MindMiner	mechanism
a mixed-initiative interface via a combination of new interaction techniques and machine learning algorithms	mechanism
MindMiner collects qualitative	mechanism
hard to express similarity measurements from users via active polling with uncertainty and example based visual constraint creation	mechanism
MindMiner also formulates human prior knowledge into a set of inequalities and learns a quantitative similarity distance metric via convex optimization	mechanism
In a 12-subject peer-review understanding task	method
Gestures during spoken dialog play a central role in human communication	background
As a consequence	background
models of gesture generation are a key challenge in research on virtual humans	background
embodied agents capable of face-to-face interaction with people	background
shows significant improvement over previous work on gesture prediction	finding
shows that DCNFs outperform the state-of-the-art approaches	finding
we proposed a gestural sign scheme and presented the DCNF model	mechanism
a model The approach we took realizes both the mapping relation between speech and gestures while taking account temporal relations among gestures	mechanism
Our experiments on human co-verbal dataset A generalization experiment performed on handwriting recognition also	method
Automated visual analysis is an effective method for understanding changes in natural phenomena over massive city-scale landscapes	background
Our results show that our approach can efficiently integrate both micro and macro-level images	finding
along with other forms of meta-data	finding
to efficiently estimate city-scale phenomena to show the ability of our method to generalize to a diverse set of estimation tasks	finding
This work presents a unified framework for robustly integrating image data taken at vastly different viewpoints	mechanism
Computer vision is increasingly becoming interested in the rapid estimation of object detectors	background
The canonical strategy of using Hard Negative Mining to train a Support Vector Machine is slow	background
since the large negative set must be traversed at least once per detector	background
Recent work has demonstrated that	background
with an assumption of signal stationarity	background
Linear Discriminant Analysis is able to learn comparable detectors without ever revisiting the negative set	background
Even with this insight	background
the time to learn a detector can still be on the order of minutes	background
Correlation filters on the other hand	background
can produce a detector in under a second However	background
this involves the unnatural assumption that the statistics are periodic	background
and requires the negative set to be re-sampled per detector size	background
These two methods differ chiefly in the structure which they impose on the covariance matrix of all examples	background
verified that periodicity is detrimental	finding
which develops techniques	mechanism
a comparative study It is experimentally	method
Given the re-broadcasts ( i	background
e	background
retweets ) of posts in Twitter	background
how can we spot fake from genuine user reactions ? What will be the tell-tale sign - the connectivity of retweeters	background
their relative timing	background
or something else ? High retweet activity indicates influential users	background
and can be monetized Hence	background
there are strong incentives for fraudulent users to artificially boost their retweets ' volume	background
Our main contributions are : ( a ) the discovery of patterns that fraudulent activity seems to follow ( the `` triangles { '' } and `` homogeneity { '' } patterns	finding
the formation of micro-clusters in appropriate feature spaces ) ; and	finding
b ) `` RTGen { '' }	mechanism
a realistic generator that mimics the behaviors of both honest and fraudulent users	mechanism
Given the retweeting activity for the posts of several Twitter users	background
Here we propose : ( A ) ND-Sync	mechanism
an efficient method for detecting group fraud	mechanism
and ( B ) a set of carefully designed features for characterizing retweet threads	mechanism
ND-Sync is effective in spotting retweet fraudsters	mechanism
robust to different types of abnormal activity	mechanism
and adaptable as it can easily incorporate additional features	mechanism
We refer to the detection of such synchronized observations as the Synchonization Fraud problem	method
and we study a specific instance of it	method
Retweet Fraud Detection	method
manifested in Twitter	method
reveal the superiority of the proposed method	finding
Comparisons with the state-of-the-art methods	method
Ensemble methods for classification have been effectively used for decades	background
while for outlier detection it has only been studied recently	background
we show that CARE performs significantly better than or at least similar to the individual baselines as well as the existing state-of-the-art outlier ensembles	finding
In this work	mechanism
we design a new ensemble approach which provides improved accuracy by reducing error through both bias and variance by considering outlier detection as a binary classification task with unobserved labels In this paper	mechanism
we propose a sequential ensemble approach called CARE that employs a two-phase aggregation of the intermediate results in each iteration to reach the final outcome Unlike existing outlier ensembles	mechanism
our ensemble incorporates both the parallel and sequential building blocks to reduce bias as well as variance by ( i ) successively eliminating outliers from the original dataset to build a better data model on which outlierness is estimated ( sequentially )	mechanism
and ( ii ) combining the results from individual base detectors and across iterations ( parallelly )	mechanism
that active learning driven experimentation using KBMF can result in highly accurate models while performing as few as 14\ % of the possible experiments	finding
and more accurately than random sampling of an equivalent number	finding
and show how it can be used in practice to decide when to stop an active learning process	finding
An active learning method is presented which considers the interaction between multiple drugs and multiple targets at the same time Kernelized Bayesian matrix factorization ( KBMF ) is used to model the interactions We also provide a method based on the learning curve	mechanism
We demonstrate on four previously characterized drug effect data sets	method
Problems of this nature arise in formal verification of continuous and hybrid dynamical systems	background
where there is an increasing need for methods to expedite formal proofs	background
The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting	finding
We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of heterogeneous benchmarks	method
In this paper	mechanism
we propose the video segmentation algorithm which is motivated by the human visual system The algorithm performs the video segmentation task by simultaneously utilizing the color histogram of the color	mechanism
the optical flow of the motion	mechanism
and the homography of the structure	mechanism
Many data structures ( e	background
g	background
matrices ) are typically accessed with multiple access patterns	background
Depending on the layout of the data structure in physical address space	background
some access patterns result in non-unit strides	background
In existing systems	background
which are optimized to store and access cache lines	background
non-unit strided accesses exhibit low spatial locality Our framework is general	background
and can benefit many modern data-intensive applications	background
We propose the Gather-Scatter DRAM ( GS-DRAM ) We observe that a commodity DRAM module contains many chips Each chip stores a part of every cache line mapped to the module	mechanism
To realize this idea	mechanism
GS-DRAM first maps the data of each cache line to different chips such that multiple values of a strided access pattern are mapped to different chips Second	mechanism
instead of sending a separate address to each chip	mechanism
GS-DRAM maps each strided pattern to a small pattern ID that is communicated to the module	mechanism
Based on the pattern ID	mechanism
each chip independently computes the address of the value to be accessed	mechanism
The cache line returned by the module contains different values of the strided pattern gathered from different chips	mechanism
We design an end-to-end system to exploit GS-DRAM	method
Though most would agree that accountability and privacy are both valuable	background
today 's Internet provides little support for either	background
Previous efforts have explored ways to offer stronger guarantees for one of the two	background
typically at the expense of the other ; indeed	background
at first glance accountability and privacy appear mutually exclusive	background
We introduce the Accountable and Private Internet Protocol ( APIP )	mechanism
which splits source addresses into two separate fields - an accountability address and a return address - and introduces independent mechanisms for managing each	mechanism
Accountability addresses rather than pointing to hosts	mechanism
point to accountability delegates	mechanism
which agree to vouch for packets on their clients ' behalves	mechanism
taking appropriate action when misbehavior is reported	mechanism
With accountability handled by delegates	mechanism
senders are now free to mask their return addresses ; we discuss a few techniques for doing so	mechanism
The growing number of sensor-based interactive applications and services are pushing the limits of the on-board computing resources in vehicles	background
With vehicles increasingly being connected to the Internet	background
offloading the computation to cloud-computing infrastructures is an attractive solution	background
we show that our mechanism can help meet application response time constraints	finding
we design a system to the cloud	mechanism
We particularly develop heuristic mechanisms for the placement and scheduling of modules on the On-Board Unit ( OBU ) and a cloud server under dynamic networking conditions during driving	mechanism
Through an experimental evaluation of the end-end application response time using our prototype vehicular cloud offloading system	method
We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services	background
This paper describes the design and implementation of HERD	mechanism
a key-value system designed Unlike prior RDMA-based key-value systems	mechanism
HERD focuses its design on reducing network round trips while using efficient RDMA primitives ; the result is substantially lower latency	mechanism
and throughput that saturates modern	mechanism
commodity RDMA hardware HERD has two unconventional decisions : First	mechanism
it does not use RDMA reads	mechanism
despite the allure of operations that bypass the remote CPU entirely Second	mechanism
it uses a mix of RDMA and messaging verbs	mechanism
despite the conventional wisdom that the messaging primitives are slow A HERD client writes its request into the server 's memory ; the server computes the reply	mechanism
This design uses a single round trip for all requests and	mechanism
TTL caching models have recently regained significant research interest due to their connection to popular caching policies such as LRU	background
results highlight that existing Poisson approximations in binary-tree topologies are subject to relative errors as large as 30\ %	finding
depending on the tree depth	finding
developing two exact methods with orthogonal generality and computational complexity The first method generalizes existing results for line networks under renewal requests to the broad class of caching policies whereby evictions are driven by stopping times ; in addition to classical policies used in DNS and web caching	mechanism
our stopping time model captures an emerging new policy implemented in SON switches and Amazon web services	mechanism
The second method further generalizes these results to feedforward networks with Markov arrival process ( MAP ) requests MAPs are particularly suitable for non-line networks because they are closed not only under superposition and splitting	mechanism
as known but also under caching operations with phase-type ( PH ) TTL distributions	mechanism
The crucial benefit of the two closure properties is that they jointly enable the first exact analysis of TTL feedforward cache networks in great generality	mechanism
Moreover numerical	method
To date the study of dispatching or load balancing in server farms has primarily focused on the minimization of response time	background
Server farms are typically modeled by a front-end router that employs a dispatching policy to route jobs to one of several servers	background
with each server scheduling all the jobs in its queue via Processor-Sharing	background
we are able to deduce many unexpected results regarding dispatching	finding
: we model each arrival as having a randomly distributed value parameter	mechanism
independent of the arrival 's service requirement ( job size )	mechanism
Given such value heterogeneity	mechanism
the correct metric is no longer the minimization or response time	mechanism
but rather the minimization of value-weighted response time	mechanism
{ '' } We propose a number of new dispatching policies that are motivated by the goal of minimizing the value-weighted response time	mechanism
Via a combination of exact analysis	method
asymptotic analysis and simulation	method
LM tends to be more expressive than other logic programming languages	finding
LM programs are naturally concurrent illustrate its use	finding
We have designed a new logic programming language called LM ( Linear Meld ) Our language is based on linear logic	mechanism
an expressive logical system where logical facts can be consumed	mechanism
Because LM integrates both classical and linear logic	mechanism
because facts are partitioned by nodes of a graph data structure	mechanism
Computation is performed at the node level while communication happens between connected nodes	mechanism
through a number of examples	mechanism
It is hard to efficiently model the light transport in scenes with translucent objects for interactive applications	background
The inter-reflection between objects and their environments and the subsurface scattering through the materials intertwine to produce visual effects like color bleeding	background
light glows and soft shading	background
we demonstrate scene relighting and dynamically varying object translucencies at near interactive rates	finding
In this paper	mechanism
we present a simple analytic model that combines diffuse inter-reflection and isotropic subsurface scattering	mechanism
Our approach extends the classical work in radiosity by including a subsurface scattering matrix that operates in conjunction with the traditional form factor matrix	mechanism
This subsurface scattering matrix can be constructed using analytic	mechanism
measurement-based or simulation-based models and can capture both homogeneous and heterogeneous translucencies	mechanism
Using a fast iterative solution to radiosity	method
Motivation : Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism	background
The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks	background
around which transcriptional regulation is focused owing to a disease or a genetic perturbation	background
Using a z-score-based scoring scheme	background
RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network	background
The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes	background
Overall MIRA is a promising algorithm for detecting metabolic drug targets and understanding the relation between gene expression and metabolic activity	background
We show that MIRA 's results are biologically sound	finding
empirically significant and more reliable than RA	finding
Results and show that MIRA captures the underlying metabolic dynamics of the switch from aerobic to anaerobic respiration Results indicate that MIRA reports metabolites that highly overlap with recently found metabolic biomarkers in the autism literature	finding
In this article	mechanism
we propose a mutual information-based multivariate reporter algorithm MIRA ) MIRA is a multivariate and combinatorial algorithm using mutual information	mechanism
We apply MIRA to gene expression analysis of six knockout strains of Escherichia coli We also apply MIRA to an Autism Spectrum Disorder gene expression dataset	method
Overall we hope that the work helps to advance concurrent programming in modern programming environments	background
The AEMINIUM implementation and all case studies are publicly available under the General Public License	finding
to demonstrate that AEMINIUM parallelized code has performance improvements compared to its sequential counterpart	finding
to show that AEMINIUM is powerful enough to encode them AEMINIUM can achieve a 70\ % performance improvement over the sequential counterpart Our evaluation demonstrates that AEMINIUM can be used to express parallelism in such data-structures and that the performance benefits scale with the amount of annotation effort which is put into the implementation	finding
Our experiments show that AEMINIUM is capable of extracting parallelism from functional code and achieving performance improvements up to the limits of Plaid 's inherent performance bounds	finding
the design of the concurrent-by-default AEMINIUM programming language	mechanism
AEMINIUM leverages the permission flow of object and group permissions through the program to validate the program 's correctness and to automatically infer a possible parallelization strategy via a dataflow graph	mechanism
AEMINIUM supports not only fork-join parallelism but more general dataflow patterns of parallelism	mechanism
In this paper we present a formal system	mechanism
called mu AEMINIUM	mechanism
modeling the core concepts of AEMINIUM	mechanism
mu AEMINIUM 's static type system is based on Featherweight Java with AEMINIUM-specific extensions	mechanism
Besides checking for correctness AEMINIUM 's type system it also uses the permission flow to compute a potential parallel execution strategy for the program mu AEMINIUM 's dynamic semantics use a concurrent-by-default evaluation approach	mechanism
We conduct our study through Along with the formal system we present its soundness proof	method
We provide a full description of the implementation along with the description of various optimization techniques we used	method
We implemented AEMINIUM as an extension of the Plaid programming language	method
which has first-class support for permissions built-in	method
We use various case studies to evaluate AEMINIUM 's applicability and We chose to use case studies from common domains or problems that are known to benefit from parallelization	method
We demonstrate through a webserver application	method
which evaluates AEMINIUM 's impact on latency-bound applications	method
that In another case study we chose to implement a dictionary function to evaluate AEMINIUM 's capabilities to express essential data structures We chose an integral computationally example to evaluate pure functional programming and computational intensive use cases	method
Vehicular multi-hop protocols typically employ distance-based metrics	background
where it achieved a 30\ % increase in packet delivery ratio over the benchmark GPSR protocol	finding
In this work we present LASP	mechanism
a geographic protocol that uses a more accurate spatial connectivity-based metric Spatial connectivity describes the historical probability of successfully delivering a packet from one geographic area to another	mechanism
Analysis of data collected from a vehicular testbed showed that	mechanism
unlike other metrics	mechanism
spatial connectivity indirectly captures all major factors affecting wireless connectivity	mechanism
Moreover it is temporally stable	mechanism
which makes it useful in estimating the quality of future co-located links	mechanism
When forwarding LASP uses spatial connectivity information to pick a well-connected geographic forwarding zone	mechanism
inside which multiple nodes cooperate in relaying through a distributed prioritization scheme	mechanism
Compared with other techniques where the sender picks a specific next hop relay	mechanism
cooperative forwarding improves resilience to losses through vehicle diversity	mechanism
We evaluated LASP on a 30-node testbed	method
Security requirements patterns represent reusable security practices that software engineers can apply to improve security in their system	background
Reusing best practices that others have employed could have a number of benefits	background
such as decreasing the time spent in the requirements elicitation process or improving the quality of the product by reducing product failure risk	background
Pattern selection can be difficult due to the diversity of applicable patterns from which an analyst has to choose	background
We propose a new method that combines an inquiry-cycle based approach with the feature diagram notation Similar to patterns themselves	mechanism
our approach captures expert knowledge The resulting pattern hierarchies allow users to be guided through these decisions by questions	mechanism
which introduce related patterns in order	mechanism
thus resulting in better requirement generation	mechanism
We evaluate our approach using access control patterns in a pattern user study	method
Government laws and regulations increasingly place requirements on software systems	background
Ideally experts trained in law will analyze and interpret legal texts to inform the software requirements process	background
However in small companies and development teams with short launch cycles	background
individuals with little or no legal training will be responsible for compliance	background
Two specific challenges commonly faced by non-experts are deciding if their system is covered by a law	background
and then deciding whether two legal requirements are similar or different	background
In so doing	finding
we discovered that legal experts achieved higher rates of consensus more frequently than technical professionals or laypersons and that all groups had slightly greater agreement when judging coverage conditions than requirements	finding
we found that technical professionals and legal experts exhibited consistently greater agreement than that found between laypersons and legal experts	finding
and that each group tended towards different justifications	finding
such as laypersons and technical professionals tendency towards categorizing different coverage conditions or requirements as equivalent if they believed them to possess the same underlying intent	finding
measured by Fleiss ' K	method
When comparing judgments between groups using a consensus-based Cohen 's Kappa	method
Many traditional challenges in reconstructing 3D motion	background
such as matching across wide baselines and handling occlusion	background
reduce in significance as the number of unique viewpoints increases	background
We demonstrate that our method estimates visibility with greater accuracy	finding
and increases tracking performance producing longer trajectories	finding
at more locations	finding
and at higher accuracies than methods that ignore visibility or use photometric consistency alone	finding
We present a maximum a posteriori ( MAP ) estimate of the time-varying visibility of the target points Our algorithm takes	mechanism
as input camera poses and image sequences	mechanism
and outputs the time-varying set of the cameras in which a target patch is visibile and its reconstructed trajectory	mechanism
We model visibility estimation as a MAP estimate by incorporating various cues including photometric consistency	mechanism
motion consistency and geometric consistency	mechanism
in conjunction with a prior that rewards consistent visibilities in proximal cameras	mechanism
An optimal estimate of visibility is obtained by finding the minimum cut of a capacitated graph over cameras	mechanism
Curse of dimensionality is a practical and challenging problem in image categorization	background
especially in cases with a large number of classes	background
Multi-class classification encounters severe computational and storage problems when dealing with these large scale tasks	background
further demonstrate the effectiveness of hierarchical feature hashing	finding
In this paper	mechanism
we propose hierarchical feature hashing	mechanism
We provide detailed theoretical analysis on our proposed hashing method Moreover	method
experimental results on object recognition and scene classification	method
With the widespread availability of video cameras	background
we are facing an ever-growing enormous collection of unedited and unstructured video data	background
Due to lack of an automatic way to generate summaries from this large collection of consumer videos	background
they can be tedious and time consuming to index or search	background
demonstrating the effectiveness of online video highlighting	finding
In this work	mechanism
we propose online video highlighting	mechanism
a principled way	mechanism
costly both time-wise and financially for manual processing	mechanism
Specifically our method learns a dictionary from given video using group sparse coding	mechanism
and updates atoms in the dictionary on-the-fly	mechanism
A summary video is then generated by combining segments that can not be sparsely reconstructed using the learned dictionary	mechanism
The online fashion of our proposed method enables it to process arbitrarily long videos and start generating summaries before seeing the end of the video Moreover	mechanism
the processing time required by our proposed method is close to the original video length	mechanism
achieving quasi real-time summarization speed	mechanism
We demonstrate the ability of our system to align 3D models with 2D objects in the challenging PASCAL VOC images	finding
which depict a wide variety of chairs in complex scenes	finding
we propose an exemplar-based 3D category representation	mechanism
which can explicitly model chairs of different styles as well as the large variation in viewpoint	mechanism
We develop an approach This is achieved by ( i ) representing each 3D model using a set of view-dependent mid-level visual elements learned from synthesized views in a discriminative fashion	mechanism
( ii ) carefully calibrating the individual element detectors on a common dataset of negative images	mechanism
and ( iii ) matching visual elements to the test image allowing for small mutual deformations but preserving the viewpoint and style constraints	mechanism
utilizing the large quantities of 3D CAD models that have been made publicly available online	method
Using the `` chair { '' } class as a running example	method
The storyline graphs can be an effective summary that visualizes various branching narrative structure of events or activities recurring across the input photo sets of a topic class	background
we show that the proposed algorithm improves other candidate methods for both storyline reconstruction and image prediction tasks	finding
In this paper	mechanism
we investigate an approach from large-scale collections of Internet images	mechanism
and optionally other side information such as friendship graphs	mechanism
we leverage them to perform the image sequential prediction tasks	mechanism
from which photo recommendation applications can benefit	mechanism
We formulate the storyline reconstruction problem as an inference of sparse time-varying directed graphs	mechanism
and develop an optimization algorithm that successfully addresses a number of key challenges of Web-scale problems	mechanism
including global optimality	mechanism
linear complexity and easy parallelization	mechanism
we demonstrate that the proposed joint summarization approach outperforms other baselines and our own methods using videos or images only	finding
Starting from the intuition that the characteristics of the two media types are different yet complementary	mechanism
we develop a fast and easily-parallelizable approach for creating not only high-quality video summaries but also novel structural summaries of online images as storyline graphs The storyline graphs can illustrate various events or activities associated with the topic in a form of a branching network	mechanism
The video summarization is achieved by diversity ranking on the similarity graphs between images and video frames	mechanism
The reconstruction of storyline graphs is formulated as the inference of sparse time-varying directed graphs from a set of photo streams with assistance of videos	mechanism
Web services offer a more reliable and efficient way to access online data than scraping web pages	background
to demonstrate our tool 's ability to create fast and reusable data extraction and manipulation programs that work with complex web service data	finding
In this paper	mechanism
we present Gneiss	mechanism
a tool that extends the familiar spreadsheet metaphor Gneiss allows users to extract the desired fields in web service data using drag-and-drop	mechanism
and refine the results through spreadsheet formulas	mechanism
along with sorting and filtering the data	mechanism
Hierarchical data are stored as nested tables in the spreadsheet and can be flattened for future operations	mechanism
Data flow is two-way between the spreadsheet and the web services	mechanism
enabling people to easily make a new request by modifying spreadsheet cells	mechanism
In addition using the dependency between spreadsheet cells	mechanism
our tool is able to create parallel-running data extractions based on the user 's sequential demonstration	mechanism
We use a set of examples	method
Research shows that commonly accepted security requirements are not generally applied in practice Instead of relying on requirements checklists	background
security experts rely on their expertise and background knowledge to identify security vulnerabilities	background
We report our preliminary results of analyzing two interviews that reveal possible decision-making patterns that could characterize how analysts perceive	finding
comprehend and project future threats which leads them to decide upon requirements and their specifications	finding
in addition to how experts use assumptions to overcome ambiguity in specifications	finding
Our goal is to build a model that researchers can use	mechanism
we conducted a series of interviews to encode the decision-making process of security experts and novices during security requirements analysis	method
Participants were asked to analyze two types of artifacts : source code	method
and network diagrams for vulnerabilities and to apply a requirements checklist to mitigate some of those vulnerabilities	method
We framed our study using Situation Awareness-a cognitive theory from psychology-to elicit responses that we later analyzed using coding theory and grounded analysis	method
The use of shared mutable state	background
commonly seen in object-oriented systems	background
is often problematic due to the potential conflicting interactions between aliases to the same state	background
We present a substructural type system outfitted with a novel lightweight interference control mechanism	mechanism
rely-guarantee protocols By assigning each alias separate roles	mechanism
encoded in a novel protocol abstraction in the spirit of rely-guarantee reasoning	mechanism
our type system ensures that challenging uses of shared state will never interfere in an unsafe fashion	mechanism
In particular rely-guarantee protocols ensure that each alias will never observe an unexpected value	mechanism
or type when inspecting shared memory regardless of how the changes to that shared state ( originating from potentially unknown program contexts ) are interleaved at run-time	mechanism
Formal verification and validation play a crucial role in making cyberphysical systems ( CPS ) safe Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained	background
including models of the controller and of the physical dynamics	background
This paper introduces ModelPlex	mechanism
a method ModelPlex provides correctness guarantees for CPS executions at runtime : it combines offline verification of CPS models with runtime validation of system executions for compliance with the model	mechanism
ModelPlex ensures that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model	mechanism
assuming the system dynamics deviation is bounded If	mechanism
at some point	mechanism
the observed behavior no longer complies with the model so that offline verification results no longer apply	mechanism
ModelPlex initiates provably safe fallback actions This paper	mechanism
furthermore develops a systematic technique	mechanism
We show that this multi-modal approach achieves superior recognition accuracy compared to using a vision system alone	finding
especially in cluttered scenes where a vision system would be unable to distinguish which object is of interest to the user without additional input	finding
We present Marvin	mechanism
a system It integrates HOG-based object recognition	mechanism
SURF-based localization information	mechanism
automatic speech recognition	mechanism
and user feedback information with a probabilistic model Once the object of interest is recognized	mechanism
the information that the user is querying	mechanism
e	mechanism
g	mechanism
reviews options etc	mechanism
is displayed on the user 's mobile or wearable device	mechanism
It is computationally able to scale to large numbers of objects by focusing compute-intensive resources on the objects most likely to be of interest	mechanism
inferred from user speech and implicit localization information	mechanism
We tested this prototype in a real-world retail store during business hours	method
with varied degree of background noise and clutter	method
Due to the diversity in appearance in real world objects	background
an object detector must capture variations in scale	background
viewpoint illumination etc	background
The current approaches do this by using mixtures of models	background
where each mixture is designed to capture one ( or a few ) axis of variation	background
Current methods usually rely on heuristics to capture these variations ; however	background
it is unclear which axes of variation exist and are relevant to a particular task	background
Another issue is the requirement of a large set of training images to capture such variations	background
Current methods do not scale to large training sets either because of training time complexity I I or test time complexity I I	background
We propose a two stage data-driven process	mechanism
which selects and learns a compact set of exemplar models These selected models have an inherent ranking	mechanism
which can be used for anytime/budgeted detection scenarios	mechanism
Another benefit of our approach ( beyond the computational speedup ) is that the selected set of exemplar models performs better than the entire set	mechanism
When people observe and interact with physical spaces	background
they are able to associate functionality to regions in the environment Additionally	background
we offer a preliminary glance of the applicability of Action Maps	background
We demonstrate that by capturing appearance-based attributes of the environment and associating these attributes with activity demonstrations	finding
our proposed mathematical framework allows for the prediction of Action Maps in new environments	finding
by leveraging sparse activity demonstrations recorded from an ego-centric viewpoint	mechanism
The method we describe Our method learns and predicts `` Action Maps { '' }	mechanism
which encode the ability for a user to perform activities at various locations	mechanism
With the usage of an egocentric camera to observe human activities	mechanism
our method scales with the size of the scene without the need for mounting multiple static surveillance cameras and is well-suited to the task of observing activities up-close	mechanism
by demonstrating a proof-of-concept application in which they are used in concert with activity detections to perform localization	method
Growing traffic volumes and the increasing complexity of attacks pose a constant scaling challenge for network intrusion prevention systems ( NIPS )	background
In this respect	background
offloading NIPS processing to compute clusters offers an immediately deployable alternative to expensive hardware upgrades	background
Our evaluations show that SNIPS can reduce the maximum load by up to 10x while only increasing the latency by 2\ %	finding
we present the SNIPS system We design a formal optimization framework that captures tradeoffs across scalability	mechanism
network load and latency	mechanism
We provide a practical implementation using recent advances in software-defined networking without requiring modifications to NIPS hardware	method
on realistic topologies	method
This is a difficult problem due to the drastic change in perspective between the ground and aerial imagery and the lack of environmental features for image comparison	background
We do not rely on GPS	background
which may be jammed or uncertain	background
The results show that vision-based UGV localization from satellite maps is not only possible	finding
but often provides better position estimates than GPS estimates	finding
enabling us to improve the location estimates of Google Street View	finding
We analyze the performance of a variety of descriptors for different satellite map sizes and various terrain and environment	method
State lattice-based planning has been used in navigation for ground	background
water aerial and space robots	background
State lattices are typically constructed of simple motion primitives connecting one state to another	background
For example if the robot has a camera it may be able to use simple visual servoing techniques to navigate through a GPS-denied region Likewise	background
a LIDAR may allow the robot to skirt along an environmental feature even if there is not enough information to generate an accurate pose estimate	background
showing the practical application of this approach	finding
In this paper we present an expansion of the state lattice framework that allows us to incorporate controller-based motion primitives and external perceptual triggers directly into the planning process	mechanism
We provide a formal description of our method of constructing the search graph in these cases	mechanism
as well as presenting real-world and simulated testing data	method
The field of object detection has made significant advances riding on the wave of region-based ConvNets	background
We present a simple yet surprisingly effective online hard example mining ( OHEM ) algorithm Our motivation is the same as it has always been detection datasets contain an overwhelming number of easy examples and a small number of hard examples	mechanism
Automatic selection of these hard examples can make training more effective and efficient	mechanism
OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use	mechanism
We also present preliminary results	finding
In this paper	mechanism
we present both centralized and distributed algorithms based on inter-robot relative position measurements Robot orientations are not measured	mechanism
but are computed by our algorithms	mechanism
Our algorithms are robust to measurement error and are useful in applications where a group of robots need to establish a common coordinate frame based on relative sensing information	mechanism
The problem of establishing a common coordinate frame is formulated in a least squares error framework minimizing the total inconsistency of the measurements We assume that robots that can sense each other can also communicate with each other	mechanism
In this paper	mechanism
our key contribution is a novel asynchronous distributed algorithm for multi-robot coordinate frame alignment that does not make any assumptions about the sensor noise model After minimizing the least squares error ( LSE ) objective for coordinate frame alignment of two robots	mechanism
we develop a novel algorithm that outperforms state-of-the-art centralized optimization algorithms for minimizing the LSE objective	mechanism
Furthermore we prove that for multi-robot systems ( a ) with redundant noiseless relative sensing information	mechanism
we will achieve the globally optimal solution ( this is non-trivial because the LSE objective is nonconvex for our problem )	mechanism
( b ) with noisy information but no redundant sensing ( e	mechanism
g	mechanism
sensing graph has a tree topology )	mechanism
our algorithm will optimally minimize the LSE objective	mechanism
of the real-world performance of our algorithm on TurtleBots equipped with Kinect sensors	method
When one uses a hand-held tool	background
the fingers often make the tool to be in contact with the palm in the form of multi-contact manipulation	background
Multi-contact manipulation is useful for object-environment interaction tasks because it can provide both powerful grasping of the object body and dexterous manipulation of the object end-effector	background
In this paper	mechanism
we propose Reactional Internal Contact Hypothesis that regards the internal contact force as a reaction force By taking a handwriting task as an example	mechanism
optimal configuration search and grasping force computation problems are addressed based on this hypothesis and	mechanism
validated via dynamic simulation	method
The average error and execution time are reduced by 63	finding
6\ % and 28	finding
5\ % respectively	finding
compared to the unaided trials Finally	finding
the automated laser photocoagulation was demonstrated also in an eye phantom	finding
including compensation for the eye movement	finding
We compared the performance of the automated scanning using various control thresholds	method
in order to find the most effective threshold in terms of accuracy and speed Given the selected threshold	method
we conducted the handheld operation above a fixed target surface	method
Transfemoral amputees often suffer from falls and a fear of falling that leads to a decreased quality of life	background
Existing control strategies for powered knee-ankle prostheses demonstrate only limited ability to react to disturbances that induce falls such as trips	background
slips and obstacles	background
In contrast prior work on neuromuscular modeling of human locomotion suggests that control strategies based on local reflexes exhibit robustness to unobserved terrain such as slopes and steps These results suggest that applying the proposed control to a powered knee-ankle prosthesis will substantially improve amputee gait stability	background
We show that the proposed control allows the amputee to walk farther over rough ground than does the state-of-the-art control	finding
The proposed controller also more readily rejects deviations from nominal walking gaits such as those encountered during a trip	finding
we simulate a neuromuscular model of a transfemoral amputee walking over rough ground with a powered knee-ankle prosthesis governed by the proposed reflexive controller	mechanism
Control design of running robots is often based on mapping the behavior of lower order models onto the robotic systems	background
and the robustness of running is largely determined by the robustness of these underlying models	background
However existing implementations do not take full advantage of the stability that the low order models can provide	background
In particular analysis of the theoretical spring mass model suggests leg placement policies that generate near deadbeat rejection of large	background
unobserved changes in ground height	background
and show that resulting system rejects ground disturbances of up to 25\ % leg length	finding
adapts to persistent ground slopes	finding
and tolerates sensor noise	finding
signal delays and modeling errors	finding
The results indicate that transferring control derived within the spring mass model is an effective technique for realizing highly robust running in robotic systems	finding
We design a control that stably embeds the spring mass model 's behavior in a planar robot model	mechanism
Current GPS-based devices have difficulty localizing in cases where the GPS signal is unavailable or insufficiently accurate	background
and show that it is effective when imagery is sparsely available	finding
This paper presents an algorithm using vision	mechanism
road curvature estimates	mechanism
or a combination of both The method uses an extension of topometric localization	mechanism
which is a hybrid between topological and metric localization	mechanism
The extension enables localization on a network of roads rather than just a single	mechanism
non-branching route	mechanism
The algorithm which does not rely on GPS	mechanism
is able to localize reliably in situations where GPS-based devices fail	mechanism
including `` urban canyons { '' } in downtown areas and along ambiguous routes with parallel roads	mechanism
We demonstrate the algorithm experimentally on several road networks in urban	method
suburban and highway scenarios	method
We also evaluate the road curvature descriptor	method
Active illumination based methods have a trade-off between acquisition time and resolution of the estimated 3D shapes	background
Multi-shot approaches can generate dense reconstructions but require stationary scenes	background
We validate the approach	finding
In this work	mechanism
we develop a single-shot approach The key to our approach is an image decomposition scheme that can recover the illumination and the texture images from their mixed appearance	mechanism
Despite the complex appearances of the illuminated textured regions	mechanism
our method can accurately compute per pixel warps from the illumination pattern and the texture template to the observed image The texture template is obtained by interleaving the projection sequence with an all-white pattern Our estimated warping functions are reliable even with infrequent interleaved projection	mechanism
Thus we obtain detailed shape reconstruction and dense motion tracking of the textured surfaces	mechanism
on synthetic and real data containing subtle non-rigid surface deformations	method
In product design	background
designers often generate a large number of concepts in the form of sketches and drawings to develop and communicate their ideas Concrete concepts typically evolve through a progressive refinement of initially coarse and ambiguous ideas	background
We demonstrate and discuss preliminary results of our technique on 2D shape design problems	finding
we describe a predictive modeling technique This helps designers take a sneak peek at the potential end result of a developing concept	mechanism
without forcing them to commit to the suggestion	mechanism
( ii ) we show that such properties can be successfully inferred from a single image ( iv ) we show that the 3D attributes trained on this dataset generalize to images of other ( non-sculpture ) object classes ; and furthermore ( v ) we show that the CNN also provides a shape embedding that can be used to match previously unseen sculptures largely independent of viewpoint	finding
Active intracellular cargo transport is essential to survival and function of eukaryotic cells	background
The models and related computational analysis methods developed in this study are general and can be used for studying molecular machinery and spatiotemporal dynamics of other cellular processes	background
We validated and benchmarked the image models	finding
To this end	mechanism
we developed related computational image models	mechanism
we developed anisotropic spatial density kernels for reconstruction and segmentation of related super-resolution STORM ( stochastic optical reconstruction microscopy ) images	mechanism
we developed hidden Markov models and principal component analysis for representation and analysis of movement of individual transported cargoes	mechanism
using simulated and actual experimental images	method
In addition the topologies predicted by the proposed method can be used as effective initial conditions for conventional topology optimization routines	background
resulting in substantial performance gains	background
We discuss the advantages and limitations of the presented approach and show its performance on a number of examples	background
The results indicate that when there is an underlying structure in the set of existing solutions	finding
the proposed method can successfully predict the optimal topologies in novel loading configurations	finding
data-driven approach Our approach takes as input a set of images representing optimal 2-D topologies	mechanism
each resulting from a random loading configuration applied to a common boundary support condition	mechanism
These images represented in a high dimensional feature space are projected into a lower dimensional space using component analysis	mechanism
Using the resulting components	mechanism
a mapping between the loading configurations and the optimal topologies is learned	mechanism
From this mapping	method
we estimate the optimal topologies for novel loading configurations	method
An increasing number of mobile devices are capable of automatically sensing and recording rich information about the surrounding environment	background
Spatial locations of such data can help to better learn about the environment	background
We will show robustness the ability to propose coarse sensor noise errors	finding
We focus on devices equipped with odometry sensors that capture changes in motion	mechanism
Odometry suffers from cumulative errors of dead reckoning but it captures the relative shape of the traversed path well	mechanism
Our approach will correct such errors by matching the shape of the trajectory from odometry to traversable paths of a known map	mechanism
Our algorithm is inspired by prior vehicular GPS map matching techniques that snap global GPS measurements to known roads	mechanism
We similarly wish to snap the trajectory from odometry to known hallways	mechanism
Several modifications are required to ensure these techniques are robust when given relative measurements from odometry	mechanism
If we assume an office-like environment with only straight hallways	mechanism
then a significant rotation indicates a transition to another hallway	mechanism
As a result	mechanism
we partition the trajectory into line segments based on significant turns	mechanism
Each trajectory segment is snapped to a corresponding hallway that best maintains the shape of the original trajectory	mechanism
These snapping decisions are made based on the similarity of the two curves as well as the rotation to transition between hallways	mechanism
under different types of noise in complex environments and	method
the proposed behavioral planning architecture improves the driving quality considerably	finding
with a 90	finding
3\ % reduction of required computation time in representative scenarios	finding
In this paper	mechanism
we propose a novel planning framework A reference planning layer first generates kinematically and dynamically feasible paths assuming no obstacles on the road	mechanism
then a behavioral planning layer takes static and dynamic obstacles into account	mechanism
Instead of directly commanding a desired trajectory	mechanism
it searches for the best directives for the controller	mechanism
such as lateral bias and distance keeping aggressiveness	mechanism
It also considers the social cooperation between the autonomous vehicle and surrounding cars	mechanism
Based on experimental results from both simulation and a real autonomous vehicle platform	method
Localization is a central problem for intelligent vehicles	background
Visual localization can supplement or replace GPS-based localization approaches in situations where GPS is unavailable or inaccurate Although visual localization has been demonstrated in a variety of algorithms and systems	background
the problem of how to best configure such a system remains largely an open question	background
Design choices such as `` where should the camera be placed ? { '' } and `` how should it be oriented ? { '' } can have substantial effect on the cost and robustness of a fielded intelligent vehicle	background
of a visual localization algorithm	mechanism
We ground the investigation using extensive field testing	method
and the data sets used for the analysis are made available for comparative evaluation	method
Our algorithms construct dual solutions using a regret-minimizing online learning algorithm in a black-box fashion	mechanism
and use them to construct primal solutions	mechanism
The adversarial guarantee that holds for the constructed duals help us to take care of most of the correlations that arise in the algorithm ; the remaining correlations are handled via martingale concentration and maximal inequalities	mechanism
These ideas lead to conceptually simple and modular algorithms	mechanism
Illumination defocus and global illumination effects are major challenges for active illumination scene recovery algorithms	background
We demonstrate the effectiveness of our approach	finding
In this paper	mechanism
we develop an algorithm for scene recovery in the presence of both defocus and global light transport effects such as interreflections and sub-surface scattering	mechanism
Our method extends the working volume by using structured light patterns at multiple projector focus settings	mechanism
A careful characterization of projector blur allows us to decode even partially out-of-focus patterns This enables our algorithm to recover scene shape and the direct and global illumination components over a large depth of field while still using a relatively small number of images ( typically 25-30 )	mechanism
by recovering high quality depth maps of scenes containing objects made of optically challenging materials such as wax	method
marble soap colored glass and translucent plastic	method
Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D point triangulation to reconstruct a static scene	background
Because the videos are aligned with sub-frame precision	finding
we reconstruct 3D trajectories of unconstrained outdoor activities at much higher temporal resolution than the input videos	finding
In this paper	mechanism
we present a spatiotemporal bundle adjustment approach Key to our joint optimization is the careful integration of physics-based motion priors within the reconstruction pipeline	mechanism
validated on a large motion capture corpus	mechanism
We present an end-to-end pipeline that takes multiple uncalibrated and unsynchronized video streams and produces a dynamic reconstruction of the event	mechanism
Turbulence is studied extensively in remote sensing	background
astronomy meteorology aerodynamics and fluid dynamics	background
The strength of turbulence is a statistical measure of local variations in the turbulent medium	background
It influences engineering decisions made in these domains	background
Turbulence strength ( TS ) also affects safety of aircraft and tethered balloons	background
and reliability of free-space electromagnetic relays	background
using videos captured from different viewpoints	mechanism
We formulate this as a linear tomography problem with a structure unique to turbulence fields	mechanism
No tight synchronization between cameras is needed Thus	mechanism
realization is very simple to deploy using consumer-grade cameras	mechanism
We experimentally demonstrate this both in a lab and in a large-scale uncontrolled complex outdoor environment	method
which includes industrial	method
rural and urban areas	method
In many behavioral domains	background
such as facial expression and gesture	background
sparse structure is prevalent	background
We propose a Kernel Structured Sparsity ( KSS ) method that can handle both the temporal alignment problem and the structured sparse reconstruction within a common framework	mechanism
and it can rely on simple features	mechanism
We characterize spatio-temporal events as time-series of motion patterns and by utilizing time-series kernels we apply standard structured-sparse coding techniques to tackle this important problem	mechanism
We evaluated the KSS method using both gesture and facial expression datasets that include spontaneous behavior and differ in degree of difficulty and type of ground truth coding	method
as measured by F-1 score	method
The primary goal of an automotive headlight is to improve safety in low light and poor weather conditions	background
But despite decades of innovation on light sources	background
more than half of accidents occur at night even with less traffic on the road Recent developments in adaptive lighting have addressed some limitations of standard headlights	background
Anti-glare high beams	finding
improved driver visibility during snowstorms	finding
increased contrast of lanes	finding
markings and sidewalks	finding
and early visual warning of obstacles are demonstrated	finding
This paper introduces an ultra-low latency reactive visual system Our single hardware design can be programmed to perform a variety of tasks	mechanism
experiments show that our method achieves good performance for parsing human motions Furthermore	finding
we found that our method achieves better performance by using unlabeled video than adding more labeled pose images into the training set	finding
In this paper	mechanism
we propose a method We use the training samples from a public image pose dataset to avoid the tediousness of labeling video streams There are two main problems confronted	mechanism
First the distribution of images and videos are different	mechanism
Second no temporal information is available in the training images	mechanism
To smooth the inconsistency between the labeled images and unlabeled videos	mechanism
our algorithm iteratively incorporates the pose knowledge harvested from the testing videos into the image pose detector via an adjust-and-refine method	mechanism
During this process	mechanism
continuity and tracking constraints are imposed to leverage the spatio-temporal information only available in videos	mechanism
For our experiments	method
we have collected two datasets from YouTube and	method
Forecasting human activities from visual evidence is an emerging area of research which aims to allow computational systems to make predictions about unseen human actions	background
results show that our proposed method is able to properly model human interactions in a high dimensional space of human poses	finding
results show that our method is able to generate highly plausible simulations of human interaction	finding
We model dual-agent interactions as an optimal control problem	mechanism
where the actions of the initiating agent induce a cost topology over the space of reactive poses - a space in which the reactive agent plans an optimal pose trajectory The technique developed in this work employs a kernel-based reinforcement learning approximation of the soft maximum value function to deal with the high-dimensional nature of human motion and applies a mean-shift procedure over a continuous cost function to infer a smooth reaction sequence	mechanism
Experimental When compared to several baseline models	method
We provide a semantics and a proof system and show its usefulness for nontrivial temporal properties of hybrid systems	mechanism
We take particular care to handle the case of alternating universal dynamic and existential temporal modalities and its dual	mechanism
solving an open problem formulated in previous work	mechanism
We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces	finding
In this work	mechanism
we present a method We propose the use of midlevel constraints for 3D scene understanding in the form of convex and concave edges and introduce a generic framework capable of incorporating these and other constraints Our method takes a variety of cues and uses them to infer a consistent interpretation of the scene	mechanism
Cyber-physical systems ( CPS )	background
which are computerized systems directly interfacing their real-world surroundings	background
leverage the construction of increasingly autonomous systems	background
To meet the high safety demands of CPS	background
verification of their behavior is crucial	background
which has led to a wide range of tools for modeling and verification of hybrid systems	background
These tools are often used in combination	background
because they employ a wide range of different formalisms for modeling	background
and aim at distinct verification goals and techniques and propose future extension	background
Furthermore we illustrate how the CRM can support comparing models	finding
we unify different terminologies and concepts of a variety of modeling and verification tools in a conceptual reference model ( CRM )	mechanism
Demographic information has a rich context from which to make decisions about how to filter or individualize computer users in forensic analysis	background
by analyzing the interactions between users and computers	mechanism
From user interaction data	mechanism
we extracted keystroke timing and mouse movement features	mechanism
and developed weighted random forest classifiers for five demographic traits : gender	mechanism
age ethnicity handedness	mechanism
and language	mechanism
We conducted a field study that gathered users ' keystroke and mouse data during interaction with a computer	method
Experiments	method
Refactoring of code is a common device in software engineering	background
As cyber-physical systems ( CPS ) become ever more complex	background
similar engineering practices become more common in CPS development	background
Proper safe developments of CPS designs are accompanied by a proof of correctness	background
For some of these we can give strong results by proving on a meta-level that they are correct for showing that the refactoring respects the refinement relation	finding
we develop proof-aware refactorings for CPS	mechanism
That is we study model transformations on CPS and show how they correspond to relations on correctness proofs	mechanism
As the main technical device	mechanism
we show how the impact of model transformations on correctness can be characterized by different notions of refinement in differential dynamic logic	mechanism
Furthermore we demonstrate the application of refinements on a series of safety-preserving and liveness-preserving refactorings	method
Where this is impossible	method
we construct proof obligations	method
These results suggest that	background
when carefully designed	background
learning by teaching can support students to not only learn cognitive skills but also employ meta-cognitive skills for effective tutoring	background
The data showed that students with the meta-cognitive help showed better problem selection and scored higher on the post-test than those who tutored SimStudent without the meta-cognitive help	finding
Recent work has shown that features such as hand appearance	background
object attributes local hand motion and camera ego-motion are important for characterizing first-person actions	background
to highlight the importance of network design decisions	background
we show that our proposed architecture naturally learns features that capture object attributes and hand-object configurations	finding
show that our deep architecture enables recognition rates that significantly outperform state-of-the-art techniques - an average 6	finding
6\ % increase in accuracy over all datasets	finding
the performance of individual recognition tasks also increase by 30\ % ( actions ) and 14\ % ( objects )	finding
We also include the results	finding
we propose a twin stream network architecture	mechanism
where one stream analyzes appearance information and the other stream analyzes motion information Our appearance stream encodes prior knowledge of the egocentric paradigm by explicitly training the network to segment hands and localize objects	mechanism
Furthermore by learning to recognize objects	mechanism
actions and activities jointly	mechanism
By visualizing certain neuron activation of our network	method
Our extensive experiments on benchmark egocentric action datasets of extensive ablative analysis	method
Collaborative learning has been shown to be beneficial for older students	background
but there has not been much research to show if these results transfer to elementary school students	background
In addition collaborative and individual modes of instruction may be better for acquiring different types of knowledge	background
Collaborative Intelligent Tutoring Systems ( ITS ) provide a platform that may be able to provide both the cognitive and collaborative support that students need This work indicates that by embedding collaboration scripts in ITSs	background
collaborative learning can be an effective instructional method even with young children	background
The collaborative groups had the same learning gains as the individual groups in both the procedural and conceptual learning conditions but were able to do so with fewer problems	finding
The amount of data available to build simulation models of schools is immense	background
but using these data effectively is difficult	background
In response we advocate a Complex Adaptive Systems approach By simulating agent-level attributes rather than system-level attributes	mechanism
the modeling is inherently transparent	mechanism
easily adjustable and facilitates analysis of the system due to the analogous nature of the simulated agents to real-world entities	mechanism
We explore the design a CAS model of schools using multiple levels of data from varied data streams	method
Heterogeneity of wireless networks has become an increasing problem in the wireless spectrum that breaks down spectrum sharing and exacerbates interference	background
In this paper	mechanism
we focus on the potential of spectrum management We introduce novel components to a spectrum management system that overcomes limitations of current models that have remained relatively focused on homogeneous environments	mechanism
Our approach is a centralized one	mechanism
where we analyze information collected from heterogeneous monitors available today	mechanism
structure the information in a hypergraph	mechanism
and perform an analysis to detect heterogeneous conflicts Introducing a mixed integer program ( in addition to other novel components )	mechanism
we reconfigure devices in the spectrum to avoid conflicts and improve performance	mechanism
When deployed in automated speech recognition ( ASR )	background
deep neural networks ( DNNs ) can be treated as a complex feature extractor plus a simple linear classifier	background
Previous work has investigated the utility of multilingual DNNs acting as language-universal feature extractors ( LUFEs )	background
Each of the proposed techniques results in word error rate reduction compared with the existing DNN-based LUFEs	finding
Combining the two methods together brings additional improvement on the target language	finding
In this paper	mechanism
we explore different strategies First	mechanism
we replace the standard sigmoid nonlinearity with the recently proposed maxout units	mechanism
The resulting maxout LUFEs have the nice property of generating sparse feature representations	mechanism
Second the convolutional neural network ( CNN ) architecture is applied	mechanism
We evaluate the performance of LUFEs on a cross-language ASR task	method
Recent studies in computer vision have shown that	background
while practically invisible to a human observer	background
skin color changes due to blood flow can be captured on face videos and	background
surprisingly be used to estimate the heart rate ( HR )	background
While considerable progress has been made in the last few years	background
still many issues remain open	background
that the proposed approach significantly outperforms state-of-the-art HR estimation methods in naturalistic conditions	finding
Opposite to previous approaches that estimate the HR by processing all the skin pixels inside a fixed region of interest	mechanism
we introduce a strategy Our approach	mechanism
inspired by recent advances on matrix completion theory	mechanism
allows us to predict the HR while simultaneously discover the best regions of the face to be used for estimation	mechanism
Thorough experimental evaluation conducted on public benchmarks suggests	method
Multilingual deep neural networks ( DNNs ) can act as deep feature extractors and have been applied successfully to cross language acoustic modeling	background
Learning these feature extractors becomes an expensive task	background
because of the enlarged multilingual training data and the sequential nature of stochastic gradient descent ( SGD )	background
better acceleration but worse recognition performance	finding
When using DistLang	method
we observe Further evaluations are conducted to scale DistModel to more languages and GPU cards	method
Solving this problem is important for ensuring correctness of the decision procedures	background
At the same time	background
it is a new approach for automated theorem proving over real numbers	background
we demonstrate how proofs generated from our solver can establish many nonlinear lemmas	finding
We design a first-order calculus	mechanism
and transform the computational steps of constraint solving into logic proofs	mechanism
which are then validated using proof-checking algorithms	mechanism
As an application	method
in the the formal proof of the Kepler Conjecture	method
Good communication is critical to seamless human-robot interaction	background
Among numerous communication channels	background
showing that the resulting pointing configurations make the goal object easier to infer for novice users	finding
We propose a mathematical model	mechanism
We study the implications of legibility on pointing	method
e	method
g	method
that the robot will sometimes need to trade off efficiency for the sake of clarity	method
Finally we test how well our model works in practice in a series of user studies	method
For compelling human-robot interaction	background
social gestures are widely believed to be important	background
We conclude that social gesturing of a robot enhances physical interactions between humans and robots	background
For half of the cases in which the catch was unsuccessful	finding
the robot made a physical gesture	finding
such as shrugging its shoulders	finding
shaking its head	finding
or throwing up its hands	finding
In the other half of cases	finding
no gestures were produced Participants smiled more and rated the robot as more engaging	finding
responsive and humanlike when it gestured	finding
Human participants repeatedly threw a ball to the robot	method
which attempted to catch it	method
If the catch was successful	method
the robot threw the ball back to the human	method
We used questionnaires and smile detection to compare participants ' feelings about the robot when it made gestures after failure versus when it did not	method
There is a saying that 95\ % of communication is body language	background
but few robot systems today make effective use of that ubiquitous channel and could be used to help design more effectively expressive mobile robots	background
Results indicate that the machine analysis ( 41	finding
7\ % match between intended and classified manner ) achieves similar accuracy overall compared to a human benchmark ( 41	finding
2\ % match )	finding
We conclude that these motion features perform well for analyzing expression in low degree of freedom systems	finding
The proposed work presents a principled set of motion features based on the Laban Effort system	mechanism
a widespread and extensively tested acting ontology for the dynamics of `` how { '' } we enact motion	mechanism
The features allow us	mechanism
in future work	mechanism
using position ( x	mechanism
y ) and orientation ( theta )	mechanism
We formulate representative features for each Effort and parameterize them on expressive motion sample trajectories collected from experts in robotics and theater	mechanism
We then produce classifiers for different `` manners { '' } of moving and	mechanism
assess the quality of results by comparing them to the humans labeling the same set of paths on Amazon Mechanical Turk	method
For service robots operating in indoor environments	background
the crucial task of navigation is often complicated by the presence of people	background
Simply treating humans in the environment as additional ( often moving ) obstacles can violate the complex set of social rules by which people navigate around each other	background
We found that both approaches were rated comparably when the robot approached from the participant 's front or side	finding
but the social approach was significantly preferred when the robot came from behind the participant	finding
We present a method of	mechanism
We also conducted a study in which a robot approached participants using both these social paths and straight-line	method
nonsocial paths	method
Human operators in today 's control centers	background
such as air or road traffic control	background
need to monitor a plethora of information obtained from diverse sources	background
To support them in detecting critical situations within this information flood and taking timely actions	background
operators thus need adequate information fusion and decision support systems	background
Research efforts on such dedicated Situation Awareness ( SAW ) systems have concentrated on assisting the operator in managing the current situations	background
which encompasses the acquisition	background
representation validation maintenance and reuse of knowledge gathered for and during the use of these systems	background
such as configuring and maintaining suitable situation templates and exploiting already assessed situations	background
If operators and domain experts are not supported in these tasks	background
however this may discourage them from a successful adoption of such systems in real-world control center applications	background
as user studies revealed	background
Based on these	mechanism
and the lessons learned from the application of our SAW system implementations BeAware ! and CSI to the domain of road traffic control	mechanism
we therefore propose a first step towards a tool suite	mechanism
which stretches from the configuration phase of the system to its runtime maintenance in the light of evolving environments and user needs	mechanism
Near-Infrared ( NIR ) images of most materials exhibit less texture or albedo variations making them beneficial for vision tasks such as intrinsic image decomposition and structured light depth estimation	background
Understanding the reflectance properties ( BRDF ) of materials in the NIR wavelength range can be further useful for many photometric methods including shape from shading and inverse rendering	background
to demonstrate fine-scale reconstruction of objects from a single NIR image	finding
The NIR BRDFs measured from material samples are used with a shape-from-shading algorithm	method
which is one of the major problems in HMM-based speech synthesis	background
results show that the modified post-filters also yield significant quality improvements in synthetic speech as yielded by the conventional post-filter	finding
Experimental	method
We will never really understand learning until we can build machines that learn many different things	background
over years and become better learners over time	background
Online popularity of a user or product ( via follows	background
page-likes etc	background
) can be monetized on the premise of higher ad click-through rates or increased sales	background
Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck	background
Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent ( but sometimes honest ) users	background
it is shown to be highly effective ( c ) it is scalable ( linear on the input size )	finding
with high precision identify many suspicious accounts which have persisted without suspension even to this day	finding
In this work	mechanism
we take an adversarial approach and propose FBOX	mechanism
an algorithm designed Our algorithm has the following desirable properties : ( a ) it has theoretical underpinnings	mechanism
( b )	mechanism
demonstrate superior computational cost ( real-time )	finding
memory efficiency and very competitive performance of our approach compared to the state of the arts	finding
In this work	mechanism
we propose to employ multi-channel correlation filters In our framework	mechanism
each action sequence is represented as a multi-channel signal ( frames ) and the goal is to learn a multi-channel filter for each action class that produces a set of desired outputs when correlated with training examples	mechanism
The experiments on the Weizmann and UCF sport datasets	method
Dynamic assignment and re-assignment of large number of simple and cheap robots across multiple sites is relevant to applications like autonomous survey	background
environmental monitoring and reconnaissance	background
This problem can be posed as an optimal control problem ( which is hard to solve optimally )	background
and has been studied to a limited extent in the literature when the cost objective is time	background
show that our method outperforms other proposed methods in the literature for the objective of time as well as more general objectives ( like total energy consumed )	finding
In this paper	mechanism
we present supervisory control laws We consider the total energy consumed as the cost objective and present a linear programming based heuristic for computing a stochastic transition law for the robots to move between sites	mechanism
We consider a robotic swarm consisting of tens to hundreds of simple robots with limited battery life and limited computation and communication capabilities	method
The robots have the capability to recognize the site that they are in and receive messages from a central supervisory controller	method
but they can not communicate with other robots	method
There is a cost ( e	method
g	method
energy time ) for the robots to move from one site to another	method
These limitations make the swarm hard to control to achieve the desired configurations	method
We evaluate our method for different objectives and through Monte Carlo simulations	method
The study of human control of robotic swarms involves designing interfaces and algorithms for allowing a human operator to influence a swarm of robots	background
Our results show that	finding
while there was a large drop in the number of goals reached when moving from a 1-hop to a 2-hop guarantee	finding
the difference between a 2-hop	finding
3-hop and 4-hop guarantee was not statistically significant	finding
Furthermore we found that sensing error impacted the explicit information-propagation method more than the tacit method conditions	finding
and caused participants more trouble the lower the density of leaders	finding
although the explicit method performed better overall	finding
This paper investigates the use of a small subset of the swarm as leaders that are dynamically selected during the scenario execution and are directly controlled by the human operator to guide the rest of the swarm	mechanism
which is operating under a flocking-style algorithm	mechanism
The goal of the operator in this study is to move the swarm to goal regions that arise dynamically in the environment	mechanism
We experimentally investigated three different aspects of dynamic leader-based swarm control and their interactions : leader density ( in terms of guaranteed hops to a leader )	method
sensing error and method of information propagation from leaders to the rest of the swarm	method
Our work will impact several first-person vision tasks that need the detailed understanding of social interactions	background
such as automatic video summarization of group events and assistive systems	background
We show that the first-person and second-person points-of-view features of two people	finding
enabled by paired egocentric videos	finding
are complementary and essential for reliably recognizing micro-actions and reactions	finding
Speaker adaptive training ( SAT ) is a well studied technique for Gaussian mixture acoustic models ( GMMs )	background
Recently we proposed to perform SAT for deep neural networks ( DNNs )	background
with speaker i-vectors applied in feature learning	background
The resulting SAT-DNN models significantly outperform DNNs on word error rates ( WERs )	background
In this paper	mechanism
we present different methods First	mechanism
we conduct detailed analysis Second	mechanism
the SAT-DNN approach is extended Third	mechanism
we enrich the i-vector representation with global speaker attributes ( age	mechanism
gender etc	mechanism
) obtained automatically from video signals	mechanism
On a collection of instructional videos	mechanism
incorporation of the additional visual features is observed to boost the recognition accuracy of SAT-DNN	mechanism
highlight the behavior and efficacy of such networks show that these networks	finding
while simple are still more accurate	finding
In this paper we introduce a simplified architecture where word-spotting needs to be done in real-time and phoneme-level information is not available for training The network operates as a self-contained block in a strictly forward-pass configuration to directly generate keyword labels	mechanism
We call these simple networks causal networks	mechanism
where the current output is only weighted by the the past inputs and outputs	mechanism
Since the basic network has a simpler architecture as compared to traditional memory networks used in keyword spotting	mechanism
it also requires less data to train	mechanism
Experiments on a standard speech database Comparisons with a standard HMM-based keyword spotter	method
Spoken language interfaces are being incorporated into various devices ( e	background
g	background
smart-phones smart TVs	background
etc )	background
We propose to dynamically add application-based domains according to users ' requests by using descriptions of applications as a retrieval cue to find relevant applications The approach uses structured knowledge resources ( e	mechanism
g	mechanism
Freebase Wikipedia FrameNet ) to induce types of slots for generating semantic seeds	mechanism
and enriches the semantics of spoken queries with neural word embeddings	mechanism
where semantically related concepts can be additionally included for acquiring knowledge that does not exist in the predefined domains	mechanism
The system can then retrieve relevant applications or dynamically suggest users install applications that support unexplored domains	mechanism
We find that vendor descriptions provide a reliable source of information for this purpose	mechanism
We present MergePoint	mechanism
a new binary-only symbolic execution system MergePoint introduces veritesting	mechanism
a new technique that employs static symbolic execution Veritesting allows MergePoint to find twice as many bugs	mechanism
explore orders of magnitude more paths	mechanism
and achieve higher code coverage than previous dynamic symbolic execution systems	mechanism
What defines an action like `` kicking ball { '' } ?	background
We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51	finding
More importantly our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset	finding
In this paper	mechanism
we propose by modeling an action as a transformation which changes the state of the environment before the action happens ( precondition ) to the state after the action ( effect )	mechanism
Motivated by recent advancements of video representation using deep learning	mechanism
we design a Siamese network which models the action as a transformation on a high-level feature space	mechanism
Product architecture structures the coordination problem that the development organization must solve	background
The modularity strategy establishes design rules that fix module functionality and interfaces	background
and assigns development work for each module to a single team	background
The modules present relatively independent coordination problems that teams attempt to solve with all the traditional coordination mechanisms available to them	background
The applicability and effectiveness of this strategy is limited with increasing technical and organizational volatility	background
I present a theory of coordination	mechanism
based on decision networks	mechanism
I review evidence testing several hypotheses derived from the theory	method
One of the pillars of the modern scientific method is model validation : comparing a scientific model 's predictions against empirical observations	background
Today a scientist demonstrates the validity of a model by making an argument in a paper and submitting it for peer review	background
a process comparable to code review in software engineering	background
While human review helps to ensure that contributions meet high-level goals	background
software engineers typically supplement it with unit testing to get a more complete picture of the status of a project	background
Scientific communities differ from software communities in several key ways	background
however	background
In this paper	mechanism
we introduce Sci Unit	mechanism
a framework for test-driven scientific model validation	mechanism
Although different approaches to decision-making in self adaptive systems have shown their effectiveness in the past by factoring in predictions about the system and its environment ( e	background
g	background
resource availability )	background
no proposal considers the latency associated with the execution of tactics upon the target system	background
However different adaptation tactics can take different amounts of time until their effects can be observed	background
In reactive adaptation	background
ignoring adaptation tactic latency can lead to suboptimal adaptation decisions ( e	background
g	background
activating a server that takes more time to boot than the transient spike in traffic that triggered its activation )	background
In proactive adaptation	background
taking adaptation latency into account is necessary to get the system into the desired state to deal with an upcoming situation	background
Our results show that factoring in tactic latency in decision making improves the outcome of adaptation and show that it achieves higher utility than an algorithm that under the assumption of no latency is optimal	finding
In this paper	mechanism
we introduce a formal analysis technique based on model checking of stochastic multiplayer games ( SMGs ) In particular	mechanism
we apply this technique We also present an algorithm that considers tactic latency	mechanism
Mobile devices have become powerful ultra-portable personal computers supporting not only communication but also running a variety of complex	background
interactive applications Because of the unique characteristics of mobile interaction Our findings underline the need for a more nuanced set of interactions that support short mobile device uses	background
in particular review sessions	background
and propose a classification of use based on duration and interaction type : glance	mechanism
review and engage	mechanism
through proactively suggesting short tasks to the user that go beyond simple application notifications	mechanism
We use the findings from our study to create and explore the design space	mechanism
We evaluate the concept through a user evaluation of an interactive lock screen prototype	method
called ProactiveTasks	method
The growing size of modern storage systems is expected to exceed billions of objects	background
making metadata scalability critical to overall performance	background
In this paper	mechanism
we introduce a middleware design called IndexFS that adds support to existing file systems such as PVFS	mechanism
Lustre and HDFS IndexFS uses a table-based architecture that incrementally partitions the namespace on a per-directory basis	mechanism
preserving server and disk locality for small directories	mechanism
An optimized log-structured layout is used to store metadata and small files efficiently We also propose two client-based storm-free caching techniques : bulk namespace insertion such as N-N checkpointing ; and stateless consistent metadata caching	mechanism
By combining these techniques	method
Experiments	method
constructions of coding schemes against two well-studied classes of tampering functions ; namely	mechanism
bit-wise tampering functions ( where the adversary tampers each bit of the encoding independently ) and the much more general class of split-state adversaries ( where two independent adversaries arbitrarily tamper each half of the encoded sequence )	mechanism
Recent advances in rendering and data-driven animation have enabled the creation of compelling characters with impressive levels of realism	background
A better understanding of the factors that make human motion recognizable and appealing would be of great value in industries where creating a variety of appealing virtual characters with realistic motion is required Average faces are perceived to be less distinctive but more attractive	background
We found that dancing motions were most easily recognized and that distinctiveness in one gait does not predict how recognizable the same actor is when performing a different motion	finding
As hypothesized average motions were always amongst the least distinctive and most attractive	finding
Furthermore as 50\ % of participants in the experiment were Caucasian European and 50\ % were Asian Korean	finding
we found that the latter were as good as or better at recognizing the motions of the Caucasian actors than their European counterparts	finding
in particular for dancing males	finding
whom they also rated more highly for attractiveness	finding
we captured thirty actors walking	method
jogging and dancing	method
and applied their motions to the same virtual character ( one each for the males and females )	method
We then conducted a series of perceptual experiments	method
Transport protocols must accommodate diverse application and network requirements	background
As a result	background
TCP has evolved over time with new congestion control algorithms such as support for generalized AIMD	background
background flows and multipath	background
On the other hand	background
explicit congestion control algorithms have been shown to be more efficient	background
However they are inherently more rigid because they rely on in-network components	background
We show that FCP allows evolution by accommodating diversity and ensuring coexistence	finding
while being as efficient as existing explicit congestion control algorithms	finding
This paper presents a flexible framework called FCP by exposing a simple abstraction for resource allocation	mechanism
FCP incorporates novel primitives for end-point flexibility ( aggregation and preloading ) into a single framework and makes economics-based congestion control practical by explicitly handling load variations and by decoupling it from actual billing	mechanism
Phase-contrast microscopy is one of the most common and convenient imaging modalities to observe long-term multi-cellular processes	background
which generates images by the interference of lights passing through transparent specimens and background medium with different retarded phases	background
demonstrate that the proposed approach produces quality segmentation of individual cells and outperforms previous approaches	finding
Experiments	method
Motivation : Several types of studies	background
including genome-wide association studies and RNA interference screens	background
strive to link genes to diseases Although these approaches have had some success	background
genetic variants are often only present in a small subset of the population	background
and screens are noisy with low overlap between experiments in different labs	background
Neither provides a mechanistic model explaining how identified genes impact the disease of interest or the dynamics of the pathways those genes regulate	background
Such mechanistic models could be used to accurately predict downstream effects of knocking down pathway members and allow comprehensive exploration of the effects of targeting pairs or higher-order combinations of genes	background
Results The resulting networks correctly identified many of the known pathways and transcriptional regulators of this disease	finding
Furthermore they accurately predict RNA interference effects and can be used to infer genetic interactions	finding
greatly improving over other methods suggested for this task	finding
allowed us to identify several strain-specific targets of this infection	finding
We developed methods Our model	mechanism
SDREM integrates static and time series data to link proteins and the pathways they regulate in these networks	mechanism
SDREM uses prior information about proteins ' likelihood of involvement in a disease ( e	mechanism
g	mechanism
from screens ) to improve the quality of the predicted signaling pathways	mechanism
We used our algorithms to study the human immune response to H1N1 influenza infection Applying our method to the more pathogenic H5N1 influenza	method
Supporting students ' self-regulated learning ( SRL ) is an important topic in the learning sciences	background
Two critical processes involved in SRL are self-assessment and study choice	background
Intelligent tutoring systems ( ITSs ) have been shown to be effective in supporting students ' domain-level learning through guided problem-solving practice	background
but it is an open question how they can support SRL processes effectively	background
while maintaining or even enhancing their effectiveness at the domain level	background
This work informs the design of future ITS that supports SRL	background
The evaluations reveal that the new OLM with self-assessment support facilitates students ' learning processes	finding
and enhances their learning outcomes significantly	finding
However we did not find significant learning gains due to the problem selection feature	finding
We used a combination of user-centered design techniques We added three features to the tutor ' Open Learner Model ( OLM ) that may scaffold students ' self-assessment ( self-assessment prompts	mechanism
delaying the update of students ' progress bars	mechanism
and providing progress information on the problem type level )	mechanism
We also designed a problem selection screen with shared student/system control and game-like features	mechanism
When human annotators are given a choice about what to label in an image	background
they apply their own subjective judgments on what to ignore and what to mention	background
Our results are highly interpretable for reporting `` what 's in the image { '' } versus `` what 's worth saying	finding
We show significant improvements over traditional algorithms for both image classification and image captioning	finding
doubling the performance of existing methods in some cases	finding
Such annotations do not use consistent vocabulary	mechanism
and miss a significant amount of the information present in an image ; however	mechanism
we demonstrate that the noise in these annotations exhibits structure and can be modeled	mechanism
We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels	mechanism
{ '' } We demonstrate the algorithm 's efficacy along a variety of metrics and datasets	method
including MS COCO and Yahoo Flickr 100M	method
leading toward a rich family of potential extensions to CW algorithms	background
show that our robust	finding
cost-sensitive extensions consistently reduce the cost incurred in both online and batch learning settings	finding
We also demonstrate a correspondence between the VaR and CVaR constraints used for classification and uncertainty sets used in robust optimization	finding
We introduce confidence-weighted ( CW ) online learning algorithms Our work extends the original confidence-weighted optimization framework in two important directions First	mechanism
we show how the original value at risk ( VaR ) probabilistic constraint in CW algorithms can be generalized to a worst-case conditional value at risk ( CVaR ) constraint for more robust learning from cost-weighted examples	mechanism
Second we show how to reduce adversarial feature noise	mechanism
which can be useful in fraud detection scenarios	mechanism
by reframing the optimization problem in terms of maximum a posteriori estimation	mechanism
The resulting optimization problems can be solved efficiently	mechanism
Experiments on real-world and synthetic datasets	method
which are currently in clinical use for rescuing hematopoietic function during bone marrow transplants	background
Our method achieved promising performance in the experiments with hematopoietic stem cell ( HSC ) populations	finding
This paper proposes a vision-based method for detecting apoptosis ( programmed cell death )	mechanism
Our method targets non-adherent cells	mechanism
which float or are suspended freely in the culture medium-in contrast to adherent cells	mechanism
which are attached to a petri dish	mechanism
The method first detects cell regions and tracks them over time	mechanism
resulting in the construction of cell tracklets	mechanism
For each of the tracklets	mechanism
visual properties of the cell are then examined to know whether and when the tracklet shows a transition from a live cell to a dead cell	mechanism
in order to determine the occurrence and timing of a cell death event	mechanism
For the validation	method
a transductive learning framework is adopted to utilize unlabeled data in addition to labeled data	method
Silhouettes provide rich information on three-dimensional shape	background
since the intersection of the associated visual cones generates the `` visual hull { '' }	background
which encloses and approximates the original shape However	background
not all silhouettes can actually be projections of the same object in space : this simple observation has implications in object recognition and multi-view segmentation	background
and has been ( often implicitly ) used as a basis for camera calibration	background
and point out some possible directions for future research	background
After discussing some general results	finding
we present a `` dual { '' } formulation for consistency	mechanism
that gives conditions for a family of planar sets to be sections of the same object	mechanism
Finally we introduce a more general notion of silhouette `` compatibility { '' } under partial knowledge of the camera projections	mechanism
We present this notion as a natural generalization of traditional multi-view geometry	method
which deals with consistency for points	method
Politeness is believed to facilitate communication in human interaction	background
as it can minimize the potential for conflict and confrontation	background
Regarding the role of politeness strategies for human-robot interaction	background
conflicting findings are presented in the literature	background
Our findings suggest that the interaction context has a greater impact on participants ' perception of the robot in HRI than the use - or lack - of politeness strategies	finding
Thus we conducted a between-participants experimental study with a receptionist robot	method
Large-scale information processing systems are able to extract massive collections of interrelated facts	background
We show that compared to existing methods	finding
our approach is able to achieve improved AUC and F1 with significantly lower running time	finding
can be transformed into a knowledge graph The extractions form an extraction graph and we refer to the task of removing noise	mechanism
inferring missing information	mechanism
and determining which candidate facts should be included into a knowledge graph as knowledge graph identification In order to perform this task	mechanism
we must reason jointly about candidate facts and their associated extraction confidences	mechanism
identify co-referent entities	mechanism
and incorporate ontological constraints	mechanism
Our proposed approach uses probabilistic soft logic ( PSL )	mechanism
a recently introduced probabilistic modeling framework which easily scales to millions of facts	mechanism
We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL project containing over 1M extractions and 70K ontological relations	method
Occlusions are common in real world scenes and are a major obstacle to robust object detection	background
Previous approaches primarily enforced local coherency or learned the occlusion structure from data	background
However local coherency ignores the occlusion structure in real world scenes and learning from data requires tediously labeling many examples of occlusions for every view of every object	background
Other approaches require binary classifications of matching scores	background
Our method demonstrates significant improvement in estimating the mask of the occluding region and improves object instance detection on a challenging dataset of objects under severe occlusions	finding
In this paper	mechanism
we present a method We address these limitations by formulating occlusion reasoning as an efficient search over occluding blocks which best explain a probabilistic matching pattern	mechanism
Binary codes that are binarizations of features represented by real numbers have recently been used in the object recognition field	background
in order to achieve reduced memory and robustness with respect to noise	background
From the results of we confirmed that the proposed method enables an increase in detection performance while maintaining the same levels of memory and computing costs as those for previous methods of binarizing features	finding
With this study	mechanism
we introduce a transition likelihood model into classifiers This enables classifications that consider transitions to the desired binary code	mechanism
even if the observed binary code differs from the actually desired binary code for some reason	mechanism
experiments	method
`` Socially cooperative driving { '' } is an integral part of our everyday driving	background
hence	background
Compared with approaches that do not take social behavior into account	finding
the iPCB algorithm shows a 41	finding
7\ % performance improvement based on the chosen cost functions	finding
In this paper	mechanism
an intention-integrated Prediction-and Cost function-Based algorithm iPCB ) framework is proposed An intention estimator is developed Then for each candidate strategy	mechanism
a prediction engine considering the interaction between host and surrounding agents is used A cost function-based evaluation is applied	mechanism
On-road motion planning for autonomous vehicles is in general a challenging problem Past efforts have proposed solutions for urban and highway environments individually	background
and propose a novel two-step motion planning system in a single framework	mechanism
Reference Trajectory Planning ( I ) makes use of dense lattice sampling and optimization techniques By focused sampling around the reference trajectory	mechanism
Tracking Trajectory Planning ( II ) generates	mechanism
evaluates and selects parametric trajectories that further satisfy kinodynamic constraints for execution The described method retains most of the performance advantages of an exhaustive spatiotemporal planner while significantly reducing computation	mechanism
Vehicular ad hoc networks ( VANETs ) are seen as an important enabling technology for improving both traffic safety and efficiency	background
Virtual Traffic Lights ( VTLs ) are a promising proposal for reducing travel time by efficiently controlling road intersections	background
VTLs use vehicle-to-vehicle communication to dynamically optimize traffic flow and they display traffic light information on the windshield	background
We show that the benefits of VTLs grow as a function of the penetration rate of equipped vehicles	finding
In this paper we present a solution for a VTL partial deployment scenario that is based on the idea of having VTL equipped cars display traffic light information on the outside of the vehicle	mechanism
in terms of intersection throughput and average delay reduction	method
which can be readily utilized in an active learning framework to improve identity-aware multi-object tracking	background
show that not only is our proposed tracker effective	finding
but also the solution path enables automatic pinpointing of potential tracking failures	finding
Experiments	method
Spoken dialogue systems typically use predefined semantic slots to parse users ' natural language inputs into unified semantic representations	background
Our slot filling evaluations also indicate the promising future of this proposed approach	background
show that the automatically induced semantic slots are in line with the reference slots created by domain experts : we observe a mean averaged precision of 69	finding
36\ % using ASR-transcribed data	finding
To do this	mechanism
we propose the use of a state-of-the-art frame-semantic parser	mechanism
and a spectral clustering based slot ranking model that adapts the generic output of the parser to the target semantic space	mechanism
Empirical experiments on a real-world spoken dialogue dataset	method
Smartphones are now targets of malicious viruses Furthermore	background
the increasing `` connectedness { '' } of smartphones has resulted in new delivery vectors for malicious viruses	background
including proximity- social- and other technology-based methods	background
In fact Cabir and CommWarrior are two viruses-observed in the wild-that spread	background
at least in part	background
using proximity-based techniques ( line-of-sight bluetooth radio )	background
find that the first eigenvalue of the system matrices lambda ( S1 )	finding
lambda ( S2 ) of the two networks ( static and dynamic networks ) appropriately captures the competitive interplay between two viruses and effectively predicts the competition 's `` winner { '' }	finding
which provides a feasible way to defend against smartphone viruses	finding
In this paper	mechanism
we propose and evaluate SI1I2S	mechanism
a competition model To approximate dynamic network behavior	mechanism
we use classic mobility models from ad hoc networking	mechanism
e	mechanism
g	mechanism
Random Waypoint Random Walk and Levy Flight	mechanism
We analyze our model using techniques from dynamic systems and	method
Together these findings suggest that head motion is strongly related to age-appropriate emotion challenge	background
are consistent with the hypothesis that perturbations of normal responsiveness carry-over even after the parent resumes normal responsiveness in the reunion	background
and that there are frequent changes in direction of influence in the postural domain	background
During infant gaze toward the parent	finding
infant angular amplitude and velocity of pitch and yaw decreased from face-to-face ( FF ) to still-face ( SF ) episodes and remained lower in the following Reunion	finding
During infant gaze away from the parent	finding
angular velocity of pitch decreased from FF to SF and remained lower in the Reunion ( RE ) Windowed cross-correlation suggested strong bidirectional effects with frequent shifts in the direction of influence	finding
The number of significant positive and negative peaks was higher during FF than RE	finding
Gaze toward and away from the parent was modestly predicted by head orientation	finding
It is common to represent photos as vertices of a weighted graph	background
where edge weights measure similarity or distance between pairs of photos Ultimately	background
our system enables everyday people to take advantage of each others ' perspectives in order to create on-the-spot spatiotemporal visual experiences similar to the popular bullet-time sequence	background
We believe that this type of application will greatly enhance shared human experiences spanning from events as personal as parents watching their children 's football game to highly publicized red carpet galas	background
We present a near real-time algorithm Our system favors immediacy and local coherency to global consistency	mechanism
We introduce Angled Graphs as a new data structure Weighted angled graphs extend weighted graphs with angles and angle weights which penalize turning along paths	mechanism
As a result	mechanism
locally straight paths can be computed by specifying a photo and a direction	mechanism
The weighted angled graphs of photos used in this paper can be regarded as the result of discretizing the Riemannian geometry of the high dimensional manifold of all possible photos	mechanism
They are important for formal verification of realistic hybrid systems and embedded software	background
We demonstrate scalability of the algorithms	finding
We develop delta-complete algorithms for SMT formulas that are purely existentially quantified	mechanism
as well as there exists for all-formulas whose universal quantification is restricted to the time variables	mechanism
as implemented in our open-source solver dReal	method
on SMT benchmarks with several hundred nonlinear ODEs and variables	method
A robotic swarm is a decentralized group of robots which overcome failure of individual robots with robust emergent behaviors based on local interactions	background
These behaviors are not well built for accomplishing complex tasks	background
however because of the changing assumptions required in various applications and environments	background
A new movement in the research field is to add human input to influence the swarm in order to help make the robots goal directed and overcome these problems Previous studies have all used visual feedback through a computer interface to give the user the swarm state information Researchers in multi-robot systems have shown benefits of haptic feedback in obstacle navigation before	background
The study shows the benefits of the additional feedback in a target searching class	finding
In most environments	finding
operators were able to cover significantly more area	finding
increasing the chance of finding more targets	finding
The other environment found no significant difference	finding
showing that the haptic feedback does not degrade performance in any of the tested environments	finding
This supports our hypothesis that haptic feedback is useful in HSI and requires further research to maximize its potential	finding
This study adapted swarm control algorithms but this study is a novel method because of the decentralized formation of the robotic swarm	mechanism
Clustering is the task of grouping a set of objects so that objects in the same cluster are more similar to each other than to those in other clusters The crucial step in most clustering algorithms is to find an appropriate similarity metric	background
which is both challenging and problem-dependent	background
Supervised clustering approaches	background
which can exploit labeled clustered training data that share a common metric with the test set	background
have thus been proposed	background
confirm several orders of magnitude speedup while still achieving state-of-the-art performance	finding
In this paper	mechanism
we propose a new structured Mahalanobis Distance Metric Learning method We formulate our problem as an instance of large margin structured prediction and prove that it can be solved very efficiently in closed-form	mechanism
The complexity of our method is ( in most cases ) linear in the size of the training dataset We further reveal a striking similarity between our approach and multivariate linear regression	mechanism
Experiments on both synthetic and real datasets	method
Previous studies have examined the characteristics of physiological tremor under laboratory settings as well as different operating conditions	background
However different test methods make the comparison of results across trials and conditions difficult	background
Two vitroretinal microsurgeons were evaluated while performing a pointing task with no entry-point constraint	method
constrained by an artificial eye model	method
and constrained by a rabbit eye in vivo For the three respective conditions A spectral analysis was also performed	method
Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D	background
Manual needle insertion allows the clinician to control the insertion speed	background
ensuring patient safety	background
demonstrate the performance of the proposed controller	finding
show the feasibility of this technique in 2D and 3D environments	finding
This paper presents a control law A look-ahead proportional controller for position and orientation is presented	mechanism
The look-ahead distance is a linear function of insertion speed	mechanism
Simulations in a 3D brain-like environment Experimental results also	method
remains a difficult task	background
We demonstrated competitive performance both in accuracy relative to human annotation and computation time	finding
In this paper	mechanism
we define interesting events as unusual events which occur rarely in the entire video and we propose a novel interesting event summarization framework based on the technique of density ratio estimation recently introduced in machine learning Our proposed framework is unsupervised and it can be applied to general video sources	mechanism
including videos from moving cameras	mechanism
We evaluated the proposed approach on a publicly available dataset in the context of anomalous crowd behavior and with a challenging personal video dataset	method
Existing semi-supervised approaches are typically unreliable and face semantic drift because the learning task is under-constrained	background
This is primarily because they ignore the strong interactions that often exist between scene categories	background
such as the common attributes shared across categories as well as the attributes which make one scene different from another For example	background
the knowledge that an image is an auditorium can improve labeling of amphitheaters by enforcing constraint that an amphitheater image should have more circular structures than an auditorium image	background
We demonstrate the effectiveness of our approach including results	finding
We propose constraints based on mutual exclusion	mechanism
binary attributes and comparative attributes and show that they help us	mechanism
through extensive experiments	method
on a very large dataset of one million images	method
As proof-of-concept results demonstrate that our model accurately predicts distributions over future actions of individuals We show how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories	finding
We denote this task activity forecasting	mechanism
our approach models the effect of the physical environment on the choice of human actions	mechanism
This is accomplished by the use of state-of-the-art semantic scene understanding combined with ideas from optimal control theory Our unified model also integrates several other key elements of activity analysis	mechanism
namely destination forecasting	mechanism
sequence smoothing and transfer learning	mechanism
we focus on the domain of trajectory-based activity analysis from visual input	method
Experimental	method
Reconstructing an arbitrary configuration of 3D points from their projection in an image is an ill-posed problem	background
When the points hold semantic meaning	background
such as anatomical landmarks on a body	background
human observers can often infer a plausible 3D configuration	background
drawing on extensive visual memory	background
show generalization to novel 3D configurations and robustness to missing data	finding
We present an activity-independent method leveraging a large motion capture corpus as a proxy for visual memory Our method solves for anthropometrically regular body pose and explicitly estimates the camera via a matching pursuit algorithm operating on the image projections Anthropometric regularity ( i	mechanism
e	mechanism
that limbs obey known proportions ) is a highly informative prior	mechanism
but directly applying such constraints is intractable	mechanism
Instead we enforce a necessary condition on the sum of squared limb-lengths that can be solved for in closed form to discourage implausible configurations in 3D	mechanism
We evaluate performance on a wide variety of human poses captured from different viewpoints and	method
Human pose estimation requires a versatile yet well-constrained spatial model for grouping locally ambiguous parts together to produce a globally consistent hypothesis	background
showing its ability to capture high-order dependencies of parts Second	finding
our model achieves accurate reconstruction of unseen poses Finally	finding
our model achieves state-of-art performance	finding
and substantially outperforms recent hierarchical models	finding
In this paper	mechanism
we propose a new hierarchical spatial model that can capture an exponential number of poses with a compact mixture representation on each part	mechanism
Using latent nodes	mechanism
it can represent high-order spatial relationship among parts with exact inference	mechanism
Different from recent hierarchical models that associate each latent node to a mixture of appearance templates ( like HoG )	mechanism
we use the hierarchical structure as a pure spatial prior avoiding the large and often confounding appearance space	mechanism
We verify the effectiveness of this model in three ways	method
First samples representing human-like poses can be drawn from our model compared to a nearest neighbor pose representation on three challenging datasets	method
Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition This success can be largely attributed to learning shared representations from multiple supervisory tasks	background
Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples	finding
In this paper	mechanism
we propose a principled approach using multi-task learning	mechanism
Specifically we propose a new sharing unit : `` cross-stitch { '' } unit	mechanism
These units combine the activations from multiple networks and can be trained end-to-end	mechanism
A network with cross-stitch units can learn an optimal combination of shared and task-specific representations	mechanism
This is an important scenario that frequently arises in practice not only when two different types of sensors are used	background
but also when the sensors are not co-located and have different sampling rates	background
Previous work has addressed this problem by restricting interpretation to a single representation in one of the domains	background
with augmented features that attempt to encode the information from the other modalities	background
we demonstrate that this co-inference approach also improves performance over the canonical approach	finding
Instead we propose to analyze all modalities simultaneously while propagating information across domains during the inference procedure	mechanism
In addition to the immediate benefit of generating a complete interpretation in all of the modalities	mechanism
Object discovery algorithms group together image regions that originate from the same object	background
This process is effective when the input collection of images contains a large number of densely sampled views of each object	background
thereby creating strong connections between nearby views	background
{ '' } Our approach can correctly discover links between regions of the same object even if they are captured from dramatically different viewpoints	finding
With the help from these added links	finding
our proposed approach can robustly discover object instances even with sparse coverage of the viewpoints	finding
The problem of training classifiers from limited data is one that particularly affects large-scale and social applications	background
and as a result	background
although carefully trained machine learning forms the backbone of many current techniques in research	background
it sees dramatically fewer applications for end-users Recently we demonstrated a technique for selecting or recommending a single good classifier from a large library even with highly impoverished training data	background
a modification to the AdaBoost algorithm that incorporates recommendation	mechanism
Evaluating on an action recognition problem	mechanism
we present two viable methods	mechanism
We have shown in prior work how to give a Curry-Howard interpretation of the proofs in the linear sequent calculus as pi-calculus processes subject to a session type discipline	background
We show that the resulting translations induce sharing and copying parallel evaluation strategies for the original lambda-terms	finding
thereby providing a new logically motivated explanation for these strategies	finding
The translations proceed in two steps : standard embeddings of simply-typed lambda-calculus in a linear lambda-calculus	method
followed by a standard translation of linear natural deduction to linear sequent calculus	method
The detection of apoptosis	background
or programmed cell death	background
is important to understand the underlying mechanism of cell development	background
the method achieved around 90\ % accuracy in terms of average precision and recall	finding
In this work	mechanism
we present an image analysis method which is non-destructive imaging	mechanism
The method first detects candidates for apoptotic cells based on the optical principle of phase-contrast microscopy in connection with the properties of apoptotic cells	mechanism
The temporal behavior of each candidate is then examined in its neighboring frames in order	mechanism
Cache compression is a promising technique to increase on-chip cache capacity and to decrease on-chip and off-chip bandwidth usage	background
Unfortunately directly applying well-known compression algorithms ( usually implemented in software ) leads to high hardware complexity and unacceptable decompression/compression latencies	background
which in turn can negatively affect performance	background
our studies show that B Delta I strikes a sweet-spot in the tradeoff between compression ratio	finding
decompression/compression latencies and hardware complexity Our results show that B Delta I compression improves performance for both single-core ( 8	finding
1\ % improvement ) and multi-core workloads ( 9	finding
5\ % /11	finding
2\ % improvement for two/four cores )	finding
For many applications	finding
B Delta I provides the performance benefit of doubling the cache size of the baseline system	finding
effectively increasing average cache capacity by 1	finding
53X	finding
In this paper	mechanism
we introduce a new compression algorithm called Base-Delta-Immediate ( B Delta I ) compression	mechanism
The key idea is that	mechanism
for many cache lines	mechanism
the values within the cache line have a low dynamic range - i	mechanism
e	mechanism
the differences between values stored within the cache line are small	mechanism
As a result	mechanism
a cache line can be represented using a base value and an array of differences whose combined size is much smaller than the original cache line ( we call this the base+ delta encoding Moreover	mechanism
many cache lines intersperse such base+ delta values with small values - our B Delta I technique efficiently incorporates such immediate values into its encoding	mechanism
Compared to prior cache compression approaches	method
Most contemporary object detection approaches assume each object instance in the training data to be uniquely represented by a single bounding box	background
The new bounding box annotations are determined based on the alignment of an object instance with the other training instances in the dataset	mechanism
Our proposal enables the training data to be reused multiple times for training richer multi-component category models We operationalize this idea by two complementary operations : bounding box shrinking	mechanism
which finds subregions of an object instance that could be shared ; and bounding box enlarging	mechanism
which enlarges object instances to include local contextual cues	mechanism
We empirically validate our approach on the PASCAL VOC detection dataset	method
In practical applications of robot swarms with bio-inspired behaviors	background
a human operator will need to exert control over the swarm to fulfill the mission objectives	background
In many operational settings	background
human operators are remotely located and the communication environment is harsh Hence	background
there exists some latency in information ( or control command ) transfer between the human and the swarm	background
Our experimental results indicate that operators exploited neglect benevolence in different ways to develop successful strategies in the foraging task	finding
Furthermore we show that the use of a predictive display can help mitigate the adverse effects of communication latency	finding
we conduct experiments of human-swarm interaction experimentally	method
Human interaction with robot swarms ( HSI ) is a young field with very few user studies that explore operator behavior All these studies assume perfect communication between the operator and the swarm A key challenge in the use of swarm robotic systems in human supervised tasks is to understand human swarm interaction in the presence of limited communication bandwidth	background
which is a constraint arising in many practical scenarios	background
The lowest bandwidth condition performs poorly	finding
but the medium and high bandwidth condition both perform well	finding
In the medium bandwidth condition	finding
we display useful aggregated swarm information ( like swarm centroid and spread ) to compress the swarm state information	finding
We also observe interesting operator behavior and adaptation of operators ' swarm reaction	finding
We consider three levels of bandwidth availability in a swarm foraging task	method
Pose Machines provide a sequential prediction framework for learning rich implicit spatial models	background
We demonstrate state-of-the-art performance and outperform competing methods	finding
In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation	mechanism
We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages	mechanism
producing increasingly refined estimates for part locations	mechanism
without the need for explicit graphical model-style inference	mechanism
Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision	mechanism
thereby replenishing back-propagated gradients and conditioning the learning procedure	mechanism
on standard benchmarks including the MPII	method
LSP and FLIC datasets	method
Finding meaningful structured representations of 3D point cloud data ( PCD ) has become a core task for spatial perception applications	background
our tests showing favorable performance when compared to octree and NDT-based methods	finding
In this paper we introduce a method As opposed to deterministic structures such as voxel grids or octrees	mechanism
we propose probabilistic subdivisions of the data through local mixture modeling	mechanism
and show how these subdivisions can provide a maximum likelihood segmentation of the data	mechanism
The final representation is hierarchical	mechanism
compact parametric and statistically derived	mechanism
facilitating run-time occupancy calculations through stochastic sampling	mechanism
Unlike traditional deterministic spatial subdivision methods	mechanism
our technique enables dynamic creation of voxel grids according the application 's best needs	mechanism
In contrast to other generative models for PCD	mechanism
we explicitly enforce sparsity among points and mixtures	mechanism
a technique which we call expectation sparsification	mechanism
This leads to a highly parallel hierarchical Expectation Maximization ( EM ) algorithm well-suited for the GPU and real-time execution	mechanism
We explore the trade-offs between model fidelity and model size at various levels of detail	method
The proposed framework provides a way to explore the interaction between climate change and policy factors at a global scale	background
This paper uses a country-level agent-based dynamic network model Some of the networks considered include : alliance networks	mechanism
shared language networks	mechanism
economic influence networks	mechanism
and proximity networks	mechanism
Validation of model is done for migration probabilities between countries	mechanism
as well as for country populations and distributions	mechanism
Sudden weight gain in patients living with Congestive Heart Failure ( CHF ) is often an indication that the individual is retaining fluid	background
which often means that patient 's heart has weakened leading to increased risk of kidney or cardiac failure	background
leading to the possibility of earlier clinical interventions	background
potentially preventing deadly medical emergencies	background
In this work	mechanism
we present a latent variable autoregression model that tracks patient weight and blood pressure over time We are also able to model continuous heart-rate signals and evaluate a subject 's response to physical activity	mechanism
This allows us to detect signs of health decline days earlier than existing rule-based systems	mechanism
Many Android apps heavily depend on collecting and sharing sensitive privacy information	background
such as device ID	background
location and postal address	background
to provide service and value	background
To protect user privacy	background
apps are typically required by market places to provide privacy policies informing users about how their private information will be processed	background
In this paper	mechanism
we present PVDetector	mechanism
an automatic tool that analyzes Android apps	mechanism
Implications for the understanding of human behavior and social agent design are discussed	background
We validated the discovered behavioral patterns Our framework performs significantly better than a baseline linear regression method that does not encode temporal information among behavioral features	finding
by predicting rapport against our ground truth via a forecasting model involving two-step fusion of learned temporal associated rules	mechanism
We mined a reciprocal peer tutoring corpus reliably annotated for nonverbals like eye gaze and smiles	method
conversational strategies like self-disclosure and social norm violation	method
and for rapport ( in 30s thin slices )	method
We then performed a fine-grained investigation of how the temporal profiles of sequences of interlocutor behaviors predict increases and decreases of rapport	method
and how this rapport management manifests differently in friends and strangers	method
Human communication literature states that people with different culture backgrounds act differently in conversations	background
We found that users from different culture context express engagement differently	finding
We implemented two versions of a virtual agent targeting American and Chinese cultures	method
Our results suggest that sequence multiple confounding factors corrections behave the best when different confounders contribute equally to response variables	finding
On the other hand	finding
when various confounders affect the response variable unevenly	finding
results mainly rely on the degree of how the major confounder is corrected	finding
we introduce three different methods for multiple confounding factors correction	mechanism
namely concatenation sequence	mechanism
and interpolation	mechanism
We first review its parameter estimation algorithms before Then we investigate the performance on variable selection task and predictive task on three different data sets	method
synthetic data set	method
semi-empirical synthetic data set based on genome sequences and brain wave data set connecting to confused mental states	method
