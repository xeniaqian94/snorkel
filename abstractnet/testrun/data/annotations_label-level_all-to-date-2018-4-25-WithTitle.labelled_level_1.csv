Social science researchers spend significant time annotating behavioral events in video data in order to quantitatively assess interactions [ 2 ]	background	2K_dev_9
These behavioral events may be instantaneous changes	background	2K_dev_9
continuous actions that span unbounded periods of time	background	2K_dev_9
or behaviors that would be best described by severity or other scalar ratings	background	2K_dev_9
These new features allow analysts to acquire more specific information about events in video datasets	finding	2K_dev_9
Glance [ 4 ] introduced a means of leveraging human intelligence by recruiting crowds of paid online workers to accurately analyze hours of video data in a matter of minutes	mechanism	2K_dev_9
This approach has been shown to expedite work in human-centered fields	mechanism	2K_dev_9
as well as generate training data for automated recognition systems	mechanism	2K_dev_9
In this paper we describe an interactive demonstration of an improved	mechanism	2K_dev_9
more expressive version of Glance that expands the initial set of supported annotation formats ( e	mechanism	2K_dev_9
time range classification etc	mechanism	2K_dev_9
) from one to nine	mechanism	2K_dev_9
Worker interfaces for each of these options are dynamically generated	mechanism	2K_dev_9
along with tutorials based on the analyst 's question	mechanism	2K_dev_9
The complexity of these judgments	purpose	2K_dev_9
coupled with the time and effort required to meticulously assess video	purpose	2K_dev_9
results in a training and evaluation process that can take days or weeks	purpose	2K_dev_9
Computational analysis of video data is still limited due to the challenges introduced by objective interpretation and varied contexts	purpose	2K_dev_9
Social influence is key in technology adoption	background	2K_dev_11
Our results suggest that social influence affects one 's likelihood to adopt a security feature	finding	2K_dev_11
but its effect varies based on the observability of the feature	finding	2K_dev_11
the current feature adoption rate among a potential adopter 's friends	finding	2K_dev_11
and the number of distinct social circles from which those feature-adopting friends originate Curiously	finding	2K_dev_11
there may be a threshold higher than which having more security feature adopting friends predicts for higher adoption likelihood	finding	2K_dev_11
but below which having more feature-adopting friends predicts for lower adoption likelihood	finding	2K_dev_11
Furthermore the magnitude of this threshold is modulated by the attributes of a feature-features that are more noticeable ( Login Approvals	finding	2K_dev_11
Trusted Contacts ) have lower thresholds	finding	2K_dev_11
Here we analyzed how three Facebook security features ' Login Approvals	method	2K_dev_11
Login Notifications and Trusted Contacts-diffused through the social networks of 1	method	2K_dev_11
but its role in security-feature adoption is unique and remains unclear	purpose	2K_dev_11
Online communities much like companies in the business world	background	2K_dev_14
often need to transfer best practices internally from one unit to another to improve their performance	background	2K_dev_14
The current research introduces a contingency perspective on practice transfer	finding	2K_dev_14
holding that the value of modifications depends on when they are introduced and who introduces them modifications are more helpful if they are introduced after the receiving project has had experience with the imported practice Furthermore	finding	2K_dev_14
modifications are more effective if they are introduced by members who have experience in a variety of other projects	finding	2K_dev_14
Empirical research on the transfer of a quality-improvement practice between projects within Wikipedia shows that	method	2K_dev_14
Organizational scholars disagree about how much a recipient unit should modify a best practice when incorporating it Some evidence indicates that modifying a practice that has been successful in one environment will introduce problems	purpose	2K_dev_14
undercut its effectiveness and harm the performance of the recipient unit Other evidence	purpose	2K_dev_14
though suggests that recipients need to adapt the practice to fit their local environment	purpose	2K_dev_14
Previous work has shown the promise of crowdsourcing analogical idea generation	background	2K_dev_28
where distributing the stages of analogical processing across many people can reduce fixation	background	2K_dev_28
identify inspirations from more diverse domains	background	2K_dev_28
and lead to more creative ideas	background	2K_dev_28
Our results show that crowds find the most useful inspirations when the problem domain is represented abstractly and constraints are represented more concretely	finding	2K_dev_28
This paper contributes a systematic crowdsourcing approach	mechanism	2K_dev_28
However prior work has only considered problems with a single constraint	purpose	2K_dev_28
while many real-world problems involve multiple constraints for eliciting multiple constraints inherent in a problem and using those constraints to find inspirations useful in solving it To do so we identify methods to elicit useful constraints at different levels of abstraction	purpose	2K_dev_28
and empirical results that identify how the level of abstraction influences creative idea generation	purpose	2K_dev_28
thus improving their ability to facilitate one-time or spontaneous exchanges of information	background	2K_dev_38
show how leveraging multiple contexts improves our ability to detect and form relevant groupings Through two prototypes	finding	2K_dev_38
we demonstrate how DIDJA enhances existing user experiences	finding	2K_dev_38
and show how developers can use our toolkit to easily facilitate frictionless collaborations between users and their environment We then perform an extended experiment and show how DIDJA is able to accurately form groups under realistic conditions	finding	2K_dev_38
In our approach devices share context with each other	mechanism	2K_dev_38
and form groups when these readings are found to be similar to one another Through a formative study	mechanism	2K_dev_38
We then present DIDJA	mechanism	2K_dev_38
a robust software toolkit that automatically collects and analyzes contextual information in order to find and form groups	mechanism	2K_dev_38
We present a new technique that allows mobile devices to opportunistically group with one another	purpose	2K_dev_38
we examine the limitations of using a single type of context to form groups	purpose	2K_dev_38
Languages for music audio processing typically offer a large assortment of unit generators There is great duplication among different language implementations	background	2K_dev_54
as each language must implement many of the same ( or nearly the same ) unit generators	background	2K_dev_54
We suggest that these techniques might eliminate most of the effort of building unit generator libraries and could help with the implementation of embedded audio systems where unit generators are needed but a full embedded Csound engine is not required	background	2K_dev_54
Using Aura as an example	mechanism	2K_dev_54
we modified Csound to allow efficient	mechanism	2K_dev_54
dynamic allocation of individual unit generators without using the Csound compiler or writing Csound instruments	mechanism	2K_dev_54
We then extended Aura using automatic code generation so that Csound unit generators can be accessed in the normal way from within Aura	mechanism	2K_dev_54
In this scheme Csound details are completely hidden from Aura users	mechanism	2K_dev_54
Csound has a large library of unit generators and could be a useful source of reusable unit generators for other languages or for direct use in applications	purpose	2K_dev_54
In this study we consider how Csound unit generators can be exposed to direct access by other audio processing languages	purpose	2K_dev_54
Motivated by a radically new peer review system that the National Science Foundation recently experimented with	background	2K_dev_56
An ( m ; k ) -selection mechanism asks each PI to review m proposals	mechanism	2K_dev_56
and uses these reviews to select ( at most ) k proposals	mechanism	2K_dev_56
We are interested in impartial mechanisms	mechanism	2K_dev_56
which guarantee that the ratings given by a PI to others ' proposals do not affect the likelihood of the PI 's own proposal being selected We design an impartial mechanism that selects a k-subset of proposals that is nearly as highly rated as the one selected by the non-impartial ( abstract version of ) the NSF pilot mechanism	mechanism	2K_dev_56
even when the latter mechanism has the `` unfair '' advantage of eliciting honest reviews	mechanism	2K_dev_56
we study peer review systems in which proposals are reviewed by PIs who have submitted proposals themselves	purpose	2K_dev_56
The fairness notion of maximin share ( MMS ) guarantee underlies a deployed algorithm for allocating indivisible goods under additive valuations	background	2K_dev_57
Previous work has shown that such an MMS allocation may not exist	background	2K_dev_57
but the counterexample requires a number of goods that is exponential in the number of players ;	background	2K_dev_57
we give a new construction that uses only a linear number of goods	mechanism	2K_dev_57
On the positive side	mechanism	2K_dev_57
we formalize the intuition that these counterexamples are very delicate by designing an algorithm when valuations are drawn at random	mechanism	2K_dev_57
Our goal is to understand when we can expect to be able to give each player his MMS guarantee	purpose	2K_dev_57
that provably finds an MMS allocation with high probability	purpose	2K_dev_57
A paradigmatic problem in social choice theory deals with the aggregation of subjective preferences of individuals represented as rankings of alternatives into a social ranking	background	2K_dev_58
We show that ignoring uncertainty altogether can lead to suboptimal outcomes	finding	2K_dev_58
Under the classic objective of minimizing the ( expected ) sum of Kendall tau distances between the input rankings and the output ranking	mechanism	2K_dev_58
we establish that preference elicitation is surprisingly straightforward and near-optimal solutions can be obtained in polynomial time	mechanism	2K_dev_58
both in theory and using real data	method	2K_dev_58
We are interested in settings where individuals are uncertain about their own preferences	purpose	2K_dev_58
and represent their uncertainty as distributions over rankings	purpose	2K_dev_58
We show that envy-free allocations of sellable goods are significantly more efficient than their unsellable counterparts	finding	2K_dev_61
Our novel setting includes an option to sell each good for a fraction of the minimum value any player has for the good	mechanism	2K_dev_61
we reason about the price of envy-freeness of allocations of sellable goods -- the ratio between the maximum social welfare and the social welfare of the best envy-free allocation	mechanism	2K_dev_61
We study the envy-free allocation of indivisible goods between two players To rigorously quantify the efficiency gain from selling	purpose	2K_dev_61
Some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives	background	2K_dev_62
results call attention to situations where approval voting is suboptimal	finding	2K_dev_62
by proposing a probabilistic framework of noisy voting	mechanism	2K_dev_62
and asking whether approval voting yields an alternative that is most likely to be the best alternative	mechanism	2K_dev_62
While the answer is generally positive	method	2K_dev_62
our theoretical and empirical	method	2K_dev_62
It seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule	purpose	2K_dev_62
which simply counts the number of times each alternative was approved	purpose	2K_dev_62
We challenge this assertion	purpose	2K_dev_62
We introduce the simultaneous model We show that this model enables the computation of divisions that satisfy proportionality -- a popular fairness notion -- using a protocol that circumvents a standard lower bound via parallel information elicitation	mechanism	2K_dev_64
Cake divisions satisfying another prominent fairness notion	mechanism	2K_dev_64
envy-freeness are impossible to compute in the simultaneous model	mechanism	2K_dev_64
but admit arbitrarily good approximations	mechanism	2K_dev_64
for cake cutting ( the fair allocation of a divisible good )	purpose	2K_dev_64
in which agents simultaneously send messages containing a sketch of their preferences over the cake	purpose	2K_dev_64
Motivated by applications to crowdsourcing	background	2K_dev_65
We show that there is such a voting rule	mechanism	2K_dev_65
which we call the modal ranking rule	mechanism	2K_dev_65
Moreover we establish that the modal ranking rule is the unique rule with the preceding robustness property within a large family of voting rules	mechanism	2K_dev_65
which includes a slew of well-studied rules	mechanism	2K_dev_65
we study voting rules that output a correct ranking of alternatives by quality from a large collection of noisy input rankings We seek voting rules that are supremely robust to noise	purpose	2K_dev_65
in the sense of being correct in the face of any `` reasonable '' type of noise	purpose	2K_dev_65
Classic social choice theory assumes that votes are independent ( but possibly conditioned on an underlying objective ground truth )	background	2K_dev_66
We establish a general framework -- based on random utility theory -- on a social network with arbitrarily many alternatives ( in contrast to previous work	mechanism	2K_dev_66
which is restricted to two alternatives )	mechanism	2K_dev_66
We identify a family of voting rules which	mechanism	2K_dev_66
without knowledge of the social network structure	mechanism	2K_dev_66
are guaranteed with high probability in large networks	mechanism	2K_dev_66
with respect to a wide range of models of correlation among input votes	mechanism	2K_dev_66
This assumption is unrealistic in settings where the voters are connected via an underlying social network structure	purpose	2K_dev_66
as social interactions lead to correlated votes	purpose	2K_dev_66
for ranked voting to recover the ground truth	purpose	2K_dev_66
Limited lookahead has been studied for decades in perfect-information games	background	2K_dev_67
The limited-lookahead player often obtains the value of the game if she knows the expected values of nodes in the game tree for some equilibrium	finding	2K_dev_67
but we prove this is not sufficient in general	finding	2K_dev_67
This uncovers a lookahead pathology	finding	2K_dev_67
We characterize the hardness of finding a Nash equilibrium or an optimal commitment strategy for either player	mechanism	2K_dev_67
showing that in some of these variations the problem can be solved in polynomial time while in others it is PPAD-hard or NP-hard	mechanism	2K_dev_67
We proceed to design algorithms for when the opponent breaks ties 1 ) favorably	mechanism	2K_dev_67
2 ) according to a fixed rule	mechanism	2K_dev_67
or 3 ) adversarially	mechanism	2K_dev_67
The impact of limited lookahead is then investigated experimentally Finally	method	2K_dev_67
we study the impact of noise in those estimates and different lookahead depths	method	2K_dev_67
This paper initiates a new direction via two simultaneous deviation points : generalization to imperfect-information games and a game-theoretic approach The question of how one should act when facing an opponent whose lookahead is limited is studied along multiple axes : lookahead depth	purpose	2K_dev_67
whether the opponent ( s )	purpose	2K_dev_67
too have imperfect information	purpose	2K_dev_67
and how they break ties for computing optimal commitment strategies	purpose	2K_dev_67
We present an efficient algorithm When combined with a result of Chillingworth	mechanism	2K_dev_68
our algorithm is applicable to convex simplicial complexes embedded in R3 The running time of our algorithm is nearly-linear in the size of the complex and is logarithmic on its numerical properties	mechanism	2K_dev_68
Our algorithm is based on projection operators and combinatorial steps for transferring between them The former relies on decomposing flows into circulations and potential flows using fast solvers for graph Laplacians	mechanism	2K_dev_68
and the latter relates Gaussian elimination to topological properties of simplicial complexes	mechanism	2K_dev_68
for solving a linear system arising from the 1-Laplacian corresponding to a collapsible simplicial complex with a known collapsing sequence	purpose	2K_dev_68
Specifically we show that each protocol in the class of generalized cut and choose ( GCC ) protocols which includes the most important discrete cake cutting protocols is guaranteed to have approximate subgame perfect Nash equilibria	finding	2K_dev_69
or even exact equilibria if the protocol 's tie-breaking rule is flexible	finding	2K_dev_69
We further observe that the ( approximate ) equilibria of proportional protocols which guarantee each of the n agents a 1/n-fraction of the cake must be ( approximately ) proportional	finding	2K_dev_69
thereby answering the above question in the positive ( at least for one common notion of fairness )	finding	2K_dev_69
we adopt a novel algorithmic approach	mechanism	2K_dev_69
proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model	mechanism	2K_dev_69
We study the paradigmatic fair division problem of fairly allocating a divisible good among agents with heterogeneous preferences	method	2K_dev_69
commonly known as cake cutting	method	2K_dev_69
Classic cake cutting protocols are susceptible to manipulation	purpose	2K_dev_69
Do their strategic outcomes still guarantee fairness ? To address this question	purpose	2K_dev_69
Com2 spots intuitive patterns	finding	2K_dev_73
that is temporal communities ( comet communities )	finding	2K_dev_73
We report our findings	finding	2K_dev_73
which include large star-like patterns	finding	2K_dev_73
near-bipartite-cores as well as tiny groups ( 5 users )	finding	2K_dev_73
calling each other hundreds of times within a few days	finding	2K_dev_73
We propose Com2 a novel and fast	mechanism	2K_dev_73
incremental tensor analysis approach	mechanism	2K_dev_73
The method is ( a ) scalable	mechanism	2K_dev_73
being linear on the input size ( b ) general	mechanism	2K_dev_73
( c ) needs no user-defined parameters and ( d ) effective	mechanism	2K_dev_73
returning results that agree with intuition	mechanism	2K_dev_73
We apply our method on real datasets	method	2K_dev_73
including a phone-call network and a computer-traffic network The phone call network consists of 4 million mobile users	method	2K_dev_73
with 51 million edges ( phonecalls )	method	2K_dev_73
Abstract : Given a large network	purpose	2K_dev_73
changing over time how can we find patterns and anomalies ? which can discover both transient and periodic/ repeating communities	purpose	2K_dev_73
A key challenge in solving extensive-form games is dealing with large	background	2K_dev_77
or even infinite action spaces	background	2K_dev_77
In games of imperfect information	background	2K_dev_77
the leading approach is to find a Nash equilibrium in a smaller abstract version of the game that includes only a few actions at each decision point	background	2K_dev_77
and then map the solution back to the original game	background	2K_dev_77
show it can outperform fixed abstractions at every stage of the run : early on it improves as quickly as equilibrium finding in coarse abstractions	finding	2K_dev_77
and later it converges to a better solution than does equilibrium finding in fine-grained abstractions	finding	2K_dev_77
We introduce a method that combines abstraction with equilibrium finding by enabling actions to be added to the abstraction at run time	mechanism	2K_dev_77
This allows an agent to begin learning with a coarse abstraction	mechanism	2K_dev_77
and then to strategically insert actions at points that the strategy computed in the current abstraction deems important The algorithm can quickly add actions to the abstraction while provably not having to restart the equilibrium finding	mechanism	2K_dev_77
It enables anytime convergence to a Nash equilibrium of the full game even in infinite games	mechanism	2K_dev_77
However it is difficult to know which actions should be included in the abstraction without first solving the game	purpose	2K_dev_77
and it is infeasible to solve the game without first abstracting it	purpose	2K_dev_77
Combined our results confer strong correctness guarantees for communicating systems	finding	2K_dev_88
To this end we develop a logically motivated theory of parametric polymorphism	mechanism	2K_dev_88
reminiscent of the Girard-Reynolds polymorphic -calculus	mechanism	2K_dev_88
but casted in the setting of concurrent processes	mechanism	2K_dev_88
In our theory polymorphism accounts for the exchange of abstract communication protocols and dynamic instantiation of heterogeneous interfaces	mechanism	2K_dev_88
as opposed to the exchange of data types and dynamic instantiation of individual message types	mechanism	2K_dev_88
Our polymorphic session-typed process language satisfies strong forms of type preservation and global progress	mechanism	2K_dev_88
is strongly normalizing and enjoys a relational parametricity principle In particular	mechanism	2K_dev_88
parametricity is key to derive non-trivial results about internal protocol independence	mechanism	2K_dev_88
a concurrent analogous of representation independence	mechanism	2K_dev_88
and non-interference properties of modular	mechanism	2K_dev_88
We investigate a notion of behavioral genericity in the context of session type disciplines	purpose	2K_dev_88
A dataset has been classified by some unknown classifier into two types of points	background	2K_dev_89
What were the most important factors in determining the classification outcome ?	background	2K_dev_89
In this work we employ an axiomatic approach We show that our influence measure takes on an intuitive form when the unknown classifier is linear	mechanism	2K_dev_89
Finally we employ our influence measure in order to analyze the effects of user profiling on Google 's online display advertising	method	2K_dev_89
in order to uniquely characterize an influence measure : a function that	purpose	2K_dev_89
given a set of classified points	purpose	2K_dev_89
outputs a value for each feature corresponding to its influence in determining the classification outcome	purpose	2K_dev_89
Given information about medical drugs and their properties	background	2K_dev_94
how can we automatically discover that Aspirin has blood-thinning properties	background	2K_dev_94
and thus prevents Expressed in more general terms	background	2K_dev_94
if we have a large in- formation network that integrates data from heterogeneous data sources	background	2K_dev_94
how can we extract semantic information that provides a better understanding of the in- tegrated data and also helps us to identify missing links ?	background	2K_dev_94
We demonstrate the effectiveness and scalability of the proposed method	finding	2K_dev_94
The discovered concepts provide semantic information as well as an abstract view on the integrated data and thus improve the understanding of complex systems	mechanism	2K_dev_94
Our proposed method has the following desirable properties : ( a ) it is parameter-free and therefore requires no user-defined parameters ( b ) it is fault-tolerant	mechanism	2K_dev_94
allowing for the detection of missing links and ( c ) it is scalable	mechanism	2K_dev_94
being linear on the input size	mechanism	2K_dev_94
on real publicly available graphs	method	2K_dev_94
We propose to extract concepts that describe groups of objects and their common properties from the integrated data	purpose	2K_dev_94
Multilinear analysis is pervasive in a wide variety of fields	background	2K_dev_95
ranging from Signal Processing to Chemometrics	background	2K_dev_95
and from Machine Vision to Data Mining	background	2K_dev_95
Determining the quality of a given tensor decomposition is a task of utmost importance that spans all fields of application of tensors	background	2K_dev_95
This task by itself is hard in its nature	background	2K_dev_95
since even determining the rank of a tensor is an NP-hard problem	background	2K_dev_95
Fortunately there exist heuristics in the literature that can be effectively used for this task ; one of these heuristics is the so-called Core Consistency Diagnostic ( CORCONDIA ) which is very intuitive and simple	background	2K_dev_95
However simple computation of this diagnostic proves to be a very daunting task even for data of medium scale	background	2K_dev_95
let alone big tensor data	background	2K_dev_95
In this work we derive a fast and exact algorithm for CORCONDIA which exploits data sparsity and scales very well as the tensor size increases	mechanism	2K_dev_95
With the increase of the size of the tensor data that need to be analyzed there grows the need for efficient and scalable algorithms to compute diagnostics such as CORCONDIA	purpose	2K_dev_95
in order to assess the modelling quality	purpose	2K_dev_95
In prior research we have developed a Curry-Howard interpretation of linear sequent calculus as session-typed processes	background	2K_dev_99
via a linear contextual monad that isolates session-based concurrency	mechanism	2K_dev_99
Monadic values are open process expressions and are first class objects in the language	mechanism	2K_dev_99
thus providing a logical foundation for higher-order session typed processes We illustrate how the combined use of the monad and recursive types allows us to cleanly write a rich variety of concurrent programs	mechanism	2K_dev_99
including higher-order programs that communicate processes We show the standard metatheoretic result of type preservation	mechanism	2K_dev_99
as well as a global progress theorem	mechanism	2K_dev_99
which to the best of our knowledge	mechanism	2K_dev_99
is new in the higher-order session typed setting	mechanism	2K_dev_99
In this paper we uniformly integrate this computational interpretation in a functional language	purpose	2K_dev_99
shows DOT2DOT correctly groups nodes for which good connection paths can be constructed	finding	2K_dev_101
while separating distant nodes	finding	2K_dev_101
in terms of the Minimum Description Length principle : a set of paths is simple when we need few bits to describe each path from one node to another	mechanism	2K_dev_101
For example we want to avoid high-degree nodes	mechanism	2K_dev_101
unless we need to visit many of its spokes	mechanism	2K_dev_101
As such the best partitioning requires the least number of bits to describe the paths that visit all marked nodes	mechanism	2K_dev_101
We show that our formulation for finding simple paths between groups of nodes has connections to well-known other problems in graph theory	mechanism	2K_dev_101
We propose fast effective solutions	mechanism	2K_dev_101
and introduce DOT2DOT an efficient algorithm	mechanism	2K_dev_101
Abstract : Suppose we are given a large graph in which	purpose	2K_dev_101
by some external process	purpose	2K_dev_101
a handful of nodes are marked	purpose	2K_dev_101
What can we say about these marked nodes ? Are they all close-by in the graph	purpose	2K_dev_101
or are they segregated into multiple groups ? How can we automatically determine how many	purpose	2K_dev_101
if any groups they form as well as find simple paths that connect the nodes in each group ? We formalize the problem for partitioning marked nodes as well as finding simple paths between nodes within parts	purpose	2K_dev_101
Summary form only given	background	2K_dev_105
We show that fractals and self-similarity can explain several of the observed patterns	finding	2K_dev_105
and we conclude and a surprising result on virus propagation and immunization	finding	2K_dev_105
We present a long list of static and temporal laws	mechanism	2K_dev_105
some recent observations on real graphs	method	2K_dev_105
What do graphs look like ? How do they evolve over time ? How does influence/news/viruses propagate	purpose	2K_dev_105
we demonstrate the appeal of this approach on synthetic examples and real power networks significantly larger than those previously considered in the literature	finding	2K_dev_107
Our focus is on an improved algorithm with convergence times that are several orders of magnitude faster than existing algorithms	mechanism	2K_dev_107
In particular we develop an efficient proximal Newton method which minimizes per-iteration cost with a coordinate descent active set approach and fast numerical solutions to the Lyapunov equations	mechanism	2K_dev_107
We consider the task of designing sparse control laws for large-scale systems by directly minimizing an infinite horizon quadratic cost with an $ \ell_1 $ penalty on the feedback controller gains that allows us to scale to large systems ( i	purpose	2K_dev_107
those where sparsity is most useful )	purpose	2K_dev_107
How can we correlate neural activity in the human brain as it responds to words	background	2K_dev_117
with behavioral data expressed as answers to questions about these same words ?	background	2K_dev_117
We show that this is an instance of the Coupled Matrix-Tensor Factorization ( CMTF ) problem	finding	2K_dev_117
we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm for CMTF	finding	2K_dev_117
along with a 5 fold increase in sparsity Scoup-SMT is able to find meaningful latent variables	finding	2K_dev_117
as well as to predict brain activity with competitive accuracy Finally	finding	2K_dev_117
we demonstrate the generality of Scoup-SMT there	finding	2K_dev_117
Scoup-SMT spots spammer-like anomalies	finding	2K_dev_117
We propose Scoup-SMT a novel	mechanism	2K_dev_117
fast and parallel algorithm and produces a sparse latent low-rank subspace of the data Moreover	mechanism	2K_dev_117
we extend Scoup-SMT to handle missing data without degradation of performance	mechanism	2K_dev_117
In our experiments We apply Scoup-SMT to BrainQ	method	2K_dev_117
a dataset consisting of a ( nouns	method	2K_dev_117
brain voxels human subjects ) tensor and a ( nouns	method	2K_dev_117
properties ) matrix with coupling along the nouns dimension by applying it on a Facebook dataset ( users	method	2K_dev_117
friends wall-postings ) ;	method	2K_dev_117
In short we want to find latent variables	purpose	2K_dev_117
that explain both the brain activity	purpose	2K_dev_117
as well as the behavioral responses	purpose	2K_dev_117
that solves the CMTF problem	purpose	2K_dev_117
How can we find useful patterns and anomalies in large scale real-world data with multiple attributes ? For example	background	2K_dev_119
network intrusion logs with ( source-ip	background	2K_dev_119
target-ip port-number timestamp ) ? Tensors are suitable for modeling these multi-dimensional data	background	2K_dev_119
and widely used for the analysis of social networks	background	2K_dev_119
web data network traffic	background	2K_dev_119
and in many other settings	background	2K_dev_119
and discover hidden concepts	finding	2K_dev_119
In this paper we propose HaTen2	mechanism	2K_dev_119
a scalable distributed suite of tensor decomposition algorithms running on the MapReduce platform By carefully reordering the operations	mechanism	2K_dev_119
and exploiting the sparsity of real world tensors	mechanism	2K_dev_119
HaTen2 dramatically reduces the intermediate data	mechanism	2K_dev_119
and the number of jobs	mechanism	2K_dev_119
As a result using HaTen2	mechanism	2K_dev_119
we analyze big real-world tensors that can not be handled by the current state of the art	method	2K_dev_119
However current tensor decomposition methods do not scale for tensors with millions and billions of rows	purpose	2K_dev_119
columns and fibers that often appear in real datasets	purpose	2K_dev_119
Both SAT and # SAT can represent difficult problems in seemingly dissimilar areas such as planning	background	2K_dev_122
verification and probabilistic inference	background	2K_dev_122
# SAT problems require counting the number of satisfiable formulas in a concisely-describable set of existentially-quantified	background	2K_dev_122
Our experiments show that	finding	2K_dev_122
despite the formidable worst-case complexity of # PNP [ 1 ]	finding	2K_dev_122
many of the instances can be solved efficiently by noticing and exploiting a particular type of frequent structure	finding	2K_dev_122
We characterize the expressiveness and worst-case difficulty of # SAT by proving it is complete for the complexity class # PNP [ 1 ]	mechanism	2K_dev_122
and relating this class to more familiar complexity classes	mechanism	2K_dev_122
We also experiment with three new general-purpose # SAT solvers on a battery of problem distributions including a simple logistics domain	method	2K_dev_122
Here we examine an expressive new language	purpose	2K_dev_122
# SAT that generalizes both of these languages	purpose	2K_dev_122
Classic cake cutting protocols -- which fairly allocate a divisible good among agents with heterogeneous preferences -- are susceptible to manipulation	background	2K_dev_123
GCC protocols are guaranteed to have exact subgame perfect Nash equilibria	finding	2K_dev_123
we adopt a novel algorithmic approach	mechanism	2K_dev_123
proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model Specifically	mechanism	2K_dev_123
we show that each protocol in the class of generalized cut and choose ( GCC ) protocols -- which includes the most important discrete cake cutting protocols -- is guaranteed to have approximate subgame perfect Nash equilibria Moreover	mechanism	2K_dev_123
we observe that the ( approximate ) equilibria of proportional protocols -- which guarantee each of the n agents a 1/n-fraction of the cake -- must be ( approximately ) proportional	mechanism	2K_dev_123
and design a GCC protocol where all Nash equilibrium outcomes satisfy the stronger fairness notion of envy-freeness	mechanism	2K_dev_123
Finally we show that under an obliviousness restriction	method	2K_dev_123
which still allows the computation of approximately envy-free allocations	method	2K_dev_123
Do their strategic outcomes still guarantee fairness ? To answer this question	purpose	2K_dev_123
The Bayesian paradigm has provided a useful conceptual theory for understanding perceptual computation in the brain	background	2K_dev_125
While the detailed neural mechanisms of Bayesian inference are not fully understood	background	2K_dev_125
recent computational and neurophysiological works have illuminated the underlying computational principles and representational architecture	background	2K_dev_125
The fundamental insights are that the visual system is organized as a modular hierarchy to encode an internal model of the world	background	2K_dev_125
and that perception is realized by statistical inference based on such internal model	background	2K_dev_125
We will argue for a unified theoretical framework	mechanism	2K_dev_125
In this paper we will discuss and analyze the varieties of representational schemes of these internal models and how they might be used to perform learning and inference for relating the internal models to the observed neural phenomena and mechanisms in the visual cortex	purpose	2K_dev_125
Differential game logic ( dG L ) is a logic for specifying and verifying properties of hybrid games	background	2K_dev_130
games that combine discrete	background	2K_dev_130
continuous and adversarial dynamics	background	2K_dev_130
Unlike hybrid systems hybrid games allow choices in the system dynamics to be resolved adversarially by different players with different objectives	background	2K_dev_130
Finally dG L is proved to be strictly more expressive than the corresponding logic of hybrid systems	finding	2K_dev_130
The logic dG L can be used i	mechanism	2K_dev_130
ways of resolving the players choices in some way so that he wins by achieving his objective for all choices of the opponent	mechanism	2K_dev_130
Hybrid games are determined	mechanism	2K_dev_130
from each state one player has a winning strategy	mechanism	2K_dev_130
yet computing their winning regions may take transfinitely many steps	mechanism	2K_dev_130
The logic dG L	mechanism	2K_dev_130
nevertheless has a sound and complete axiomatization relative to any expressive logic	mechanism	2K_dev_130
Separating axioms are identified that distinguish hybrid games from hybrid systems	mechanism	2K_dev_130
by characterizing the expressiveness of both	method	2K_dev_130
to study the existence of winning strategies for such hybrid games	purpose	2K_dev_130
Modern organizations ( e	background	2K_dev_137
hospitals social networks government agencies ) rely heavily on audit to detect and punish insiders who inappropriately access and disclose confidential information	background	2K_dev_137
that this transformation significantly speeds up computation of solutions for a class of audit games and security games	finding	2K_dev_137
We significantly generalize this audit game model where each resource is restricted to audit a subset of all potential violations	mechanism	2K_dev_137
thus We provide an FPTAS that computes an approximately optimal solution to the resulting non-convex optimization problem The main technical novelty is in the design and correctness proof of an optimization transformation that enables the construction of this FPTAS	mechanism	2K_dev_137
In addition we experimentally demonstrate	method	2K_dev_137
Recent work on audit games models the strategic interaction between an auditor with a single audit resource and auditees as a Stackelberg game	purpose	2K_dev_137
augmenting associated well-studied security games with a configurable punishment parameter	purpose	2K_dev_137
to account for multiple audit resources enabling application to practical auditing scenarios	purpose	2K_dev_137
A variety of problems in computing	background	2K_dev_142
service and manufacturing systems can be modeled via infinite repeating Markov chains with an infinite number of levels and a finite number of phases	background	2K_dev_142
We present a procedure	mechanism	2K_dev_142
which we call Clearing Analysis on Phases ( CAP ) The CAP method yields the limiting probability of each state in the repeating portion of the chain as a linear combination of scalar bases raised to a power corresponding to the level of the state	mechanism	2K_dev_142
The weights in these linear combinations can be determined by solving a finite system of linear equations	mechanism	2K_dev_142
Many such chains are quasi-birth-death processes with transitions that are skip-free in level	purpose	2K_dev_142
in that one can only transition between consecutive levels	purpose	2K_dev_142
and unidirectional in phase	purpose	2K_dev_142
in that one can only transition from lower-numbered phases to higher-numbered phases for determining the limiting probabilities of such Markov chains exactly	purpose	2K_dev_142
Any strong Nash equilibrium outcome is Pareto efficient for each coalition	background	2K_dev_143
Our main result in its simplest form	finding	2K_dev_143
states that if a game has a strong Nash equilibrium with full support ( that is	finding	2K_dev_143
both players randomize among all pure strategies )	finding	2K_dev_143
then the game is strictly competitive	finding	2K_dev_143
we use the indifference principle fulfilled by any Nash equilibrium	mechanism	2K_dev_143
and the classical KKT conditions ( in the vector setting )	mechanism	2K_dev_143
that are necessary conditions for Pareto efficiency	mechanism	2K_dev_143
Our characterization enables us to design a strong-Nash-equilibrium-finding algorithm with complexity in Smoothed- $ \mathcal { P } $	mechanism	2K_dev_143
So this problem -- -that Conitzer and Sandholm [ Conitzer	mechanism	2K_dev_143
New complexity results about Nash equilibria	mechanism	2K_dev_143
63 621 -- 641 ] proved to be computationally hard in the worst case -- -is generically easy	mechanism	2K_dev_143
Hence although the worst case complexity of finding a strong Nash equilibrium is harder than that of finding a Nash equilibrium	mechanism	2K_dev_143
once small perturbations are applied	mechanism	2K_dev_143
finding a strong Nash is easier than finding a Nash equilibrium Next we switch to the setting with more than two players	mechanism	2K_dev_143
We demonstrate that a strong Nash equilibrium can exist in which an outcome that is strictly Pareto dominated by a Nash equilibrium occurs with positive probability Finally	mechanism	2K_dev_143
we prove that games that have a strong Nash equilibrium where at least one player puts positive probability on at least two pure strategies are extremely rare : they are of zero measure	mechanism	2K_dev_143
First we analyze the two -- player setting	method	2K_dev_143
In this paper we consider strong Nash equilibria	purpose	2K_dev_143
in mixed strategies for finite games	purpose	2K_dev_143
In order to get our result	purpose	2K_dev_143
which may be of independent interest	background	2K_dev_149
We present faster algorithms such as bounded genus	mechanism	2K_dev_149
minor free and geometric graphs Given such a graph with n vertices	mechanism	2K_dev_149
m edges along with a recursive n-vertex separator structure	mechanism	2K_dev_149
our algorithm finds an 1 -- e approximate maximum flow in time O ( m6/5poly ( e -- 1 ) )	mechanism	2K_dev_149
ignoring poly-logarithmic terms Similar speedups are also achieved for separable graphs with larger size separators albeit with larger run times These bounds also apply to image problems in two and three dimensions	mechanism	2K_dev_149
Key to our algorithm is an intermediate problem that we term grouped L2 flow	mechanism	2K_dev_149
which exists between maximum flows and electrical flows	mechanism	2K_dev_149
Our algorithm also makes use of spectral vertex sparsifiers in order to remove vertices while preserving the energy dissipation of electrical flows We also give faster spectral vertex sparsification algorithms on well separated graphs	mechanism	2K_dev_149
for approximate maximum flow in undirected graphs with good separator structures	purpose	2K_dev_149
The cake cutting problem models the fair division of a heterogeneous good between multiple agents	background	2K_dev_150
Previous work assumes that each agent derives value only from its own piece	background	2K_dev_150
We extend the classical model Our technical results characterize the relationship between these generalized properties	mechanism	2K_dev_150
establish the existence or nonexistence of fair allocations	mechanism	2K_dev_150
and explore the computational feasibility of fairness in the face of externalities	mechanism	2K_dev_150
However agents may also care about the pieces assigned to other agents ; such externalities naturally arise in fair division settings	purpose	2K_dev_150
to capture externalities and generalize the classical fairness notions of proportionality and envyfreeness	purpose	2K_dev_150
Our first contribution is the discovery that the relative frequencies obey a power-law ( sub-linear	finding	2K_dev_151
or super-linear ) for a wide variety of diverse settings : tasks in a phone- call network	finding	2K_dev_151
like count of friends	finding	2K_dev_151
count of phone-calls total count of minutes ; tasks in a twitter-like network	finding	2K_dev_151
like count of tweets	finding	2K_dev_151
count of followees etc	finding	2K_dev_151
We show how to use our observations to spot clusters and outliers	finding	2K_dev_151
telemarketers in our phone-call network	finding	2K_dev_151
The second contribution is that we further provide a full	mechanism	2K_dev_151
digitized 2-d distribution which we call the Almond-DG model	mechanism	2K_dev_151
thanks to the shape of its iso-surfaces	mechanism	2K_dev_151
The Almond-DG model matches all our empirical observations : super-linear relationships among variables	mechanism	2K_dev_151
and ( provably ) log-logistic marginals	mechanism	2K_dev_151
We illustrate our observations on two large	method	2K_dev_151
real network datasets spanning 2	method	2K_dev_151
1M individu- als with 5 features each	method	2K_dev_151
If Alice has double the friends of Bob	purpose	2K_dev_151
will she also have dou- ble the phone-calls ( or wall-postings	purpose	2K_dev_151
or tweets ) ?	purpose	2K_dev_151
Kidney exchange provides a life-saving alternative to long waiting lists for patients in need of a new kidney	background	2K_dev_153
Fielded exchanges typically match under utilitarian or near-utilitarian rules ; this approach marginalizes certain classes of patients	background	2K_dev_153
we formally adapt a recently introduced measure of the tradeoff between fairness and efficiency -- -the price of fairness -- -to the standard kidney exchange model	mechanism	2K_dev_153
We show that the price of fairness in the standard theoretical model is small	mechanism	2K_dev_153
We then introduce two natural definitions of fairness	mechanism	2K_dev_153
and empirically explore the tradeoff between matching more hard-to-match patients and the overall utility of a utilitarian matching	method	2K_dev_153
on real data from the UNOS nationwide kidney exchange and simulated data from each of the standard kidney exchange distributions	method	2K_dev_153
In this paper we focus on improving access to kidneys for highly-sensitized	purpose	2K_dev_153
or hard-to-match patients Toward this end	purpose	2K_dev_153
An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer	background	2K_dev_160
For these schemes we prove that forging passwords is equivalent to recovering the secret mapping Thus	finding	2K_dev_160
our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts	finding	2K_dev_160
We propose several candidate authentication protocols -- - a computer that stores information and performs computations correctly but does not provide confidentiality	mechanism	2K_dev_160
Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $	mechanism	2K_dev_160
The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate	mechanism	2K_dev_160
We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $	mechanism	2K_dev_160
for a security parameter $ s ( f ) $ that depends on two key properties To obtain our results	mechanism	2K_dev_160
we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $	mechanism	2K_dev_160
Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate )	mechanism	2K_dev_160
and generalize recent results of Feldman et al	mechanism	2K_dev_160
for a setting in which the human user can only receive assistance from a semi-trusted computer	purpose	2K_dev_160
To achieve maximum security	background	2K_dev_161
defenders must perfectly synchronize their randomized allocations of resources	background	2K_dev_161
indicate that the loss may be extremely high in the worst case	finding	2K_dev_161
establish a smaller yet significant loss in practice	finding	2K_dev_161
We introduce two notions under different assumptions : the price of miscoordination	mechanism	2K_dev_161
and the price of sequential commitment	mechanism	2K_dev_161
Generally speaking our theoretical bounds while our simulations	method	2K_dev_161
We study security games with multiple defenders However	purpose	2K_dev_161
in real-life scenarios ( such as protection of the port of Boston ) this is not the case	purpose	2K_dev_161
Our goal is to quantify the loss incurred by miscoordination between defenders	purpose	2K_dev_161
both theoretically and empirically	purpose	2K_dev_161
that capture this loss	purpose	2K_dev_161
The proliferation of mobile devices that are capable of estimating their position	background	2K_dev_163
has lead to the emergence of a new class of social networks	background	2K_dev_163
namely location-based social networks ( LBSNs for short )	background	2K_dev_163
The main interaction between users in an LBSN is location sharing	background	2K_dev_163
To the best of our knowledge	background	2K_dev_163
this is the first attempt to model this problem using tensor analysis	background	2K_dev_163
In this work we propose the use of tensor decomposition	mechanism	2K_dev_163
While the latter can be realized through continuous tracking of a user 's whereabouts from the service provider	purpose	2K_dev_163
the majority of LBSNs allow users to voluntarily share their location	purpose	2K_dev_163
LBSNs provide incentives to users to perform check-ins	purpose	2K_dev_163
However these incentives can also lead to people faking their location	purpose	2K_dev_163
thus generating false information	purpose	2K_dev_163
for spotting anomalies in the check-in behavior of users	purpose	2K_dev_163
The computational characterization of game-theoretic solution concepts is a central topic in artificial intelligence	background	2K_dev_164
with the aim of developing computationally efficient tools for finding optimal ways to behave in strategic interactions	background	2K_dev_164
The central solution concept in game theory is Nash equilibrium ( NE )	background	2K_dev_164
However it fails to capture the possibility that agents can form coalitions ( even in the 2-agent case )	background	2K_dev_164
Strong Nash equilibrium ( SNE ) refines NE to this setting	background	2K_dev_164
It is known that finding an SNE is NP-complete when the number of agents is constant	background	2K_dev_164
This hardness is solely due to the existence of mixed-strategy SNEs	background	2K_dev_164
given that the problem of enumerating all pure-strategy SNEs is trivially in P	background	2K_dev_164
First we develop worst-case instances for support-enumeration algorithms	mechanism	2K_dev_164
These instances have only one SNE and the support size can be chosen to be of any size-in particular	mechanism	2K_dev_164
Second we prove that	mechanism	2K_dev_164
unlike NE finding an SNE is in smoothed polynomial time : generic game instances ( i	mechanism	2K_dev_164
all instances except knife-edge cases ) have only pure-strategy SNEs	mechanism	2K_dev_164
Our central result is that	purpose	2K_dev_164
in order for a game to have at least one non-pure-strategy SNE	purpose	2K_dev_164
the agents ' payoffs restricted to the agents ' supports must	purpose	2K_dev_164
in the case of 2 agents	purpose	2K_dev_164
lie on the same line	purpose	2K_dev_164
and in the case of n agents	purpose	2K_dev_164
lie on an ( n - 1 ) -dimensional hyperplane	purpose	2K_dev_164
Leveraging this result we provide two contributions	purpose	2K_dev_164
If we know most of Smith 's friends are from Boston	background	2K_dev_165
what can we say about the rest of Smith 's friends ? which is one of the most important topics in AI and Web communities	background	2K_dev_165
We also prove the theoretical connections of our algorithm to the semi-supervised learning ( SSL ) algorithms and to random-walks demonstrate the benefits of the proposed algorithm	finding	2K_dev_165
where OMNI-Prop outperforms the top competitors	finding	2K_dev_165
Our proposed algorithm which is referred to as OMNI-Prop has the following properties : ( a ) seamless and accurate ; it works well on any label correlations ( i	mechanism	2K_dev_165
homophily het-erophily and mixture of them ) ( b ) fast ; it is efficient and guaranteed to converge on arbitrary graphs ( c ) quasi-parameter free ; it has just one well-interpretable parameter with heuristic default value of 1	mechanism	2K_dev_165
Experiments on four real	method	2K_dev_165
In this paper we focus on the node classification problem on networks	purpose	2K_dev_165
It is unsolved even for two bidders and two items for sale	background	2K_dev_170
show that our algorithms create mechanisms that yield significantly higher revenue than the VCG and scale dramatically better than prior automated mechanism design algorithms The algorithms yielded deterministic mechanisms with the highest known revenues for the settings tested	finding	2K_dev_170
including the canonical setting with two bidders	finding	2K_dev_170
two items and uniform additive valuations	finding	2K_dev_170
Rather than pursuing the manual approach of attempting to characterize the optimal CA	mechanism	2K_dev_170
we introduce a family of CAs and then seek a high-revenue auction within that family	mechanism	2K_dev_170
The family is based on bidder weighting and allocation boosting ; we coin such CAs virtual valuations combinatorial auctions ( VVCAs ) VVCAs are the Vickrey-Clarke-Groves ( VCG ) mechanism executed on virtual valuations that are affine transformations of the bidders valuations	mechanism	2K_dev_170
The auction family is parameterized by the coefficients in the transformations The problem of designing a CA is thereby reduced to search in the parameter space of VVCAor the more general space of affine maximizer auctions	mechanism	2K_dev_170
We first construct VVCAs with logarithmic approximation guarantees in canonical special settings : ( 1 ) limited supply with additive valuations and ( 2 ) unlimited supply	mechanism	2K_dev_170
In the main part of the paper	mechanism	2K_dev_170
we develop algorithms that design high-revenue CAs for general valuations using samples from the prior distribution over bidders valuations	mechanism	2K_dev_170
( Priors turn out to be necessary for achieving high revenue	mechanism	2K_dev_170
) We prove properties of the problem that guide our design of algorithms	mechanism	2K_dev_170
We then introduce a series of algorithms that use economic insights to guide the search and thus reduce the computational complexity	mechanism	2K_dev_170
Designing optimalthat is revenue-maximizingcombinatorial auctions ( CAs ) is an important elusive problem	purpose	2K_dev_170
How many listens will an artist receive on a online radio ? How about plays on a YouTube video ? How many of these visits are new or returning users ? Modeling and mining popularity dynamics of social activity has important implications for researchers	background	2K_dev_172
content creators and providers	background	2K_dev_172
we show the effect of revisits in the popularity evolution of such objects	finding	2K_dev_172
Secondly we propose the Phoenix-R model Phoenix-R has the desired properties of being : ( 1 ) parsimonious	mechanism	2K_dev_172
being based on the minimum description length principle	mechanism	2K_dev_172
and achieving lower root mean squared error than state-of-the-art baselines ; ( 2 ) applicable	mechanism	2K_dev_172
the model is effective for predicting future popularity values of objects	mechanism	2K_dev_172
Using four datasets of social activity	method	2K_dev_172
with up to tens of millions media objects ( e	method	2K_dev_172
YouTube videos Twitter hashtags or LastFM artists )	method	2K_dev_172
We here investigate the effect of revisits ( successive visits from a single user ) on content popularity which captures the popularity dynamics of individual objects	purpose	2K_dev_172
Given a large number of taxi trajectories	background	2K_dev_177
we would like to find interesting and unexpected patterns from the data	background	2K_dev_177
How can we summarize the major trends	background	2K_dev_177
and how can we spot anomalies ? The anal- ysis of trajectories has been an issue of considerable interest with many applications such as tracking trails of migrating animals and predicting the path of hurricanes	background	2K_dev_177
In fact F-Trail does produce concise	finding	2K_dev_177
informative and interesting patterns	finding	2K_dev_177
we develop a novel method	mechanism	2K_dev_177
called F-Trail w hich al- lows us Our approach has the following advantages : ( a ) it is fast	mechanism	2K_dev_177
and scales linearly on the input size	mechanism	2K_dev_177
( b ) it is effective	mechanism	2K_dev_177
leading to novel discoveries	mechanism	2K_dev_177
We demonstrate the effectiveness of our approach	method	2K_dev_177
by performing exper- iments on real taxi trajectories	method	2K_dev_177
Several recent works propose methods on clus- tering and indexing trajectories data	purpose	2K_dev_177
However these approaches are not especially well suited to pattern discovery with respect to the dynamics of social and economic behavior To further analyze a huge collection of taxi trajectories	purpose	2K_dev_177
to find meaningful patterns and anomalies	purpose	2K_dev_177
Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in incomplete-information games	background	2K_dev_182
In this paper we show the first general	mechanism	2K_dev_182
algorithm-agnostic solution quality guarantees and approximate self-trembling equilibria computed in imperfect-recall abstractions	mechanism	2K_dev_182
when implemented in the original ( perfect-recall ) game	mechanism	2K_dev_182
Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions where any results had been obtained	mechanism	2K_dev_182
Further our analysis is tighter in two ways	mechanism	2K_dev_182
each of which can lead to an exponential reduction in the solution quality error bound We then show that for extensive-form games that satisfy certain properties	mechanism	2K_dev_182
the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem	mechanism	2K_dev_182
where the increase in our bound is the distance function This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds We proceed to show a divide in the class of abstraction problems If payoffs are at the same scale at all information sets considered for abstraction	mechanism	2K_dev_182
the input forms a metric space	mechanism	2K_dev_182
Conversely if this condition is not satisfied	mechanism	2K_dev_182
we show that the input does not form a metric space	mechanism	2K_dev_182
Finally we use these results to experimentally investigate the quality of our bound for single-level abstraction	method	2K_dev_182
However imperfect-recall abstractions are poorly understood	purpose	2K_dev_182
and only weak algorithm-specific guarantees on solution quality are known for Nash equilibria	purpose	2K_dev_182
Formal verification of industrial systems is very challenging	background	2K_dev_184
due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams	background	2K_dev_184
More importantly industrial systems are rarely designed for verification	background	2K_dev_184
but rather for operational needs The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team	background	2K_dev_184
In this paper we present an overview of our experience using hybrid systems theorem proving an airborne collision avoidance system for airliners scheduled to be operational around 2020 The methods and proof techniques presented here are an overview of the work already presented in [ 8 ]	mechanism	2K_dev_184
while the evaluation of ACAS X has been significantly expanded and updated to the most recent version of the system	mechanism	2K_dev_184
to formally verify ACAS X	purpose	2K_dev_184
How much did a network change since yesterday ? How different is the wiring between Bob 's brain ( a left-handed male ) and Alice 's brain ( a right-handed female ) ? Graph similarity with known node correspondence	background	2K_dev_187
the detection of changes in the connectivity of graphs	background	2K_dev_187
arises in numerous settings	background	2K_dev_187
showcase the advantages of our method over existing similarity measures	finding	2K_dev_187
We propose DeltaCon a principled	mechanism	2K_dev_187
intuitive and scalable algorithm ( e	mechanism	2K_dev_187
g employees of a company	mechanism	2K_dev_187
customers of a mobile carrier )	mechanism	2K_dev_187
Experiments on various synthetic and real graphs Finally	method	2K_dev_187
we employ DeltaCon to real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs	method	2K_dev_187
and ( b ) do temporal anomaly detection in the who-emails-whom Enron graph	method	2K_dev_187
In this work we formally state the axioms and desired properties of the graph similarity functions	purpose	2K_dev_187
and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs	purpose	2K_dev_187
that assesses the similarity between two graphs on the same nodes	purpose	2K_dev_187
Why does Smith follow Johnson on Twitter ?	background	2K_dev_188
results show that TagF uncovers different	finding	2K_dev_188
but explainable reasons why users follow other users	finding	2K_dev_188
by proposing TagF which analyzes the who-follows-whom network ( matrix ) and the who-tags-whom network ( tensor ) simultaneously Concretely	mechanism	2K_dev_188
our method decomposes a coupled tensor constructed from these matrix and tensor	mechanism	2K_dev_188
The experimental on million-scale Twitter networks	method	2K_dev_188
In most cases the reason why users follow other users is unavailable In this work	purpose	2K_dev_188
we answer this question	purpose	2K_dev_188
How often do individuals perform a given communication activity in the Web	background	2K_dev_190
such as posting comments on blogs or news ? Could we have a generative model to create communication events with realistic inter-event time distributions ( IEDs ) ? Which properties should we strive to match ?	background	2K_dev_190
reveal that the SFP mimics their properties very well	finding	2K_dev_190
being corner cases of the proposed Self-Feeding Process ( SFP )	mechanism	2K_dev_190
We show that the SFP ( a ) exhibits a unifying power	mechanism	2K_dev_190
which generates power law tails ( including the so-called `` top-concavity '' that real data exhibits )	mechanism	2K_dev_190
as well as short-term Poisson behavior ; ( b ) avoids the `` i	mechanism	2K_dev_190
d fallacy '' which none of the prevailing models have studied before ; and ( c ) is extremely parsimonious	mechanism	2K_dev_190
requiring usually only one	mechanism	2K_dev_190
and in general at most two parameters	mechanism	2K_dev_190
Experiments conducted on eight large	method	2K_dev_190
diverse real datasets ( e	method	2K_dev_190
Youtube and blog comments	method	2K_dev_190
e-mails SMSs etc )	method	2K_dev_190
Current literature has seemingly contradictory results for IED : some studies claim good fits with power laws ; others with non-homogeneous Poisson processes	purpose	2K_dev_190
Given these two approaches	purpose	2K_dev_190
we ask : which is the correct one ? Can we reconcile them all ? We show here that	purpose	2K_dev_190
surprisingly both approaches are correct	purpose	2K_dev_190
The difference is that hybrid games also provide all the features of hybrid systems and discrete games	background	2K_dev_192
but only deterministic differential equations	background	2K_dev_192
Differential games instead provide differential equations with continuous-time game input by both players	background	2K_dev_192
but not the luxury of hybrid games	background	2K_dev_192
such as mode switches and discrete-time or alternating adversarial interaction	background	2K_dev_192
This article introduces differential hybrid games	mechanism	2K_dev_192
which combine differential games with hybrid games	mechanism	2K_dev_192
In both kinds of games	mechanism	2K_dev_192
two players interact with continuous dynamics This article augments differential game logic with modalities and introduces differential game invariants and differential game variants	mechanism	2K_dev_192
for the combined dynamics of differential hybrid games It shows how hybrid games subsume differential games for proving properties of differential games inductively	purpose	2K_dev_192
Generative score spaces provide a principled method to exploit generative information	background	2K_dev_195
data distribution and hidden variables	background	2K_dev_195
The underlying methodology is to derive measures or score functions from generative models	background	2K_dev_195
The derived score functions	background	2K_dev_195
spanning the so-called score space	background	2K_dev_195
provide features of a fixed dimension for discriminative classification	background	2K_dev_195
shows that performance of the score space approach coupled with the proposed discriminative learning method is competitive with state-of-the-art classification methods	finding	2K_dev_195
In this paper we propose a simple yet effective score space We further propose a discriminative learning method by constraining the classification margin over the score space	mechanism	2K_dev_195
The form of score function allows the formulation of simple learning rules	mechanism	2K_dev_195
which are essentially the same learning rules for a generative model with an extra posterior imposed over its hidden variables	mechanism	2K_dev_195
Experimental evaluation of this approach over two generative models	method	2K_dev_195
which is essentially the sufficient statistics of the adopted generative models and does not involve the parameters of generative models	purpose	2K_dev_195
for the score space that seeks to utilize label information	purpose	2K_dev_195
This paper introduces a new proof calculus that is entirely based on uniform substitution	mechanism	2K_dev_200
a proof rule that substitutes a formula for a predicate symbol everywhere	mechanism	2K_dev_200
Uniform substitutions make it possible to rely on axioms rather than axiom schemata	mechanism	2K_dev_200
Instead of nontrivial schema variables and soundness-critical side conditions on the occurrence patterns of variables	mechanism	2K_dev_200
the resulting calculus adopts only a finite number of ordinary dL formulas as axioms	mechanism	2K_dev_200
The static semantics of differential dynamic logic is captured exclusively in uniform substitutions and bound variable renamings as opposed to being spread in delicate ways across the prover implementation In addition to sound uniform substitutions	mechanism	2K_dev_200
this paper introduces differential forms that make it possible	mechanism	2K_dev_200
for differential dynamic logic ( dL ) for differential dynamic logic to internalize differential invariants	purpose	2K_dev_200
differential substitutions and derivations as first-class axioms in dL	purpose	2K_dev_200
Hidden information derived from probabilistic generative models of data distributions can be used to construct features for discriminative classifiers	background	2K_dev_202
This observation has motivated the development of approaches that attempt to couple generative and discriminative models together for classification	background	2K_dev_202
this new framework produces a general classification tool with state-of-the-art performance	finding	2K_dev_202
In this paper we propose a coupling mechanism developed under the PAC-Bayes framework that can fine-tune the generative models and the feature mapping functions iteratively In our approach	mechanism	2K_dev_202
a stochastic feature mapping	mechanism	2K_dev_202
which is a function over the random variables of a generative model	mechanism	2K_dev_202
is derived to generate feature vectors for a stochastic classifier	mechanism	2K_dev_202
We construct a stochastic classifier over the feature mapping and derive the PAC-Bayes generalization bound for the classifier	mechanism	2K_dev_202
for both supervised and semi-supervised learning This allows us to jointly learn the feature mapping and the classifier by minimizing the bound with an EM-like iterative algorithm using labeled and unlabeled data	mechanism	2K_dev_202
The resulting framework integrates the learning of the discriminative classifier and the generative model and allows iterative fine-tuning of the generative models	mechanism	2K_dev_202
and the feedforward feature mappings based on task performance feedback	mechanism	2K_dev_202
Our experiments show in three distinct applications	method	2K_dev_202
However existing approaches typically feed features derived from generative models to discriminative classifiers	purpose	2K_dev_202
and do not refine the generative models or the feature mapping functions based on classification results	purpose	2K_dev_202
to improve the classifier 's performance	purpose	2K_dev_202
Community detection plays a key role in understanding the structure of real-life graphs with impact on recommendation systems	background	2K_dev_203
load balancing and routing	background	2K_dev_203
Previous community detection methods look for uniform blocks in adjacency matrices	background	2K_dev_203
we provide empirical evidence that communities are best represented as having an hyperbolic structure	finding	2K_dev_203
We show that our method is effective in finding communities with a similar structure to self-declared ones	finding	2K_dev_203
We detail HyCoM - the Hyperbolic Community Model - and show improvements in compression compared to standard methods We also introduce HyCoM-FIT	mechanism	2K_dev_203
a fast parameter free algorithm	mechanism	2K_dev_203
However after studying four real networks with ground-truth communities in real social networks	method	2K_dev_203
including a community in a blogging platform with over 34 million edges in which more than 1000 users established over 300 000 relations	method	2K_dev_203
What do real communities in social networks look like ? as a better representation of communities and the relationships between their members	purpose	2K_dev_203
to detect communities with hyperbolic structure	purpose	2K_dev_203
We show that our model can outperform state-of-art performances of gated Boltzmann machines ( GBM ) Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information We show it achieves state-of-art performances and possesses the ability to interpolate missing frames	finding	2K_dev_205
a function that is lacking in GBM	finding	2K_dev_205
We propose a new neurally-inspired model that can learn and by synthesis process in a predictive coding framework	mechanism	2K_dev_205
The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes	mechanism	2K_dev_205
In contrast to standard predictive coding models	mechanism	2K_dev_205
the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer	mechanism	2K_dev_205
and is thus more consistent with neurophysiological observations	mechanism	2K_dev_205
We establish the computational feasibility of this model by demonstrating its ability in several aspects	method	2K_dev_205
in estimation of contextual information	method	2K_dev_205
in terms of prediction accuracy in a variety of tasks	method	2K_dev_205
to encode the global relationship context of visual events across time and space to use the contextual information to modulate the analysis	purpose	2K_dev_205
We propose the use of contracts concisely capturing the conditions for a safe operation in the context of a traffic network This reduces the analysis of flows in the full traffic network to simple arithmetic checks of the local compatibility of the traffic component contracts	mechanism	2K_dev_206
while retaining higher-fidelity correctness guarantees of the global hybrid systems models that inherits from correct contracts of the hybrid system components	mechanism	2K_dev_206
We evaluate our approach in a case study of a modular traffic network and a prototypical implementation in a model-based analysis and design tool for traffic flow networks	method	2K_dev_206
We address the problem how high-fidelity verification results about the hybrid systems dynamics of cyber-physical flow systems can be provided at the scale of large ( traffic ) networks without prohibitive analytic cost	purpose	2K_dev_206
for traffic flow components	purpose	2K_dev_206
Background Computer-assisted diagnosis of dermoscopic images of skin lesions has the potential to improve melanoma early detection	background	2K_dev_207
Conclusions Our classifier may aid clinicians in deciding if a skin lesion should be biopsied and can easily be incorporated into a portable tool ( that uses no proprietary equipment ) that could aid clinicians in noninvasively evaluating cutaneous lesions	background	2K_dev_207
Results The classifier sensitivity for melanoma was 97	finding	2K_dev_207
4 % ; specificity was 44	finding	2K_dev_207
2 % in a test set of images	finding	2K_dev_207
In the reader study	finding	2K_dev_207
the classifier 's sensitivity to melanoma was higher ( P P Limitations This is a retrospective study using existing images primarily chosen for biopsy by a dermatologist	finding	2K_dev_207
The size of the test set is small	finding	2K_dev_207
We sought to evaluate the performance of a novel classifier that uses decision forest classification of dermoscopic images	mechanism	2K_dev_207
Methods Severity scores were calculated for 173 dermoscopic images of skin lesions with known histologic diagnosis ( 39 melanomas	method	2K_dev_207
14 nonmelanoma skin cancers	method	2K_dev_207
and 120 benign lesions )	method	2K_dev_207
A threshold score was used to measure classifier sensitivity and specificity A reader study was conducted to compare the sensitivity and specificity of the classifier with those of 30 dermatology clinicians	method	2K_dev_207
Objective to generate a lesion severity score	purpose	2K_dev_207
The use of deductive techniques	background	2K_dev_212
such as theorem provers	background	2K_dev_212
has several advantages in safety verification of hybrid systems ; however	background	2K_dev_212
state-of-the-art theorem provers require manual intervention to handle complex systems	background	2K_dev_212
This paper presents an extension to KeYmaera	mechanism	2K_dev_212
a deductive verification tool ; the new technique using system designer intuition about performance within particular modes as part of a proof task	mechanism	2K_dev_212
Our approach allows the theorem prover to leverage forward invariants	mechanism	2K_dev_212
discovered using numerical techniques	mechanism	2K_dev_212
as part of a proof of safety	mechanism	2K_dev_212
We introduce a new inference rule into the proof calculus of KeYmaera	mechanism	2K_dev_212
the forward invariant cut rule	mechanism	2K_dev_212
and we present a methodology	mechanism	2K_dev_212
which are then used with the new cut rule	mechanism	2K_dev_212
We demonstrate how our new approach can be used to complete verification tasks that lie out of the reach of existing automatic verification approaches using several examples	method	2K_dev_212
including one involving an automotive powertrain control system	method	2K_dev_212
Furthermore there is often a gap between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide directly for differential dynamic logic allows local reasoning to discover useful forward invariants to complete verification tasks	purpose	2K_dev_212
we discovered that duplicate points create subtle issues	finding	2K_dev_216
that the literature has ignored : if d max is the multiplicity of the most over-plotted point	finding	2K_dev_216
typical algorithms are quadratic on d max	finding	2K_dev_216
we report wall-clock times and our time savings ; and we show that our methods give either exact results	finding	2K_dev_216
or highly accurate approximate ones	finding	2K_dev_216
We propose several ways ;	mechanism	2K_dev_216
Given a large cloud of multi-dimensional points	purpose	2K_dev_216
and an off-the shelf outlier detection method	purpose	2K_dev_216
why does it take a week to finish ? to eliminate the problem	purpose	2K_dev_216
We propose using the statistical measurement of the sample skewness of the distribution of mean firing rates of a tuning curve For some features	mechanism	2K_dev_218
like binocular disparity tuning curves are best described by relatively complex and sometimes diverse functions	mechanism	2K_dev_218
making it difficult to quantify sharpness with a single function and parameter	mechanism	2K_dev_218
Skewness provides a robust nonparametric measure of tuning curve sharpness that is invariant with respect to the mean and variance of the tuning curve and is straightforward to apply to a wide range of tuning	mechanism	2K_dev_218
including simple orientation tuning curves and complex object tuning curves that often can not even be described parametrically	mechanism	2K_dev_218
Because skewness does not depend on a specific model or function of tuning	mechanism	2K_dev_218
it is especially appealing to cases of sharpening where recurrent interactions among neurons produce sharper tuning curves that deviate in a complex manner from the feedforward function of tuning	mechanism	2K_dev_218
Since tuning curves for all neurons are not typically well described by a single parametric function	mechanism	2K_dev_218
this model independence additionally allows skewness to be applied to all recorded neurons	mechanism	2K_dev_218
maximizing the statistical power of a set of data We also compare skewness with other nonparametric measures of tuning curve sharpness and selectivity Compared to these other nonparametric measures tested	mechanism	2K_dev_218
skewness is best used for capturing the sharpness of multimodal tuning curves defined by narrow peaks maximum and broad valleys minima Finally	mechanism	2K_dev_218
we provide a more formal definition of sharpness using a shape-based information gain measure and derive and show that skewness is correlated with this definition	mechanism	2K_dev_218
to quantify sharpness of tuning	purpose	2K_dev_218
How can we describe a large	background	2K_dev_219
dynamic graph over time ? Is it random ? If not	background	2K_dev_219
what are the most apparent deviations from randomness -- a dense block of actors that persists over time	background	2K_dev_219
or perhaps a star with many satellite nodes that appears with some fixed periodicity ? In practice	background	2K_dev_219
these deviations indicate patterns -- for example	background	2K_dev_219
botnet attackers forming a bipartite core with their victims over the duration of an attack	background	2K_dev_219
family members bonding in a clique-like fashion over a difficult period of time	background	2K_dev_219
or research collaborations forming and fading away over the years	background	2K_dev_219
We show that TIMECRUNCH is able to compress these graphs	finding	2K_dev_219
( a ) formulation : we show how to formalize this problem as minimizing the encoding cost in a data compression paradigm	mechanism	2K_dev_219
( b ) algorithm : we propose TIMECRUNCH	mechanism	2K_dev_219
an effective scalable and parameter-free method for finding coherent	mechanism	2K_dev_219
temporal patterns in dynamic graphs and ( c ) practicality : by summarizing important temporal structures and finds patterns that agree with intuition	mechanism	2K_dev_219
we apply our method to several large	method	2K_dev_219
diverse real-world datasets with up to 36 million edges and 6	method	2K_dev_219
Which patterns exist in real-world dynamic graphs	purpose	2K_dev_219
and how can we find and rank them in terms of importance ? These are exactly the problems we focus on in this work	purpose	2K_dev_219
Our main contributions are	purpose	2K_dev_219
Modern offices are crowded with personal computers	background	2K_dev_223
While studies have shown these to be idle most of the time	background	2K_dev_223
they remain powered consuming up to 60p of their peak power Hardware-based solutions engendered by PC vendors ( e	background	2K_dev_223
low-power states Wake-on-LAN ) have proved unsuccessful because	background	2K_dev_223
in spite of user inactivity	background	2K_dev_223
these machines often need to remain network active in support of background applications that maintain network presence	background	2K_dev_223
can deliver 44 -- 91p energy savings during idle periods of at least 10 minutes	finding	2K_dev_223
while providing low migration latencies of about 4 seconds and migrating minimal state that is under an order of magnitude of the VMs memory footprint	finding	2K_dev_223
We present partial VM migration	mechanism	2K_dev_223
an approach It creates a partial replica of the desktop VM on the consolidation server by copying only VM metadata	mechanism	2K_dev_223
and it transfers pages to the server on-demand	mechanism	2K_dev_223
as the VM accesses them	mechanism	2K_dev_223
This approach places desktop PCs in low-power mode when inactive and switches them to running mode when pages are needed by the VM running on the consolidation server	mechanism	2K_dev_223
To ensure that desktops save energy	mechanism	2K_dev_223
we have developed sleep scheduling and prefetching algorithms	mechanism	2K_dev_223
as well as the context-aware selective resume framework	mechanism	2K_dev_223
a novel approach to reduce the latency of power mode transition operations in commodity PCs	mechanism	2K_dev_223
Jettison our software prototype of partial VM migration for off-the-shelf PCs	method	2K_dev_223
Recent proposals have advocated the use of consolidation of idle desktop Virtual Machines ( VMs )	purpose	2K_dev_223
However desktop VMs are often large	purpose	2K_dev_223
requiring gigabytes of memory	purpose	2K_dev_223
Consolidating such VMs creates large network transfers lasting in the order of minutes and utilizes server memory inefficiently	purpose	2K_dev_223
When multiple VMs migrate concurrently	purpose	2K_dev_223
networks become congested and the resulting migration latencies are prohibitive	purpose	2K_dev_223
that transparently migrates only the working set of an idle VM	purpose	2K_dev_223
These problems are motivated by the LASSO framework and have applications in machine learning and computer vision	background	2K_dev_226
While this connection demonstrates the difficulties of obtaining runtime guarantees	mechanism	2K_dev_226
it also suggests an approach of using techniques originally developed for graph algorithms We then show that most of these problems can be formulated as a grouped least squares problem	mechanism	2K_dev_226
and give efficient algorithms for this formulation Our algorithms rely on routines for solving quadratic minimization problems	mechanism	2K_dev_226
which in turn are equivalent to solving linear systems	mechanism	2K_dev_226
Some preliminary experimental work on image processing tasks are also presented	method	2K_dev_226
We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems	purpose	2K_dev_226
Our work shows a close connection between these problems and core questions in algorithmic graph theory	purpose	2K_dev_226
Consider networks in harsh environments	background	2K_dev_233
where nodes may be lost due to failure	background	2K_dev_233
attack or infection -- how is the topology affected by such events ?	background	2K_dev_233
We propose a new generative model of network evolution in dynamic and harsh environments	mechanism	2K_dev_233
Our model can reproduce the range of topologies observed across known robust and fragile biological networks	mechanism	2K_dev_233
as well as several additional transport	mechanism	2K_dev_233
communication and social networks	mechanism	2K_dev_233
We also develop a new optimization measure based on preserving high connectivity following random or adversarial bursty node loss	mechanism	2K_dev_233
propose a new distributed algorithm	mechanism	2K_dev_233
Using this measure we evaluate the robustness of several real-world networks and	method	2K_dev_233
Can we mimic and measure the effect ? to evaluate robustness to construct secure networks operating within malicious environments	purpose	2K_dev_233
Modern robots like todays smartphones	background	2K_dev_243
are complex devices with intricate software systems	background	2K_dev_243
This paper focuses on teaching with Tekkotsu	mechanism	2K_dev_243
an open source robot application development framework designed specifically for education But	mechanism	2K_dev_243
the curriculum described here can also be taught using ROS	mechanism	2K_dev_243
the Robot Operating System that is now widely used for robotics research	mechanism	2K_dev_243
Introductory robot programming courses must evolve to reflect this reality	purpose	2K_dev_243
by teaching students to make use of the sophisticated tools their robots provide rather than reimplementing basic algorithms	purpose	2K_dev_243
As airspace becomes ever more crowded	background	2K_dev_244
air traffic management must reduce both space and time between aircraft to increase throughput	background	2K_dev_244
making on-board collision avoidance systems ever more important	background	2K_dev_244
These safety-critical systems must be extremely reliable	background	2K_dev_244
and as such many resources are invested into ensuring that the protocols they implement are accurate	background	2K_dev_244
Still it is challenging to guarantee that such a controller works properly under every circumstance	background	2K_dev_244
This is an important step in formally verified	background	2K_dev_244
flyable and distributed air traffic control	background	2K_dev_244
We prove that the controllers never allow the aircraft to get too close to one another	finding	2K_dev_244
even when new planes approach an in-progress avoidance maneuver that the new plane may not be aware of	finding	2K_dev_244
Because these safety guarantees always hold	finding	2K_dev_244
the aircraft are protected against unexpected emergent behavior which simulation and testing may miss	finding	2K_dev_244
We consider a class of distributed collision avoidance controllers designed to work even in environments with arbitrarily many aircraft or UAVs	method	2K_dev_244
In tough scenarios where a large number of aircraft must execute a collision avoidance maneuver	purpose	2K_dev_244
a human pilot under stress is not necessarily able to understand the complexity of the distributed system and may not take the right course	purpose	2K_dev_244
especially if actions must be taken quickly	purpose	2K_dev_244
the importance of studying the implications of sampling is twofold : First	background	2K_dev_247
sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second	background	2K_dev_247
because every such data collection can be perceived as a sample of the real world To the best of our knowledge	background	2K_dev_247
our work represents the largest study of propagation patterns of executables	background	2K_dev_247
We discover the SharkFin temporal propagation pattern of executable files	mechanism	2K_dev_247
the GeoSplit pattern in the geographical spread of machines that report executables to Symantecs servers	mechanism	2K_dev_247
the Periodic Power Law ( Ppl ) distribution of the lifetime of URLs	mechanism	2K_dev_247
and we show how	mechanism	2K_dev_247
by analyzing patterns from 22 million malicious ( and benign ) files	method	2K_dev_247
6 million hosts worldwide during the month of June 2011 We conduct this study using the WINE database available at Symantec Research Labs Additionally	method	2K_dev_247
we explore the research questions raised by sampling on such large databases of executables ; We further investigate the propagation pattern of benign and malicious executables	method	2K_dev_247
unveiling latent structures in the way these files spread	method	2K_dev_247
How does malware propagate ? Does it form spikes over time ? Does it resemble the propagation pattern of benign files	purpose	2K_dev_247
such as software patches ? Does it spread uniformly over countries ? How long does it take for a URL that distributes malware to be detected and shut down ? In this work	purpose	2K_dev_247
we answer these questions to efficiently extrapolate crucial properties of the data from a small sample	purpose	2K_dev_247
Our approach is not limited to images	background	2K_dev_249
but they provide a convenient query space to test search optimizations	background	2K_dev_249
We present a cloud-based approach that is sensitive to bandwidth and energy constraints Our approach is inspired by the long-established practice of photographers using contact sheets to rapidly visualize a new collection of photographs	mechanism	2K_dev_249
and then selecting a subset on which to focus attention	mechanism	2K_dev_249
On behalf of each smartphone	mechanism	2K_dev_249
the cloud maintains a virtual contact sheet of images that have been captured but not yet uploaded	mechanism	2K_dev_249
The virtual contact sheet consists of a set of low-fidelity images as well as full or partial meta-data associated with each image	mechanism	2K_dev_249
If search processing on the cloud indicates that a particular low-fidelity object is relevant	mechanism	2K_dev_249
then its full-fidelity image can be obtained just-in-time from the corresponding smartphone for further search processing or presentation to the user	mechanism	2K_dev_249
to opportunistic near real-time search of untagged images on smartphones	purpose	2K_dev_249
Although widely touted as a replacement for glass slides and microscopes in pathology	background	2K_dev_254
digital slides present major challenges in data storage	background	2K_dev_254
transmission processing and interoperability OpenSlide is in use today by many academic and industrial organizations world-wide	background	2K_dev_254
including many research sites in the United States that are funded by the National Institutes of Health	background	2K_dev_254
can transparently handle multiple vendor formats	finding	2K_dev_254
In this paper we present the design and implementation of OpenSlide	mechanism	2K_dev_254
a vendor-neutral C library The library is extensible and easily interfaced to various programming languages	mechanism	2K_dev_254
An application written to the OpenSlide interface	method	2K_dev_254
Since no universal data format is in widespread use for these images today	purpose	2K_dev_254
each vendor defines its own proprietary data formats	purpose	2K_dev_254
analysis tools viewers and software libraries	purpose	2K_dev_254
This creates issues not only for pathologists	purpose	2K_dev_254
but also for interoperability for reading and manipulating digital slides of diverse vendor formats	purpose	2K_dev_254
As part of a collaboration with a major California school district	background	2K_dev_262
show that a nontrivial implementation of the leximin mechanism scales gracefully in terms of running time ( even though the problem is intractable in theory )	finding	2K_dev_262
and performs extremely well with respect to a number of efficiency objectives	finding	2K_dev_262
Our approach revolves around the randomized leximin mechanism We extend previous work to the classroom allocation setting	mechanism	2K_dev_262
showing that the leximin mechanism is proportional	mechanism	2K_dev_262
envy-free efficient and group strategyproof	mechanism	2K_dev_262
We also prove that the leximin mechanism provides a ( worst-case ) 4-approximation to the maximum number of classrooms that can possibly be allocated	mechanism	2K_dev_262
We take great pains to establish the practicability of our approach	mechanism	2K_dev_262
and discuss issues related to its deployment	mechanism	2K_dev_262
Our experiments which are based on real data	method	2K_dev_262
we study the problem of fairly allocating unused classrooms in public schools to charter schools	purpose	2K_dev_262
Given a set of k networks	background	2K_dev_264
possibly with different sizes and no overlaps in nodes or links	background	2K_dev_264
how can we quickly assess similarity between them ? Analogously	background	2K_dev_264
are there a set of social theories which	background	2K_dev_264
when represented by a small number of descriptive	background	2K_dev_264
numerical features effectively serve as a `` signature '' for the network ?	background	2K_dev_264
NETSIMILE outperforms baseline competitors	finding	2K_dev_264
We propose a novel	mechanism	2K_dev_264
effective and scalable method	mechanism	2K_dev_264
called NETSIMILE Our approach has the following desirable properties : ( a ) It is supported by a set of social theories	mechanism	2K_dev_264
( b ) It gives similarity scores that are size-invariant	mechanism	2K_dev_264
( c ) It is scalable	mechanism	2K_dev_264
being linear on the number of links for graph signature extraction our approach enables several mining tasks such as clustering	mechanism	2K_dev_264
visualization discontinuity detection network transfer learning	mechanism	2K_dev_264
and re-identification across networks	mechanism	2K_dev_264
In extensive experiments on numerous synthetic and real networks from disparate domains	method	2K_dev_264
We also demonstrate how	method	2K_dev_264
Having such signatures will enable a wealth of graph mining and social network analysis tasks	purpose	2K_dev_264
including clustering outlier detection	purpose	2K_dev_264
visualization etc for solving the above problem	purpose	2K_dev_264
Short-term forecasting is a ubiquitous practice in a wide range of energy systems	background	2K_dev_265
including forecasting demand renewable generation	background	2K_dev_265
we show that this probabilistic model greatly outperforms other methods on the task of accurately modeling potential distributions of power ( as would be necessary in a stochastic dispatch problem	finding	2K_dev_265
In this paper we apply a recently-proposed algorithm for modeling high-dimensional conditional Gaussian distributions to forecasting wind power and extend it to the non-Gaussian case using the copula transform	mechanism	2K_dev_265
On a wind power forecasting task	method	2K_dev_265
Although it is known that probabilistic forecasts ( which give a distribution over possible future outcomes ) can improve planning and control	purpose	2K_dev_265
many forecasting systems in practice are just used as point forecast tools	purpose	2K_dev_265
as it is challenging to represent high-dimensional non-Gaussian distributions over multiple spatial and temporal points	purpose	2K_dev_265
Question answering ( Q & A ) communities have been gaining popularity in the past few years	background	2K_dev_268
The success of such sites depends mainly on the contribution of a small number of expert users who provide a significant portion of the helpful answers	background	2K_dev_268
Interestingly we find that while the majority of questions on the site are asked by low reputation users	finding	2K_dev_268
on average a high reputation user asks more questions than a user with low reputation and find they are effective in detecting extreme behaviors such as those of spam users we predict who will become influential long-term contributors	finding	2K_dev_268
We consider a number of graph analysis methods	mechanism	2K_dev_268
We present a study of the popular Q & A website StackOverflow ( SO )	method	2K_dev_268
in which users ask and answer questions about software development	method	2K_dev_268
algorithms math and other technical topics	method	2K_dev_268
The dataset includes information on 3	method	2K_dev_268
5 million questions and 6	method	2K_dev_268
9 million answers created by 1	method	2K_dev_268
3 million users in the years 2008 -- 2012	method	2K_dev_268
Participation in activities on the site ( such as asking and answering questions ) earns users reputation	method	2K_dev_268
which is an indicator of the value of that user to the site	method	2K_dev_268
We describe an analysis of the SO reputation system	method	2K_dev_268
and the participation patterns of high and low reputation users	method	2K_dev_268
The contributions of very high reputation users to the site indicate that they are the primary source of answers	method	2K_dev_268
and especially of high quality answers	method	2K_dev_268
Lastly we show an application of our analysis : by considering user contributions over first months of activity on the site	method	2K_dev_268
and so identifying users that have the potential of becoming strong contributers is an important task for owners of such communities	purpose	2K_dev_268
for detecting influential and anomalous users in the underlying user interaction network	purpose	2K_dev_268
Kidney exchanges allow incompatible donor-patient pairs to swap kidneys	background	2K_dev_273
but each donation must pass three tests : blood	background	2K_dev_273
tissue and crossmatch In practice a matching is computed based on the first two tests	background	2K_dev_273
and then a single crossmatch test is performed for each matched patient	background	2K_dev_273
Our main result is a polynomial time algorithm that almost surely computes optimal -- - up to lower order terms -- - solutions on random large kidney exchange instances	mechanism	2K_dev_273
However if two crossmatches could be performed per patient	purpose	2K_dev_273
in principle significantly more successful exchanges could take place	purpose	2K_dev_273
In this paper we ask : If we were allowed to perform two crossmatches per patient	purpose	2K_dev_273
could we harness this additional power optimally and efficiently ? for this problem	purpose	2K_dev_273
Recent computer systems research has proposed using redundant requests to reduce latency	background	2K_dev_275
The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request )	background	2K_dev_275
We find some surprising results	finding	2K_dev_275
First the response time of a fully redundant class follows a simple Exponential distribution and that of the non-redundant class follows a Generalized Hyperexponential	finding	2K_dev_275
Second fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant	finding	2K_dev_275
We find that in many cases	finding	2K_dev_275
redundancy outperforms JSQ and Opt-Split with respect to overall response time	finding	2K_dev_275
making it an attractive solution	finding	2K_dev_275
We allow for any number of classes of redundant requests	method	2K_dev_275
any number of classes of non-redundant requests	method	2K_dev_275
any degree of redundancy	method	2K_dev_275
and any number of heterogeneous servers	method	2K_dev_275
In all cases we derive the limiting distribution on the state of the system	method	2K_dev_275
In small ( two or three server ) systems	method	2K_dev_275
we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes	method	2K_dev_275
and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy We also compare redundancy with other approaches for reducing latency	method	2K_dev_275
such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and Join-the-Shortest-Queue ( JSQ ) routing of a class	method	2K_dev_275
However there is no exact analysis of systems with redundancy	purpose	2K_dev_275
This paper presents the first exact analysis of systems with redundancy	purpose	2K_dev_275
For example the Project Tycho provides open access to the count infections for U	background	2K_dev_279
states from 1888 to 2013	background	2K_dev_279
for 56 contagious diseases ( e	background	2K_dev_279
measles influenza ) which include missing values	background	2K_dev_279
possible recording errors sudden spikes ( or dives ) of infections	background	2K_dev_279
demonstrate that FUNNELFIT does indeed discover important properties of epidemics : ( P1 ) disease seasonality	finding	2K_dev_279
influenza spikes in January	finding	2K_dev_279
Lyme disease spikes in July and the absence of yearly periodicity for gonorrhea ; ( P2 ) disease reduction effect	finding	2K_dev_279
the appearance of vaccines ; ( P3 ) local/state-level sensitivity	finding	2K_dev_279
many measles cases in NY ; ( P4 ) external shock events	finding	2K_dev_279
historical flu pandemics ; ( P5 ) detect incongruous values	finding	2K_dev_279
In this paper we present FUNNEL	mechanism	2K_dev_279
a unifying analytical model as well as a novel fitting algorithm	mechanism	2K_dev_279
FUNNELFIT Our method has the following properties : ( a ) Sense-making : it detects important patterns of epidemics	mechanism	2K_dev_279
such as periodicities the appearance of vaccines	mechanism	2K_dev_279
external shock events and more ; ( b ) Parameter-free : our modeling framework frees the user from providing parameter values ; ( c ) Scalable : FUNNELFIT is carefully designed to be linear on the input size ; ( d ) General : our model is general and practical	mechanism	2K_dev_279
which can be applied to various types of epidemics	mechanism	2K_dev_279
including computer-virus propagation as well as human diseases	mechanism	2K_dev_279
Extensive experiments on real data	method	2K_dev_279
Given a large collection of epidemiological data consisting of the count of d contagious diseases for l locations of duration n	purpose	2K_dev_279
how can we find patterns	purpose	2K_dev_279
rules and outliers ? So how can we find a combined model	purpose	2K_dev_279
for all these diseases	purpose	2K_dev_279
locations and time-ticks ? for large scale epidemiological data which solves the above problem	purpose	2K_dev_279
The M/M/k/setup model where there is a penalty for turning servers on	background	2K_dev_281
is common in data centers	background	2K_dev_281
call centers and manufacturing systems	background	2K_dev_281
Setup costs take the form of a time delay	background	2K_dev_281
and sometimes there is additionally a power penalty	background	2K_dev_281
as in the case of data centers	background	2K_dev_281
by a new way of combining renewal reward theory and recursive techniques to solve Markov chains with a repeating structure	mechanism	2K_dev_281
Our renewal-based approach uses ideas from renewal reward theory and busy period analysis to obtain closed-form expressions for metrics of interest such as the transform of time in system and the transform of power consumed by the system	mechanism	2K_dev_281
The simplicity intuitiveness and versatility of our renewal-based approach makes it useful for analyzing Markov chains far beyond the M/M/k/setup	mechanism	2K_dev_281
In general our renewal-based approach should be used to reduce the analysis of any 2-dimensional Markov chain which is infinite in at most one dimension and repeating to the problem of solving a system of polynomial equations	mechanism	2K_dev_281
In the case where all transitions in the repeating portion of the Markov chain are skip-free and all up/down arrows are unidirectional	mechanism	2K_dev_281
the resulting system of equations will yield a closed-form solution	mechanism	2K_dev_281
Our analysis is made possible	method	2K_dev_281
While the M/M/1/setup was exactly analyzed in 1964	purpose	2K_dev_281
no exact analysis exists to date for the M/M/k/setup with $ $ k > 1 $ $ k In this paper	purpose	2K_dev_281
we provide the first exact	purpose	2K_dev_281
closed-form analysis for the M/M/k/setup and some of its important variants including systems in which idle servers delay for a period of time before turning off or can be put to sleep	purpose	2K_dev_281
If Lisa visits Dr	background	2K_dev_286
Brown and there is no record of the drug he prescribed her	background	2K_dev_286
can we find it ? Data sources	background	2K_dev_286
much to analysts ' dismay	background	2K_dev_286
are too often plagued with incompleteness	background	2K_dev_286
making business analytics over the data difficult	background	2K_dev_286
We introduce a principled way of performing value imputation on missing values	mechanism	2K_dev_286
allowing a user We achieve this by turning our data into a graph network and performing link prediction on nodes of interest using the belief propagation algorithm	mechanism	2K_dev_286
Data entries with incomplete values are ignored	purpose	2K_dev_286
making some analytic queries fail to accurately describe how an organization is performing to choose a correct value after viewing possible values and why they were inferred	purpose	2K_dev_286
A cognitive assistance application combines a wearable device such as Google Glass with cloudlet processing to provide step-by-step guidance on a complex task	background	2K_dev_287
We then reflect on the difficulties we faced in building these applications	background	2K_dev_287
and suggest future research that could simplify the creation of similar applications	background	2K_dev_287
In this paper we focus on user assistance We describe proof-of-concept implementations for four different tasks : assembling 2D Lego models	mechanism	2K_dev_287
freehand sketching playing ping-pong	mechanism	2K_dev_287
and recommending context-relevant YouTube tutorials	mechanism	2K_dev_287
for narrow and well-defined tasks that require specialized knowledge and/or skills	purpose	2K_dev_287
Let us consider that someone is starting a research on a topic that is unfamiliar to them	background	2K_dev_289
we show the effectiveness and efficiency of our approach	finding	2K_dev_289
First we propose an algorithm We also address the performance and scalability issues of this sophisticated algorithm Next	mechanism	2K_dev_289
we discuss the measures to decide how much a paper is influenced by another paper	mechanism	2K_dev_289
Then we propose an algorithm by using the influence measure and citation information	mechanism	2K_dev_289
Finally through extensive experiments with a large volume of a real-world academic literature data	method	2K_dev_289
Which seminal papers have influenced the topic the most ? What is the genealogy of the seminal papers in this topic ? These are the questions that they can raise	purpose	2K_dev_289
which we try to answer in this paper	purpose	2K_dev_289
that finds a set of seminal papers on a given topic	purpose	2K_dev_289
that constructs a genealogy of the seminal papers	purpose	2K_dev_289
As kidney exchange programs are growing	background	2K_dev_291
manipulation by hospitals becomes more of an issue	background	2K_dev_291
suggest that in practice our mechanism performs much closer to optimal	finding	2K_dev_291
We study mechanisms for two-way exchanges that are strategyproof	mechanism	2K_dev_291
make it a dominant strategy We establish lower bounds on the welfare loss of strategyproof mechanisms	mechanism	2K_dev_291
both deterministic and randomized	mechanism	2K_dev_291
and propose a randomized mechanism that guarantees at least half of the maximum social welfare in the worst case	mechanism	2K_dev_291
Simulations using realistic distributions for blood types and other parameters	method	2K_dev_291
Assuming that hospitals wish to maximize the number of their own patients who receive a kidney	purpose	2K_dev_291
they may have an incentive to withhold some of their incompatible donorpatient pairs and match them internally	purpose	2K_dev_291
thus harming social welfare	purpose	2K_dev_291
for hospitals to report all their incompatible pairs	purpose	2K_dev_291
We propose a non-intrusive approach At the core of this approach is a mechanism for selective real-time monitoring of guest file updates within VM instances	mechanism	2K_dev_295
This mechanism is agentless	mechanism	2K_dev_295
requiring no guest VM support	mechanism	2K_dev_295
It has low virtual I/O overhead	mechanism	2K_dev_295
low latency for emitting file updates	mechanism	2K_dev_295
and a scalable design	mechanism	2K_dev_295
Its central design principle is distributed streaming of file updates inferred from introspected disk sector writes	mechanism	2K_dev_295
The mechanism called DS-VMI	mechanism	2K_dev_295
enables many system administration tasks that involve monitoring files to be performed outside VMs	mechanism	2K_dev_295
for monitoring virtual machines ( VMs ) in the cloud	purpose	2K_dev_295
imply improved parallel randomized algorithms for several problems	background	2K_dev_299
including single-source shortest paths	background	2K_dev_299
maximum flow minimum-cost flow	background	2K_dev_299
and approximate maximum flow	background	2K_dev_299
We present the design and analysis of a nearly-linear work parallel algorithm On input an SDD n-by-n matrix A with m nonzero entries and a vector b	mechanism	2K_dev_299
our algorithm computes a vector $ \tilde { x } $ such that $ \|\tilde { x } - A^ { + } b\|_ { A } \leq\varepsilon\cdot\| { A^ { + } b } \|_ { A } $ in $ O ( m\log^ { O ( 1 ) } { n } \log { \frac { 1 } { \varepsilon } } ) $ work and $ O ( m^ { 1/3+\theta } \log\frac { 1 } { \varepsilon } ) $ depth for any ? > 0	mechanism	2K_dev_299
where A + denotes the Moore-Penrose pseudoinverse of A	mechanism	2K_dev_299
The algorithm relies on a parallel algorithm for generating low-stretch spanning trees or spanning subgraphs To this end	mechanism	2K_dev_299
we first develop a parallel decomposition algorithm that in O ( mlog O ( 1 ) n ) work and polylogarithmic depth	mechanism	2K_dev_299
partitions a graph with n nodes and m edges into components with polylogarithmic diameter such that only a small fraction of the original edges are between the components	mechanism	2K_dev_299
This can be used to generate low-stretch spanning trees with average stretch O ( n ? ) in O ( mlog O ( 1 ) n ) work and O ( n ? ) depth for any ? > 0	mechanism	2K_dev_299
Alternatively it can be used to generate spanning subgraphs with polylogarithmic average stretch in O ( mlog O ( 1 ) n ) work and polylogarithmic depth	mechanism	2K_dev_299
We apply this subgraph construction to derive a parallel linear solver	mechanism	2K_dev_299
By using this solver in known applications	method	2K_dev_299
for solving symmetric diagonally dominant ( SDD ) linear systems	purpose	2K_dev_299
A password composition policy restricts the space of allowable passwords to eliminate weak passwords that are vulnerable to statistical guessing attacks	background	2K_dev_301
We introduce the first theoretical model Our main positive result is an algorithm that -- with high probability -- - constructs almost optimal policies ( which are specified as a union of subsets of allowed passwords )	mechanism	2K_dev_301
and requires only a small number of samples of users ' preferred passwords	mechanism	2K_dev_301
We study the computational and sample complexity of this problem under different assumptions on the structure of policies and on users ' preferences over passwords	method	2K_dev_301
We complement our with simulations using a real-world dataset of 32 million passwords	method	2K_dev_301
Usability studies have demonstrated that existing password composition policies can sometimes result in weaker password distributions ; hence a more principled approach is needed	purpose	2K_dev_301
for optimizing password composition policies	purpose	2K_dev_301
This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities	background	2K_dev_310
We defined over 100 data features in seven categories We have achieved area under the ROC curve values of up to 0	finding	2K_dev_310
979 and lift values of 65 on the top 50 user-days identified on two months of real data	finding	2K_dev_310
Our system combines structural and semantic information from a real corporate database of monitored activity on their users ' computers We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior	mechanism	2K_dev_310
indicators of unusual activities	mechanism	2K_dev_310
high-dimensional statistical patterns temporal sequences	mechanism	2K_dev_310
and normal graph evolution Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams	mechanism	2K_dev_310
We have also developed a visual language for specifying combinations of features	mechanism	2K_dev_310
baselines peer groups time periods	mechanism	2K_dev_310
and algorithms to detect anomalies suggestive of instances of insider threat behavior	mechanism	2K_dev_310
based on approximately 5	method	2K_dev_310
5 million actions per day from approximately 5	method	2K_dev_310
to develop integrate and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations ' information systems	purpose	2K_dev_310
to detect independently developed red team inserts of malicious insider activities	purpose	2K_dev_310
We describe a new algorithm The running time of our algorithm is O ( f log n log ) where f is the output complexity of the Voronoi diagram and is the spread of the input	mechanism	2K_dev_312
the ratio of largest to smallest pairwise distances	mechanism	2K_dev_312
Despite the simplicity of the algorithm and its analysis	mechanism	2K_dev_312
it improves on the state of the art for all inputs with polynomial spread and near-linear output size	mechanism	2K_dev_312
The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation Then	mechanism	2K_dev_312
the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity	mechanism	2K_dev_312
yielding the output sensitive bound	mechanism	2K_dev_312
The removal only involves local flips and is inspired by kinetic data structures	mechanism	2K_dev_312
for computing the Voronoi diagram of a set of n points in constant-dimensional Euclidean space	purpose	2K_dev_312
Color descriptors are one of the important features used in content-based image retrieval	background	2K_dev_313
The dominant color descriptor ( DCD ) represents a few perceptually dominant colors in an image through color quantization	background	2K_dev_313
For image retrieval based on DCD	background	2K_dev_313
the earth movers distance ( EMD ) and the optimal color composition distance were proposed to measure the dissimilarity between two images	background	2K_dev_313
The results reveal that our approach achieves almost the same results with the EMD in linear time	finding	2K_dev_313
we propose a new distance function that calculates an approximate earth movers distance in linear time To calculate the dissimilarity in linear time	mechanism	2K_dev_313
the proposed approach employs the space-filling curve for multidimensional color space	mechanism	2K_dev_313
To improve the accuracy	mechanism	2K_dev_313
the proposed approach uses multiple curves and adjusts the color positions	mechanism	2K_dev_313
As a result our approach achieves order-of-magnitude time improvement but incurs small errors	mechanism	2K_dev_313
We have performed extensive experiments to show the effectiveness and efficiency of the proposed approach	method	2K_dev_313
Although providing good retrieval results	purpose	2K_dev_313
both methods are too time-consuming to be used in a large image database	purpose	2K_dev_313
To solve the problem	purpose	2K_dev_313
Current applications have produced graphs on the order of hundreds of thousands of nodes and millions of edges	background	2K_dev_323
To take advantage of such graphs	background	2K_dev_323
one must be able to find patterns	background	2K_dev_323
These tasks are better performed in an interactive environment	background	2K_dev_323
where human expertise can guide the process	background	2K_dev_323
we propose an innovative framework suited for any kind of tree-like graph visual design GMine integrates 1 ) a representation for graphs organized as hierarchies of partitions-the concepts of SuperGraph and Graph-Tree ; and 2 ) a graph summarization methodology-CEPS Our graph representation deals with the problem of tracing the connection aspects of a graph hierarchy with sub linear complexity	mechanism	2K_dev_323
allowing one to grasp the neighborhood of a single node or of a group of nodes in a single click	mechanism	2K_dev_323
As a proof of concept	method	2K_dev_323
the visual environment of GMine is instantiated as a system in which large graphs can be investigated globally and locally	method	2K_dev_323
For large graphs though	purpose	2K_dev_323
there are some challenges : the excessive processing requirements are prohibitive	purpose	2K_dev_323
and drawing hundred-thousand nodes results in cluttered images hard to comprehend	purpose	2K_dev_323
To cope with these problems	purpose	2K_dev_323
Existing methods are typically superlinear in space or execution time	background	2K_dev_333
Effectiveness : it is accurate	finding	2K_dev_333
providing results with equal or better quality compared to top related works Halite was in average at least 12 times faster than seven representative works	finding	2K_dev_333
and always presented highly accurate results	finding	2K_dev_333
Halite was at least 11 times faster than others	finding	2K_dev_333
increasing their accuracy in up to 35 percent	finding	2K_dev_333
This paper proposes Halite	mechanism	2K_dev_333
a novel fast and scalable clustering method Halite 's strengths are that it is fast and scalable	mechanism	2K_dev_333
while still giving highly accurate results	mechanism	2K_dev_333
Specifically the main contributions of Halite are : 1 ) Scalability : it is linear or quasi linear in time and space regarding the data size and dimensionality	mechanism	2K_dev_333
and the dimensionality of the clusters ' subspaces ; 2 ) Usability : it is deterministic	mechanism	2K_dev_333
robust to noise does n't take the number of clusters as an input parameter	mechanism	2K_dev_333
and detects clusters in subspaces generated by original axes or by their linear combinations	mechanism	2K_dev_333
including space rotation ; 3 ) ; and 4 ) Generality : it includes a soft clustering approach	mechanism	2K_dev_333
Experiments on synthetic data ranging from five to 30 axes and up to 1 \rm million points were performed On real data Finally	method	2K_dev_333
we report experiments in a real scenario where soft clustering is desirable	method	2K_dev_333
that looks for clusters in subspaces of multidimensional data	purpose	2K_dev_333
Computers are often used in performance of popular music	background	2K_dev_341
but most often in very restricted ways	background	2K_dev_341
such as keyboard synthesizers where musicians are in complete control	background	2K_dev_341
or pre-recorded or sequenced music where musicians follow the computer 's drums or click track	background	2K_dev_341
An interesting and yet little-explored possibility is the computer as highly autonomous performer of popular music	background	2K_dev_341
capable of joining a mixed ensemble of computers and humans	background	2K_dev_341
and our experience with them	finding	2K_dev_341
We describe a general architecture	mechanism	2K_dev_341
and describe some early implementations	method	2K_dev_341
Considering the skills and functional requirements of musicians leads to a number of predictions about future humancomputer music performance ( HCMP ) systems for popular music for such systems	purpose	2K_dev_341
How can we detect suspicious users in large online networks ? Online popularity of a user or product ( via follows	background	2K_dev_343
) can be monetized on the premise of higher ad click-through rates or increased sales	background	2K_dev_343
Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck	background	2K_dev_343
Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent ( but sometimes honest ) users	background	2K_dev_343
( b ) it is shown to be highly effective on real data and and with high precision identify many suspicious accounts which have persisted without suspension even to this day	finding	2K_dev_343
and propose fBox an algorithm designed Our algorithm has the following desirable properties : ( a ) it has theoretical underpinnings	mechanism	2K_dev_343
( c ) it is scalable ( linear on the input size )	mechanism	2K_dev_343
We evaluate fBox on a large	method	2K_dev_343
7 million node 1	method	2K_dev_343
5 billion edge who-follows-whom social graph from Twitter in 2010	method	2K_dev_343
However small-scale stealthy attacks may go unnoticed due to the nature of low-rank Eigen analysis used in practice	purpose	2K_dev_343
In this work we take an adversarial approach to find and prove claims about the weaknesses of modern	purpose	2K_dev_343
state-of-the-art spectral methods to catch small-scale	purpose	2K_dev_343
stealth attacks that slip below the radar	purpose	2K_dev_343
A well-studied approach to the design of voting rules views them as maximum likelihood estimators ; given votes that are seen as noisy estimates of a true ranking of the alternatives	background	2K_dev_345
the rule must reconstruct the most likely true ranking	background	2K_dev_345
and show that for all rules in this family the number of samples required from the Mallows noise model is logarithmic in the number of alternatives	finding	2K_dev_345
and that no rule can do asymptotically better ( while some rules like plurality do much worse ) and find voting rules that are accurate in the limit for all noise models in such general families	finding	2K_dev_345
We define the family of pairwise-majority consistent rules Taking a more normative point of view	mechanism	2K_dev_345
we consider voting rules that surely return the true ranking as the number of samples tends to infinity ( we call this property accuracy in the limit ) ; this allows us to move to a higher level of abstraction We characterize the distance functions that induce noise models for which pairwise-majority consistent rules are accurate in the limit	mechanism	2K_dev_345
and provide a similar result for another novel family of position-dominance consistent rules These characterizations capture three well-known distance functions	mechanism	2K_dev_345
We study families of noise models that are parametrized by distance functions	method	2K_dev_345
We argue that this is too stringent a requirement	purpose	2K_dev_345
and instead ask : How many votes does a voting rule need to reconstruct the true ranking ?	purpose	2K_dev_345
The quality and effectiveness of the load following services provided by centralized control of thermostatically controlled loads depend highly on the communication requirements and the underlying cyberinfrastructure characteristics	background	2K_dev_346
Specifically ensuring end-user comfort while providing real-time demand response services depends on the availability of the information provided from the thermostatically controlled loads to the main controller regarding their operating statuses and internal temperatures	background	2K_dev_346
The results show that some improvement is possible for scenarios when loads are expected to be toggled frequently	finding	2K_dev_346
In this paper we introduce a moving horizon mean squared error state estimator with constraints which assumes a linear model without constraints	mechanism	2K_dev_346
State estimation techniques can be used to infer the necessary information from the aggregate power consumption of these loads	purpose	2K_dev_346
replacing the need for an upstream communication platform carrying information from appliances to the main controller in real-time	purpose	2K_dev_346
as an alternative to a Kalman filter approach	purpose	2K_dev_346
Big graphs are everywhere	background	2K_dev_349
ranging from social networks and mobile call networks to biological networks and the World Wide Web Mining big graphs leads to many interesting applications including cyber security	background	2K_dev_349
fraud detection Web search	background	2K_dev_349
recommendation and many more	background	2K_dev_349
Our findings include anomalous spikes in the connected component size distribution	finding	2K_dev_349
the 7 degrees of separation in a Web graph	finding	2K_dev_349
and anomalous adult advertisers in the who-follows-whom Twitter social network	finding	2K_dev_349
In this paper we describe Pegasus built on top of MapReduce	mechanism	2K_dev_349
a modern distributed We introduce GIM-V	mechanism	2K_dev_349
an important primitive that Pegasus uses for its algorithms to analyze structures of large graphs We also introduce HEigen	mechanism	2K_dev_349
a large scale eigensolver which is also a part of Pegasus Both GIM-V and HEigen are highly optimized	mechanism	2K_dev_349
achieving linear scale up on the number of machines and edges	mechanism	2K_dev_349
2x and 76x faster performance than their naive counterparts	mechanism	2K_dev_349
Using Pegasus we analyze very large	method	2K_dev_349
real world graphs with billions of nodes and edges	method	2K_dev_349
How do we find patterns and anomalies in very large graphs with billions of nodes and edges ? How to mine such big graphs efficiently ? a big graph mining system data processing platform	purpose	2K_dev_349
The convergence of mobile computing and cloud computing is predicated on a reliable	background	2K_dev_359
This basic requirement is hard to guarantee in hostile environments such as military operations and disaster recovery This article is part of a special issue on the edge of the cloud	background	2K_dev_359
how VM-based cloudlets that are located in close proximity to associated mobile devices	mechanism	2K_dev_359
In this article the authors examine can overcome this challenge	purpose	2K_dev_359
Spliddit is a first-of-its-kind fair division website	mechanism	2K_dev_369
which In this note	mechanism	2K_dev_369
we discuss Spliddit 's goals	mechanism	2K_dev_369
offers provably fair solutions for the division of rent	purpose	2K_dev_369
Hybrid systems with both discrete and continuous dynamics are an important model for real-world cyber-physical systems	background	2K_dev_370
The key challenge is to ensure their correct functioning w	background	2K_dev_370
Promising techniques to ensure safety seem to be model-driven engineering to develop hybrid systems in a well-defined and traceable manner	background	2K_dev_370
and formal verification to prove their correctness	background	2K_dev_370
Their combination forms the vision of verification-driven engineering	background	2K_dev_370
Often hybrid systems are rather complex in that they require expertise from many domains ( e	background	2K_dev_370
robotics control systems computer science	background	2K_dev_370
software engineering and mechanical engineering )	background	2K_dev_370
This paper introduces a verification-driven engineering toolset that extends our previous work on hybrid and arithmetic verification with This toolset makes it easier	mechanism	2K_dev_370
Moreover despite the remarkable progress in automating formal verification of hybrid systems	purpose	2K_dev_370
the construction of proofs of complex systems often requires nontrivial human guidance	purpose	2K_dev_370
since hybrid systems verification tools solve undecidable problems	purpose	2K_dev_370
It is thus not uncommon for development and verification teams to consist of many players with diverse expertise	purpose	2K_dev_370
tools for ( 1 ) graphical ( UML ) and textual modeling of hybrid systems	purpose	2K_dev_370
( 2 ) exchanging and comparing models and proofs	purpose	2K_dev_370
and ( 3 ) managing verification tasks to tackle large-scale verification tasks	purpose	2K_dev_370
Cake cutting is a common metaphor for the division of a heterogeneous divisible good	background	2K_dev_372
There are numerous papers that study the problem of fairly dividing a cake	background	2K_dev_372
where for the first time our notion of dominant strategy truthfulness is the ubiquitous one in social choice and computer science	mechanism	2K_dev_372
We design both deterministic and randomized cake cutting mechanisms that are truthful and fair under different assumptions with respect to the valuation functions of the agents	mechanism	2K_dev_372
; a small number of them also take into account self-interested agents and consequent strategic issues	purpose	2K_dev_372
but these papers focus on fairness and consider a strikingly weak notion of truthfulness In this paper we investigate the problem of cutting a cake in a way that is truthful	purpose	2K_dev_372
Mobile crowdsensing is becoming a vital technique for environment monitoring	background	2K_dev_373
infrastructure management and social computing	background	2K_dev_373
and to offer our initial thoughts on the potential solutions to lowering the barriers	mechanism	2K_dev_373
However deploying mobile crowdsensing applications in large-scale environments is not a trivial task	purpose	2K_dev_373
It creates a tremendous burden on application developers as well as mobile users	purpose	2K_dev_373
In this paper we try to reveal the barriers hampering the scale-up of mobile crowdsensing applications	purpose	2K_dev_373
Rating data is ubiquitous on websites such as Amazon	background	2K_dev_376
Since ratings are not static but given at various points in time	background	2K_dev_376
a temporal analysis of rating data provides deeper insights into the evolution of a product 's quality	background	2K_dev_376
we show the effectiveness of our method and we present interesting discoveries on multiple real world datasets	finding	2K_dev_376
We propose a Bayesian model that represents the rating data as sequence of categorical mixture models	mechanism	2K_dev_376
In contrast to existing methods	mechanism	2K_dev_376
our method does not require any aggregation of the input but it operates on the original time stamped data To capture the dynamic effects of the ratings	mechanism	2K_dev_376
the categorical mixtures are temporally constrained : Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time	mechanism	2K_dev_376
Our method automatically determines the intervals where anomalies occur	mechanism	2K_dev_376
and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions we propose an efficient algorithm combining principles from variational inference and dynamic programming	mechanism	2K_dev_376
In our experimental study	method	2K_dev_376
In this work we tackle the following question : Given the time stamped rating data for a product or service	purpose	2K_dev_376
how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous ? For learning our model	purpose	2K_dev_376
Abstraction has emerged as a key component in solving extensive-form games of incomplete information	background	2K_dev_378
However lossless abstractions are typically too large to solve	background	2K_dev_378
so lossy abstraction is needed	background	2K_dev_378
show that it finds a lossless abstraction when one is available and lossy abstractions when smaller abstractions are desired	finding	2K_dev_378
We introduce a theoretical framework that can be used The framework uses a new notion for mapping abstract strategies to the original game	mechanism	2K_dev_378
and it leverages a new equilibrium refinement for analysis	mechanism	2K_dev_378
Using this framework we develop the first general lossy extensive-form game abstraction method with bounds While our framework can be used for lossy abstraction	mechanism	2K_dev_378
it is also a powerful tool for lossless abstraction if we set the bound to zero	mechanism	2K_dev_378
Prior abstraction algorithms typically operate level by level in the game tree	mechanism	2K_dev_378
We introduce the extensive-form game tree isomorphism and action subset selection problems	mechanism	2K_dev_378
both important problems for computing abstractions on a level-by-level basis We show that the former is graph isomorphism complete	mechanism	2K_dev_378
and the latter NP-complete	mechanism	2K_dev_378
We also prove that level-by-level abstraction can be too myopic and thus fail to find even obvious lossless abstractions	mechanism	2K_dev_378
All prior lossy abstraction algorithms for extensive-form games either 1 ) had no bounds on solution quality or 2 ) depended on specific equilibrium computation approaches	purpose	2K_dev_378
limited forms of abstraction	purpose	2K_dev_378
and only decreased the number of information sets rather than nodes in the game tree to give bounds on solution quality for any perfect-recall extensive-form game	purpose	2K_dev_378
The increase in the number of bloggers and the amount of information diffused in the blogosphere makes the blogosphere an important medium through which to communicate and exchange information	background	2K_dev_387
Accordingly the interest in understanding the nature of the information diffusion in the blogosphere has also been increased	background	2K_dev_387
BlogCast a functionality provided by blog-service providers to expose a high quality post on the portal main page	finding	2K_dev_387
is found to be one of the main causes of the information diffusion without explicit relationships	finding	2K_dev_387
We analyze the characteristics of the information diffusion through the BlogCast and its halo effect on the bloggers whose post has been exposed on the portal main page	method	2K_dev_387
In addition we examine the sustainability of the halo effect of the BlogCast over time	method	2K_dev_387
Existing studies in social networks have mainly focused on the information diffusion through explicit relationships between members	purpose	2K_dev_387
In this paper we analyze the causes for the information diffusion without explicit relationships in the blogosphere	purpose	2K_dev_387
Autonomous agents that operate as components of dynamic spatial systems are becoming increasingly popular and mainstream	background	2K_dev_394
Applications can be found in consumer robotics	background	2K_dev_394
in road rail and air transportation	background	2K_dev_394
manufacturing and military operations	background	2K_dev_394
In this article we discuss reasoning approaches	mechanism	2K_dev_394
which requires a sufficiently detailed description of the agents behavior and environment but may still be conducted in a qualitative manner	mechanism	2K_dev_394
We introduce a conceptual reference model	mechanism	2K_dev_394
which summarizes the current understanding of the characteristics of dynamic spatial systems based on a catalog of evaluation criteria derived from the model We provide a comparative summary of the modeling features	mechanism	2K_dev_394
discuss lessons learned and introduce a research roadmap	mechanism	2K_dev_394
We survey logic-based qualitative and hybrid modeling and commonsense reasoning approaches with respect to their features for describing and analyzing dynamic spatial systems in general	method	2K_dev_394
and the actions of autonomous agents operating therein in particular	method	2K_dev_394
We assess the modeling features provided by logic-based qualitative commonsense and hybrid approaches for projection	method	2K_dev_394
planning simulation and verification of dynamic spatial systems	method	2K_dev_394
Unfortunately the approaches to modeling and analyzing the behavior of dynamic spatial systems are just as diverse as these application domains for the medium-term control of autonomous agents in dynamic spatial systems for integrating different approaches of dynamic spatial system analysis to achieve coverage of all required features	purpose	2K_dev_394
with the same asymptotic guarantees as the best sequential algorithm	finding	2K_dev_395
We show an improved parallel algorithm with a small fraction of the edges in between These decompositions form critical subroutines in a number of graph algorithms	mechanism	2K_dev_395
Our algorithm builds upon the shifted shortest path approach introduced in [ Blelloch	mechanism	2K_dev_395
Gupta Koutis Miller Peng	mechanism	2K_dev_395
Tangwongsan SPAA 2011 ]	mechanism	2K_dev_395
By combining various stages of the previous algorithm	mechanism	2K_dev_395
we obtain a significantly simpler algorithm	mechanism	2K_dev_395
for decomposing an undirected unweighted graph into small diameter pieces	purpose	2K_dev_395
This can potentially lead to a deeper understanding of programming	background	2K_dev_398
bringing students closer to true computational thinking	background	2K_dev_398
We describe a three-stage model beginning with a simple	mechanism	2K_dev_398
highly scaffolded programming environment ( Kodu ) and progressing to more challenging frameworks ( Alice and Lego NXT-G )	mechanism	2K_dev_398
In moving between frameworks	mechanism	2K_dev_398
students explore the similarities and differences in how concepts such as variables	mechanism	2K_dev_398
conditionals and looping are realized Some novel strategies for teaching with Kodu are outlined	mechanism	2K_dev_398
Finally we briefly report on our methodology and select preliminary results from a pilot study using this curriculum with students ages 10-17	method	2K_dev_398
including several with disabilities	method	2K_dev_398
In a Stackelberg Security Game	background	2K_dev_406
a defender commits to a randomized deployment of security resources	background	2K_dev_406
and an attacker best-responds by attacking a target that maximizes his utility	background	2K_dev_406
via an online learning approach	mechanism	2K_dev_406
We are interested in algorithms that prescribe a randomized strategy for the defender at each step against an adversarially chosen sequence of attackers	mechanism	2K_dev_406
and obtain feedback on their choices ( observing either the current attacker type or merely which target was attacked We design no-regret algorithms whose regret ( when compared to the best fixed strategy in hindsight ) is polynomial in the parameters of the game	mechanism	2K_dev_406
and sublinear in the number of times steps	mechanism	2K_dev_406
While algorithms for computing an optimal strategy for the defender to commit to have had a striking real-world impact	purpose	2K_dev_406
deployed applications require significant information about potential attackers	purpose	2K_dev_406
We address this problem	purpose	2K_dev_406
results confirm that TransPart offers low overhead and startup cost	finding	2K_dev_422
while improving user experience	finding	2K_dev_422
This article investigates the transient use of free local storage We use the term TransientPC systems to refer to these types of systems	mechanism	2K_dev_422
The solution we propose	mechanism	2K_dev_422
called TransPart uses the higher-performing local storage of host hardware Our solution constructs a virtual storage device on demand ( which we call transient storage ) by borrowing free disk blocks from the hosts storage	mechanism	2K_dev_422
In this article we present the design	mechanism	2K_dev_422
and evaluation of a TransPart prototype	method	2K_dev_422
which requires no modifications to the software or hardware of a host computer Experimental	method	2K_dev_422
for improving performance in VM-based mobile computing systems implemented as thick clients on host PCs	purpose	2K_dev_422
to speed up performance-critical operations	purpose	2K_dev_422
due to the recent penetration of distributed green energy	background	2K_dev_425
distributed intelligence and plug-in electric vehicles Finally	background	2K_dev_425
the proposed method can be implemented given recent advances in machine learning	background	2K_dev_425
which are becoming drivers and sources of data previously unavailable in the electric power industry	background	2K_dev_425
results of the proposed method show that the new method produces a topology estimate excelling the current industrial approach	finding	2K_dev_425
Instead of taking the traditional complex physical model based approach	mechanism	2K_dev_425
this paper proposes a data-driven method	mechanism	2K_dev_425
leading to an effective Specifically	mechanism	2K_dev_425
we first introduce the data-driven topology estimation problem	mechanism	2K_dev_425
Then a novel Logistic Kernel Regression is proposed in a Bayesian framework based on Nearest Neighbors search	mechanism	2K_dev_425
Notably unlike many machine learning approaches that do not account for physical constraints	mechanism	2K_dev_425
and distinctive from deterministic engineering modeling defined solely by physical laws	mechanism	2K_dev_425
this paper for the first time combines the two into one single regression modeling for topology estimation	mechanism	2K_dev_425
This paper is motivated by major needs for fast and accurate on-line data analysis tools in the emerging electric energy systems topology estimation approach for the smart grid	purpose	2K_dev_425
Effortless one-touch capture of video is a unique capability of wearable devices such as Google Glass	background	2K_dev_427
We use this capability in which users receive queries relevant to their current location and opt-in preferences	mechanism	2K_dev_427
In response they can send back live video snippets of their surroundings	mechanism	2K_dev_427
A system of result caching	mechanism	2K_dev_427
geolocation and query similarity detection shields users from being overwhelmed by a flood of queries	mechanism	2K_dev_427
to create a new type of crowd-sourced system	purpose	2K_dev_427
It is known that in this setting both revenue and social welfare can be maximized by a threshold policy	background	2K_dev_429
whereby customers are barred from entry once the queue length reaches a certain threshold	background	2K_dev_429
allowing for settings with multiple servers	background	2K_dev_429
Finally we present a generalization of our results	finding	2K_dev_429
This paper presents the first derivation of the optimal threshold in closed form	mechanism	2K_dev_429
and a surprisingly simple formula Utilizing properties of the Lambert W function	mechanism	2K_dev_429
we also provide explicit scaling results of the optimal threshold as the customer valuation grows	mechanism	2K_dev_429
We consider the social welfare model of Naor [ 20 ] and revenue-maximization model of Chen and Frank [ 7 ]	purpose	2K_dev_429
where a single class of delay-sensitive customers seek service from a server with an observable queue	purpose	2K_dev_429
under state dependent pricing	purpose	2K_dev_429
However no explicit expression for this threshold has been found	purpose	2K_dev_429
for the ( maximum ) revenue under this optimal threshold	purpose	2K_dev_429
Clustering is one of the fundamental data mining tasks	background	2K_dev_434
we show the strengths of our novel clustering technique	finding	2K_dev_434
In this work we present a Bayesian framework We exploit the ideas of subspace clustering where the relevance of dimensions might be different for each cluster	mechanism	2K_dev_434
Combining the relevance of the dimensions with the cluster membership degree of the objects	mechanism	2K_dev_434
we propose a novel type of mixture model able to represent data containing mixed membership subspace clusters	mechanism	2K_dev_434
we develop an efficient algorithm based on variational inference allowing easy parallelization	mechanism	2K_dev_434
In our empirical study on synthetic and real data	method	2K_dev_434
While traditional clustering techniques assign each object to a single cluster only	purpose	2K_dev_434
in many applications it has been observed that objects might belong to multiple clusters with different degrees	purpose	2K_dev_434
to tackle the challenge of mixed membership clustering for vector data For learning our model	purpose	2K_dev_434
We use exponential start time clustering Previous algorithms usually rely on graph decomposition routines with strict restrictions on the diameters of the decomposed pieces	mechanism	2K_dev_443
We weaken these bounds in favor of stronger local probabilistic guarantees	mechanism	2K_dev_443
This allows more direct analyses of the overall process	mechanism	2K_dev_443
giving : Linear work parallel algorithms that construct spanners with O ( k ) stretch and size O ( n 1+1/ k ) in unweighted graphs	mechanism	2K_dev_443
and size O ( n 1+1/ k log k ) in weighted graphs Hopsets that lead to the first parallel algorithm for approximating shortest paths in undirected graphs with O ( m poly log n ) work	mechanism	2K_dev_443
to design faster parallel graph algorithms involving distances	purpose	2K_dev_443
Disparity tuning measured in the primary visual cortex ( V1 ) is described well by the disparity energy model	background	2K_dev_445
but not all aspects of disparity tuning are fully explained by the model	background	2K_dev_445
Our model predicted sharper disparity tuning for larger stimuli	finding	2K_dev_445
In this case our model predicted reduced sharpening and strength of inverted disparity tuning	finding	2K_dev_445
the dynamics of disparity tuning observed from the neurophysiological recordings in macaque V1 matched model simulation predictions	finding	2K_dev_445
Overall the results of this study support the notion that	finding	2K_dev_445
while the disparity energy model provides a primary account of disparity tuning in V1 neurons	finding	2K_dev_445
neural disparity processing in V1 neurons is refined by recurrent interactions among elements in the neural circuit	finding	2K_dev_445
Here we propose a neuronal circuit model with recurrent connections The model is based on recurrent connections inferred from neurophysiological observations on spike timing correlations	mechanism	2K_dev_445
and is in good accord with existing data on disparity tuning dynamics	mechanism	2K_dev_445
We further performed two additional experiments to test predictions of the model	method	2K_dev_445
First we increased the size of stimuli to drive more neurons and provide a stronger recurrent input Second	method	2K_dev_445
we displayed anti-correlated stereograms	method	2K_dev_445
where dots of opposite luminance polarity are matched between the left- and right-eye images and result in inverted disparity tuning in the disparity energy model	method	2K_dev_445
Such deviations from the disparity energy model provide us with insight into how network interactions may play a role in disparity processing and help to solve the stereo correspondence problem	purpose	2K_dev_445
that provides a simple account of the observed deviations	purpose	2K_dev_445
Machine-learning ( ML ) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices	background	2K_dev_459
making medical diagnoses and facial recognition	background	2K_dev_459
In a model inversion attack	background	2K_dev_459
recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al	background	2K_dev_459
adversarial access to an ML model is abused to learn sensitive genomic information about individuals	background	2K_dev_459
show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and	finding	2K_dev_459
in the other context	finding	2K_dev_459
show how to recover recognizable images of people 's faces given only their name and access to the ML model The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility	finding	2K_dev_459
We develop a new class of model inversion attack that exploits confidence values revealed along with predictions	mechanism	2K_dev_459
Our new attacks are applicable in a variety of settings	mechanism	2K_dev_459
and we explore two in depth : decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition	mechanism	2K_dev_459
In both cases confidence values are revealed to those with the ability to make prediction queries to models	mechanism	2K_dev_459
We experimentally We also initiate experimental exploration of natural countermeasures	method	2K_dev_459
investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning	method	2K_dev_459
as well as revealing only rounded confidence values	method	2K_dev_459
Whether model inversion attacks apply to settings outside theirs	purpose	2K_dev_459
Given a large graph	background	2K_dev_462
like who-calls-whom or who-likes-whom	background	2K_dev_462
what behavior is normal and what should be surprising	background	2K_dev_462
possibly due to fraudulent activity ? How do graphs evolve over time ? How does influence/news/viruses propagate	background	2K_dev_462
over time ? We conclude with some open research questions for graph mining	background	2K_dev_462
as well as some discoveries such settings	finding	2K_dev_462
For the third we show that for virus propagation	finding	2K_dev_462
a single number is enough to characterize the connectivity of graph	finding	2K_dev_462
For the first we present a list of static and temporal laws	mechanism	2K_dev_462
including advances patterns like 'eigenspokes ' ; we show how to use them to spot suspicious activities	mechanism	2K_dev_462
in on-line buyer-and-seller settings	mechanism	2K_dev_462
in FaceBook in twitter-like networks	mechanism	2K_dev_462
For the second we show how to handle time-evolving graphs as tensors	mechanism	2K_dev_462
how to handle large tensors in map-reduce environments	mechanism	2K_dev_462
thus we show how to do efficient immunization for almost any type of virus ( SIS - no immunity ; SIR - lifetime immunity ; etc )	mechanism	2K_dev_462
We focus on three topics : ( a ) anomaly detection in large static graphs ( b ) patterns and anomalies in large time-evolving graphs and ( c ) cascades and immunization	purpose	2K_dev_462
We show that offload shaping can produce significant reduction in resource demand	finding	2K_dev_466
with little loss of application-level fidelity	finding	2K_dev_466
When offloading computation from a mobile device	mechanism	2K_dev_466
we show that it can pay to perform additional on-device work We call this offload shaping	mechanism	2K_dev_466
demonstrate its application at many different levels of abstraction using a variety of techniques	method	2K_dev_466
in order to reduce the offloading workload	purpose	2K_dev_466
These interrelated issues underpin advanced correctness analysis in models of structured communications	background	2K_dev_468
we prove that all proof conversions induced by the logic interpretation actually express observational equivalences	finding	2K_dev_468
and explain how type isomorphisms resulting from linear logic equivalences are realized by coercions between interface types of session-based concurrent systems	finding	2K_dev_468
by developing a theory of logical relations	mechanism	2K_dev_468
Defined upon a linear type structure	mechanism	2K_dev_468
our logical relations remain remarkably similar to those for functional languages	mechanism	2K_dev_468
We also introduce a natural notion of observational equivalence Strong normalization and confluence come in handy in the associated coinductive reasoning :	mechanism	2K_dev_468
The starting point for our study is an interpretation of linear logic propositions as session types for communicating processes	method	2K_dev_468
proposed in prior work	method	2K_dev_468
We investigate strong normalization	purpose	2K_dev_468
confluence and behavioral equality in the realm of session-based concurrency	purpose	2K_dev_468
Strong normalization and confluence are established for session-typed processes	purpose	2K_dev_468
Given a large social graph	background	2K_dev_469
what can we say about its robustness ? Broadly speaking	background	2K_dev_469
the property of robustness is crucial in real graphs	background	2K_dev_469
since it is related to the structural behavior of graphs to retain their connectivity properties after losing a portion of their edges/nodes	background	2K_dev_469
Can we estimate a robustness index for a graph quickly ? Additionally	background	2K_dev_469
if the graph evolves over time	background	2K_dev_469
how this property changes ?	background	2K_dev_469
and we observe interesting properties for both static and time-evolving social graphs	finding	2K_dev_469
we show how to spot outliers and anomalies in graphs over time	finding	2K_dev_469
Finally we examine how graph generating models that mimic several properties of real-world graphs and behave in terms of robustness dynamics	finding	2K_dev_469
First we present a measure that characterizes the robustness properties of a graph and also serves as global measure of the community structure ( or lack thereof )	mechanism	2K_dev_469
We show how to compute this measure efficiently by exploiting the special spectral properties of real-world networks	mechanism	2K_dev_469
We apply our method on several diverse real networks with millions of nodes	method	2K_dev_469
As an application example	method	2K_dev_469
In this work we are trying to answer the above questions studying the expansion properties of large social graphs	purpose	2K_dev_469
User provided rating data about products and services is one key feature of websites such as Amazon	background	2K_dev_470
Since these ratings are rather static but might change over time	background	2K_dev_470
a temporal analysis of rating distributions provides deeper insights into the evolution of a products ' quality	background	2K_dev_470
and we present interesting findings	finding	2K_dev_470
we model the base behavior of users regarding a product as a latent multivariate autoregressive process	mechanism	2K_dev_470
This latent behavior is mixed with a sparse anomaly signal finally leading to the observed data We propose an efficient algorithm	mechanism	2K_dev_470
on various real world datasets	method	2K_dev_470
Given a time-series of rating distributions	purpose	2K_dev_470
in this work we answer the following questions : ( 1 ) How to detect the base behavior of users regarding a product 's evaluation over time ? ( 2 ) How to detect points in time where the rating distribution differs from this base behavior	purpose	2K_dev_470
due to attacks or spontaneous changes in the product 's quality ? To achieve these goals solving our objective	purpose	2K_dev_470
QuMinS scales linearly on the data size	finding	2K_dev_476
being up to 40 times faster than top competitors ( GCap )	finding	2K_dev_476
still achieving better or equal accuracy	finding	2K_dev_476
it spots images that potentially require unpredicted labels	finding	2K_dev_476
and it works even with tiny initial label sets	finding	2K_dev_476
nearly five examples to show that QuMinS is a viable tool for automatic coffee crop detection from remote sensing images	finding	2K_dev_476
Specifically we propose QuMinS	mechanism	2K_dev_476
a fast scalable - given an image set	mechanism	2K_dev_476
very few images have labels in the same setting	mechanism	2K_dev_476
find clusters the top-N '' O outlier images	mechanism	2K_dev_476
and the N '' R images	mechanism	2K_dev_476
Experiments on satellite images spanning up to 2	method	2K_dev_476
25 GB show that	method	2K_dev_476
contrasting to the state-of-the-art labeling techniques	method	2K_dev_476
We also report a case study of our method 's practical usage	method	2K_dev_476
Given a large image set	purpose	2K_dev_476
in which very few images have labels	purpose	2K_dev_476
how to guess labels for the remaining majority ? How to spot images that need brand new labels different from the predefined ones ? How to summarize these data to route the user 's attention to what really matters ? Here we answer all these questions	purpose	2K_dev_476
solution to two problems : ( i ) Low-labor labeling ( LLL )	purpose	2K_dev_476
find the most appropriate labels for the rest ; and ( ii ) Mining and attention routing - that best represent the data	purpose	2K_dev_476
by extending todays unmodified cloud to a second level consisting of self-managed data centers with no hard state called cloudlets These are located at the edge of the Internet	mechanism	2K_dev_477
just one wireless hop away from associated mobile devices By leveraging lowlatency offload	mechanism	2K_dev_477
cloudlets enable a new class of real-time cognitive assistive applications on wearable devices By processing high data rate sensor inputs such as video close to the point of capture	mechanism	2K_dev_477
cloudlets can reduce ingress bandwidth demand into the cloud By serving as proxies for distant cloud services that are unavailable due to failures or cyberattacks	mechanism	2K_dev_477
cloudlets can improve robustness and availability	mechanism	2K_dev_477
We caution that proprietary software ecosytems surrounding cloudlets will lead to a fragmented marketplace that fails to realize the full business potential of mobile-cloud convergence	mechanism	2K_dev_477
Instead we urge that the software ecosystem surrounding cloudlets be based on the same principles of openness and end-to-end design that have made the Internet so successful	mechanism	2K_dev_477
We show how a disruptive force in mobile computing can be created	purpose	2K_dev_477
A descending ( multi-item ) clock auction ( DCA ) is a mechanism for buying items from multiple potential sellers	background	2K_dev_487
In the DCA bidder-specific prices are decremented over the course of the auction	background	2K_dev_487
In each round each bidder might accept or decline his offer price	background	2K_dev_487
Accepting means the bidder is willing to sell at that price	background	2K_dev_487
Rejecting means the bidder will not sell at that price or a lower price DCAs have been proposed as the method for procuring spectrum from existing holders in the FCC 's imminent incentive auctions so spectrum can be repurposed to higher-value uses	background	2K_dev_487
An unexpected paradox about DCAs is that sometimes when the number of rounds allowed increases	finding	2K_dev_487
the final payment increases	finding	2K_dev_487
We provide an explanation for this	finding	2K_dev_487
We develop a percentile-based approach which provides a means We also develop an optimization model for setting prices so as while stochastically satisfying the feasibility constraint	mechanism	2K_dev_487
( The DCA has a final adjustment round that obtains feasibility after feasibility has been lost in the final round of the main DCA	mechanism	2K_dev_487
) We prove attractive properties of this	mechanism	2K_dev_487
such as symmetry and monotonicity	mechanism	2K_dev_487
We develop computational methods for solving the model	mechanism	2K_dev_487
( We also develop optimization models with recourse	mechanism	2K_dev_487
but they are not computationally practical	mechanism	2K_dev_487
We present experiments both on the homogeneous items case and the case of FCC incentive auctions	method	2K_dev_487
where we use real interference constraint data to get a fully faithful model of feasibility	method	2K_dev_487
However the DCA design has lacked a way to determine the prices to offer the bidders in each round	purpose	2K_dev_487
This is a recognized	purpose	2K_dev_487
important and timely problem	purpose	2K_dev_487
We present to our knowledge	purpose	2K_dev_487
the first techniques for this	purpose	2K_dev_487
to naturally reduce the offer prices to the bidders through the bidding rounds to minimize expected payment	purpose	2K_dev_487
We show an algorithm Our approach follows the recursive preconditioning framework	mechanism	2K_dev_489
which aims to reduce graphs to trees using iterative methods	mechanism	2K_dev_489
We improve two key components of this framework : random sampling and tree embeddings	mechanism	2K_dev_489
Both of these components are used in a variety of other algorithms	mechanism	2K_dev_489
and our approach also extends to the dual problem of computing electrical flows	mechanism	2K_dev_489
We show that preconditioners constructed by random sampling can perform well without meeting the standard requirements of iterative methods	mechanism	2K_dev_489
In the graph setting	mechanism	2K_dev_489
this leads to ultra-sparsifiers that have optimal behavior in expectation The improved running time makes previous low stretch embedding algorithms the running time bottleneck in this framework In our analysis	mechanism	2K_dev_489
we relax the requirement of these embeddings to snowflake spaces	mechanism	2K_dev_489
We then obtain a two-pass approach algorithm This algorithm is also readily parallelizable	mechanism	2K_dev_489
for solving symmetric diagonally dominant ( SDD ) linear systems with m non-zero entries to a relative error of e in O ( m log 1/2 n log c n log ( 1/ e ) ) time	purpose	2K_dev_489
for constructing optimal embeddings in snowflake spaces that runs in O ( m log log n ) time	purpose	2K_dev_489
Given a real world graph	background	2K_dev_495
how should we lay-out its edges ? How can we compress it ? These questions are closely related	background	2K_dev_495
and the typical approach so far is to find clique-like communities	background	2K_dev_495
like the cavemen graph	background	2K_dev_495
we show that SlashBurn consistently outperforms other methods for all data sets	finding	2K_dev_495
resulting in better compression and faster running time	finding	2K_dev_495
Moreover we show that SlashBurn with the appropriate spokes ordering can further improve compression while hardly sacrificing the running time	finding	2K_dev_495
Based on the idea	mechanism	2K_dev_495
we propose the SlashBurn method to recursively split a graph into hubs and spokes connected only by the hubs	mechanism	2K_dev_495
We also propose techniques to select the hubs and give an ordering to the spokes	mechanism	2K_dev_495
in addition to the basic SlashBurn	mechanism	2K_dev_495
We give theoretical analysis of the proposed hub selection methods	mechanism	2K_dev_495
Our view point has several advantages : ( a ) it avoids the no good cuts problem	mechanism	2K_dev_495
( b ) it gives better compression	mechanism	2K_dev_495
and ( c ) it leads to faster execution times for matrix-vector operations	mechanism	2K_dev_495
which are the back-bone of most graph processing tools	mechanism	2K_dev_495
We show that the block-diagonal mental image of the cavemen graph is the wrong paradigm	purpose	2K_dev_495
in full agreement with earlier results that real world graphs have no good cuts Instead	purpose	2K_dev_495
we propose to envision graphs as a collection of hubs connecting spokes	purpose	2K_dev_495
with super-hubs connecting the hubs	purpose	2K_dev_495
and so on recursively	purpose	2K_dev_495
Our model is underpinned by two key concepts	mechanism	2K_dev_499
a structural graph model ( composite network ) and a viral propagation model ( SI 1 I 2 S )	mechanism	2K_dev_499
Using this framework we formulate a non-linear dynamic system and perform an eigenvalue analysis to identify the tipping point of the epidemic behavior Based on insights gained from this analysis	mechanism	2K_dev_499
we demonstrate an effective and accurate prediction method to determine viral dominance	mechanism	2K_dev_499
which we call the EigenPredictor	mechanism	2K_dev_499
Next using a combination of synthetic and real composite networks	method	2K_dev_499
we evaluate the effectiveness of various viral suppression techniques by either a ) concurrently suppressing both memes or b ) unilaterally suppressing a single meme while leaving the other relatively unaffected	method	2K_dev_499
In this paper we study the intertwined propagation of two competing `` memes '' ( or data	purpose	2K_dev_499
) in a composite network	purpose	2K_dev_499
Within the constraints of this scenario	purpose	2K_dev_499
we ask two key questions : ( a ) which meme will prevail ? and ( b ) can one influence the outcome of the propagations ?	purpose	2K_dev_499
The Gates Hillman prediction market ( GHPM ) was an internet prediction market designed to predict the opening day of the Gates and Hillman Centers	background	2K_dev_500
the new computer science complex at Carnegie Mellon University Unlike a traditional continuous double auction format	background	2K_dev_500
the GHPM was mediated by an automated market maker	background	2K_dev_500
a central agent responsible for pricing transactions with traders over the possible opening days	background	2K_dev_500
The GHPMs event partition was	background	2K_dev_500
at the time the largest ever elicited in any prediction market by an order of magnitude	background	2K_dev_500
and dealing with the markets size required new advances	background	2K_dev_500
including a novel span-based elicitation interface that simplified interactions with the market maker	background	2K_dev_500
We use the large set of identity-linked trades generated by the GHPM	method	2K_dev_500
to examine issues of trader performance and market microstructure	purpose	2K_dev_500
including how the market both reacted to and anticipated official news releases about the buildings opening day	purpose	2K_dev_500
We describe a new algorithm The running time of our algorithm is $ $ O ( f \log n \log \varDelta ) $ $ O ( flognlog ) where $ $ f $ $ f is the output complexity of the Voronoi diagram and $ $ \varDelta $ $ is the spread of the input	mechanism	2K_dev_503
the ratio of largest to smallest pairwise distances Despite the simplicity of the algorithm and its analysis	mechanism	2K_dev_503
it improves on the state of the art for all inputs with polynomial spread and near-linear output size	mechanism	2K_dev_503
The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation	mechanism	2K_dev_503
Then the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity	mechanism	2K_dev_503
yielding the output sensitive bound	mechanism	2K_dev_503
The removal only involves local flips and is inspired by kinetic data structures	mechanism	2K_dev_503
for computing the Voronoi diagram of a set of $ $ n $ $ n points in constant-dimensional Euclidean space	purpose	2K_dev_503
has an approximation ratio of 3/2 to the maximum cardinality matching	finding	2K_dev_507
This is an improvement over a recent upper bound of 2 ( Ashlagi et al	finding	2K_dev_507
2010 2 ] ) and	finding	2K_dev_507
furthermore our mechanism beats for the first time the lower bound on the approximation ratio of deterministic truthful mechanisms We complement our positive result with new lower bounds	finding	2K_dev_507
Among other statements we prove that the weaker incentive compatibility property of truthfulness in expectation in our mechanism is necessary ; universally truthful mechanisms that have an inclusion-maximality property have an approximation ratio of at least 2	finding	2K_dev_507
We study a mechanism design version of matching computation in graphs that models We present a new randomized matching mechanism for two agents which is truthful in expectation and	mechanism	2K_dev_507
the game played by hospitals participating in pairwise kidney exchange programs	purpose	2K_dev_507
The openness of wireless communication and the recent development of software-defined radio technology	background	2K_dev_510
respectively provide a low barrier and a wide range of capabilities for misbehavior	background	2K_dev_510
attacks and defenses against attacks	background	2K_dev_510
Matching our intuition the aggressiveness of an attacker is related to how much of a discount is placed on data delay	finding	2K_dev_510
This results in the defender often choosing to sleep despite the latency implication	finding	2K_dev_510
because the threat of jamming is high	finding	2K_dev_510
We also present several other findings	finding	2K_dev_510
In this work we present finite-energy jamming games	mechanism	2K_dev_510
a game model We also allow the jammer A major addition in finite-energy jamming games is that the jammer and sender both have a limited amount of energy which is drained according to the actions a player takes We develop a model of our system as a zero-sum finite-horizon stochastic game with deterministic transitions	mechanism	2K_dev_510
We leverage the zero-sum and finite-horizon properties of our model to design a simple polynomial-time algorithm to compute optimal randomized strategies for both players	mechanism	2K_dev_510
The utility function of our game model can be decoupled into a recursive equation	mechanism	2K_dev_510
Our algorithm exploits this fact to use dynamic programming to construct solutions in a bottom-up fashion	mechanism	2K_dev_510
For each state of energy levels	mechanism	2K_dev_510
a linear program is solved to find Nash equilibrium strategies for the subgame	mechanism	2K_dev_510
With these techniques our algorithm has only a linear dependence on the number of states	mechanism	2K_dev_510
and quadratic dependence on the number of actions	mechanism	2K_dev_510
allowing us to solve very large instances	mechanism	2K_dev_510
By computing Nash equilibria for our game models	mechanism	2K_dev_510
we explore what kind of performance guarantees can be achieved both for the sender and jammer	mechanism	2K_dev_510
when playing against an optimal opponent	mechanism	2K_dev_510
We also use the optimal strategies to simulate finite-energy jamming games and provide insights into robust communication among reconfigurable	mechanism	2K_dev_510
yet energy-limited radio systems	mechanism	2K_dev_510
To test the performance of the optimal strategies we compare their performance with a random and adaptive strategy from simulations where we vary the strategies for one or both of the players	method	2K_dev_510
that allows a jammer and sender to choose ( 1 ) whether to transmit or sleep	purpose	2K_dev_510
( 2 ) a power level to transmit with	purpose	2K_dev_510
and ( 3 ) what channel to transmit on	purpose	2K_dev_510
to choose on how many channels it simultaneously attacks	purpose	2K_dev_510
Traditional power system state estimation methods lack the ability to track and manage increasing uncertainties inherent in the new technologies	background	2K_dev_521
such as recent and ongoing massive penetration of renewable energy	background	2K_dev_521
distribution intelligence and plug-in electric vehicles	background	2K_dev_521
To deal with the inability	background	2K_dev_521
a recent work proposes to utilize the unused historical data for power system state estimation First	background	2K_dev_521
because the power systems are with periodic patterns	background	2K_dev_521
which create clustered measurement data	background	2K_dev_521
results show that the new method can dramatically reduce the necessary computational time for online data-driven state estimation	finding	2K_dev_521
while producing a highly accurate state estimate	finding	2K_dev_521
dimension reduction is proposed	mechanism	2K_dev_521
but still able to retrieve similar measurements the k-dimensional tree indexing approach is employed in step two resulting in a log-reduction over searching time	mechanism	2K_dev_521
Finally we verify the obtained historical power system states via AC power system model and the current measurements to filter out bad historical data	method	2K_dev_521
Although able to achieve much higher accuracy	purpose	2K_dev_521
the new approach is slow due to the burden by sequential similarity check over large volumes of high dimensional historical measurements	purpose	2K_dev_521
making it unsuitable for online services	purpose	2K_dev_521
This calls for a general approach to preprocess the historical data	purpose	2K_dev_521
In this paper we propose to achieve such a goal with three steps	purpose	2K_dev_521
to remove redundancy To further reduce the computational time to group the clustered power system data into a tree structure	purpose	2K_dev_521
A key idea in object-oriented programming is that objects encapsulate state and interact with each other by message exchange This perspective suggests a model of computation that is inherently concurrent ( to facilitate simultaneous message exchange ) and that accounts for the effect of message exchange on an object 's state ( to express valid sequences of state transitions )	background	2K_dev_524
we show that our language supports the typical patterns of object-oriented programming ( e	finding	2K_dev_524
encapsulation dynamic dispatch and subtyping ) while guaranteeing session fidelity in a concurrent setting In addition	finding	2K_dev_524
we show that our language facilitates new forms of expression ( e	finding	2K_dev_524
type-directed reuse internal choice )	finding	2K_dev_524
which are not available in current object-oriented languages	finding	2K_dev_524
We introduce an object-oriented programming language that has processes as its only objects and employs linear session types	mechanism	2K_dev_524
Based on various examples We have implemented our language in a prototype compiler	method	2K_dev_524
In this paper we show that such a model of computation arises naturally from session-based communication to express the protocols of message exchange and to reason about concurrency and state	purpose	2K_dev_524
The Pascaline was the first working mechanical calculator	background	2K_dev_525
created in 1642 by the French polymath Blaise Pascal	background	2K_dev_525
Over the next two decades Pascal built 40 of these machines	background	2K_dev_525
of which nine survive today	background	2K_dev_525
Several good web resources describe the Pascaline	background	2K_dev_525
but to properly appreciate the sautoir	background	2K_dev_525
Pascal 's kinetic energy solution to jam-free ripple carry	background	2K_dev_525
building a working replica is invaluable	background	2K_dev_525
The Pascaline kit designed in SolidWorks	finding	2K_dev_525
is open source and available at http : //www	finding	2K_dev_525
I 've created a Pascaline kit using laser-cut acrylic and standard fasteners that can be assembled with just a screwdriver	mechanism	2K_dev_525
pliers and Loctite High school or college students with minimal skills can put it together in a few hours and have a functioning calculator	mechanism	2K_dev_525
Exploring the Pascaline 's design is an engaging way to connect a milestone in the early history of computing with more modern theoretical concepts	mechanism	2K_dev_525
Students can investigate questions such as : What makes a device `` digital '' ? ( Slide rules have numeric scales but are analog devices	mechanism	2K_dev_525
) How does nonlinearity produce discrete states in a continuous world ? How are nonlinearities induced in the Pascaline vs	mechanism	2K_dev_525
in digital electronics ? How do the logic design concepts `` half adder '' and `` full adder '' map onto the components of the Pascaline ? Is the Pascaline really adding	mechanism	2K_dev_525
or merely counting ? How does the Pascaline use nines complement arithmetic to perform subtraction	mechanism	2K_dev_525
and why is n't it tens complement ?	mechanism	2K_dev_525
Thanks to the growing availability of rapid prototyping tools	purpose	2K_dev_525
it has become relatively easy for CS educators to fabricate physical artifacts to help students explore computational ideas	purpose	2K_dev_525
User review is a crucial component of open mobile app markets such as the Google Play Store	background	2K_dev_526
How do we automatically summarize millions of user reviews and make sense out of them ? We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users	background	2K_dev_526
Results using our techniques are reported	finding	2K_dev_526
In this paper we propose Wiscom	mechanism	2K_dev_526
a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail	mechanism	2K_dev_526
Our system is able to ( a ) discover inconsistencies in reviews ; ( b ) identify reasons why users like or dislike a given app	mechanism	2K_dev_526
and provide an interactive	mechanism	2K_dev_526
zoomable view of how users ' reviews evolve over time ; and ( c ) provide valuable insights into the entire app market	mechanism	2K_dev_526
identifying users ' major concerns and preferences of different types of apps	mechanism	2K_dev_526
on a 32GB dataset consisting of over 13 million user reviews of 171	method	2K_dev_526
493 Android apps in the Google Play Store	method	2K_dev_526
Unfortunately beyond simple summaries such as histograms of user ratings	purpose	2K_dev_526
there are few analytic tools that can provide insights into user reviews	purpose	2K_dev_526
With the advancement of information systems	background	2K_dev_537
means of communications are becoming cheaper	background	2K_dev_537
faster and more available	background	2K_dev_537
Today millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want	background	2K_dev_537
They can access their e-mails	background	2K_dev_537
comment on weblogs watch and post videos and photos ( as well as comment on them )	background	2K_dev_537
and make phone calls or text messages almost ubiquitously	background	2K_dev_537
We also show three potential applications of the SFP : as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications	background	2K_dev_537
as a technique to detect anomalies	background	2K_dev_537
and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems	background	2K_dev_537
Moreover we propose the use of the Self-Feeding Process ( SFP ) The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data	mechanism	2K_dev_537
we analyzed eight different datasets from real and modern communication data and found four well-defined patterns seen in all the eight datasets	method	2K_dev_537
Given this scenario in this article	purpose	2K_dev_537
we tackle a fundamental aspect of this new era of communication : How the time intervals between communication events behave for different technologies and means of communications Are there universal patterns for the Inter-Event Time Distribution ( IED ) q How do inter-event times behave differently among particular technologiesq To answer these questions to generate inter-event times between communications	purpose	2K_dev_537
demonstrate that AutoPlait does indeed detect meaningful patterns correctly	finding	2K_dev_540
and it outperforms state-of-the-art competitors as regards accuracy and speed : AutoPlait achieves near-perfect	finding	2K_dev_540
over 95 % precision and recall	finding	2K_dev_540
and it is up to 472 times faster than its competitors	finding	2K_dev_540
In this paper we present AutoPlait	mechanism	2K_dev_540
a fully automatic mining algorithm Our method has the following properties : ( a ) effectiveness : it operates on large collections of time-series	mechanism	2K_dev_540
and finds similar segment groups that agree with human intuition ; ( b ) scalability : it is linear with the input size	mechanism	2K_dev_540
and thus scales up very well ; and ( c ) AutoPlait is parameter-free	mechanism	2K_dev_540
and requires no user intervention	mechanism	2K_dev_540
no prior training and no parameter tuning	mechanism	2K_dev_540
Extensive experiments on 67GB of real datasets	method	2K_dev_540
Given a large collection of co-evolving multiple time-series	purpose	2K_dev_540
which contains an unknown number of patterns of different durations	purpose	2K_dev_540
how can we efficiently and effectively find typical patterns and the points of variation ? How can we statistically summarize all the sequences	purpose	2K_dev_540
and achieve a meaningful segmentation ? for co-evolving time sequences	purpose	2K_dev_540
Can we identify patterns of temporal activities caused by human communications in social media ? Is it possible to model these patterns and tell if a user is a human or a bot based only on the timing of their postings ? Social media services allow users to make postings	background	2K_dev_542
generating large datasets of human activity time-stamps	background	2K_dev_542
and find that the distribution of postings inter-arrival times ( IAT ) is characterized by four patterns : ( i ) positive correlation between consecutive IATs	finding	2K_dev_542
( ii ) heavy tails	finding	2K_dev_542
( iii ) periodic spikes and ( iv ) bimodal distribution	finding	2K_dev_542
by showing that it can accurately fit real time-stamp data from Reddit and Twitter	finding	2K_dev_542
We also show that RSC can be used to spot outliers and detect users with non-human behavior	finding	2K_dev_542
RSC consistently provides a better fit to real data and clearly outperform existing models for human dynamics	finding	2K_dev_542
RSC was also able to detect bots with a precision higher than 94 %	finding	2K_dev_542
Based on our findings	mechanism	2K_dev_542
we propose Rest-Sleep-and-Comment ( RSC )	mechanism	2K_dev_542
We demonstrate the utility of RSC We validate RSC using real data consisting of over 35 million postings from Twitter and Reddit	method	2K_dev_542
In this paper we analyze time-stamp data from social media services that is able to match all four discovered patterns	purpose	2K_dev_542
These theoretical results have direct practical implications	background	2K_dev_548
we show that such allocations may not exist	finding	2K_dev_548
but allocations guaranteeing each player 2/3 of the above value always exist	finding	2K_dev_548
and can be computed in polynomial time when the number of players is constant	mechanism	2K_dev_548
Assuming additive valuation functions	method	2K_dev_548
We consider the problem of fairly allocating indivisible goods	purpose	2K_dev_548
focusing on a recently-introduced notion of fairness called maximin share guarantee : Each player 's value for his allocation should be at least as high as what he can guarantee by dividing the items into as many bundles as there are players and receiving his least desirable bundle	purpose	2K_dev_548
In 1876 Charles Lutwidge Dodgson suggested the intriguing voting rule that today bears his name Although Dodgsons rule is one of the most well-studied voting rules	background	2K_dev_562
it suffers from serious deficiencies	background	2K_dev_562
both from the computational point of viewit is NP-hard even to approximate the Dodgson score within sublogarithmic factorsand from the social choice point of viewit fails basic social choice desiderata such as monotonicity and homogeneity	background	2K_dev_562
Furthermore we show that a slight variation on a known voting rule yields a monotonic	finding	2K_dev_562
homogeneous polynomial-time O ( m log m ) -approximation algorithm and establish that it is impossible to achieve a better approximation ratio even if one just asks for homogeneity we prove that algorithms with an approximation ratio that depends only on m do not exist	finding	2K_dev_562
We design a monotonic exponential-time algorithm that yields a 2-approximation to the Dodgson score	mechanism	2K_dev_562
while matching this result with a tight lower bound We also present a monotonic polynomial-time O ( log m ) -approximation algorithm ( where m is the number of alternatives ) ; this result is tight as well due to a complexity-theoretic lower bound	mechanism	2K_dev_562
We complete the picture by studying several additional social choice properties ; for these properties	method	2K_dev_562
However this does not preclude the existence of approximation algorithms for Dodgson that are monotonic or homogeneous	purpose	2K_dev_562
and indeed it is natural to ask whether such algorithms exist	purpose	2K_dev_562
In this article we give definitive answers to these questions	purpose	2K_dev_562
shows high accuracy in the detection of seed nodes	finding	2K_dev_563
in addition to the correct automatic identification of their number Moreover	finding	2K_dev_563
NetSleuth scales linearly in the number of nodes of the graph	finding	2K_dev_563
and give an efficient method called NetSleuth for the well-known susceptible-infected virus propagation model	mechanism	2K_dev_563
We propose to employ the minimum description length principle as the one by which we can most succinctly describe the infected graph	mechanism	2K_dev_563
We give an highly efficient algorithm given a snapshot	mechanism	2K_dev_563
Then given these seed nodes	mechanism	2K_dev_563
we show we can optimize the virus propagation ripple in a principled way by maximizing likelihood	mechanism	2K_dev_563
With all three combined	mechanism	2K_dev_563
NetSleuth can automatically identify the correct number of seed nodes	mechanism	2K_dev_563
as well as which nodes are the culprits	mechanism	2K_dev_563
Experimentation on our method	method	2K_dev_563
Given a snapshot of a large graph	purpose	2K_dev_563
in which an infection has been spreading for some time	purpose	2K_dev_563
can we identify those nodes from which the infection started to spread ? In other words	purpose	2K_dev_563
can we reliably tell who the culprits are ? In this paper	purpose	2K_dev_563
we answer this question affirmatively Essentially	purpose	2K_dev_563
we are after that set of seed nodes that best explain the given snapshot	purpose	2K_dev_563
to identify the best set of seed nodes and virus propagation ripple to identify likely sets of seed nodes	purpose	2K_dev_563
We introduce GOTCHAs ( Generating panOptic Turing Tests to Tell Computers and Humans Apart ) as A GOTCHA is a randomized puzzle generation protocol	mechanism	2K_dev_568
which involves interaction between a computer and a human	mechanism	2K_dev_568
Informally a GOTCHA should satisfy two key properties : ( 1 ) The puzzles are easy for the human to solve	mechanism	2K_dev_568
( 2 ) The puzzles are hard for a computer to solve even if it has the random bits used by the computer to generate the final puzzle -- - unlike a CAPTCHA [ 44 ]	mechanism	2K_dev_568
Our main theorem demonstrates that GOTCHAs can be used to mitigate the threat of offline dictionary attacks against passwords by ensuring that a password cracker must receive constant feedback from a human being while mounting an attack Finally	mechanism	2K_dev_568
we provide a candidate construction of GOTCHAs based on Inkblot images	mechanism	2K_dev_568
Our construction relies on the usability assumption that users can recognize the phrases that they originally used to describe each Inkblot image -- - a much weaker usability assumption than previous password systems based on Inkblots which required users to recall their phrase exactly	mechanism	2K_dev_568
We conduct a user study to evaluate the usability of our GOTCHA construction	method	2K_dev_568
We also generate a GOTCHA challenge where we encourage artificial intelligence and security researchers to try to crack several passwords protected with our scheme	method	2K_dev_568
a way of preventing automated offline dictionary attacks against user selected passwords	purpose	2K_dev_568
We consider an adaptive cruise control system in which based on position and velocity information received from other vehicles via V2V wireless communication If the vehicles follow each other at a close distance	mechanism	2K_dev_572
they have better wireless reception but collisions may occur when a follower car does not receive notice about the decelerations of the leader car fast enough to react before it is too late	mechanism	2K_dev_572
If the vehicles are farther apart	mechanism	2K_dev_572
they would have a bigger safety margin	mechanism	2K_dev_572
but the wireless communication drops out more often	mechanism	2K_dev_572
so that the follower car no longer receives what the leader car is doing	mechanism	2K_dev_572
In order to guarantee safety	mechanism	2K_dev_572
such a system must return control to the driver if it does not receive an update from a nearby vehicle within some timeout period	mechanism	2K_dev_572
The value of this timeout parameter encodes a tradeoff between the likelihood that an update is received and the maximum safe acceleration Combining formal verification techniques for hybrid systems with a wireless communication model	mechanism	2K_dev_572
we analyze how the expected efficiency of a provably-safe adaptive cruise control system is affected by the value of this timeout	method	2K_dev_572
control decisions are made	purpose	2K_dev_572
demonstrate the effectiveness and intuitiveness of our discovered patterns	finding	2K_dev_578
We propose TSum a method ordered by their `` representativeness It can decide both which these patterns are	mechanism	2K_dev_578
as well as how many are necessary to properly summarize the data	mechanism	2K_dev_578
Our main contribution is formulating a general framework	mechanism	2K_dev_578
TSum using compression principles	mechanism	2K_dev_578
TSum can easily accommodate different optimization strategies for selecting and refining patterns	mechanism	2K_dev_578
The discovered patterns can be used to both represent the data efficiently	mechanism	2K_dev_578
as well as interpret it quickly	mechanism	2K_dev_578
Given a table where rows correspond to records and columns correspond to attributes	purpose	2K_dev_578
we want to find a small number of patterns that succinctly summarize the dataset	purpose	2K_dev_578
For example given a set of patient records with several attributes each	purpose	2K_dev_578
how can we find ( a ) that the `` most representative '' pattern is	purpose	2K_dev_578
say ( male adult	purpose	2K_dev_578
* ) followed by ( *	purpose	2K_dev_578
child low-cholesterol ) etc ? that provides a sequence of patterns	purpose	2K_dev_578
High-data-rate sensors such as video cameras	background	2K_dev_584
are becoming ubiquitous in the Internet of Things	background	2K_dev_584
This article is part of a special issue on smart spaces	background	2K_dev_584
This article describes GigaSight	mechanism	2K_dev_584
with strong enforcement of privacy preferences and access controls The GigaSight architecture is a federated system of VM-based cloudlets that perform video analytics at the edge of the Internet	mechanism	2K_dev_584
thus reducing the demand for ingress bandwidth into the cloud	mechanism	2K_dev_584
Denaturing which is an owner-specific reduction in fidelity of video content to preserve privacy	mechanism	2K_dev_584
is one form of analytics on cloudlets	mechanism	2K_dev_584
Content-based indexing for search is another form of cloudlet-based analytics	mechanism	2K_dev_584
an Internet-scale repository of crowd-sourced video content	purpose	2K_dev_584
Computers have the potential to significantly extend the practice of popular music based on steady tempo and mostly determined form	background	2K_dev_587
We describe an approach to synchronization across media that We also address with repeats and other structures to an actual performance	mechanism	2K_dev_587
which can involve both flattening the score and rearranging it	mechanism	2K_dev_587
as is common in popular music	mechanism	2K_dev_587
Finally we illustrate the possibilities of the score as a bidirectional user interface in a real-time system for music performance	mechanism	2K_dev_587
allowing the user to direct the computer through a digitally displayed score	mechanism	2K_dev_587
and allowing the computer to indicate score position back to human performers	mechanism	2K_dev_587
There are significant challenges to overcome	purpose	2K_dev_587
however due to constraints including accurate timing based on beats and adherence to a form or structure despite possible changes that may occur	purpose	2K_dev_587
possibly even during performance	purpose	2K_dev_587
takes into account latency due to communication delays and audio buffering	purpose	2K_dev_587
the problem of mapping from a conventional score	purpose	2K_dev_587
There has been significant interest and progress recently in algorithms that solve regression problems involving tall and thin matrices in input sparsity time	background	2K_dev_589
Our results build upon the close connection between randomized matrix algorithms	background	2K_dev_589
iterative methods and graph sparsification	background	2K_dev_589
We show these iterative methods can be adapted Our approaches are based on computing the importances of the rows	mechanism	2K_dev_589
known as leverage scores	mechanism	2K_dev_589
in an iterative manner We show that alternating between computing a short matrix estimate and finding more accurate approximate leverage scores leads to a series of geometrically smaller instances	mechanism	2K_dev_589
This gives an algorithm whose runtime is input sparsity plus an overhead comparable to the cost of solving a regression problem on the smaller approximation	mechanism	2K_dev_589
Given a n * d matrix where n g d	purpose	2K_dev_589
these algorithms find an approximation with fewer rows	purpose	2K_dev_589
allowing one to solve a poly ( d ) sized problem instead	purpose	2K_dev_589
In practice the best performances are often obtained by invoking these routines in an iterative fashion to give theoretical guarantees comparable to and better than the current state of the art	purpose	2K_dev_589
Several researchers proposed using non-Euclidean metrics on point sets in Euclidean space for clustering noisy data	background	2K_dev_593
Almost always a distance function is desired that recognizes the closeness of the points in the same cluster	background	2K_dev_593
even if the Euclidean cluster diameter is large	background	2K_dev_593
which we call the nearest neighbor metric	mechanism	2K_dev_593
Given a point set P and a path	mechanism	2K_dev_593
t his metric is the integral of the distance to P along	mechanism	2K_dev_593
W e describe a ( 3 +e ) - approximation algorithm and a more intricate ( 1 + e ) -approximation algorithm to compute the nearest neighbor metric Both approximation algorithms work in near-linear time	mechanism	2K_dev_593
The former uses shortest paths on a sparse graph defined over the input points	mechanism	2K_dev_593
The latter uses a sparse sample of the ambient space	mechanism	2K_dev_593
to find good approximate geodesic paths	mechanism	2K_dev_593
There- fore it is preferred to assign smaller costs to the paths that stay close to the input points	purpose	2K_dev_593
In this paper we consider a natural metric with this property	purpose	2K_dev_593
We present an algorithm that The algorithm runs in time $ \tilde { O } ( ( m \log { n } + n\log^2 { n } ) \log ( 1/p ) )	mechanism	2K_dev_594
$ As a result	mechanism	2K_dev_594
we obtain an algorithm that on input of an $ n\times n $ symmetric diagonally dominant matrix $ A $ with $ m $ nonzero entries and a vector $ b $ computes a vector $ { x } $ satisfying $ || { x } -A^ { + } b||_A < \epsilon ||A^ { + } b||_A $	mechanism	2K_dev_594
in expected time $ \tilde { O } ( m\log^2 { n } \log ( 1/\epsilon ) )	mechanism	2K_dev_594
$ The solver is based on repeated applications of the incremental sparsifier that produces a chain of graphs which is then used as input to the recursive preconditioned Chebyshev iteration	mechanism	2K_dev_594
on input of an $ n $ -vertex $ m $ -edge weighted graph $ G $ and a value $ k $ produces an incremental sparsifier $ \hat { G } $ with $ n-1 + m/k $ edges	purpose	2K_dev_594
such that the relative condition number of $ G $ with $ \hat { G } $ is bounded above by $ \tilde { O } ( k\log^2 n ) $	purpose	2K_dev_594
with probability $ 1-p $ ( we use the $ \tilde { O } ( ) $ notation to hide a factor of at most $ ( \log\log n ) ^4 $ )	purpose	2K_dev_594
reveal the bottlenecks for video upload	finding	2K_dev_598
denaturing indexing and content-based search	finding	2K_dev_598
They also provide insight on how parameters such as frame rate and resolution impact scalability	finding	2K_dev_598
We propose a scalable Internet system from devices such as Google Glass	mechanism	2K_dev_598
Our hybrid cloud architecture	mechanism	2K_dev_598
GigaSight is effectively a Content Delivery Network ( CDN ) in reverse It achieves scalability by decentralizing the collection infrastructure using cloudlets based on virtual machines~ ( VMs )	mechanism	2K_dev_598
Based on time location	mechanism	2K_dev_598
and content privacy sensitive information is automatically removed from the video	mechanism	2K_dev_598
This process which we refer to as denaturing	mechanism	2K_dev_598
is executed in a user-specific VM on the cloudlet Users can perform content-based searches on the total catalog of denatured videos	mechanism	2K_dev_598
for continuous collection of crowd-sourced video	purpose	2K_dev_598
Suspicious graph patterns show up in many applications	background	2K_dev_600
from Twitter users who buy fake followers	background	2K_dev_600
manipulating the social network	background	2K_dev_600
to botnet members performing distributed denial of service attacks	background	2K_dev_600
disturbing the network traffic graph	background	2K_dev_600
CatchSync consistently outperforms existing competitors	finding	2K_dev_600
both in detection accuracy by 36 % on Twitter and 20 % on Tencent Weibo	finding	2K_dev_600
as well as in speed	finding	2K_dev_600
We propose a fast and effective method	mechanism	2K_dev_600
CatchSync which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior pattern	mechanism	2K_dev_600
because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority	mechanism	2K_dev_600
We introduce novel measures and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots Thanks to careful design	mechanism	2K_dev_600
CatchSync has the following desirable properties : ( a ) it is scalable to large datasets	mechanism	2K_dev_600
being linear on the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology	mechanism	2K_dev_600
without needing labeled data	mechanism	2K_dev_600
nor timing information etc	mechanism	2K_dev_600
while still capable of using side information	mechanism	2K_dev_600
We applied CatchSync on two large	method	2K_dev_600
real datasets 1-billion-edge Twitter social graph and 3-billion-edge Tencent Weibo social graph	method	2K_dev_600
and several synthetic ones	method	2K_dev_600
Given a directed graph of millions of nodes	purpose	2K_dev_600
how can we automatically spot anomalous	purpose	2K_dev_600
suspicious nodes judging only from their connectivity patterns ? to quantify both concepts ( `` synchronicity '' and `` normality '' )	purpose	2K_dev_600
that a uniform team	finding	2K_dev_602
consisting of multiple instances of any single agent	finding	2K_dev_602
must make a significant number of mistakes	finding	2K_dev_602
whereas a diverse team converges to perfection as the number of agents grows provide evidence for the effectiveness of voting when agents are diverse	finding	2K_dev_602
With teams of computer Go agents in mind	mechanism	2K_dev_602
we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning	mechanism	2K_dev_602
This model which furthermore	mechanism	2K_dev_602
apply randomized algorithms to evaluate alternatives and produce votes ( captured by the second-stage noise models )	mechanism	2K_dev_602
We analytically demonstrate Our experiments	method	2K_dev_602
which pit teams of computer Go agents against strong agents	method	2K_dev_602
We investigate the power of voting among diverse	purpose	2K_dev_602
randomized software agents allows us to reason about a collection of agents with different biases ( determined by the first-stage noise models )	purpose	2K_dev_602
Game-theoretic algorithms for physical security have made an impressive real-world impact These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game	background	2K_dev_614
where the attacker observes the defender 's strategy and best-responds	background	2K_dev_614
We design an algorithm	mechanism	2K_dev_614
by observing the attacker 's responses to randomized deployments of resources and learning his priorities In contrast to previous work	mechanism	2K_dev_614
our algorithm requires a number of queries that is polynomial in the representation of the game	mechanism	2K_dev_614
In order to build the game model	purpose	2K_dev_614
though the payoffs of potential attackers for various outcomes must be estimated ; inaccurate estimates can lead to significant inefficiencies that optimizes the defender 's strategy with no prior information	purpose	2K_dev_614
Most algorithmic matches in fielded kidney exchanges do not result in an actual transplant	background	2K_dev_617
We show that failure-aware kidney exchange can significantly increase the expected number of lives saved and show that this new solver scales well	finding	2K_dev_617
From the computational viewpoint	mechanism	2K_dev_617
we design a branch-and-price-based optimal clearing algorithm specifically	mechanism	2K_dev_617
( i ) in theory	method	2K_dev_617
on random graph models ; ( ii ) on real data from kidney exchange match runs between 2010 and 2012 ; ( iii ) on synthetic data generated via a model of dynamic kidney exchange on large simulated data	method	2K_dev_617
unlike prior clearing algorithms	method	2K_dev_617
In this paper we address the problem of cycles and chains in a proposed match failing after the matching algorithm has committed to them	purpose	2K_dev_617
for the probabilistic exchange clearing problem	purpose	2K_dev_617
Cloud offload is an important technique in mobile computing	background	2K_dev_618
VM-based cloudlets have been proposed as offload sites for the resource-intensive and latency-sensitive computations typically associated with mobile multimedia applications	background	2K_dev_618
we demonstrate a prototype system that is capable of provisioning a cloudlet with a non-trivial VM image in 10 seconds	finding	2K_dev_618
we describe just-in-time ( JIT ) provisioning of cloudlets under the control of an associated mobile device This speed is achieved through dynamic VM synthesis and a series of optimizations to aggressively reduce transfer costs and startup latency	mechanism	2K_dev_618
Using a suite of five representative mobile applications	method	2K_dev_618
Since cloud offload relies on precisely-configured back-end software	purpose	2K_dev_618
it is difficult to support at global scale across cloudlets in multiple domains To address this problem	purpose	2K_dev_618
PriorityMeister outperforms most recent reactive request scheduling approaches	finding	2K_dev_622
with more workloads satisfying latency SLOs at higher latency percentiles	finding	2K_dev_622
This paper describes PriorityMeister -- a system that employs a combination of per-workload priorities and rate limits	mechanism	2K_dev_622
even with bursty workloads	mechanism	2K_dev_622
PriorityMeister automatically and proactively configures workload priorities and rate limits across multiple stages ( e	mechanism	2K_dev_622
a shared storage stage followed by a shared network stage ) to meet end-to-end tail latency SLOs PriorityMeister is also robust to mis-estimation of underlying storage device performance and contains the effect of misbehaving workloads	mechanism	2K_dev_622
In real system experiments and under production trace workloads	method	2K_dev_622
Meeting service level objectives ( SLOs ) for tail latency is an important and challenging open problem in cloud computing infrastructures	purpose	2K_dev_622
The challenges are exacerbated by burstiness in the workloads to provide tail latency QoS for shared networked storage	purpose	2K_dev_622
Regret-based methods have largely been favored in practice	background	2K_dev_624
in spite of their theoretically inferior convergence rates	background	2K_dev_624
we find that mirror prox and the excessive gap technique outperform the prior regret-based methods for finding medium accuracy solutions	finding	2K_dev_624
In this paper we investigate the acceleration of first-order methods both theoretically and experimentally An important component of many first-order methods is a distance-generating function	mechanism	2K_dev_624
Motivated by this we investigate a specific distance-generating function	mechanism	2K_dev_624
namely the dilated entropy function	mechanism	2K_dev_624
over treeplexes which are convex polytopes that encompass the strategy spaces of perfect-recall extensive-form games	mechanism	2K_dev_624
We develop significantly stronger bounds on the associated strong convexity parameter	mechanism	2K_dev_624
In terms of extensive-form game solving	mechanism	2K_dev_624
this improves the convergence rate of several first-order methods by a factor of O ( ( # information sets depth M ) / ( 2 depth ) ) where M is the maximum value of the l 1 norm over the treeplex encoding the strategy spaces	mechanism	2K_dev_624
In order to instantiate stochastic mirror prox	mechanism	2K_dev_624
we develop a class of gradient sampling schemes for game trees	mechanism	2K_dev_624
Equipped with our distance-generating function and sampling scheme	mechanism	2K_dev_624
Experimentally we investigate the performance of three first-order methods ( the excessive gap technique	method	2K_dev_624
mirror prox and stochastic mirror prox ) and compare their performance to the regret-based algorithms	method	2K_dev_624
We study the problem of computing a Nash equilibrium in large-scale two-player zero-sum extensive-form games While this problem can be solved in polynomial time	purpose	2K_dev_624
first-order or regret-based methods are usually preferred for large games	purpose	2K_dev_624
How does a new startup drive the popularity of competing websites into oblivion like Facebook famously did to MySpace ? This question is of great interest to academics	background	2K_dev_625
technologists and financial investors alike	background	2K_dev_625
The resulting model not only accurately fits the observed Daily Active Users ( DAU ) of Facebook and its competitors but also predicts their fate four years into the future	finding	2K_dev_625
Our model provides new insights into what Nobel Laure- ate Herbert A	mechanism	2K_dev_625
Simon called the `` marketplace of attention	mechanism	2K_dev_625
'' which we recast as the attention-activity marketplace	mechanism	2K_dev_625
Our model design is further substantiated by user-level activity of 250	method	2K_dev_625
000 MySpace users obtained between 2004 and 2009	method	2K_dev_625
In this work we exploit the singular way in which Facebook wiped out the popularity of MySpace	purpose	2K_dev_625
Hi5 Friendster and Multiply to guide the design of a new popularity competition model	purpose	2K_dev_625
Cloud-sourced virtual appliances ( VAs ) have been touted as powerful solutions for many software maintenance	background	2K_dev_632
mobility backward compatibility and security challenges	background	2K_dev_632
supports fluid interaction even in challenging network conditions	finding	2K_dev_632
such as 4G LTE	finding	2K_dev_632
to create a VA cloud service More specifically	mechanism	2K_dev_632
we wish to support a YouTube-like streaming service for executable content	mechanism	2K_dev_632
such as games interactive books	mechanism	2K_dev_632
Users should be able to post	mechanism	2K_dev_632
browse through and interact with executable content swiftly and without long interruptions Intuitively	mechanism	2K_dev_632
this seems impossible ; the bandwidths	mechanism	2K_dev_632
latencies and costs of last-mile networks would be prohibitive given the sheer sizes of virtual machines ! Yet	mechanism	2K_dev_632
we show that a set of carefully crafted	mechanism	2K_dev_632
novel prefetching and streaming techniques can bring this goal surprisingly close to reality	mechanism	2K_dev_632
We show that vTube	method	2K_dev_632
a VA streaming system that incorporates our techniques	method	2K_dev_632
In this paper we ask whether it is possible that supports fluid	purpose	2K_dev_632
interactive user experience even over mobile networks	purpose	2K_dev_632
improving upon past bounds with convergence rates that depend logarithmically on the data dimension	finding	2K_dev_633
and demonstrating state-of-the-art performance on two real-world tasks	finding	2K_dev_633
This paper considers the sparse Gaussian conditional random field	mechanism	2K_dev_633
a discriminative extension of sparse inverse covariance estimation	mechanism	2K_dev_633
where we use convex methods The model has been proposed by multiple researchers within the past year	mechanism	2K_dev_633
yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems In this paper	mechanism	2K_dev_633
we make three contributions : 1 ) we develop a second-order active-set method which is several orders of magnitude faster than previously proposed optimization approaches for this problem	mechanism	2K_dev_633
2 ) we analyze the model from a theoretical standpoint	method	2K_dev_633
3 ) we apply the method to large-scale energy forecasting problems	method	2K_dev_633
to learn a high-dimensional conditional distribution of outputs given inputs	purpose	2K_dev_633
Previously it has been shown that	background	2K_dev_634
under some conditions on the distribution of votes	background	2K_dev_634
if the number of manipulators is o ( n )	background	2K_dev_634
where n is the number of voters	background	2K_dev_634
then the probability that a random profile is manipulable by the coalition goes to zero as the number of voters goes to infinity	background	2K_dev_634
whereas if the number of manipulators is ( n )	background	2K_dev_634
then the probability that a random profile is manipulable goes to one This result analytically validates recent empirical results	background	2K_dev_634
and suggests that deciding the coalitional manipulation problem may be of limited computational hardness in practice	background	2K_dev_634
and we show that as c goes from zero to infinity	finding	2K_dev_634
the limiting probability that a random profile is manipulable goes from zero to one in a smooth fashion	finding	2K_dev_634
there is a smooth phase transition between the two regimes	finding	2K_dev_634
Here we consider the critical window	mechanism	2K_dev_634
where a coalition has size cn	mechanism	2K_dev_634
We study the phase transition of the coalitional manipulation problem for generalized scoring rules	purpose	2K_dev_634
In a multimillion-node network of who-follows-whom like Twitter	background	2K_dev_637
since a high count of followers leads to higher profits	background	2K_dev_637
users have the incentive to boost their in-degree	background	2K_dev_637
Moreover we show it is effective	finding	2K_dev_637
we propose CatchSync which exploits two tell-tale signs of the suspicious behavior : ( a ) synchronized behavior : the zombie followers have extremely similar following behavior pattern	mechanism	2K_dev_637
because say they are generated by a script ; and ( b ) abnormal behavior : their behavior pattern is very different from the majority	mechanism	2K_dev_637
Our CatchSync introduces novel measures to quantify both concepts and catches the suspicious behavior	mechanism	2K_dev_637
in a real-world social network	method	2K_dev_637
Can we spot the suspicious following behavior	purpose	2K_dev_637
which may indicate zombie followers and suspicious followees ? To answer the above question	purpose	2K_dev_637
Recently data with complex characteristics such as epilepsy electroencephalography ( EEG ) time series has emerged	background	2K_dev_640
Epilepsy EEG data has special characteristics including nonlinearity	background	2K_dev_640
results show that when compared to previous methods	finding	2K_dev_640
the proposed method can forecast faster and accurately	finding	2K_dev_640
In this paper we propose a coercively adjusted autoregression ( CA-AR ) method that forecasts future values from a multivariable epilepsy EEG time series	mechanism	2K_dev_640
We use the technique of random coefficients	mechanism	2K_dev_640
which forcefully adjusts the coefficients with 1 and 1	mechanism	2K_dev_640
The fractal dimension is used to determine the order of the CA-AR model	mechanism	2K_dev_640
We applied the CA-AR method reflecting special characteristics of data to forecast the future value of epilepsy EEG data	mechanism	2K_dev_640
Therefore it is important to find a suitable forecasting method that covers these special characteristics	purpose	2K_dev_640
We report surprising patterns	finding	2K_dev_645
the most striking of which are : ( a ) the FIZZLE pattern	finding	2K_dev_645
excitement about Polly shows a power-law decay over time with ex- ponent of -1	finding	2K_dev_645
2 ; ( b ) the RENDEZVOUS pattern	finding	2K_dev_645
that obeys a power law ( we explain RENDEZVOUS in the text ) ; ( c ) the DISPERSION pattern	finding	2K_dev_645
we find that the more a person uses Polly	finding	2K_dev_645
the fewer friends he will use it with	finding	2K_dev_645
but in a reciprocal fashion	finding	2K_dev_645
Finally we also propose a generator of influence networks	mechanism	2K_dev_645
using data from the Polly telephone-based application	method	2K_dev_645
a large influence network of 72	method	2K_dev_645
000 people with about 173	method	2K_dev_645
000 in- teractions spanning 500MB of log data and 200 GB of audio data	method	2K_dev_645
When a free catchy application shows up	purpose	2K_dev_645
how quickly will people notify their friends about it ? Will the enthusiasm drop exponentially with time	purpose	2K_dev_645
or oscillate ? What other patterns emerge ? Here we answer these questions which generate networks that mimic our discovered patterns	purpose	2K_dev_645
We describe the architecture and prototype implementation of an assistive system based on Google Glass devices It combines the first-person image capture and sensing capabilities of Glass with remote processing to perform real-time scene interpretation	mechanism	2K_dev_649
The system architecture is multi-tiered It offers tight end-to-end latency bounds on compute-intensive operations	mechanism	2K_dev_649
while addressing concerns such as limited battery capacity and limited processing capability of wearable devices The system gracefully degrades services in the face of network failures and unavailability of distant architectural tiers	mechanism	2K_dev_649
for users in cognitive decline	purpose	2K_dev_649
Formal verification and validation play a crucial role in making cyber-physical systems ( CPS ) safe	background	2K_dev_652
Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained	background	2K_dev_652
including models of the controller and of the physical dynamics	background	2K_dev_652
In CPS models are essential ; but any model we could possibly build necessarily deviates from the real world	background	2K_dev_652
Overall ModelPlex generates provably correct monitor conditions that	finding	2K_dev_652
are provably guaranteed to imply that the offline safety verification results about the CPS model apply to the present run of the actual CPS implementation	finding	2K_dev_652
This article introduces ModelPlex	mechanism	2K_dev_652
a method ModelPlex provides correctness guarantees for CPS executions at runtime : it combines offline verification of CPS models with runtime validation of system executions for compliance with the model	mechanism	2K_dev_652
ModelPlex ensures in a provably correct way that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model If	mechanism	2K_dev_652
at some point the observed behavior no longer complies with the model so that offline verification results no longer apply	mechanism	2K_dev_652
ModelPlex initiates provably safe fallback actions	mechanism	2K_dev_652
assuming the system dynamics deviation is bounded This article	mechanism	2K_dev_652
furthermore develops a systematic technique by a correct-by-construction approach	mechanism	2K_dev_652
leading to verifiably correct runtime model validation	mechanism	2K_dev_652
if checked to hold at runtime	method	2K_dev_652
If the real system fits to the model	purpose	2K_dev_652
its behavior is guaranteed to satisfy the correctness properties verified with respect to the model	purpose	2K_dev_652
Otherwise all bets are off	purpose	2K_dev_652
ensuring that verification results about models apply to CPS implementations	purpose	2K_dev_652
to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic	purpose	2K_dev_652
Demand response has gained significant attention in recent years as it demonstrates potentials to enhance the power system 's operational flexibility in a cost-effective way	background	2K_dev_654
Industrial loads such as steel manufacturing plants consume large amounts of electric energy	background	2K_dev_654
and their electricity bills account for a remarkable percentage of their total operation cost	background	2K_dev_654
Meanwhile lots of industrial loads are very flexible in terms of adjusting their power consumption rate	background	2K_dev_654
through switching the transformer tap position	background	2K_dev_654
In this paper we focus on the steel plant and optimize its scheduling from both the energy and the spinning reserve markets	mechanism	2K_dev_654
Hence industrial loads such as the steel plants have both the motivation and the ability to support power system operation through demand response to maximize its profits	purpose	2K_dev_654
Given a graph with billions of nodes and edges	background	2K_dev_660
how can we find patterns and anomalies ? Are there nodes that participate in too many or too few triangles ? Are there close-knit near-cliques ? These questions are expensive to answer unless we have the first several eigenvalues and eigenvectors of the graph adjacency matrix	background	2K_dev_660
We report important discoveries about nearcliques and triangles on several real-world graphs	finding	2K_dev_660
including a snapshot of the Twitter social network ( 56 Gb	finding	2K_dev_660
2 billion edges ) and the YahooWeb data set	finding	2K_dev_660
one of the largest publicly available graphs ( 120 Gb	finding	2K_dev_660
4 billion nodes 6	finding	2K_dev_660
6 billion edges )	finding	2K_dev_660
with the proposed HEIGEN algorithm	mechanism	2K_dev_660
which we carefully design to be accurate	mechanism	2K_dev_660
efficient and able to run on the highly scalable MAPREDUCE ( HADOOP ) environment	mechanism	2K_dev_660
This enables HEIGEN to handle matrices more than 1 ; 000 larger than those which can be analyzed by existing algorithms	mechanism	2K_dev_660
We implement HEIGEN and run it on the M45 cluster	method	2K_dev_660
one of the top 50 supercomputers in the world	method	2K_dev_660
However eigensolvers suffer from subtle problems ( e	purpose	2K_dev_660
convergence ) for large sparse matrices	purpose	2K_dev_660
let alone for billion-scale ones	purpose	2K_dev_660
We address this problem	purpose	2K_dev_660
How can web services that depend on user generated content discern fraudulent input by spammers from legitimate input ? as well as potential extensions to anomaly detection problems in other domains	background	2K_dev_667
Finally we demonstrate and discuss the effectiveness of CopyCatch	finding	2K_dev_667
Our method which we refer to as CopyCatch	mechanism	2K_dev_667
by analyzing only the social graph between users and Pages and the times at which the edges in the graph ( the Likes ) were created We offer the following contributions : ( 1 ) We give a novel problem formulation	mechanism	2K_dev_667
with a simple concrete definition of suspicious behavior in terms of graph structure and edge constraints ( 2 ) We offer two algorithms - one provably-convergent iterative algorithm and one approximate	mechanism	2K_dev_667
scalable MapReduce implementation 3 ) We show that our method severely limits `` greedy attacks '' and analyze the bounds from the application of the Zarankiewicz problem to our setting CopyCatch is actively in use at Facebook	mechanism	2K_dev_667
searching for attacks on Facebook 's social graph of over a billion users	mechanism	2K_dev_667
many millions of Pages	mechanism	2K_dev_667
and billions of Page Likes	mechanism	2K_dev_667
at Facebook and on synthetic data	method	2K_dev_667
In this paper we focus on the social network Facebook and the problem of discerning ill-gotten Page Likes	purpose	2K_dev_667
made by spammers hoping to turn a profit	purpose	2K_dev_667
from legitimate Page Likes detects lockstep Page Like patterns on Facebook to find such suspicious lockstep behavior	purpose	2K_dev_667
Given a simple noun such as { \em apple }	background	2K_dev_669
and a question such as `` is it edible ? ``	background	2K_dev_669
what processes take place in the human brain ?	background	2K_dev_669
GeBM produces brain activity patterns that are strikingly similar to the real ones	finding	2K_dev_669
and the inferred functional connectivity is able to provide neuroscientific insights towards a better understanding of the way that neurons interact with each other	finding	2K_dev_669
as well as detect regularities and outliers in multi-subject brain activity measurements	finding	2K_dev_669
In this work we present a simple	mechanism	2K_dev_669
novel good-enough brain model	mechanism	2K_dev_669
or GeBM in short	mechanism	2K_dev_669
and a novel algorithm Sparse-SysId Moreover	mechanism	2K_dev_669
GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text )	mechanism	2K_dev_669
We evaluate GeBM by using both synthetic and real brain data	method	2K_dev_669
Using the real data	method	2K_dev_669
More specifically given the stimulus	purpose	2K_dev_669
what are the interactions between ( groups of ) neurons ( also known as functional connectivity ) and how can we automatically infer those interactions	purpose	2K_dev_669
given measurements of the brain activity ? Furthermore	purpose	2K_dev_669
how does this connectivity differ across different human subjects ? which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity	purpose	2K_dev_669
Given a network with attributed edges	background	2K_dev_670
how can we identify anomalous behavior ? Networks with edge attributes are ubiquitous	background	2K_dev_670
and capture rich information about interactions between nodes	background	2K_dev_670
: we show that EdgeCentric successfully spots numerous such anomalies where it achieved 0	finding	2K_dev_670
87 precision over the top 100 results	finding	2K_dev_670
Our work has a number of notable contributions	mechanism	2K_dev_670
including ( a ) formulation : while most other graph-based anomaly detection works use structural graph connectivity or node information	mechanism	2K_dev_670
we focus on the new problem of leveraging edge information	mechanism	2K_dev_670
( b ) methodology : we introduce EdgeCentric	mechanism	2K_dev_670
an intuitive and scalable compression-based approach and ( c ) practicality	mechanism	2K_dev_670
in several large edge-attributed real-world graphs	method	2K_dev_670
including the Flipkart e-commerce graph with over 3 million product reviews between 1	method	2K_dev_670
1 million users and 545 thousand products	method	2K_dev_670
In this paper we aim to utilize exactly this information to discern suspicious from typical behavior in an unsupervised fashion	purpose	2K_dev_670
lending well to the traditional scarcity of ground-truth labels in practical anomaly detection scenarios	purpose	2K_dev_670
for detecting edge-attributed graph anomalies	purpose	2K_dev_670
This paper explores a PAC ( probably approximately correct ) learning model in cooperative games	mechanism	2K_dev_672
Specifically we are given m random samples of coalitions and their values	mechanism	2K_dev_672
taken from some unknown cooperative game ; We also establish a novel connection between PAC learnability and core stability : for games that are efficiently learnable	mechanism	2K_dev_672
it is possible to find payoff divisions that are likely to be stable using a polynomial number of samples	mechanism	2K_dev_672
We study the PAC learnability of several well-known classes of cooperative games	method	2K_dev_672
such as network flow games	method	2K_dev_672
threshold task games and induced subgraph games	method	2K_dev_672
can we predict the values of unseen coalitions ?	purpose	2K_dev_672
There has been recent interest in applying Stackelberg games to infrastructure security	background	2K_dev_675
in which a defender must protect targets from attack by an adaptive adversary	background	2K_dev_675
In real-world security settings the adversaries are humans and are thus boundedly rational	background	2K_dev_675
We propose a new solution concept	mechanism	2K_dev_675
monotonic maximin We propose a mixed-integer linear program formulation We also consider top-monotonic maximin	mechanism	2K_dev_675
a related solution concept that is more conservative	mechanism	2K_dev_675
and propose a polynomial-time algorithm for top-monotonic maximin	mechanism	2K_dev_675
Most existing approaches for computing defender strategies against boundedly rational adversaries try to optimize against specific behavioral models of adversaries	purpose	2K_dev_675
and provide no quality guarantee when the estimated model is inaccurate	purpose	2K_dev_675
which provides guarantees against all adversary behavior models satisfying monotonicity	purpose	2K_dev_675
including all in the family of Regular Quantal Response functions for computing monotonic maximin	purpose	2K_dev_675
We present a new algorithm that produces in any dimension with guaranteed optimal output size We also provide an approximate Delaunay graph Our algorithm runs in expected time O ( 2 O ( d ) ( n log n + m ) )	mechanism	2K_dev_682
where n is the input size	mechanism	2K_dev_682
m is the output point set size	mechanism	2K_dev_682
and d is the ambient dimension The constants only depend on the desired element quality bounds	mechanism	2K_dev_682
To gain this new efficiency	mechanism	2K_dev_682
the algorithm approximately maintains the Voronoi diagram of the current set of points by storing a superset of the Delaunay neighbors of each point By retaining quality of the Voronoi diagram and avoiding the storage of the full Voronoi diagram	mechanism	2K_dev_682
a simple exponential dependence on d is obtained in the running time Thus	mechanism	2K_dev_682
if one only wants the approximate neighbors structure of a refined Delaunay mesh conforming to a set of input points	mechanism	2K_dev_682
the algorithm will return a size 2 O ( d ) m graph in 2 O ( d ) ( n log n + m ) expected time	mechanism	2K_dev_682
If m is superlinear in n	mechanism	2K_dev_682
then we can produce a hierarchically well-spaced superset of size 2 O ( d ) n in 2 O ( d ) n log n expected time	mechanism	2K_dev_682
a well-spaced superset of points conforming to a given input set on the output points	purpose	2K_dev_682
It is typically expected that if a mechanism is truthful	background	2K_dev_720
then the agents would	background	2K_dev_720
indeed truthfully report their private information	background	2K_dev_720
We wish to design truthful mechanisms that are simple	mechanism	2K_dev_720
that is whose Our approach involves three steps : ( i ) specifying the structure of mechanisms	mechanism	2K_dev_720
( ii ) constructing a verification algorithm	mechanism	2K_dev_720
and ( iii ) measuring the quality of verifiably truthful mechanisms	mechanism	2K_dev_720
We demonstrate this approach using a case study : approximate mechanism design without money for facility location	method	2K_dev_720
But why would an agent believe that the mechanism is truthful ? truthfulness can be verified efficiently ( in the computational sense )	purpose	2K_dev_720
that guarantees the new algorithm is safe for all possible inputs	finding	2K_dev_725
We then applied QdL to guide the development of a new algorithm	mechanism	2K_dev_725
We applied quantified differential-dynamic logic ( QdL ) We identified problems with the algorithm	method	2K_dev_725
proved that it was in general unsafe	method	2K_dev_725
and described exactly what could go wrong	method	2K_dev_725
Using \KeYmaeraD ( a tool that mechanizes QdL )	method	2K_dev_725
we created a machine-checked proof	method	2K_dev_725
to analyze a control algorithm designed to provide directional force feedback for a surgical robot	purpose	2K_dev_725
that provides safe operation along with directional force feedback	purpose	2K_dev_725
I010178 Complex software systems are becoming increasingly prevalent in aerospace applications : in particular	background	2K_dev_728
to accomplish critical tasks	background	2K_dev_728
Ensuring the safety of these systems is crucial	background	2K_dev_728
as they can have subtly different behaviors under slight variations in operating and proposals are given on how to address these issues	background	2K_dev_728
The challenges that naturally arise when applying such technology to industrial-scale applications is then detailed	finding	2K_dev_728
As an illustration of these techniques	method	2K_dev_728
a novel lateral midair collision-avoidance maneuver is studied in an ideal setting	method	2K_dev_728
without accounting for the uncertainties of the physical reality	method	2K_dev_728
This paper advocates the use of formal verification techniques and in particulartheoremprovingfor hybridsoftware-intensivesystemsasawell-foundedcomplementaryapproachtothe classical aerospace verification and validation techniques	purpose	2K_dev_728
such as testing or simulation	purpose	2K_dev_728
The convergence of mobile computing and cloud computing enables new multimedia applications that are both resource-intensive and interaction-intensive	background	2K_dev_731
For these applications end-to-end network bandwidth and latency matter greatly when cloud resources are used to augment the computational power and battery life of a mobile device	background	2K_dev_731
We then describe an architectural solution that is a seamless extension of today 's cloud computing infrastructure	mechanism	2K_dev_731
We first present quantitative evidence	method	2K_dev_731
that this crucial design consideration to meet interactive performance criteria limits data center consolidation	purpose	2K_dev_731
How can we tell when accounts are fake or real in a social network ? And how can we tell which accounts belong to liberal	background	2K_dev_734
conservative or centrist users ? Often	background	2K_dev_734
we can answer such questions and label nodes in a network based on the labels of their neighbors and appropriate assumptions of homophily ( `` birds of a feather flock together '' ) or heterophily ( `` opposites attract '' )	background	2K_dev_734
One of the most widely used methods for this kind of inference is Belief Propagation ( BP ) which iteratively propagates the information from a few nodes with explicit labels throughout a network until convergence	background	2K_dev_734
show that LinBP and SBP are orders of magnitude faster than standard BP	finding	2K_dev_734
while leading to almost identical node labels	finding	2K_dev_734
This paper introduces Linearized Belief Propagation ( LinBP )	mechanism	2K_dev_734
a linearization of BP via intuitive matrix equations and	mechanism	2K_dev_734
thus comes with exact convergence guarantees	mechanism	2K_dev_734
It handles homophily heterophily	mechanism	2K_dev_734
and more general cases that arise in multi-class settings	mechanism	2K_dev_734
Plus it allows a compact implementation in SQL	mechanism	2K_dev_734
The paper also introduces Single-pass Belief Propagation ( SBP )	mechanism	2K_dev_734
a localized ( or `` myopic '' ) version of LinBP and for which the final class assignments depend only on the nearest labeled neighbors In addition	mechanism	2K_dev_734
SBP allows fast incremental updates in dynamic networks	mechanism	2K_dev_734
A well-known problem with BP	purpose	2K_dev_734
however is that there are no known exact guarantees of convergence in graphs with loops that allows a closed-form solution that propagates information across every edge at most once	purpose	2K_dev_734
We show that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches	finding	2K_dev_736
In this paper we provide edge and set query algorithms that provably achieve some fraction of the omniscient optimal solution	mechanism	2K_dev_736
Our main theoretical result for the stochastic matching ( i	mechanism	2K_dev_736
2-set packing ) problem is the design of an adaptive algorithm that queries only a constant number of edges per vertex and achieves a ( 1-e ) fraction of the omniscient optimal solution	mechanism	2K_dev_736
for an arbitrarily small e > 0 Moreover	mechanism	2K_dev_736
this adaptive algorithm performs the queries in only a constant number of rounds	mechanism	2K_dev_736
We complement this result with a non-adaptive ( i	mechanism	2K_dev_736
one round of queries ) algorithm that achieves a ( 0	mechanism	2K_dev_736
5 - e ) fraction of the omniscient optimum	mechanism	2K_dev_736
We also extend both our results to stochastic k-set packing by designing an adaptive algorithm that achieves a ( 2/k - e ) fraction of the omniscient optimal solution	mechanism	2K_dev_736
again with only O ( 1 ) queries per element	mechanism	2K_dev_736
This guarantee is close to the best known polynomial-time approximation ratio of 3/k+1 -e for the deterministic k-set packing problem [ Furer 2013	mechanism	2K_dev_736
We empirically explore the application of ( adaptations of ) these algorithms to the kidney exchange problem	method	2K_dev_736
where patients with end-stage renal failure swap willing but incompatible donors on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange	method	2K_dev_736
The stochastic matching problem deals with finding a maximum matching in a graph whose edges are unknown but can be accessed via queries	purpose	2K_dev_736
This is a special case of stochastic k-set packing	purpose	2K_dev_736
where the problem is to find a maximum packing of sets	purpose	2K_dev_736
each of which exists with some probability	purpose	2K_dev_736
for these two problems	purpose	2K_dev_736
( 2 ) We provide proof of our model 's robustness to spam and anomalous behavior	finding	2K_dev_751
to demonstrate the model 's effectiveness in accurately predicting user 's ratings	finding	2K_dev_751
avoiding prediction skew in the face of injected spam	finding	2K_dev_751
and finding interesting patterns in real world ratings data	finding	2K_dev_751
In this paper we describe a unified Bayesian approach to Collaborative Filtering It models the discrete structure of ratings and is flexible to the often non-Gaussian shape of the distribution	mechanism	2K_dev_751
Additionally our method finds a co-clustering of the users and items	mechanism	2K_dev_751
which improves the model 's accuracy and makes the model robust to fraud	mechanism	2K_dev_751
We offer three main contributions : ( 1 ) We provide a novel model and Gibbs sampling algorithm that accurately models the quirks of real world ratings	mechanism	2K_dev_751
such as convex ratings distributions	mechanism	2K_dev_751
3 ) We use several real world datasets	method	2K_dev_751
Given a large dataset of users ' ratings of movies	purpose	2K_dev_751
what is the best model to accurately predict which movies a person will like ? And how can we prevent spammers from tricking our algorithms into suggesting a bad movie ? Is it possible to infer structure between movies simultaneously ? that accomplishes all of these goals	purpose	2K_dev_751
Standard approaches to stochastic discrete systems require numerical solutions for large optimization problems and quickly become infeasible with larger state spaces	background	2K_dev_753
Generalizations of these techniques to hybrid systems with stochastic effects are even more challenging It is in principle applicable to a variety of stochastic models from other domains	background	2K_dev_753
While the answer to the verification problem is not guaranteed to be correct	finding	2K_dev_753
we prove that Bayesian SMC can make the probability of giving a wrong answer arbitrarily small We show that our technique enables faster verification than state-of-the-art statistical techniques	finding	2K_dev_753
In particular we present a Statistical Model Checking ( SMC ) approach based on Bayesian statistics We show that our approach is feasible for a certain class of hybrid systems with stochastic transitions	mechanism	2K_dev_753
a generalization of Simulink/Stateflow models	mechanism	2K_dev_753
The SMC approach was pioneered by Younes and Simmons in the discrete and non-Bayesian case	mechanism	2K_dev_753
It solves the verification problem by combining randomized sampling of system traces ( which is very efficient for Simulink/Stateflow ) with hypothesis testing ( i	mechanism	2K_dev_753
testing against a probability threshold ) or estimation ( i	mechanism	2K_dev_753
computing with high probability a value close to the true probability )	mechanism	2K_dev_753
We believe SMC is essential for scaling up to large Stateflow/Simulink models	mechanism	2K_dev_753
The advantage is that answers can usually be obtained much faster than with standard	mechanism	2K_dev_753
exhaustive model checking techniques We emphasize that Bayesian SMC is by no means restricted to Stateflow/Simulink models	mechanism	2K_dev_753
We apply our Bayesian SMC approach to a representative example of stochastic discrete-time hybrid system models in Stateflow/Simulink : a fuel control system featuring hybrid behavior and fault tolerance	method	2K_dev_753
We address the problem of model checking stochastic systems	purpose	2K_dev_753
checking whether a stochastic system satisfies a certain temporal property with a probability greater ( or smaller ) than a fixed threshold	purpose	2K_dev_753
Which song will Smith listen to next ? Which restaurant will Alice go to tomorrow ? Which product will John click next	background	2K_dev_759
TribeFlow is more accurate and up to 413x faster than top competitors	finding	2K_dev_759
Mindful of these challenges we propose TribeFlow	mechanism	2K_dev_759
a method designed TribeFlow is a general method that can perform next product recommendation	mechanism	2K_dev_759
next song recommendation next location prediction	mechanism	2K_dev_759
and general arbitrary-length user trajectory prediction without domain-specific knowledge	mechanism	2K_dev_759
These applications have in common the prediction of user trajectories that are in a constant state of flux over a hidden network ( e	purpose	2K_dev_759
website links geographic location )	purpose	2K_dev_759
Moreover what users are doing now may be unrelated to what they will be doing in an hour from now to cope with the complex challenges of learning personalized predictive models of non-stationary	purpose	2K_dev_759
transient and time-heterogeneous user trajectories	purpose	2K_dev_759
Review fraud is a pervasive problem in online commerce	background	2K_dev_762
in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services	background	2K_dev_762
show that BIRDNEST successfully spots review fraud in large real-world graphs : the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart	finding	2K_dev_762
Hence in this paper	mechanism	2K_dev_762
we propose an approach which combines these 2 approaches in a principled manner	mechanism	2K_dev_762
allowing successful detection even when one of these signs is not present To combine these 2 approaches	mechanism	2K_dev_762
we formulate our Bayesian Inference for Rating Data ( BIRD ) model	mechanism	2K_dev_762
a flexible Bayesian model of user rating behavior	mechanism	2K_dev_762
Based on our model we formulate a likelihood-based suspiciousness metric	mechanism	2K_dev_762
Normalized Expected Surprise Total ( NEST )	mechanism	2K_dev_762
We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric	mechanism	2K_dev_762
Experiments on real data	method	2K_dev_762
Fake reviews are often detected based on several signs	purpose	2K_dev_762
including 1 ) they occur in short bursts of time ; 2 ) fraudulent user accounts have skewed rating distributions	purpose	2K_dev_762
However these may both be true in any given dataset	purpose	2K_dev_762
for detecting fraudulent reviews	purpose	2K_dev_762
The ap- proach can	background	2K_dev_771
generate nontrivial algebraic invariant equations capturing the airplane behavior during take-off or landing in longitudinal motion	background	2K_dev_771
by one polynomial and a finite set of its successive Lie derivatives	mechanism	2K_dev_771
This so-called differential radical characterization re- lies on a sound abstraction of the reachable set of solutions by the smallest variety that contains it	mechanism	2K_dev_771
The characterization leads to a differential radical invariant proof rule that is sound and complete	mechanism	2K_dev_771
which implies that invariance of algebraic equa- tions over real-closed fields is decidable Furthermore	mechanism	2K_dev_771
the problem of generating invariant varieties is shown to be as hard as minimizing the rank of a symbolic matrix	mechanism	2K_dev_771
and is therefore NP-hard	mechanism	2K_dev_771
We investigate symbolic linear algebra tools based on Gaussian elimination	method	2K_dev_771
We prove that any invariant algebraic set of a given polynomial vector field can be algebraically represented to efficiently automate the generation	purpose	2K_dev_771
When companies operate on the graphs with monetary incentives to sell Twitter `` Followers '' and Facebook page `` Likes ''	background	2K_dev_772
the graphs show strange connectivity patterns	background	2K_dev_772
We report strange deviations from typical patterns like smooth degree distributions	finding	2K_dev_772
We find that such deviations are often due to `` lockstep behavior '' that large groups of followers connect to the same groups of followees	finding	2K_dev_772
We discover that ( a ) the lockstep behaviors on the graph shape dense `` block '' in its adjacency matrix and creates `` rays '' in spectral subspaces	finding	2K_dev_772
and ( b ) partially overlapping of the behaviors shape `` staircase '' in its adjacency matrix and creates `` pearls '' in spectral subspaces The results demonstrate the scalability and effectiveness of our proposed algorithm	finding	2K_dev_772
The second contribution is that we provide a fast algorithm	mechanism	2K_dev_772
using the discovery as a guide for practitioners	mechanism	2K_dev_772
In this paper we study a complete graph from a large Twitter-style social network	method	2K_dev_772
spanning up to 3	method	2K_dev_772
Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep	method	2K_dev_772
We carry out extensive experiments on both synthetic and real datasets	method	2K_dev_772
as well as public datasets from IMDb and US Patent	method	2K_dev_772
Given multimillion-node graphs such as `` who-follows-whom ''	purpose	2K_dev_772
`` patent-cites-patent '' `` user-likes-page '' and `` actor/director-makes-movie '' networks	purpose	2K_dev_772
how can we find unexpected behaviors ? to detect users who offer the lockstep behaviors in undirected/directed/bipartite graphs	purpose	2K_dev_772
Revenue maximization in combinatorial auctions ( and other multidimensional selling settings ) is one of the most important and elusive problems in mechanism design	background	2K_dev_774
Such priors do not exist in most applications	background	2K_dev_774
Rather in many applications ( such as premium display advertising markets )	background	2K_dev_774
there is essentially a point prior	background	2K_dev_774
which may not be accurate	background	2K_dev_774
validate the approach and show that our techniques dramatically improve scalability over a leading general-purpose MIP solver	finding	2K_dev_774
In this paper we instead study a common revenue-enhancement approach - bundling - in the context of the most commonly studied combinatorial auction mechanism	mechanism	2K_dev_774
the Vickrey-Clarke-Groves ( VCG ) mechanism	mechanism	2K_dev_774
We adopt the point prior model	mechanism	2K_dev_774
and prove robustness to inaccuracy in the prior	mechanism	2K_dev_774
Then we present a branch-and-bound framework for finding the optimal bundling	mechanism	2K_dev_774
We introduce several techniques	mechanism	2K_dev_774
Experiments on CATS distributions	method	2K_dev_774
The optimal design is unknown	purpose	2K_dev_774
and is known to include features that are not acceptable in many applications	purpose	2K_dev_774
such as favoring some bidders over others and randomization A second challenge in mechanism design for combinatorial auctions is that the prior distribution on each bidder 's valuation can be doubly exponential	purpose	2K_dev_774
for branching upper bounding	purpose	2K_dev_774
lower bounding and lazy bounding	purpose	2K_dev_774
we provide very positive results for plurality and very negative results for Borda	finding	2K_dev_776
and place veto in the middle of this spectrum	finding	2K_dev_776
via the notion of the price of anarchy	mechanism	2K_dev_776
using the scores of alternatives as a proxy for their quality and bounding the ratio between the score of the optimal alternative and the score of the winning alternative in Nash equilibrium Specifically	mechanism	2K_dev_776
we are interested in Nash equilibria that are obtained via sequences of rational strategic moves	mechanism	2K_dev_776
Focusing on three common voting rules -- plurality	mechanism	2K_dev_776
veto and Borda --	mechanism	2K_dev_776
It is well known that strategic behavior in elections is essentially unavoidable ; we therefore ask : how bad can the rational outcome be ? We answer this question	purpose	2K_dev_776
Given a large graph	background	2K_dev_777
like a computer communication network	background	2K_dev_777
which $ k $ nodes should we immunize ( or monitor	background	2K_dev_777
or remove ) to make it as robust as possible against a computer virus attack ? This problem	background	2K_dev_777
referred to as the node immunization problem	background	2K_dev_777
is the core building block in many high-impact applications	background	2K_dev_777
ranging from public health	background	2K_dev_777
cybersecurity to viral marketing	background	2K_dev_777
A central component in node immunization is to find the best $ k $ bridges of a given graph	background	2K_dev_777
( 1 ) the proposed bridging score gives mining results consistent with intuition ; and ( 2 ) the proposed fast solution is up to seven orders of magnitude faster than straightforward alternatives	finding	2K_dev_777
First of all we propose a novel bridging score $ \Delta \lambda $	mechanism	2K_dev_777
inspired by immunology and we show that its results agree with intuition for several realistic settings	mechanism	2K_dev_777
Since the straightforward way to compute $ \Delta \lambda $ is computationally intractable	mechanism	2K_dev_777
we then focus on the computational issues and propose a surprisingly efficient way ( $ O ( nk^2+m ) $ ) to estimate it	mechanism	2K_dev_777
Experimental results on real graphs show that	method	2K_dev_777
In this setting we typically want to determine the relative importance of a node ( or a set of nodes ) within the graph	purpose	2K_dev_777
for example how valuable ( as a bridge ) a person or a group of persons is in a social network	purpose	2K_dev_777
To the best of our knowledge	background	2K_dev_778
our work represents the largest study of propagation patterns of executables	background	2K_dev_778
Finally we discover the SharkFin temporal propagation pattern of executable files	finding	2K_dev_778
the GeoSplit pattern in the geographical spread of machines that report executables to Symantec 's servers	finding	2K_dev_778
the Periodic Power Law ( Ppl ) distribution of the life-time of URLs	finding	2K_dev_778
and we show how to efficiently extrapolate crucial properties of the data from a small sample	finding	2K_dev_778
by analyzing patterns from 22 million malicious ( and benign ) files	method	2K_dev_778
6 million hosts worldwide during the month of June 2011	method	2K_dev_778
We conduct this study using the WINE database available at Symantec Research Labs Additionally	method	2K_dev_778
we explore the research questions raised by sampling on such large databases of executables ; the importance of studying the implications of sampling is twofold : First	method	2K_dev_778
sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second	method	2K_dev_778
because every such data collection can be perceived as a sample of the real world	method	2K_dev_778
How does malware propagate ? Does it form spikes over time ? Does it resemble the propagation pattern of benign files	purpose	2K_dev_778
such as software patches ? Does it spread uniformly over countries ? How long does it take for a URL that distributes malware to be detected and shut down ? In this work	purpose	2K_dev_778
we answer these questions	purpose	2K_dev_778
we discovered that duplicate points create subtle issues	finding	2K_dev_779
that the literature has ignored : if dmax is the multiplicity of the most over-plotted point	finding	2K_dev_779
typical algorithms are quadratic on dmax	finding	2K_dev_779
we show that our methods give either exact results	finding	2K_dev_779
or highly accurate approximate ones	finding	2K_dev_779
We propose several ways we report wall-clock times and our time savings ; and	mechanism	2K_dev_779
Given a large cloud of multi-dimensional points	purpose	2K_dev_779
and an off-theshelf outlier detection method	purpose	2K_dev_779
why does it take a week to finish ? to eliminate the problem	purpose	2K_dev_779
Counterfactual Regret Minimization ( CFR ) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games	background	2K_dev_780
CFR is an iterative algorithm that repeatedly traverses the game tree	background	2K_dev_780
updating regrets at each information set	background	2K_dev_780
show an order of magnitude speed improvement	finding	2K_dev_780
and the relative speed improvement increases with the size of the game	finding	2K_dev_780
We introduce to CFR It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive	mechanism	2K_dev_780
had that path been explored on every iteration The new algorithm maintains CFR 's convergence guarantees while making iterations significantly fastereven if previously known pruning techniques are used in the comparison	mechanism	2K_dev_780
This improvement carries over to CFR+	mechanism	2K_dev_780
a recent variant of CFR	mechanism	2K_dev_780
an improvement that prunes any path of play in the tree	purpose	2K_dev_780
and its descendants that has negative regret	purpose	2K_dev_780
Influence maximization is a problem of maximizing the aggregate adoption of products	background	2K_dev_781
technologies or even beliefs	background	2K_dev_781
Most past algorithms leveraged an assumption of submodularity that captures diminishing returns to scale	background	2K_dev_781
prove that this policy is optimal in a very general setting	finding	2K_dev_781
that the proposed `` best-time '' algorithm remains quite effective the `` best-time '' policy becomes suboptimal	finding	2K_dev_781
and is significantly outperformed by our more general heuristic	finding	2K_dev_781
We formulate a dynamic influence maximization problem under increasing returns We propose a simple algorithm in this model which chooses the best time period to use up the entire budget ( called Best-Stage )	mechanism	2K_dev_781
and We also propose a heuristic algorithm for this problem of which Best-Stage decision is a special case	mechanism	2K_dev_781
Additionally we experimentally verify even as we relax the assumptions under which optimality can be proved	method	2K_dev_781
However we find that when we add a `` learning-by-doing '' effect	method	2K_dev_781
in which the adoption costs decrease not as a function of time	method	2K_dev_781
but as a function of aggregate adoption	method	2K_dev_781
While submodularity is natural in many domains	purpose	2K_dev_781
early stages of innovation adoption are often better characterized by convexity	purpose	2K_dev_781
which is evident for renewable technologies	purpose	2K_dev_781
such as rooftop solar to scale over a finite time horizon	purpose	2K_dev_781
in which the decision maker faces a budget constraint	purpose	2K_dev_781
Pareto efficiency is a widely used property in solution concepts for cooperative and non -- cooperative game -- theoretic settings and	background	2K_dev_782
more generally in multi -- objective problems	background	2K_dev_782
In this paper we show that the Pareto curve of a bimatrix game can be found exactly in polynomial time and that it is composed of a polynomial number of pieces	mechanism	2K_dev_782
Furthermore each piece is a quadratic function We use this result to provide algorithms	mechanism	2K_dev_782
However finding or even approximating ( when the objective functions are not convex ) the Pareto curve is hard	purpose	2K_dev_782
Most of the literature focuses on computing concise representations to approximate the Pareto curve or on exploiting evolutionary approaches to generate approximately Pareto efficient samples of the curve for game-theoretic solution concepts that incorporate Pareto efficiency	purpose	2K_dev_782
Extensive-form games are a powerful tool for modeling a large range of multiagent scenarios Finally we discuss how our theory applies to several practical problems for which no solution quality bounds could be derived before	background	2K_dev_783
Leveraging recent results on abstraction solution quality	mechanism	2K_dev_783
we develop the first framework For games where the error is Lipschitz-continuous in the distance of a continuous point to its nearest discrete point	mechanism	2K_dev_783
we show that a uniform discretization of the space is optimal	mechanism	2K_dev_783
When the error is monotonically increasing in distance to nearest discrete point	mechanism	2K_dev_783
we develop an integer program for finding the optimal discretization when the error is described by piecewise linear functions	mechanism	2K_dev_783
This result can further be used to approximate optimal solutions to general monotonic error functions	mechanism	2K_dev_783
However most solution algorithms require discrete	purpose	2K_dev_783
In contrast many real-world domains require modeling with continuous action spaces	purpose	2K_dev_783
This is usually handled by heuristically discretizing the continuous action space without solution quality bounds	purpose	2K_dev_783
In this paper we address this issue	purpose	2K_dev_783
for providing bounds on solution quality for discretization of continuous action spaces in extensive-form games	purpose	2K_dev_783
Kidney exchange where candidates with organ failure trade incompatible but willing donors	background	2K_dev_784
is a life-saving alternative to the deceased donor waitlist	background	2K_dev_784
which has inadequate supply to meet demand We conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view	background	2K_dev_784
In this paper we begin by proposing the idea of liver exchange	mechanism	2K_dev_784
and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level	mechanism	2K_dev_784
We then explore cross-organ donation where kidneys and livers can be bartered for each other	mechanism	2K_dev_784
We show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges ; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool	mechanism	2K_dev_784
We support this result experimentally on demographically accurate multi-organ exchanges	method	2K_dev_784
While fielded kidney exchanges see huge benefit from altruistic kidney donors ( who give an organ without a paired needy candidate )	purpose	2K_dev_784
a significantly higher medical risk to the donor deters similar altruism with livers	purpose	2K_dev_784
A multi-faceted graph defines several facets on a set of nodes	background	2K_dev_786
Each facet is a set of edges that represent the relationships between the nodes in a specific context	background	2K_dev_786
where NeSim is shown to be superior to MCL	finding	2K_dev_786
JP and AP the well-established clustering algorithms We also report the success stories of MuFace in finding advertisement click rings	finding	2K_dev_786
We propose NeSim a distributed efficient clustering algorithm We also propose optimizations to further improve the scalability	mechanism	2K_dev_786
the efficiency and the clusters quality We employ generalpurpose graph-clustering algorithms in a novel way Due to the qualities of NeSim	mechanism	2K_dev_786
we employ it as a backbone in the distributed MuFace algorithm	mechanism	2K_dev_786
which discovers multi-faceted communities	mechanism	2K_dev_786
We evaluate the proposed algorithms on several real and synthetic datasets	method	2K_dev_786
Mining multi-faceted graphs have several applications	purpose	2K_dev_786
including finding fraudster rings that launch advertising traffic fraud attacks	purpose	2K_dev_786
tracking IP addresses of botnets over time	purpose	2K_dev_786
analyzing interactions on social networks and co-authorship of scientific papers that does soft clustering on individual facets	purpose	2K_dev_786
to discover communities across facets	purpose	2K_dev_786
The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm	background	2K_dev_787
It won the 2014 Annual Computer Poker Competition	finding	2K_dev_787
beating each opponent with statistical significance	finding	2K_dev_787
We introduce a distributed version of the most commonly used equilibrium-finding algorithm	mechanism	2K_dev_787
counterfactual regret minimization ( CFR )	mechanism	2K_dev_787
The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint We introduce an algorithm while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and the earth-mover's-distance similarity metric	mechanism	2K_dev_787
Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency	mechanism	2K_dev_787
Prior approaches run slowly on this architecture Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency	mechanism	2K_dev_787
Finally we introduce a family of post-processing techniques that outperform prior ones	mechanism	2K_dev_787
We applied these techniques to generate an agent for two-player no-limit Texas Hold'em	method	2K_dev_787
which enables CFR to scale to dramatically larger abstractions and numbers of cores	purpose	2K_dev_787
for generating such abstractions	purpose	2K_dev_787
How can we succinctly describe a million-node graph with a few simple sentences ? Given a large graph	background	2K_dev_788
how can we find its most `` important '' structures	background	2K_dev_788
so that we can summarize it and easily visualize it ? How can we measure the `` importance '' of a set of discovered subgraphs in a large graph ?	background	2K_dev_788
To this end we first mine candidate subgraphs using one or more graph partitioning algorithms Next	mechanism	2K_dev_788
we identify the optimal summarization using the minimum description length MDL principle	mechanism	2K_dev_788
picking only those subgraphs from the candidates that together yield the best lossless compression of the graph-or	mechanism	2K_dev_788
equivalently that most succinctly describe its adjacency matrix	mechanism	2K_dev_788
Starting with the observation that real graphs often consist of stars	purpose	2K_dev_788
bipartite cores cliques and chains	purpose	2K_dev_788
our main idea is to find the most succinct description of a graph in these `` vocabulary '' terms	purpose	2K_dev_788
showing good performance for modeling previously unseen molecular configurations we show substantial improvement over the state of the art in molecular energy optimization	finding	2K_dev_792
Motivated by problems such as molecular energy prediction	mechanism	2K_dev_792
we derive an ( improper ) kernel between geometric inputs	mechanism	2K_dev_792
that is able Since many physical simulations based upon geometric data produce derivatives of the output quantity with respect to the input positions	mechanism	2K_dev_792
we derive an approach that incorporates derivative information into our kernel learning	mechanism	2K_dev_792
We further show how to exploit the low rank structure of the resulting kernel matrices to speed up learning	mechanism	2K_dev_792
Finally we evaluated the method in the context of molecular energy prediction	method	2K_dev_792
Integrating the approach into a Bayesian optimization	method	2K_dev_792
to capture the relevant rotational and translation invariances in geometric data	purpose	2K_dev_792
The leading approach for computing strong game-theoretic strategies in large imperfect-information games is to first solve an abstracted version of the game offline	background	2K_dev_793
then perform a table lookup during game play	background	2K_dev_793
show that our algorithm leads to significantly stronger performance against the strongest agents from the 2013 AAAI Annual Computer Poker Competition	finding	2K_dev_793
We consider where we solve the portion of the game that we have actually reached in real time to a greater degree of accuracy than in the initial computation	mechanism	2K_dev_793
We call this approach endgame solving	mechanism	2K_dev_793
Theoretically we show that endgame solving can produce highly exploitable strategies in some games ; however	mechanism	2K_dev_793
we show that it can guarantee a low exploitability in certain games where the opponent is given sufficient exploitative power within the endgame	mechanism	2K_dev_793
Furthermore despite the lack of a general worst-case guarantee	mechanism	2K_dev_793
we describe benefits of endgame solving	mechanism	2K_dev_793
We present an efficient algorithm and present a new variance-reduction technique	mechanism	2K_dev_793
Experiments on no-limit Texas Hold'em	method	2K_dev_793
a modification to this approach for performing endgame solving in large imperfect-information games	purpose	2K_dev_793
for evaluating the performance of an agent that uses endgame solving	purpose	2K_dev_793
Living organisms adapt to challenges through evolution and adaptation	background	2K_dev_796
Potential application classes include therapeutics at the population	background	2K_dev_796
individual and molecular levels ( drug design )	background	2K_dev_796
as well as cell repurposing and synthetic biology	background	2K_dev_796
propose the wild idea of computational game theory for ( typically incomplete-information ) multistage games and opponent exploitation techniques	mechanism	2K_dev_796
A sequential contingency plan for steering is constructed computationally for the setting at hand In the biological context	mechanism	2K_dev_796
the opponent ( e	mechanism	2K_dev_796
a disease ) has a systematic handicap because it evolves myopically	mechanism	2K_dev_796
This can be exploited by computing trapping strategies that cause the opponent to evolve into states where it can be handled effectively	mechanism	2K_dev_796
This has proven to be a key difficulty in developing therapies	purpose	2K_dev_796
since the organisms develop resistance	purpose	2K_dev_796
We show that the mechanism results in significant gains on data from a national kidney exchange that includes 59 % of all US transplant centers	finding	2K_dev_806
We present a credit-based matching mechanism in particularthat is both strategy proof and efficient	mechanism	2K_dev_806
that is it guarantees truthful disclosure of donor-patient pairs from the transplant centers and results in the maximum global matching Furthermore	mechanism	2K_dev_806
the mechanism is individually rational in the sense that	mechanism	2K_dev_806
in the long run	mechanism	2K_dev_806
it guarantees each transplant center more matches than the center could have achieved alone	mechanism	2K_dev_806
The mechanism does not require assumptions about the underlying distribution of compatibility graphsa nuance that has previously produced conflicting results in other aspects of theoretical kidney exchange Our results apply not only to matching via 2-cycles : the matchings can also include cycles of any length and altruist-initiated chains	mechanism	2K_dev_806
which is important at least in kidney exchanges	mechanism	2K_dev_806
The mechanism can also be adjusted to guarantee immediate individual rationality at the expense of economic efficiency	mechanism	2K_dev_806
while preserving strategy proofness via the credits	mechanism	2K_dev_806
This circumvents a well-known impossibility result in static kidney exchange concerning the existence of an individually rational	mechanism	2K_dev_806
strategy-proof and maximal mechanism	mechanism	2K_dev_806
for dynamic barter marketsand kidney exchange	purpose	2K_dev_806
The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm	background	2K_dev_807
that won the 2014 Annual Computer Poker Competition	finding	2K_dev_807
beating each opponent with statistical significance	finding	2K_dev_807
We introduce a distributed version of the most commonly used equilibrium-finding algorithm	mechanism	2K_dev_807
counterfactual regret minimization ( CFR )	mechanism	2K_dev_807
The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint We introduce an algorithm while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and earth-mover 's distance	mechanism	2K_dev_807
Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency	mechanism	2K_dev_807
Prior approaches run slowly on this architecture	mechanism	2K_dev_807
Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency	mechanism	2K_dev_807
Finally we introduce a family of post-processing techniques that outperform prior ones	mechanism	2K_dev_807
We applied these techniques to generate an agent for two-player no-limit Texas Hold'em	method	2K_dev_807
which enables CFR to scale to dramatically larger abstractions and numbers of cores	purpose	2K_dev_807
for generating such abstractions	purpose	2K_dev_807
Bayesian theory has provided a compelling conceptualization for perceptual inference in the brain	background	2K_dev_809
Central to Bayesian inference is the notion of statistical priors	background	2K_dev_809
To understand the neural mechanisms of Bayesian inference	background	2K_dev_809
we need to understand the neural representation of statistical regularities in the natural environment These findings demonstrate that there is a relationship between the functional connectivity observed in the visual cortex and the statistics of natural scenes	background	2K_dev_809
They also suggest that the Boltzmann machine can be a viable model for conceptualizing computations in the visual cortex and	background	2K_dev_809
as such can be used to predict neural circuits in the visual cortex from natural scene statistics	background	2K_dev_809
and found that the units in the model exhibited cooperative and competitive interactions	finding	2K_dev_809
forming a disparity association field	finding	2K_dev_809
analogous to the contour association field The cooperative and competitive interactions in the disparity association field are consistent with constraints of computational models for stereo matching	finding	2K_dev_809
and found the results to be consistent with neurophysiological data in terms of the functional connectivity measurements between disparity-tuned neurons in the macaque primary visual cortex	finding	2K_dev_809
a Boltzmann machine model	mechanism	2K_dev_809
We applied In addition	method	2K_dev_809
we simulated neurophysiological experiments on the model	method	2K_dev_809
In this paper we investigated empirically how statistical regularities in natural 3D scenes are represented in the functional connectivity of disparity-tuned neurons in the primary visual cortex of primates	purpose	2K_dev_809
to learn from 3D natural scenes	purpose	2K_dev_809
show that our approach produces significantly more accurate rankings than alternative approaches	finding	2K_dev_811
We present the first model From this viewpoint	mechanism	2K_dev_811
voting rules are seen as error-correcting codes : their goal is to correct errors in the input rankings and recover a ranking that is close to the ground truth We derive worst-case bounds on the relation between the average accuracy of the input votes	mechanism	2K_dev_811
and the accuracy of the output ranking	mechanism	2K_dev_811
Empirical results from real data	method	2K_dev_811
of optimal voting under adversarial noise	purpose	2K_dev_811
The use of deductive techniques	background	2K_dev_812
such as theorem provers	background	2K_dev_812
has several advantages in safety verification of hybrid systems	background	2K_dev_812
we present an extension to the deductive verification framework of differential dynamic logic by leveraging forward invariant sets provided by external methods	mechanism	2K_dev_812
such as numerical techniques and designer insights Our key contribution is a new inference rule	mechanism	2K_dev_812
the forward invariant cut rule	mechanism	2K_dev_812
introduced into the proof calculus of KeYmaera	mechanism	2K_dev_812
We demonstrate the cut rule in action on an example involving an automotive powertrain control systems	method	2K_dev_812
in which we make use of a simulation-driven numerical technique to compute a local barrier function	method	2K_dev_812
There is often a gap	purpose	2K_dev_812
however between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide	purpose	2K_dev_812
To address this deficiency that allows the theorem prover KeYmaera to locally reason about behaviors	purpose	2K_dev_812
When solving extensive-form games with large action spaces	background	2K_dev_814
typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective	background	2K_dev_814
When this occurs a procedure is needed to interpret actions of the opponent that fall outside of our abstraction ( by mapping them to actions in our abstraction )	background	2K_dev_814
This is called an action translation mapping	background	2K_dev_814
our mapping performs competitively with the prior mappings	finding	2K_dev_814
We present a new mapping and has significantly lower exploitability than the prior mappings Furthermore	mechanism	2K_dev_814
we observe that the cost of this worst-case performance benefit ( low exploitability ) is not high in practice ; We also observe several paradoxes that can arise when performing action abstraction and translation ; for example	mechanism	2K_dev_814
we show that it is possible to improve performance by including suboptimal actions in our abstraction and excluding optimal actions	mechanism	2K_dev_814
against no-limit Texas Hold'em agents submitted to the 2012 Annual Computer Poker Competition	method	2K_dev_814
Prior action translation mappings have been based on heuristics without theoretical justification	purpose	2K_dev_814
We show that the prior mappings are highly exploitable and that most of them violate certain natural desiderata	purpose	2K_dev_814
that satisfies these desiderata	purpose	2K_dev_814
Problems of this nature arise in formal verification of continuous and hybrid dynamical systems	background	2K_dev_815
where there is an increasing need for methods to expedite formal proofs	background	2K_dev_815
The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting	finding	2K_dev_815
We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of benchmarks	method	2K_dev_815
This paper studies sound proof rules for checking positive invariance of algebraic and semi-algebraic sets	purpose	2K_dev_815
that is sets satisfying polynomial equalities and those satisfying finite boolean combinations of polynomial equalities and inequalities	purpose	2K_dev_815
under the flow of polynomial ordinary differential equations	purpose	2K_dev_815
How can we efficiently decompose a tensor into sparse factors	background	2K_dev_817
when the data do not fit in memory ?	background	2K_dev_817
enabling reproducibility of our work	background	2K_dev_817
indicate over 90p sparser outputs and 14 times faster execution	finding	2K_dev_817
with approximation error close to the current state of the art irrespective of computation and memory requirements	finding	2K_dev_817
demonstrating its effectiveness for data-mining practitioners	finding	2K_dev_817
In this work we propose P ar C ube	mechanism	2K_dev_817
a new and highly parallelizable method that is well suited to produce sparse approximations In particular	mechanism	2K_dev_817
we are the first to analyze the very large N ell dataset using a sparse tensor decomposition	mechanism	2K_dev_817
demonstrating that P ar C ube enables us to handle effectively and efficiently very large datasets Finally	mechanism	2K_dev_817
we make our highly scalable parallel implementation publicly available	mechanism	2K_dev_817
Experiments with even moderately large data We provide theoretical guarantees for the algorithms correctness and we experimentally validate our claims through extensive experiments	method	2K_dev_817
including four different real world datasets ( E nron	method	2K_dev_817
L bnl F acebook and N ell )	method	2K_dev_817
Tensor decompositions have gained a steadily increasing popularity in data-mining applications ; however	purpose	2K_dev_817
the current state-of-art decomposition algorithms operate on main memory and do not scale to truly large datasets	purpose	2K_dev_817
for speeding up tensor decompositions	purpose	2K_dev_817
the resulting methods often perform as well or better than existing latent variable models	finding	2K_dev_829
while being substantially easier to train	finding	2K_dev_829
We present a convex approach Our approach builds upon recent advances in multivariate total variation regularization	mechanism	2K_dev_829
and seeks to learn a separate set of parameters for the distribution over the observations at each time point	mechanism	2K_dev_829
but with an additional penalty that encourages the parameters to remain constant over time	mechanism	2K_dev_829
We propose efficient optimization methods	mechanism	2K_dev_829
and a two-stage procedure under such models	mechanism	2K_dev_829
based upon kernel density estimation	mechanism	2K_dev_829
Finally we show on a number of real-world segmentation tasks	method	2K_dev_829
to probabilistic segmentation and modeling of time series data for solving the resulting ( large ) optimization problems for estimating recurring clusters	purpose	2K_dev_829
Given a large collection of co-evolving online activities	background	2K_dev_831
such as searches for the keywords `` Xbox ''	background	2K_dev_831
`` PlayStation '' and `` Wii ''	background	2K_dev_831
how can we find patterns and rules ? Are these keywords related ? If so	background	2K_dev_831
are they competing against each other ? Can we forecast the volume of user activity for the coming month ?	background	2K_dev_831
show that ECOWEB is effective	finding	2K_dev_831
in that it can capture long-range dynamics and meaningful patterns such as seasonalities	finding	2K_dev_831
and practical in that it can provide accurate long-range forecasts	finding	2K_dev_831
ECOWEB consistently outperforms existing methods in terms of both accuracy and execution speed	finding	2K_dev_831
We present ECOWEB ( i	mechanism	2K_dev_831
Ecosystem on the Web )	mechanism	2K_dev_831
which is an intuitive model designed as a non-linear dynamical system Our second contribution is a novel	mechanism	2K_dev_831
parameter-free and scalable fitting algorithm	mechanism	2K_dev_831
ECOWEB-FIT that estimates the parameters of ECOWEB	mechanism	2K_dev_831
Extensive experiments on real data	method	2K_dev_831
We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food for mining large-scale co-evolving online activities	purpose	2K_dev_831
With the rise of online social networks and smartphones that record the user 's location	background	2K_dev_833
a new type of online social network has gained popularity during the last few years	background	2K_dev_833
the so called Location-based Social Networks ( LBSNs )	background	2K_dev_833
In such networks users voluntarily share their location with their friends via a check-in	background	2K_dev_833
In exchange they get recommendations tailored to their particular location as well as special deals that businesses offer when users check-in frequently	background	2K_dev_833
LBSNs started as specialized platforms such as Gowalla and Foursquare	background	2K_dev_833
however their immense popularity has led online social networking giants like Facebook to adopt this functionality	background	2K_dev_833
The spatial aspect of LBSNs directly ties the physical with the online world	background	2K_dev_833
creating a very rich ecosystem where users interact with their friends both online as well as declare their physical ( co- ) presence in various locations	background	2K_dev_833
In this work we propose to model and analyze LBSNs using Tensors and Tensor Decompositions	mechanism	2K_dev_833
powerful analytical tools that have enjoyed great growth and success in fields like Machine Learning	mechanism	2K_dev_833
Data Mining and Signal Processing alike	mechanism	2K_dev_833
In addition to Tensor Decompositions	mechanism	2K_dev_833
we use Signal Processing tools that have been previously used in Direction of Arrival ( DOA ) estimations	mechanism	2K_dev_833
Such a rich environment calls for novel analytic tools that can model the aforementioned types of interactions	purpose	2K_dev_833
By doing so we identify tightly knit	purpose	2K_dev_833
hidden communities of users and locations which they frequent to study the temporal dynamics of hidden communities in LBSNs	purpose	2K_dev_833
Prior work has among other techniques	background	2K_dev_834
used canonical correlation analysis to project pre-trained vectors in two languages into a common space	background	2K_dev_834
that our method outperforms prior work on multilingual tasks	finding	2K_dev_834
matches the performance of prior work on monolingual tasks	finding	2K_dev_834
and scales linearly with the size of the input data ( and thus the number of languages being embedded )	finding	2K_dev_834
We propose a simple and scalable method that is inspired by the notion that the learned vector representations should be invariant to translation between languages	mechanism	2K_dev_834
This work focuses on the task of finding latent vector representations of the words in a corpus	purpose	2K_dev_834
In particular we address the issue of what to do when there are multiple languages in the corpus	purpose	2K_dev_834
show that this often improves running times by an order of magnitude or more vs	finding	2K_dev_838
existing approaches based on conic solvers	finding	2K_dev_838
We present Epsilon a system using fast linear and proximal operators	mechanism	2K_dev_838
As with existing convex programming frameworks	mechanism	2K_dev_838
users specify convex optimization problems using a natural grammar for mathematical expressions	mechanism	2K_dev_838
composing functions in a way that is guaranteed to be convex by the rules of disciplined convex programming Given such an input	mechanism	2K_dev_838
the Epsilon compiler transforms the optimization problem into a mathematically equivalent form consisting only of functions with ecient proximal operators|an intermediate representation we refer to as prox-ane form	mechanism	2K_dev_838
By reducing problems to this form	mechanism	2K_dev_838
Epsilon enables solving general convex problems using a large library of fast proximal and linear operators ;	mechanism	2K_dev_838
numerical examples on many popular problems from statistics and machine learning	method	2K_dev_838
for general convex programming	purpose	2K_dev_838
Unease over data privacy will retard consumer acceptance of IoT deployments	background	2K_dev_844
The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud	background	2K_dev_844
that interposes a locally-controlled software component called a privacy mediator on every raw sensor stream Each mediator is in the same administrative domain as the sensors whose data is being collected	mechanism	2K_dev_844
and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain This solution necessitates a logical point of presence for mediators within the administrative boundaries of each organization	mechanism	2K_dev_844
Such points of presence are provided by cloudlets	mechanism	2K_dev_844
which are small locally-administered data centers at the edge of the Internet that can support code mobility The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility	mechanism	2K_dev_844
This is a direct consequence of the over-centralization of today 's cloud-based IoT hub designs	purpose	2K_dev_844
We propose a solution	purpose	2K_dev_844
The widespread use of social networks enables the rapid diffusion of information	background	2K_dev_853
news among users in very large communities	background	2K_dev_853
It is a substantial challenge to be able to observe and understand such diffusion processes	background	2K_dev_853
which may be modeled as networks that are both large and dynamic A key tool in this regard is data summarization	background	2K_dev_853
However few existing studies aim to summarize graphs/networks for dynamics	background	2K_dev_853
Dynamic networks raise new challenges not found in static settings	background	2K_dev_853
including time sensitivity and the needs for online interestingness evaluation and summary traceability	background	2K_dev_853
which render existing techniques inapplicable	background	2K_dev_853
The study offers insight into the effectiveness and design properties of OSNet	background	2K_dev_853
Based on the concepts of diffusion radius and scope	mechanism	2K_dev_853
we define interestingness measures for dynamic networks	mechanism	2K_dev_853
and we propose OSNet	mechanism	2K_dev_853
an online summarization framework	mechanism	2K_dev_853
We report on extensive experiments with both synthetic and real-life data	method	2K_dev_853
We study the topic of dynamic network summarization : how to summarize dynamic networks with millions of nodes by only capturing the few most interesting nodes or edges over time	purpose	2K_dev_853
and we address the problem by finding interestingness-driven diffusion processes	purpose	2K_dev_853
Session types provide a means to prescribe the communication behavior between concurrent message-passing processes However	background	2K_dev_854
in a distributed setting	background	2K_dev_854
some processes may be written in languages that do not support static typing of sessions or may be compromised by a malicious intruder	background	2K_dev_854
violating invariants of the session types	background	2K_dev_854
We prove that dynamic monitoring does not change system behavior for welltyped processes	finding	2K_dev_854
and that one of an indicated set of possible culprits must have been compromised in case of an alarm	finding	2K_dev_854
We present a system of in the case when the monitor detects an undesirable action and an alarm is raised	mechanism	2K_dev_854
In such a setting	purpose	2K_dev_854
dynamically monitoring communication between processes becomes a necessity for identifying undesirable actions	purpose	2K_dev_854
In this paper we show how to dynamically monitor communication to enforce adherence to session types in a higher-order setting	purpose	2K_dev_854
One typically proves infeasibility in satisfiability/ constraint satisfaction ( or optimality in integer programming ) by constructing a tree certificate	background	2K_dev_860
We explore the power of a simple paradigm	mechanism	2K_dev_860
that of throwing random darts into the assignment space and then This method seems to work well when the number of short certificates of infeasibility is moderate	mechanism	2K_dev_860
suggesting that the overhead of throwing darts is more than paid for by the information gained by these darts	mechanism	2K_dev_860
However deciding how to branch in the search tree is hard	purpose	2K_dev_860
and impacts search time drastically	purpose	2K_dev_860
using information gathered by that dart to guide what to do next	purpose	2K_dev_860
showing that rising 5th and 6th graders can understand the lawfulness of Kodu programs	finding	2K_dev_871
We also discuss some misconceptions students may develop about Kodu	finding	2K_dev_871
their causes and potential remedies	finding	2K_dev_871
We present an analysis of assessment data	method	2K_dev_871
This paper introduces reasoning about lawful behavior as an important computational thinking skill and provides examples from a novel introductory programming curriculum using Microsoft 's Kodu Game Lab	purpose	2K_dev_871
User-generated online reviews can play a significant role in the success of retail products	background	2K_dev_876
where FRAUDEAGLE successfully reveals fraud-bots in a large online app review database	finding	2K_dev_876
We propose a fast and effective framework	mechanism	2K_dev_876
FRAUDEAGLE Our method has several advantages : ( 1 ) it exploits the network effect among reviewers and products	mechanism	2K_dev_876
unlike the vast majority of existing methods that focus on review text or behavioral analysis	mechanism	2K_dev_876
( 2 ) it consists of two complementary steps ; scoring users and reviews for fraud detection	mechanism	2K_dev_876
and grouping for visualization and sensemaking	mechanism	2K_dev_876
( 3 ) it operates in a completely unsupervised fashion requiring no labeled data	mechanism	2K_dev_876
while still incorporating side information if available	mechanism	2K_dev_876
and ( 4 ) it is scalable to large datasets as its run time grows linearly with network size	mechanism	2K_dev_876
We demonstrate the effectiveness of our framework on syntheticand real datasets ;	method	2K_dev_876
However review systems are often targeted by opinion spammers who seek to distort the perceived quality of a product by creating fraudulent reviews	purpose	2K_dev_876
for spotting fraudsters and fake reviews in online review datasets	purpose	2K_dev_876
Networked or telematic music performances take many forms	background	2K_dev_877
ranging from small laptop ensembles using local area networks to long-distance musical collaborations using audio and video links	background	2K_dev_877
Two important concerns for any networked performance are :	background	2K_dev_877
which achieved a coordinated performance involving 68 computer musicians	finding	2K_dev_877
each with their own connection to the network	finding	2K_dev_877
A recent project the Global Net Orchestra	mechanism	2K_dev_877
the technical aspects of the project	mechanism	2K_dev_877
( 1 ) what is the role of communication in the music performance ? In particular	purpose	2K_dev_877
what are the esthetic and pragmatic justifications for performing music at a distance	purpose	2K_dev_877
and ( 2 ) how are the effects of communication latency ameliorated or incorporated into the performance ? In addition to addressing these two concerns	purpose	2K_dev_877
In this demo we present Perseus	mechanism	2K_dev_878
a large-scale system by supporting the coupled summarization of graph properties and structures	mechanism	2K_dev_878
guiding attention to outliers	mechanism	2K_dev_878
and allowing the user to interactively explore normal and anomalous node behaviors Specifically	mechanism	2K_dev_878
Perseus provides for the following operations : 1 ) ( e	mechanism	2K_dev_878
degree PageRank real eigenvectors ) by performing scalable	mechanism	2K_dev_878
offline batch processing on Hadoop ; 2 ) ; 3 ) ; 4 )	mechanism	2K_dev_878
by incrementally revealing its neighbors	mechanism	2K_dev_878
In our demonstration we invite the audience to interact with Perseus to explore a variety of multi-million-edge social networks including a Wikipedia vote network	method	2K_dev_878
a friendship/foeship network in Slashdot	method	2K_dev_878
and a trust network based on the consumer review website Epinions	method	2K_dev_878
Given a large graph with several millions or billions of nodes and edges	purpose	2K_dev_878
such as a social network	purpose	2K_dev_878
how can we explore it efficiently and find out what is in the data ? that enables the comprehensive analysis of large graphs It automatically extracts graph invariants It interactively visualizes univariate and bivariate distributions for those invariants It summarizes the properties of the nodes that the user selects It efficiently visualizes the induced subgraph of a selected node and its neighbors	purpose	2K_dev_878
Besides the application to password generation	background	2K_dev_880
our proposed Human Usability Model ( HUM ) will have other applications	background	2K_dev_880
We show that our password generation methods are humanly computable and	finding	2K_dev_880
to a well-defined extent	finding	2K_dev_880
Then motivated by the special case of password creation	mechanism	2K_dev_880
we propose a collection of well-defined password-generation methods	mechanism	2K_dev_880
For the proof of security	mechanism	2K_dev_880
we posit that password generation methods are public	mechanism	2K_dev_880
but that the humans privately chosen seed is not	mechanism	2K_dev_880
and that the adversary will have observed only a few input-output pairs	mechanism	2K_dev_880
What can a human compute in his/her head that a powerful adversary can not infer ? To answer this question	purpose	2K_dev_880
we define a model of human computation and a measure of security	purpose	2K_dev_880
Suppose you are a teacher	background	2K_dev_882
and have to convey a set of object-property pairs ( 'lions eat meat '	background	2K_dev_882
or 'aspirin is a blood-thinner ' )	background	2K_dev_882
A good teacher will convey a lot of information	background	2K_dev_882
with little effort on the student side	background	2K_dev_882
Specifically given a list of objects ( like animals or medical drugs ) and their associated properties	background	2K_dev_882
what is the best and most intuitive way to convey this information to the student	background	2K_dev_882
without the student being overwhelmed ? A related	background	2K_dev_882
harder problem is : how can we assign a numerical score to each lesson plan ( i	background	2K_dev_882
way of conveying information ) ?	background	2K_dev_882
it is effective achieving excellent results on real data	finding	2K_dev_882
both with respect to our proposed metric	finding	2K_dev_882
but also with respect to encoding length	finding	2K_dev_882
demonstrate the effectiveness of HYTRA	finding	2K_dev_882
and we provide a metric for comparing different approaches based on information theory	mechanism	2K_dev_882
We also design a multi-pronged algorithm	mechanism	2K_dev_882
HYTRA for this problem	mechanism	2K_dev_882
Our proposed HYTRA is scalable ( near-linear in the dataset size )	mechanism	2K_dev_882
and it is intuitive	mechanism	2K_dev_882
conforming to well-known educational principles	mechanism	2K_dev_882
such as grouping related concepts	mechanism	2K_dev_882
and `` comparing '' and `` contrasting ''	mechanism	2K_dev_882
Experiments on real and synthetic datasets	method	2K_dev_882
Here we give a formal definition of this problem of forming learning units	purpose	2K_dev_882
Differential dynamic logic is a logic for specifying and verifying safety	background	2K_dev_884
liveness and other properties about models of cyber-physical systems	background	2K_dev_884
Theorem provers based on differential dynamic logic have been used to verify safety properties for models of self-driving cars and collision avoidance protocols for aircraft	background	2K_dev_884
Examples include : an unambiguous separation between proof checking and proof search	background	2K_dev_884
the ability to extract program traces corresponding to counter-examples	background	2K_dev_884
and synthesis of surely-live deterministic programs from liveness proofs for nondeterministic programs	background	2K_dev_884
This paper presents a differential dynamic logic The resulting logic extends both the syntax and semantics of differential dynamic logic with proof terms -- syntactic representations of logical deductions the logic allows equivalence rewriting deep within formulas and supports both uniform renaming and uniform substitutions	mechanism	2K_dev_884
Unfortunately these theorem provers do not have explicit proof terms	purpose	2K_dev_884
which makes the implementation of a number of important features unnecessarily complicated without soundness-critical and extra-logical extensions to the theorem prover	purpose	2K_dev_884
with such an explicit representation of proofs	purpose	2K_dev_884
To support axiomatic theorem proving	purpose	2K_dev_884
Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet	background	2K_dev_888
present a framework where image annotation and image retrieval are considered as the special cases Specifically	mechanism	2K_dev_888
the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables	mechanism	2K_dev_888
we propose a new max margin structure learning approach called Enhanced Max Margin Learning ( EMML ) framework	mechanism	2K_dev_888
which is much more efficient with a much faster convergence rate than the existing max margin learning methods	mechanism	2K_dev_888
as verified through empirical evaluations	mechanism	2K_dev_888
Furthermore we apply EMML framework that is highly scalable in the sense that the query response time is independent of the database scale The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database	mechanism	2K_dev_888
and excels many existing multimodal data mining methods in the literature that do not scale up at all	mechanism	2K_dev_888
The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases	method	2K_dev_888
In this article we exploit the relations among different modalities in a multimedia database and for general multimodal data mining problem In addition	purpose	2K_dev_888
in order to reduce the demanding computation to develop an effective and efficient solution to the multimodal data mining problem	purpose	2K_dev_888
How do people interact with their Facebook wall ? At a high level	background	2K_dev_900
this question captures the essence of our work	background	2K_dev_900
While most prior efforts focus on Twitter	background	2K_dev_900
the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study	background	2K_dev_900
Our work provides a solid step towards a systematic and quantitative wall-centric profiling of Facebook user activity	background	2K_dev_900
Our key results can be summarized in the following points	finding	2K_dev_900
First we find that many wall activities	finding	2K_dev_900
including number of posts	finding	2K_dev_900
number of likes number of posts of type photo	finding	2K_dev_900
can be described by the PowerWall distribution	finding	2K_dev_900
What is more surprising is that most of these distributions have similar slope	finding	2K_dev_900
with a value close to 1 ! Second	finding	2K_dev_900
we show how our patterns and metrics can help us spot surprising behaviors and anomalies	finding	2K_dev_900
For example we find a user posting every two days	finding	2K_dev_900
exactly the same count of posts ; another user posting at midnight	finding	2K_dev_900
with no other activity before or after	finding	2K_dev_900
We propose PowerWall a lesser known heavy-tailed distribution	mechanism	2K_dev_900
We conduct an extensive study of roughly 7K users over three years during four month intervals each year	method	2K_dev_900
In this work we model Facebook user behavior : we analyze the wall activities of users focusing on identifying common patterns and surprising phenomena to fit our data	purpose	2K_dev_900
The preferred treatment for kidney failure is a transplant ; however	background	2K_dev_903
demand for donor kidneys far outstrips supply	background	2K_dev_903
Kidney exchange an innovation where willing but incompatible patient-donor pairs can exchange organsvia barter cycles and altruist-initiated chainsprovides a life-saving alternative	background	2K_dev_903
Typically fielded exchanges act myopically	background	2K_dev_903
considering only the current pool of pairs when planning the cycles and chains	background	2K_dev_903
It results in higher values of the objective it yields better solutions for the efficient objective ( which does not incorporate equity ) than traditional myopic matching that uses the efficiency objective	finding	2K_dev_903
Motivated by our experience running the computational side of a large nationwide kidney exchange	mechanism	2K_dev_903
we present FUTURE-MATCH a framework FUTUREMATCH takes as input a high-level objective ( e	mechanism	2K_dev_903
`` maximize graft survival of transplants over time '' ) decided on by experts	mechanism	2K_dev_903
then automatically ( i ) learns based on data how to make this objective concrete and ( ii ) learns the `` means '' to accomplish this goala task	mechanism	2K_dev_903
in our experience that humans handle poorly It uses data from all live kidney transplants in the US since 1987 to learn the quality of each possible match ; it then learns the potentials of elements of the current input graph offline ( e	mechanism	2K_dev_903
potentials of pairs based on features such as donor and patient blood types )	mechanism	2K_dev_903
translates these to weights	mechanism	2K_dev_903
and performs a computationally feasible batch matching that incorporates dynamic	mechanism	2K_dev_903
failure-aware considerations through the weights	mechanism	2K_dev_903
We validate FUTUREMATCH on real fielded exchange data Furthermore	method	2K_dev_903
even under economically inefficient objectives that enforce equity	method	2K_dev_903
Yet kidney exchange is inherently dynamic	purpose	2K_dev_903
with participants arriving and departing	purpose	2K_dev_903
Also many planned exchange transplants do not go to surgery due to various failures	purpose	2K_dev_903
So it is important to consider the future when matching	purpose	2K_dev_903
for learning to match in a general dynamic model	purpose	2K_dev_903
There is often a large disparity between the size of a game we wish to solve and the size of the largest instances solvable by the best algorithms ; for example	background	2K_dev_906
a popular variant of poker has about 10165 nodes in its game tree	background	2K_dev_906
while the currently best approximate equilibrium-finding algorithms scale to games with around 1012 nodes	background	2K_dev_906
In order to approximate equilibrium strategies in these games	background	2K_dev_906
the leading approach is to create a sufficiently small strategic approximation of the full game	background	2K_dev_906
called an abstraction and to solve that smaller game instead	background	2K_dev_906
The leading abstraction algorithm for imperfect-information games generates abstractions that have imperfect recall and are distribution aware	background	2K_dev_906
using k-means with the earth mover 's distance metric to cluster similar states together	background	2K_dev_906
A distribution-aware abstraction groups states together at a given round if their full distributions over future strength are similar ( as opposed to	background	2K_dev_906
for example just the expectation of their strength )	background	2K_dev_906
The leading algorithm considers distributions over future strength at the final round of the game	background	2K_dev_906
show that our algorithm improves performance over the previously best approach	finding	2K_dev_906
We present the first algorithm using earth mover 's distance	mechanism	2K_dev_906
Experiments on no-limit Texas Hold'em	method	2K_dev_906
However one might benefit by considering the trajectory of distributions over strength in all future rounds	purpose	2K_dev_906
not just the final round	purpose	2K_dev_906
An abstraction algorithm that takes all future rounds into account is called potential aware	purpose	2K_dev_906
for computing potential-aware imperfect-recall abstractions	purpose	2K_dev_906
For the important task of binocular depth perception from complex natural-image stimuli	background	2K_dev_908
the neurophysiological basis for disambiguating multiple matches between the eyes across similar features has remained a long-standing problem	background	2K_dev_908
Recurrent interactions among binocular disparity-tuned neurons in the primary visual cortex ( V1 ) could play a role in stereoscopic computationsbyalteringresponsesto favorthemost likelydepthinterpretation fora givenimagepair	background	2K_dev_908
Psychophysicalresearch has shown that binocular disparity stimuli displayed in 1 region of the visualfield can be extrapolated into neighboring regions that contain ambiguous depth information	background	2K_dev_908
by cooperative algorithms play an important role in solving the stereo correspondence problem	background	2K_dev_908
and found that unambiguous binocular disparity stimuli displayed in the surrounding visualfields of disparity-selective V1 neurons indeed modified their responses when either bistable stereoscopic or uniform featureless stimuli were presented within their receptivefield centers	finding	2K_dev_908
The delayed timing of the response behavior compared with the timing of classical surround suppression and multiple control experiments suggests that these modulations are carried out by slower disparity-specific recurrentconnectionsamongV1neurons	finding	2K_dev_908
We tested whether neurons in macaque V1 interact in a similar manner	purpose	2K_dev_908
Regret matching is a widely-used algorithm for learning how to	background	2K_dev_909
We prove how this can be done by carefully discounting the prior regrets	mechanism	2K_dev_909
This provides to our knowledge	mechanism	2K_dev_909
the first principled warm-starting method It also extends to warm-starting the widely-adopted counterfactual regret minimization ( CFR ) algorithm We then study optimizing a parameter vector for a player in a two-player zero-sum game ( e	mechanism	2K_dev_909
optimizing bet sizes to use in poker )	mechanism	2K_dev_909
We propose a custom gradient descent algorithm that provably finds a locally optimal parameter vector while leveraging our warm-start theory to significantly save regret-matching iterations at each step	mechanism	2K_dev_909
It optimizes the parameter vector while simultaneously finding an equilibrium This amounts to the first action abstraction algorithm ( algorithm for selecting a small number of discrete actions to use from a continuum of actions -- a key preprocessing step for solving large games using current equilibrium-finding algorithms ) with convergence guarantees for extensive-form games	mechanism	2K_dev_909
we show this experimentally as well We present experiments in no-limit Leduc Hold'em and nolimit Texas Hold'em to optimize bet sizing	method	2K_dev_909
We begin by proving that regrets on actions in one setting ( game ) can be transferred to warm start the regrets for solving a different setting with same structure but different payoffs that can be written as a function of parameters	purpose	2K_dev_909
for large incomplete-information games	purpose	2K_dev_909
For many reasons one might consider mechanisms	background	2K_dev_910
or social choice functions	background	2K_dev_910
that only have access to the ordinal rankings of alternatives by the individual agents rather than their utility functions	background	2K_dev_910
sample complexity results for the class of scoring functions	finding	2K_dev_910
under three different models	mechanism	2K_dev_910
In our worst-case model	mechanism	2K_dev_910
no assumptions are made about the underlying distribution and we analyze the worst-case distortion-or degree to which the selected alternative does not maximize social welfare-of optimal ( randomized ) social choice functions	mechanism	2K_dev_910
In our average-case model	mechanism	2K_dev_910
we derive optimal functions under neutral ( or impartial culture ) probabilistic models Finally	mechanism	2K_dev_910
a very general learning-theoretic model allows for the computation of optimal social choice functions ( i	mechanism	2K_dev_910
ones that maximize expected social welfare ) under arbitrary	mechanism	2K_dev_910
sampleable distributions In the latter case	mechanism	2K_dev_910
we provide both algorithms and	mechanism	2K_dev_910
and further validate the approach empirically	method	2K_dev_910
We adopt a utilitarian perspective on social choice	purpose	2K_dev_910
assuming that agents have ( possibly latent ) utility functions over some space of alternatives	purpose	2K_dev_910
In this context one possible objective for a social choice function is the maximization of ( expected ) social welfare relative to the information contained in these rankings We study such optimal social choice functions and underscore the important role played by scoring functions	purpose	2K_dev_910
For decades researchers have struggled with the problem of envy-free cake cutting : how to divide a divisible good between multiple agents so that each agent likes his own allocation best	background	2K_dev_913
Our main result is an envy-free cake cutting protocol for agents with piecewise linear valuations	mechanism	2K_dev_913
which requires a number of operations that is polynomial in natural parameters of the given instance	mechanism	2K_dev_913
Although an envy-free cake cutting protocol was ultimately devised	purpose	2K_dev_913
it is unbounded in the sense that the number of operations can be arbitrarily large	purpose	2K_dev_913
depending on the preferences of the agents	purpose	2K_dev_913
We ask whether bounded protocols exist when the agents ' preferences are restricted	purpose	2K_dev_913
The resulting axiomatization of differential dynamic logic is proved to be sound and relatively complete	finding	2K_dev_915
This article introduces a relatively complete proof calculus ( dL ) that is entirely based on uniform substitution	mechanism	2K_dev_915
a proof rule that substitutes a formula for a predicate symbol everywhere Uniform substitutions make it possible to use axioms instead of axiom schemata	mechanism	2K_dev_915
thereby substantially simplifying implementations Instead of subtle schema variables and soundness-critical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones	mechanism	2K_dev_915
the resulting calculus adopts only a finite number of ordinary dLformulas as axioms	mechanism	2K_dev_915
which uniform substitutions instantiate soundly	mechanism	2K_dev_915
The static semantics of differential dynamic logic and the soundness-critical restrictions it imposes on proof steps is captured exclusively in uniform substitutions and variable renamings as opposed to being spread in delicate ways across the prover implementation In addition to sound uniform substitutions	mechanism	2K_dev_915
this article introduces differential forms for differential dynamic logic that make it possible to internalize differential invariants	mechanism	2K_dev_915
differential substitutions and derivatives as first-class axioms to reason about differential equations axiomatically	mechanism	2K_dev_915
for differential dynamic logic	purpose	2K_dev_915
How can we analyze large-scale real-world data with various attributes ? Many real-world data ( e	background	2K_dev_926
network traffic logs web data	background	2K_dev_926
social networks knowledge bases	background	2K_dev_926
and sensor streams ) with multiple attributes are represented as multi-dimensional arrays	background	2K_dev_926
For analyzing a tensor	background	2K_dev_926
tensor decompositions are widely used in many data mining applications : detecting malicious attackers in network traffic logs ( with source IP	background	2K_dev_926
destination IP port-number timestamp )	background	2K_dev_926
finding telemarketers in a phone call history ( with sender	background	2K_dev_926
receiver date ) and identifying interesting concepts in a knowledge base ( with subject	background	2K_dev_926
and discover hidden concepts	finding	2K_dev_926
In this paper we propose HaTen2	mechanism	2K_dev_926
a distributed method that runs on the MapReduce framework	mechanism	2K_dev_926
Our careful design and implementation of HaTen2 dramatically reduce the size of intermediate data and the number of jobs leading to achieve high scalability compared with the state-of-the-art method	mechanism	2K_dev_926
Thanks to HaTen2 we analyze big real-world sparse tensors that can not be handled by the current state of the art	method	2K_dev_926
However current tensor decomposition methods do not scale to large and sparse real-world tensors with millions of rows and columns and `fibers for large-scale tensor decompositions	purpose	2K_dev_926
Mental simulation is an important skill for program understanding and prediction of program behavior Finally	background	2K_dev_927
we present recommendations for question prompt design to foster better student simulation of program execution	background	2K_dev_927
Analysis of student responses suggest that this type of question can be used to identify misconceptions and misinterpretation of instructions	finding	2K_dev_927
This poster presents the iterative design and refinement process using a novel introductory computational thinking curriculum for Microsoft 's Kodu Game Lab	mechanism	2K_dev_927
We present an analysis of question prompts and student responses from data collected from three rising 3rd - 6th graders where the curriculum was implemented	method	2K_dev_927
Assessing students ' ability to mentally simulate program execution can be challenging in graphical programming environments and on paper-based assessments	purpose	2K_dev_927
for assessing students ability to mentally simulate and predict code behavior	purpose	2K_dev_927
We show that using our approximate maximum flow algorithm	finding	2K_dev_928
we can efficiently determine whether a given directed graph is -balanced	finding	2K_dev_928
We introduce the notion of balance for directed graphs : aweighted directed graph is -balanced if for every cut S V	mechanism	2K_dev_928
the total weight of edges going from S to V S is within factor of the total weight of edges going from V S to S	mechanism	2K_dev_928
We first revisit oblivious routings in directed graphs	mechanism	2K_dev_928
Our main algorithmic result is an oblivious routing scheme for single-source instances that achieve an O ( log 3 n / loglog n ) competitive ratio	mechanism	2K_dev_928
In the process we make several technical contributions which may be of independent interest	mechanism	2K_dev_928
In particular we give an efficient algorithm We also define and construct low-stretch arborescences	mechanism	2K_dev_928
a generalization of low-stretch spanning trees to directed graphs	mechanism	2K_dev_928
On the negative side	mechanism	2K_dev_928
we present new lower bounds for oblivious routing problems on directed graphs We show that the competitive ratio of oblivious routing algorithms for directed graphs is ( n ) in general ; this result improves upon the long-standing best known lower bound of ( n ) by Hajiaghayi et al	mechanism	2K_dev_928
We also show that our restriction to single-source instances is necessary by showing an ( n ) lower bound for multiple-source oblivious routing in Eulerian graphs	mechanism	2K_dev_928
We also study the maximum flow problem in balanced directed graphs with arbitrary capacities We develop an efficient algorithm that finds an ( 1+ ) -approximate maximum flows in -balanced graphs in time O ( m 2 / 2 )	mechanism	2K_dev_928
Additionally we give an application to the directed sparsest cut problem	method	2K_dev_928
We use the notion of balance to give a more fine-grained understanding of several well-studied routing questions that are considerably harder in directed graphs for computing low-radius decompositions of directed graphs parameterized by balance	purpose	2K_dev_928
Virtual machine ( VM ) migration demands distinct properties under resource oversubscription and workload surges	background	2K_dev_929
We show that our implementation	finding	2K_dev_929
resolves VM contention up to several times faster than live migration	finding	2K_dev_929
We present enlightened post-copy	mechanism	2K_dev_929
a new mechanism that evicts the target VM with fast execution transfer and short total duration This design contrasts with common live migration	mechanism	2K_dev_929
which uses the down time of the migrated VM as its primary metric ; it instead focuses on recovering the aggregate performance of the VMs being affected	mechanism	2K_dev_929
In enlightened post-copy the guest OS identifies memory state that is expected to encompass the VM 's working set The hypervisor accordingly transfers its state	mechanism	2K_dev_929
mitigating the performance impact on the migrated VM resulting from post-copy transfer	mechanism	2K_dev_929
with modest instrumentation in guest Linux	method	2K_dev_929
for VMs under contention	purpose	2K_dev_929
Sequential games of perfect information can be solved by backward induction	background	2K_dev_941
where solutions to endgames are propagated up the game tree	background	2K_dev_941
show that our approach leads to significant performance improvements in practice	finding	2K_dev_941
Nonetheless we show that endgame solving can have significant benefits in imperfectinformation games with large state and action spaces : computation of exact ( rather than approximate ) equilibrium strategies	mechanism	2K_dev_941
computation of relevant equilibrium refinements	mechanism	2K_dev_941
significantly finer-grained action and information abstraction	mechanism	2K_dev_941
new information abstraction algorithms that take into account the relevant distribution of players types entering the endgame	mechanism	2K_dev_941
being able to select the coarseness of the action abstraction dynamically	mechanism	2K_dev_941
additional abstraction techniques for speeding up endgame solving	mechanism	2K_dev_941
a solution to the off-tree problem	mechanism	2K_dev_941
and using different degrees of probability thresholding in modeling versus playing	mechanism	2K_dev_941
We discuss each of these topics in detail	mechanism	2K_dev_941
and introduce techniques that enable one even when the number of states and actions in the game is large	mechanism	2K_dev_941
Our experiments on two-player no-limit Texas Holdem poker	method	2K_dev_941
However this does not work in imperfect-information games because different endgames can contain states that belong to the same information set and can not be treated independently	purpose	2K_dev_941
In fact we show that this approach can fail even in a simple game with a unique equilibrium and a single endgame	purpose	2K_dev_941
to conduct endgame solving in a scalable way	purpose	2K_dev_941
Many commercial products and academic research activities are embracing behavior analysis as a technique for improving detection of attacks of many sortsfrom retweet boosting	background	2K_dev_946
hashtag hijacking to link advertising	background	2K_dev_946
Traditional approaches focus on detecting dense blocks in the adjacency matrix of graph data	background	2K_dev_946
and recently the tensors of multimodal data	background	2K_dev_946
where it improves the F1 score over previous techniques by 68percent and finds suspicious behavioral patterns in social datasets spanning 0	finding	2K_dev_946
In this paper we first give a list of axioms that any metric of suspiciousness should satisfy ; we propose an intuitive	mechanism	2K_dev_946
principled metric that satisfies the axioms	mechanism	2K_dev_946
and is fast to compute ; moreover	mechanism	2K_dev_946
we propose CrossSpot an algorithm typically indicating fraud or some other noteworthy deviation from the usual	mechanism	2K_dev_946
and sort them in the order of importance ( suspiciousness )	mechanism	2K_dev_946
Finally we apply CrossSpot to the real data	method	2K_dev_946
No method gives a principled way to score the suspiciousness of dense blocks with different numbers of modes and rank them to draw human attention accordingly	purpose	2K_dev_946
to spot dense blocks that are worth inspecting	purpose	2K_dev_946
How much has a network changed since yesterday ? How different is the wiring of Bobs brain ( a left-handed male ) and Alices brain ( a right-handed female )	background	2K_dev_952
and how is it different ? Graph similarity with given node correspondence	background	2K_dev_952
the detection of changes in the connectivity of graphs	background	2K_dev_952
arises in numerous settings	background	2K_dev_952
showcase the advantages of our method over existing similarity measures	finding	2K_dev_952
We propose D elta C on	mechanism	2K_dev_952
a principled intuitive and scalable algorithm ( e	mechanism	2K_dev_952
employees of a company	mechanism	2K_dev_952
customers of a mobile carrier In conjunction	mechanism	2K_dev_952
we propose D elta C on -A ttr	mechanism	2K_dev_952
and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs	method	2K_dev_952
Experiments on various synthetic and real graphs Finally	method	2K_dev_952
we employ D elta C on and D elta C on -A ttr on real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs	method	2K_dev_952
( b ) do temporal anomaly detection in the who-emails-whom Enron graph and find the top culprits for the changes in the temporal corporate email graph	method	2K_dev_952
and ( c ) recover pairs of test-retest large brain scans ( 17M edges	method	2K_dev_952
up to 90M edges ) for 21 subjects	method	2K_dev_952
In this work we formally state the axioms and desired properties of the graph similarity functions	purpose	2K_dev_952
that assesses the similarity between two graphs on the same nodes that enables attribution of change or dissimilarity to responsible nodes and edges	purpose	2K_dev_952
Effective enforcement of laws and policies requires expending resources to prevent and detect offenders	background	2K_dev_953
as well as appropriate punishment schemes to deter violators	background	2K_dev_953
In particular enforcement of privacy laws and policies in modern organizations that hold large volumes of personal information ( e	background	2K_dev_953
hospitals banks ) relies heavily on internal audit mechanisms	background	2K_dev_953
We present an audit game model that is a natural generalization of a standard security game model Computing the Stackelberg equilibrium for this game is challenging because it involves solving an optimization problem with non-convex quadratic constraints We present an additive FPTAS that efficiently computes the solution	mechanism	2K_dev_953
We study economic considerations in the design of these mechanisms	purpose	2K_dev_953
focusing in particular on effective resource allocation and appropriate punishment schemes	purpose	2K_dev_953
for resource allocation with an additional punishment parameter	purpose	2K_dev_953
Given a large collection of time-evolving activities	background	2K_dev_957
such as Google search queries	background	2K_dev_957
which consist of d keywords/activities for m locations of duration n	background	2K_dev_957
how can we analyze temporal patterns and relationships among all these activities and find location-specific trends ? How do we go about capturing non-linear evolutions of local activities and forecasting future patterns ? For example	background	2K_dev_957
assume that we have the online search volume for multiple keywords	background	2K_dev_957
`` Nokia/Nexus/Kindle '' or `` CNN/BBC '' for 236 countries/territories	background	2K_dev_957
from 2004 to 2015	background	2K_dev_957
demonstrate that COMPCUBE consistently outperforms the best state-of- the-art methods in terms of both accuracy and execution speed	finding	2K_dev_957
We present COMPCUBE a unifying non-linear model	mechanism	2K_dev_957
which provides a compact and powerful representation of co-evolving activities ; and also a novel fitting algorithm	mechanism	2K_dev_957
COMPCUBE-FIT which is parameter-free and scalable Our method captures the following important patterns : ( B ) asic trends	mechanism	2K_dev_957
non-linear dynamics of co-evolving activities	mechanism	2K_dev_957
signs of ( C ) ompetition and latent interaction	mechanism	2K_dev_957
Nexus ( S ) easonality	mechanism	2K_dev_957
a Christmas spike for iPod in the U	mechanism	2K_dev_957
and Europe and ( D ) eltas	mechanism	2K_dev_957
unrepeated local events such as the U	mechanism	2K_dev_957
election in 2008 Thanks to its concise but effective summarization	mechanism	2K_dev_957
COMPCUBE can also forecast long-range future activities	mechanism	2K_dev_957
Extensive experiments on real datasets	method	2K_dev_957
Our goal is to analyze a large collection of multi-evolving activities	purpose	2K_dev_957
and specifically to answer the following questions : ( a ) Is there any sign of interaction/competition between two different keywords If so	purpose	2K_dev_957
who competes with whom ? ( b ) In which country is the competition strong ? ( c ) Are there any seasonal/annual activities ? ( d ) How can we automatically detect important world-wide ( or local ) events ?	purpose	2K_dev_957
Given a large collection of co-evolving online activities	background	2K_dev_959
such as searches for the keywords `` Xbox ''	background	2K_dev_959
`` PlayStation '' and `` Wii ''	background	2K_dev_959
how can we find patterns and rules ? Are these keywords related ? If so	background	2K_dev_959
are they competing against each other ?	background	2K_dev_959
show that EcoWeb is effective	finding	2K_dev_959
in that it can capture long-range dynamics and meaningful patterns such as seasonalities	finding	2K_dev_959
and practical in that it can provide accurate long-range forecasts	finding	2K_dev_959
EcoWeb consistently outperforms existing methods in terms of both accuracy and execution speed	finding	2K_dev_959
We present EcoWeb ( i	mechanism	2K_dev_959
Ecosystem on the Web )	mechanism	2K_dev_959
which is an intuitive model designed as a non-linear dynamical system Our second contribution is a novel	mechanism	2K_dev_959
parameter-free and scalable fitting algorithm	mechanism	2K_dev_959
EcoWeb-Fit that estimates the parameters of EcoWeb	mechanism	2K_dev_959
Extensive experiments on real data	method	2K_dev_959
Can we forecast the volume of user activity for the coming month ? We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food for mining large-scale co-evolving online activities	purpose	2K_dev_959
results show that the proposed data-driven approach works well in a smart grid setting with increasing uncertainties and it produces an online state estimate excelling current industrial approach	finding	2K_dev_962
we propose a data-driven state estimation approach based on recent targeted investment on sensors	mechanism	2K_dev_962
data storage and computing devices	mechanism	2K_dev_962
An architecture is proposed to use power system physics and pattern to systematically clean historical data and conduct supervised learning	mechanism	2K_dev_962
where historical similar measurements and their states are used to learn the relationship between the current measurement and the state	mechanism	2K_dev_962
In order to deal with nonlinearity	mechanism	2K_dev_962
kernel trick is used to produce linear mapping in a carefully selected higher dimensional space To speed up the data-driven approach for online services	mechanism	2K_dev_962
we analyze power system data set and discover its clustering property due to the periodic pattern of power systems	mechanism	2K_dev_962
This leads to significant dimension reduction and the idea of preorganizing data points in a tree structure for inquiry	mechanism	2K_dev_962
leading to 1000 times speedup	mechanism	2K_dev_962
A grand challenge for state estimation in newly built smart grid lies in how to deal with the increasing uncertainties	purpose	2K_dev_962
To solve the problem	purpose	2K_dev_962
Cyber-physical systems ( CPS ) are heterogeneous	background	2K_dev_964
be- cause they tightly couple computation	background	2K_dev_964
communication and control along with physical dynamics	background	2K_dev_964
which are traditionally considered separately	background	2K_dev_964
Without a comprehensive modeling formalism	background	2K_dev_964
model- based development of CPS involves using a multitude of models in a variety of formalisms that capture various aspects of the system design	background	2K_dev_964
such as software design	background	2K_dev_964
networking design physical mod- els	background	2K_dev_964
In this paper we propose a multi-view architecture framework that treats models as views of the under- lying system structure and uses structural and semantic mappings Index TermsControl design	mechanism	2K_dev_964
control engineering formal veri- fication	mechanism	2K_dev_964
Throughout the paper the theoretical concepts are illustrated using two examples : a quad- rotor and an automotive intersection collision avoidance system	method	2K_dev_964
Without a rigorous unifying framework	purpose	2K_dev_964
system integration and integration of the analysis results for vari- ous models remains ad hoc to ensure consistency and enable system-level verification in a hierarchical and compositional manner	purpose	2K_dev_964
The maximum Nash welfare ( MNW ) solution -- - which selects an allocation that maximizes the product of utilities -- - is known to provide outstanding fairness guarantees when allocating divisible goods	background	2K_dev_966
These results lead us to believe that MNW is the ultimate solution for allocating indivisible goods	background	2K_dev_966
and underlie its deployment on a popular fair division website	background	2K_dev_966
demonstrate that it scales well	finding	2K_dev_966
And while it seems to lose its luster when applied to indivisible goods	mechanism	2K_dev_966
we show that in fact	mechanism	2K_dev_966
the MNW solution is unexpectedly	mechanism	2K_dev_966
strikingly fair even in that setting	mechanism	2K_dev_966
We also establish that the MNW solution provides a good approximation to another popular ( yet possibly infeasible ) fairness property	mechanism	2K_dev_966
the maximin share guarantee	mechanism	2K_dev_966
in theory and -- - even more so -- - in practice	mechanism	2K_dev_966
While finding the MNW solution is computationally hard	mechanism	2K_dev_966
we develop a nontrivial implementation	mechanism	2K_dev_966
In particular we prove that it selects allocations that are envy free up to one good -- - a compelling notion that is quite elusive when coupled with economic efficiency	purpose	2K_dev_966
The safety of mobile robots in dynamic environments is predicated on making sure that they do not collide with obstacles	background	2K_dev_968
Our verification results are generic in the sense that they are not limited to the particul ar choices of one specific control algorithm but identify conditions that make them simultaneously apply to a broad class of control algorithms	background	2K_dev_968
we prove that provably safe motion is flexible enough to let the r obot still navigate waypoints and pass intersections Moreover	finding	2K_dev_968
we formally prove that safety can still be guaranteed despite s ensor uncertainty and actuator perturbation	finding	2K_dev_968
and when control choices for more aggressive maneuvers are introduced	finding	2K_dev_968
and formally verify a series of increasingly powerful safety properties of controllers ( i ) static safety	mechanism	2K_dev_968
which ensures that no collisions can happen with stationary obstacles	mechanism	2K_dev_968
( ii ) passive safety	mechanism	2K_dev_968
which ensures that no collisions can happen with stationary or moving obstacles while the robot moves	mechanism	2K_dev_968
( iii ) the stronger passive friendly safety in which the robot further maintains sufficient maneuvering distance for obstacles to avoid collision as well	mechanism	2K_dev_968
and ( iv ) passive orientation safety	mechanism	2K_dev_968
which allows for imperfect sensor coverage of the robot	mechanism	2K_dev_968
the robot is aw are that not everything in its environment will be visible	mechanism	2K_dev_968
We complement these provably correct safety properties with liveness properties : We use hybrid system models and theorem proving techniques	mechanism	2K_dev_968
In support of such safety arguments	purpose	2K_dev_968
we analyze fo r avoiding both stationary and moving obstacles that describe and formally verify the robots discrete control decisions along with it s continuous	purpose	2K_dev_968
Given a bipartite graph of users and the products that they review	background	2K_dev_970
or followers and followees	background	2K_dev_970
how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral	background	2K_dev_970
) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph	background	2K_dev_970
Fraudsters can evade these methods using camouflage	background	2K_dev_970
by adding reviews or follows with honest targets so that they look `` normal ''	background	2K_dev_970
Even worse some fraudsters use hijacked accounts from honest users	background	2K_dev_970
and then the camouflage is indeed organic	background	2K_dev_970
( c ) is effective in real-world data	finding	2K_dev_970
show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud	finding	2K_dev_970
FRAUDAR successfully detected a subgraph of more than 4000 detected accounts	finding	2K_dev_970
of which a majority had tweets showing that they used follower-buying services	finding	2K_dev_970
We propose FRAUDAR an algorithm that ( a ) is camouflage-resistant	mechanism	2K_dev_970
( b ) provides upper bounds on the effectiveness of fraudsters	mechanism	2K_dev_970
Experimental results under various attacks Additionally	method	2K_dev_970
in real-world experiments with a Twitter follower-followee graph of 1	method	2K_dev_970
Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts	purpose	2K_dev_970
We describe a system called Olive that freezes and precisely reproduces the environment necessary It uses virtual machine ( VM ) technology	mechanism	2K_dev_977
complete with all its software dependencies	mechanism	2K_dev_977
This legacy world can be completely closed-source : there is no requirement for availability of source code	mechanism	2K_dev_977
nor a requirement for recompilation or relinking	mechanism	2K_dev_977
The entire VM is streamed over the Internet from a web server	mechanism	2K_dev_977
much as video is streamed today	mechanism	2K_dev_977
to execute software long after its creation to encapsulate legacy software	purpose	2K_dev_977
; and we observe power law growth for both nodes and links	finding	2K_dev_978
a fact that completely breaks the sigmoid models ( like SI	finding	2K_dev_978
where NETTIDE gives good fitting accuracy	finding	2K_dev_978
and more importantly applied on the WeChat data	finding	2K_dev_978
our NETTIDE forecasted more than 730 days into the future	finding	2K_dev_978
with 3 % error	finding	2K_dev_978
In its place we propose NETTIDE	mechanism	2K_dev_978
along with differential equations for the growth of the count of nodes	mechanism	2K_dev_978
as well as links	mechanism	2K_dev_978
Our model accurately fits the growth patterns of real graphs ; it is general	mechanism	2K_dev_978
encompassing as special cases all the known	mechanism	2K_dev_978
traditional models ( including Bass	mechanism	2K_dev_978
SI log-logistic growth ) ; while still remaining parsimonious	mechanism	2K_dev_978
requiring only a handful of parameters Moreover	mechanism	2K_dev_978
our NETTIDE for link growth is the first one of its kind	mechanism	2K_dev_978
accurately fitting real data	mechanism	2K_dev_978
and naturally leading to the densification phenomenon	mechanism	2K_dev_978
We examine the growth of several real networks	method	2K_dev_978
including one of the world 's largest online social network	method	2K_dev_978
`` WeChat '' with 300 million nodes and 4	method	2K_dev_978
75 billion links by 2013 We validate our model with four real	method	2K_dev_978
What is the growth pattern of social networks	purpose	2K_dev_978
like Facebook and WeChat ? Does it truly exhibit exponential early growth	purpose	2K_dev_978
as predicted by textbook models like the Bass model	purpose	2K_dev_978
SI or the Branching Process ? How about the count of links	purpose	2K_dev_978
over time for which there are few published models ?	purpose	2K_dev_978
We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR For example	finding	2K_dev_981
we observe both diffusion and non-diffusion mechanism in the group joining process	finding	2K_dev_981
and power-law decay in group quitting process	finding	2K_dev_981
rather than exponential decay as expected in SIR model	finding	2K_dev_981
Therefore we propose a new model comeNgo	mechanism	2K_dev_981
a concise yet flexible dynamic model Our model has the following advantages : ( a ) unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation	mechanism	2K_dev_981
( b ) succinctness and interpretability : it contains only six parameters with clear physical meanings	mechanism	2K_dev_981
( c ) accuracy : it can capture various kinds of group evolution patterns preciously and the goodness of fit increase by 58 % over baseline	mechanism	2K_dev_981
( d ) usefulness : it can be used in multiple application scenarios such as forecasting and pattern discovery	mechanism	2K_dev_981
In this paper we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users	method	2K_dev_981
How do social groups	purpose	2K_dev_981
such as Facebook groups and Wechat groups	purpose	2K_dev_981
dynamically evolve over time ? How do people join the social groups	purpose	2K_dev_981
uniformly or with burst ? What is the pattern of people quitting from groups ? Is there a simple universal model to depict the come-and-go patterns of various groups ? for group evolution	purpose	2K_dev_981
System management includes the selection of maintenance actions depending on the available observations : when a system is made up by components known to be similar	background	2K_dev_982
data collected on one is also relevant for the management of others	background	2K_dev_982
This is typically the case of wind farms	background	2K_dev_982
which are made up by similar turbines	background	2K_dev_982
Optimal management of wind farms is an important task due to high cost of turbines operation and maintenance : in this context	background	2K_dev_982
we recently proposed a method for planning and learning at system-level	background	2K_dev_982
called PLUS built upon the Partially Observable Markov Decision Process ( POMDP ) framework	background	2K_dev_982
which treats transition and emission probabilities as random variables	background	2K_dev_982
and is therefore suitable for including model uncertainty	background	2K_dev_982
and discuss its potential and computational complexity	finding	2K_dev_982
The proposed approach called Multiple Uncertain POMDP ( MU-POMDP )	mechanism	2K_dev_982
models the components as POMDPs	mechanism	2K_dev_982
and assumes the corresponding parameters as dependent random variables Through this framework	mechanism	2K_dev_982
we can calibrate specific degradation and emission models for each component while	mechanism	2K_dev_982
at the same time	mechanism	2K_dev_982
process observations at system-level	mechanism	2K_dev_982
We compare the performance of the proposed MU-POMDP with PLUS	method	2K_dev_982
PLUS models the components as independent or identical	purpose	2K_dev_982
In this paper we extend that formulation	purpose	2K_dev_982
allowing for a weaker similarity among components	purpose	2K_dev_982
Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type	background	2K_dev_983
and the tensor-based summaries look for high-order dense blocks by clustering the values ( including timestamps ) in each dimension	background	2K_dev_983
CatchTartan outperforms the baselines on both the accuracy and speed	finding	2K_dev_983
providing comprehensive summaries for the events	finding	2K_dev_983
human life and scientific development	finding	2K_dev_983
In this paper as a two-level matrix ( temporal-behaviors by dimensional-values ) and propose a novel representation called Tartan that includes a set of dimensions	mechanism	2K_dev_983
the values in each dimension	mechanism	2K_dev_983
a list of consecutive time slices and the behaviors in each slice	mechanism	2K_dev_983
We further develop a propagation method CatchTartan it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner	mechanism	2K_dev_983
We apply CatchTartan to four Twitter datasets up to 10 million tweets and the DBLP data	method	2K_dev_983
However the human behaviors are multicontextual and dynamic : ( 1 ) each behavior takes place within multiple contexts in a few dimensions	purpose	2K_dev_983
which requires the representation to enable non-value and set-values for each dimension ; ( 2 ) many behavior collections	purpose	2K_dev_983
such as tweets or papers	purpose	2K_dev_983
evolve over time we represent the behavioral data for behavioral summary to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way	purpose	2K_dev_983
We show that on these problems the proposed method performs very well	finding	2K_dev_987
solving the problems faster than state-of-the-art methods and to higher accuracy	finding	2K_dev_987
We present a new algorithmic approach This model has found many applications in multiple change point detection	mechanism	2K_dev_987
signal compression and total variation denoising	mechanism	2K_dev_987
though existing algorithms typically using first-order or alternating minimization schemes	mechanism	2K_dev_987
In this paper we instead develop a specialized projected Newton method	mechanism	2K_dev_987
combined with a primal active set approach	mechanism	2K_dev_987
which we show to be substantially faster that existing methods	mechanism	2K_dev_987
Furthermore we present two applications that use this algorithm as a fast subroutine for a more complex outer loop : segmenting linear regression models for time series data	mechanism	2K_dev_987
and color image denoising	mechanism	2K_dev_987
to the group fused lasso	purpose	2K_dev_987
a convex model that approximates a multi-dimensional signal via an approximately piecewise-constant signal	purpose	2K_dev_987
Strong Nash equilibrium ( SNE ) is an appealing solution concept when rational agents can form coalitions	background	2K_dev_990
A strategy profile is an SNE if no coalition of agents can benefit by deviating	background	2K_dev_990
validate the overall approach and show that the new conditions significantly reduce search tree size compared to using NE conditions alone	finding	2K_dev_990
We present the first general-purpose algorithms An SNE must simultaneously be a Nash equilibrium ( NE ) and the optimal solution of multiple non-convex optimization problems	mechanism	2K_dev_990
This makes even the derivation of necessary and sufficient mathematical equilibrium constraints difficult	mechanism	2K_dev_990
We show that forcing an SNE to be resilient only to pure-strategy deviations by coalitions	mechanism	2K_dev_990
unlike for NEs is only a necessary condition here	mechanism	2K_dev_990
Second we show that the application of Karush-Kuhn-Tucker conditions leads to another set of necessary conditions that are not sufficient	mechanism	2K_dev_990
Third we show that forcing the Pareto efficiency of an SNE for each coalition with respect to coalition correlated strategies is sufficient but not necessary We then develop a tree search algorithm for SNE finding At each node	mechanism	2K_dev_990
it calls an oracle to suggest a candidate SNE and then verifies the candidate	mechanism	2K_dev_990
We show that our new necessary conditions can be leveraged to make the oracle more powerful	mechanism	2K_dev_990
for SNE finding in games with more than two agents	purpose	2K_dev_990
How can we predict Smith 's main hobby if we know the main hobby of Smith 's friends ? Can we measure the confidence in our predic- tion if we are given the main hobby of only a few of Smith 's friends ?	background	2K_dev_991
results demonstrate that our algorithm outperforms other algorithms on graphs with less smoothness and low label density	finding	2K_dev_991
Providing a confidence level for the classification prob- lem is important because most nodes in real world networks tend to have few neighbors	mechanism	2K_dev_991
and thus a small amount of evidence	mechanism	2K_dev_991
Our contributions are three-fold : ( a ) novel algorithm ; we propose a semi-supervised learning algorithm that converges fast	mechanism	2K_dev_991
and provides the confidence estimate	mechanism	2K_dev_991
( b ) theoretical analysis ; we show the solid theoretical foundation of our algo- rithm and the connections to label propagation and Bayesian inference ( c ) empirical analysis ; we perform extensive experiments on three dif- ferent real networks Specifically	method	2K_dev_991
In this paper we focus on how to estimate the confidence on the node classi- fication problem	purpose	2K_dev_991
The fair division of indivisible goods has long been an important topic in economics and	background	2K_dev_993
more recently computer science	background	2K_dev_993
we show that even when the number of goods is larger than the number of agents by a linear fraction	finding	2K_dev_993
envy-free allocations are unlikely to exist	finding	2K_dev_993
We then show that when the number of goods is larger by a logarithmic factor	finding	2K_dev_993
such allocations exist with high probability show that the asymptotic behavior of the theory holds even when the number of goods and agents is quite small	finding	2K_dev_993
We demonstrate that there is a sharp phase transition from nonexistence to existence of envy-free allocations	finding	2K_dev_993
and that on average the computational problem is hardest at that transition	finding	2K_dev_993
We investigate the existence of envyfree allocations of indivisible goods	mechanism	2K_dev_993
Under additive valuations We support these results experimentally and	method	2K_dev_993
that is allocations where each player values her own allocated set of goods at least as highly as any other player 's allocated set of goods	purpose	2K_dev_993
Kidney exchanges are organized markets where patients swap willing but incompatible donors	background	2K_dev_997
In the last decade	background	2K_dev_997
kidney exchanges grew from small and regional to large and national -- -and soon	background	2K_dev_997
This growth results in more lives saved	background	2K_dev_997
but exacerbates the empirical hardness of the $ \mathcal { NP } $ -complete problem of optimally matching patients to donors	background	2K_dev_997
show that indeed small numbers of attributes suffice	finding	2K_dev_997
In this paper we observe that if the kidney exchange compatibility graph can be encoded by a constant number of patient and donor attributes	mechanism	2K_dev_997
We give necessary and sufficient conditions for losslessly shrinking the representation of an arbitrary compatibility graph	mechanism	2K_dev_997
Then using real compatibility graphs from the UNOS nationwide kidney exchange	mechanism	2K_dev_997
we show how many attributes are needed to encode real compatibility graphs	mechanism	2K_dev_997
State-of-the-art matching engines use integer programming techniques to clear fielded kidney exchanges	purpose	2K_dev_997
but these methods must be tailored to specific models and objective functions	purpose	2K_dev_997
and may fail to scale to larger exchanges the clearing problem is solvable in polynomial time	purpose	2K_dev_997
Our main theoretical result is that any graph specifying synergistic and antagonistic pairs can arise even from a restricted class of cooperative games	finding	2K_dev_1001
We think of a pair of agents as synergistic ( resp	mechanism	2K_dev_1001
antagonistic ) if the Shapley value of one agent when the other agent participates in a joint effort is higher ( resp	mechanism	2K_dev_1001
lower ) than when the other agent does not participate We also study the computational complexity of determining whether a given pair of agents is synergistic	mechanism	2K_dev_1001
Finally we use the concepts developed in the paper to uncover the structure of synergies in two real-world organizations	method	2K_dev_1001
the European Union and the International Monetary Fund	method	2K_dev_1001
We investigate synergy or lack thereof	purpose	2K_dev_1001
between agents in co-operative games	purpose	2K_dev_1001
building on the popular notion of Shapley value	purpose	2K_dev_1001
How do users behave if they can tag each other in social networks ? Twitter lists can be regarded as the tagging process ; a user ( i	background	2K_dev_1002
tagger ) creates a list with a name ( i	background	2K_dev_1002
tag ) and adds other users ( i	background	2K_dev_1002
tagged users ) into the list	background	2K_dev_1002
This tagging network is by nature different from the resource tagging networks ( e	background	2K_dev_1002
Flickr and Delicious ) because users on this network can tag each other	background	2K_dev_1002
This study sheds light on the underlying characteristics of the interactive tagging network	background	2K_dev_1002
which is relevant to the social scientists and the system designers of the tagging systems	background	2K_dev_1002
we found the pervasive patterns across the different tagging networks	finding	2K_dev_1002
and the interactive patterns within the interactive tagging network	finding	2K_dev_1002
By quantitatively studying million-scale networks	method	2K_dev_1002
In this paper we answer this question by studying the interactive tagging network constructed by Twitter lists	purpose	2K_dev_1002
We address the following research questions : ( RQ1 ) What is the common patterns and the difference between the interactive tagging network and the resource tagging networks ? ( RQ2 ) Do users tag each other on the interactive tagging network ? And if so	purpose	2K_dev_1002
to what extent ? ( RQ3 ) What is the difference between the two types of relationships on Twitter : who-tags-whom and who-follows-whom ?	purpose	2K_dev_1002
We introduce disciplined convex stochastic programming ( DCSP )	mechanism	2K_dev_1006
a modeling framework by allowing modelers to naturally express a wide variety of convex stochastic programs in a manner that reflects their underlying mathematical representation DCSP allows modelers to express expectations of arbitrary expressions	mechanism	2K_dev_1006
partial optimizations and chance constraints across a wide variety of convex optimization problem families ( e	mechanism	2K_dev_1006
linear quadratic second order cone	mechanism	2K_dev_1006
We illustrate DCSP 's expressivity through a number of sample implementations of problems drawn from the operations research	method	2K_dev_1006
finance and machine learning literatures	method	2K_dev_1006
that can significantly lower the barrier for modelers to specify and solve convex stochastic optimization problems	purpose	2K_dev_1006
This is one of the oldest non-trivial problems in computational geometry yet despite a long history of research the previous fastest running times for computing a ( 1+ ) -approximate geometric median were O ( d n 4/3 8/3 ) by Chin et	background	2K_dev_1010
al O ( d exp 4 log 1 ) by Badoiu et	background	2K_dev_1010
al O ( nd + poly ( d	background	2K_dev_1010
1 ) ) by Feldman and Langberg	background	2K_dev_1010
and the polynomial running time of O ( ( nd ) O ( 1 ) log1/ ) by Parrilo and Sturmfels and Xue and Ye	background	2K_dev_1010
In this paper we provide faster algorithms While our O ( d 2 ) is a fairly straightforward application of stochastic subgradient descent	mechanism	2K_dev_1010
our O ( nd log 3 n / ) time algorithm is a novel long step interior point method	mechanism	2K_dev_1010
We start with a simple O ( ( nd ) O ( 1 ) log1/ ) time interior point method and show how to improve it	mechanism	2K_dev_1010
ultimately building an algorithm that is quite non-standard from the perspective of interior point literature	mechanism	2K_dev_1010
Our result is one of few cases of outperforming standard interior point theory	mechanism	2K_dev_1010
Furthermore it is the only case we know of where interior point methods yield a nearly linear time algorithm for a canonical optimization problem that traditionally requires superlinear time	mechanism	2K_dev_1010
for solving the geometric median problem given n points in d compute a point that minimizes the sum of Euclidean distances to the points In this paper we show how to compute such an approximate geometric median in time O ( nd log 3 n / ) and O ( d 2 )	purpose	2K_dev_1010
Our main result is that biased games satisfying certain mild conditions always admit an equilibrium	finding	2K_dev_1014
We present that we call biased games	mechanism	2K_dev_1014
In these games a player 's utility is influenced by the distance between his mixed strategy and a given base strategy	mechanism	2K_dev_1014
We argue that biased games capture important aspects of the interaction between software agents We also tackle the computation of equilibria in biased games	mechanism	2K_dev_1014
a novel extension of normal form games	purpose	2K_dev_1014
A kidney exchange is an organized barter market where patients in need of a kidney swap willing but incompatible donors	background	2K_dev_1015
Determining an optimal set of exchanges is theoretically and empirically hard Traditionally	background	2K_dev_1015
exchanges took place in cycles	background	2K_dev_1015
with each participating patient-donor pair both giving and receiving a kidney	background	2K_dev_1015
The recent introduction of chains	background	2K_dev_1015
where a donor without a paired patient triggers a sequence of donations without requiring a kidney in return	background	2K_dev_1015
increased the efficacy of fielded kidney exchanges -- -while also dramatically raising the empirical computational hardness of clearing the market in practice Finally	background	2K_dev_1015
we note that our position-indexed chain-edge formulation can be modified in a straightforward way to take post-match edge failure into account	background	2K_dev_1015
under the restriction that edges have equal probabilities of failure	background	2K_dev_1015
Post-match edge failure is a primary source of inefficiency in presently-fielded kidney exchanges	background	2K_dev_1015
we show that our new models are competitive with all existing solvers -- -in many cases outperforming all other solvers by orders of magnitude	finding	2K_dev_1015
In this paper we address the tractable clearing of kidney exchanges with short cycles and chains that are long but bounded	mechanism	2K_dev_1015
This corresponds to the practice at most modern fielded kidney exchanges	mechanism	2K_dev_1015
We introduce three new integer programming formulations	mechanism	2K_dev_1015
two of which are compact Furthermore	mechanism	2K_dev_1015
one of these models has a linear programming relaxation that is exactly as tight as the previous tightest formulation ( which was not compact ) for instances in which each donor has a paired patient	mechanism	2K_dev_1015
We show how to implement such failure-aware matching in our model	mechanism	2K_dev_1015
and also extend the state-of-the-art general branch-and-price-based non-compact formulation for the failure-aware problem to run its pricing problem in polynomial time	mechanism	2K_dev_1015
On real data from the UNOS nationwide exchange in the United States and the NLDKSS nationwide exchange in the United Kingdom	method	2K_dev_1015
as well as on generated realistic large-scale data	method	2K_dev_1015
While chains can be quite long	purpose	2K_dev_1015
unbounded-length chains are not desirable : planned donations can fail before transplant for a variety of reasons	purpose	2K_dev_1015
and the failure of a single donation causes the rest of that chain to fail	purpose	2K_dev_1015
so parallel shorter chains are better in practice	purpose	2K_dev_1015
Kidney exchange is a barter market where patients trade willing but medically incompatible donors	background	2K_dev_1016
These trades occur via cycles	background	2K_dev_1016
where each patient-donor pair both gives and receives a kidney	background	2K_dev_1016
and via chains which begin with an altruistic donor who does not require a kidney in return For logistical reasons	background	2K_dev_1016
the maximum length of a cycle is typically limited to a small constant	background	2K_dev_1016
while chains can be much longer	background	2K_dev_1016
Given a compatibility graph of patient-donor pairs	background	2K_dev_1016
altruists and feasible potential transplants between them	background	2K_dev_1016
finding even a maximum-cardinality set of vertex-disjoint cycles and chains is NP-hard	background	2K_dev_1016
There has been much work on developing provably optimal solvers that are efficient in practice	background	2K_dev_1016
One of the leading techniques has been branch and price	background	2K_dev_1016
where column generation is used to incrementally bring cycles and chains into the optimization model on an as-needed basis	background	2K_dev_1016
This shows incorrectness of two leading branch-and-price solvers that suggested polynomial-time chain pricing algorithms	background	2K_dev_1016
In particular only positive-price columns need to be brought into the model	mechanism	2K_dev_1016
We prove that finding a positive-price chain is NP-complete	purpose	2K_dev_1016
An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy	background	2K_dev_1017
In a system with redundant requests	background	2K_dev_1017
each job that arrives to the system is copied and dispatched to multiple servers	background	2K_dev_1017
As soon as the first copy completes service	background	2K_dev_1017
the job is considered complete	background	2K_dev_1017
and all remaining copies are deleted	background	2K_dev_1017
We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity	finding	2K_dev_1017
We propose a theoretical model of redundancy	mechanism	2K_dev_1017
the Redundancy-d system in which each job sends redundant copies to d servers chosen uniformly at random	mechanism	2K_dev_1017
We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers	mechanism	2K_dev_1017
A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists	purpose	2K_dev_1017
Complex networks have been shown to exhibit universal properties	background	2K_dev_1018
with one of the most consistent patterns being the scale-free degree distribution	background	2K_dev_1018
and we show the pervasiveness of the power-hop	finding	2K_dev_1018
by identifying another power-law pattern that describes the relationship between the fractions of node pairs C ( r ) within r hops and the hop count r	mechanism	2K_dev_1018
This scale-free distribution is pervasive and describes a large variety of networks	mechanism	2K_dev_1018
ranging from social and urban to technological and biological networks In particular	mechanism	2K_dev_1018
inspired by the definition of the fractal correlation dimension D2 on a point-set	mechanism	2K_dev_1018
we consider the hop-count r to be the underlying distance metric between two vertices of the network	mechanism	2K_dev_1018
and we examine the scaling of C ( r ) with r	mechanism	2K_dev_1018
We find that this relationship follows a power-law in real networks within the range 2 r d	mechanism	2K_dev_1018
where d is the effective diameter of the network	mechanism	2K_dev_1018
that is the 90-th percentile distance We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h	mechanism	2K_dev_1018
We provide theoretical justification for this pattern under successful existing network models	method	2K_dev_1018
while we analyze a large set of real and synthetic network datasets	method	2K_dev_1018
but are there regularities obeyed by the r-hop neighborhood in real networks ? We answer this question	purpose	2K_dev_1018
An age-old problem in the design of server farms is the choice of the task assignment policy	background	2K_dev_1025
This is the algorithm that determines how to assign incoming jobs to servers	background	2K_dev_1025
Popular policies include Round-Robin assignment	background	2K_dev_1025
Join-the-Shortest-Queue Join-Queue-with-Least-Work and so on	background	2K_dev_1025
We show that when server-side variability dominates runtime	finding	2K_dev_1025
replication of jobs can be very beneficial	finding	2K_dev_1025
We introduce the Replication-d algorithm	mechanism	2K_dev_1025
where the job is considered `` done '' as soon as the first replica completes	mechanism	2K_dev_1025
We provide an exact closed-form analysis of Replication-d	mechanism	2K_dev_1025
We next introduce a much more general model	mechanism	2K_dev_1025
one which takes both the inherent job size distribution and the server-side variability into account	mechanism	2K_dev_1025
This is a departure from traditional queueing models which only allow for one `` size '' distribution We propose and analyze a new	mechanism	2K_dev_1025
Replicate-Idle-Queue ( RIQ )	mechanism	2K_dev_1025
which is designed to perform well given these dual sources of variability	mechanism	2K_dev_1025
While much research has studied assignment policies	purpose	2K_dev_1025
little has taken into account server-side variability -- the fact that the server we choose might be temporarily and unpredictably slow that replicates each arrival to d servers chosen at random task assignment policy	purpose	2K_dev_1025
Large graphs are prevalent in many applications and enable a variety of information dissemination processes	background	2K_dev_1030
meme virus and influence propagation	background	2K_dev_1030
How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way ( e	background	2K_dev_1030
stop a virus propagation	background	2K_dev_1030
facilitate the propagation of a piece of good idea	background	2K_dev_1030
etc ) ? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models	background	2K_dev_1030
In addition we reveal the intrinsic relationship between edge deletion and node deletion problems	finding	2K_dev_1030
results validate the effectiveness and efficiency of the proposed algorithms	finding	2K_dev_1030
We propose effective scalable algorithms	mechanism	2K_dev_1030
In this paper we study the problem of how to optimally place a set of edges ( e	purpose	2K_dev_1030
edge deletion and edge addition ) to optimize the leading eigenvalue of the underlying graph	purpose	2K_dev_1030
so that we can guide the dissemination process in a desired way for edge deletion and edge addition	purpose	2K_dev_1030
Recently fair division theory has emerged as a promising approach for allocation of multiple computational resources among agents While in reality agents are not all present in the system simultaneously	background	2K_dev_1032
previous work has studied static settings where all relevant information is known upfront	background	2K_dev_1032
We believe that our work informs the design of superior multiagent systems	background	2K_dev_1032
and at the same time expands the scope of fair division theory by initiating the study of dynamic and fair resource allocation mechanisms	background	2K_dev_1032
On the conceptual level	mechanism	2K_dev_1032
we develop a dynamic model of fair division	mechanism	2K_dev_1032
and propose desirable axiomatic properties On the technical level	mechanism	2K_dev_1032
we construct two novel mechanisms that provably satisfy some of these properties	mechanism	2K_dev_1032
and analyze their performance using real data	method	2K_dev_1032
Our goal is to better understand the dynamic setting for dynamic resource allocation mechanisms	purpose	2K_dev_1032
Our main result is a characterization of worst-case optimal truthful estimators	finding	2K_dev_1034
which provably outperform the median	finding	2K_dev_1034
for possibly asymmetric distributions with bounded support	finding	2K_dev_1034
taking a game-theoretic viewpoint In our setting	mechanism	2K_dev_1034
samples are supplied by strategic agents	mechanism	2K_dev_1034
who wish to pull the estimate as close as possible to their own value In this setting	mechanism	2K_dev_1034
the sample mean gives rise to manipulation opportunities	mechanism	2K_dev_1034
whereas the sample median does not We show that when the underlying distribution is symmetric	mechanism	2K_dev_1034
there are truthful estimators that dominate the median	mechanism	2K_dev_1034
We revisit the classic problem of estimating the population mean of an unknown single-dimensional distribution from samples Our key question is whether the sample median is the best ( in terms of mean squared error ) truthful estimator of the population mean	purpose	2K_dev_1034
and show it often achieves order of magnitude speedups over existing general-purpose optimization solvers	finding	2K_dev_1035
This paper develops an approach a common general-purpose modeling framework	mechanism	2K_dev_1035
Specifically we develop an algorithm based upon fast epigraph projections	mechanism	2K_dev_1035
projections onto the epigraph of a convex function	mechanism	2K_dev_1035
an approach closely linked to proximal operator methods	mechanism	2K_dev_1035
We show that by using these operators	mechanism	2K_dev_1035
we can solve any disciplined convex program without transforming the problem to a standard cone form	mechanism	2K_dev_1035
as is done by current DCP libraries We then develop a large library of efficient epigraph projection operators	mechanism	2K_dev_1035
mirroring and extending work on fast proximal algorithms	mechanism	2K_dev_1035
for many common convex functions	mechanism	2K_dev_1035
Finally we evaluate the performance of the algorithm	method	2K_dev_1035
for efficiently solving general convex optimization problems specified as disciplined convex programs ( DCP )	purpose	2K_dev_1035
Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines	background	2K_dev_1036
which could be of interest to studies on website profiling	background	2K_dev_1036
Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models	background	2K_dev_1036
First we find that legitimate but compromised websites constitute 33	finding	2K_dev_1036
1 % of the malicious websites in our dataset	finding	2K_dev_1036
with an accuracy of 95	finding	2K_dev_1036
3 % Second we find that malicious URLs can be surprisingly long-lived	finding	2K_dev_1036
with 10 % of malicious sites staying active for three months or more Third	finding	2K_dev_1036
we observe that a significant number of URLs exhibit the same temporal pattern that suggests a flush-crowd behavior	finding	2K_dev_1036
inflicting most of their damage during the first few days of appearance	finding	2K_dev_1036
Finally the distribution of the visits to malicious sites per user is skewed	finding	2K_dev_1036
4 % of users visiting more than 10 malicious sites in 8 months	finding	2K_dev_1036
we develop a classifier	mechanism	2K_dev_1036
We conduct an extensive study and follow a website-centric and user-centric point of view We collect data from four online databases	method	2K_dev_1036
including Symantec 's WINE Project	method	2K_dev_1036
for a total of more than 600K malicious URLs and over 500K users	method	2K_dev_1036
Yet there has been relatively little work in modeling the behaviors and temporal properties of websites	purpose	2K_dev_1036
as most research focuses on detecting whether a website distributes malware	purpose	2K_dev_1036
In this paper we ask : How does web-based malware spread ? In order to conduct this study	purpose	2K_dev_1036
to distinguish between compromised vs	purpose	2K_dev_1036
The design of revenue-maximizing combinatorial auctions	background	2K_dev_1037
multi item auctions over bundles of goods	background	2K_dev_1037
is one of the most fundamental problems in computational economics	background	2K_dev_1037
unsolved even for two bidders and two items for sale	background	2K_dev_1037
In the traditional economic models	background	2K_dev_1037
it is assumed that the bidders ' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution Despite this strong and oftentimes unrealistic assumption	background	2K_dev_1037
it is remarkable that the revenue-maximizing combinatorial auction remains unknown The most scalable automated mechanism design algorithms take as input samples from the bidders ' valuation distribution and then search for a high-revenue auction in a rich auction class	background	2K_dev_1037
In this work we provide the first sample complexity analysis In particular	mechanism	2K_dev_1037
we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying	mechanism	2K_dev_1037
unknown distribution over bidder valuations	mechanism	2K_dev_1037
for each of the auction classes in the hierarchy In addition to helping set automated mechanism design on firm foundations	mechanism	2K_dev_1037
our results also push the boundaries of learning theory	mechanism	2K_dev_1037
In particular the hypothesis functions used in our contexts are defined through multi stage combinatorial optimization procedures	mechanism	2K_dev_1037
rather than simple decision boundaries	mechanism	2K_dev_1037
as are common in machine learning	mechanism	2K_dev_1037
In recent years automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design	purpose	2K_dev_1037
that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data	finding	2K_dev_1040
We introduce the Multiple Quantile Graphical Model ( MQGM )	mechanism	2K_dev_1040
which extends the neighborhood selection approach of Meinshausen and Buhlmann The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others	mechanism	2K_dev_1040
Our approach models a set of conditional quantiles of one variable as a sparse function of all others	mechanism	2K_dev_1040
and hence offers a much richer	mechanism	2K_dev_1040
more expressive class of conditional distribution estimates We establish that	mechanism	2K_dev_1040
under suitable regularity conditions	mechanism	2K_dev_1040
the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows	mechanism	2K_dev_1040
even outside of the usual homoskedastic Gaussian data model	mechanism	2K_dev_1040
We develop an efficient algorithm using the alternating direction method of multipliers	mechanism	2K_dev_1040
We also describe a strategy	mechanism	2K_dev_1040
Lastly we present detailed experiments	method	2K_dev_1040
for learning sparse graphical models	purpose	2K_dev_1040
for fitting the MQGM for sampling from the joint distribution that underlies the MQGM estimate	purpose	2K_dev_1040
For each user we discover and explain a surprising	finding	2K_dev_1041
bi-modal pattern of the inter-arrival time ( IAT ) of landed queries ( queries with user click-through ) we then notice the correlations among its parameters at the group level	finding	2K_dev_1041
In this paper we present a novel	mechanism	2K_dev_1041
user-and group-level framework M3A : Model	mechanism	2K_dev_1041
MetaModel and Anomaly detection Specifically	mechanism	2K_dev_1041
the model Camel-Log is proposed Thus	mechanism	2K_dev_1041
we further propose the metamodel Meta-Click	mechanism	2K_dev_1041
Combining Camel-Log and Meta-Click	mechanism	2K_dev_1041
the proposed M3A has the following strong points : ( 1 ) the accurate modeling of marginal IAT distribution	mechanism	2K_dev_1041
( 2 ) quantitative interpretations	mechanism	2K_dev_1041
and ( 3 ) anomaly detection	mechanism	2K_dev_1041
We studied what is probably the largest	method	2K_dev_1041
publicly available query log that contains more than 30 million queries from 0	method	2K_dev_1041
'Alice ' is submitting one web search per five minutes	purpose	2K_dev_1041
for three hours in a row - is it normal ? How to detect abnormal search behaviors	purpose	2K_dev_1041
among Alice and other users ? Is there any distinct pattern in Alice 's ( or other users ' ) search behavior ? to describe such an IAT distribution to capture and explain the two-dimensional	purpose	2K_dev_1041
heavy-tail distribution of the parameters	purpose	2K_dev_1041
Recent computer systems research has proposed using redundant requests to reduce latency	background	2K_dev_1044
The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request	background	2K_dev_1044
We find some surprising results First	finding	2K_dev_1044
the response time of a fully redundant class follows a simple exponential distribution and that of the non-redundant class follows a generalized hyperexponential	finding	2K_dev_1044
Second fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant We find that	finding	2K_dev_1044
in many cases redundancy outperforms JSQ and Opt-Split with respect to overall response time	finding	2K_dev_1044
making it an attractive solution	finding	2K_dev_1044
This paper presents the first exact analysis of systems with redundancy We allow for any number of classes of redundant requests	mechanism	2K_dev_1044
any number of classes of non-redundant requests	mechanism	2K_dev_1044
any degree of redundancy	mechanism	2K_dev_1044
and any number of heterogeneous servers	mechanism	2K_dev_1044
In all cases we derive the limiting distribution of the state of the system	mechanism	2K_dev_1044
In small ( two or three server ) systems	mechanism	2K_dev_1044
we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes	mechanism	2K_dev_1044
and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy	mechanism	2K_dev_1044
We also compare redundancy with other approaches for reducing latency	method	2K_dev_1044
such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and join-the-shortest-queue ( JSQ ) routing of a class	method	2K_dev_1044
However there is no exact analysis of systems with redundancy	purpose	2K_dev_1044
Suspicious graph patterns show up in many applications	background	2K_dev_1059
from Twitter users who buy fake followers	background	2K_dev_1059
manipulating the social network	background	2K_dev_1059
to botnet members performing distributed denial of service attacks	background	2K_dev_1059
disturbing the network traffic graph	background	2K_dev_1059
C atch S ync consistently outperforms existing competitors	finding	2K_dev_1059
both in detection accuracy by 36p on Twitter and 20p on Tencent Weibo	finding	2K_dev_1059
as well as in speed	finding	2K_dev_1059
We propose a fast and effective method	mechanism	2K_dev_1059
C atch S ync	mechanism	2K_dev_1059
which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority We introduce novel measures and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots	mechanism	2K_dev_1059
Thanks to careful design	mechanism	2K_dev_1059
C atch S ync has the following desirable properties : ( a ) it is scalable to large datasets	mechanism	2K_dev_1059
being linear in the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology	mechanism	2K_dev_1059
without needing labeled data	mechanism	2K_dev_1059
nor timing information and the like	mechanism	2K_dev_1059
while still capable of using side information if available	mechanism	2K_dev_1059
We applied C atch S ync on three large	method	2K_dev_1059
real datasets 1-billion-edge Twitter social graph	method	2K_dev_1059
3-billion-edge and 12-billion-edge Tencent Weibo social graphs	method	2K_dev_1059
and several synthetic ones ;	method	2K_dev_1059
Given a directed graph of millions of nodes	purpose	2K_dev_1059
how can we automatically spot anomalous	purpose	2K_dev_1059
suspicious nodes judging only from their connectivity patterns ? to quantify both concepts ( synchronicity and normality )	purpose	2K_dev_1059
Suppose you are a teacher	background	2K_dev_1061
and have to convey a set of object-property pairs 'lions eat meat '	background	2K_dev_1061
A good teacher will convey a lot of information	background	2K_dev_1061
with little effort on the student side	background	2K_dev_1061
What is the best and most intuitive way to convey this information to the student	background	2K_dev_1061
without the student being overwhelmed ?	background	2K_dev_1061
it is effective achieving excellent results on real data	finding	2K_dev_1061
both with respect to our proposed metric	finding	2K_dev_1061
but also with respect to encoding length demonstrate the effectiveness of groupNteach	finding	2K_dev_1061
we provide a metric based on information theory	mechanism	2K_dev_1061
We also design an algorithm	mechanism	2K_dev_1061
groupNteach Our proposed groupNteach is scalable near-linear in the dataset size ; and it is intuitive	mechanism	2K_dev_1061
conforming to well-known educational principles	mechanism	2K_dev_1061
Experiments on real and synthetic datasets	method	2K_dev_1061
A related harder problem is : how can we assign a numerical score to each lesson plan i	purpose	2K_dev_1061
way of conveying information ? Here	purpose	2K_dev_1061
we give a formal definition of this problem of forming learning units and for comparing different approaches for this problem	purpose	2K_dev_1061
Acute hypotensive episodes ( AHEs ) are serious clinical events in intensive care units ( ICUs )	background	2K_dev_1070
and require immediate treatment to prevent patient injury	background	2K_dev_1070
HeartCast was found to outperform other state-of-the-art methods found in the literature with a 13	finding	2K_dev_1070
7 % improvement in classification accuracy	finding	2K_dev_1070
We propose HeartCast a model that extracts essential features from such data HeartCast combines a non-linear support vector machine with best-feature extraction via analysis of the baseline threshold	mechanism	2K_dev_1070
quartile parameters and window size of the physiological signals	mechanism	2K_dev_1070
Our approach has the following benefits : ( a ) it extracts the most relevant features ; ( b ) it provides the best results for identification of an AHE event ; ( c ) it is fast and scales with linear complexity over the length of the window ; and ( d ) it can manage missing values and noise/outliers by using a best-feature extraction method	mechanism	2K_dev_1070
We performed experiments on data continuously captured from physiological time series of ICU patients ( roughly 3 GB of processed data )	method	2K_dev_1070
Reducing the risks associated with an AHE requires effective and efficient mining of data generated from multiple physiological time series	purpose	2K_dev_1070
to effectively predict AHE	purpose	2K_dev_1070
What is a fair way to assign rooms to several housemates	background	2K_dev_1073
and divide the rent between them ? This is not just a theoretical question : many people have used the Spliddit website to obtain envy-free solutions to rent division instances	background	2K_dev_1073
But envy freeness in and of itself	background	2K_dev_1073
is insufficient to guarantee outcomes that people view as intuitive and acceptable	background	2K_dev_1073
Based on these results	background	2K_dev_1073
the maximin solution has been deployed on Spliddit since April 2015	background	2K_dev_1073
and identify the maximin solution	finding	2K_dev_1073
which maximizes the minimum utility subject to envy freeness	finding	2K_dev_1073
as the most attractive that the maximin solution gives rise to significant gains in terms of our optimization objectives demonstrates that people find the maximin solution to be significantly fairer than arbitrary envy-free solutions ; this user study is unprecedented in that it asks people about their real-world rent division instances	finding	2K_dev_1073
We develop a general algorithmic framework	mechanism	2K_dev_1073
We then study the relations between natural optimization objectives	method	2K_dev_1073
We demonstrate in theory and using experiments on real data from Spliddit	method	2K_dev_1073
Finally a user study with Spliddit users as subjects	method	2K_dev_1073
We therefore focus on solutions that optimize a criterion of social justice	purpose	2K_dev_1073
subject to the envy freeness constraint	purpose	2K_dev_1073
in order to pinpoint the `` fairest '' solutions	purpose	2K_dev_1073
that enables the computation of such solutions in polynomial time	purpose	2K_dev_1073
Component-based modeling can be used to split large models into partial models to reduce modeling complexity	background	2K_dev_1075
In this paper we propose a component-based hybrid system verification approach that combines the advantages of component-based modeling e	mechanism	2K_dev_1075
reduced model complexity with the advantages of formal verification e	mechanism	2K_dev_1075
guaranteed contract compliance Our strategy is to decompose the system into components	mechanism	2K_dev_1075
verify their local safety individually and compose them to form an overall system that provably satisfies a global contract	mechanism	2K_dev_1075
without proving the whole system We introduce the necessary formalism and a technique such that safety properties provably emerge from component safety	mechanism	2K_dev_1075
We study a component-based approach to simplify the challenges of verifying large-scale hybrid systems	purpose	2K_dev_1075
Yet verification results also need to transfer from components to composites	purpose	2K_dev_1075
to define the structure and behavior of components how to compose components	purpose	2K_dev_1075
Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in imperfect-information games	background	2K_dev_1082
They show that running counterfactual regret minimization on such abstractions leads to good strategies in the original games	finding	2K_dev_1082
We develop the first general	mechanism	2K_dev_1082
algorithm-agnostic solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions	mechanism	2K_dev_1082
when implemented in the original ( perfect-recall ) game	mechanism	2K_dev_1082
Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions for which any such results have been obtained Further	mechanism	2K_dev_1082
our analysis is tighter in two ways	mechanism	2K_dev_1082
each of which can lead to an exponential reduction in the solution quality error bound	mechanism	2K_dev_1082
We then show that for extensive-form games that satisfy certain properties	mechanism	2K_dev_1082
the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem	mechanism	2K_dev_1082
where the increase in our bound is the distance function This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds	mechanism	2K_dev_1082
We proceed to show a divide in the class of abstraction problems	mechanism	2K_dev_1082
If payoffs are at the same scale at all information sets considered for abstraction	mechanism	2K_dev_1082
the input forms a metric space	mechanism	2K_dev_1082
and this immediately yields a $ 2 $ -approximation algorithm for abstraction Conversely	mechanism	2K_dev_1082
if this condition is not satisfied	mechanism	2K_dev_1082
we show that the input does not form a metric space	mechanism	2K_dev_1082
Finally we provide computational experiments to evaluate the practical usefulness of the abstraction techniques	method	2K_dev_1082
However imperfect-recall abstractions are poorly understood	purpose	2K_dev_1082
and only weak algorithm-specific guarantees on solution quality are known	purpose	2K_dev_1082
Computational offloading services at the edge of the Internet for mobile devices are becoming a reality	background	2K_dev_1085
We present experimental results that confirm substantial wins from edge computing for highly interactive mobile applications	finding	2K_dev_1085
Using a wide range of mobile applications	mechanism	2K_dev_1085
from WiFi and 4G LTE networks	method	2K_dev_1085
we explore how such infrastructure improves latency and energy consumption relative to the cloud	purpose	2K_dev_1085
Demand response is seeing increased popularity worldwide and industrial loads are actively taking part in this trend	background	2K_dev_1086
As a host of energy-intensive industrial processes	background	2K_dev_1086
steel plants have both the motivation and potential to provide demand response	background	2K_dev_1086
and propose methods such as adding cuts and implementing an application-specific branch and bound algorithm	mechanism	2K_dev_1086
However the scheduling of steel plants is very complex and the involved computations are intense	purpose	2K_dev_1086
In this paper we focus on these difficulties to make the computations more tractable	purpose	2K_dev_1086
Given a large-scale and high-order tensor	background	2K_dev_1090
how can we find dense blocks in it ? Can we find them in near-linear time but with a quality guarantee ? Extensive previous work has shown that dense blocks in tensors as well as graphs indicate anomalous or fraudulent behavior e	background	2K_dev_1090
lockstep behavior in social networks	background	2K_dev_1090
upito 114 $ $ \times $ $ faster than state-of-the-art methods with similar accuracy 4 Effective : M-Zoom successfully detected edit wars and bot activities and spotted network attacks with near-perfect accuracy AUCi=i0	finding	2K_dev_1090
The data and software related to this paper are available at http : //www	finding	2K_dev_1090
In this work we propose M-Zoom	mechanism	2K_dev_1090
a flexible framework which works with a broad class of density measures	mechanism	2K_dev_1090
M-Zoom has the following properties : 1 Scalable : M-Zoom scales linearly with all aspects of tensors and is 2 Provably accurate : M-Zoom provides a guarantee on the lowest density of the blocks it finds	mechanism	2K_dev_1090
3 Flexible : M-Zoom supports multi-block detection and size bounds as well as diverse density measures	mechanism	2K_dev_1090
in Wikipedia from a TCP dump	method	2K_dev_1090
However available methods for detecting such dense blocks are not satisfactory in terms of speed	purpose	2K_dev_1090
accuracy or flexibility for finding dense blocks in tensors	purpose	2K_dev_1090
An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer	background	2K_dev_1096
For these schemes we prove that forging passwords is equivalent to recovering the secret mapping	finding	2K_dev_1096
Thus our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts	finding	2K_dev_1096
We propose several candidate authentication protocols -- - a computer that stores information and performs computations correctly but does not provide confidentiality Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $	mechanism	2K_dev_1096
The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate	mechanism	2K_dev_1096
We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $	mechanism	2K_dev_1096
for a security parameter $ s ( f ) $ that depends on two key properties of $ f $	mechanism	2K_dev_1096
To obtain our results	mechanism	2K_dev_1096
we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $	mechanism	2K_dev_1096
Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate )	mechanism	2K_dev_1096
and generalize recent results of Feldman et al	mechanism	2K_dev_1096
for a setting in which the human user can only receive assistance from a semi-trusted computer	purpose	2K_dev_1096
Counterfactual Regret Minimization ( CFR ) is the most popular iterative algorithm for solving zero-sum imperfect-information games	background	2K_dev_1098
Regret-Based Pruning ( RBP ) is an improvement that allows poorly-performing actions to be temporarily pruned	background	2K_dev_1098
thus speeding up CFR	background	2K_dev_1098
We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium	finding	2K_dev_1098
This leads to provably faster convergence and lower space requirements	finding	2K_dev_1098
show that Total RBP results in an order of magnitude reduction in space	finding	2K_dev_1098
and the reduction factor increases with game size	finding	2K_dev_1098
We introduce Total RBP	mechanism	2K_dev_1098
a new form of RBP as actions are pruned	mechanism	2K_dev_1098
that reduces the space requirements of CFR	purpose	2K_dev_1098
A method is presented The invention provides correctness guarantees for CPS executions at runtime Offline verification of CPS models are combined with runtime validation of system executions for compliance with the model	mechanism	2K_dev_1100
The invention ensures that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model	mechanism	2K_dev_1100
assuming the system dynamics deviation is bounded If	mechanism	2K_dev_1100
at some point the observed behavior no longer complies with the model	mechanism	2K_dev_1100
such that offline verification results no longer apply	mechanism	2K_dev_1100
provably safe fallback actions are initiated	mechanism	2K_dev_1100
The invention includes a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic	mechanism	2K_dev_1100
for ensuring that verification results about models apply to cyber-physical systems ( CPS ) implementations	purpose	2K_dev_1100
Meeting tail latency Service Level Objectives ( SLOs ) in shared cloud networks is both important and challenging	background	2K_dev_1104
One primary challenge is determining limits on the multi-tenancy such that SLOs are met	background	2K_dev_1104
Doing so involves estimating latency	background	2K_dev_1104
which is difficult especially when tenants exhibit bursty behavior as is common in production environments	background	2K_dev_1104
Nevertheless recent papers in the past two years ( Silo	background	2K_dev_1104
QJump and PriorityMeister ) show techniques for calculating latency based on a branch of mathematical modeling called Deterministic Network Calculus ( DNC )	background	2K_dev_1104
The DNC theory is designed for adversarial worst-case conditions	background	2K_dev_1104
which is sometimes necessary	background	2K_dev_1104
but is often overly conservative	background	2K_dev_1104
SNC-Meister supports 75 % more tenants than the state-of-the-art	finding	2K_dev_1104
This paper describes SNC-Meister	mechanism	2K_dev_1104
a new admission control system SNC-Meister improves upon the state-of-the-art DNC-based systems by using a new theory	mechanism	2K_dev_1104
Stochastic Network Calculus ( SNC )	mechanism	2K_dev_1104
which is designed for tail latency percentiles Focusing on tail latency percentiles	mechanism	2K_dev_1104
rather than the adversarial worst-case DNC latency	mechanism	2K_dev_1104
allows SNC-Meister to pack together many more tenants	mechanism	2K_dev_1104
: in experiments with production traces	method	2K_dev_1104
Typical tenants do not require strict worst-case guarantees	purpose	2K_dev_1104
but are only looking for SLOs at lower percentiles ( e	purpose	2K_dev_1104
9th ) for tail latency SLOs	purpose	2K_dev_1104
The best prior complete algorithm has significantly worse complexity and has	background	2K_dev_1106
to our knowledge never been implemented	background	2K_dev_1106
We present a complete algorithm in games with more than two players	mechanism	2K_dev_1106
The main components of our tree-search-based method are a node-selection strategy	mechanism	2K_dev_1106
an exclusion oracle and a subdivision scheme	mechanism	2K_dev_1106
The node-selection strategy determines the next region to be explored -- -based on the region 's size and an estimate of whether the region contains an equilibrium The exclusion oracle provides a provably correct sufficient condition for there not to exist an equilibrium in the region	mechanism	2K_dev_1106
The subdivision scheme determines how the region is split if it can not be excluded	mechanism	2K_dev_1106
Unlike well-known incomplete methods	mechanism	2K_dev_1106
our method does not need to proceed locally	mechanism	2K_dev_1106
which avoids it getting stuck in a local minimum that may be far from any actual equilibrium	mechanism	2K_dev_1106
The run time grows rapidly with the game size	mechanism	2K_dev_1106
and this suggests a hybrid scheme where one of the relatively fast prior incomplete algorithms is run	mechanism	2K_dev_1106
and if it fails to find an equilibrium	mechanism	2K_dev_1106
then our method is used	mechanism	2K_dev_1106
for finding an epsilon-Nash equilibrium for arbitrarily small epsilon	purpose	2K_dev_1106
We discuss the implications of our results for market design in general	background	2K_dev_1108
and kidney exchange in particular	background	2K_dev_1108
Our main result asserts that	finding	2K_dev_1108
any fixed optimal matching is likely to be individually rational up to lower-order terms We also show that a simple and practical mechanism is ( fully ) individually rational	finding	2K_dev_1108
and likely to be optimal up to lower-order terms	finding	2K_dev_1108
by considering an arbitrary graph	mechanism	2K_dev_1108
but assuming that vertices are associated with players at random	mechanism	2K_dev_1108
We revisit the problem of designing optimal	purpose	2K_dev_1108
individually rational matching mechanisms ( in a general sense	purpose	2K_dev_1108
allowing for cycles in directed graphs )	purpose	2K_dev_1108
where each player -- -who is associated with a subset of vertices -- -matches as many of his own vertices when he opts into the matching mechanism as when he opts out We offer a new perspective on this problem	purpose	2K_dev_1108
A descending clock auction ( DCA ) is for buying items from multiple sellers	background	2K_dev_1109
The literature has focused on the case where each bidder has two options : to accept or reject the offered price	background	2K_dev_1109
show that the optimization-based approach dramatically outperforms the percentile-based approach -- because it takes feasibility into account in pricing	finding	2K_dev_1109
Both pricing techniques scale to the large	finding	2K_dev_1109
We present a multi-option DCA ( MDCA ) framework where at each round	mechanism	2K_dev_1109
the auctioneer offers each bidder different prices for different options	mechanism	2K_dev_1109
and a bidder may find multiple options still acceptable Setting prices during a MDCA is trickier than in a DCA	mechanism	2K_dev_1109
We develop a Markov chain model ( which options are still acceptable )	mechanism	2K_dev_1109
We leverage it This is unlike most auctions which only compute the next price vector	mechanism	2K_dev_1109
Computing the trajectory enables better planning	mechanism	2K_dev_1109
We reoptimize the trajectory after each round	mechanism	2K_dev_1109
Each optimization minimizes total payment while ensuring feasibility in a stochastic sense We also introduce percentile-based approaches to decrementing prices	mechanism	2K_dev_1109
Experiments with real FCC incentive auction interference constraint data	method	2K_dev_1109
However in many settings -- such as the FCC 's imminent incentive auction -- each bidder may be able to sell one from a set of options	purpose	2K_dev_1109
for the dynamics of each bidder 's state to optimize the trajectory of price offers to different bidders for different options	purpose	2K_dev_1109
Which team is the best in the league ? How does my team fare with respect to the rest of the league ? These are questions that every sports fan is interested in knowing the answers to	background	2K_dev_1112
In other cases such as in college sports	background	2K_dev_1112
knowing the answer to these questions is crucial for shaping the picture of spe- cific contests	background	2K_dev_1112
In professional sports sports networks provide power rankings regularly - typically every week or month de- pending on the season length of the league - based on their experts opinion	background	2K_dev_1112
We finally propose an ad- vanced ranking technique based on tensor decomposition	background	2K_dev_1112
we show that the cycles in the network are significantly correlated with the performance	finding	2K_dev_1112
In this work we propose an alternative	mechanism	2K_dev_1112
ob- jective and network-based In brief	mechanism	2K_dev_1112
our method is based on analyzing a directed network formed between the teams of the corresponding leagues that captures their win-lose relationships Using data from the National Football League and the National Basketball As- sociation	mechanism	2K_dev_1112
we show that even simple network theory metrics ( e	mechanism	2K_dev_1112
Page Rank ) can provide a ranking that has the same ac- curacy in predicting winners of upcoming match-ups as more complicated systems ( e	mechanism	2K_dev_1112
We further explore the impact of the network structure on the prediction accuracy and	method	2K_dev_1112
way of ranking sports teams	purpose	2K_dev_1112
Green and Laffonti ? [ 13 ] proved that one can not generically achieve both	background	2K_dev_1117
show that the inefficiency for a simple randomized mechanism is 5 -- -100 times smaller than the worst case	finding	2K_dev_1117
This relative difference increases with the number ofi ? agents	finding	2K_dev_1117
We consider strategyproof budget-balanced mechanisms that are approximately efficient For deterministic mechanisms	mechanism	2K_dev_1117
we show that a strategyproof and budget-balanced mechanism must have a sink agent whose valuation function is ignored in selecting an alternative	mechanism	2K_dev_1117
and she is compensated with the payments made by the other agents	mechanism	2K_dev_1117
We assume the valuations of the agents come from a bounded open interval	mechanism	2K_dev_1117
This result strengthens Green and Laffont 's impossibility result by showing that even in a restricted domain of valuations	mechanism	2K_dev_1117
there does not exist a mechanism that is strategyproof	mechanism	2K_dev_1117
budget balanced and takes every agent 's valuation into consideration -- a corollary of which is that it can not be efficient	mechanism	2K_dev_1117
Using this result we find a tight lower bound on the inefficiencies of strategyproof	mechanism	2K_dev_1117
budget-balanced mechanisms in this domain	mechanism	2K_dev_1117
The bound shows that the inefficiency asymptotically disappears when the number of agents is large -- a result close in spirit to Green and Laffonti ? [ 13	mechanism	2K_dev_1117
However our results provide worst-case bounds and the best possible rate of convergence Next	mechanism	2K_dev_1117
we consider minimizing any convex combination of inefficiency and budget imbalance	mechanism	2K_dev_1117
We show that if the valuations are unrestricted	mechanism	2K_dev_1117
no deterministic mechanism can do asymptotically better than minimizing inefficiency alone	mechanism	2K_dev_1117
Finally we investigate randomized mechanisms and provide improved lower bounds on expected inefficiency We give a tight lower bound for an interesting class of strategyproof	mechanism	2K_dev_1117
We also use an optimization-based approach -- in the spirit of automated mechanism design -- to provide a lower bound on the minimum achievable inefficiency of any randomized mechanism	mechanism	2K_dev_1117
Experiments with real data from two applications	method	2K_dev_1117
We study efficiency and budget balance for designing mechanisms in general quasi-linear domains	purpose	2K_dev_1117
The Next-Generation Airborne Collision Avoidance System ( ACAS X ) is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft	background	2K_dev_1119
It is currently being developed by the Federal Aviation Administration ( FAA ) Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety	background	2K_dev_1119
In this paper we determine the geometric configurations under a precise set of assumptions and using hybrid systems theorem proving techniques We consider subsequent advisories and show how to adapt our formal verification to take them into account	mechanism	2K_dev_1119
We examine the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version	mechanism	2K_dev_1119
demonstrating how formal hybrid systems proving approaches are helping to ensure the safety of ACAS X	mechanism	2K_dev_1119
under which the advice given by ACAS X is safe formally verify these configurations	purpose	2K_dev_1119
An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy	background	2K_dev_1121
In a system with redundant requests	background	2K_dev_1121
each job that arrives to the system is copied and dispatched to multiple servers	background	2K_dev_1121
As soon as the first copy completes service	background	2K_dev_1121
the job is considered complete	background	2K_dev_1121
and all remaining copies are deleted	background	2K_dev_1121
We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity	finding	2K_dev_1121
We propose a theoretical model of redundancy	mechanism	2K_dev_1121
the Redundancy- d system	mechanism	2K_dev_1121
in which each job sends redundant copies to d servers chosen uniformly at random	mechanism	2K_dev_1121
We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers	mechanism	2K_dev_1121
A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists	purpose	2K_dev_1121
The approach finds a dual flow solution to this linear system through a sequence of flow adjustments along cycles	background	2K_dev_1124
Our methods demonstrate significant speedups over previous implementations	finding	2K_dev_1124
and are competitive with standard numerical routines	finding	2K_dev_1124
We study both data structure oriented and recursive methods The primary difficulty faced by this approach	mechanism	2K_dev_1124
updating and querying long cycles	mechanism	2K_dev_1124
motivated us to study an important special case : instances where all cycles are formed by fundamental cycles on a length $ n $ path	mechanism	2K_dev_1124
We study the performance of linear solvers for graph Laplacians based on the combinatorial cycle adjustment methodology proposed by [ Kelner-Orecchia-Sidford-Zhu STOC-13 ]	purpose	2K_dev_1124
for handling these adjustments	purpose	2K_dev_1124
The empirical results show that the patterns of parameters as a seizure approach and the method is efficient in analyzing nonlinear epilepsy electroencephalogram data	finding	2K_dev_1134
The accuracy of estimating the optimal parameters is improved by using the nonlinear dynamic model	finding	2K_dev_1134
We propose a nonlinear dynamic model for an invasive electroencephalogram analysis via the LevenbergMarquardt algorithm	mechanism	2K_dev_1134
We introduce the crucial windows where the estimated parameters present patterns before seizure onset The optimal parameters minimizes the error between the observed signal and the generated signal by the model	mechanism	2K_dev_1134
The proposed approach effectively discriminates between healthy signals and epileptic seizure signals	mechanism	2K_dev_1134
We evaluate the proposed method using an electroencephalogram dataset with normal and epileptic seizure sequences	method	2K_dev_1134
that learns the optimal parameters of the neural population model	purpose	2K_dev_1134
Given `` who-trusts/distrusts-whom '' information	background	2K_dev_1136
how can we propagate the trust and distrust ? With the appearance of fraudsters in social network sites	background	2K_dev_1136
the importance of trust prediction has increased	background	2K_dev_1136
confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy	finding	2K_dev_1136
achieving up to 50	finding	2K_dev_1136
4 percentage relative improvement	finding	2K_dev_1136
In this paper we propose PIN -TRUST	mechanism	2K_dev_1136
a novel method The novelties of our method are the following : ( a ) it is carefully designed	mechanism	2K_dev_1136
to take into account positive	mechanism	2K_dev_1136
implicit and negative information	mechanism	2K_dev_1136
( b ) it is scalable ( i	mechanism	2K_dev_1136
linear on the input size )	mechanism	2K_dev_1136
( c ) most importantly	mechanism	2K_dev_1136
it is effective and accurate	mechanism	2K_dev_1136
Our extensive experiments with a real dataset	method	2K_dev_1136
com data of 100K nodes and 1M edges	method	2K_dev_1136
Most such methods use only explicit and implicit trust information ( e	purpose	2K_dev_1136
if Smith likes several of Johnson 's reviews	purpose	2K_dev_1136
then Smith implicitly trusts Johnson )	purpose	2K_dev_1136
but they do not consider distrust to handle all three types of interaction information : explicit trust	purpose	2K_dev_1136
implicit trust and explicit distrust	purpose	2K_dev_1136
As mobile computing and cloud computing converge	background	2K_dev_1139
the sensing and interaction capabilities of mobile devices can be seamlessly fused with compute-intensive and data-intensive processing in the cloud	background	2K_dev_1139
Cloudlets are important architectural components in this convergence	background	2K_dev_1139
representing the middle tier of a mobile device cloudlet cloud hierarchy	background	2K_dev_1139
We describe a plug-and-play architecture	mechanism	2K_dev_1139
and a proof of concept using Google Glass	method	2K_dev_1139
We show how cloudlets enable a new genre of applications called cognitive assistance applications that augment human perception and cognition for cognitive assistance	purpose	2K_dev_1139
With the potential to enhance the power system 's operational flexibility in a cost-effective way	background	2K_dev_1152
demand response is gaining increased attention worldwide	background	2K_dev_1152
Industrial loads such as cement crushing plants consume large amounts of electric energy and therefore are prime candidates for the provision of significant amounts of demand response	background	2K_dev_1152
They have the capability to turn on/off an arbitrary number of their crushers thereby adjusting their electric power consumption	background	2K_dev_1152
In this paper we propose a coordination method based on model predictive control	mechanism	2K_dev_1152
However the change in power consumption by cement crushing plants and also other industrial loads are often not granular enough to provide valuable ancillary services such as regulation and load following	purpose	2K_dev_1152
to overcome the granularity restriction with the help of an energy storage	purpose	2K_dev_1152
The core number of a node is the highest k-core in which the node participates	background	2K_dev_1157
Core numbers are useful in many graph mining tasks	background	2K_dev_1157
especially ones that involve finding communities of nodes	background	2K_dev_1157
influential spreaders and dense subgraphs	background	2K_dev_1157
Large graphs often do not fit on the memory of a single machine	background	2K_dev_1157
demonstrate that NimbleCore gives space savings up to 60X	finding	2K_dev_1157
while accurately estimating core numbers with average relative error less than 2	finding	2K_dev_1157
We propose NimbleCore an iterative external-memory algorithm	mechanism	2K_dev_1157
using O ( n log d max ) space	mechanism	2K_dev_1157
where n is the number of nodes and d max is the maximum node-degree in the graph	mechanism	2K_dev_1157
We also show that NimbleCore requires O ( n ) space for graphs with power-law degree distributions	mechanism	2K_dev_1157
Experiments on forty-eight large graphs from various domains	method	2K_dev_1157
We address the problem of estimating core numbers of nodes by reading edges of a large graph stored in external memory Existing external memory solutions do not give bounds on the required space	purpose	2K_dev_1157
In practice existing solutions also do not scale with the size of the graph which estimates core numbers of nodes	purpose	2K_dev_1157
How can we design a product or movie that will attract	background	2K_dev_1159
for example the interest of Pennsylvania adolescents or liberal newspaper critics ? What should be the genre of that movie and who should be in the cast	background	2K_dev_1159
and show that it is highly scalable and effectively provides movie designs oriented towards different groups of users	finding	2K_dev_1159
including men women and adolescents	finding	2K_dev_1159
We formulate the movie design as an optimization problem over the inference of user-feature scores and selection of the features that maximize the number of attracted users Our approach	mechanism	2K_dev_1159
PNP is based on a heterogeneous	mechanism	2K_dev_1159
tripartite graph of users	mechanism	2K_dev_1159
movies and features ( e	mechanism	2K_dev_1159
actors directors genres )	mechanism	2K_dev_1159
where users rate movies and features contribute to movies	mechanism	2K_dev_1159
We learn the preferences by leveraging user similarities defined through different types of relations	mechanism	2K_dev_1159
and show that our method outperforms state-of-the-art approaches	mechanism	2K_dev_1159
including matrix factorization and other heterogeneous graph-based analysis	mechanism	2K_dev_1159
We evaluate PNP on publicly available real-world data	method	2K_dev_1159
In this work we seek to identify how we can design new movies with features tailored to a specific user population	purpose	2K_dev_1159
The World Wide Web ( WWW ) has become a rapidly growing platform consisting of numerous sources which provide supporting or contradictory information about claims ( e	background	2K_dev_1160
`` Chicken meat is healthy '' )	background	2K_dev_1160
In order to decide whether a claim is true or false	background	2K_dev_1160
one needs to analyze content of different sources of information on the Web	background	2K_dev_1160
measure credibility of information sources	background	2K_dev_1160
and aggregate all these information	background	2K_dev_1160
we demonstrate ClaimEval 's capability in determining validity of a set of claims	finding	2K_dev_1160
resulting in improved accuracy compared to state-of-the-art baselines	finding	2K_dev_1160
In this paper we present ClaimEval	mechanism	2K_dev_1160
a novel and integrated approach which given a set of claims to validate	mechanism	2K_dev_1160
extracts a set of pro and con arguments from the Web information sources	mechanism	2K_dev_1160
and jointly ClaimEval uses Probabilistic Soft Logic ( PSL )	mechanism	2K_dev_1160
resulting in a flexible and principled framework which makes it easy to state and incorporate different forms of prior-knowledge	mechanism	2K_dev_1160
Through extensive experiments on real-world datasets	method	2K_dev_1160
This is a tedious process and the Web search engines address only part of the overall problem	purpose	2K_dev_1160
producing only a list of relevant sources	purpose	2K_dev_1160
estimates credibility of sources and correctness of claims	purpose	2K_dev_1160
Human-Computer Music Performance for popular music - where musical structure is important	background	2K_dev_1161
but where musicians often decide on the spur of the moment exactly what the musical form will be - presents many challenges to make computer systems that are flexible and adaptable to human musicians	background	2K_dev_1161
We present new formalisms and representations	mechanism	2K_dev_1161
and a corresponding implementation	mechanism	2K_dev_1161
One particular challenge is that humans easily follow scores and chord charts	purpose	2K_dev_1161
adapt these to new performance plans	purpose	2K_dev_1161
and understand media locations in musical terms ( beats and measures )	purpose	2K_dev_1161
while computer music systems often use rigid and even numerical representations that are difficult to work with	purpose	2K_dev_1161
where musical material in various media is synchronized	purpose	2K_dev_1161
where musicians can quickly alter the performance order by specifying ( re- ) arrangements of the material	purpose	2K_dev_1161
and where interfaces are supported in a natural way by music notation	purpose	2K_dev_1161
Recent computer systems research has proposed using redundant requests to reduce latency	background	2K_dev_1170
The idea is to replicate a request so that it joins the queue at multiple servers	background	2K_dev_1170
The request is considered complete as soon as any one copy of the request completes	background	2K_dev_1170
Redundancy is beneficial because it allows us to overcome server-side variability the fact that the server we choose might be temporarily slow due to factors such as background load	background	2K_dev_1170
network interrupts and garbage collection	background	2K_dev_1170
When there is significant server-side variability	background	2K_dev_1170
replicating requests can greatly reduce response times	background	2K_dev_1170
In the past few years	background	2K_dev_1170
queueing theorists have begun to study redundancy	background	2K_dev_1170
first via approximations and	background	2K_dev_1170
more recently via exact analysis	background	2K_dev_1170
Unfortunately for analytical tractability	background	2K_dev_1170
most existing theoretical analysis has assumed an Independent Runtimes ( IR ) model	background	2K_dev_1170
wherein the replicas of a job each experience independent runtimes ( service times ) at different servers	background	2K_dev_1170
and has provably excellent performance	finding	2K_dev_1170
This paper introduces a much more realistic model of redundancy	mechanism	2K_dev_1170
Our model allows us where we track both S and X for each job	mechanism	2K_dev_1170
Analysis within the S & X model is	mechanism	2K_dev_1170
of course much more difficult	mechanism	2K_dev_1170
Nevertheless we design a policy	mechanism	2K_dev_1170
Redundant-to-Idle-Queue ( RIQ ) which is both analytically tractable within the S & X model	mechanism	2K_dev_1170
The IR model is unrealistic and has led to theoretical results which can be at odds with computer systems implementation results to decouple the inherent job size ( X ) from the server-side slowdown ( S )	purpose	2K_dev_1170
Counterfactual Regret Minimization ( CFR ) is a popular iterative algorithm for approximating Nash equilibria in imperfect-information multi-step two-player zero-sum games	background	2K_dev_1173
demonstrate that one can improve overall convergence in a game by first running CFR on a smaller	finding	2K_dev_1173
coarser abstraction of the game and then using the strategy in the abstract game to warm start CFR in the full game	finding	2K_dev_1173
We introduce the first general	mechanism	2K_dev_1173
principled method Our approach requires only a strategy for each player	mechanism	2K_dev_1173
and accomplishes the warm start at the cost of a single traversal of the game tree	mechanism	2K_dev_1173
The method provably warm starts CFR to as many iterations as it would have taken to reach a strategy profile of the same quality as the input strategies	mechanism	2K_dev_1173
and does not alter the convergence bounds of the algorithms	mechanism	2K_dev_1173
Unlike prior approaches to warm starting	mechanism	2K_dev_1173
ours can be applied in all cases Our method is agnostic to the origins of the input strategies	mechanism	2K_dev_1173
For example they can be based on human domain knowledge	mechanism	2K_dev_1173
the observed strategy of a strong agent	mechanism	2K_dev_1173
the solution of a coarser abstraction	mechanism	2K_dev_1173
or the output of some algorithm that converges rapidly at first but slowly as it gets closer to an equilibrium	mechanism	2K_dev_1173
for warm starting CFR	purpose	2K_dev_1173
Kidney exchange is a type of barter market where patients exchange willing but incompatible donors	background	2K_dev_1184
These exchanges are conducted via cycleswhere each incompatible patient-donor pair in the cycle both gives and receives a kidneyand chains	background	2K_dev_1184
which are started by an altruist donor who does not need a kidney in return	background	2K_dev_1184
Algorithms from our group autonomously make the transplant plans for that exchange	finding	2K_dev_1184
our new solver scales significantly better than the prior leading approaches	finding	2K_dev_1184
We develop a provably correct which also necessarily changes the algorithm 's complexity	mechanism	2K_dev_1184
as well as other improvements to the search algorithm A cap is desirable in practice since if even one edge in the chain fails	mechanism	2K_dev_1184
the rest of the chain fails : the cap precludes very long chains that are extremely unlikely to execute and instead causes the solution to have more parallel chains and cycles that are more likely to succeed We work with the UNOS nationwide kidney exchange	mechanism	2K_dev_1184
which uses a chain cap	mechanism	2K_dev_1184
Next we compare our solver to the leading constraint-generation-based solver and to the best prior correct branch-and-price-based solver	method	2K_dev_1184
We focus on the setting where chains have a length cap	method	2K_dev_1184
On that real data and demographically-accurate generated data	method	2K_dev_1184
Finding the best combination of cycles and chains is hard The leading algorithms for this optimization problem use either branch and pricea combination of branch and bound and column generationor constraint generation	purpose	2K_dev_1184
We show a correctness error in the leading prior branch-and-price-based approach [ Glorie et al	purpose	2K_dev_1184
2014 ] fix to it	purpose	2K_dev_1184
Concurrent C0 is an imperative programming language in the C family with session-typed message-passing concurrency	background	2K_dev_1191
and show the results obtained	finding	2K_dev_1191
While the abstract measure of span always decreases ( or remains unchanged )	finding	2K_dev_1191
only a few of the examples reap a practical benefit	finding	2K_dev_1191
A key idea is to postpone message reception as much as possible by interpreting receive commands as a request for a message	mechanism	2K_dev_1191
We implemented our ideas as a translation from a blocking intermediate language to a non-blocking language	mechanism	2K_dev_1191
Finally we evaluated our techniques with several benchmark programs	method	2K_dev_1191
The previously proposed semantics implements asynchronous ( non-blocking ) output ; we extend it here with non-blocking input	purpose	2K_dev_1191
that outperforms traditional message passing techniques	finding	2K_dev_1192
We describe Concurrent C0 with contracts and session-typed communication over channels	mechanism	2K_dev_1192
Concurrent C0 supports an operation called forwarding which allows channels to be combined in a well-defined way The language 's type system enables elegant expression of session types and message-passing concurrent programs	mechanism	2K_dev_1192
We provide a Go-based implementation with language based optimizations	method	2K_dev_1192
a type-safe C-like language	purpose	2K_dev_1192
Regular SGVB estimators rely on sampling of parameters once per minibatch of data	mechanism	2K_dev_1194
and have variance that is constant w	mechanism	2K_dev_1194
The efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch	mechanism	2K_dev_1194
Such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size	mechanism	2K_dev_1194
generally leading to much faster convergence	mechanism	2K_dev_1194
We find an important connection with regularization by dropout : the original Gaussian dropout objective corresponds to SGVB with local noise	mechanism	2K_dev_1194
a scale-invariant prior and proportionally fixed posterior variance	mechanism	2K_dev_1194
Our method allows ; specifically	mechanism	2K_dev_1194
we propose \emph { variational dropout }	mechanism	2K_dev_1194
a generalization of Gaussian dropout	mechanism	2K_dev_1194
but with a more flexibly parameterized posterior	mechanism	2K_dev_1194
The method is demonstrated through several experiments	method	2K_dev_1194
We explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational Bayes ( SGVB ) with global model parameters inference of more flexibly parameterized posteriors often leading to better generalization	purpose	2K_dev_1194
Optical music recognition ( OMR ) is the task of recognizing images of musical scores	background	2K_dev_1197
In this paper improved algorithms were developed	mechanism	2K_dev_1197
which facilitated bulk annotation of scanned scores for use in an interactive score display system	mechanism	2K_dev_1197
Creating an initial annotation by OMR and verifying by hand substantially reduced the manual effort required to process scanned scores to be used in a live performance setting	mechanism	2K_dev_1197
for the first steps of optical music recognition	purpose	2K_dev_1197
Computer music systems can interact with humans at different levels	background	2K_dev_1198
including scores phrases notes	background	2K_dev_1198
However most current systems lack basic musicianship skills	background	2K_dev_1198
and claim that a more human-like interaction is achieved	finding	2K_dev_1198
We have built an artificial pianist that can automatically improve its ability to sense and coordinate with a human pianist	mechanism	2K_dev_1198
learning from rehearsal experience	mechanism	2K_dev_1198
We describe different machine learning algorithms to learn musicianship for duet interaction	mechanism	2K_dev_1198
explore the properties of the learned models	method	2K_dev_1198
such as dominant features	method	2K_dev_1198
limits of validity and minimal training size	method	2K_dev_1198
As a consequence the results of human-computer interaction are often far less musical than the interaction between human musicians	purpose	2K_dev_1198
In this paper we explore the possibility of learning some basic music performance skills from rehearsal data	purpose	2K_dev_1198
In particular we consider the piano duet scenario where two musicians expressively interact with each other	purpose	2K_dev_1198
Our work extends previous automatic accompaniment systems	purpose	2K_dev_1198
Biological adaptation is a powerful mechanism that makes many disorders hard to combat	background	2K_dev_1204
We show that for the development of regulatory cells	finding	2K_dev_1204
sequential plans yield significantly higher utility than the best static therapy In contrast	finding	2K_dev_1204
for developing effector cells	finding	2K_dev_1204
we find that ( at least for the given simulator	finding	2K_dev_1204
objective function action possibilities	finding	2K_dev_1204
and measurement possibilities ) single-step plans suffice for optimal treatment	finding	2K_dev_1204
We propose a general approach where we leverage Monte Carlo tree search and the biological entity is modeled by a black-box simulator that the planner calls during planning We show that the framework can be used to steer a biological entity modeled via a complex signaling pathway network that has numerous feedback loops that operate at different rates and have hard-to-understand aggregate behavior We apply the framework to steering the adaptation of a patient 's immune system	mechanism	2K_dev_1204
In particular we apply it to a leading T cell simulator ( available in the biological modeling package BioNetGen	mechanism	2K_dev_1204
We run experiments with two alternate goals : developing regulatory T cells or developing effector T cells	method	2K_dev_1204
The former is important for preventing autoimmune diseases while the latter is associated with better survival rates in cancer patients We are especially interested in the effect of sequential plans	method	2K_dev_1204
an approach that has not been explored extensively in the biological literature	method	2K_dev_1204
In this paper we study steering such adaptation through sequential planning to compute a treatment plan	purpose	2K_dev_1204
As publishers gather more information about their users	background	2K_dev_1205
they can use that information to enable advertisers to create increasingly targeted campaigns	background	2K_dev_1205
This enables better usage of advertising inventory	background	2K_dev_1205
it yields two orders of magnitude improvement in run time and significant improvement in abstraction quality These benefits hold both for guaranteed and non-guaranteed campaigns	finding	2K_dev_1205
We develop an optimal anytime algorithm The performance stems from three improvements : 1 ) a quadratic-time ( as opposed to doubly exponential or heuristic ) algorithm for finding an optimal split of an abstract segment	mechanism	2K_dev_1205
2 ) a better scoring function for evaluating splits	mechanism	2K_dev_1205
and 3 ) splitting time lossily like any other targeting attribute ( instead of losslessly segmenting time first )	mechanism	2K_dev_1205
Compared to the segment abstraction algorithm by Walsh et al	method	2K_dev_1205
[ 2010 ] for the same problem	method	2K_dev_1205
However it also dramatically increases the complexity that the publisher faces when optimizing campaign admission decisions and inventory allocation to campaigns	purpose	2K_dev_1205
for abstracting fine-grained audience segments into coarser abstract segments that are not too numerous for use in such optimization	purpose	2K_dev_1205
State-of-the-art applications of Stackelberg security games -- including wildlife protection -- offer a wealth of data	background	2K_dev_1207
which can be used to learn the behavior of the adversary	background	2K_dev_1207
We also validate our approach	finding	2K_dev_1207
We develop a new approach	mechanism	2K_dev_1207
by observing how the attacker responds to only three defender strategies	mechanism	2K_dev_1207
using experiments on real and synthetic data	method	2K_dev_1207
But existing approaches either make strong assumptions about the structure of the data	purpose	2K_dev_1207
or gather new data through online algorithms that are likely to play severely suboptimal strategies	purpose	2K_dev_1207
to learning the parameters of the behavioral model of a bounded rational attacker ( thereby pinpointing a near optimal strategy )	purpose	2K_dev_1207
Hybrid systems verification is quite important for developing correct controllers for physical systems	background	2K_dev_1212
but is also challenging	background	2K_dev_1212
Verification engineers thus need to be empowered with ways of guiding hybrid systems verification while receiving as much help from automation as possible	background	2K_dev_1212
We also share thoughts how the success of such a user interface design could be evaluated and anecdotal observations about it	background	2K_dev_1212
Unsurprisingly the most difficult user interface challenges come from the desire to integrate automation and human guidance	finding	2K_dev_1212
This paper presents the design ideas behind the user interface for the hybrid systems theorem prover KeYmaera X	mechanism	2K_dev_1212
Due to undecidability verification tools need sufficient means for intervening during the verification and need to allow verification engineers to provide system design insights	purpose	2K_dev_1212
We discuss how they make it easier to prove hybrid systems as well as help learn how to conduct proofs in the first place	purpose	2K_dev_1212
In human-robot teams humans often start with an inaccurate model of the robot capabilities	background	2K_dev_1213
As they interact with the robot	background	2K_dev_1213
they infer the robot 's capabilities and partially adapt to the robot	background	2K_dev_1213
they might change their actions based on the observed outcomes and the robot 's actions	background	2K_dev_1213
without replicating the robot 's policy	background	2K_dev_1213
We prove that the optimal policy can be computed efficiently	finding	2K_dev_1213
that the proposed model significantly improves human-robot team performance	finding	2K_dev_1213
We present a game-theoretic model where the human responds to the robot 's actions by maximizing a reward function that changes stochastically over time	mechanism	2K_dev_1213
The robot can then use this model to decide optimally between taking actions that reveal its capabilities to the human and taking the best action given the information that the human currently has	mechanism	2K_dev_1213
under certain observability assumptions We demonstrate through a human subject experiment compared to policies that assume complete adaptation of the human to the robot	method	2K_dev_1213
of human partial adaptation to the robot capturing the evolution of their expectations of the robot 's capabilities	purpose	2K_dev_1213
Generalized canonical correlation analysis ( GCCA ) aims at extracting common structure from multiple 'views '	background	2K_dev_1217
high-dimensional matrices representing the same objects in different feature domains an extension of classical two-view CCA	background	2K_dev_1217
further reduce the runtime significantly ( by 30 % ) if multiple cores are available	finding	2K_dev_1217
to showcase the effectiveness of the proposed algorithms	finding	2K_dev_1217
we propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements	mechanism	2K_dev_1217
respectively Consequently the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100	mechanism	2K_dev_1217
000 while the current approaches can only handle thousands of features / samples	mechanism	2K_dev_1217
Our second contribution is a distributed algorithm for GCCA	mechanism	2K_dev_1217
which computes the canonical components of different views in parallel and thus can	mechanism	2K_dev_1217
in experiments Judiciously designed synthetic and real-data experiments using a multilingual dataset are employed	method	2K_dev_1217
Existing ( G ) CCA algorithms have serious scalability issues	purpose	2K_dev_1217
since they involve square root factorization of the correlation matrices of the views	purpose	2K_dev_1217
The memory and computational complexity associated with this step grow as a quadratic and cubic function of the problem dimension ( the number of samples / features )	purpose	2K_dev_1217
To circumvent such difficulties	purpose	2K_dev_1217
Display appropriation provides a means by which mobile users can cyber-forage local display hardware to provide them with access to a high-quality output device	background	2K_dev_1218
In this demonstration we show a system that presents an alternative vision in which users are able to cyber-forage for both display and compute resources in their local area enabling them The demonstration leverages a cohesive suite of existing systems	mechanism	2K_dev_1218
cloudlets Internet Suspend/Resume ( ISR )	mechanism	2K_dev_1218
Yarely and Tacita to deliver this vision	mechanism	2K_dev_1218
However displays are of little use without applications to drive them and yet the nature of application support has been largely ignored in the field with the prevailing assumption being that applications will be cloud-based and Web-centric	purpose	2K_dev_1218
to execute high-performance applications that would not be possible using purely Web-centric technologies	purpose	2K_dev_1218
Our first contribution is two discoveries : ( i ) the number of comments grows as a power-law on the number of votes and ( ii ) the time between a submission creation and a user 's reaction obeys a log-logistic distribution	finding	2K_dev_1220
VnC outperformed state-of-the-art baselines on accuracy Additionally	finding	2K_dev_1220
we illustrate VnC usefulness for forecasting and outlier detection	finding	2K_dev_1220
Based on these patterns	mechanism	2K_dev_1220
we propose VnC ( Vote-and-Comment )	mechanism	2K_dev_1220
a parsimonious but accurate and scalable model	mechanism	2K_dev_1220
We analyzed over 20	method	2K_dev_1220
000 submissions corresponding to more than 100 million user interactions from three social voting Web sites : Reddit	method	2K_dev_1220
Imgur and Digg In our experiments on real data	method	2K_dev_1220
In social voting Web sites	purpose	2K_dev_1220
how do the user actions up-votes	purpose	2K_dev_1220
down-votes and comments evolve over time ? Are there relationships between votes and comments ? What is normal and what is suspicious ? These are the questions we focus on	purpose	2K_dev_1220
that models the coevolution of user activities	purpose	2K_dev_1220
Given a heterogeneous network	background	2K_dev_1221
with nodes of different types - e	background	2K_dev_1221
products users and sellers from an online recommendation site like Amazon - and labels for a few nodes ( 'honest '	background	2K_dev_1221
'suspicious ' etc )	background	2K_dev_1221
can we find a closed formula for Belief Propagation ( BP )	background	2K_dev_1221
exact or approximate ? Can we say whether it will converge ?	background	2K_dev_1221
( 4 ) Effectiveness ZooBP identifies fraudulent users with a near-perfect precision of 92	finding	2K_dev_1221
3 % over the top 300 results	finding	2K_dev_1221
We propose ZooBP a method with provable convergence guarantees ZooBP has the following advantages : ( 1 ) Generality : It works on heterogeneous graphs with multiple types of nodes and edges ; ( 2 ) Closed-form solution : ZooBP gives a closed-form solution as well as convergence guarantees ; ( 3 ) Scalability : ZooBP is linear on the graph size and is up to 600 faster than BP	mechanism	2K_dev_1221
running on graphs with 3	mechanism	2K_dev_1221
3 million edges in a few seconds	mechanism	2K_dev_1221
Applied on real data ( a Flipkart e-commerce network with users	method	2K_dev_1221
products and sellers )	method	2K_dev_1221
BP traditionally an inference algorithm for graphical models	purpose	2K_dev_1221
exploits so-called `` network effects '' to perform graph classification tasks when labels for a subset of nodes are provided ; and it has been successful in numerous settings like fraudulent entity detection in online retailers and classification in social networks	purpose	2K_dev_1221
However it does not have a closed-form nor does it provide convergence guarantees in general	purpose	2K_dev_1221
to perform fast BP on undirected heterogeneous graphs	purpose	2K_dev_1221
A k-core is the maximal subgraph where all vertices have degree at least k	background	2K_dev_1223
This concept has been applied to such diverse areas as hierarchical structure analysis	background	2K_dev_1223
graph visualization and graph clustering	background	2K_dev_1223
Our discoveries are as follows : ( 1 ) Mirror Pattern : coreness of vertices ( i	finding	2K_dev_1223
maximum k such that each vertex belongs to the k-core ) is strongly correlated to their degree	finding	2K_dev_1223
( 2 ) Core-Triangle Pattern : degeneracy of a graph ( i	finding	2K_dev_1223
maximum k such that the k-core exists in the graph ) obeys a 3-to-1 power law with respect to the count of triangles	finding	2K_dev_1223
( 3 ) Structured Core Pattern : degeneracy-cores are not cliques but have non-trivial structures such as core-periphery and communities	finding	2K_dev_1223
Our algorithmic contributions show the usefulness of these patterns	mechanism	2K_dev_1223
( 1 ) Core-A	mechanism	2K_dev_1223
which measures the deviation from Mirror Pattern	mechanism	2K_dev_1223
successfully finds anomalies in real-world graphs complementing densest-subgraph based anomaly detection methods	mechanism	2K_dev_1223
( 2 ) Core-D	mechanism	2K_dev_1223
a single-pass streaming algorithm based on Core-Triangle Pattern	mechanism	2K_dev_1223
accurately estimates the degeneracy of billion-scale graphs up to 7 faster than a recent multipass algorithm	mechanism	2K_dev_1223
( 3 ) Core-S	mechanism	2K_dev_1223
inspired by Structured Core Pattern	mechanism	2K_dev_1223
identifies influential spreaders up to 17 faster than top competitors with comparable accuracy	mechanism	2K_dev_1223
How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we use them for algorithm design and applications ? Here	purpose	2K_dev_1223
we explore pervasive patterns that are related to k-cores and emerging in graphs from several diverse domains	purpose	2K_dev_1223
Multi-aspect data appear frequently in many web-related applications	background	2K_dev_1230
For example product reviews are quadruplets of ( user	background	2K_dev_1230
product keyword timestamp )	background	2K_dev_1230
How can we analyze such web-scale multi-aspect data ? Can we analyze them on an off-the-shelf workstation with limited amount of memory ? Tucker decomposition has been widely used for discovering patterns in relationships among entities in multi-aspect data	background	2K_dev_1230
naturally expressed as high-order tensors	background	2K_dev_1230
S-HOT showed better scalability not only with the order but also with the dimensionality and the rank than baseline methods In particular	finding	2K_dev_1230
S-HOT decomposed tensors 1000 larger than baseline methods in terms dimensionality S- HOT also successfully analyzed real-world tensors that are both large-scale and high-order on an off-the-shelf workstation with limited amount of memory	finding	2K_dev_1230
while baseline methods failed The source code of S-HOT is publicly available at http : //dm	finding	2K_dev_1230
kr/shot to encourage reproducibility	finding	2K_dev_1230
we propose S-HOT a scalable high-order tucker decomposition method that employs the on-the-fly computation Moreover	mechanism	2K_dev_1230
S-HOT is designed for handling disk-resident tensors	mechanism	2K_dev_1230
too large to fit in memory	mechanism	2K_dev_1230
without loading them all in memory at once	mechanism	2K_dev_1230
We provide theoretical analysis on the amount of memory space and the number of scans of data required by S-HOT	method	2K_dev_1230
However existing algorithms for Tucker decomposition have limited scalability	purpose	2K_dev_1230
and especially fail to decompose high-order tensors since they explicitly materialize intermediate data	purpose	2K_dev_1230
whose size rapidly grows as the order increases ( 4 )	purpose	2K_dev_1230
We call this problem M-Bottleneck ( `` Materialization Bottleneck '' )	purpose	2K_dev_1230
To avoid M-Bottleneck to minimize the materialized intermediate data	purpose	2K_dev_1230
How can we detect fraudulent lockstep behavior in large-scale multi-aspect data ( i	background	2K_dev_1231
tensors ) ? Can we detect it when data are too large to fit in memory or even on a disk ? Past studies have shown that dense blocks in real-world tensors ( e	background	2K_dev_1231
social media Wikipedia TCP dumps	background	2K_dev_1231
) signal anomalous or fraudulent behavior such as retweet boosting	background	2K_dev_1231
bot activities and network attacks	background	2K_dev_1231
Thus various approaches including tensor decomposition and search	background	2K_dev_1231
have been used for rapid and accurate dense-block detection in tensors	background	2K_dev_1231
D-Cube is ( 1 ) Memory Efficient : requires up to 1	finding	2K_dev_1231
600 times less memory and handles 1	finding	2K_dev_1231
000 times larger data ( 2	finding	2K_dev_1231
6TB ) ( 2 ) Fast : up to 5 times faster due to its near-linear scalability with all aspects of data	finding	2K_dev_1231
( 3 ) Provably Accurate : gives a guarantee on the densities of the blocks it finds	finding	2K_dev_1231
and ( 4 ) Effective : successfully spotted network attacks from TCP dumps and synchronized behavior in rating data with the highest accuracy	finding	2K_dev_1231
we propose D-Cube a disk-based dense-block detection method	mechanism	2K_dev_1231
which also can be run in a distributed manner across multiple machines	mechanism	2K_dev_1231
Compared with state-of-the-art methods	method	2K_dev_1231
However all such methods have low accuracy	purpose	2K_dev_1231
or assume that tensors are small enough to fit in main memory	purpose	2K_dev_1231
which is not true in many real-world applications such as social media and web	purpose	2K_dev_1231
To overcome these limitations	purpose	2K_dev_1231
This generalizes a prior decomposition result for an M/M/k/staggeredsetup	background	2K_dev_1233
We show the response time of an M/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/k and the setup time	finding	2K_dev_1233
where the approximation is nearly exact	finding	2K_dev_1233
that for exponentially distributed setup times	method	2K_dev_1233
We consider the M/G/k/staggered-setup	purpose	2K_dev_1233
where idle servers are turned off to save cost	purpose	2K_dev_1233
necessitating a setup time for turning a server back on ; however	purpose	2K_dev_1233
at most one server may be in setup mode at any time	purpose	2K_dev_1233
Summary Successful application of two-photon imaging withgenetic tools in awake macaque monkeys will enable fundamental advances in our understanding of higher cognitive function at the level of molecular and neuronal circuits	background	2K_dev_1236
By providing two-photon imaging access to cortical neuronal populations at single-cell or single dendritic spine resolution in awake monkeys	background	2K_dev_1236
the techniques reported can help bridge the use of modern genetic and molecular tools and the study of higher cognitive function	background	2K_dev_1236
confirm that fluorescence activity is linearly proportional to neuronal spiking activity across a wide range of firing rates ( 10Hz to 150Hz )	finding	2K_dev_1236
Here we report techniques Using genetically encoded indicators including GCaMP5 and GCaMP6s delivered by AAV2/1 into the visual cortex	mechanism	2K_dev_1236
we demonstrate that high-quality two-photon imaging of large neuronal populations can be achieved and maintained in awake monkeys for months	mechanism	2K_dev_1236
Simultaneous intracellular recording and two-photon calcium imaging	method	2K_dev_1236
for long-term two-photon imaging in awake macaque monkeys	purpose	2K_dev_1236
Our work provides a solid step toward a systematic and quantitative wall-centric profiling of Facebook user activity	background	2K_dev_1237
Our key results can be summarized in the following points	finding	2K_dev_1237
First we find that many wall activities	finding	2K_dev_1237
including number of posts	finding	2K_dev_1237
number of likes number of posts of type photo	finding	2K_dev_1237
can be described by the PowerWall distribution	finding	2K_dev_1237
What is more surprising is that most of these distributions have similar slope	finding	2K_dev_1237
with a value close to 1 ! Second	finding	2K_dev_1237
we show how our patterns and metrics can help us spot surprising behaviors and anomalies	finding	2K_dev_1237
For example we find a user posting every two days	finding	2K_dev_1237
exactly the same count of posts ; another user posting at midnight	finding	2K_dev_1237
with no other activity before or after	finding	2K_dev_1237
In this work we model Facebook user behavior : We propose PowerWall	mechanism	2K_dev_1237
a lesser known heavy-tailed distribution to fit our data	mechanism	2K_dev_1237
we analyze the wall activities of users focusing on identifying common patterns and surprising phenomena	method	2K_dev_1237
We conduct an extensive study of roughly 7k users over 3 years during 4-month intervals each year	method	2K_dev_1237
How do people interact with their Facebook wall ? At a high level	purpose	2K_dev_1237
this question captures the essence of our work	purpose	2K_dev_1237
While most prior efforts focus on Twitter	purpose	2K_dev_1237
the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study	purpose	2K_dev_1237
Sparse iterative methods in particular first-order methods	background	2K_dev_1241
are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games	background	2K_dev_1241
The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on	background	2K_dev_1241
we show that for the first time	finding	2K_dev_1241
the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm	finding	2K_dev_1241
We investigate the acceleration of first-order methods through better design of the dilated entropy function -- -a class of distance-generating functions related to the domains associated with the extensive-form games	mechanism	2K_dev_1241
By introducing a new weighting scheme for the dilated entropy function	mechanism	2K_dev_1241
we develop the first distance-generating function that only a logarithmic dependence on the branching factor of the player	mechanism	2K_dev_1241
This result improves the convergence rate of several first-order methods by a factor of ( b dd )	mechanism	2K_dev_1241
where b is the branching factor of the player	mechanism	2K_dev_1241
and d is the depth of the game tree	mechanism	2K_dev_1241
Thus far counterfactual regret minimization methods have been faster in practice	mechanism	2K_dev_1241
and more popular than first-order methods despite their theoretically inferior convergence rates	mechanism	2K_dev_1241
Using our new weighting scheme and practical tuning	method	2K_dev_1241
for solving extensive-form games for the strategy spaces of sequential games	purpose	2K_dev_1241
Researchers and educators have designed curricula and resources for introductory programming environments such as Scratch	background	2K_dev_1247
App Inventor and Kodu to foster computational thinking in K-12	background	2K_dev_1247
We found that the students who used physical manipulatives performed well in rule construction	finding	2K_dev_1247
whereas the students who engaged more with the rule editor of the programming environment had better mental simulation of the rules and understanding of the concepts	finding	2K_dev_1247
In particular we investigated the impact of physical manipulatives on 3rd -- 5th grade students ' ability to understand	method	2K_dev_1247
recognize construct and use game programming design patterns	method	2K_dev_1247
This paper is an empirical study of the effectiveness and usefulness of tiles and flashcards developed for Microsoft Kodu Game Lab to support students in learning how to program and develop games	purpose	2K_dev_1247
Voting systems typically treat all voters equally	background	2K_dev_1256
We derive possibility and impossibility results for the existence of such weighting schemes	finding	2K_dev_1256
depending on whether the voting rule and the weighting scheme are deterministic or randomized	finding	2K_dev_1256
as well as on the social choice axioms satisfied by the voting rule	finding	2K_dev_1256
we draw on no-regret learning	mechanism	2K_dev_1256
Specifically given a voting rule	mechanism	2K_dev_1256
we wish to design a weighting scheme such that applying the voting rule	mechanism	2K_dev_1256
with voters weighted by the scheme	mechanism	2K_dev_1256
leads to choices that are almost as good as those endorsed by the best voter in hindsight	mechanism	2K_dev_1256
We argue that perhaps they should not : Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones	purpose	2K_dev_1256
To develop a formal framework for desirable weighting schemes	purpose	2K_dev_1256
In this paper we present reasoning techniques comprising discrete dynamics as well as continuous dynamics	mechanism	2K_dev_1258
in which the components have local responsibilities	mechanism	2K_dev_1258
Our approach supports component contracts i	mechanism	2K_dev_1258
input assumptions and output guarantees of interfaces that are more general than previous component-based hybrid systems verification techniques in the following ways : We introduce change contracts	mechanism	2K_dev_1258
which characterize how current values exchanged between components along ports relate to previous values	mechanism	2K_dev_1258
We also introduce delay contracts	mechanism	2K_dev_1258
which describe the change relative to the time that has passed since the last value was exchanged Together	mechanism	2K_dev_1258
these contracts can take into account what has changed between two components in a given amount of time since the last exchange of information	mechanism	2K_dev_1258
Most crucially we prove that the safety of compatible components implies safety of the composite	mechanism	2K_dev_1258
The proof steps of the theorem are also implemented as a tactic in KeYmaerai ? X	method	2K_dev_1258
allowing automatic generation of a KeYmaerai ? X proof for the composite system from proofs of the concrete components	method	2K_dev_1258
for a component-based modeling and verification approach for hybrid systems	purpose	2K_dev_1258
This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract	background	2K_dev_1266
likely at the levels of parts	background	2K_dev_1266
and independent of the specific objects or categories experienced during training	background	2K_dev_1266
suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks	background	2K_dev_1266
Interestingly the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet	finding	2K_dev_1266
We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network ( DCNN ) that learns to associate different views of each 3D object The re-training process effectively performs distance metric learning under the object persistency constraints	mechanism	2K_dev_1266
to modify the view-manifold of object representations	mechanism	2K_dev_1266
It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects	mechanism	2K_dev_1266
resulting in the untangling of the view-manifolds between individual objects within the same category and across categories	mechanism	2K_dev_1266
This untangling enables the model to discriminate and recognize objects within the same category	mechanism	2K_dev_1266
We found that this ability is not limited to the trained objects	mechanism	2K_dev_1266
but transfers to novel objects in both trained and untrained categories	mechanism	2K_dev_1266
as well as to a variety of completely novel artificial synthetic objects	mechanism	2K_dev_1266
to capture the notion of object persistence and continuity in our visual experience	purpose	2K_dev_1266
Given a collection of seasonal time-series	background	2K_dev_1267
how can we find regular ( cyclic ) patterns and outliers ( i	background	2K_dev_1267
rare events ) ? These two types of patterns are hidden and mixed in the time-varying activities	background	2K_dev_1267
demonstrate the benefits of the proposed model and algorithm	finding	2K_dev_1267
in that the model can capture latent cyclic patterns	finding	2K_dev_1267
trends and rare events	finding	2K_dev_1267
and the algorithm outperforms the existing state-of-the-art approaches	finding	2K_dev_1267
CycloneFact was up to 5 times more accurate and 20 times faster than top competitors	finding	2K_dev_1267
We present CycloneM a unifying model and CycloneFact	mechanism	2K_dev_1267
a novel algorithm We also present an automatic mining framework AutoCyclone	mechanism	2K_dev_1267
based on CycloneM and CycloneFact Our method has the following properties ; ( a ) effective : it captures important cyclic features such as trend and seasonality	mechanism	2K_dev_1267
and distinguishes regular patterns and rare events clearly ; ( b ) robust and accurate : it detects the above features and patterns accurately against outliers ; ( c ) fast : CycloneFact takes linear time in the data size and typically converges in a few iterations ; ( d ) parameter free : our modeling framework frees the user from having to provide parameter values	mechanism	2K_dev_1267
Extensive experiments on 4 real datasets	method	2K_dev_1267
How can we robustly separate regular patterns and outliers	purpose	2K_dev_1267
without requiring any prior information ? to capture both cyclic patterns and outliers	purpose	2K_dev_1267
which solves the above problem	purpose	2K_dev_1267
Reading tracing and explaining the behavior of code are strongly correlated with the ability to write code effectively	background	2K_dev_1271
Kodu reasoning problems appear to be a promising tool for assessing computational thinking in young programmers	background	2K_dev_1271
Explicitly teaching semantics proved helpful with one type of misconception but not with others We found different styles of student reasoning ( analytical and analogical ) that may correspond to distinct neo-Piagetian stages of development as described by Teague and Lister ( 2014 )	finding	2K_dev_1271
we introduced two groups of third graders to Microsoft 's Kodu Game Lab ; the second group was also given four semantic `` Laws of Kodu '' to better scaffold their reasoning and discourage some common misconceptions During each session	method	2K_dev_1271
students were asked to predict the behavior of short Kodu programs	method	2K_dev_1271
To investigate program understanding in young children	purpose	2K_dev_1271
When tasked to find fraudulent social network users	background	2K_dev_1273
what is a practitioner to do ?	background	2K_dev_1273
We report the signs of such behaviors	finding	2K_dev_1273
including oddities in local network connectivity	finding	2K_dev_1273
account attributes and similarities and differences across fraud providers We discover several types of fraud behaviors	finding	2K_dev_1273
with the possibility of even more	finding	2K_dev_1273
which give exceptionally strong ( > 0	finding	2K_dev_1273
95 precision/recall ) discriminative power on ground-truth data	finding	2K_dev_1273
and which reduces misclassification rate by > 18 % over baselines and routes practitioner attention to samples at high-risk of misclassification	finding	2K_dev_1273
and building algorithms First	mechanism	2K_dev_1273
we set up honeypots	mechanism	2K_dev_1273
or `` dummy '' social network accounts on which we solicit fake followers ( after careful IRB approval )	mechanism	2K_dev_1273
We discuss how to leverage these insights in practice	mechanism	2K_dev_1273
build strongly performing entropy-based features	mechanism	2K_dev_1273
and propose OEC ( Open-ended Classification )	mechanism	2K_dev_1273
an approach for `` future-proofing '' existing algorithms to account for the complexities of link fraud	mechanism	2K_dev_1273
Our contributions are ( b ) features : we engineer features ( c ) algorithm : we motivate and discuss OEC	mechanism	2K_dev_1273
by analyzing fraudulent behavioral patterns	method	2K_dev_1273
featurizing users to yield strong discriminative performance	method	2K_dev_1273
( a ) observations : we analyze our honeypot fraudster ecosystem and give insights regarding various fraud behaviors	method	2K_dev_1273
Traditional classification can lead to poor generalization and high misclassification given few and possibly biased labels We tackle this problem to handle new and multimodal fraud types	purpose	2K_dev_1273
The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors	background	2K_dev_1287
demonstrate that S pike M accurately and succinctly describes all patterns of the rise and fall spikes in social networks	finding	2K_dev_1287
In this article we propose S pike M	mechanism	2K_dev_1287
a concise yet flexible analytical model of Our model has the following advantages First	mechanism	2K_dev_1287
unification power : it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models We provide the threshold of the take-off versus die-out conditions for S pike M and discuss the generality of our model by applying it to an arbitrary graph topology Second	mechanism	2K_dev_1287
practicality : it matches the observed behavior of diverse sets of real data Third	mechanism	2K_dev_1287
parsimony : it requires only a handful of parameters	mechanism	2K_dev_1287
Fourth usefulness : it makes it possible to perform analytic tasks such as forecasting	mechanism	2K_dev_1287
spotting anomalies and interpretation by reverse engineering the system parameters of interest ( quality of news	mechanism	2K_dev_1287
number of interested bloggers	mechanism	2K_dev_1287
) We also introduce an efficient and effective algorithm namely S pike S tream	mechanism	2K_dev_1287
which identifies multiple diffusion patterns in a large collection of online event streams	mechanism	2K_dev_1287
Extensive experiments on real datasets	method	2K_dev_1287
How quickly does a piece of news spread over these media ? How does its popularity diminish over time ? Does the rising and falling pattern follow a simple universal law ? the rise and fall patterns of information diffusion for the real-time monitoring of information diffusion	purpose	2K_dev_1287
The mechanism classes we study are significantly different from well-understood function classes typically found in machine learning	background	2K_dev_1289
so bounding their complexity requires a sharp understanding of the interplay between mechanism parameters and buyer valuations	background	2K_dev_1289
We present a single	mechanism	2K_dev_1289
overarching theorem that uses empirical Rademacher complexity	mechanism	2K_dev_1289
including affine maximizer auctions	mechanism	2K_dev_1289
mixed-bundling auctions and second-price item auctions Despite the extensive applicability of our main theorem	mechanism	2K_dev_1289
we match and improve over the best-known generalization guarantees for many auction classes	mechanism	2K_dev_1289
This all-encompassing theorem also applies to multi- and single-item pricing mechanisms in both multi- and single-unit settings	mechanism	2K_dev_1289
such as linear and non-linear pricing mechanisms	mechanism	2K_dev_1289
Finally our central theorem allows us to easily derive generalization guarantees for every class in several finely grained hierarchies of auction and pricing mechanism classes	mechanism	2K_dev_1289
Instead the mechanism designer receives a set of samples from this distribution and his goal is to use the sample to design a pricing mechanism or auction with high expected profit	method	2K_dev_1289
We provide generalization guarantees which bound the difference between average profit on the sample and expected profit over the distribution	method	2K_dev_1289
These bounds are directly proportional to the intrinsic complexity of the mechanism class the designer is optimizing over	method	2K_dev_1289
We study the design of pricing mechanisms and auctions when the mechanism designer does not know the distribution of buyers ' values	purpose	2K_dev_1289
to measure the intrinsic complexity of a variety of widely-studied single- and multi-item auction classes We demonstrate how to determine the precise level in a hierarchy with the optimal tradeoff between profit and generalization using structural profit maximization	purpose	2K_dev_1289
The same techniques can be applied to monitor other types of traffic data	background	2K_dev_1293
We are able to approximately recover the taxi-pick activities in Manhattan by sampling at only 5 selected intersections	finding	2K_dev_1293
This paper proposes a series of sampling	mechanism	2K_dev_1293
recovery and representation techniques based on graph signal processing	mechanism	2K_dev_1293
We validate our proposed techniques on Manhattan 's taxi pickups during the years of 2014 and 2015	method	2K_dev_1293
Is it possible to monitor the entire traffic in Manhattan at a few intersections ? to handle complex	purpose	2K_dev_1293
Information cascades are ubiquitous in both physical society and online social media	background	2K_dev_1309
taking on large variations in structures	background	2K_dev_1309
dynamics and semantics potentially providing insights into intrinsic mechanisms governing information spreading in nature and new models to forecast as well as to impose good control over information cascades in real applications	background	2K_dev_1309
We find that the structural complexity of information cascades is far beyond the previous conjectures	finding	2K_dev_1309
finding some brand new structure patterns of information cascades	finding	2K_dev_1309
In this paper we explore a large-scale dataset including 432 million information cascades with explicit records of spreading traces We first propose seven-dimensional metrics	mechanism	2K_dev_1309
which reflect size and spreading orientation aspects	mechanism	2K_dev_1309
Further we analyze the correlations of these metrics	method	2K_dev_1309
Although there has been much progress on understanding the dynamics and semantics of information cascades	purpose	2K_dev_1309
little is known about their structural patterns to quantify the structural characteristics of millions of information cascades	purpose	2K_dev_1309
As online fraudsters invest more resources	background	2K_dev_1313
including purchasing large pools of fake user accounts and dedicated IPs	background	2K_dev_1313
fraudulent attacks become less obvious and their detection becomes increasingly challenging	background	2K_dev_1313
showed that HoloScope achieved significant accuracy improvements on synthetic and real data	finding	2K_dev_1313
compared with state-of-the-art fraud detection methods	finding	2K_dev_1313
Hence we propose HoloScope	mechanism	2K_dev_1313
which uses information from graph topology and temporal spikes In terms of graph topology	mechanism	2K_dev_1313
we introduce `` contrast suspiciousness	mechanism	2K_dev_1313
'' a dynamic weighting approach	mechanism	2K_dev_1313
which allows us to more accurately detect fraudulent blocks	mechanism	2K_dev_1313
In terms of temporal spikes	mechanism	2K_dev_1313
HoloScope takes into account the sudden bursts and drops of fraudsters ' attacking patterns In addition	mechanism	2K_dev_1313
we provide theoretical bounds for how much this increases the time cost needed for fraudsters to conduct adversarial attacks Additionally	mechanism	2K_dev_1313
from the perspective of ratings	mechanism	2K_dev_1313
HoloScope incorporates the deviation of rating scores in order to catch fraudsters more accurately	mechanism	2K_dev_1313
Moreover HoloScope has a concise framework and sub-quadratic time complexity	mechanism	2K_dev_1313
making the algorithm reproducible and scalable	mechanism	2K_dev_1313
Existing approaches such as average degree maximization suffer from the bias of including more nodes than necessary	purpose	2K_dev_1313
resulting in lower accuracy and increased need for manual verification	purpose	2K_dev_1313
to more accurately detect groups of fraudulent users	purpose	2K_dev_1313
In comparison-shopping services ( CSS )	background	2K_dev_1314
there exist frauds who perform excessive clicks on a target item in order to boost the popularity of it	background	2K_dev_1314
propose three anomaly scores designed based on click behaviors of users in CSS	mechanism	2K_dev_1314
In this paper we introduce the problem of detecting frauds in CSS and	purpose	2K_dev_1314
As one of the featured initiatives in smart grids	background	2K_dev_1315
demand response is enabling active participation of electricity consumers in the supply/demand balancing process	background	2K_dev_1315
thereby enhancing the power systems operational flexibility in a costeffective way	background	2K_dev_1315
Industrial load plays an important role in demand response because of its intense power consumption	background	2K_dev_1315
already existing advanced monitoring and control infrastructure	background	2K_dev_1315
and its strong economic incentive due to the high energy costs	background	2K_dev_1315
As typical industrial loads	background	2K_dev_1315
cement plants are able to quickly adjust their power consumption rate by switching on/off the crushers	background	2K_dev_1315
by proposing methods that enable these loads to provide regulation or load following with the support of an on-site energy storage system	mechanism	2K_dev_1315
However in the cement plant as well as other industrial loads	purpose	2K_dev_1315
switching on/off the loading units only achieves discrete power changes	purpose	2K_dev_1315
which restricts the load from offering valuable ancillary services such as regulation and load following	purpose	2K_dev_1315
as continuous power changes are required for these services	purpose	2K_dev_1315
In this paper we overcome this restriction of poor granularity	purpose	2K_dev_1315
Large graph datasets have caused renewed interest for graph partitioning	background	2K_dev_1316
Towards this we introduce the idea of skew-resistant graph partitioning	mechanism	2K_dev_1316
where Skewresistant graph partitioning tries to mitigate skewness by taking the characteristics of both the target workload and the graph structure into consideration	mechanism	2K_dev_1316
However existing well-studied graph partitioners often assume that vertices of the graph are always active during the computation	purpose	2K_dev_1316
which may lead to time-varying skewness for traversal-style graph workloads	purpose	2K_dev_1316
like Breadth First Search	purpose	2K_dev_1316
since they only explore part of the graph in each superstep	purpose	2K_dev_1316
Additionally existing solutions do not consider what vertices each partition will have	purpose	2K_dev_1316
as a result high-degree vertices may be concentrated into a few partitions	purpose	2K_dev_1316
causing imbalance the objective is to create an initial partitioning that will `` hold well '' over time without suffering from skewness	purpose	2K_dev_1316
Utility maximization under a budget constraint is a classical problem in economics and management science	background	2K_dev_1323
It is commonly assumed that the utility is a `` nice '' known analytic function	background	2K_dev_1323
for example continuous and concave	background	2K_dev_1323
In many domains such as marketing	background	2K_dev_1323
increased availability of computational resources and data has enabled the development of sophisticated simulations to evaluate the impact of allocating a fixed budget among alternatives ( e	background	2K_dev_1323
marketing channels ) on outcomes	background	2K_dev_1323
While simulations enable high resolution evaluation of alternative budget allocation strategies	background	2K_dev_1323
they significantly complicate the associated budget optimization problem	background	2K_dev_1323
demonstrates the effectiveness of our approach	finding	2K_dev_1323
by first converting the problem into a multi-choice knapsack optimization problem with unknown weights We show that if weights ( corresponding to marginal impact thresholds for each channel ) are well approximated	mechanism	2K_dev_1323
we can achieve a solution within a factor of 2 of optimal	mechanism	2K_dev_1323
and this bound is tight	mechanism	2K_dev_1323
We then develop several parsimonious query algorithms	mechanism	2K_dev_1323
In particular simulation runs are time consuming	purpose	2K_dev_1323
significantly limiting the space of options that can be explored	purpose	2K_dev_1323
An important second challenge is the common presence of budget complementarities	purpose	2K_dev_1323
where non-negligible budget increments are required for an appreciable marginal impact from a channel	purpose	2K_dev_1323
This introduces a combinatorial structure on the decision space We propose to address these challenges for achieving this approximation in an online fashion	purpose	2K_dev_1323
In total this work substantially expands the scope and scale of problems that can be solved using semidefinite programming methods	background	2K_dev_1325
We show that for certain problems	finding	2K_dev_1325
the method is strictly decreasing and guaranteed to converge to a critical point	finding	2K_dev_1325
In all settings we demonstrate improvement over the existing state of the art along various dimensions	finding	2K_dev_1325
In this paper we propose a coordinate descent approach The approach	mechanism	2K_dev_1325
which we call the Mixing method	mechanism	2K_dev_1325
is extremely simple to implement	mechanism	2K_dev_1325
has no free parameters	mechanism	2K_dev_1325
and typically attains an order of magnitude or better improvement in optimization performance over the current state of the art	mechanism	2K_dev_1325
We then apply the algorithm to three separate domains : solving the maximum cut semidefinite relaxation	method	2K_dev_1325
solving a ( novel ) maximum satisfiability relaxation	method	2K_dev_1325
and solving the GloVe word embedding optimization problem	method	2K_dev_1325
to low-rank structured semidefinite programming	purpose	2K_dev_1325
demonstrates that H-FUSE reconstructs the original data 30 81 % better than the least squares method	finding	2K_dev_1327
We propose H-FUSE a novel method that solves above problems by allowing injection of domain knowl- edge in a principled way	mechanism	2K_dev_1327
and turning the task into a well- defined optimization problem H-FUSE has the following desirable properties : ( a ) Effectiveness	mechanism	2K_dev_1327
recovering histori- cal data from aggregated reports with high accuracy ; ( b ) Self-awareness	mechanism	2K_dev_1327
providing an assessment of when the re- covery is not reliable ; ( c ) Scalability	mechanism	2K_dev_1327
computationally lin- ear on the size of the input data	mechanism	2K_dev_1327
Experiments on the real data ( epidemiology counts from the Tycho project [ 13 ]	method	2K_dev_1327
In this paper we address the challenge of recovering a time sequence of counts from aggregated historical data	purpose	2K_dev_1327
For example given a mixture of the monthly and weekly sums	purpose	2K_dev_1327
how can we find the daily counts of people infected with flu ? In general	purpose	2K_dev_1327
what is the best way to recover historical counts from aggregated	purpose	2K_dev_1327
possibly overlapping historical reports	purpose	2K_dev_1327
in the presence of missing values ? Equally importantly	purpose	2K_dev_1327
how much should we trust this reconstruction ?	purpose	2K_dev_1327
We present OpenFace our new that approaches state-of-the-art accuracy	mechanism	2K_dev_1329
Integrating OpenFace with inter-frame tracking	mechanism	2K_dev_1329
we build RTFace a mechanism that selectively blurs faces according to specified policies at full frame rates	mechanism	2K_dev_1329
This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions	mechanism	2K_dev_1329
Finally we present a scalable	mechanism	2K_dev_1329
open-source face recognition system for denaturing video streams for large camera networks using RTFace	purpose	2K_dev_1329
As video cameras proliferate	background	2K_dev_1330
the ability to scalably capture and search their data becomes important	background	2K_dev_1330
In this setting we describe interactive data exploration ( IDE )	mechanism	2K_dev_1330
which refers using predicates that may not have been part of any prior indexing We also describe a new technique called just-in-time indexing ( JITI ) that improves response times in IDE	mechanism	2K_dev_1330
Scalability is improved by performing video analytics on cloudlets at the edge of the Internet	purpose	2K_dev_1330
and only shipping extracted index information and meta-data to the cloud	purpose	2K_dev_1330
to human-in-the-loop content-based retrospective search	purpose	2K_dev_1330
We show that even though GAN optimization does not correspond to a convex-concave game	finding	2K_dev_1335
even for simple parameterizations	finding	2K_dev_1335
under proper conditions equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation	finding	2K_dev_1335
On the other hand	finding	2K_dev_1335
we show that the recently-proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium	finding	2K_dev_1335
Motivated by this stability analysis	mechanism	2K_dev_1335
we propose an additional regularization term for gradient descent GAN updates	mechanism	2K_dev_1335
In this paper we analyze the `` gradient descent '' form of GAN optimization ( i	method	2K_dev_1335
the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters )	method	2K_dev_1335
Despite their growing prominence	purpose	2K_dev_1335
optimization in generative adversarial networks ( GANs ) is still a poorly-understood topic which is able to guarantee local stability for both the WGAN and for the traditional GAN	purpose	2K_dev_1335
and also shows practical promise in speeding up convergence and addressing mode collapse	purpose	2K_dev_1335
Consider a stream of retweet events - how can we spot fraudulent lock-step behavior in such multi-aspect data ( i	background	2K_dev_1336
tensors ) evolving over time ? Can we detect it in real time	background	2K_dev_1336
with an accuracy guarantee ? Past studies have shown that dense subtensors tend to indicate anomalous or even fraudulent behavior in many tensor data	background	2K_dev_1336
including social media Wikipedia	background	2K_dev_1336
updates by our algorithms are up to a million times faster than the fastest batch algorithms Effective : our DENSESALERT successfully spots anomalies especially those overlooked by existing algorithms	finding	2K_dev_1336
We propose DENSESTREAM an incremental algorithm that maintains and updates a dense subtensor in a tensor stream ( i	mechanism	2K_dev_1336
a sequence of changes in a tensor )	mechanism	2K_dev_1336
and DENSESALERT an incremental algorithm spotting the sudden appearances of dense subtensors	mechanism	2K_dev_1336
Our algorithms are : ( 1 ) Fast and `` any time '' :	mechanism	2K_dev_1336
( 2 ) Provably accurate : our algorithms guarantee a lower bound on the density of the subtensor they maintain	mechanism	2K_dev_1336
and ( 3 )	mechanism	2K_dev_1336
Thus several algorithms have been proposed for detecting dense subtensors rapidly and accurately	purpose	2K_dev_1336
However existing algorithms assume that tensors are static	purpose	2K_dev_1336
while many real-world tensors	purpose	2K_dev_1336
including those mentioned above	purpose	2K_dev_1336
showed that depth continuity is a prerequisite for facilitation of Gabor target detection in the context of flanking Gabors	finding	2K_dev_1337
and that similarly surface continuity in purely disparity-defined slanted surfaces was strongly enhanced in distributed patch detections as a function of stimulus duration in this dual discrimination task	finding	2K_dev_1337
showing that the perceptual processing of disparity and integration of 3D surface information across depth cues has time courses of several seconds	finding	2K_dev_1337
attesting to complexity of the neural processing hardware showed how surface reconstruction could be accomplished across the typically sparse depth information available	finding	2K_dev_1337
and integrated among sparse	finding	2K_dev_1337
in a computational model based on a novel Leaky Drift Diffusion Theory that we developed for the underlying neural signals	mechanism	2K_dev_1337
which can serve as an analytic basis for the time course of all neural decision processes	mechanism	2K_dev_1337
Three complementary computational modeling projects from three collaborating laboratories	mechanism	2K_dev_1337
Psychophysical studies The time course of depth surface perception was studied in a coordinated trio of psychophysical	method	2K_dev_1337
neurophysiological and functional imaging studies	method	2K_dev_1337
Abstract : The multidisciplinary goal was to develop an integrated conceptualization of the mid-level encoding of 3D object structure from multiple surface cues	purpose	2K_dev_1337
The unprecedented dips of performance reduction in the component psychometric functions was captured	purpose	2K_dev_1337
Previous work has replaced structural assumptions on the noise with a worst-case approach that aims to choose an outcome that minimizes the maximum error with respect to any feasible true ranking	background	2K_dev_1345
This approach underlies algorithms that have recently been deployed on the social choice website RoboVote	background	2K_dev_1345
We derive ( mostly sharp ) analytical bounds on the expected error and establish the practical benefits of our approach	finding	2K_dev_1345
We take a less conservative viewpoint by minimizing the average error with respect to the set of feasible ground truth rankings	mechanism	2K_dev_1345
We revisit the classic problem of designing voting rules that aggregate objective opinions	purpose	2K_dev_1345
in a setting where voters have noisy estimates of a true ranking of the alternatives	purpose	2K_dev_1345
It is known that such allocations can be computed using O ( n ln ( 1/e ) ) operations in the standard Robertson-Webb Model	background	2K_dev_1347
implies that allocations that are exactly equitable can not be computed	background	2K_dev_1347
We establish a lower bound of ( ln ( 1/e ) /lnln ( 1/e ) ) on the complexity of this problem	mechanism	2K_dev_1347
which is almost tight for a constant number of players	mechanism	2K_dev_1347
We are interested in the problem of dividing a cake -- a heterogeneous divisible good -- among n players	purpose	2K_dev_1347
in a way that is e- equitable : every pair of players must have the same value for their own allocated pieces	purpose	2K_dev_1347
up to a difference of at most e	purpose	2K_dev_1347
What can humans compute in their heads ? We are thinking of a variety of Crypto Protocols	background	2K_dev_1354
games like Sudoku Crossword Puzzles	background	2K_dev_1354
Speed Chess and so on	background	2K_dev_1354
we propose a rigorous model of human computation and associated measures of complexity We apply the model and measures first and foremost to the problem of ( 1 ) humanly computable password generation	mechanism	2K_dev_1354
and then consider related problems of ( 2 ) humanly computable `` one-way functions '' and ( 3 ) humanly computable `` pseudorandom generators '' The theory of Human Computability developed here plays by different rules than standard computability	mechanism	2K_dev_1354
and this takes some getting used to For reasons to be made clear	mechanism	2K_dev_1354
the polynomial versus exponential time divide of modern computability theory is irrelevant to human computation In human computability	mechanism	2K_dev_1354
the step-counts for both humans and computers must be more concrete	mechanism	2K_dev_1354
Specifically we restrict the adversary to at most 10^24 ( Avogadro number of ) steps	mechanism	2K_dev_1354
An alternate view of this work is that it deals with the analysis of algorithms and counting steps for the case that inputs are small as opposed to the usual case of inputs large-in-the-limit	mechanism	2K_dev_1354
The intent of this paper is to apply the ideas and methods of theoretical computer science to better understand what humans can compute in their heads	purpose	2K_dev_1354
For example can a person compute a function in their head so that an eavesdropper with a powerful computer -- - who sees the responses to random input -- - still can not infer responses to new inputs ? To address such questions	purpose	2K_dev_1354
How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we exploit them for applications ? A k-core is the maximal subgraph in which all vertices have degree at least k	background	2K_dev_1355
This concept has been applied to such diverse areas as hierarchical structure analysis	background	2K_dev_1355
graph visualization and graph clustering	background	2K_dev_1355
Our discoveries are : ( 1 ) Mirror Pattern : coreness ( i	finding	2K_dev_1355
maximum k such that each vertex belongs to the k-core ) is strongly correlated with degree	finding	2K_dev_1355
( 2 ) Core-Triangle Pattern : degeneracy ( i	finding	2K_dev_1355
maximum k such that the k-core exists ) obeys a 3-to-1 power-law with respect to the count of triangles	finding	2K_dev_1355
( 3 ) Structured Core Pattern : degeneracycores are not cliques but have non-trivial structures such as coreperiphery and communities	finding	2K_dev_1355
Our algorithmic contributions show the usefulness of these patterns	mechanism	2K_dev_1355
( 1 ) Core-A	mechanism	2K_dev_1355
which measures the deviation from Mirror Pattern	mechanism	2K_dev_1355
successfully spots anomalies in real-world graphs	mechanism	2K_dev_1355
( 2 ) Core-D	mechanism	2K_dev_1355
a single-pass streaming algorithm based on Core-Triangle Pattern	mechanism	2K_dev_1355
accurately estimates degeneracy up to 12\ ( \times \ ) faster than its competitor ( 3 ) Core-S	mechanism	2K_dev_1355
inspired by Structured Core Pattern	mechanism	2K_dev_1355
identifies influential spreaders up to 17\ ( \times \ ) faster than its competitors with comparable accuracy	mechanism	2K_dev_1355
Here we explore pervasive patterns related to k-cores and emerging in graphs from diverse domains	purpose	2K_dev_1355
Given a bipartite graph of users and the products that they review	background	2K_dev_1357
or followers and followees	background	2K_dev_1357
how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral	background	2K_dev_1357
) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph	background	2K_dev_1357
Fraudsters can evade these methods using camouflage	background	2K_dev_1357
by adding reviews or follows with honest targets so that they look normal	background	2K_dev_1357
Even worse some fraudsters use hijacked accounts from honest users	background	2K_dev_1357
and then the camouflage is indeed organic	background	2K_dev_1357
show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud FRAUDAR successfully detected a subgraph of more than 4	finding	2K_dev_1357
000 detected accounts of which a majority had tweets showing that they used follower-buying services	finding	2K_dev_1357
We propose FRAUDAR an algorithm that ( a ) is camouflage resistant	mechanism	2K_dev_1357
( b ) provides upper bounds on the effectiveness of fraudsters	mechanism	2K_dev_1357
and ( c ) is effective in real-world data	mechanism	2K_dev_1357
Experimental results under various attacks Additionally	method	2K_dev_1357
in real-world experiments with a Twitter follower -- followee graph of 1	method	2K_dev_1357
Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts	purpose	2K_dev_1357
Situational awareness involves the timely acquisition of knowledge about real-world events	background	2K_dev_1362
distillation of those events into higher-level conceptual constructs	background	2K_dev_1362
and their synthesis into a coherent context-sensitive view	background	2K_dev_1362
We explore how convergent trends in video sensing	mechanism	2K_dev_1362
crowd sourcing and edge computing can be harnessed to create a shared real-time information system	mechanism	2K_dev_1362
for situational awareness in vehicular systems that span driverless and drivered vehicles	purpose	2K_dev_1362
Social media has become a popular and important tool for human communication	background	2K_dev_1364
However due to this popularity	background	2K_dev_1364
spam and the distribution of malicious content by computer-controlled users	background	2K_dev_1364
known as bots has become a widespread problem	background	2K_dev_1364
At the same time	background	2K_dev_1364
when users use social media	background	2K_dev_1364
they generate valuable data that can be used to understand the patterns of human communication	background	2K_dev_1364
is characterized by following four patterns : ( i ) heavy-tails	finding	2K_dev_1364
( ii ) periodic-spikes	finding	2K_dev_1364
( iii ) correlation between consecutive values	finding	2K_dev_1364
and ( iv ) bimodallity	finding	2K_dev_1364
Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics	finding	2K_dev_1364
Additionally when detecting bots	finding	2K_dev_1364
Act-M provided a precision higher than 93 % and 77 % with a sensitivity of 70 % for the Twitter and Reddit datasets	finding	2K_dev_1364
As our second contribution	mechanism	2K_dev_1364
we propose a mathematical model named Act-M ( Activity Model ) We show that Act-M can accurately from social media users Finally	mechanism	2K_dev_1364
we use Act-M to develop a method	mechanism	2K_dev_1364
The first contribution of this article is showing that the distribution of inter-arrival times ( IATs ) between postings We validate Act-M using data from over 55 million postings from four social media services : Reddit	method	2K_dev_1364
Twitter Stack-Overflow and Hacker-News	method	2K_dev_1364
In this article we focus on the following important question : Can we identify and use patterns of human communication to decide whether a human or a bot controls a user ? fit the distribution of IATs that detects if users are bots based only on the timing of their postings	purpose	2K_dev_1364
Abstract Server-side variability the idea that the same job can take longer to run on one server than another due to server-dependent factors isan increasingly important concern in many queueing systems	background	2K_dev_1369
One strategy for overcoming server-side variability to achieve low response time is redundancy	background	2K_dev_1369
under which jobs create copies of themselves and send these copies to multiple different servers	background	2K_dev_1369
waiting for only one copy to complete service	background	2K_dev_1369
Most of the existing theoretical work on redundancy has focused on developing bounds	background	2K_dev_1369
approximations and exact analysis to study the response time gains offered by redundancy	background	2K_dev_1369
shows that FCFS can be unfair in that it can hurt non-redundant jobs	finding	2K_dev_1369
which is provably fair and also achieves excellent overall mean response time	finding	2K_dev_1369
We develop new exact analysis under First-Come First-Served ( FCFS ) scheduling for a general type of system structure ; We then introduce the Least Redundant First ( LRF ) scheduling policy	mechanism	2K_dev_1369
which we prove is optimal with respect to overall system response time	mechanism	2K_dev_1369
but which can be unfair in that it can hurt the jobs that become redundant	mechanism	2K_dev_1369
Finally we introduce the Primaries First ( PF ) scheduling policy	mechanism	2K_dev_1369
However response time is not the only important metric in redundancy systems : in addition to providing low overall response time	purpose	2K_dev_1369
the system should also be fair in the sense that no job class should have a worse mean response time in the system with redundancy than it did in the system before redundancy is allowed In this paper we use scheduling to address the simultaneous goals of ( 1 ) achieving low response time and ( 2 ) maintaining fairness across job classes	purpose	2K_dev_1369
for per-class response time	purpose	2K_dev_1369
We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR	finding	2K_dev_1374
For example we observe both diffusion and non-diffusion mechanism in the group joining process	finding	2K_dev_1374
and power-law decay in group quitting process	finding	2K_dev_1374
rather than exponential decay as expected in SIR model	finding	2K_dev_1374
Therefore we propose a new model come N go	mechanism	2K_dev_1374
a concise yet flexible dynamic model Our model has the following advantages : ( a ) Unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation	mechanism	2K_dev_1374
( b ) Succinctness and interpretability : it contains only six parameters with clear physical meanings ( c ) Accuracy : it can capture various kinds of group evolution patterns preciously	mechanism	2K_dev_1374
and the goodness of fit increases by 58 % over baseline	mechanism	2K_dev_1374
( d ) Usefulness : it can be used in multiple application scenarios	mechanism	2K_dev_1374
such as forecasting and pattern discovery Furthermore	mechanism	2K_dev_1374
our model can provide insights about different evolution patterns of social groups	mechanism	2K_dev_1374
and we also find that group structure and its evolution has notable relations with temporal patterns of group evolution	mechanism	2K_dev_1374
In this article we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users	method	2K_dev_1374
How do social groups	purpose	2K_dev_1374
such as Facebook groups and Wechat groups	purpose	2K_dev_1374
dynamically evolve over time ? How do people join the social groups	purpose	2K_dev_1374
uniformly or with burst ? What is the pattern of people quitting from groups ? Is there a simple universal model to depict the come-and-go patterns of various groups ? for group evolution	purpose	2K_dev_1374
Computer vision based technologies have seen widespread adoption over the recent years	background	2K_dev_1375
This use is not limited to the rapid adoption of facial recognition technology but extends to facial expression recognition	background	2K_dev_1375
scene recognition and more	background	2K_dev_1375
In this paper we introduce a novel distributed privacy infrastructure for the Internet-of-Things and discuss in particular how it can help The infrastructure	mechanism	2K_dev_1375
supports the automated discovery of IoT resources and the selective notification of users This includes the presence of computer vision applications that collect data about users	mechanism	2K_dev_1375
In particular we describe an implementation of functionality that helps users discover nearby cameras and choose whether or not they want their faces to be denatured in the video streams	mechanism	2K_dev_1375
which has undergone early deployment and evaluation on two campuses	method	2K_dev_1375
These developments raise privacy concerns and call for novel solutions to ensure adequate user awareness	purpose	2K_dev_1375
and ideally control over the resulting collection and use of potentially sensitive data	purpose	2K_dev_1375
While cameras have become ubiquitous	purpose	2K_dev_1375
most of the time users are not even aware of their presence	purpose	2K_dev_1375
enhance user 's awareness of and control over the collection and use of video data about them	purpose	2K_dev_1375
Information cascades are ubiquitous in both physical society and online social media	background	2K_dev_1376
taking on large variations in structures	background	2K_dev_1376
Our discoveries also provide a foundation for the microscopic mechanisms for information spreading	background	2K_dev_1376
potentially leading to implications for cascade prediction and outlier detection	background	2K_dev_1376
We find that the structural complexity of information cascades is far beyond the previous conjectures We find that bimodal law governs majority of the metrics	finding	2K_dev_1376
information flows in cascades have four directions	finding	2K_dev_1376
and the self-loop number and average activity of cascades follows power law and finally uncover some notable implications of structural patterns in information cascades	finding	2K_dev_1376
We first propose a ten-dimensional metric	mechanism	2K_dev_1376
reflecting cascade size silhouette	mechanism	2K_dev_1376
direction and activity aspects	mechanism	2K_dev_1376
Here we explore a large-scale dataset including $ 432 $ million information cascades with explicit records of spreading traces	method	2K_dev_1376
spreading behaviors information content as well as user profiles We then analyze the high-order structural patterns of information cascades Finally	method	2K_dev_1376
we evaluate to what extent the structural features of information cascades can explain its dynamic patterns and semantics	method	2K_dev_1376
Although the dynamics and semantics of information cascades have been studied	purpose	2K_dev_1376
the structural patterns and their correlations with dynamics and semantics are largely unknown to quantify the structural characteristics of information cascades	purpose	2K_dev_1376
How do people make friends dynamically in social networks ? What are the temporal patterns for an individual increasing its social connectivity ? What are the basic mechanisms governing the formation of these temporal patterns ? No matter cyber or physical social systems	background	2K_dev_1379
their structure and dynamics are mainly driven by the connectivity dynamics of each individual	background	2K_dev_1379
Our model and discoveries provide a foundation for the microscopic mechanisms of network growth dynamics	background	2K_dev_1379
potentially leading to implications for prediction	background	2K_dev_1379
clustering and outlier detection on human dynamics	background	2K_dev_1379
We uncover a wide range of long-term power law growth and short-term bursty growth for the social connectivity of different users	finding	2K_dev_1379
We propose three key ingredients	finding	2K_dev_1379
namely average-effect multiscale-effect and correlation-effect	finding	2K_dev_1379
which govern the observed growth patterns at microscopic level	finding	2K_dev_1379
we discover statistical regularities underlying the empirical growth dynamics	finding	2K_dev_1379
As a result we propose the long short memory process incorporating these ingredients	mechanism	2K_dev_1379
demonstrating that it successfully reproduces the complex growth patterns observed in the empirical data	mechanism	2K_dev_1379
We examine the detailed growth process of `` WeChat ''	method	2K_dev_1379
the largest online social network in China	method	2K_dev_1379
with 300 million users and 4	method	2K_dev_1379
75 billion links spanning two years	method	2K_dev_1379
By analyzing modeling parameters	method	2K_dev_1379
However due to the lack of empirical data	purpose	2K_dev_1379
little is known about the empirical dynamic patterns of social connectivity at microscopic level	purpose	2K_dev_1379
let alone the regularities or models governing these microscopic dynamics	purpose	2K_dev_1379
This generalizes a prior decomposition result for an M/M/k/staggeredM/M/k/staggered-setup	background	2K_dev_1381
We show that the response time of an M/G/k/staggeredM/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/kM/G/k and the setup time	finding	2K_dev_1381
where the approximation is nearly exact	finding	2K_dev_1381
for exponentially distributed setup times	method	2K_dev_1381
We consider the M/G/k/staggeredM/G/k/staggered-setup	purpose	2K_dev_1381
where idle servers are turned off to save cost	purpose	2K_dev_1381
necessitating a setup time for turning a server back on ; however	purpose	2K_dev_1381
at most one server may be in setup mode at any time	purpose	2K_dev_1381
The Next-Generation Airborne Collision Avoidance System ACASi ? X is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft	background	2K_dev_1387
It is currently being developed by the Federal Aviation Administration FAA	background	2K_dev_1387
Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety	background	2K_dev_1387
In this paper we determine the geometric configurations under a precise set of assumptions and formally verify these configurations using hybrid systems theorem proving techniques	mechanism	2K_dev_1387
We conduct an initial examination of the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version	mechanism	2K_dev_1387
demonstrating how formal hybrid approaches are helping ensure the safety of ACAS X	mechanism	2K_dev_1387
under which the advice given by ACAS X is safe	purpose	2K_dev_1387
Facility location and committee selection are classic embodiments of this problem	background	2K_dev_1389
demonstrate the viability of this approach and the value of such optimized mechanisms vis-a-vis mechanisms derived through worst-case analysis	finding	2K_dev_1389
We propose a class of percentile mechanisms	mechanism	2K_dev_1389
a form of generalized median mechanisms	mechanism	2K_dev_1389
that are strategy-proof and for L1 and L2 cost models More importantly	mechanism	2K_dev_1389
we propose a sample-based framework	mechanism	2K_dev_1389
Our empirical investigations using social cost and maximum load as objectives	method	2K_dev_1389
We consider the mechanism design problem for agents with single-peaked preferences over multi-dimensional domains when multiple alternatives can be chosen derive worst-case approximation ratios for social cost and maximum load for optimizing the choice of percentiles relative to any prior distribution over preferences	purpose	2K_dev_1389
Social choice theory provides insights into a variety of collective decision making settings	background	2K_dev_1391
In this paper we model the problem via Markov decision processes ( MDP )	mechanism	2K_dev_1391
where the states of the MDP coincide with preference profiles and a ( deterministic	mechanism	2K_dev_1391
stationary ) policy corresponds to a social choice function We can therefore employ the axioms studied in the social choice literature as guidelines in the design of socially desirable policies	mechanism	2K_dev_1391
We present tractable algorithms that compute optimal policies under different prominent social choice constraints	mechanism	2K_dev_1391
Our machinery relies on techniques for exploiting symmetries and isomorphisms between MDPs	mechanism	2K_dev_1391
but nowadays some of its tenets are challenged by internet environments	purpose	2K_dev_1391
which call for dynamic decision making under constantly changing preferences	purpose	2K_dev_1391
While the analysis of unlabeled networks has been studied extensively in the past	background	2K_dev_1392
finding patterns in different kinds of labeled graphs is still an open challenge	background	2K_dev_1392
We show that Com $ $ ^2 $ $ 2 spots intuitive patterns regarding edge labels that carry temporal or other discrete information	finding	2K_dev_1392
Our findings include large `` star '' -like patterns	finding	2K_dev_1392
near-bipartite cores as well as tiny groups ( five users )	finding	2K_dev_1392
calling each other hundreds of times within a few days	finding	2K_dev_1392
We also show that we are able to automatically identify competing airline companies	finding	2K_dev_1392
We propose Com $ $ ^2 $ $ 2	mechanism	2K_dev_1392
a novel fast and incremental tensor analysis approach The method is ( a ) scalable	mechanism	2K_dev_1392
being linear on the input size	mechanism	2K_dev_1392
( b ) general	mechanism	2K_dev_1392
( c ) needs no user-defined parameters and ( d ) effective	mechanism	2K_dev_1392
returning results that agree with intuition	mechanism	2K_dev_1392
We apply our method to real datasets	method	2K_dev_1392
including a phone call network	method	2K_dev_1392
a computer-traffic network and a flight information network	method	2K_dev_1392
The phone call network consists of 4 million mobile users	method	2K_dev_1392
with 51 million edges ( phone calls )	method	2K_dev_1392
over 14 days while the flights dataset consists of 7733 airports and 5995 airline companies flying 67	method	2K_dev_1392
Given a large edge-labeled network	purpose	2K_dev_1392
a time-evolving network how can we find interesting patterns ? which can discover communities appearing over subsets of the labels	purpose	2K_dev_1392
We show that OpenEval is able to respond to the queries within a limited amount of time while also achieving high F1 score In addition	finding	2K_dev_1394
we show that the accuracy of responses provided by OpenEval is increased as more time is given for evaluation	finding	2K_dev_1394
that illustrate the effectiveness of our approach compared to related techniques	finding	2K_dev_1394
We introduce OpenEval a which uses information on the web that are stated as multiargument predicate instances ( e	mechanism	2K_dev_1394
DrugHasSideEffect ( Aspirin GI Bleeding ) ) )	mechanism	2K_dev_1394
OpenEval gets a small number of instances of a predicate as seed positive examples and automatically learns how to evaluate the truth of a new predicate instance by querying the web and processing the retrieved unstructured web pages	mechanism	2K_dev_1394
We have extensively tested our model and shown empirical results	method	2K_dev_1394
In this paper we investigate information validation tasks that are initiated as queries from either automated agents or humans new online information validation technique	purpose	2K_dev_1394
to automatically evaluate the truth of queries	purpose	2K_dev_1394
How can we correlate the neural activity in the human brain as it responds to typed words	background	2K_dev_1395
with properties of these terms ( like edible	background	2K_dev_1395
fits in hand ) ? In short	background	2K_dev_1395
we want to find latent variables	background	2K_dev_1395
that jointly explain both the brain activity	background	2K_dev_1395
as well as the behavioral responses	background	2K_dev_1395
This is one of many settings of the Coupled Matrix-Tensor Factorization ( CMTF ) problem	background	2K_dev_1395
by up to 200	finding	2K_dev_1395
along with an up to 65 fold increase in sparsity	finding	2K_dev_1395
with comparable accuracy to the baseline	finding	2K_dev_1395
TURBO-SMT is able to find meaningful latent variables	finding	2K_dev_1395
as well as to predict brain activity with competitive accuracy	finding	2K_dev_1395
We introduce TURBO-SMT a meta-method : it boosts the performance of any CMTF algorithm	mechanism	2K_dev_1395
We apply TURBO-SMT to BRAINQ	method	2K_dev_1395
a dataset consisting of a ( nouns	method	2K_dev_1395
brain voxels human subjects ) tensor and a ( nouns	method	2K_dev_1395
properties ) matrix with coupling along the nouns dimension	method	2K_dev_1395
Can we accelerate any CMTF solver	purpose	2K_dev_1395
so that it runs within a few minutes instead of tens of hours to a day	purpose	2K_dev_1395
while maintaining good accuracy ? capable of doing exactly that	purpose	2K_dev_1395
The typical approach thus far is to use tensors or dynamical systems	background	2K_dev_1396
EEG-MINE ( a ) can successfully reconstruct the signals with high accuracy ; ( b ) can spot surprising patterns within seizure EEG signals ; and ( c ) may provide early warning of epileptic seizures	finding	2K_dev_1396
Here we present EEG-MINE	mechanism	2K_dev_1396
a nonlinear chaos-based `` gray box model ''	mechanism	2K_dev_1396
that blends domain knowledge with data observations	mechanism	2K_dev_1396
When applied to numerous	method	2K_dev_1396
Given electroencephalogram time series data from patients with epilepsy	purpose	2K_dev_1396
can we find patterns and regularities ?	purpose	2K_dev_1396
Live music performance with computers has motivated many research projects in science	background	2K_dev_1398
engineering and the arts	background	2K_dev_1398
We conclude with directions for future work	background	2K_dev_1398
outline our efforts to establish a new direction	mechanism	2K_dev_1398
Human-Computer Music Performance ( HCMP )	mechanism	2K_dev_1398
as a framework for a variety of coordinated studies	mechanism	2K_dev_1398
Our work in this area spans performance analysis	mechanism	2K_dev_1398
synchronization techniques and interactive performance systems	mechanism	2K_dev_1398
We review the development of techniques for live music performance and	method	2K_dev_1398
In spite of decades of work	purpose	2K_dev_1398
it is surprising that there is not more technology for	purpose	2K_dev_1398
and a better understanding of the computer as music performer	purpose	2K_dev_1398
Our goal is to enable musicians to ncorporate computers into performances easily and effectively through a better understanding of requirements	purpose	2K_dev_1398
new techniques and practical	purpose	2K_dev_1398
Complex systems are designed using the model-based design paradigm in which mathematical models of systems are created and checked against specifications	background	2K_dev_1400
Cyber-physical systems ( CPS ) are complex systems in which the physical environment is sensed and controlled by computational or cyber elements possibly distributed over communication networks	background	2K_dev_1400
Various aspects of CPS design such as physical dynamics	background	2K_dev_1400
software control and communication networking must interoperate correctly for correct functioning of the systems	background	2K_dev_1400
Modeling formalisms analysis techniques and tools for designing these different aspects have evolved ind ependently	background	2K_dev_1400
and remain dissimilar and disparate	background	2K_dev_1400
There is no unifying formalism in which one can model all these aspects equally well	background	2K_dev_1400
In current practice there is no principled approach that deals with this modeling heterogeneity within a formal framework	background	2K_dev_1400
Composition of analysis results	finding	2K_dev_1400
this thesis develops a framework based on behavioral semantics Heterogeneity arising from the different interacting aspects of CPS design must be addressed in order to enable system-level verification We develop behavioral semantics to address heterogeneity in a general yet formal manner	mechanism	2K_dev_1400
Our framework makes no assumptions about the specifics of any particular formalism	mechanism	2K_dev_1400
therefore it readily supports various formalisms	mechanism	2K_dev_1400
Models can be analyzed independently in isolation	mechanism	2K_dev_1400
supporting separation of concerns	mechanism	2K_dev_1400
Mappings across heterogeneous semantic domains enable associations between analysis results	mechanism	2K_dev_1400
Interdependencies across different models and specifications can be formally represented as constraints over parameters and verification can be carried out in a semantically consistent manner	mechanism	2K_dev_1400
is supported both hierarchically across different levels of abstraction and structurally into interacting component models at a given level of abstraction	method	2K_dev_1400
The theoretical concepts developed in the thesis are illustrated using a case study on the hierarchical heterogeneous verification of an automotive intersection collision avoidance system	method	2K_dev_1400
Therefore model-based design of CPS must make use of a collection of models in several different formalisms and use respective analysis methods and tools together to ensure correct system design	purpose	2K_dev_1400
To enable doing this in a formal manner for multi-model verification of cyber-physical systems	purpose	2K_dev_1400
we demonstrate that contextual supervision improves significantly over a reasonable baseline and existing unsupervised methods for source separation and show that recovery of the signal components depends only on cross-correlation between features for different signals	finding	2K_dev_1405
not on correlations between features for the same signal	finding	2K_dev_1405
We propose a new framework that lies between the fully supervised and unsupervised setting	mechanism	2K_dev_1405
Instead of supervision we provide input features for each source signal and use convex methods Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision ; our motivating application is energy disaggregation of hourly smart meter data ( the separation of whole-home power signals into different energy uses )	mechanism	2K_dev_1405
Here contextual supervision allows us for thousands homes	mechanism	2K_dev_1405
a task previously impossible due to the need for specialized data collection hardware	mechanism	2K_dev_1405
On smaller datasets which include labels	method	2K_dev_1405
Finally we analyze the case of l2 loss theoretically	method	2K_dev_1405
for single-channel source separation to estimate the correlations between these features and the unobserved signal decomposition	purpose	2K_dev_1405
to provide itemized energy usage	purpose	2K_dev_1405
Computing equilibria of games is a central task in computer science	background	2K_dev_1406
A large number of results are known for Nash equilibrium ( NE )	background	2K_dev_1406
showing that the problem is in P	mechanism	2K_dev_1406
We then design a spatial branch -- and -- bound algorithm to find an SNE	mechanism	2K_dev_1406
and we experimentally evaluate the algorithm	method	2K_dev_1406
However these can be adopted only when coalitions are not an issue	purpose	2K_dev_1406
When instead agents can form coalitions	purpose	2K_dev_1406
NE is inadequate and an appropriate solution concept is strong Nash equilibrium ( SNE )	purpose	2K_dev_1406
Few computational results are known about SNE	purpose	2K_dev_1406
In this paper we first study the problem of verifying whether a strategy profile is an SNE	purpose	2K_dev_1406
Given the re-broadcasts ( i	background	2K_dev_1408
retweets ) of posts in Twitter	background	2K_dev_1408
how can we spot fake from genuine user reactions ? What will be the tell-tale sign the connectivity of retweeters	background	2K_dev_1408
their relative timing or something else ? High retweet activity indicates influential users	background	2K_dev_1408
and can be monetized	background	2K_dev_1408
Hence there are strong incentives for fraudulent users to artificially boost their retweets ' volume	background	2K_dev_1408
Our main contribu- tions are : ( a ) the discovery of patterns that fraudulent activity seems to follow ( the `` triangles `` a nd `` homogeneity '' patterns	finding	2K_dev_1408
the formation of micro-clusters in appropriate feature spaces ) ; and	finding	2K_dev_1408
( b ) `` RTGen ''	mechanism	2K_dev_1408
a realistic generator that mimics the behaviors of both honest and fraud- ulent users	mechanism	2K_dev_1408
We present experiments on a dataset of more than 6 million retweets crawled from Twitter	method	2K_dev_1408
Here we explore the identifi- cation of fraudulent and genuine retweet threads	purpose	2K_dev_1408
Given a multimillion-node social network	background	2K_dev_1413
how can we sum- marize connectivity pattern from the data	background	2K_dev_1413
and how can we find unex- pected user behavior ?	background	2K_dev_1413
We discover that ( a ) the lockstep behavior on the graph shapes dense `` block '' in its adjacency matrix and creates `` ray '' in spectral subspaces	finding	2K_dev_1413
and ( b ) partially overlapping of the behavior shapes `` staircase '' in the matrix and creates `` pearl '' in the subspaces	finding	2K_dev_1413
We demonstrate that our approach is effective	finding	2K_dev_1413
The second contribution is that we provide a fast algorithm	mechanism	2K_dev_1413
using the discovery as a guide for practi- tioners	mechanism	2K_dev_1413
Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep	method	2K_dev_1413
on both synthetic and real data	method	2K_dev_1413
In this paper we study a complete graph from a large who-follows-whom network and spot lockstep behavior that large groups of followers connect to the same groups of followees	purpose	2K_dev_1413
to detect users who offer the lockstep behavior	purpose	2K_dev_1413
We propose a novel method The proposed method considers positive	mechanism	2K_dev_1419
implicit and negative information of all users in a network based on belief propagation to predict trust relationships of a target user	mechanism	2K_dev_1419
to predict accurately trust relationships of a target user even if he/she does not have much interaction information	purpose	2K_dev_1419
Given the retweeting activity for the posts of several Twitter users	background	2K_dev_1421
how can we distinguish organic activity from spammy retweets by paid followers to boost a post 's appearance of popularity ? More gen- erally	background	2K_dev_1421
given groups of observations	background	2K_dev_1421
can we spot strange groups ?	background	2K_dev_1421
Our method achieves a 97 % accuracy on a real dataset of 12 million retweets crawled from Twitter	finding	2K_dev_1421
Here we propose : ( A ) ND-Sync	mechanism	2K_dev_1421
an efficient method and ( B ) a set of carefully designed features ND-Sync is effec- tive in spotting retweet fraudsters	mechanism	2K_dev_1421
robust to different types of abnormal activity	mechanism	2K_dev_1421
and adaptable as it can easily incorporate additional features	mechanism	2K_dev_1421
Our main intuition is that organic behavior has more variability	purpose	2K_dev_1421
while fraud- ulent behavior	purpose	2K_dev_1421
like retweets by botnet members	purpose	2K_dev_1421
We refer to the detection of such synchronized observations as the Syn- chonization Fraud problem	purpose	2K_dev_1421
and we study a specific instance of it	purpose	2K_dev_1421
Retweet Fraud Detection manifested in Twitter	purpose	2K_dev_1421
for detecting group fraud for characterizing retweet threads	purpose	2K_dev_1421
Refactoring of code is a common device in software engineering	background	2K_dev_1423
As cyber-physical systems CPS become ever more complex	background	2K_dev_1423
similar engineering practices become more common in CPS development	background	2K_dev_1423
Proper safe developments of CPS designs are accompanied by a proof of correctness	background	2K_dev_1423
For some of these we can give strong results that they are correct	finding	2K_dev_1423
we develop proof-aware refactorings for CPS	mechanism	2K_dev_1423
That is we study model transformations on CPS and show how they correspond to relations on correctness proofs	mechanism	2K_dev_1423
As the main technical device	mechanism	2K_dev_1423
we show how the impact of model transformations on correctness can be characterized by different notions of refinement in differential dynamic logic Furthermore	mechanism	2K_dev_1423
we demonstrate the application of refinements on a series of safety-preserving and liveness-preserving refactorings	mechanism	2K_dev_1423
by proving on a meta-level Where this is impossible	method	2K_dev_1423
we construct proof obligations for showing that the refactoring respects the refinement relation	method	2K_dev_1423
Since the inherent complexities of CPS practically mandate iterative development	purpose	2K_dev_1423
frequent changes of models are standard practice	purpose	2K_dev_1423
but require reverification of the resulting models after every change	purpose	2K_dev_1423
To overcome this issue	purpose	2K_dev_1423
Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender 's randomized assignment of resources to targets	background	2K_dev_1424
implies that in some realistic situations	background	2K_dev_1424
limited surveillance may not need to be explicitly addressed	background	2K_dev_1424
that in zero-sum security games	finding	2K_dev_1424
lazy defenders who simply keep optimizing against perfectly informed attackers	finding	2K_dev_1424
are almost optimal against diligent attackers	finding	2K_dev_1424
who go to the effort of gathering a reasonable number of observations	finding	2K_dev_1424
This assumption has been challenged by recent papers	purpose	2K_dev_1424
which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance	purpose	2K_dev_1424
Designers of human computation systms often face the need to aggregate noisy information provided by multiple people	background	2K_dev_1429
Our short-term goal is to motivate the design of better human computation systems ; our long-term goal is to spark an interaction between researchers in ( computational ) social choice and human computation	background	2K_dev_1429
Our empirical conclusions show that noisy human voting can differ from what popular theoretical models would predict	finding	2K_dev_1429
We conduct extensive experiments on Amazon Mechanical Turk	method	2K_dev_1429
While voting is often used for this purpose	purpose	2K_dev_1429
the choice of voting method is typically not principled to better understand how different voting rules perform in practice	purpose	2K_dev_1429
Establishing quantitative bounds on the execution cost of programs is essential in many areas of computer science such as complexity analysis	background	2K_dev_1607
compiler optimizations security and privacy	background	2K_dev_1607
Techniques based on program analysis	background	2K_dev_1607
type systems and abstract interpretation are well-studied	background	2K_dev_1607
We prove our type system sound We demonstrate the precision and generality of our technique	finding	2K_dev_1607
In this work we propose a relational cost analysis technique by making use of relational properties of programs and inputs	mechanism	2K_dev_1607
We develop Rel Cost	mechanism	2K_dev_1607
a refinement type and effect system The key novelty of our technique is the combination of relational refinements with two modes of typing-relational typing for reasoning about similar computations/inputs and unary typing for reasoning about unrelated computations/inputs	mechanism	2K_dev_1607
This combination allows us to analyze the execution cost difference of two programs more precisely than a naive non-relational approach	mechanism	2K_dev_1607
using a semantic model based on step-indexed unary and binary logical relations accounting for non-relational and relational reasoning principles with their respective costs	method	2K_dev_1607
but methods for analyzing how the execution costs of two programs compare to each other have not received attention	purpose	2K_dev_1607
Naively combining the worst and best case execution costs of the two programs does not work well in many cases because such analysis forgets the similarities between the programs or the inputs	purpose	2K_dev_1607
that is capable of establishing precise bounds on the difference in the execution cost of two programs for a higher-order functional language with recursion and subtyping	purpose	2K_dev_1607
This is the first amortized analysis	background	2K_dev_1614
that automatically derives polynomial bounds for higher-order functions and polynomial bounds that depend on user-defined inductive types	background	2K_dev_1614
The practicality of the analysis system is the system infers bounds on the number of queries that are sent by OCaml programs to DynamoDB	finding	2K_dev_1614
a commercial NoSQL cloud database service	finding	2K_dev_1614
The system automatically derives worst-case resource bounds for higher-order polymorphic programs with user-defined inductive types	mechanism	2K_dev_1614
The technique is parametric in the resource and can derive bounds for time	mechanism	2K_dev_1614
memory allocations and energy usage The derived bounds are multivariate resource polynomials which are functions of different size parameters that depend on the standard OCaml types	mechanism	2K_dev_1614
Bound inference is fully automatic and reduced to a linear optimization problem that is passed to an off-the-shelf LP solver Technically	mechanism	2K_dev_1614
the analysis system is based on a novel multivariate automatic amortized resource analysis ( AARA ) It builds on existing work on linear AARA for higher-order programs with user-defined inductive types and on multivariate AARA for first-order programs with built-in lists and binary trees Moreover	mechanism	2K_dev_1614
the analysis handles a limited form of side effects and even outperforms the linear bound inference of previous systems At the same time	mechanism	2K_dev_1614
it preserves the expressivity and efficiency of existing AARA techniques	mechanism	2K_dev_1614
This article presents a resource analysis system for OCaml programs	purpose	2K_dev_1614
Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed	background	2K_dev_1627
Some of these issues might be addressed by future tools which autocomplete more complete handlers	background	2K_dev_1627
We found that programmers handle exceptions locally in catch blocks much of the time	finding	2K_dev_1627
rather than propagating by throwing an Exception	finding	2K_dev_1627
Programmers make heavy use of actions like Log	finding	2K_dev_1627
Print Return or Throw in catch blocks	finding	2K_dev_1627
and also frequently copy code between handlers We found bad practices like empty catch blocks or catching Exception are indeed widespread	finding	2K_dev_1627
We discuss evidence that programmers may misjudge risk when catching Exception	finding	2K_dev_1627
and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception	finding	2K_dev_1627
We used the Boa tool	method	2K_dev_1627
A major complaint is that programmers often write minimal and low quality handlers	purpose	2K_dev_1627
to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions	purpose	2K_dev_1627
determine whether a ( T ) X and b ( T ) Y are uncorrelated for every a is an element of R-p	background	2K_dev_1636
b is an element of R-q or not	background	2K_dev_1636
Linear independence testing is a fundamental information-theoretic and statistical problem that can be posed as follows : given n points \ { ( X ( i ) ; Y-i ) \ } ( n ) ( i=1 ) from a p + q dimensional multivariate distribution where X-i is an element of R-p and Y-i is an element of R-q	background	2K_dev_1636
We give minimax lower bound for this problem	purpose	2K_dev_1636
Such incentives are likely aligned with benefits to utilities and grid operators	background	2K_dev_1644
which might take the form of peak-shaving or ancillary services However	background	2K_dev_1644
private cost savings are not strictly aligned with public benefits related to the avoidance of health and environmental damages from power plant emissions	background	2K_dev_1644
We find that feasible strategies exist to simultaneously realize public and private benefits and that load shifting can result in substantial cost savings and avoided damages in some circumstances	finding	2K_dev_1644
Concerns over increased latency and bandwidth costs can be mitigated with modifications to the model	finding	2K_dev_1644
However the level of realized savings is dependent upon the specifics of a particular network operator and electricity rate schedule	finding	2K_dev_1644
so we compare private cost minimization with a strategy that minimizes these externalities	method	2K_dev_1644
This paper assesses the potential cost-saving incentives for content distribution networks to shift traffic load among geographically distributed data centers in response to hourly variation in electricity prices	purpose	2K_dev_1644
Personal informatics systems are becoming increasing prevalent as their price	background	2K_dev_1645
form and ease of use improves	background	2K_dev_1645
Though these systems offer great potential value to users	background	2K_dev_1645
many systems are hampered by issues that limit their ability to foster engagement	background	2K_dev_1645
and people often abandon use of these systems without garnering meaningful outcomes	background	2K_dev_1645
While continued use of these systems is not necessary for all people	background	2K_dev_1645
there is an opportunity to better support people working towards achievement-based goals and discuss how these strategies could be used to foster engagement with PI systems	background	2K_dev_1645
We then propose seven strategies for the design community to explore	mechanism	2K_dev_1645
In this paper we draw from the literature and our own prior Work to identify a number of problems that hinder engagement with achievement-based personal informatics systems-problems related to inadequate support for goal setting	purpose	2K_dev_1645
misalignment of user and system goals	purpose	2K_dev_1645
and the burden of system maintenance	purpose	2K_dev_1645
for mitigating these problems	purpose	2K_dev_1645
Machine learning improves mobile user experience	background	2K_dev_1646
Interestingly envisioning apps with adaptive interfaces that reduce navigation and selection effort is not standard UX practice	background	2K_dev_1646
When implementing an adaptive UI for our mobile transit app	background	2K_dev_1646
we encountered a number of problems	background	2K_dev_1646
extracted six design patterns where UI adaptation can improve in-app navigation	finding	2K_dev_1646
Next we designed an exemplar set of wireframes	mechanism	2K_dev_1646
illustrating how UX designers might annotate their interaction flows	mechanism	2K_dev_1646
we reviewed the interfaces of popular apps and	method	2K_dev_1646
Our original design did not log necessary information nor did it induce users to provide good labels	purpose	2K_dev_1646
On reflection we realized UX designers should identify and refine UI adaptions when sketching wireframes	purpose	2K_dev_1646
To advance on this insight to communicate planned adaptation and note the information ( logs and labels ) needed to make the desired inferences	purpose	2K_dev_1646
Compressed sensing is a simple and efficient technique that has a number of applications in signal processing and machine learning	background	2K_dev_1649
In machine learning it provides answers to questions such as : `` under what conditions is the sparse representation of data efficient ? { '' } ; `` when is learning a large margin classifier directly on the compressed domain possible ? { '' } ; and `` why does a large margin classifier learn more effectively if the data is sparse ? { '' }	background	2K_dev_1649
and show for the high dimensional sparse signals	finding	2K_dev_1649
when the bounds are tight	finding	2K_dev_1649
directly learning in the compressed domain is possible	finding	2K_dev_1649
by leveraging compressed sensing from the learning perspective We show	mechanism	2K_dev_1649
for a full-rank signal	mechanism	2K_dev_1649
the high dimensional sparse representation of data is efficient because from the classifiers viewpoint such a representation is in fact a low dimensional problem	mechanism	2K_dev_1649
We provide practical bounds on the linear classifier to investigate the relationship between the SVM classifier in the high dimensional and compressed domains	method	2K_dev_1649
This work tackles the problem of feature representation from the context of sparsity and affine rank minimization in order to provide answers to the aforementioned questions	purpose	2K_dev_1649
An important research problem in learning analytics is	background	2K_dev_1650
which suggests ways in which we might foster these social benefits through intervention	finding	2K_dev_1650
we propose a pipeline that includes data infrastructure	mechanism	2K_dev_1650
learning analytics and intervention	mechanism	2K_dev_1650
along with computational models for individual components	mechanism	2K_dev_1650
Next we describe an example of applying this pipeline to real data in a case study	method	2K_dev_1650
whose goal is to investigate the positive effects that goal-setting students have on their peers	method	2K_dev_1650
to expedite the cycle of data leading to the analysis of student progress and the improvement of student support	purpose	2K_dev_1650
For this goal in the context of social learning	purpose	2K_dev_1650
Ambiguity arises in requirements when a statement is unintentionally or otherwise incomplete	background	2K_dev_1656
missing information or when a word or phrase has more than one possible meaning	background	2K_dev_1656
to yield a rank order of vague terms in both isolation and composition	finding	2K_dev_1656
to show how increases in vagueness will decrease users ' acceptance of privacy risk and thus decrease users ' willingness to share personal information	finding	2K_dev_1656
The taxonomy was evaluated in a paired comparison experiment and results were analyzed using the Bradley-Terry model We further provide empirical evidence based on factorial vignette surveys	method	2K_dev_1656
For web-based and mobile information systems	purpose	2K_dev_1656
ambiguity and vagueness in particular	purpose	2K_dev_1656
undermines the ability of organizations to align their privacy policies with their data practices	purpose	2K_dev_1656
which can confuse or mislead users thus leading to an increase in privacy risk	purpose	2K_dev_1656
The theory predicts how vague modifiers to information actions and information types can be composed to increase or decrease overall vagueness	purpose	2K_dev_1656
Increasing proliferation of mobile and online social networking platforms have given us unprecedented opportunity to observe and study social interactions at a fine temporal scale	background	2K_dev_1669
A collection of all such social interactions among a group of individuals ( or agents ) observed over an interval of time is referred to as a temporally-detailed ( TD ) social network	background	2K_dev_1669
A TD social network opens up the opportunity to explore TD questions on the underlying social system	background	2K_dev_1669
`` How is the betweenness centrality of an individual changing with time ? { '' } To this end	background	2K_dev_1669
related work has proposed temporal extensions of centrality metrics ( e	background	2K_dev_1669
betweenness and closeness )	background	2K_dev_1669
We prove the correctness and completeness of our algorithm	finding	2K_dev_1669
shows that the proposed algorithm out performs the alternatives by a wide margin	finding	2K_dev_1669
To this end we propose a novel computational paradigm called epoch-point based techniques Using the concept of epoch-points	mechanism	2K_dev_1669
we develop a novel algorithm for computing shortest path based centrality metric such as betweenness on a TD social network	mechanism	2K_dev_1669
However scalable computation of these metrics for long time-intervals is challenging	purpose	2K_dev_1669
This is due to the non-stationary ranking of shortest paths ( the underlying structure of betweenness and closeness ) between a pair of nodes which violates the assumptions of classical dynamic programming based techniques	purpose	2K_dev_1669
for addressing the non-stationarity challenge of TD social networks	purpose	2K_dev_1669
Nonverbal behaviors play an important role in communication for both humans and social robots	background	2K_dev_1680
However adding contextually appropriate animations by hand is time consuming and does not scale well	background	2K_dev_1680
Previous researchers have developed automated systems for inserting animations based on utterance text	background	2K_dev_1680
yet these systems lack human understanding of social context and are still being improved	background	2K_dev_1680
Results showed untrained workers are capable of providing reasonable labeling of semantic information and that emotional expressions derived from the labels were rated more highly than control videos	finding	2K_dev_1680
More study is needed to determine the effects of emphasis labels	finding	2K_dev_1680
To test this approach	method	2K_dev_1680
untrained workers from Mechanical Turk labeled semantic information	method	2K_dev_1680
specifically emotion and emphasis	method	2K_dev_1680
for each utterance which was used to automatically add animations	method	2K_dev_1680
Videos of a robot performing the animated dialogue were rated by a second set of participants	method	2K_dev_1680
This work proposes a middle ground where untrained human workers label semantic information	purpose	2K_dev_1680
which is input to an automatic system to produce appropriate gestures	purpose	2K_dev_1680
Morphable face models are a powerful tool	background	2K_dev_1681
We present a new multi-part model of the eye that includes a morphable model of the facial eye region	mechanism	2K_dev_1681
as well as an anatomy-based eyeball model	mechanism	2K_dev_1681
It is the first morphable model that accurately	mechanism	2K_dev_1681
since it was built from high-quality head scans	mechanism	2K_dev_1681
It is also the first	mechanism	2K_dev_1681
since we treat it as a separate part	mechanism	2K_dev_1681
To showcase our model we present a new method for illumination-and head-pose invariant gaze estimation from a single RGB image	mechanism	2K_dev_1681
We fit our model to an image through analysis-by-synthesis	mechanism	2K_dev_1681
solving for eye region shape	mechanism	2K_dev_1681
texture eyeball pose and illumination simultaneously	mechanism	2K_dev_1681
The fitted eyeball pose parameters are then used to estimate gaze direction	mechanism	2K_dev_1681
Through evaluation on two standard datasets	method	2K_dev_1681
but have previously failed to model the eye accurately due to complexities in its material and motion captures eye region shape to allow independent eyeball movement	purpose	2K_dev_1681
Discrete energy minimization is widely-used in computer vision and machine learning for problems such as MAP inference in graphical models The problem	background	2K_dev_1685
in general is notoriously intractable	background	2K_dev_1685
and finding the global optimal solution is known to be NP-hard This paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms	background	2K_dev_1685
Specifically we show that general energy minimization	finding	2K_dev_1685
even in the 2-label pairwise case	finding	2K_dev_1685
and planar energy minimization with three or more labels are exp-APX-complete	finding	2K_dev_1685
This finding rules out the existence of any approximation algorithm with a sub-exponential approximation ratio in the input size for these two problems	finding	2K_dev_1685
including constant factor approximations	finding	2K_dev_1685
Moreover we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes - PO	mechanism	2K_dev_1685
APX and exp-APX corresponding to problems that are solvable	mechanism	2K_dev_1685
approximable and inapproximable in polynomial time	mechanism	2K_dev_1685
Problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones	mechanism	2K_dev_1685
However is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time ? We show in this paper that the answer is no	purpose	2K_dev_1685
New IT functions have greatly increased the amount of in-car information delivered to drivers	background	2K_dev_1687
Although valuable that information can distract drivers when delivered during vehicle operation	background	2K_dev_1687
By inferring driver state from sensor data	background	2K_dev_1687
prior research has shown that it can accurately identify opportune moments to deliver information	background	2K_dev_1687
With these results researchers can then build information delivery systems that can deliver information to drivers both when they are interruptible and when they find the information valuable	background	2K_dev_1687
we identified driving situations when each of the in-car information items is highly valuable	finding	2K_dev_1687
and verified these situations Results from our study offer important insights for understanding the diversity of drivers ' experiences about the value of in-car information and the ability to determine situations in which this information is valuable to drivers	finding	2K_dev_1687
To answer this question	method	2K_dev_1687
we conducted a series of surveys and interviews and compiled a list of representative in-car information items and context factors that affect the importance of these items By combining and exploring those context factors through a large online survey of drivers	method	2K_dev_1687
Lastly we examined what technology is available for detecting these driving situations	method	2K_dev_1687
and which situations require further advanced technologies for detection	method	2K_dev_1687
Now that we know when to best deliver information	purpose	2K_dev_1687
it raises the question : what information should we deliver at those interruptible moments ?	purpose	2K_dev_1687
Recent research has improved our understanding of how to create strong	background	2K_dev_1695
and suggest ways to ease password entry for mobile users	background	2K_dev_1695
We compare the strength and usability of passwords created and used on mobile devices with those created and used on desktops and laptops	method	2K_dev_1695
while varying password policy requirements and input methods	method	2K_dev_1695
However this research has generally been in the context of desktops and laptops	purpose	2K_dev_1695
while users are increasingly creating and entering passwords on mobile devices	purpose	2K_dev_1695
In this paper we study whether recent password guidance carries over to the mobile setting	purpose	2K_dev_1695
Playtesting or using play to guide game design	background	2K_dev_1714
gives designers feedback about whether their game is meeting their goals and the player 's expectations We conclude with lessons learned and next steps in our research on playtesting	background	2K_dev_1714
novice game designers leveraged playtest methods and tools	finding	2K_dev_1714
employed playtesting and data collection methods appropriate for their goals	finding	2K_dev_1714
and effectively applied playtest data in iterative design	finding	2K_dev_1714
case study We identify common missteps made by novice designers and address these missteps through the concept of purposefulness	method	2K_dev_1714
understanding why you are playtesting as well as how to playtest	method	2K_dev_1714
We ground our workshops in the development of rich player experience goals	method	2K_dev_1714
which inform playtest design	method	2K_dev_1714
data collection and iteration	method	2K_dev_1714
We show that by applying methods taught in our workshops	method	2K_dev_1714
We report a of designing	purpose	2K_dev_1714
deploying and iterating on a series of playtesting workshops for novice game designers	purpose	2K_dev_1714
We demonstrate its usefulness results in easier and better-structured proofs	finding	2K_dev_1716
We introduce differential refinement logic ( dRL )	mechanism	2K_dev_1716
a logic with first-class support and a proof calculus dRL simultaneously solves several seemingly different challenges common in theorem proving for hybrid systems : By using a refinement relation to arrange proofs hierarchically according to the structure of natural subsystems	mechanism	2K_dev_1716
we can increase the readability and modularity of the resulting proof	mechanism	2K_dev_1716
dRL extends an existing specification and verification language for hybrid systems ( differential dynamic logic	mechanism	2K_dev_1716
dL ) by adding a refinement relation to directly compare hybrid systems	mechanism	2K_dev_1716
with examples where using refinement	method	2K_dev_1716
for refinement relations on hybrid systems for verifying such relations This paper gives a syntax	purpose	2K_dev_1716
semantics and proof calculus for dRL	purpose	2K_dev_1716
Patients researching medical diagnoses	background	2K_dev_1719
scientist exploring new fields of literature	background	2K_dev_1719
and students learning about new domains are all faced with the challenge of capturing information they find for later use	background	2K_dev_1719
However saving information is challenging on mobile devices	background	2K_dev_1719
where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use Furthermore	background	2K_dev_1719
beyond the challenge of simply selecting a region of bounded text on a mobile device	background	2K_dev_1719
in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user	background	2K_dev_1719
In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster	background	2K_dev_1719
we find that this approach reduced selection time and was preferred by participants over the default system text selection method	finding	2K_dev_1719
We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context	mechanism	2K_dev_1719
In a two part user study	method	2K_dev_1719
we introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration	purpose	2K_dev_1719
to support identifying and saving information in an intentionally uncertain way on mobile devices	purpose	2K_dev_1719
We present results which indicate that starting with the ASR output is worse unless it is sufficiently accurate ( Word Error Rate of under 30\ % )	finding	2K_dev_1732
In this paper we empirically explore how the latency of transcriptions created by participants recruited on Amazon Mechanical Turk vary based on the accuracy of speech recognition output	purpose	2K_dev_1732
The topic of kinematics of laser rangefinders has received little attention in the robotics literature	background	2K_dev_1749
even though such sensors have been the perception sensors of choice on commercial AGVs	background	2K_dev_1749
field robots and aerial robots for some time	background	2K_dev_1749
In recognition of the uniqueness of optical reflection mechanisms	mechanism	2K_dev_1749
this paper presents a formulation based on a matrix reflection operator	mechanism	2K_dev_1749
The resulting invariant generation method is observed to be much more scalable and efficient than the na `` ive approach	finding	2K_dev_1752
exhibiting orders of magnitude performance improvement on many of the problems	finding	2K_dev_1752
Based on the notion of discrete abstraction	mechanism	2K_dev_1752
our method eliminates unsoundness and unnecessary coarseness found in existing approaches for computing abstractions for non-linear continuous systems and is able to construct invariants with intricate boolean structure	mechanism	2K_dev_1752
in contrast to invariants typically generated using template-based methods	mechanism	2K_dev_1752
In order to tackle the state explosion problem associated with discrete abstraction	mechanism	2K_dev_1752
we present invariant generation algorithms that exploit sound proof rules for safety verification	mechanism	2K_dev_1752
such as differential cut ( DC )	mechanism	2K_dev_1752
and a new proof rule that we call differential divide-and-conquer ( DDC )	mechanism	2K_dev_1752
which splits the verification problem into smaller sub-problems	mechanism	2K_dev_1752
This paper presents a method for generating semi-algebraic invariants for systems governed by non-linear polynomial ordinary differential equations under semi-algebraic evolution constraints	purpose	2K_dev_1752
We demonstrate the power of our approach Our method achieves superior performance	finding	2K_dev_1765
This paper presents a novel solution that automatically transforms unlabeled	mechanism	2K_dev_1765
heterogeneous motion data into new styles	mechanism	2K_dev_1765
The key idea of our approach is an online learning algorithm that automatically constructs a series of local mixtures of autoregressive models ( MAR ) We construct local MAR models on the fly by searching for the closest examples of each input pose in the database	mechanism	2K_dev_1765
Once the model parameters are estimated from the training data	mechanism	2K_dev_1765
the model adapts the current pose with simple linear transformations	mechanism	2K_dev_1765
In addition we introduce an efficient local regression model	mechanism	2K_dev_1765
by transferring stylistic human motion for a wide variety of actions	method	2K_dev_1765
including walking running punching	method	2K_dev_1765
kicking jumping and transitions between those behaviors in a comparison against alternative methods	method	2K_dev_1765
We have also performed experiments to evaluate the generalization ability of our data-driven model as well as the key components of our system	method	2K_dev_1765
for realtime generation of stylistic human motion to capture the complex relationships between styles of motion	purpose	2K_dev_1765
to predict the timings of synthesized poses in the output style	purpose	2K_dev_1765
show that our method compares competitively with the state of the art on both 2D and 3D measures	finding	2K_dev_1772
while yielding a richer interpretation of the 3D scene behind the image	finding	2K_dev_1772
In this paper we propose a novel algorithm that infers the 3D layout of building facades from a single 2D image of an urban scene	mechanism	2K_dev_1772
Different from existing methods that only yield coarse orientation labels or qualitative block approximations	mechanism	2K_dev_1772
our algorithm quantitatively using a set of planes mutually related by 3D geometric constraints	mechanism	2K_dev_1772
Each plane is characterized by a continuous orientation vector and a depth distribution	mechanism	2K_dev_1772
An optimal solution is reached through inter-planar interactions Due to the quantitative and plane-based nature of our geometric reasoning	mechanism	2K_dev_1772
our model is more expressive and informative than existing approaches	mechanism	2K_dev_1772
reconstructs building facades in 3D space	purpose	2K_dev_1772
A popular approach in this regard is to represent a sequence using a bag of words ( BOW ) representation due to its : ( i ) fixed dimensionality irrespective of the sequence length	background	2K_dev_1783
and ( ii ) its ability to compactly model the statistics in A drawback to the BOW representation	background	2K_dev_1783
however is the intrinsic destruction of the temporal ordering information	background	2K_dev_1783
show significant performance improvements across both isolated and continuous event detection tasks	finding	2K_dev_1783
In this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering Our representation	mechanism	2K_dev_1783
like BOW is of a fixed dimensionality making it easily integrated with a linear detection function	mechanism	2K_dev_1783
Extensive experiments on CK+	method	2K_dev_1783
6DMG and UvA-NEMO databases	method	2K_dev_1783
In this paper we tackle the problem of efficient video event detection	purpose	2K_dev_1783
We argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation	purpose	2K_dev_1783
Understanding the purpose of why sensitive data is used could help improve privacy as well as enable new kinds of access control	background	2K_dev_1785
and achieved an accuracy of about 85\ % and 94\ % respectively in inferring purposes	finding	2K_dev_1785
We have also found that text-based features alone are highly effective in inferring purposes	finding	2K_dev_1785
In this paper we introduce a new technique We extract multiple kinds of features from decompiled code	mechanism	2K_dev_1785
focusing on app-specific features and text-based features These features are then used to train a machine learning classifier	mechanism	2K_dev_1785
We have evaluated our approach in the context of two sensitive permissions	method	2K_dev_1785
namely ACCESS FINE LOCATION and READ CONTACT LIST	method	2K_dev_1785
for inferring the purpose of sensitive data usage in the context of Android smartphone apps	purpose	2K_dev_1785
Some languages have very consistent mappings between graphemes and phonemes	background	2K_dev_1786
while in other languages	background	2K_dev_1786
this mapping is more ambiguous	background	2K_dev_1786
Consonantal writing systems prove to be a challenge for Text to Speech Systems ( TTS ) because they do not indicate short vowels	background	2K_dev_1786
which creates an ambiguity in pronunciation Special letter-to-sound rules may be needed for some cases in languages that otherwise have a good correspondence between graphemes and phonemes Our methods can be generalized to other languages that exhibit similar phenomena	background	2K_dev_1786
and show significant improvements for dialects of Arabic	finding	2K_dev_1786
We propose a technique during ITS training and predict pronunciations from text during synthesis time	mechanism	2K_dev_1786
We conduct experiments on dialects of Arabic for disambiguating homographs and Hindi for discovering the schwa-deletion rules	mechanism	2K_dev_1786
We evaluate our systems using objective and subjective metrics of TTS	method	2K_dev_1786
In the low-resource scenario	purpose	2K_dev_1786
we may not have linguistic resources such as diacritizers or hand-written rules for the language	purpose	2K_dev_1786
to automatically learn pronunciations iteratively from acoustics	purpose	2K_dev_1786
Ensuring language coverage in dialog systems can be a challenge	background	2K_dev_1789
since the language in a domain may drift over time	background	2K_dev_1789
creating a mismatch between the original training data and current input	background	2K_dev_1789
This in turn degrades performance by increasing misunderstanding and eventually leading to task failure	background	2K_dev_1789
Without the capability of adapting the vocabulary and the language model based on certain domains or users	background	2K_dev_1789
recognition errors may degrade the understanding performance	background	2K_dev_1789
and even lead to a task failure	background	2K_dev_1789
which incurs more time and effort to recover	background	2K_dev_1789
show that both recognition and semantic parsing accuracy can thereby be improved	finding	2K_dev_1789
by leveraging different types of relatedness between vocabulary items and words retrieved from web-based resources	mechanism	2K_dev_1789
This paper investigates how coverage can be maintained by automatically acquiring potential out-of-vocabulary ( OOV ) words	purpose	2K_dev_1789
The experiments show that high-level semantic information can accurately estimate the prominence of slots	finding	2K_dev_1790
significantly improving the slot induction performance ; furthermore	finding	2K_dev_1790
a semantic decoder trained on the data with automatically extracted slots achieves about 68\ % F-measure	finding	2K_dev_1790
which is close to the one from hand-crafted grammars	finding	2K_dev_1790
Given unlabelled conversations we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots ( e	method	2K_dev_1790
both slots `` direction { '' } and `` locale { '' } convey location-related information ) for building a coherent semantic hierarchy	method	2K_dev_1790
and then estimate the slot importance at different levels	method	2K_dev_1790
The high-level semantic estimation involves not only within-slot but also cross slot relations	method	2K_dev_1790
We study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems	purpose	2K_dev_1790
in particular learning the hierarchical semantic structure from the data	purpose	2K_dev_1790
Multimodal analysis has long been an integral part of studying learning	background	2K_dev_1805
Historically multimodal analyses of learning have been extremely laborious and time intensive and the implications that this work may have in non-education-related contexts	background	2K_dev_1805
we find that affect-and pose-based segmentation are more effective	finding	2K_dev_1805
than traditional approaches for drawing correlations between learning-relevant constructs	finding	2K_dev_1805
and multimodal behaviors We also find that pose-based segmentation outperforms the two more traditional segmentation strategies for predicting student success on the hands-on task	finding	2K_dev_1805
In particular we propose affect-and pose-based data segmentation	mechanism	2K_dev_1805
as alternatives to human-based segmentation	mechanism	2K_dev_1805
we present a comparative analysis of four different data segmentation techniques	method	2K_dev_1805
In a study of ten dyads working on an open-ended engineering design task	method	2K_dev_1805
However researchers have recently been exploring ways to use multimodal computational analysis in the service of studying how people learn in complex learning environments	purpose	2K_dev_1805
In an effort to advance this research agenda	purpose	2K_dev_1805
In this paper we discuss the algorithms used	purpose	2K_dev_1805
Do we really need 3D labels in order to learn how to predict 3D ?	background	2K_dev_1808
Despite never seeing a 3D label	finding	2K_dev_1808
our method produces competitive results	finding	2K_dev_1808
Rather than use explicit supervision	mechanism	2K_dev_1808
we use the regularity of indoor scenes	mechanism	2K_dev_1808
We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D is unavailable	method	2K_dev_1808
In this paper we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label to learn the mapping in a completely unsupervised manner	purpose	2K_dev_1808
Given only a large	mechanism	2K_dev_1810
unlabeled image collection we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first	mechanism	2K_dev_1810
This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation	purpose	2K_dev_1810
We argue that doing well on this task requires the model to learn to recognize objects and their parts	purpose	2K_dev_1810
Mobile applications frequently access sensitive personal information to meet user or business requirements	background	2K_dev_1811
Because such information is sensitive in general	background	2K_dev_1811
regulators increasingly require mobile-app developers to publish privacy policies that describe what information is collected	background	2K_dev_1811
Furthermore regulators have fined companies when these policies are inconsistent with the actual data practices of mobile apps	background	2K_dev_1811
we propose a semi-automated framework that consists of a policy terminology-API method map that links policy phrases to API methods that produce sensitive information	mechanism	2K_dev_1811
and information flow analysis to detect misalignments	mechanism	2K_dev_1811
We present an implementation of our framework based on a privacy-policy-phrase ontology and a collection of mappings from API methods to policy phrases	method	2K_dev_1811
To help mobile-app developers check their privacy policies against their apps ' code for consistency	purpose	2K_dev_1811
Specifically inspired by curriculum learning	mechanism	2K_dev_1812
we present a two-step approach for CNN training	mechanism	2K_dev_1812
First we use easy images to train an initial visual representation	mechanism	2K_dev_1812
We then use this initial CNN and adapt it to harder	mechanism	2K_dev_1812
more realistic images by leveraging the structure of data and categories	mechanism	2K_dev_1812
We present an approach to utilize large amounts of web data for learning CNNs	purpose	2K_dev_1812
Varied sources of error contribute to the challenge of facial action unit detection	background	2K_dev_1815
Previous approaches address specific and known sources	background	2K_dev_1815
However many sources are unknown	background	2K_dev_1815
With few exceptions CPM outperformed baseline and state-of-the art methods	finding	2K_dev_1815
we propose a Confident Preserving Machine ( CPM that follows an easy-to-hard classification strategy	mechanism	2K_dev_1815
During training CPM learns two confident classifiers	mechanism	2K_dev_1815
A confident positive classifier separates easily identified positive samples from all else ; a confident negative classifier does same for negative samples	mechanism	2K_dev_1815
During testing CPM then learns a person-specific classifier using `` virtual labels { '' } provided by confident classifiers	mechanism	2K_dev_1815
This step is achieved using a quasi-semi-supervised ( QSS ) approach	mechanism	2K_dev_1815
Hard samples are typically close to the decision boundary	mechanism	2K_dev_1815
and the QSS approach disambiguates them using spatio-temporal constraints	mechanism	2K_dev_1815
To evaluate CPM we compared it with a baseline single-margin classifier and state-of-the-art semi-supervised learning	method	2K_dev_1815
transfer learning and boosting methods in three datasets of spontaneous facial behavior	method	2K_dev_1815
To address the ubiquity of error	purpose	2K_dev_1815
Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent	background	2K_dev_1816
semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ	background	2K_dev_1816
LDA classifiers have two distinct benefits : ( i ) they exhibit higher average precision than similarity metrics typically used in correspondence problems	mechanism	2K_dev_1816
and ( ii ) unlike exemplar SVM	mechanism	2K_dev_1816
can output globally interpretable posterior probabilities without calibration	mechanism	2K_dev_1816
whilst also being significantly faster to train	mechanism	2K_dev_1816
We pose the correspondence problem as a graphical model	mechanism	2K_dev_1816
where the unary potentials are computed via convolution with the set of exemplar classifiers	mechanism	2K_dev_1816
and the joint potentials enforce smoothly varying correspondence assignment	mechanism	2K_dev_1816
Motivated by object recognition literature and recent work on rapidly estimating linear classifiers	purpose	2K_dev_1816
we treat semantic correspondence as a constrained detection problem	purpose	2K_dev_1816
where an exemplar LDA classifier is learned for each pixel	purpose	2K_dev_1816
Starting with the seminal work by Kempe et al	background	2K_dev_1817
a broad variety of problems	background	2K_dev_1817
such as targeted marketing and the spread of viruses and malware	background	2K_dev_1817
have been modeled as selecting a subset of nodes to maximize diffusion through a network	background	2K_dev_1817
In cyber-security applications however	background	2K_dev_1817
a key consideration largely ignored in this literature is stealth	background	2K_dev_1817
In particular an attacker often has a specific target in mind	background	2K_dev_1817
but succeeds only if the target is reached ( e	background	2K_dev_1817
by malware ) before the malicious payload is detected and corresponding countermeasures deployed	background	2K_dev_1817
The dual side of this problem is deployment of a limited number of monitoring units	background	2K_dev_1817
such as cyber-forensics specialists	background	2K_dev_1817
so as to limit the likelihood of such targeted and stealthy diffusion processes reaching their intended targets	background	2K_dev_1817
show that a number of natural variants of this problem are NP-hard to approximate	finding	2K_dev_1817
On the positive side	finding	2K_dev_1817
we show that if stealthy diffusion starts from randomly selected nodes	finding	2K_dev_1817
the defender 's objective is submodular	finding	2K_dev_1817
and a fast greedy algorithm has provable approximation guarantees	finding	2K_dev_1817
show that the proposed algorithms are highly effective and scalable	finding	2K_dev_1817
In addition we present approximation algorithms by adaptively selecting the starting nodes for the diffusion process	mechanism	2K_dev_1817
We investigate the problem of optimal monitoring of targeted stealthy diffusion processes	purpose	2K_dev_1817
and for the setting in which an attacker optimally responds to the placement of monitoring nodes	purpose	2K_dev_1817
Poor spelling is a challenge faced by people with dyslexia throughout their lives	background	2K_dev_1818
Spellcheckers are therefore a crucial tool for people with dyslexia	background	2K_dev_1818
but current spellcheckers do not detect real-word errors	background	2K_dev_1818
which are a common type of errors made by people with dyslexia	background	2K_dev_1818
Real-word errors are spelling mistakes that result in an unintended but real word	background	2K_dev_1818
for instance form instead of from	background	2K_dev_1818
Nearly 20\ % of the errors that people with dyslexia make are real-word	background	2K_dev_1818
and showed that it detects more of these errors than widely used spellcheckers	finding	2K_dev_1818
people with dyslexia corrected sentences more accurately and in less time with Real Check	finding	2K_dev_1818
In this paper we introduce a system called Real Check that uses a probabilistic language model	mechanism	2K_dev_1818
a statistical dependency parser and Google n-grams	mechanism	2K_dev_1818
to detect real-world errors	purpose	2K_dev_1818
Modern cyber-physical systems interact closely with continuous physical processes like kinematic movement	background	2K_dev_1821
Software component frameworks do not provide an explicit way to represent or reason about these processes	background	2K_dev_1821
Meanwhile hybrid program models have been successful in proving critical properties of discrete-continuous systems	background	2K_dev_1821
These programs deal with diverse aspects of a cyber-physical system such as controller decisions	background	2K_dev_1821
component communication protocols and mechanical dynamics	background	2K_dev_1821
requiring several programs to address the variation	background	2K_dev_1821
However currently these aspects are often intertwined in mostly monolithic hybrid programs	background	2K_dev_1821
which are difficult to understand	background	2K_dev_1821
architectural models We build formal architectural abstractions of hybrid programs and formulas	mechanism	2K_dev_1821
enabling analysis of hybrid programs at the component level	mechanism	2K_dev_1821
reusing parts of hybrid programs	mechanism	2K_dev_1821
and automatic transformation from views into hybrid programs and formulas	mechanism	2K_dev_1821
Our approach is evaluated in the context of a robotic collision avoidance case study	method	2K_dev_1821
These issues can be addressed by component-based engineering	purpose	2K_dev_1821
making hybrid modeling more practical	purpose	2K_dev_1821
This paper lays the foundation for using to provide component-based benefits to developing hybrid programs	purpose	2K_dev_1821
Research has shown that understanding conversational structure between students is paramount to evaluating the productivity of the collaboration and estimating outcomes	background	2K_dev_1823
However previous methods often rely on human supplied dialogue act labels or discourse parsing algorithms requiring large labeled datasets	background	2K_dev_1823
In this paper we present a new method In particular	mechanism	2K_dev_1823
we introduce a machine learning method	mechanism	2K_dev_1823
for example when one student provides the answer to a question or comments on something another student previously said Our method	mechanism	2K_dev_1823
which utilizes a fast	mechanism	2K_dev_1823
exact optimization process known as spectral optimization	mechanism	2K_dev_1823
does not require manually annotated training data and is highly scalable and generalizable	mechanism	2K_dev_1823
Empirical using real world datasets consisting of conversations between students participating in Coursera courses	method	2K_dev_1823
for understanding discussions between students in MOOC forums	purpose	2K_dev_1823
for discovering instances in which a response relation exists between a pair of posts in a forum thread	purpose	2K_dev_1823
This approach is complementary to other efforts in the literature on speeding up computation through GPU implementation	background	2K_dev_1830
fast matrix operations or quantization	background	2K_dev_1830
in that any of these optimizations can be incorporated	background	2K_dev_1830
we are able to dramatically reduce the rate of growth of computation as the number of models increases	finding	2K_dev_1830
show that we are able to maintain	finding	2K_dev_1830
or even exceed the level of performance compared to the default approach of using all the models directly	finding	2K_dev_1830
in both detection and classification tasks	finding	2K_dev_1830
We show that by formulating the visual task as a large matrix multiplication problem	mechanism	2K_dev_1830
something that is possible for a broad set of modern detectors and classifiers The approach	mechanism	2K_dev_1830
based on a bilinear separation model	mechanism	2K_dev_1830
combines standard matrix factorization with a task-dependent term which ensures that the resulting smaller size problem maintains performance on the original task	mechanism	2K_dev_1830
In this paper we investigate the issue of evaluating efficiently a large set of models on an input image in detection and classification tasks	purpose	2K_dev_1830
Requirements analysts can model regulated data practices to identify and reason about risks of noncompliance	background	2K_dev_1833
If terminology is inconsistent or ambiguous	background	2K_dev_1833
however these models and their conclusions will be unreliable	background	2K_dev_1833
Tregex is a utility to match regular expressions against constituency parse trees	background	2K_dev_1833
which are hierarchical expressions of natural language clauses	background	2K_dev_1833
including noun and verb phrases	background	2K_dev_1833
To study this problem	purpose	2K_dev_1833
we investigated an approach to automatically construct an information type ontology by identifying information type hyponymy in privacy policies using Tregex patterns	purpose	2K_dev_1833
Cultural events are kinds of typical events closely related to history and nationality	background	2K_dev_1837
which play an important role in cultural heritage through generations	background	2K_dev_1837
by combining both ideas of object / scene contents mining and strong image representation via CNN into a whole framework	mechanism	2K_dev_1837
Specifically  we employ selective search to extract a batch of bottom-up region proposals	mechanism	2K_dev_1837
which are served as key object / scene candidates in each event image ; while	mechanism	2K_dev_1837
we investigate two state-of-the-art deep architectures	mechanism	2K_dev_1837
VGGNet and GoogLeNet and adapt them to our task by performing domain-specific ( i	mechanism	2K_dev_1837
event ) fine-tuning on both global image and hierarchical region proposals	mechanism	2K_dev_1837
These two models can complementarily exploit feature hierarchies spatially	mechanism	2K_dev_1837
which simultaneously capture the global context and local evidences within the image	mechanism	2K_dev_1837
However automatically recognizing cultural events still remains a great challenge since it depends on understanding of complex image contents such as people	purpose	2K_dev_1837
objects and scene context	purpose	2K_dev_1837
Therefore it is intuitive to associate this task with other high-level vision problems	purpose	2K_dev_1837
object detection recognition and scene understanding	purpose	2K_dev_1837
In this paper we address this problem for object / scene contents mining for representation via CNN	purpose	2K_dev_1837
Automated program repair ( APR ) is a challenging process of detecting bugs	background	2K_dev_1839
localizing buggy code generating fix candidates and validating the fixes	background	2K_dev_1839
Effectiveness of program repair methods relies on the generated fix candidates	background	2K_dev_1839
and the methods used to traverse the space of generated candidates to search for the best ones Existing approaches generate fix candidates based on either syntactic searches over source code or semantic analysis of specification	background	2K_dev_1839
In this paper we propose to combine both syntactic and semantic fix candidates We present an automated repair method based on structured specifications	mechanism	2K_dev_1839
deductive verification and genetic programming	mechanism	2K_dev_1839
Given a function with its specification	mechanism	2K_dev_1839
we utilize a modular verifier to detect bugs and localize both program statements and sub-formulas in the specification that relate to those bugs While the former are identified as buggy code	mechanism	2K_dev_1839
the latter are transformed as semantic fix candidates	mechanism	2K_dev_1839
We additionally generate syntactic fix candidates via various mutation operators	mechanism	2K_dev_1839
Best candidates which receives fewer warnings via a static verification	mechanism	2K_dev_1839
are selected for evolution though genetic programming until we find one satisfying the specification	mechanism	2K_dev_1839
Another interesting feature of our proposed approach is that we efficiently ensure the soundness of repaired code through modular ( or compositional ) verification	mechanism	2K_dev_1839
We implemented our proposal and tested it on C programs taken from the SIR benchmark that are seeded with bugs	method	2K_dev_1839
to enhance the search space of APR	purpose	2K_dev_1839
and provide a function to effectively traverse the search space	purpose	2K_dev_1839
Applications such as construction monitoring and planning for renovations	background	2K_dev_1846
require the accurate recovery of existing conditions of structures	background	2K_dev_1846
We demonstrate the capability and robustness of our approach	finding	2K_dev_1846
To address this issue	mechanism	2K_dev_1846
this paper presents an approach from a 3D point cloud containing a complex network of thin structures	mechanism	2K_dev_1846
and In our approach	mechanism	2K_dev_1846
each beam is evolved from a seed by matching and aligning the cross section images	mechanism	2K_dev_1846
This growing algorithm can model beams with arbitrary cross sections By performing the algorithm on a point connectivity graph	mechanism	2K_dev_1846
we distinguish beams from joints and improve the algorithm 's robustness to closely spaced objects	mechanism	2K_dev_1846
In parallel planes and joints are also extracted and modeled	mechanism	2K_dev_1846
The connectivity graph of these primitives allows for a compact	mechanism	2K_dev_1846
object-level understanding of the entire structure	mechanism	2K_dev_1846
on both synthetic and real datasets	method	2K_dev_1846
Many types of infrastructure are primarily comprised of arbitrarily-shaped thin structures ( e	purpose	2K_dev_1846
truss bridges steel frame buildings under construction	purpose	2K_dev_1846
and transmission towers )	purpose	2K_dev_1846
which existing automatic modeling methods are incapable of handling	purpose	2K_dev_1846
to automatically recognize and model beams	purpose	2K_dev_1846
planes and joints to recover their topology	purpose	2K_dev_1846
Many learning-based computer vision algorithms perform poorly when faced with examples that are dissimilar to those on which they were trained	background	2K_dev_1847
show that the proposed approach outperforms existing single-step methods on a dataset of nine building styles	finding	2K_dev_1847
We propose a two-step approach The first step uses a small number of labeled examples	mechanism	2K_dev_1847
while the second step uses traditional domain adaptation methods	mechanism	2K_dev_1847
Domain adaptation methods attempt to address this problem	purpose	2K_dev_1847
but usually assume that the source domain is specified a priori for situations where more than one source domain available to choose the source domain most similar to the target domain to further adapt the chosen source domain to the target data	purpose	2K_dev_1847
A key challenge of developing robots that work closely with people is creating a user interface that allows a user to communicate complex instructions to a robot quickly and easily	background	2K_dev_1870
to show that the proposed system is able to detect and track a leader through unconstrained and cluttered off-road environments under a wide variety of illumination and motion conditions	finding	2K_dev_1870
This paper presents a marker tracking system that uses near infrared cameras	mechanism	2K_dev_1870
retro-reflective markers and LIDAR	mechanism	2K_dev_1870
In this application the robot is carrying equipment and supplies for a group of pedestrians	method	2K_dev_1870
and the primary task for the user interface is to keep the robot traveling with the overall group in the right formation	method	2K_dev_1870
We provide an extensive quantitative evaluation	method	2K_dev_1870
We consider a walking logistics support robot	purpose	2K_dev_1870
which is designed to carry heavy loads to locations that are too difficult to reach with a wheeled or tracked vehicle to allow a particular user to designate himself as the robot 's leader	purpose	2K_dev_1870
and guide the robot along a desired path	purpose	2K_dev_1870
Robotic swarms are distributed systems whose members interact via local control laws to achieve a variety of behaviors	background	2K_dev_1875
In many practical applications	background	2K_dev_1875
human operators may need to change the current behavior of a swarm from the goal that the swarm was going towards into a new goal due to dynamic changes in mission objectives	background	2K_dev_1875
There are two related but distinct capabilities needed to supervise a robotic swarm The first is comprehension of the swarm 's state and the second is prediction of the effects of human inputs on the swarm 's behavior	background	2K_dev_1875
Both of them are very challenging	background	2K_dev_1875
Prior work in the literature has shown that inserting the human input as soon as possible to divert the swarm from its original goal towards the new goal does not always result in optimal performance ( measured by some criterion such as the total time required by the swarm to reach the second goal )	background	2K_dev_1875
This phenomenon has been called Neglect Benevolence	background	2K_dev_1875
conveying the idea that in many cases it is preferable to neglect the swarm for some time before inserting human input	background	2K_dev_1875
Our results show that humans can learn to approximate optimal timing and that displays which make consensus variables perceptually accessible can enhance performance	finding	2K_dev_1875
We developed the swarm configuration shape-changing Neglect Benevolence Task as a Human Swarm Interaction ( HSI ) reference task	mechanism	2K_dev_1875
In this paper we study how humans can develop an understanding of swarm dynamics so they can predict the effects of the timing of their input on the state and performance of the swarm allowing comparison between human and optimal input timing performance in control of swarms	purpose	2K_dev_1875
We demonstrate the ability to solve more rearrangement by pushing tasks than existing primitive based solutions	finding	2K_dev_1889
Finally we show the plans we generate are feasible for execution on a real robot	finding	2K_dev_1889
We present a randomized kinodynamic planner We embed a physics model into the planner to allow reasoning about interaction with objects in the environment By carefully selecting this model	mechanism	2K_dev_1889
we are able to reduce our state and action space	mechanism	2K_dev_1889
gaining tractability in the search The result is a planner capable of generating trajectories for full arm manipulation and simultaneous object interaction	mechanism	2K_dev_1889
that solves rearrangement planning problems	purpose	2K_dev_1889
and show that on a variety of environments we can achieve a higher planning success rate given a restricted time budget for planning	finding	2K_dev_1890
In this work we present a fast kinodynamic RRT-planner that uses dynamic nonprehensile actions In contrast to many previous works	mechanism	2K_dev_1890
the presented planner is not restricted to quasi-static interactions and monotonicity	mechanism	2K_dev_1890
Instead the results of dynamic robot actions are predicted using a black box physics model	mechanism	2K_dev_1890
Given a general set of primitive actions and a physics model	mechanism	2K_dev_1890
the planner randomly explores the configuration space of the environment to find a sequence of actions that transform the environment into some goal configuration	mechanism	2K_dev_1890
In contrast to a naive kinodynamic RRT-planner we show that we can exploit the physical fact that in an environment with friction any object eventually comes to rest	mechanism	2K_dev_1890
This allows a search on the configuration space rather than the state space	mechanism	2K_dev_1890
reducing the dimension of the search space by a factor of two without restricting us to non-dynamic interactions	mechanism	2K_dev_1890
We compare our algorithm against a naive kinodynamic RRT-planner	method	2K_dev_1890
to rearrange cluttered environments	purpose	2K_dev_1890
We present an execution monitoring framework In our approach	mechanism	2K_dev_1891
plans are initially based on a model of the world that is only as faithful as computational and algorithmic limitations allow Through experience	mechanism	2K_dev_1891
the monitor discovers previously unmodeled modes of the world	mechanism	2K_dev_1891
defined as regions of a feature space in which the experienced outcome of a plan deviates significantly from the predicted outcome	mechanism	2K_dev_1891
The monitor may then make suggestions to change the model to match the real world more accurately	mechanism	2K_dev_1891
We demonstrate this approach on the adversarial domain of robot soccer : we monitor pass interception performance of potentially unknown opponents to try to find unforeseen modes of behavior that affect their interception performance	method	2K_dev_1891
capable of finding statistically significant discrepancies	purpose	2K_dev_1891
determining the situations in which they occur	purpose	2K_dev_1891
and making simple corrections to the world model to improve performance	purpose	2K_dev_1891
With the prevalence of social media	background	2K_dev_1895
such as Twitter short-length text like microblogs have become an important mode of text on the Internet	background	2K_dev_1895
In contrast to other forms of media	background	2K_dev_1895
such as newspaper the text in these social media posts usually contains fewer words	background	2K_dev_1895
and is concentrated on a much narrower selection of topics	background	2K_dev_1895
For these reasons traditional LDA-based sentiment and topic modeling techniques generally do not work well in case of social media data	background	2K_dev_1895
Another characteristic feature of this data is the use of special meta tokens	background	2K_dev_1895
such as hashtags which contain unique semantic meanings that are not captured by other ordinary words	background	2K_dev_1895
In this paper we propose probabilistic graphical models We first propose MTM ( Microblog Topic Model )	mechanism	2K_dev_1895
a generative model that assumes each social media post generates from a single topic	mechanism	2K_dev_1895
and models both words and hashtags separately	mechanism	2K_dev_1895
We then propose MSTM ( Microblog Sentiment Topic Model )	mechanism	2K_dev_1895
an extension of MTM	mechanism	2K_dev_1895
which also embodies the sentiment associated with the topics	mechanism	2K_dev_1895
We evaluated our models using Twitter dataset	method	2K_dev_1895
In the recent years	purpose	2K_dev_1895
many topic modeling techniques have been proposed for social media data	purpose	2K_dev_1895
but the majority of this work does not take into account the specialty of tokens	purpose	2K_dev_1895
such as hashtags and treats them as ordinary words	purpose	2K_dev_1895
to address the problem of discovering latent topics and their sentiment from social media data	purpose	2K_dev_1895
mainly microblogs like Twitter	purpose	2K_dev_1895
Our work is motivated by the potential impact of realistic simulators on the development cycle of software for real robots	background	2K_dev_1899
Unlike calibration where the goal is to identify and remove error from a signal	background	2K_dev_1899
The case is made for building models from approximate state information	mechanism	2K_dev_1899
relieving the burden of ground truth	mechanism	2K_dev_1899
Instead of physically modeling sensor behavior	mechanism	2K_dev_1899
a data-driven approach is taken	mechanism	2K_dev_1899
The implementation of our approach to simulate a simple but noisy laser rangefinder is described	method	2K_dev_1899
Finally approaches to validate the simulator are discussed	method	2K_dev_1899
We compare not only raw sensor predictions	method	2K_dev_1899
but also overall performance of algorithms on simulated versus real data	method	2K_dev_1899
We study the problem of building a sensor model for the purpose of simulation our aim to reproduce the signal in its entirety	purpose	2K_dev_1899
including its error properties	purpose	2K_dev_1899
One example is server-side scheduling for video service	background	2K_dev_1902
where clients request flows of content from a server with limited capacity	background	2K_dev_1902
and any content not delivered by its deadline is lost State-of-the-art policies	background	2K_dev_1902
like Discriminatory Processor Sharing and Weighted Fair Queueing	background	2K_dev_1902
use a fixed static proportional allocation of service rate and fail to achieve both goals	background	2K_dev_1902
The well-known Earliest Deadline First policy minimizes overall loss	background	2K_dev_1902
but fails to provide proportional loss across flows	background	2K_dev_1902
because it treats packets as independent jobs	background	2K_dev_1902
We prove that all policies in this broad class minimize overall loss Furthermore	finding	2K_dev_1902
we demonstrate that many EPDF policies accurately differentiate loss fractions in proportion to class weights	finding	2K_dev_1902
satisfying the second goal	finding	2K_dev_1902
This paper introduces the Earliest Progressive Deadline First ( EPDF ) class of policies	mechanism	2K_dev_1902
Weighted signed networks ( WSNs ) are networks in which edges are labeled with positive and negative weights	background	2K_dev_1908
WSNs can capture like/dislike	background	2K_dev_1908
trust/distrust and other social relationships between people	background	2K_dev_1908
We propose two novel measures of node behavior : the goodness of a node intuitively captures how much this node is liked/trusted by other nodes	mechanism	2K_dev_1908
while the fairness of a node captures how fair the node is in rating other nodes ' likeability or trust level We provide axioms that these two notions need to satisfy and show that past work does not meet these requirements for WSNs	mechanism	2K_dev_1908
We provide a mutually recursive definition of these two concepts and prove that they converge to a unique solution in linear time	mechanism	2K_dev_1908
We use the two measures to predict the edge weight in WSNs	mechanism	2K_dev_1908
that when compared against several individual algorithms from both the signed and unsigned social network literature We then use these as features in different multiple regression models	method	2K_dev_1908
In this paper we consider the problem of predicting the weights of edges in such networks	purpose	2K_dev_1908
The collective buys energy as a group through a central coordinator who also decides about the storage and usage of renewable energy produced by the collective	background	2K_dev_1909
Minimizing the cost is not only of interest to the consumers but is also socially desirable because it reduces the consumption at times of peak demand	background	2K_dev_1909
We prove that our algorithm converges	finding	2K_dev_1909
and it achieves the optimal solution We also present simulation results to quantify the performance of our algorithm	finding	2K_dev_1909
We develop an iterative coordination algorithm in which the coordinator makes the storage decision and shapes the demands of the consumers by designing a virtual price signal for the agents	mechanism	2K_dev_1909
based on real world consumption data	method	2K_dev_1909
In this paper we focus on demand side management in consumer collectives with community owned renewable energy generation and storage facilities for effective integration of renewable energy with the existing fossil fuel-based power supply system	purpose	2K_dev_1909
Our objective is to design coordination algorithms to minimize the cost of electricity consumption of the consumer collective while allowing the consumers to make their own consumption decisions based on their private consumption constraints and preferences	purpose	2K_dev_1909
To be able to provide better support for collaborative learning in Intelligent Tutoring Systems	background	2K_dev_1911
it is important to understand how collaboration patterns change Although interactive talk is often held as a gold standard in collaboration	background	2K_dev_1911
as students become more proficient	background	2K_dev_1911
it may not be as important	background	2K_dev_1911
We found that over time	finding	2K_dev_1911
the frequency of interactive talk and errors both decrease in dyads working together on conceptual problems	finding	2K_dev_1911
Prior work has looked at the interdependencies between utterances and the change of dialogue over time	purpose	2K_dev_1911
but it has not addressed how dialogue changes during a lesson	purpose	2K_dev_1911
an analysis that allows us to investigate the adaptivity of student strategies as students gain domain knowledge	purpose	2K_dev_1911
Collaborative and individual learning appear to have complementary strengths ; however	background	2K_dev_1913
the best way to combine these learning methods is still unclear	background	2K_dev_1913
While previous work has demonstrated the effectiveness of Intelligent Tutoring Systems ( ITSs ) for individual learning	background	2K_dev_1913
collaborative learning with ITSs is much less frequent - especially for young students	background	2K_dev_1913
In addition we propose future research to understand how to best combine individual and collaborative learning within an ITS	background	2K_dev_1913
Our previous findings demonstrate that ITSs are able to support collaboration	finding	2K_dev_1913
as well as individual learning	finding	2K_dev_1913
In this paper we discuss our prior and future work with elementary school students that aims to investigate how to best combine individual and collaborative learning using their complementary strengths within an ITS	purpose	2K_dev_1913
Computer vision is increasingly becoming interested in the rapid estimation of object detectors	background	2K_dev_1919
The canonical strategy of using Hard Negative Mining to train a Support Vector Machine is slow	background	2K_dev_1919
since the large negative set must be traversed at least once per detector	background	2K_dev_1919
Recent work has demonstrated that	background	2K_dev_1919
with an assumption of signal stationarity	background	2K_dev_1919
Linear Discriminant Analysis is able to learn comparable detectors without ever revisiting the negative set	background	2K_dev_1919
Even with this insight	background	2K_dev_1919
the time to learn a detector can still be on the order of minutes	background	2K_dev_1919
Correlation filters on the other hand	background	2K_dev_1919
can produce a detector in under a second However	background	2K_dev_1919
this involves the unnatural assumption that the statistics are periodic	background	2K_dev_1919
and requires the negative set to be re-sampled per detector size	background	2K_dev_1919
These two methods differ chiefly in the structure which they impose on the covariance matrix of all examples	background	2K_dev_1919
verified that periodicity is detrimental	finding	2K_dev_1919
a comparative study It is experimentally	method	2K_dev_1919
This paper is ( i ) to assume periodic statistics without needing to revisit the negative set and ( ii ) to accelerate the estimation of detectors with aperiodic statistics	purpose	2K_dev_1919
to improve the automatic detection of events in short sentences when in the presence of a large number of event classes	purpose	2K_dev_1925
Problems of this nature arise in formal verification of continuous and hybrid dynamical systems	background	2K_dev_1926
where there is an increasing need for methods to expedite formal proofs	background	2K_dev_1926
The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting	finding	2K_dev_1926
We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of heterogeneous benchmarks	method	2K_dev_1926
This paper presents a theoretical and experimental comparison of sound proof rules for proving invariance of algebraic sets	purpose	2K_dev_1926
that is sets satisfying polynomial equalities	purpose	2K_dev_1926
under the flow of polynomial ordinary differential equations	purpose	2K_dev_1926
To date the study of dispatching or load balancing in server farms has primarily focused on the minimization of response time	background	2K_dev_1933
Server farms are typically modeled by a front-end router that employs a dispatching policy to route jobs to one of several servers	background	2K_dev_1933
with each server scheduling all the jobs in its queue via Processor-Sharing	background	2K_dev_1933
we are able to deduce many unexpected results regarding dispatching	finding	2K_dev_1933
: we model each arrival as having a randomly distributed value parameter	mechanism	2K_dev_1933
independent of the arrival 's service requirement ( job size )	mechanism	2K_dev_1933
Given such value heterogeneity	mechanism	2K_dev_1933
the correct metric is no longer the minimization or response time	mechanism	2K_dev_1933
but rather the minimization of value-weighted response time	mechanism	2K_dev_1933
{ '' } We propose a number of new dispatching policies that are motivated by the goal of minimizing the value-weighted response time	mechanism	2K_dev_1933
Via a combination of exact analysis	method	2K_dev_1933
asymptotic analysis and simulation	method	2K_dev_1933
However the common assumption has been that all jobs are equally important or valuable	purpose	2K_dev_1933
in that they are equally sensitive to delay	purpose	2K_dev_1933
Our work departs from this assumption In this context	purpose	2K_dev_1933
we ask `` what is a good dispatching policy to minimize the value-weighted response time metric ?	purpose	2K_dev_1933
LM tends to be more expressive than other logic programming languages	finding	2K_dev_1934
LM programs are naturally concurrent illustrate its use	finding	2K_dev_1934
We have designed a new logic programming language called LM ( Linear Meld ) Our language is based on linear logic	mechanism	2K_dev_1934
an expressive logical system where logical facts can be consumed	mechanism	2K_dev_1934
Because LM integrates both classical and linear logic	mechanism	2K_dev_1934
because facts are partitioned by nodes of a graph data structure	mechanism	2K_dev_1934
Computation is performed at the node level while communication happens between connected nodes	mechanism	2K_dev_1934
through a number of examples	mechanism	2K_dev_1934
for programming graph- based algorithms in a declarative fashion	purpose	2K_dev_1934
In this paper we present the syntax and operational semantics of our language and	purpose	2K_dev_1934
Security requirements patterns represent reusable security practices that software engineers can apply to improve security in their system	background	2K_dev_1939
Reusing best practices that others have employed could have a number of benefits	background	2K_dev_1939
such as decreasing the time spent in the requirements elicitation process or improving the quality of the product by reducing product failure risk	background	2K_dev_1939
Pattern selection can be difficult due to the diversity of applicable patterns from which an analyst has to choose	background	2K_dev_1939
We propose a new method that combines an inquiry-cycle based approach with the feature diagram notation Similar to patterns themselves	mechanism	2K_dev_1939
our approach captures expert knowledge The resulting pattern hierarchies allow users to be guided through these decisions by questions	mechanism	2K_dev_1939
which introduce related patterns in order	mechanism	2K_dev_1939
thus resulting in better requirement generation	mechanism	2K_dev_1939
We evaluate our approach using access control patterns in a pattern user study	method	2K_dev_1939
The challenge is that identifying the most appropriate pattern for a situation can be cumbersome and time-consuming	purpose	2K_dev_1939
to review only relevant patterns and quickly select the most appropriate patterns for the situation	purpose	2K_dev_1939
to relate patterns based on decisions made by the pattern user	purpose	2K_dev_1939
to help the pattern user select the most appropriate patterns for their situation	purpose	2K_dev_1939
Government laws and regulations increasingly place requirements on software systems	background	2K_dev_1940
Ideally experts trained in law will analyze and interpret legal texts to inform the software requirements process	background	2K_dev_1940
However in small companies and development teams with short launch cycles	background	2K_dev_1940
individuals with little or no legal training will be responsible for compliance	background	2K_dev_1940
Two specific challenges commonly faced by non-experts are deciding if their system is covered by a law	background	2K_dev_1940
and then deciding whether two legal requirements are similar or different	background	2K_dev_1940
In so doing we discovered that legal experts achieved higher rates of consensus more frequently than technical professionals or laypersons and that all groups had slightly greater agreement when judging coverage conditions than requirements	finding	2K_dev_1940
we found that technical professionals and legal experts exhibited consistently greater agreement than that found between laypersons and legal experts	finding	2K_dev_1940
and that each group tended towards different justifications	finding	2K_dev_1940
such as laypersons and technical professionals tendency towards categorizing different coverage conditions or requirements as equivalent if they believed them to possess the same underlying intent	finding	2K_dev_1940
measured by Fleiss ' K	method	2K_dev_1940
When comparing judgments between groups using a consensus-based Cohen 's Kappa	method	2K_dev_1940
In this study we assess the ability of laypersons	purpose	2K_dev_1940
technical professionals and legal experts to judge the similarity between legal coverage conditions and requirements	purpose	2K_dev_1940
Curse of dimensionality is a practical and challenging problem in image categorization	background	2K_dev_1942
especially in cases with a large number of classes	background	2K_dev_1942
Multi-class classification encounters severe computational and storage problems when dealing with these large scale tasks	background	2K_dev_1942
further demonstrate the effectiveness of hierarchical feature hashing	finding	2K_dev_1942
In this paper we propose hierarchical feature hashing	mechanism	2K_dev_1942
We provide detailed theoretical analysis on our proposed hashing method Moreover	method	2K_dev_1942
experimental results on object recognition and scene classification	method	2K_dev_1942
to effectively reduce dimensionality of parameter space without sacrificing classification accuracy	purpose	2K_dev_1942
and at the same time exploit information in semantic taxonomy among categories	purpose	2K_dev_1942
Research shows that commonly accepted security requirements are not generally applied in practice Instead of relying on requirements checklists	background	2K_dev_1949
security experts rely on their expertise and background knowledge to identify security vulnerabilities	background	2K_dev_1949
We report our preliminary results of analyzing two interviews that reveal possible decision-making patterns that could characterize how analysts perceive	finding	2K_dev_1949
comprehend and project future threats which leads them to decide upon requirements and their specifications	finding	2K_dev_1949
in addition to how experts use assumptions to overcome ambiguity in specifications	finding	2K_dev_1949
Our goal is to build a model that researchers can use	mechanism	2K_dev_1949
we conducted a series of interviews to encode the decision-making process of security experts and novices during security requirements analysis	method	2K_dev_1949
Participants were asked to analyze two types of artifacts : source code	method	2K_dev_1949
and network diagrams for vulnerabilities and to apply a requirements checklist to mitigate some of those vulnerabilities	method	2K_dev_1949
We framed our study using Situation Awareness-a cognitive theory from psychology-to elicit responses that we later analyzed using coding theory and grounded analysis	method	2K_dev_1949
To understand the gap between available checklists and practice	purpose	2K_dev_1949
to evaluate their security requirements methods against how experts transition through different situation awareness levels in their decision-making process	purpose	2K_dev_1949
( ii ) we show that such properties can be successfully inferred from a single image ( iv ) we show that the 3D attributes trained on this dataset generalize to images of other ( non-sculpture ) object classes ; and furthermore ( v ) we show that the CNN also provides a shape embedding that can be used to match previously unseen sculptures largely independent of viewpoint	finding	2K_dev_1968
In this paper we investigate 3D attributes as a means to understand the shape of an object in a single image	purpose	2K_dev_1968
An increasing number of mobile devices are capable of automatically sensing and recording rich information about the surrounding environment	background	2K_dev_1971
Spatial locations of such data can help to better learn about the environment	background	2K_dev_1971
We will show robustness the ability to propose coarse sensor noise errors	finding	2K_dev_1971
We focus on devices equipped with odometry sensors that capture changes in motion	mechanism	2K_dev_1971
Odometry suffers from cumulative errors of dead reckoning but it captures the relative shape of the traversed path well	mechanism	2K_dev_1971
Our approach will correct such errors by matching the shape of the trajectory from odometry to traversable paths of a known map	mechanism	2K_dev_1971
Our algorithm is inspired by prior vehicular GPS map matching techniques that snap global GPS measurements to known roads	mechanism	2K_dev_1971
We similarly wish to snap the trajectory from odometry to known hallways	mechanism	2K_dev_1971
Several modifications are required to ensure these techniques are robust when given relative measurements from odometry	mechanism	2K_dev_1971
If we assume an office-like environment with only straight hallways	mechanism	2K_dev_1971
then a significant rotation indicates a transition to another hallway	mechanism	2K_dev_1971
As a result we partition the trajectory into line segments based on significant turns	mechanism	2K_dev_1971
Each trajectory segment is snapped to a corresponding hallway that best maintains the shape of the original trajectory	mechanism	2K_dev_1971
These snapping decisions are made based on the similarity of the two curves as well as the rotation to transition between hallways	mechanism	2K_dev_1971
under different types of noise in complex environments and	method	2K_dev_1971
In this work we address the problem of identifying the locations visited by a mobile device as it moves within an indoor environment	purpose	2K_dev_1971
the proposed behavioral planning architecture improves the driving quality considerably	finding	2K_dev_1972
3\ % reduction of required computation time in representative scenarios	finding	2K_dev_1972
In this paper we propose a novel planning framework A reference planning layer first generates kinematically and dynamically feasible paths assuming no obstacles on the road	mechanism	2K_dev_1972
then a behavioral planning layer takes static and dynamic obstacles into account	mechanism	2K_dev_1972
Instead of directly commanding a desired trajectory	mechanism	2K_dev_1972
it searches for the best directives for the controller	mechanism	2K_dev_1972
such as lateral bias and distance keeping aggressiveness	mechanism	2K_dev_1972
It also considers the social cooperation between the autonomous vehicle and surrounding cars	mechanism	2K_dev_1972
Based on experimental results from both simulation and a real autonomous vehicle platform	method	2K_dev_1972
that can greatly improve the level of intelligence and driving quality of autonomous vehicles	purpose	2K_dev_1972
Illumination defocus and global illumination effects are major challenges for active illumination scene recovery algorithms	background	2K_dev_1975
We demonstrate the effectiveness of our approach	finding	2K_dev_1975
In this paper we develop an algorithm for scene recovery in the presence of both defocus and global light transport effects such as interreflections and sub-surface scattering	mechanism	2K_dev_1975
Our method extends the working volume by using structured light patterns at multiple projector focus settings	mechanism	2K_dev_1975
A careful characterization of projector blur allows us to decode even partially out-of-focus patterns This enables our algorithm to recover scene shape and the direct and global illumination components over a large depth of field while still using a relatively small number of images ( typically 25-30 )	mechanism	2K_dev_1975
by recovering high quality depth maps of scenes containing objects made of optically challenging materials such as wax	method	2K_dev_1975
marble soap colored glass and translucent plastic	method	2K_dev_1975
Illumination defocus limits the working volume of projector-camera systems and global illumination can induce large errors in shape estimates	purpose	2K_dev_1975
Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D point triangulation to reconstruct a static scene	background	2K_dev_1976
Because the videos are aligned with sub-frame precision	finding	2K_dev_1976
we reconstruct 3D trajectories of unconstrained outdoor activities at much higher temporal resolution than the input videos	finding	2K_dev_1976
In this paper we present a spatiotemporal bundle adjustment approach Key to our joint optimization is the careful integration of physics-based motion priors within the reconstruction pipeline	mechanism	2K_dev_1976
validated on a large motion capture corpus	mechanism	2K_dev_1976
We present an end-to-end pipeline that takes multiple uncalibrated and unsynchronized video streams and produces a dynamic reconstruction of the event	mechanism	2K_dev_1976
The triangulation constraint however is invalid for moving points captured in multiple unsynchronized videos and bundle adjustment is not purposed to estimate the temporal alignment between cameras that jointly optimizes four coupled sub-problems : estimating camera intrinsics and extrinsics	purpose	2K_dev_1976
triangulating 3D static points	purpose	2K_dev_1976
as well as subframe temporal alignment between cameras and estimating 3D trajectories of dynamic points	purpose	2K_dev_1976
Turbulence is studied extensively in remote sensing	background	2K_dev_1977
astronomy meteorology aerodynamics and fluid dynamics	background	2K_dev_1977
The strength of turbulence is a statistical measure of local variations in the turbulent medium	background	2K_dev_1977
It influences engineering decisions made in these domains	background	2K_dev_1977
Turbulence strength ( TS ) also affects safety of aircraft and tethered balloons	background	2K_dev_1977
and reliability of free-space electromagnetic relays	background	2K_dev_1977
using videos captured from different viewpoints	mechanism	2K_dev_1977
We formulate this as a linear tomography problem with a structure unique to turbulence fields	mechanism	2K_dev_1977
No tight synchronization between cameras is needed Thus	mechanism	2K_dev_1977
realization is very simple to deploy using consumer-grade cameras	mechanism	2K_dev_1977
We experimentally demonstrate this both in a lab and in a large-scale uncontrolled complex outdoor environment	method	2K_dev_1977
which includes industrial rural and urban areas	method	2K_dev_1977
We show that it is possible to estimate TS	purpose	2K_dev_1977
without having to reconstruct instantaneous fluid flow fields	purpose	2K_dev_1977
Instead the TS field can be directly recovered	purpose	2K_dev_1977
experiments show that our method achieves good performance for parsing human motions Furthermore	finding	2K_dev_1980
we found that our method achieves better performance by using unlabeled video than adding more labeled pose images into the training set	finding	2K_dev_1980
In this paper we propose a method We use the training samples from a public image pose dataset to avoid the tediousness of labeling video streams There are two main problems confronted	mechanism	2K_dev_1980
First the distribution of images and videos are different	mechanism	2K_dev_1980
Second no temporal information is available in the training images	mechanism	2K_dev_1980
To smooth the inconsistency between the labeled images and unlabeled videos	mechanism	2K_dev_1980
our algorithm iteratively incorporates the pose knowledge harvested from the testing videos into the image pose detector via an adjust-and-refine method	mechanism	2K_dev_1980
During this process continuity and tracking constraints are imposed to leverage the spatio-temporal information only available in videos	mechanism	2K_dev_1980
For our experiments we have collected two datasets from YouTube and	method	2K_dev_1980
to parse human motion in unconstrained Internet videos without labeling any videos for training	purpose	2K_dev_1980
We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces	finding	2K_dev_1983
In this work we present a method We propose the use of midlevel constraints for 3D scene understanding in the form of convex and concave edges and introduce a generic framework capable of incorporating these and other constraints Our method takes a variety of cues and uses them to infer a consistent interpretation of the scene	mechanism	2K_dev_1983
for single-view reasoning about 3D surfaces and their relationships	purpose	2K_dev_1983
Collaborative learning has been shown to be beneficial for older students	background	2K_dev_1989
but there has not been much research to show if these results transfer to elementary school students	background	2K_dev_1989
In addition collaborative and individual modes of instruction may be better for acquiring different types of knowledge	background	2K_dev_1989
Collaborative Intelligent Tutoring Systems ( ITS ) provide a platform that may be able to provide both the cognitive and collaborative support that students need This work indicates that by embedding collaboration scripts in ITSs	background	2K_dev_1989
collaborative learning can be an effective instructional method even with young children	background	2K_dev_1989
The collaborative groups had the same learning gains as the individual groups in both the procedural and conceptual learning conditions but were able to do so with fewer problems	finding	2K_dev_1989
This paper presents a study comparing collaborative and individual methods while receiving instruction on either procedural or conceptual knowledge	purpose	2K_dev_1989
Spoken language interfaces are being incorporated into various devices ( e	background	2K_dev_2011
smart-phones smart TVs etc )	background	2K_dev_2011
We propose to dynamically add application-based domains according to users ' requests by using descriptions of applications as a retrieval cue to find relevant applications The approach uses structured knowledge resources ( e	mechanism	2K_dev_2011
Freebase Wikipedia FrameNet ) to induce types of slots for generating semantic seeds	mechanism	2K_dev_2011
and enriches the semantics of spoken queries with neural word embeddings	mechanism	2K_dev_2011
where semantically related concepts can be additionally included for acquiring knowledge that does not exist in the predefined domains	mechanism	2K_dev_2011
The system can then retrieve relevant applications or dynamically suggest users install applications that support unexplored domains	mechanism	2K_dev_2011
We find that vendor descriptions provide a reliable source of information for this purpose	mechanism	2K_dev_2011
However current technology typically limits conversational interactions to a few narrow predefined domains/topics	purpose	2K_dev_2011
For example dialogue systems for smart-phone operation fail to respond when users ask for functions not supported by currently installed applications	purpose	2K_dev_2011
Silhouettes provide rich information on three-dimensional shape	background	2K_dev_2029
since the intersection of the associated visual cones generates the `` visual hull { '' }	background	2K_dev_2029
which encloses and approximates the original shape However	background	2K_dev_2029
not all silhouettes can actually be projections of the same object in space : this simple observation has implications in object recognition and multi-view segmentation	background	2K_dev_2029
and has been ( often implicitly ) used as a basis for camera calibration	background	2K_dev_2029
and point out some possible directions for future research	background	2K_dev_2029
After discussing some general results	finding	2K_dev_2029
we present a `` dual { '' } formulation for consistency	mechanism	2K_dev_2029
that gives conditions for a family of planar sets to be sections of the same object	mechanism	2K_dev_2029
Finally we introduce a more general notion of silhouette `` compatibility { '' } under partial knowledge of the camera projections	mechanism	2K_dev_2029
We present this notion as a natural generalization of traditional multi-view geometry	method	2K_dev_2029
which deals with consistency for points	method	2K_dev_2029
In this paper we investigate the conditions for multiple silhouettes	purpose	2K_dev_2029
or more generally arbitrary closed image sets	purpose	2K_dev_2029
to be geometrically `` consistent { '' }	purpose	2K_dev_2029
Large-scale information processing systems are able to extract massive collections of interrelated facts	background	2K_dev_2031
We show that compared to existing methods	finding	2K_dev_2031
our approach is able to achieve improved AUC and F1 with significantly lower running time	finding	2K_dev_2031
can be transformed into a knowledge graph The extractions form an extraction graph and we refer to the task of removing noise	mechanism	2K_dev_2031
inferring missing information and determining which candidate facts should be included into a knowledge graph as knowledge graph identification In order to perform this task	mechanism	2K_dev_2031
we must reason jointly about candidate facts and their associated extraction confidences	mechanism	2K_dev_2031
identify co-referent entities and incorporate ontological constraints	mechanism	2K_dev_2031
Our proposed approach uses probabilistic soft logic ( PSL )	mechanism	2K_dev_2031
a recently introduced probabilistic modeling framework which easily scales to millions of facts	mechanism	2K_dev_2031
We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL project containing over 1M extractions and 70K ontological relations	method	2K_dev_2031
but unfortunately transforming these candidate facts into useful knowledge is a formidable challenge	purpose	2K_dev_2031
In this paper we show how uncertain extractions about entities and their relations	purpose	2K_dev_2031
Occlusions are common in real world scenes and are a major obstacle to robust object detection	background	2K_dev_2032
Previous approaches primarily enforced local coherency or learned the occlusion structure from data	background	2K_dev_2032
However local coherency ignores the occlusion structure in real world scenes and learning from data requires tediously labeling many examples of occlusions for every view of every object	background	2K_dev_2032
Other approaches require binary classifications of matching scores	background	2K_dev_2032
Our method demonstrates significant improvement in estimating the mask of the occluding region and improves object instance detection on a challenging dataset of objects under severe occlusions	finding	2K_dev_2032
In this paper we present a method We address these limitations by formulating occlusion reasoning as an efficient search over occluding blocks which best explain a probabilistic matching pattern	mechanism	2K_dev_2032
to coherently reason about occlusions on many types of detectors	purpose	2K_dev_2032
Binary codes that are binarizations of features represented by real numbers have recently been used in the object recognition field	background	2K_dev_2033
in order to achieve reduced memory and robustness with respect to noise	background	2K_dev_2033
From the results of we confirmed that the proposed method enables an increase in detection performance while maintaining the same levels of memory and computing costs as those for previous methods of binarizing features	finding	2K_dev_2033
With this study we introduce a transition likelihood model into classifiers This enables classifications that consider transitions to the desired binary code	mechanism	2K_dev_2033
even if the observed binary code differs from the actually desired binary code for some reason	mechanism	2K_dev_2033
However binarizing features represented by real numbers has a problem in that a great deal of the information within the features drops out	purpose	2K_dev_2033
That is why we focus on quantization residual	purpose	2K_dev_2033
which is information that drops out when features are binarized in order to take into consideration the possibility that a binary code which has been observed from an image will transition to another binary code	purpose	2K_dev_2033
`` Socially cooperative driving { '' } is an integral part of our everyday driving	background	2K_dev_2034
Compared with approaches that do not take social behavior into account	finding	2K_dev_2034
the iPCB algorithm shows a 41	finding	2K_dev_2034
7\ % performance improvement based on the chosen cost functions	finding	2K_dev_2034
In this paper an intention-integrated Prediction-and Cost function-Based algorithm iPCB ) framework is proposed An intention estimator is developed Then for each candidate strategy	mechanism	2K_dev_2034
a prediction engine considering the interaction between host and surrounding agents is used A cost function-based evaluation is applied	mechanism	2K_dev_2034
requiring special attention to imbue the autonomous driving with a more natural driving behavior	purpose	2K_dev_2034
to enable an autonomous vehicle to perform cooperative social behavior to extract the probability of surrounding agents ' intentions in real time	purpose	2K_dev_2034
to predict future scenarios to compute the cost for each scenario and select the decision corresponding to the lowest cost	purpose	2K_dev_2034
Smartphones are now targets of malicious viruses Furthermore	background	2K_dev_2040
the increasing `` connectedness { '' } of smartphones has resulted in new delivery vectors for malicious viruses	background	2K_dev_2040
including proximity- social- and other technology-based methods	background	2K_dev_2040
In fact Cabir and CommWarrior are two viruses-observed in the wild-that spread	background	2K_dev_2040
at least in part	background	2K_dev_2040
using proximity-based techniques ( line-of-sight bluetooth radio )	background	2K_dev_2040
find that the first eigenvalue of the system matrices lambda ( S1 )	finding	2K_dev_2040
lambda ( S2 ) of the two networks ( static and dynamic networks ) appropriately captures the competitive interplay between two viruses and effectively predicts the competition 's `` winner { '' }	finding	2K_dev_2040
which provides a feasible way to defend against smartphone viruses	finding	2K_dev_2040
In this paper we propose and evaluate SI1I2S	mechanism	2K_dev_2040
a competition model To approximate dynamic network behavior	mechanism	2K_dev_2040
we use classic mobility models from ad hoc networking	mechanism	2K_dev_2040
Random Waypoint Random Walk and Levy Flight	mechanism	2K_dev_2040
We analyze our model using techniques from dynamic systems and	method	2K_dev_2040
that describes the spread of two mutually exclusive viruses across heterogeneous composite networks	purpose	2K_dev_2040
one static ( social connections ) and one dynamic ( mobility pattern )	purpose	2K_dev_2040
A robotic swarm is a decentralized group of robots which overcome failure of individual robots with robust emergent behaviors based on local interactions	background	2K_dev_2044
These behaviors are not well built for accomplishing complex tasks	background	2K_dev_2044
however because of the changing assumptions required in various applications and environments	background	2K_dev_2044
A new movement in the research field is to add human input to influence the swarm in order to help make the robots goal directed and overcome these problems Previous studies have all used visual feedback through a computer interface to give the user the swarm state information Researchers in multi-robot systems have shown benefits of haptic feedback in obstacle navigation before	background	2K_dev_2044
The study shows the benefits of the additional feedback in a target searching class	finding	2K_dev_2044
In most environments operators were able to cover significantly more area	finding	2K_dev_2044
increasing the chance of finding more targets	finding	2K_dev_2044
The other environment found no significant difference	finding	2K_dev_2044
showing that the haptic feedback does not degrade performance in any of the tested environments	finding	2K_dev_2044
This supports our hypothesis that haptic feedback is useful in HSI and requires further research to maximize its potential	finding	2K_dev_2044
This study adapted swarm control algorithms but this study is a novel method because of the decentralized formation of the robotic swarm	mechanism	2K_dev_2044
This research in Human Swarm Interaction ( HSI ) focuses on different control laws and ways to integrate the human intent with local control laws of the robots	purpose	2K_dev_2044
to give the operator haptic feedback as well as visual feedback	purpose	2K_dev_2044
Previous studies have examined the characteristics of physiological tremor under laboratory settings as well as different operating conditions	background	2K_dev_2046
However different test methods make the comparison of results across trials and conditions difficult	background	2K_dev_2046
Two vitroretinal microsurgeons were evaluated while performing a pointing task with no entry-point constraint	method	2K_dev_2046
constrained by an artificial eye model	method	2K_dev_2046
and constrained by a rabbit eye in vivo For the three respective conditions A spectral analysis was also performed	method	2K_dev_2046
This paper presents the characterization and comparison of physiological tremor for pointing tasks in multiple environments	purpose	2K_dev_2046
as a baseline for performance evaluation of microsurgical robotics	purpose	2K_dev_2046
Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D	background	2K_dev_2047
Manual needle insertion allows the clinician to control the insertion speed	background	2K_dev_2047
demonstrate the performance of the proposed controller	finding	2K_dev_2047
show the feasibility of this technique in 2D and 3D environments	finding	2K_dev_2047
This paper presents a control law A look-ahead proportional controller for position and orientation is presented	mechanism	2K_dev_2047
The look-ahead distance is a linear function of insertion speed	mechanism	2K_dev_2047
Simulations in a 3D brain-like environment Experimental results also	method	2K_dev_2047
for automatic 3D steering of manually inserted flexible needles	purpose	2K_dev_2047
Pose Machines provide a sequential prediction framework for learning rich implicit spatial models	background	2K_dev_2063
We demonstrate state-of-the-art performance and outperform competing methods	finding	2K_dev_2063
In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation	mechanism	2K_dev_2063
We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages	mechanism	2K_dev_2063
producing increasingly refined estimates for part locations	mechanism	2K_dev_2063
without the need for explicit graphical model-style inference	mechanism	2K_dev_2063
Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision	mechanism	2K_dev_2063
thereby replenishing back-propagated gradients and conditioning the learning procedure	mechanism	2K_dev_2063
on standard benchmarks including the MPII	method	2K_dev_2063
LSP and FLIC datasets	method	2K_dev_2063
for the task of pose estimation	purpose	2K_dev_2063
Finding meaningful structured representations of 3D point cloud data ( PCD ) has become a core task for spatial perception applications	background	2K_dev_2064
our tests showing favorable performance when compared to octree and NDT-based methods	finding	2K_dev_2064
In this paper we introduce a method As opposed to deterministic structures such as voxel grids or octrees	mechanism	2K_dev_2064
we propose probabilistic subdivisions of the data through local mixture modeling	mechanism	2K_dev_2064
and show how these subdivisions can provide a maximum likelihood segmentation of the data	mechanism	2K_dev_2064
The final representation is hierarchical	mechanism	2K_dev_2064
compact parametric and statistically derived	mechanism	2K_dev_2064
facilitating run-time occupancy calculations through stochastic sampling	mechanism	2K_dev_2064
Unlike traditional deterministic spatial subdivision methods	mechanism	2K_dev_2064
our technique enables dynamic creation of voxel grids according the application 's best needs	mechanism	2K_dev_2064
In contrast to other generative models for PCD	mechanism	2K_dev_2064
we explicitly enforce sparsity among points and mixtures	mechanism	2K_dev_2064
a technique which we call expectation sparsification	mechanism	2K_dev_2064
This leads to a highly parallel hierarchical Expectation Maximization ( EM ) algorithm well-suited for the GPU and real-time execution	mechanism	2K_dev_2064
We explore the trade-offs between model fidelity and model size at various levels of detail	method	2K_dev_2064
for constructing compact generative representations of PCD at multiple levels of detail	purpose	2K_dev_2064
Sudden weight gain in patients living with Congestive Heart Failure ( CHF ) is often an indication that the individual is retaining fluid	background	2K_dev_2066
which often means that patient 's heart has weakened leading to increased risk of kidney or cardiac failure	background	2K_dev_2066
leading to the possibility of earlier clinical interventions	background	2K_dev_2066
potentially preventing deadly medical emergencies	background	2K_dev_2066
In this work we present a latent variable autoregression model that tracks patient weight and blood pressure over time We are also able to model continuous heart-rate signals and evaluate a subject 's response to physical activity	mechanism	2K_dev_2066
This allows us to detect signs of health decline days earlier than existing rule-based systems	mechanism	2K_dev_2066
Clinical interventions can be made at this stage	purpose	2K_dev_2066
leading to better outcomes	purpose	2K_dev_2066
however it is essential that the interventions take place before the patient 's health declines too drastically	purpose	2K_dev_2066
allowing us to predict weight values into the future	purpose	2K_dev_2066
Human communication literature states that people with different culture backgrounds act differently in conversations	background	2K_dev_2069
We found that users from different culture context express engagement differently	finding	2K_dev_2069
We implemented two versions of a virtual agent targeting American and Chinese cultures	method	2K_dev_2069
Currently most virtual agents are designed for a single targeted popular culture	purpose	2K_dev_2069
