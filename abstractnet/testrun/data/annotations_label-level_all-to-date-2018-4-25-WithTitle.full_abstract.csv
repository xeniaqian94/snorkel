2K_dev_0	Online communities have the potential to be supportive, cruel, or anywhere in between . The development of positive norms for interaction can help users build bonds, grow, and learn . Using millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors . Consistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users . Proactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors . This work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .
2K_dev_1	Recent research has demonstrated that ( a ) groups can be characterized by a collective intelligence ( CI ) factor that measures their ability to perform together on a wide range of different tasks, and ( b ) this factor can predict groups ' performance on other tasks in the future . The current study examines whether these results translate into the world of teams in competitive online video games where self-organized, time-pressured, and intense collaboration occurs purely online . In this study of teams playing the online game League of Legends, we find that CI does, indeed, predict the competitive performance of teams controlling for the amount of time played as a team . We also find that CI is positively correlated with the presence of a female team member and with the team members ' average social perceptiveness . Finally, unlike in prior studies, tacit coordination in this setting plays a larger role than verbal communication .
2K_dev_2	Forming work teams involves matching people with complementary skills and personalities, but requires obtaining such data a priori . We introduce team dating, where people interact on brief tasks before working with a dedicated partner for longer, more complex tasks . We studied team dating through two online experiments . In Experiment 1, workers from a crowd platform independently wrote an ad slogan, discussed it with three consecutive people and evaluated their team date interactions . They then selected preferred teammates from a list showing average ratings for people they had dated and not dated . Results show that participants evaluated their dates based on evidence beyond externally judged slogan quality, and relied heavily on their dyad-specific judgments in selecting teammates . In Experiment 2, we replicated the individual and team dating tasks, and formed teams, either i ) by honoring pairwise team dating preferences, ii ) randomly from their pool of dates, or iii ) randomly from those not dated . Results show that teams formed from preferred dates performed better on a final creative task compared to random dates or non-dates . Team dating provides a dynamic technique for forming ad hoc teams accounting for interpersonal dynamics . The initial interactions provided information that helped people select and work with an appropriate teammate .
2K_dev_3	Collective intelligence ( CI ) , a group 's capacity to perform a wide variety of tasks, is a key factor in successful collaboration . Group composition, particularly diversity and member social perceptiveness, are consistent predictors of CI, but we have limited knowledge about the mechanisms underlying their effects . To address this gap, we examine how physiological synchrony, as an indicator of coordination and rapport, relates to CI in computer-mediated teams, and if synchrony might serve as a mechanism explaining the effect of group composition on CI . We present results from a laboratory experiment where 60 dyads completed the Test of Collective Intelligence ( TCI ) together online and rated their group satisfaction, while wearing physiological sensors . We find that synchrony in facial expressions ( indicative of shared experience ) was associated with CI and synchrony in skin conductance ( indicative of shared arousal ) with group satisfaction . Furthermore, various forms of synchrony mediated the effect of member diversity and social perceptiveness on CI and group satisfaction . Our results have important implications for online collaborations and distributed teams .
2K_dev_4	Feedback is information that can improve task performance . Online communities, educational forums, and crowd-based feedback platforms all support feedback exchange among a more diverse set of sources than ever before, with greater control over how to moderate this exchange . In this work, we study how the power relationship between the source and receiver and the tone of language influence the recep-tivity, effort, and work performance resulting from online feedback exchange . We conducted an online experiment manipulating affective language and source of feedback on a writing task . We found that critiques with positive affec-tive language increased positive emotions and reduced participants ' annoyance and frustration, which led to an increase in work quality, compared to critiques without positive language . Feedback without positive affective language led to more edits, but not better work outcomes . Participants reacted more positively to feedback from an anonymous source than from a peer or an authority . Our findings provide design implications for platforms to support more fruitful feedback exchange .
2K_dev_5	How do individuals perceive algorithmic vs. group-made decisions ? We investigated people 's perceptions of mathematically-proven fair division algorithms making social division decisions . In our first qualitative study, about one third of the participants perceived algorithmic decisions as less than fair ( 30 % for self, 36 % for group ) , often because algorithmic assumptions about users did not account for multiple concepts of fairness or social behaviors, and the process of quantifying preferences through interfaces was prone to error . In our second experiment, algorithmic decisions were perceived to be less fair than discussion-based decisions, dependent on participants ' interpersonal power and computer programming knowledge . Our work suggests that for algorithmic mediation to be fair, algorithms and their interfaces should account for social and altruistic behaviors that may be difficult to define in mathematical terms .
2K_dev_6	Social identities carry widely agreed upon meanings, called stereotypes, that have important effects on social processes . In the present work, we develop a method to extract the stereotypes of Twitter users . Our method is grounded in two distinct strands of theory, one that represents stereotypes as identities ' affective meanings and the other that represents stereotypes as semantic relationships between identities . After validating our approach via a prediction task, we apply the model to a dataset of 45 thousand Twitter users who actively tweeted about the Michael Brown and Eric Garner tragedies . Our work provides unique insights into the stereotypes of these users, as well as providing a way of quantifying stereotypes that blends existing sociological and psychological theory in a novel, parsimonious way .
2K_dev_7	A key issue, whenever people work together to solve a complex problem, is how to divide the problem into parts done by different people and combine the parts into a solution for the whole problem . This paper presents a novel way of doing this with groups of contests called contest webs . Based on the analogy of supply chains for physical products, the method provides incentives for people to ( a ) reuse work done by themselves and others, ( b ) simultaneously explore multiple ways of combining interchangeable parts, and ( c ) work on parts of the problem where they can contribute the most . The paper also describes a field test of this method in an online community of over 50,000 people who are developing proposals for what to do about global climate change . The early results suggest that the method can, indeed, work at scale as intended .
2K_dev_8	Crowd workers are distributed and decentralized . While decentralization is designed to utilize independent judgment to promote high-quality results, it paradoxically undercuts behaviors and institutions that are critical to high-quality work . Reputation is one central example: crowdsourcing systems depend on reputation scores from decentralized workers and requesters, but these scores are notoriously inflated and uninformative . In this paper, we draw inspiration from historical worker guilds ( e.g., in the silk trade ) to design and implement crowd guilds: centralized groups of crowd workers who collectively certify each other 's quality through double-blind peer assessment . A two-week field experiment compared crowd guilds to a traditional decentralized crowd work model . Crowd guilds produced reputation signals more strongly correlated with ground-truth worker quality than signals available on current crowd working platforms, and more accurate than in the traditional model .
2K_dev_9	Social science researchers spend significant time annotating behavioral events in video data in order to quantitatively assess interactions [ 2 ] . These behavioral events may be instantaneous changes, continuous actions that span unbounded periods of time, or behaviors that would be best described by severity or other scalar ratings . The complexity of these judgments, coupled with the time and effort required to meticulously assess video, results in a training and evaluation process that can take days or weeks . Computational analysis of video data is still limited due to the challenges introduced by objective interpretation and varied contexts . Glance [ 4 ] introduced a means of leveraging human intelligence by recruiting crowds of paid online workers to accurately analyze hours of video data in a matter of minutes . This approach has been shown to expedite work in human-centered fields, as well as generate training data for automated recognition systems . In this paper, we describe an interactive demonstration of an improved, more expressive version of Glance that expands the initial set of supported annotation formats ( e.g . time range, classification, etc . ) from one to nine . Worker interfaces for each of these options are dynamically generated, along with tutorials, based on the analyst 's question . These new features allow analysts to acquire more specific information about events in video datasets .
2K_dev_10	How effective are call and SMS logs in modeling tie strength ? Frequency and duration of communication has long been cited as a major aspect of tie strength . Intuitively, this makes sense: people communicate with those that they feel close to . Highly cited research papers have pushed this idea further, using communication as a direct proxy for tie strength . However, this operationalization has not been validated . Our work evaluates this assumption . We collected call and SMS logs and ground truth relationship data from 36 participants . Consistent with theory, we found that frequent or long-duration communication likely indicates a strong tie . However, the use of call and SMS logs produced many errors in separating strong and weak ties, suggesting this approach is incomplete . Follow-up interviews indicate fundamental challenges for inferring tie strength from communication logs .
2K_dev_11	Social influence is key in technology adoption, but its role in security-feature adoption is unique and remains unclear . Here, we analyzed how three Facebook security features ' Login Approvals, Login Notifications, and Trusted Contacts-diffused through the social networks of 1.5 million people . Our results suggest that social influence affects one 's likelihood to adopt a security feature, but its effect varies based on the observability of the feature, the current feature adoption rate among a potential adopter 's friends, and the number of distinct social circles from which those feature-adopting friends originate . Curiously, there may be a threshold higher than which having more security feature adopting friends predicts for higher adoption likelihood, but below which having more feature-adopting friends predicts for lower adoption likelihood . Furthermore, the magnitude of this threshold is modulated by the attributes of a feature-features that are more noticeable ( Login Approvals, Trusted Contacts ) have lower thresholds .
2K_dev_12	Feedback is an important component of the design process, but gaining access to high-quality critique outside a classroom or firm is challenging . We present CrowdCrit, a web-based system that allows designers to receive design critiques from non-expert crowd workers . We evaluated CrowdCrit in three studies focusing on the designer 's experience and benefits of the critiques . In the first study, we compared crowd and expert critiques and found evidence that aggregated crowd critique approaches expert critique . In a second study, we found that designers who got crowd feedback perceived that it improved their design process . The third study showed that designers were enthusiastic about crowd critiques and used them to change their designs . We conclude with implications for the design of crowd feedback services .
2K_dev_13	Massive Open Online Courses ( MOOCs ) enable everyone to receive high-quality education . However, current MOOC creators can not provide an effective, economical, and scalable method to detect cheating on tests, which would be required for any certification . In this paper, we propose a Massive Open Online Proctoring ( MOOP ) framework, which combines both automatic and collaborative approaches to detect cheating behaviors in online tests . The MOOP framework consists of three major components: Automatic Cheating Detector ( ACD ) , Peer Cheating Detector ( PCD ) , and Final Review Committee ( FRC ) . ACD uses webcam video or other sensors to monitor students and automatically flag suspected cheating behavior . Ambiguous cases are then sent to the PCD, where students peer-review flagged webcam video to confirm suspicious cheating behaviors . Finally, the list of suspicious cheating behaviors is sent to the FRC to make the final punishing decision . Our experiment show that ACD and PCD can detect usage of a cheat sheet with good accuracy and can reduce the overall human resources required to monitor MOOCs for cheating .
2K_dev_14	Online communities, much like companies in the business world, often need to transfer best practices internally from one unit to another to improve their performance . Organizational scholars disagree about how much a recipient unit should modify a best practice when incorporating it . Some evidence indicates that modifying a practice that has been successful in one environment will introduce problems, undercut its effectiveness and harm the performance of the recipient unit . Other evidence, though, suggests that recipients need to adapt the practice to fit their local environment . The current research introduces a contingency perspective on practice transfer, holding that the value of modifications depends on when they are introduced and who introduces them . Empirical research on the transfer of a quality-improvement practice between projects within Wikipedia shows that modifications are more helpful if they are introduced after the receiving project has had experience with the imported practice . Furthermore, modifications are more effective if they are introduced by members who have experience in a variety of other projects .
2K_dev_15	Online crowds are a promising source of new innovations . However, crowd innovation quality does not always match its quantity . In this paper, we explore how to improve crowd innovation with real-time expert guidance . One approach would for experts to provide personalized feed-back, but this scales poorly, and may lead to premature convergence during creative work . Drawing on strategies for facilitating face-to-face brainstorms, we introduce a crowd ideation system where experts monitor incoming ideas through a dashboard and offer high-level `` inspirations '' to guide ideation . A series of controlled experiments show that experienced facilitators increased the quantity and creativity of workers ' ideas compared to unfacilitated workers, while Novice facilitators reduced workers ' creativity . Analyses of inspiration strategies suggest these opposing results stem from differential use of successful inspiration strategies ( e.g., provoking mental simulations ) . The results show that expert facilitation can significantly improve crowd innovation, but inexperienced facilitators may need scaffolding to be successful .
2K_dev_16	In this paper, we present the Group Context Framework ( GCF ) , a general-purpose toolkit that allows mobile devices to opportunistically share contextual information . GCF provides a standardized way for developers to request contextual data for their applications . The framework then intelligently groups with other devices to satisfy these requirements . Through two prototypes, we demonstrate how GCF can be used to support a broad range of collaborative and cooperative tasks . We then show how our framework 's architecture allows devices to opportunistically detect and collaborate with one another, even when running different applications . Finally, we present two real-world domains that show how GCF 's ability to form groups increases users ' access to relevant and timely information, and discuss possible incentives and safeguards to context sharing from a user standpoint .
2K_dev_17	Telepresence means business people can make deals in other countries, doctors can give remote medical advice, and soldiers can rescue someone from thousands of miles away . When interaction is mediated, people are removed from and lack context about the person they are making decisions about . In this paper, we explore the impact of technological mediation on risk and dehumanization in decision-making . We conducted a laboratory experiment involving medical treatment decisions . The results suggest that technological mediation influences decision making, but its influence depends on an individual 's self-construal: participants who saw themselves as defined through their relationships ( interdependent self-construal ) recommended riskier and more painful treatments in video conferencing than when face-to-face . We discuss implications of our results for theory and future research .
2K_dev_18	Online question and answer ( Q & A ) sites, which are platforms for users to post and answer questions on a wide range of topics, are becoming large repositories of valuable knowledge and important to societies . In order to sustain success, Q & A sites face the challenges of ensuring content quality and encouraging user contributions . This paper examines a particular design decision in Q & A sites-allowing Wikipedia-like collaborative editing on questions and answers, and explores its beneficial effects on content quality and potential detrimental effects on users ' contributions . By examining five years ' archival data of Stack Overflow, we found that the benefits of collaborative editing outweigh its risks . For example, each substantive edit from other users can increase the number of positive votes by 181 % for the questions and 119 % for the answers . On the other hand, each edit only decreases askers and answerers ' subsequent contributions by no more than 5 % . This work has implications for understanding and designing large-scale social computing systems .
2K_dev_19	The Internet has the potential to accelerate scientific problem solving by engaging a global pool of contributors . Existing approaches focus on broadcasting problems to many independent solvers . We investigate other approaches that may be advantageous by examining a community for mathematical problem solving -- MathOverflow -- in which contributors communicate and collaborate to solve new mathematical 'micro-problems ' online . We contribute a simple taxonomy of collaborative acts derived from a process-level examination of collaborations and a quantitative analysis relating collaborative acts to solution quality . Our results indicate a diversity of ways in which mathematicians are reaching a solution, including by iteratively advancing a solution . A better understanding of such collaborative strategies can inform the design of tools to support distributed collaboration on complex problems .
2K_dev_20	Crowdsourcing has become a popular and indispensable component of many problem-solving pipelines in the research literature, with crowd workers often treated as computational resources that can reliably solve problems that computers have trouble with, such as image labeling/classification, natural language processing, or document writing . Yet, obviously crowd workers are human, and long sequences of the same monotonous tasks might intuitively reduce the amount of good quality work done by the workers . Here we propose an investigation into how we can use diversions containing small amounts of entertainment to improve crowd workers ' experiences . We call these small period of entertainment ``micro-diversions '' , which we hypothesize to provide timely relief to workers during long sequences of micro-tasks . We hope to improve productivity by retaining workers to work on our tasks longer and to either improve or retain the quality of work . We experimentally test micro-diversions on Amazon 's Mechanical Turk, a large paid-crowdsourcing platform . We find that micro-diversions can significantly improve worker retention rate while retaining the same work quality .
2K_dev_21	When health services involve long-term treatment over months or years, providers have the ability, not present in acute emergency care, to collaboratively reflect on clients ' changing health data and adjust interventions . In this paper, we discuss temporality as a factor in the design of health information technology . We define a temporal spectrum ranging from time-critical services that benefit from standardization to long-term services that require more flexibility . We provide empirical evidence from fieldwork that we performed in organizations providing long-term behavioral and mental health services for children . Our fieldwork in this context complements and provides contrasts to previous CSCW studies performed in time-critical hospital settings . Current literature shows a bias toward standardized records and routines in the implementation of health information technology, a policy that may not be appropriate for long-term health services . We discuss how the design of information systems should vary based on temporal factors .
2K_dev_22	My dissertation work focuses on understanding how and why professionals use activity traces generated by social work-sharing sites online to form impressions of fellow professionals ' expertise and inform personal interactions around work artifacts . I have conducted interviews with professionals in different domains who post and share their work online . These findings will then inform a model of factors contributing to impression formation in this specific context, as well as experiments testing and providing design recommendations for improving members ' ability to interact and effectively learn about each other .
2K_dev_23	Despite benefits and uses of social networking sites ( SNSs ) users are not always satisfied with their behaviors on the sites . These desires for behavior change both provide insight into users ' perceptions of how SNSs impact their lives ( positively or negatively ) and can inform tools for helping users achieve desired behavior changes . We use a 604-participant online survey to explore SNS users ' behavior-change goals for Facebook, Instagram, and Twitter . While some participants want to reduce site use, others want to improve their use or increase a range of behaviors . These desired changes differ by SNS, and, for Twitter, by participants ' levels of site use . Participants also expect a range of benefits from these goals, including increased time, contact with others, intrinsic benefits, better security/privacy, and improved self presentation . Based on these results we provide insights both into how participants perceive different SNSs, as well as potential designs for behavior-change mechanisms to target SNS behaviors .
2K_dev_24	Analysts synthesize complex, qualitative data to uncover themes and concepts, but the process is time-consuming, cognitively taxing, and automated techniques show mixed success . Crowdsourcing could help this process through on-demand harnessing of flexible and powerful human cognition, but incurs other challenges including limited attention and expertise . Further, text data can be complex, high-dimensional, and ill-structured . We address two major challenges unsolved in prior crowd clustering work: scaffolding expertise for novice crowd workers, and creating consistent and accurate categories when each worker only sees a small portion of the data . To address these challenges we present an empirical study of a two-stage approach to enable crowds to create an accurate and useful overview of a dataset: A ) we draw on cognitive theory to assess how re-representing data can shorten and focus the data on salient dimensions ; and B ) introduce an iterative clustering approach that provides workers a global overview of data . We demonstrate a classification-plus-context approach elicits the most accurate categories at the most useful level of abstraction .
2K_dev_25	Hackathons are events where people who are not normally collocated converge for a few days to write code together . Hackathons, it seems, are everywhere . We know that long- term collocation helps advance technical work and facilitate enduring interpersonal relationships, but can similar benefits come from brief, hackathon-style collocation ? How do participants spend their time preparing, working face-to- face, and following through these brief encounters ? Do the activities participants select suggest a tradeoff between the social and technical benefits of collocation ? We present results from a multiple-case study that suggest the way that hackathon-style collocation advances technical work varies across technical domain, community structure, and expertise of participants . Building social ties, in contrast, seems relatively constant across hackathons . Results from different hackathon team formation strategies suggest a tradeoff between advancing technical work and building social ties . Our findings have implications for technology support that needs to be in place for hackathons and for understanding the role of brief interludes of collocation in loosely-coupled, geographically distributed work .
2K_dev_26	A growing number of large collaborative idea generation platforms promise that by generating ideas together, people can create better ideas than any would have alone . But how might these platforms best leverage the number and diversity of contributors to help each contributor generate even better ideas ? Prior research suggests that seeing particularly creative or diverse ideas from others can inspire you, but few scalable mechanisms exist to assess diversity . We contribute a new scalable crowd-powered method for evaluating the diversity of sets of ideas . The method relies on similarity comparisons ( is idea A more similar to B or C ) generated by non-experts to create an abstract spatial idea map . Our validation study reveals that human raters agree with the estimates of dissimilarity derived from our idea map as much or more than they agree with each other . People seeing the diverse sets of examples from our idea map generate more diverse ideas than those seeing randomly selected examples . Our results also corroborate findings from prior research showing that people presented with creative examples generated more creative ideas than those who saw a set of random examples . We see this work as a step toward building more effective online systems for supporting large scale collective ideation .
2K_dev_27	One main challenge in large creative online communities is helping their members find inspirational ideas from a large pool of ideas . A high-level approach to address this challenge is to create a synthesis of emerging solution space that can be used to provide participants with creative and diverse inspirational ideas of others . Existing approaches to generate the synthesis of solution space either require community members to engage in tasks that detract from the main activity of generating ideas or depend on external crowd workers to help organize the ideas . We built IDEAHOUND a collaborative idea generation system that demonstrates an alternative `` organic '' human computation approach, where community members ( rather than external crowds ) contribute feedback about ideas as a byproduct of an activity that naturally integrates into the ideation process . This feedback in turn helps the community identify diverse inspirational ideas that can prompt community members to generate more high-quality and diverse ideas .
2K_dev_28	Previous work has shown the promise of crowdsourcing analogical idea generation, where distributing the stages of analogical processing across many people can reduce fixation, identify inspirations from more diverse domains, and lead to more creative ideas . However, prior work has only considered problems with a single constraint, while many real-world problems involve multiple constraints . This paper contributes a systematic crowdsourcing approach for eliciting multiple constraints inherent in a problem and using those constraints to find inspirations useful in solving it . To do so we identify methods to elicit useful constraints at different levels of abstraction, and empirical results that identify how the level of abstraction influences creative idea generation . Our results show that crowds find the most useful inspirations when the problem domain is represented abstractly and constraints are represented more concretely .
2K_dev_29	People are more creative at solving difficult design problems when they use relevant examples from outside of the problem 's domain as inspirations . However, finding such `` outside-the-box '' inspirations is difficult, particularly in large idea repositories such as the web, because without guidance people select domains to search based on surface similarity to the problem 's domain . In this paper, we demonstrate an approach in which non-experts identify domains that have the potential to yield useful and non-obvious inspirations for solutions . We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains . Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own, or drew inspiration from domains closer to or not sharing structural correspondence with the original problem .
2K_dev_30	Eye tracking is a compelling tool for revealing people 's spatial-temporal distribution of visual attention . But quality eye tracking hardware is expensive and can only be used with one person at a time . Further, webcam eye tracking systems have significant limitations on head movement and lighting conditions that result in significant data loss and inaccuracies . To address these drawbacks, we introduce a new approach that harnesses the crowd to understand allocation of visual attention . In our approach, crowdsourcing participants use mouse clicks to self-report the positions and trajectory for the following valuable eye tracking measures: first gaze, last gaze and all gazes . We validate our crowdsourcing approach with a user study, which demonstrated good accuracy when compared to a real eye tracker . We then deployed our prototype, GazeCrowd, in a crowdsourcing setting, and showed that it accurately generated gaze heatmaps and trajectory maps . Such an approach will allow designers to evaluate and refine their visual design without requiring the use of limited/expensive eye trackers .
2K_dev_31	Researchers and theorists have proposed that feelings of attachment to subgroups within a larger online community or site can increase users ' loyalty to the site . They have identified two types of attachment, with distinct causes and consequences . With bond-based attachment, people feel connections to other group members, while with identity-based attachment they feel connections to the group as a whole . In two experiments we show that these feelings of attachment to subgroups increase loyalty to the larger community . Communication with other people in a subgroup but not simple awareness of them increases attachment to the larger community . By varying how the communication is structured, between dyads or with all group members simultaneously, the experiments show that bond- and identity-based attachment have different causes . But the experiments show no evidence that bond and identity attachment have different consequences . We consider both theoretical and methodological reasons why the consequences of bond-based and identity-based attachment are so similar .
2K_dev_32	People spend an enormous amount of time searching for complex information online ; for example, consumers researching new purchases or patients learning about their conditions . As they search, people build up rich mental schemas about their target domains ; which, if effectively shared, could accelerate learning for others with similar interests . In this paper we introduce a novel approach for integrating the schemas individuals develop as they gather information online and surfacing them for others with similar interests . Through a controlled experiment we show that having access to others ' schemas while foraging for information helps new users to induce more useful, prototypical, and better-structured schemas than gathering information alone .
2K_dev_33	Online collaboration tools enable developers of interactive systems to quickly reach potential users for usability testing . Can these technologies serve designers who seek feedback on user needs during the earliest stages of design ? Online needfinding may help designers create products and services that can target a more diverse user population . To explore this, we conducted a feasibility study to compare face-to-face methods with online needfinding sessions . We found that video can sufficiently capture nuanced reactions to preliminary concept storyboards, but that feedback providers need guidance and structure . We then introduce a tool for collecting early-stage design feedback from online participants and conduct a case study with a professional design team . The team conducted needfinding activities with local participants, as well as a cost-equivalent number of online participants The case study demonstrates that combining online crowdsourcing with a video survey tool provides a simple and cost-efficient way to collect early-stage feedback .
2K_dev_34	This paper describes an empirical study of 1.6M deleted tweets collected over a continuous one-week period from a set of 292K Twitter users . We examine several aggregate properties of deleted tweets, including their connections to other tweets ( e.g., whether they are replies or retweets ) , the clients used to produce them, temporal aspects of deletion, and the presence of geotagging information . Some significant differences were discovered between the two collections, namely in the clients used to post them, their conversational aspects, the sentiment vocabulary present in them, and the days of the week they were posted . However, in other dimensions for which analysis was possible, no substantial differences were found . Finally, we discuss some ramifications of this work for understanding Twitter usage and management of one 's privacy .
2K_dev_35	This paper presents a study of the life cycle of news articles posted online . We describe the interplay between website visitation patterns and social media reactions to news content . We show that we can use this hybrid observation method to characterize distinct classes of articles . We also find that social media reactions can help predict future visitation patterns early and accurately . We validate our methods using qualitative analysis as well as quantitative analysis on data from a large international news network, for a set of articles generating more than 3,000,000 visits and 200,000 social media reactions . We show that it is possible to model accurately the overall traffic articles will ultimately receive by observing the first ten to twenty minutes of social media reactions . Achieving the same prediction accuracy with visits alone would require to wait for three hours of data . We also describe significant improvements on the accuracy of the early prediction of shelf-life for news stories .
2K_dev_36	A challenge for many online production communities is to direct their members to accomplish tasks that are important to the group, even when these tasks may not match individual members ' interests . Here we investigate how combining group identification and direction setting can motivate volunteers in online communities to accomplish tasks important to the success of the group as a whole . We hypothesize that group identity, the perception of belonging to a group, triggers in-group favoritism ; and direction setting ( including explicit direction from group goals and implicit direction from role models ) focuses people 's group-oriented motivation towards the group 's important tasks . We tested our hypotheses in the context of Wikipedia 's Collaborations of the Week ( COTW ) , a group goal setting mechanism and a social event within Wikiprojects . Results demonstrate that 1 ) publicizing important group goals via COTW can have a strong motivating influence on editors who have voluntarily identified themselves as group members compared to those who have not self-identified ; 2 ) the effects of goals spill over to non-goal related tasks ; and 3 ) editors exposed to group role models in COTW are more likely to perform similarly to the models on group-relevant citizenship behaviors . Finally, we discuss design and managerial implications based on our findings .
2K_dev_37	In crowd-collaborative innovation platforms, other contributors ' ideas can serve as sources of inspiration for creative ideas, but what patterns of interactions with others ' ideas are most helpful ? We investigate the hypothesis that building on inspiration sources that are conceptually far from one 's target domain are most helpful, a popular hypothesis with mixed empirical support . We predict the success rate of 2,344 ideas for 12 different design challenges in a collaborative Web-based innovation platform based on their cited sources ' conceptual distance from the target domain ( measured using probabilistic topic modeling of the ideas ) . Surprisingly, we find that innovators who cite conceptually near sources of inspiration achieve a higher success rate than those who prefer far sources . We discuss implications for research and development of crowd-collaborative innovation platforms .
2K_dev_38	We present a new technique that allows mobile devices to opportunistically group with one another, thus improving their ability to facilitate one-time or spontaneous exchanges of information . In our approach, devices share context with each other, and form groups when these readings are found to be similar to one another . Through a formative study, we examine the limitations of using a single type of context to form groups, and show how leveraging multiple contexts improves our ability to detect and form relevant groupings . We then present DIDJA, a robust software toolkit that automatically collects and analyzes contextual information in order to find and form groups . Through two prototypes, we demonstrate how DIDJA enhances existing user experiences, and show how developers can use our toolkit to easily facilitate frictionless collaborations between users and their environment . We then perform an extended experiment and show how DIDJA is able to accurately form groups under realistic conditions .
2K_dev_39	A significant challenge for crowdsourcing has been increasing worker engagement and output quality . We explore the effects of social, learning, and financial strategies, and their combinations, on increasing worker retention across tasks and change in the quality of worker output . Through three experiments, we show that 1 ) using these strategies together increased workers ' engagement and the quality of their work ; 2 ) a social strategy was most effective for increasing engagement ; 3 ) a learning strategy was most effective in improving quality . The findings of this paper provide strategies for harnessing the crowd to perform complex tasks, as well as insight into crowd workers ' motivation .
2K_dev_40	HomeProxy is a research prototype that uses a physical proxy to support video messaging at home among distributed family members . A physical artifact dedicated to remote family members makes it easier to chat with them over video . HomeProxy combines a form factor designed for the home environment with a `` no-touch '' user experience and an interface that quickly transitions between recorded and live video communication . We designed and implemented a prototype and our early experiences with it indicate the promise of offering quick video messaging at home and the challenges of a no-touch interface .
2K_dev_41	Sharing scientific data, software, and instruments is becoming increasingly common as science moves toward large-scale, distributed collaborations . Sharing these resources requires extra work to make them generally useful . Although we know much about the extra work associated with sharing data, we know little about the work associated with sharing contributions to software, even though software is of vital importance to nearly every scientific result . This paper presents a qualitative, interview-based study of the extra work that developers and end users of scientific software undertake . Our findings indicate that they conduct a rich set of extra work around community management, code maintenance, education and training, developer-user interaction, and foreseeing user needs . We identify several conditions under which they are likely to do this work, as well as design principles that can facilitate it . Our results have important implications for future empirical studies as well as funding policy .
2K_dev_42	Crowd feedback systems offer designers an emerging approach for improving their designs, but there is little empirical evidence of the benefit of these systems . This paper reports the results of a study of using a crowd feedback system to iterate on visual designs . Users in an introductory visual design course created initial designs satisfying a design brief and received crowd feedback on the designs . Users revised the designs and the system was used to generate feedback again . This format enabled us to detect the changes between the initial and revised designs and how the feedback related to those changes . Further, we analyzed the value of crowd feedback by comparing it with expert evaluation and feedback generated via free-form prompts . Results showed that the crowd feedback system prompted deep and cosmetic changes and led to improved designs, the crowd recognized the design improvements, and structured workflows generated more interpretative, diverse and critical feedback than free-form prompts .
2K_dev_43	In a variety of peer production settings, from Wikipedia to open source software development to crowdsourcing, individuals may encounter, edit, or review the work of unknown others . Typically this is done without much context to the person 's past behavior or performance . To understand how exposure to an unknown individual 's activity history influences attitudes and behaviors, we conducted an online experiment on Mechanical Turk varying the content, quality, and presentation of information about another Turker 's work history . Surprisingly, negative work history did not lead to negative outcomes, but in contrast, a positive work history led to positive initial impressions that persisted in the face of contrary information . This work provides insight into the impact of activity history design factors on psychological and behavioral outcomes that can be of use in other related settings .
2K_dev_44	When personalities clash, teams operate less effectively . Personality differences affect face-to-face collaboration and may lower trust in virtual teams . For relatively short-lived assignments, like those of online crowdsourcing, personality matching could provide a simple, scalable strategy for effective team formation . However, it is not clear how ( or if ) personality differences affect teamwork in this novel context where the workforce is more transient and diverse . This study examines how personality compatibility in crowd teams affects performance and individual perceptions . Using the DISC personality test, we composed 14 five-person teams ( N=70 ) with either a harmonious coverage of personalities ( balanced ) or a surplus of leader-type personalities ( imbalanced ) . Results show that balancing for personality leads to significantly better performance on a collaborative task . Balanced teams exhibited less conflict and their members reported higher levels of satisfaction and acceptance . This work demonstrates a simple personality matching strategy for forming more effective teams in crowdsourcing contexts .
2K_dev_45	We present the first formal study of crowdworkers who have disabilities via in-depth open-ended interviews of 17 people ( disabled crowdworkers and job coaches for people with disabilities ) and a survey of 631 adults with disabilities . Our findings establish that people with a variety of disabilities currently participate in the crowd labor marketplace, despite challenges such as crowdsourcing workflow designs that inadvertently prohibit participation by, and may negatively affect the worker reputations of, people with disabilities . Despite such challenges, we find that crowdwork potentially offers different opportunities for people with disabilities relative to the normative office environment, such as job flexibility and lack of a need to rely on public transit . We close by identifying several ways in which crowd labor platform operators and/or individual task requestors could improve the accessibility of this increasingly important form of employment .
2K_dev_46	Increasingly, the advice people receive on the Internet is socially transparent in the sense that it displays contextual information about the advice-givers or their actions . We hypothesize that activity transparency -seeing an advice giver 's process while creating his or her recommendations - will increase advice taking . We report three experiments testing the effect of activity transparency on taking mediocre advice . We found that the presence of a web history increased the likelihood of following a financial advisor 's advice and reduced participant earnings ( Exp . 1 ) , especially when the web history implied greater task focus ( Exp . 2, 3 ) . CSCW research usually emphasizes how to increase information sharing ; this work suggests when shared information may be inappropriate . We suggest ways to counter activity transparency 's potential downsides .
2K_dev_47	Social networking sites ( SNSs ) offer users a platform to build and maintain social connections . Understanding when people feel comfortable sharing information about themselves on SNSs is critical to a good user experience, because self-disclosure helps maintain friendships and increase relationship closeness . This observational research develops a machine learning model to measure self-disclosure in SNSs and uses it to understand the contexts where it is higher or lower . Features include emotional valence, social distance between the poster and people mentioned in the post, the language similarity between the post and the community and post topic . To validate the model and advance our understanding about online self-disclosure, we applied it to de-identified, aggregated status updates from Facebook users . Results show that women self-disclose more than men . People with a stronger desire to manage impressions self-disclose less . Network size is negatively associated with self-disclosure, while tie strength and network density are positively associated .
2K_dev_48	Participatory sensing systems ( PSS ) require frequent injection of information that has a short shelf-life . The use of crowds to gather information for PSS is therefore particularly challenging . In this study, we explore the impact of two policies on user contributions . A quid-pro-quo policy exchanges contributions from users for access to critical information in the system . A request policy simply reminds the user that information is needed to make the system function well . Prior research has shown that request for help in crowdsourced system is an effective mechanism to increase contributions . During a large-scale experimental study within a publicly deployed, crowdsourced, transit information system, we analyzed metrics associated with frequency of contribution and commitment to long-term use over a 10-month period . Our results confirmed that quid-pro-quo led to more contribution, but at a cost of faster departure from the study . When a participant was simply requested to contribute, but could still access community-generated data if they ignored a request, was largely ineffective and was statistically similar to the control condition where no request for contribution occurred . Thus crowdsource system designers should consider imposing quid-pro-quo type policies for PSS that concentrate on fewer users, but makes them more productive .
2K_dev_49	Expert feedback is valuable but hard to obtain for many designers . Online crowds can provide fast and affordable feedback, but workers may lack relevant domain knowledge and experience . Can expert rubrics address this issue and help novices provide expert-level feedback ? To evaluate this, we conducted an experiment with a 2x2 factorial design . Student designers received feedback on a visual design from both experts and novices, who produced feedback using either an expert rubric or no rubric . We found that rubrics helped novice workers provide feedback that was rated nearly as valuable as expert feedback . A follow-up analysis on writing style showed that student designers found feedback most helpful when it was emotionally positive and specific, and that a rubric increased the occurrence of these characteristics in feedback . The analysis also found that expertise correlated with longer critiques, but not the other favorable characteristics . An informal evaluation indicates that experts may instead have produced value by providing clearer justifications .
2K_dev_50	A novel method of using task fingerprinting to predict outcome measures such quality , errors , and the likelihood of cheating , particularly as applied to crowd sourced tasks . The technique focuses on the way workers work rather than the products they produce . The technique captures behavioral traces from online crowd workers and uses them to build predictive models of task performance . The effectiveness of the approach is evaluated across three contexts including classification , generation , and comprehension tasks .
2K_dev_51	A method performed by a processing device , the method comprising : obtaining first waveform data indicative of traversal of a first signal through a structure at a first time ; applying a scale transform to the first waveform data and the second waveform data ; computing , by the processing device and based on applying the scale transform , a scale-cross correlation function that promotes identification of scaling behavior between the first waveform data and the second waveform data ; performing one or more of : computing , by the processing device and based on the scale-cross correlation function , a scale factor for the first waveform data and the second waveform data ; and computing , by the processing device and based on the scale-cross correlation function , a scale invariant correlation coefficient between the first waveform data and the second waveform data .
2K_dev_52	An electronic device includes a touch-sensitive surface , for example a touch pad or touch screen . The user physically interacts with the touch-sensitive surface , producing touch events . The resulting interface actions taken depend at least in part on the touch type . The type of touch is determined in part based on vibro-acoustic data and touch data produced by the touch event .
2K_dev_53	Fast Fourier transform algorithms on large data sets achieve poor performance on various platforms because of the inefficient strided memory access patterns . These inefficient access patterns need to be reshaped to achieve high performance implementations . In this paper we formally restructure 1D , 2D and 3D FFTs targeting a generic machine model with a two-level memory hierarchy requiring block data transfers , and derive memory access pattern efficient algorithms using custom block data layouts . These algorithms need to be carefully mapped to the targeted platform 's architecture , particularly the memory subsystem , to fully utilize performance and energy efficiency potentials . Using the Kronecker product formalism , we integrate our optimizations into Spiral framework and evaluate a family of DRAM-optimized FFT algorithms and their hardware implementation design space via automated techniques . In our evaluations , we demonstrate DRAM-optimized accelerator designs over a large tradeoff space given various problem ( single/double precision 1D , 2D and 3D FFTs ) and hardware platform ( off-chip DRAM , 3D-stacked DRAM , ASIC , FPGA , etc . ) parameters . We show that Spiral generated pareto optimal designs can achieve close to theoretical peak performance of the targeted platform offering 6x and 6.5x system performance and power efficiency improvements respectively over conventional row-column FFT algorithms .
2K_dev_54	Languages for music audio processing typically offer a large assortment of unit generators . There is great duplication among different language implementations , as each language must implement many of the same ( or nearly the same ) unit generators . Csound has a large library of unit generators and could be a useful source of reusable unit generators for other languages or for direct use in applications . In this study , we consider how Csound unit generators can be exposed to direct access by other audio processing languages . Using Aura as an example , we modified Csound to allow efficient , dynamic allocation of individual unit generators without using the Csound compiler or writing Csound instruments . We then extended Aura using automatic code generation so that Csound unit generators can be accessed in the normal way from within Aura . In this scheme , Csound details are completely hidden from Aura users . We suggest that these techniques might eliminate most of the effort of building unit generator libraries and could help with the implementation of embedded audio systems where unit generators are needed but a full embedded Csound engine is not required .
2K_dev_55	We investigate whether personal information posted by job candidates on social media sites is sought and used by prospective employers . We create profiles for job candidates on popular social networks , manipulating information protected under U.S. laws , and submit job applications on their behalf to over 4,000 employers . We find evidence of employers searching online for the candidates . After comparing interview invitations for a Muslim versus a Christian candidate , and a gay versus a straight candidate , we find no difference in callback rates for the gay candidate compared to the straight candidate , but a 13 % lower callback rate for the Muslim candidate compared to the Christian candidate . While the difference is not significant at the national level , it exhibits significant and robust heterogeneity in bias at the local level , compatible with existing theories of discrimination . In particular , employers in Republican areas exhibit significant bias both against the Muslim candidate , and in favor of the Christian candidate . This bias is significantly larger than the bias in Democratic areas . The results are robust to using state- and county-level data , to controlling for firm , job , and geographical characteristics , and to several model specifications . The results suggest that 1 ) the online disclosure of certain personal traits can influence the hiring decisions of U.S. firms and 2 ) the likelihood of hiring discrimination via online searches varies across employers . The findings also highlight the surprisingly lasting behavioral influence of traditional , offline networks in processes and scenarios where online interactions are becoming increasingly common .
2K_dev_56	Motivated by a radically new peer review system that the National Science Foundation recently experimented with , we study peer review systems in which proposals are reviewed by PIs who have submitted proposals themselves . An ( m ; k ) -selection mechanism asks each PI to review m proposals , and uses these reviews to select ( at most ) k proposals . We are interested in impartial mechanisms , which guarantee that the ratings given by a PI to others ' proposals do not affect the likelihood of the PI 's own proposal being selected . We design an impartial mechanism that selects a k-subset of proposals that is nearly as highly rated as the one selected by the non-impartial ( abstract version of ) the NSF pilot mechanism , even when the latter mechanism has the `` unfair '' advantage of eliciting honest reviews .
2K_dev_57	The fairness notion of maximin share ( MMS ) guarantee underlies a deployed algorithm for allocating indivisible goods under additive valuations . Our goal is to understand when we can expect to be able to give each player his MMS guarantee . Previous work has shown that such an MMS allocation may not exist , but the counterexample requires a number of goods that is exponential in the number of players ; we give a new construction that uses only a linear number of goods . On the positive side , we formalize the intuition that these counterexamples are very delicate by designing an algorithm that provably finds an MMS allocation with high probability when valuations are drawn at random .
2K_dev_58	A paradigmatic problem in social choice theory deals with the aggregation of subjective preferences of individuals represented as rankings of alternatives into a social ranking . We are interested in settings where individuals are uncertain about their own preferences , and represent their uncertainty as distributions over rankings . Under the classic objective of minimizing the ( expected ) sum of Kendall tau distances between the input rankings and the output ranking , we establish that preference elicitation is surprisingly straightforward and near-optimal solutions can be obtained in polynomial time . We show , both in theory and using real data , that ignoring uncertainty altogether can lead to suboptimal outcomes .
2K_dev_59	Abstract : How do we formally verify security properties in today s malleable and evolving Commodity System Software ( COSS ) ecosystem ? Recent advances in applying formal methods to systems software , e.g. , IronClad [ 16 ] and seL4 [ 19 ] , promise that this vision is not a fool s errand after all . In this position paper we explore the challenges involved in this problem , what research questions the state of the art leaves still open , and our proposal for the next step towards realizing this vision .
2K_dev_60	Simultaneously reverse engineering a collection of condition-specific gene networks from gene expression microarray data to uncover dynamic mechanisms is a key challenge in systems biology . However , existing methods for this task are very sensitive to variations in the size of the microarray samples across different biological conditions ( which we term sample size heterogeneity in network reconstruction ) , and can potentially produce misleading results that can lead to incorrect biological interpretation . In this work , we develop a more robust framework that addresses this novel problem . Just like microarray measurements across conditions must undergo proper normalization on their magnitudes before entering subsequent analysis , we argue that networks across conditions also need to be normalized on their density when they are constructed , and we provide an algorithm that allows such normalization to be facilitated while estimating the networks . We show the quantitative advantages of our approach on synthetic and real data . Our analysis of a hematopoietic stem cell dataset reveals interesting results , some of which are confirmed by previously validated results .
2K_dev_61	We study the envy-free allocation of indivisible goods between two players . Our novel setting includes an option to sell each good for a fraction of the minimum value any player has for the good . To rigorously quantify the efficiency gain from selling , we reason about the price of envy-freeness of allocations of sellable goods -- the ratio between the maximum social welfare and the social welfare of the best envy-free allocation . We show that envy-free allocations of sellable goods are significantly more efficient than their unsellable counterparts .
2K_dev_62	Some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives . It seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule , which simply counts the number of times each alternative was approved . We challenge this assertion by proposing a probabilistic framework of noisy voting , and asking whether approval voting yields an alternative that is most likely to be the best alternative , given k-approval votes . While the answer is generally positive , our theoretical and empirical results call attention to situations where approval voting is suboptimal .
2K_dev_63	Personal photos are enjoying explosive growth with the popularity of photo-taking devices and social media . The vast amount of online photos largely exhibit users ' interests , emotion and opinions . Mining user interests from personal photos can boost a number of utilities , such as advertising , interest based community detection and photo recommendation . In this paper , we study the problem of user interests mining from personal photos . We propose a User Image Latent Space Model to jointly model user interests and image contents . User interests are modeled as latent factors and each user is assumed to have a distribution over them . By inferring the latent factors and users ' distributions , we can discover what the users are interested in . We model image contents with a four-level hierarchical structure where the layers correspond to themes , semantic regions , visual words and pixels respectively . Users ' latent interests are embedded in the theme layer . Given image contents , users ' interests can be discovered by doing posterior inference . We use variational inference to approximate the posteriors of latent variables and learn model parameters . Experiments on 180K Flickr photos demonstrate the effectiveness of our model .
2K_dev_64	We introduce the simultaneous model for cake cutting ( the fair allocation of a divisible good ) , in which agents simultaneously send messages containing a sketch of their preferences over the cake . We show that this model enables the computation of divisions that satisfy proportionality -- a popular fairness notion -- using a protocol that circumvents a standard lower bound via parallel information elicitation . Cake divisions satisfying another prominent fairness notion , envy-freeness , are impossible to compute in the simultaneous model , but admit arbitrarily good approximations .
2K_dev_65	Motivated by applications to crowdsourcing , we study voting rules that output a correct ranking of alternatives by quality from a large collection of noisy input rankings . We seek voting rules that are supremely robust to noise , in the sense of being correct in the face of any `` reasonable '' type of noise . We show that there is such a voting rule , which we call the modal ranking rule . Moreover , we establish that the modal ranking rule is the unique rule with the preceding robustness property within a large family of voting rules , which includes a slew of well-studied rules .
2K_dev_66	Classic social choice theory assumes that votes are independent ( but possibly conditioned on an underlying objective ground truth ) . This assumption is unrealistic in settings where the voters are connected via an underlying social network structure , as social interactions lead to correlated votes . We establish a general framework -- based on random utility theory -- for ranked voting on a social network with arbitrarily many alternatives ( in contrast to previous work , which is restricted to two alternatives ) . We identify a family of voting rules which , without knowledge of the social network structure , are guaranteed to recover the ground truth with high probability in large networks , with respect to a wide range of models of correlation among input votes .
2K_dev_67	Limited lookahead has been studied for decades in perfect-information games . This paper initiates a new direction via two simultaneous deviation points : generalization to imperfect-information games and a game-theoretic approach . The question of how one should act when facing an opponent whose lookahead is limited is studied along multiple axes : lookahead depth , whether the opponent ( s ) , too , have imperfect information , and how they break ties . We characterize the hardness of finding a Nash equilibrium or an optimal commitment strategy for either player , showing that in some of these variations the problem can be solved in polynomial time while in others it is PPAD-hard or NP-hard . We proceed to design algorithms for computing optimal commitment strategies for when the opponent breaks ties 1 ) favorably , 2 ) according to a fixed rule , or 3 ) adversarially . The impact of limited lookahead is then investigated experimentally . The limited-lookahead player often obtains the value of the game if she knows the expected values of nodes in the game tree for some equilibrium , but we prove this is not sufficient in general . Finally , we study the impact of noise in those estimates and different lookahead depths . This uncovers a lookahead pathology .
2K_dev_68	We present an efficient algorithm for solving a linear system arising from the 1-Laplacian corresponding to a collapsible simplicial complex with a known collapsing sequence . When combined with a result of Chillingworth , our algorithm is applicable to convex simplicial complexes embedded in R3 . The running time of our algorithm is nearly-linear in the size of the complex and is logarithmic on its numerical properties . Our algorithm is based on projection operators and combinatorial steps for transferring between them . The former relies on decomposing flows into circulations and potential flows using fast solvers for graph Laplacians , and the latter relates Gaussian elimination to topological properties of simplicial complexes .
2K_dev_69	We study the paradigmatic fair division problem of fairly allocating a divisible good among agents with heterogeneous preferences , commonly known as cake cutting . Classic cake cutting protocols are susceptible to manipulation . Do their strategic outcomes still guarantee fairness ? To address this question we adopt a novel algorithmic approach , proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model . Specifically , we show that each protocol in the class of generalized cut and choose ( GCC ) protocols which includes the most important discrete cake cutting protocols is guaranteed to have approximate subgame perfect Nash equilibria , or even exact equilibria if the protocol 's tie-breaking rule is flexible . We further observe that the ( approximate ) equilibria of proportional protocols which guarantee each of the n agents a 1/n-fraction of the cake must be ( approximately ) proportional , thereby answering the above question in the positive ( at least for one common notion of fairness ) .
2K_dev_70	This disclosure relates to a three-dimensional ( 3D ) integrated circuit ( 3DIC ) memory chip including computational logic-in-memory ( LiM ) for performing accelerated data processing . Related memory systems and methods are also disclosed . In one embodiment , the 3DIC memory chip includes at least one memory layer that provides a primary memory configured to store data . The 3DIC memory chip also includes a computational LiM layer . The computational LiM layer is a type of memory layer having application-specific computational logic integrated into local memory while externally appearing as regular memory . The computational LiM layer and the primary memory are interconnected through through-silica vias ( TSVs ) . In this manner , the computational LiM layer may load data from the primary memory with the 3DIC memory chip without having to access an external bus coupling the 3DIC memory chip to a central processing unit ( CPU ) or other processors to computationally process the data and generate a computational result .
2K_dev_71	An adversary who has obtained the cryptographic hash of a user 's password can mount an offline attack to crack the password by comparing this hash value with the cryptographic hashes of likely password guesses . This offline attacker is limited only by the resources he is willing to invest to crack the password . Key-stretching techniques like hash iteration and memory hard functions have been proposed to mitigate the threat of offline attacks by making each password guess more expensive for the adversary to verify . However , these techniques also increase costs for a legitimate authentication server . We introduce a novel Stackelberg game model which captures the essential elements of this interaction between a defender and an offline attacker . In the game the defender first commits to a key-stretching mechanism , and the offline attacker responds in a manner that optimizes his utility ( expected reward minus expected guessing costs ) . We then introduce Cost Asymmetric Secure Hash ( CASH ) , a randomized key-stretching mechanism that minimizes the fraction of passwords that would be cracked by a rational offline attacker without increasing amortized authentication costs for the legitimate authentication server . CASH is motivated by the observation that the legitimate authentication server will typically run the authentication procedure to verify a correct password , while an offline adversary will typically use incorrect password guesses . By using randomization we can ensure that the amortized cost of running CASH to verify a correct password guess is significantly smaller than the cost of rejecting an incorrect password . Using our Stackelberg game framework we can quantify the quality of the underlying CASH running time distribution in terms of the fraction of passwords that a rational offline adversary would crack . We provide an efficient algorithm to compute high quality CASH distributions for the defender . Finally , we analyze CASH using empirical data from two large scale password frequency datasets . Our analysis shows that CASH can significantly reduce ( up to 50 % ) the fraction of password cracked by a rational offline adversary .
2K_dev_72	This is the first time the ASONAM conference has been organized in North America . I am so delighted to watch the conference moving up so fast with consistent and hopefully sustainable success . Achieving acceptance rate of 13 % this year is a new record which brings a challenge for the new organizers . I am sure they are aware of the mission and will be able to maintain the same quality for the coming years . We need to show the success as permanent for ASONAM to continue its mission as the leading venue in the area of social networks analysis and mining . The number of quality submissions is rapidly increasing every year demonstrating the visibility of the conference and making it harder to select the papers to accommodate in the program . The quality of workshops co-located with ASONAM has been considerably improved this year . In addition , I am happy to witness the success of the two symposiums which have been integrated into the organization to have more specialized coverage of two key areas related to network based modeling and analysis . The symposium on the Foundations of Open Source Intelligence and Security Informatics ( FOSINT-SI ) is serving mostly researchers and practitioners interested in terror and criminal data analysis . On the other hand , the symposium on Network Enabled Health Informatics , Biomedicine and Bioinformatics ( HI-BI-BI ) covers the network applications in the health domain from the wet-lab to the clinic .
2K_dev_73	Abstract : Given a large network , changing over time , how can we find patterns and anomalies ? We propose Com2 , a novel and fast , incremental tensor analysis approach , which can discover both transient and periodic/ repeating communities . The method is ( a ) scalable , being linear on the input size ( b ) general , ( c ) needs no user-defined parameters and ( d ) effective , returning results that agree with intuition . We apply our method on real datasets , including a phone-call network and a computer-traffic network . The phone call network consists of 4 million mobile users , with 51 million edges ( phonecalls ) , over 14 days . Com2 spots intuitive patterns , that is , temporal communities ( comet communities ) . We report our findings , which include large star-like patterns , near-bipartite-cores , as well as tiny groups ( 5 users ) , calling each other hundreds of times within a few days .
2K_dev_74	Recognition of humans based on their biometric signatures is becoming of increasing importance because of its applications in access security , reliable identification for benefits distribution and homeland security among others . Popular biometric modalities include face images , fingerprints , iris images , palm prints , gait patterns , voice , etc . Iris images are of significant interest because of the excellent recognition rates they offer in controlled image acquisition conditions where the image quality is expected to be good enough to capture the details of the iris . However , in more realistic scenarios , iris images may not be of necessary quality because of image distortions and occlusions due to eyelids and eyelashes and the recognition rates provided by standard approaches may not be sufficient . In this chapter , we discuss how Bayesian graphical models can be used to achieve improved iris recognition in the presence of image impairments such as nonlinear deformations and occlusions . We illustrate the performance of these methods on sample iris image data sets .
2K_dev_75	Many graph mining and analysis services have been deployed on the cloud , which can alleviate users from the burden of implementing and maintaining graph algorithms . However , putting graph analytics on the cloud can invade users ' privacy . To solve this problem , we propose CryptGraph , which runs graph analytics on encrypted graph to preserve the privacy of both users ' graph data and the analytic results . In CryptGraph , users encrypt their graphs before uploading them to the cloud . The cloud runs graph analysis on the encrypted graphs and obtains results which are also in encrypted form that the cloud can not decipher . During the process of computing , the encrypted graphs are never decrypted on the cloud side . The encrypted results are sent back to users and users perform the decryption to obtain the plaintext results . In this process , users ' graphs and the analytics results are both encrypted and the cloud knows neither of them . Thereby , users ' privacy can be strongly protected . Meanwhile , with the help of homomorphic encryption , the results analyzed from the encrypted graphs are guaranteed to be correct . In this paper , we present how to encrypt a graph using homomorphic encryption and how to query the structure of an encrypted graph by computing polynomials . To solve the problem that certain operations are not executable on encrypted graphs , we propose hard computation outsourcing to seek help from users . Using two graph algorithms as examples , we show how to apply our methods to perform analytics on encrypted graphs . Experiments on two datasets demonstrate the correctness and feasibility of our methods .
2K_dev_76	Abstract Given a simple noun such as apple , and a question such as Is it edible ? , what processes take place in the human brain ? More specifically , given the stimulus , what are the interactions between ( groups of ) neurons ( also known as functional connectivity ) and how can we automatically infer those interactions , given measurements of the brain activity ? Furthermore , how does this connectivity differ across different human subjects ? In this work , we show that this problem , even though originating from the field of neuroscience , can benefit from big data techniques ; we present a simple , novel good-enough brain model , or GeBM in short , and a novel algorithm Sparse-SysId , which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity . Moreover , GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text ) . We evaluate GeBM by using real brain data . GeBM produces brain activity pattern ...
2K_dev_77	A key challenge in solving extensive-form games is dealing with large , or even infinite , action spaces . In games of imperfect information , the leading approach is to find a Nash equilibrium in a smaller abstract version of the game that includes only a few actions at each decision point , and then map the solution back to the original game . However , it is difficult to know which actions should be included in the abstraction without first solving the game , and it is infeasible to solve the game without first abstracting it . We introduce a method that combines abstraction with equilibrium finding by enabling actions to be added to the abstraction at run time . This allows an agent to begin learning with a coarse abstraction , and then to strategically insert actions at points that the strategy computed in the current abstraction deems important . The algorithm can quickly add actions to the abstraction while provably not having to restart the equilibrium finding . It enables anytime convergence to a Nash equilibrium of the full game even in infinite games . Experiments show it can outperform fixed abstractions at every stage of the run : early on it improves as quickly as equilibrium finding in coarse abstractions , and later it converges to a better solution than does equilibrium finding in fine-grained abstractions .
2K_dev_78	The success of Amazon Mechanical Turk ( MTurk ) as an online research platform has come at a price : MTurk exhibits slowing rates of population replenishment , and growing participants non-naivety . Recently , a number of alternative platforms have emerged , offering capabilities similar to MTurk while providing access to new and more naive populations . We examined two such platforms , CrowdFlower ( CF ) and Prolific Academic ( ProA ) . We found that both platforms participants were more naive and less dishonest compared to MTurk . CF showed the best response rate , but CF participants failed more attention-check questions and did not reproduce known effects replicated on ProA and MTurk . Moreover , ProA participants produced data quality that was higher than CFs and comparable to MTurks . We also found important demographic differences between the platforms . We discuss how researchers can use these findings to better plan online research , and their implications for the study of crowdsourcing research platforms .
2K_dev_79	Anecdotal evidence and scholarly research have shown that a significant portion of Internet users experience regrets over their online disclosures . To help individuals avoid regrettable online disclosures , we employed lessons from behavioral decision research and research on soft paternalism to design mechanisms that `` nudge '' users to consider the content and context of their online disclosures before posting them . We developed three such privacy nudges on Facebook . The first nudge provides visual cues about the audience for a post . The second nudge introduces time delays before a post is published . The third nudge gives users feedback about their posts . We tested the nudges in a three-week exploratory field trial with 21 Facebook users , and conducted 13 follow-up interviews . Our system logs , results from exit surveys , and interviews suggest that privacy nudges could be a promising way to prevent unintended disclosure . We discuss limitations of the current nudge designs and future directions for improvement .
2K_dev_80	An electronic device includes a touch-sensitive surface , for example a touch pad or touch screen . The user interacts with the touch-sensitive surface , producing touch interactions . Some of these touch interactions may be detected as indicative of a grasp for manipulating a physical tool ( e.g. , the grasp for holding a pen ) . When these touch interactions are encountered , a corresponding virtual tool is instantiated . The virtual tool controls an action on the electronic device that is similar to an action that can be performed by the physical tool . For example , the virtual pen can be used to draw on the display , whereas the physical pen draws on paper . A representation of the virtual tool is also displayed on a display for the electronic device , possibly providing additional affordances , at a location that corresponds to a location of the detected touch interaction .
2K_dev_81	We study cyber-physical systems subject to dynamic sensor attacks , relating them to the system 's strong observability . First , we find necessary and sufficient conditions for an attacker to create a dynamically undetectable sensor attack and relate these conditions to properties of the system dynamics eigenvectors . Next , we provide an index that gives the minimum number of sensors that must be attacked in order for an attack to be undetectable . Finally , we illustrate our results with a numerical example on the Quadruple Tank Process .
2K_dev_82	Tree structured graphical models are powerful at expressing long range or hierarchical dependency among many variables , and have been widely applied in different areas of computer science and statistics . However , existing methods for parameter estimation , inference , and structure learning mainly rely on the Gaussian or discrete assumptions , which are restrictive under many applications . In this paper , we propose new nonparametric methods based on reproducing kernel Hilbert space embeddings of distributions that can recover the latent tree structures , estimate the parameters , and perform inference for high dimensional continuous and non-Gaussian variables . The usefulness of the proposed methods are illustrated by thorough numerical results .
2K_dev_83	One common problem plaguing crowdsourcing tasks is tuning the set of worker responses : Depending on task requirements , requesters may want a large set of rich and varied worker responses ( typically in subjective evaluation tasks ) or a more convergent response-set ( typically for more objective tasks such as fact-checking ) . This problem is especially salient in tasks that combine workers responses to present a single output : Divergence in these settings could either add richness and complexity to the unified answer , or noise . In this paper we present HiveMind , a system of methods that allow requesters to tune different levels of convergence in worker participation for different tasks simply by adjusting the value of one variable .
2K_dev_84	Active learning has shown to reduce the number of exper- iments needed to obtain high-confidence drug-target predictions . How- ever , in order to actually save experiments using active learning , it is crucial to have a method to evaluate the quality of the current pre- diction and decide when to stop the experimentation process . Only by applying reliable stoping criteria to active learning , time and costs in the experimental process can be actually saved . We compute active learning traces on simulated drug-target matrices in order to learn a regression model for the accuracy of the active learner . By analyzing the perfor- mance of the regression model on simulated data , we design stopping criteria for previously unseen experimental matrices . We demonstrate on four previously characterized drug effect data sets that applying the stopping criteria can result in upto 40 % savings of the total experiments for highly accurate predictions .
2K_dev_85	This paper presents a machine learning system that is capable of predicting the speed and gear position of a moving vehicle from the sound it makes . While audio classification is widely used in other research areas such as music information retrieval and bioacoustics , its application to vehicle sounds is rare . Therefore , we investigate predicting the state of a vehicle using audio features in a classification task . We improve the classification results using correlation matrices , calculated from signals correlating with the audio . In an experiment , the sound of a moving vehicle is classified into discretized speed intervals and gear positions . The experiment shows that the system is capable of predicting the vehicle speed and gear position with near-perfect accuracy over 99 % . These results show that this system could be a valuable addition to vehicle anomaly detection and safety systems .
2K_dev_86	Training large machine learning ( ML ) models with many variables or parameters can take a long time if one employs sequential procedures even with stochastic updates . A natural solution is to turn to distributed computing on a cluster ; however , naive , unstructured parallelization of ML algorithms does not usually lead to a proportional speedup and can even result in divergence , because dependencies between model elements can attenuate the computational gains from parallelization and compromise correctness of inference . Recent efforts toward this issue have benefited from exploiting the static , a priori block structures residing in ML algorithms . In this paper , we take this path further by exploring the dynamic block structures and workloads therein present during ML program execution , which offers new opportunities for improving convergence , correctness , and load balancing in distributed ML . We propose and showcase a general-purpose scheduler , STRADS , for coordinating distributed updates in ML algorithms , which harnesses the aforementioned opportunities in a systematic way . We provide theoretical guarantees for our scheduler , and demonstrate its efficacy versus static block structures on Lasso and Matrix Factorization .
2K_dev_87	Interface-confinement is a common mechanism that secures untrusted code by executing it inside a sandbox . The sandbox limits ( confines ) the code 's interaction with key system resources to a restricted set of interfaces . This practice is seen in web browsers , hypervisors , and other security-critical systems . Motivated by these systems , we present a program logic , called System M , for modeling and proving safety properties of systems that execute adversary-supplied code via interface-confinement . In addition to using computation types to specify effects of computations , System M includes a novel invariant type to specify the properties of interface-confined code . The interpretation of invariant type includes terms whose effects satisfy an invariant . We construct a step-indexed model built over traces and prove the soundness of System M relative to the model . System M is the first program logic that allows proofs of safety for programs that execute adversary-supplied code without forcing the adversarial code to be available for deep static analysis . System M can be used to model and verify protocols as well as system designs . We demonstrate the reasoning principles of System M by verifying the state integrity property of the design of Memoir , a previously proposed trusted computing system .
2K_dev_88	We investigate a notion of behavioral genericity in the context of session type disciplines . To this end , we develop a logically motivated theory of parametric polymorphism , reminiscent of the Girard-Reynolds polymorphic -calculus , but casted in the setting of concurrent processes . In our theory , polymorphism accounts for the exchange of abstract communication protocols and dynamic instantiation of heterogeneous interfaces , as opposed to the exchange of data types and dynamic instantiation of individual message types . Our polymorphic session-typed process language satisfies strong forms of type preservation and global progress , is strongly normalizing , and enjoys a relational parametricity principle . Combined , our results confer strong correctness guarantees for communicating systems . In particular , parametricity is key to derive non-trivial results about internal protocol independence , a concurrent analogous of representation independence , and non-interference properties of modular , distributed systems .
2K_dev_89	A dataset has been classified by some unknown classifier into two types of points . What were the most important factors in determining the classification outcome ? In this work , we employ an axiomatic approach in order to uniquely characterize an influence measure : a function that , given a set of classified points , outputs a value for each feature corresponding to its influence in determining the classification outcome . We show that our influence measure takes on an intuitive form when the unknown classifier is linear . Finally , we employ our influence measure in order to analyze the effects of user profiling on Google 's online display advertising .
2K_dev_90	In this paper , we investigate a multivariate multi-response ( MVMR ) linear regression problem , which contains multiple linear regression models with differently distributed design matrices , and different regression and output vectors . The goal is to recover the support union of all regression vectors using $ l_1/l_2 $ -regularized Lasso . We characterize sufficient and necessary conditions on sample complexity \emph { as a sharp threshold } to guarantee successful recovery of the support union . Namely , if the sample size is above the threshold , then $ l_1/l_2 $ -regularized Lasso correctly recovers the support union ; and if the sample size is below the threshold , $ l_1/l_2 $ -regularized Lasso fails to recover the support union . In particular , the threshold precisely captures the impact of the sparsity of regression vectors and the statistical properties of the design matrices on sample complexity . Therefore , the threshold function also captures the advantages of joint support union recovery using multi-task Lasso over individual support recovery using single-task Lasso .
2K_dev_91	Upsampling of a multi-dimensional data-set is an operation with wide application in image processing and quantum mechanical calculations using density functional theory . For small up sampling factors as seen in the quantum chemistry code ONETEP , a time-shift based implementation that shifts samples by a fraction of the original grid spacing to fill in the intermediate values using a frequency domain Fourier property can be a good choice . Readily available highly optimized multidimensional FFT implementations are leveraged at the expense of extra passes through the entire working set . In this paper we present an optimized variant of the time-shift based up sampling . Since ONETEP handles threading , we address the memory hierarchy and SIMD vectorization , and focus on problem dimensions relevant for ONETEP . We present a formalization of this operation within the SPIRAL framework and demonstrate auto-generated and auto-tuned interpolation libraries . We compare the performance of our generated code against the previous best implementations using highly optimized FFT libraries ( FFTW and MKL ) . We demonstrate speed-ups in isolation averaging 3x and within ONETEP of up to 15 % .
2K_dev_92	When asked to mentally simulate coin tosses , people generate sequences that differ systematically from those generated by fair coins . It has been rarely noted that this divergence is apparent already in the very 1st mental toss . Analysis of several existing data sets reveals that about 80 % of respondents start their sequence with Heads . We attributed this to the linguistic convention describing coin toss outcomes as Heads or Tails , not vice versa . However , our subsequent experiments found the first-toss bias reversible under minor changes in the experimental setup , such as mentioning Tails before Heads in the instructions . We offer a comprehensive account in terms of a novel response bias , which we call reachability . It is more general than the 1st-toss bias , and it reflects the relative ease of reaching 1 option compared to its alternative in any binary choice context . When faced with a choice between 2 options ( e.g. , Heads and Tails , when tossing mental coins ) , whichever of the 2 is presented first by the choice architecture ( hence , is more reachable ) will be favored . This bias has far-reaching implications extending well beyond the context of randomness cognition ; in particular , to binary surveys ( e.g. , accept vs. reject ) and tests ( e.g. , TrueFalse ) . In binary choice , there is an advantage to what presents first .
2K_dev_93	This paper proves the asymptotic convergence of the meansquared error ( MSE ) of a distributed Kalman filter that we have previously proposed . This result shows that our distributed Kalman filter can track with bounded MSE any arbitrary linear dynamics .
2K_dev_94	Given information about medical drugs and their properties , how can we automatically discover that Aspirin has blood-thinning properties , and thus prevents heart attacks ? Expressed in more general terms , if we have a large in- formation network that integrates data from heterogeneous data sources , how can we extract semantic information that provides a better understanding of the in- tegrated data and also helps us to identify missing links ? We propose to extract concepts that describe groups of objects and their common properties from the integrated data . The discovered concepts provide semantic information as well as an abstract view on the integrated data and thus improve the understanding of complex systems . Our proposed method has the following desirable properties : ( a ) it is parameter-free and therefore requires no user-defined parameters ( b ) it is fault-tolerant , allowing for the detection of missing links and ( c ) it is scalable , being linear on the input size . We demonstrate the effectiveness and scalability of the proposed method on real , publicly available graphs .
2K_dev_95	Multilinear analysis is pervasive in a wide variety of fields , ranging from Signal Processing to Chemometrics , and from Machine Vision to Data Mining . Determining the quality of a given tensor decomposition is a task of utmost importance that spans all fields of application of tensors . This task by itself is hard in its nature , since even determining the rank of a tensor is an NP-hard problem . Fortunately , there exist heuristics in the literature that can be effectively used for this task ; one of these heuristics is the so-called Core Consistency Diagnostic ( CORCONDIA ) which is very intuitive and simple . However simple , computation of this diagnostic proves to be a very daunting task even for data of medium scale , let alone big tensor data . With the increase of the size of the tensor data that need to be analyzed there grows the need for efficient and scalable algorithms to compute diagnostics such as CORCONDIA , in order to assess the modelling quality . In this work we derive a fast and exact algorithm for CORCONDIA which exploits data sparsity and scales very well as the tensor size increases .
2K_dev_96	As Machine Learning ( ML ) applications embrace greater data size and model complexity , practitioners turn to distributed clusters to satisfy the increased computational and memory demands . Effective use of clusters for ML programs requires considerable expertise in writing distributed code , but existing highly-abstracted frameworks like Hadoop that pose low barriers to distributed-programming have not , in practice , matched the performance seen in highly specialized and advanced ML implementations . The recent Parameter Server ( PS ) paradigm is a middle ground between these extremes , allowing easy conversion of single-machine parallel ML programs into distributed ones , while maintaining high throughput through relaxed `` consistency models '' that allow asynchronous ( and , hence , inconsistent ) parameter reads . However , due to insufficient theoretical study , it is not clear which of these consistency models can really ensure correct ML algorithm output ; at the same time , there remain many theoretically-motivated but undiscovered opportunities to maximize computational throughput . Inspired by this challenge , we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models . We then use the gleaned insights to improve a consistency model using an `` eager '' PS communication mechanism , and implement it as a new PS system that enables ML programs to reach their solution more quickly .
2K_dev_97	Principal Component Analysis ( PCA ) has wide applications in machine learning , text mining and computer vision . Classical PCA based on a Gaussian noise model is fragile to noise of large magnitude . Laplace noise assumption based PCA methods can not deal with dense noise effectively . In this paper , we propose Cauchy Principal Component Analysis ( Cauchy PCA ) , a very simple yet effective PCA method which is robust to various types of noise . We utilize Cauchy distribution to model noise and derive Cauchy PCA under the maximum likelihood estimation ( MLE ) framework with low rank constraint . Our method can robustly estimate the low rank matrix regardless of whether noise is large or small , dense or sparse . We analyze the robustness of Cauchy PCA from a robust statistics view and present an efficient singular value projection optimization method . Experimental results on both simulated data and real applications demonstrate the robustness of Cauchy PCA to various noise patterns .
2K_dev_98	People with disabilities can be reluctant to friendsource help from their own friends for fear of appearing dependent or annoying . Our social microvolunteering approach has volunteers post friendsourcing tasks on behalf of people with disabilities . We demonstrate this approach via a Facebook application that answers visual questions on behalf of blind users .
2K_dev_99	In prior research we have developed a Curry-Howard interpretation of linear sequent calculus as session-typed processes . In this paper we uniformly integrate this computational interpretation in a functional language via a linear contextual monad that isolates session-based concurrency . Monadic values are open process expressions and are first class objects in the language , thus providing a logical foundation for higher-order session typed processes . We illustrate how the combined use of the monad and recursive types allows us to cleanly write a rich variety of concurrent programs , including higher-order programs that communicate processes . We show the standard metatheoretic result of type preservation , as well as a global progress theorem , which to the best of our knowledge , is new in the higher-order session typed setting .
2K_dev_100	Multi-modal data is dramatically increasing with the fast growth of social media . Learning a good distance measure for data with multiple modalities is of vital importance for many applications , including retrieval , clustering , classification and recommendation . In this paper , we propose an effective and scalable multi-modal distance metric learning framework . Based on the multi-wing harmonium model , our method provides a principled way to embed data of arbitrary modalities into a single latent space , of which an optimal distance metric can be learned under proper supervision , i.e. , by minimizing the distance between similar pairs whereas maximizing the distance between dissimilar pairs . The parameters are learned by jointly optimizing the data likelihood under the latent space model and the loss induced by distance supervision , thereby our method seeks a balance between explaining the data and providing an effective distance metric , which naturally avoids overfitting . We apply our general framework to text/image data and present empirical results on retrieval and classification to demonstrate the effectiveness and scalability .
2K_dev_101	Abstract : Suppose we are given a large graph in which , by some external process , a handful of nodes are marked . What can we say about these marked nodes ? Are they all close-by in the graph , or are they segregated into multiple groups ? How can we automatically determine how many , if any groups they form as well as find simple paths that connect the nodes in each group ? We formalize the problem in terms of the Minimum Description Length principle : a set of paths is simple when we need few bits to describe each path from one node to another . For example , we want to avoid high-degree nodes , unless we need to visit many of its spokes . As such , the best partitioning requires the least number of bits to describe the paths that visit all marked nodes . We show that our formulation for finding simple paths between groups of nodes has connections to well-known other problems in graph theory , and is NP-hard . We propose fast effective solutions , and introduce DOT2DOT , an efficient algorithm for partitioning marked nodes as well as finding simple paths between nodes within parts . Experimentation shows DOT2DOT correctly groups nodes for which good connection paths can be constructed , while separating distant nodes .
2K_dev_102	We consider the problem of signal recovery on graphs . Graphs model data with complex structure as signals on a graph . Graph signal recovery recovers one or multiple smooth graph signals from noisy , corrupted , or incomplete measurements . We formulate graph signal recovery as an optimization problem , for which we provide a general solution through the alternating direction methods of multipliers . We show how signal inpainting , matrix completion , robust principal component analysis , and anomaly detection all relate to graph signal recovery and provide corresponding specific solutions and theoretical analysis . We validate the proposed methods on real-world recovery problems , including online blog classification , bridge condition identification , temperature estimation , recommender system for jokes , and expert opinion combination of online blog classification .
2K_dev_103	This paper presents a computationally tractable algorithm for estimating the graph structure of graph signals is presented . The algorithm is demonstrated on simulated and real network time series datasets , and the performance of the new method is compared to that of related methods for estimating graph structure . The adjacency matrices estimated using the new method are shown to be close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset .
2K_dev_104	In this paper , we investigate a single-sample periocular-based alignment-robust face recognition technique that is pose-tolerant under unconstrained face matching scenarios . Our Spartans framework starts by utilizing one single sample per subject class , and generate new face images under a wide range of 3D rotations using the 3D generic elastic model which is both accurate and computationally economic . Then , we focus on the periocular region where the most stable and discriminant features on human faces are retained , and marginalize out the regions beyond the periocular region since they are more susceptible to expression variations and occlusions . A novel facial descriptor , high-dimensional Walsh local binary patterns , is uniformly sampled on facial images with robustness toward alignment . During the learning stage , subject-dependent advanced correlation filters are learned for pose-tolerant non-linear subspace modeling in kernel feature space followed by a coupled max-pooling mechanism which further improve the performance . Given any unconstrained unseen face image , the Spartans can produce a highly discriminative matching score , thus achieving high verification rate . We have evaluated our method on the challenging Labeled Faces in the Wild database and solidly outperformed the state-of-the-art algorithms under four evaluation protocols with a high accuracy of 89.69 % , a top score among image-restricted and unsupervised protocols . The advancement of Spartans is also proven in the Face Recognition Grand Challenge and Multi-PIE databases . In addition , our learning method based on advanced correlation filters is much more effective , in terms of learning subject-dependent pose-tolerant subspaces , compared with many well-established subspace methods in both linear and non-linear cases .
2K_dev_105	Summary form only given . What do graphs look like ? How do they evolve over time ? How does influence/news/viruses propagate , over time ? We present a long list of static and temporal laws , and some recent observations on real graphs . We show that fractals and self-similarity can explain several of the observed patterns , and we conclude with cascade analysis and a surprising result on virus propagation and immunization .
2K_dev_106	Facial hair detection and segmentation play an important role in forensic facial analysis . In this paper , we propose a fast , robust , fully automatic and self-training system for beard/moustache detection and segmentation in challenging facial images . In order to overcome the limitations of illumination , facial hair color and near-clear shaving , our facial hair detection self-learns a transformation vector to separate a hair class and a non-hair class from the testing image itself . A feature vector , consisting of Histogram of Gabor ( HoG ) and Histogram of Oriented Gradient of Gabor ( HOGG ) at different directions and frequencies , is proposed for both beard/moustache detection and segmentation in this paper . A feature-based segmentation is then proposed to segment the beard/moustache from a region on the face that is discovered to contain facial hair . Experimental results have demonstrated the robustness and effectiveness of our proposed system in detecting and segmenting facial hair in images drawn from three entire databases i.e . the Multiple Biometric Grand Challenge ( MBGC ) still face database , the NIST color Facial Recognition Technology FERET database and a large subset from Pinellas County database .
2K_dev_107	We consider the task of designing sparse control laws for large-scale systems by directly minimizing an infinite horizon quadratic cost with an $ \ell_1 $ penalty on the feedback controller gains . Our focus is on an improved algorithm that allows us to scale to large systems ( i.e . those where sparsity is most useful ) with convergence times that are several orders of magnitude faster than existing algorithms . In particular , we develop an efficient proximal Newton method which minimizes per-iteration cost with a coordinate descent active set approach and fast numerical solutions to the Lyapunov equations . Experimentally we demonstrate the appeal of this approach on synthetic examples and real power networks significantly larger than those previously considered in the literature .
2K_dev_108	We present the design of an algorithm to maximize the number of bugs found for black-box mutational fuzzing given a program and a seed input . The major intuition is to leverage white-box symbolic analysis on an execution trace for a given program-seed pair to detect dependencies among the bit positions of an input , and then use this dependency relation to compute a probabilistically optimal mutation ratio for this program-seed pair . Our result is promising : we found an average of 38.6 % more bugs than three previous fuzzers over 8 applications using the same amount of fuzzing time .
2K_dev_109	Crowdsourcing can solve problems beyond the reach of state-of-the-art fully automated systems . A common pattern found in many such systems is for the workers to discover , in parallel , a number of candidate solutions and then vote on the best one to pass forward , often within a fixed amount of time . We present the propose-vote-abstain mechanism for eliciting from crowd workers the proper balance between solution discovery and selection . Each crowd worker is given a choice among proposing an answer , voting among the answers proposed so far , or abstaining , i.e. , doing nothing . When a stopping condition is reached , the mechanism returns the answer with the most votes . Workers are paid a base amount , with bonuses if they propose or vote for the winning answer .
2K_dev_110	Crowdsourcing systems leverage short bursts of focused attention from many contributors to achieve a goal . By requiring peoples full attention , existing crowdsourcing systems fail to leverage peoples cognitive surplus in the many settings for which they may be distracted , performing or waiting to perform another task , or barely paying attention . In this paper , we study opportunities for low-effort crowdsourcing that enable people to contribute to problem solving in such settings . We discuss the design space for low-effort crowdsourcing , and through a series of prototypes , demonstrate interaction techniques , mechanisms , and emerging principles for enabling low-effort crowdsourcing .
2K_dev_111	In large scale machine learning and data mining problems with high feature dimensionality , the Euclidean distance between data points can be uninformative , and Distance Metric Learning ( DML ) is often desired to learn a proper similarity measure ( using side information such as example data pairs being similar or dissimilar ) . However , high dimensionality and large volume of pairwise constraints in modern big data can lead to prohibitive computational cost for both the original DML formulation in Xing et al . ( 2002 ) and later extensions . In this paper , we present a distributed algorithm for DML , and a large-scale implementation on a parameter server architecture . Our approach builds on a parallelizable reformulation of Xing et al . ( 2002 ) , and an asynchronous stochastic gradient descent optimization procedure . To our knowledge , this is the first distributed solution to DML , and we show that , on a system with 256 CPU cores , our program is able to complete a DML task on a dataset with 1 million data points , 22-thousand features , and 200 million labeled data pairs , in 15 hours ; and the learned metric shows great effectiveness in properly measuring distances .
2K_dev_112	Maintaining consistency is a difficult challenge in crowd-powered systems in which constituent crowd workers may change over time . We discuss an initial outline for Chorus : Mnemonic , a system that augments the crowd 's collective memory of a conversation by automatically recovering past knowledge based on topic , allowing the system to support consistent multi-session interactions . We present the design of the system itself , and discuss methods for testing its effectiveness . Our goal is to provide consistency between long interactions with crowd-powered conversational assistants by using AI to augment crowd workers .
2K_dev_113	We consider the class of projector-camera systems that adaptively image and illuminate a dynamic environment . Examples include adaptive front lighting in vehicles , dynamic stage performance lighting , adaptive dynamic range imaging and volumetric displays . A simulator is developed to explore the design space of such Reactive Visual Systems . Simulations are conducted to characterize system performance by analyzing the effects of end-to-end latency , jitter , and prediction algorithm complexity . Key operating points are identified where systems with simple prediction algorithms can outperform systems with more complex prediction algorithms . Based on the lessons learned from simulations , a low latency and low jitter , tight closed-loop reactive visual system is built . For the first time , we measure end-to-end latency , perform jitter analysis , investigate various prediction algorithms and their effect on system performance , compare our system 's performance to previous work , and demonstrate dis-illumination of falling snow-like particles and photography of fast moving scenes .
2K_dev_114	Thousands of web APIs expose data and services that would be useful to access with natural dialog , from weather and sports to Twitter and movies . The process of adapting each API to a robust dialog system is difficult and time-consuming , as it requires not only programming but also anticipating what is mostly likely to be asked and how it is likely to be asked . We present a crowd-powered system able to generate a natural languageinterface for arbitrary web APIs from scratch without domain-dependent training data or knowledge.Our approach combines two types of crowd workers : non-expert Mechanical Turk workers interpret the functions of the API and elicit information from the user , and expert oDesk workers provide a minimal sufficient scaffolding around the API to allow us to make general queries.We describe our multi-stage process and present results for each stage .
2K_dev_115	We have been investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous driving . In this work , we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols and suggest required modifications to guarantee their safety and efficiency despite these impairments .
2K_dev_116	Many big data applications collect a large number of time series , for example , the financial data of companies quoted in a stock exchange , the health care data of all patients that visit the emergency room of a hospital , or the temperature sequences continuously measured by weather stations across the US . A first task in the analytics of these data is to derive a low dimensional representation , a graph or discrete manifold , that describes well the interrelations among the time series and their intrarelations across time . This paper presents a computationally tractable algorithm for estimating this graph structure from the available data . This graph is directed and weighted , possibly representing causation relations , not just correlations as in most existing approaches in the literature . The algorithm is demonstrated on random graph and real network time series datasets , and its performance is compared to that of related methods . The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested .
2K_dev_117	How can we correlate neural activity in the human brain as it responds to words , with behavioral data expressed as answers to questions about these same words ? In short , we want to find latent variables , that explain both the brain activity , as well as the behavioral responses . We show that this is an instance of the Coupled Matrix-Tensor Factorization ( CMTF ) problem . We propose Scoup-SMT , a novel , fast , and parallel algorithm that solves the CMTF problem and produces a sparse latent low-rank subspace of the data . In our experiments , we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm for CMTF , along with a 5 fold increase in sparsity . Moreover , we extend Scoup-SMT to handle missing data without degradation of performance . We apply Scoup-SMT to BrainQ , a dataset consisting of a ( nouns , brain voxels , human subjects ) tensor and a ( nouns , properties ) matrix , with coupling along the nouns dimension . Scoup-SMT is able to find meaningful latent variables , as well as to predict brain activity with competitive accuracy . Finally , we demonstrate the generality of Scoup-SMT , by applying it on a Facebook dataset ( users , friends , wall-postings ) ; there , Scoup-SMT spots spammer-like anomalies .
2K_dev_118	We propose a method for representing motion information for video classification and retrieval . We improve upon local descriptor based methods that have been among the most popular and successful models for representing videos . The desired local descriptors need to satisfy two requirements : 1 ) to be representative , 2 ) to be discriminative . Therefore , they need to occur frequently enough in the videos and to be be able to tell the difference among different types of motions . To generate such local descriptors , the video blocks they are based on must contain just the right amount of motion information . However , current state-of-the-art local descriptor methods use video blocks with a single fixed size , which is insufficient for covering actions with varying speeds . In this paper , we introduce a long-short term motion feature that generates descriptors from video blocks with multiple lengths , thus covering motions with large speed variance . Experimental results show that , albeit simple , our model achieves state-of-the-arts results on several benchmark datasets .
2K_dev_119	How can we find useful patterns and anomalies in large scale real-world data with multiple attributes ? For example , network intrusion logs , with ( source-ip , target-ip , port-number , timestamp ) ? Tensors are suitable for modeling these multi-dimensional data , and widely used for the analysis of social networks , web data , network traffic , and in many other settings . However , current tensor decomposition methods do not scale for tensors with millions and billions of rows , columns and fibers , that often appear in real datasets . In this paper , we propose HaTen2 , a scalable distributed suite of tensor decomposition algorithms running on the MapReduce platform . By carefully reordering the operations , and exploiting the sparsity of real world tensors , HaTen2 dramatically reduces the intermediate data , and the number of jobs . As a result , using HaTen2 , we analyze big real-world tensors that can not be handled by the current state of the art , and discover hidden concepts .
2K_dev_120	Background Active learning is a powerful tool for guiding an experimentation process . Instead of doing all possible experiments in a given domain , active learning can be used to pick the experiments that will add the most knowledge to the current model . Especially , for drug discovery and development , active learning has been shown to reduce the number of experiments needed to obtain high-confidence predictions . However , in practice , it is crucial to have a method to evaluate the quality of the current predictions and decide when to stop the experimentation process . Only by applying reliable stopping criteria to active learning can time and costs in the experimental process actually be saved .
2K_dev_121	Building information models ( BIMs ) provide opportunities to serve as an information repository to store and deliver as-built information . Since a building is not always constructed exactly as the design information specifies , there will be discrepancies between a BIM created in the design phase ( called as-designed BIM ) and the as-built conditions . Point clouds captured by laser scans can be used as a reference to update an as-designed BIM into an as-built BIM ( i.e. , the BIM that captures the as-built information ) . Occlusions and construction progress prevent a laser scan performed at a single point in time to capture a complete view of building components . Progressively scanning a building during the construction phase and combining the progressively captured point cloud data together can provide the geometric information missing in the point cloud data captured previously . However , combining all point cloud data will result in large file sizes and might not always guarantee additional building component information . This paper provides the details of an approach developed to help engineers decide on which progressively captured point cloud data to combine in order to get more geometric information and eliminate large file sizes due to redundant point clouds .
2K_dev_122	Both SAT and # SAT can represent difficult problems in seemingly dissimilar areas such as planning , verification , and probabilistic inference . Here , we examine an expressive new language , # SAT , that generalizes both of these languages . # SAT problems require counting the number of satisfiable formulas in a concisely-describable set of existentially-quantified , propositional formulas . We characterize the expressiveness and worst-case difficulty of # SAT by proving it is complete for the complexity class # PNP [ 1 ] , and relating this class to more familiar complexity classes . We also experiment with three new general-purpose # SAT solvers on a battery of problem distributions including a simple logistics domain . Our experiments show that , despite the formidable worst-case complexity of # PNP [ 1 ] , many of the instances can be solved efficiently by noticing and exploiting a particular type of frequent structure .
2K_dev_123	Classic cake cutting protocols -- which fairly allocate a divisible good among agents with heterogeneous preferences -- are susceptible to manipulation . Do their strategic outcomes still guarantee fairness ? To answer this question we adopt a novel algorithmic approach , proposing a concrete computational model and reasoning about the game-theoretic properties of algorithms that operate in this model . Specifically , we show that each protocol in the class of generalized cut and choose ( GCC ) protocols -- which includes the most important discrete cake cutting protocols -- is guaranteed to have approximate subgame perfect Nash equilibria . Moreover , we observe that the ( approximate ) equilibria of proportional protocols -- which guarantee each of the n agents a 1/n-fraction of the cake -- must be ( approximately ) proportional , and design a GCC protocol where all Nash equilibrium outcomes satisfy the stronger fairness notion of envy-freeness . Finally , we show that under an obliviousness restriction , which still allows the computation of approximately envy-free allocations , GCC protocols are guaranteed to have exact subgame perfect Nash equilibria .
2K_dev_124	The paper introduces a method by which to design the topology of a distributed sensor network that is minimal with respect to a communication cost function . In the scenario considered , sensor nodes communicate with each other within a graph structure to update their data according to linear dynamics using neighbor node data . A subset of sensors can also report their state to a central location . One physical interpretation of this situation would be a set of spatially distributed wireless sensors which can communicate with other sensors within range to update data and can possibly connect to a network backbone . The costs would then be related to transmission energy . The objective is to recover the vector of initial sensor measurements from the backbone outputs over time , which requires that the dynamics of the overall networked system be observable . The topology of the network is then determined by the nonzero elements of the optimal observable dynamics . The following text contributes an efficient algorithm for designing the optimal observable dynamics and the network topology for a given set of sensors and cost function , providing proof of correctness and example implementation .
2K_dev_125	The Bayesian paradigm has provided a useful conceptual theory for understanding perceptual computation in the brain . While the detailed neural mechanisms of Bayesian inference are not fully understood , recent computational and neurophysiological works have illuminated the underlying computational principles and representational architecture . The fundamental insights are that the visual system is organized as a modular hierarchy to encode an internal model of the world , and that perception is realized by statistical inference based on such internal model . In this paper , we will discuss and analyze the varieties of representational schemes of these internal models and how they might be used to perform learning and inference . We will argue for a unified theoretical framework for relating the internal models to the observed neural phenomena and mechanisms in the visual cortex .
2K_dev_126	In this paper , we present the design and evaluation of a platform that can be used for time synchronization and indoor positioning of mobile devices . The platform uses the Time-Difference-Of-Arrival ( TDOA ) of multiple ultrasonic chirps broadcast from a network of beacons placed throughout the environment to find an initial location as well as synchronize a receivers clock with the infrastructure . These chirps encode identification data and ranging information that can be used to compute the receivers location . Once the clocks have been synchronized , the system can continue performing localization directly using Time-of-Flight ( TOF ) ranging as opposed to TDOA . This provides similar position accuracy with fewer beacons ( for tens of minutes ) until the mobile device clock drifts enough that a TDOA signal is once again required . Our hardware platform uses RF-based time synchronization to distribute clock synchronization from a subset of infrastructure beacons connected to a GPS source . Mobile devices use a novel time synchronization technique leverages the continuously free-running audio sampling subsystem of a smartphone to synchronize with global time . Once synchronized , each device can determine an accurate proximity from as little as one beacon using TOF measurements . This significantly decreases the number of beacons required to cover an indoor space and improves performance in the face of obstructions . We show through experiments that this approach outperforms the Network Time Protocol ( NTP ) on smartphones by an order of magnitude , providing an average 720s synchronization accuracy with clock drift rates as low as 2ppm .
2K_dev_127	Document clustering and topic modeling are two closely related tasks which can mutually benefit each other . Topic modeling can project documents into a topic space which facilitates effective document clustering . Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters . In this paper , we propose a multi-grain clustering topic model ( MGCTM ) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance . Our model tightly couples two components : a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters . We employ variational inference to approximate the posterior of hidden variables and learn model parameters . Experiments on two datasets demonstrate the effectiveness of our model .
2K_dev_128	In the United States , over three billion dollars are spent due to office equipment being left on when not in use during the weekend and at night . There is very little incentive for office workers to save energy because utility bills are not directly their responsibility . Our goal is to find ways to reduce the negative impact of this pervasive phenomenon by applying persuasive technologies to create awareness and encourage office workers towards more environmentally sustainable behavior . To this end , we conducted a literature review to investigate the persuasive methods appropriate to the field of building controls . We then proceeded to develop `` dashboard-controllers '' that enable office workers to control energy-using components with expert feedback to save energy .
2K_dev_129	AbstractDams are continuing to age and deteriorate that affect the service life of the dams in the U.S. Risk assessment is required to understand the behavior of dams and to prioritize the remedial actions and maintenance , and yet it is quite challenging to do risk assessment since it requires extensive current and historical data from in situ instrumentation on dams and detailed design , construction , and operation information . Currently , the required data for risk assessment resides in various paper-based and digital documents that hinder the capabilities of risk assessors to gather , analyze , and visualize the data flexibly from various engineering perspectives such as geotechnical , hydrologic , geologic , and structural . As a result , getting a holistic view of a dams current condition and its behavior over time becomes a time and resource intensive task . There is a need for an integrated shared knowledge repository for dams to streamline the risk assessment process . The research described in this paper p ...
2K_dev_130	Differential game logic ( dG L ) is a logic for specifying and verifying properties of hybrid games , i.e. , games that combine discrete , continuous , and adversarial dynamics . Unlike hybrid systems , hybrid games allow choices in the system dynamics to be resolved adversarially by different players with different objectives . The logic dG L can be used to study the existence of winning strategies for such hybrid games , i.e. , ways of resolving the players choices in some way so that he wins by achieving his objective for all choices of the opponent . Hybrid games are determined , i.e. , from each state , one player has a winning strategy , yet computing their winning regions may take transfinitely many steps . The logic dG L , nevertheless , has a sound and complete axiomatization relative to any expressive logic . Separating axioms are identified that distinguish hybrid games from hybrid systems . Finally , dG L is proved to be strictly more expressive than the corresponding logic of hybrid systems by characterizing the expressiveness of both .
2K_dev_131	In this paper , we examine the emerging copycat issue in the mobile apps market . Using machine learning techniques on large-scale unstructured data , we detect two types of copycats ( deceptive and non-deceptive ) from 10,100 action game apps from iOS App Store over five years . Based on our detected copycats , we model the key drivers of mobile app copycats as well as their major impacts . Our results indicate significant heterogeneity in the interactions between copycats and original apps over time : ( 1 ) Non-deceptive copycats are reluctant to enter the market when the original app is popular and free . However , this negative effect does not hold in other cases ; ( 2 ) Copycats can be either friends or foes of the original apps . High quality copycats always have a negative effect on the original app downloads . Interestingly , low quality deceptive copycats have a positive effect on the original app downloads , suggesting a potential positive spillover effect .
2K_dev_132	Protocols for tasks such as authentication , electronic voting , and secure multiparty computation ensure desirable security properties if agents follow their prescribed programs . However , if some agents deviate from their prescribed programs and a security property is violated , it is important to hold agents accountable by determining which deviations actually caused the violation . Motivated by these applications , we initiate a formal study of program actions as actual causes . Specifically , we define in an interacting program model what it means for a set of program actions to be an actual cause of a violation . We present a sound technique for establishing program actions as actual causes . We demonstrate the value of this formalism in two ways . First , we prove that violations of a specific class of safety properties always have an actual cause . Thus , our definition applies to relevant security properties . Second , we provide a cause analysis of a representative protocol designed to address weaknesses in the current public key certification infrastructure .
2K_dev_133	Privacy has become a significant concern in modern society as personal information about individuals is increasingly collected , used , and shared , often using digital technologies , by a wide range of organizations . To mitigate privacy concerns , organizations are required to respect privacy laws in regulated sectors e.g. , HIPAA in healthcare , GLBA in financial sector and to adhere to self-declared privacy policies in self-regulated sectors e.g. , privacy policies of companies such as Google and Facebook in Web services . This article provides an overview of a body of work on formalizing and enforcing privacy policies . We formalize privacy policies that prescribe and proscribe flows of personal information as well as those that place restrictions on the purposes for which a governed entity may use personal information . Recognizing that traditional preventive access control and information flow control mechanisms are inadequate for enforcing such privacy policies , we develop principled accountability mechanisms that seek to encourage policy-compliant behavior by detecting policy violations , assigning blame , and punishing violators . We apply these techniques to several U.S. privacy laws and organizational privacy policies , in particular , producing the first complete logical specification and audit of all disclosure-related clauses of the HIPAA Privacy Rule .
2K_dev_134	When building large-scale machine learning ( ML ) programs , such as massive topic models or deep neural networks with up to trillions of parameters and training examples , one usually assumes that such massive tasks can only be attempted with industrial-sized clusters with thousands of nodes , which are out of reach for most practitioners and academic researchers . We consider this challenge in the context of topic modeling on web-scale corpora , and show that with a modest cluster of as few as 8 machines , we can train a topic model with 1 million topics and a 1-million-word vocabulary ( for a total of 1 trillion parameters ) , on a document collection with 200 billion tokens -- - a scale not yet reported even with thousands of machines . Our major contributions include : 1 ) a new , highly-efficient O ( 1 ) Metropolis-Hastings sampling algorithm , whose running cost is ( surprisingly ) agnostic of model size , and empirically converges nearly an order of magnitude more quickly than current state-of-the-art Gibbs samplers ; 2 ) a model-scheduling scheme to handle the big model challenge , where each worker machine schedules the fetch/use of sub-models as needed , resulting in a frugal use of limited memory capacity and network bandwidth ; 3 ) a differential data-structure for model storage , which uses separate data structures for high- and low-frequency words to allow extremely large models to fit in memory , while maintaining high inference speed . These contributions are built on top of the Petuum open-source distributed ML framework , and we provide experimental evidence showing how this development puts massive data and models within reach on a small cluster , while still enjoying proportional time cost reductions with increasing cluster size .
2K_dev_135	Energy-efficient control mechanisms are necessary to manage the ever increasing energy demand . Recently several tools for building energy consumption control have been proposed for small ( e.g . homes ) and large ( e.g . offices ) buildings . The mechanism each tool uses is different , e.g . HVAC control and appliance rescheduling , but they share the goal of improving consumption of the buildings with respect to a given cost function . Some examples of cost functions are reduced energy consumption , reduced electricity bill , lower peak power , and increased ancillary service participation . The tools however do not capture the impacts of their control actions on the grid . These actions can lead to supply/demand imbalance and voltage/frequency deviation and thus , threaten grid stability . Utilities can take protective actions against those who cause instability by increasing electricity price or even momentarily disconnecting them from the grid . The effects of these protective actions can be so severe that the savings obtained by building management tools might disappear .
2K_dev_136	In distributed ML applications , shared parameters are usually replicated among computing nodes to minimize network overhead . Therefore , proper consistency model must be carefully chosen to ensure algorithm 's correctness and provide high throughput . Existing consistency models used in general-purpose databases and modern distributed ML systems are either too loose to guarantee correctness of the ML algorithms or too strict and thus fail to fully exploit the computing power of the underlying distributed system . Many ML algorithms fall into the category of \emph { iterative convergent algorithms } which start from a randomly chosen initial point and converge to optima by repeating iteratively a set of procedures . We 've found that many such algorithms are to a bounded amount of inconsistency and still converge correctly . This property allows distributed ML to relax strict consistency models to improve system performance while theoretically guarantees algorithmic correctness . In this paper , we present several relaxed consistency models for asynchronous parallel computation and theoretically prove their algorithmic correctness . The proposed consistency models are implemented in a distributed parameter server and evaluated in the context of a popular ML application : topic modeling .
2K_dev_137	Modern organizations ( e.g. , hospitals , social networks , government agencies ) rely heavily on audit to detect and punish insiders who inappropriately access and disclose confidential information . Recent work on audit games models the strategic interaction between an auditor with a single audit resource and auditees as a Stackelberg game , augmenting associated well-studied security games with a configurable punishment parameter . We significantly generalize this audit game model to account for multiple audit resources where each resource is restricted to audit a subset of all potential violations , thus enabling application to practical auditing scenarios . We provide an FPTAS that computes an approximately optimal solution to the resulting non-convex optimization problem . The main technical novelty is in the design and correctness proof of an optimization transformation that enables the construction of this FPTAS . In addition , we experimentally demonstrate that this transformation significantly speeds up computation of solutions for a class of audit games and security games .
2K_dev_138	To partly address people 's concerns over web tracking , Google has created the Ad Settings webpage to provide information about and some choice over the profiles Google creates on users . We present AdFisher , an automated tool that explores how user behaviors , Google 's ads , and Ad Settings interact . AdFisher can run browser-based experiments and analyze data using machine learning and significance tests . Our tool uses a rigorous experimental design and statistical analysis to ensure the statistical soundness of our results . We use AdFisher to find that the Ad Settings was opaque about some features of a user 's profile , that it does provide some choice on ads , and that these choices can lead to seemingly discriminatory ads . In particular , we found that visiting webpages associated with substance abuse changed the ads shown but not the settings page . We also found that setting the gender to female resulted in getting fewer instances of an ad related to high paying jobs than setting it to male . We can not determine who caused these findings due to our limited visibility into the ad ecosystem , which includes Google , advertisers , websites , and users . Nevertheless , these results can form the starting point for deeper investigations by either the companies themselves or by regulatory bodies .
2K_dev_139	We introduce quantitative usability and security models to guide the design of password management schemes systematic strate- gies to help users create and remember multiple passwords . In the same way that security proofs in cryptography are based on complexity- theoretic assumptions ( e.g. , hardness of factoring and discrete loga- rithm ) , we quantify usability by introducing usability assumptions .I n particular , password management relies on assumptions about human memory , e.g. , that a user who follows a particular rehearsal schedule will successfully maintain the corresponding memory . These assumptions are informed by research in cognitive science and can be tested empirically . Given rehearsal requirements and a user 's visitation schedule for each account , we use the total number of extra rehearsals that the user would have to do to remember all of his passwords as a measure of the usability of the password scheme . Our usability model leads us to a key observa- tion : password reuse benefits users not only by reducing the number of passwords that the user has to memorize , but more importantly by in- creasing the natural rehearsal rate for each password . We also present a security model which accounts for the complexity of password man- agement with multiple accounts and associated threats , including online , offline , and plaintext password leak attacks . Observing that current pass- word management schemes are either insecure or unusable , we present Shared Cues a new scheme in which the underlying secret is strategi- cally shared across accounts to ensure that most rehearsal requirements are satisfied naturally while simultaneously providing strong security . The construction uses the Chinese Remainder Theorem to achieve these competing goals .
2K_dev_140	Many important applications fall into the broad class of iterative convergent algorithms . Parallel implementations of these algorithms are naturally expressed using the Bulk Synchronous Parallel ( BSP ) model of computation . However , implementations using BSP are plagued by the straggler problem , where every transient slowdown of any given thread can delay all other threads . This paper presents the Stale Synchronous Parallel ( SSP ) model as a generalization of BSP that preserves many of its advantages , while avoiding the straggler problem . Algorithms using SSP can execute efficiently , even with significant delays in some threads , addressing the oft-faced straggler problem .
2K_dev_141	Ductal Carcinoma In Situ ( DCIS ) is a precursor lesion of Invasive Ductal Carcinoma ( IDC ) of the breast . Investigating its temporal progression could provide fundamental new insights for the development of better diagnostic tools to predict which cases of DCIS will progress to IDC . We investigate the problem of reconstructing a plausible progression from single-cell sampled data of an individual with synchronous DCIS and IDC . Specifically , by using a number of assumptions derived from the observation of cellular atypia occurring in IDC , we design a possible predictive model using integer linear programming ( ILP ) . Computational experiments carried out on a preexisting data set of 13 patients with simultaneous DCIS and IDC show that the corresponding predicted progression models are classifiable into categories having specific evolutionary characteristics . The approach provides new insights into mechanisms of clonal progression in breast cancers and helps illustrate the power of the ILP approach for similar problems in reconstructing tumor evolution scenarios under complex sets of constraints .
2K_dev_142	A variety of problems in computing , service , and manufacturing systems can be modeled via infinite repeating Markov chains with an infinite number of levels and a finite number of phases . Many such chains are quasi-birth-death processes with transitions that are skip-free in level , in that one can only transition between consecutive levels , and unidirectional in phase , in that one can only transition from lower-numbered phases to higher-numbered phases . We present a procedure , which we call Clearing Analysis on Phases ( CAP ) , for determining the limiting probabilities of such Markov chains exactly . The CAP method yields the limiting probability of each state in the repeating portion of the chain as a linear combination of scalar bases raised to a power corresponding to the level of the state . The weights in these linear combinations can be determined by solving a finite system of linear equations .
2K_dev_143	In this paper we consider strong Nash equilibria , in mixed strategies , for finite games . Any strong Nash equilibrium outcome is Pareto efficient for each coalition . First , we analyze the two -- player setting . Our main result , in its simplest form , states that if a game has a strong Nash equilibrium with full support ( that is , both players randomize among all pure strategies ) , then the game is strictly competitive . In order to get our result we use the indifference principle fulfilled by any Nash equilibrium , and the classical KKT conditions ( in the vector setting ) , that are necessary conditions for Pareto efficiency . Our characterization enables us to design a strong-Nash-equilibrium-finding algorithm with complexity in Smoothed- $ \mathcal { P } $ . So , this problem -- -that Conitzer and Sandholm [ Conitzer , V. , Sandholm , T. , 2008 . New complexity results about Nash equilibria . Games Econ . Behav . 63 , 621 -- 641 ] proved to be computationally hard in the worst case -- -is generically easy . Hence , although the worst case complexity of finding a strong Nash equilibrium is harder than that of finding a Nash equilibrium , once small perturbations are applied , finding a strong Nash is easier than finding a Nash equilibrium . Next we switch to the setting with more than two players . We demonstrate that a strong Nash equilibrium can exist in which an outcome that is strictly Pareto dominated by a Nash equilibrium occurs with positive probability . Finally , we prove that games that have a strong Nash equilibrium where at least one player puts positive probability on at least two pure strategies are extremely rare : they are of zero measure .
2K_dev_144	Many real world network problems often concern multivariate nodal attributes such as image , textual , and multi-view feature vectors on nodes , rather than simple univariate nodal attributes . The existing graph estimation methods built on Gaussian graphical models and covariance selection algorithms can not handle such data , neither can the theories developed around such methods be directly applied . In this paper , we propose a new principled framework for estimating multi-attribute graphs . Instead of estimating the partial correlation as in current literature , our method estimates the partial canonical correlations that naturally accommodate complex nodal features . Computationally , we provide an efficient algorithm which utilizes the multi-attribute structure . Theoretically , we provide sufficient conditions which guarantee consistent graph recovery . Extensive simulation studies demonstrate performance of our method under various conditions .
2K_dev_145	Proceedings : AACR 106th Annual Meeting 2015 ; April 18-22 , 2015 ; Philadelphia , PA Intratumor heterogeneity has long been a confounding factor in interpreting cancer genomic data , but has also been useful in reconstructing progression processes based on variation between clonal populations in single tumors . We previously developed a strategy of applying computational deconvolution algorithms to gene expression or DNA copy number data to reconstruct models of progression of cell populations from bulk tumor samples . Many tools have since been proposed for similar deconvolution analysis , but all are limited by the computational difficulty of unambiguously distinguishing small cell populations and variants from noise in genomic assays . We present a novel approach to infer tumor progression pathways from deconvolved genomic data designed to leverage the fact that tumors that partition into subtypes with similar evolutionary trajectories would be expected to lead to a mathematical substructure in the genomic data , known as a simplicial complex , which can be modeled computationally to better deconvolve cell populations across tumor types . We have developed a computational pipeline to perform tumor deconvolution while taking into consideration this kind of mathematical structure . The pipeline clusters tumors to identify genetically distinct subgroups , fits mixture models to these subgroups , and uses overlap between them to infer a refined deconvolution of major cellular populations and possible pathways of progression between them . We apply our methods to a set of RNASeq data from the TCGA breast cancer data set and to synthetic data modeling distinct scenarios of tumor progression . We first compare our methods on the synthetic data to our earlier work and to a comparative Gaussian mixture model . All methods perform comparably on unstructured data but the new method substantially outperforms the others on data consistent with simple scenarios for tumor progression along multiple discrete subtypes . The novel method , however , shows lower tolerance for noisy data than a Gaussian mixture model . Application to the TCGA RNASeq data showed our method could partition tumors into discrete subcategories associated with HER2+ , ER/PR+ , and triple-negative status and could exploit the resulting substructure of the data to deconvolve tumor data and infer progression models reflecting partial sharing of progression states between subtypes . Gene enrichment analysis showed association of deconvolved cell populations with a variety of gene functional categories suggestive of distinct progression mechanisms of the subtypes . Citation Format : Theodore Roman , Russell Schwartz . Improved deconvolution of heterogeneous tumor data to reconstruct clonal evolution from bulk genomic samples . [ abstract ] . In : Proceedings of the 106th Annual Meeting of the American Association for Cancer Research ; 2015 Apr 18-22 ; Philadelphia , PA. Philadelphia ( PA ) : AACR ; Cancer Res 2015 ; 75 ( 15 Suppl ) : Abstract nr 1934. doi:10.1158/1538-7445.AM2015-1934
2K_dev_146	Signals and datasets that arise in physical and engineering applications , as well as social , genetics , biomolecular , and many other domains , are becoming increasingly larger and more complex . In contrast to traditional time and image signals , data in these domains are supported by arbitrary graphs . Signal processing on graphs extends concepts and techniques from traditional signal processing to data indexed by generic graphs . This paper studies the concepts of low and high frequencies on graphs , and low- , high- and band-pass graph signals and graph filters . In traditional signal processing , these concepts are easily defined because of a natural frequency ordering that has a physical interpretation . For signals residing on graphs , in general , there is no obvious frequency ordering . We propose a definition of total variation for graph signals that naturally leads to a frequency ordering on graphs and defines low- , high- , and band-pass graph signals and filters . We study the design of graph filters with specified frequency response , and illustrate our approach with applications to sensor malfunction detection and data classification .
2K_dev_147	Block tridiagonal matrices arise in applied mathematics , physics , and signal processing . Many applications require knowledge of eigenvalues and eigenvectors of block tridiagonal matrices , which can be prohibitively expensive for large matrix sizes . In this paper , we address the problem of the eigendecomposition of block tridiagonal matrices by studying a connection between their eigenvalues and zeros of appropriate matrix polynomials . We use this connection with matrix polynomials to derive a closed-form expression for the eigenvectors of block tridiagonal matrices , which eliminates the need for their direct calculation and can lead to a faster calculation of eigenvalues . We also demonstrate with an example that our work can lead to fast algorithms for the eigenvector expansion for block tridiagonal matrices .
2K_dev_148	We aim to detect complex events in long Internet videos that may last for hours . A major challenge in this setting is that only a few shots in a long video are relevant to the event of interest while others are irrelevant or even misleading . Instead of indifferently pooling the shots , we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest . We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event detector . Next , we propose a new isotonic regularizer that is able to exploit the semantic ordering information . The resulting nearly-isotonic SVM classifier exhibits higher discriminative power . Computationally , we develop an efficient implementation using the proximal gradient algorithm , and we prove new , closed-form proximal steps . We conduct extensive experiments on three real-world video datasets and confirm the effectiveness of the proposed approach .
2K_dev_149	We present faster algorithms for approximate maximum flow in undirected graphs with good separator structures , such as bounded genus , minor free , and geometric graphs . Given such a graph with n vertices , m edges along with a recursive n-vertex separator structure , our algorithm finds an 1 -- e approximate maximum flow in time O ( m6/5poly ( e -- 1 ) ) , ignoring poly-logarithmic terms . Similar speedups are also achieved for separable graphs with larger size separators albeit with larger run times . These bounds also apply to image problems in two and three dimensions . Key to our algorithm is an intermediate problem that we term grouped L2 flow , which exists between maximum flows and electrical flows . Our algorithm also makes use of spectral vertex sparsifiers in order to remove vertices while preserving the energy dissipation of electrical flows . We also give faster spectral vertex sparsification algorithms on well separated graphs , which may be of independent interest .
2K_dev_150	The cake cutting problem models the fair division of a heterogeneous good between multiple agents . Previous work assumes that each agent derives value only from its own piece . However , agents may also care about the pieces assigned to other agents ; such externalities naturally arise in fair division settings . We extend the classical model to capture externalities , and generalize the classical fairness notions of proportionality and envyfreeness . Our technical results characterize the relationship between these generalized properties , establish the existence or nonexistence of fair allocations , and explore the computational feasibility of fairness in the face of externalities .
2K_dev_151	If Alice has double the friends of Bob , will she also have dou- ble the phone-calls ( or wall-postings , or tweets ) ? Our first contribution is the discovery that the relative frequencies obey a power-law ( sub-linear , or super-linear ) , for a wide variety of diverse settings : tasks in a phone- call network , like count of friends , count of phone-calls , total count of minutes ; tasks in a twitter-like network , like count of tweets , count of followees etc . The second contribution is that we further provide a full , digitized 2-d distribution , which we call the Almond-DG model , thanks to the shape of its iso-surfaces . The Almond-DG model matches all our empirical observations : super-linear relationships among variables , and ( provably ) log-logistic marginals . We illustrate our observations on two large , real network datasets , spanning 2.2M and 3.1M individu- als with 5 features each . We show how to use our observations to spot clusters and outliers , like , e.g. , telemarketers in our phone-call network .
2K_dev_152	Abstract Human rights organizations are increasingly monitoring social media for identification , verification , and documentation of human rights violations . Since manual extraction of events from the massive amount of online social network data is difficult and time-consuming , we propose an approach for automated , large-scale discovery and analysis of human rightsrelated events . We apply our recently developed Non-Parametric Heterogeneous Graph Scan ( NPHGS ) , which models social media data such as Twitter as a heterogeneous network ( with multiple different node types , features , and relationships ) and detects emerging patterns in the network , to identify and characterize human rights events . NPHGS efficiently maximizes a nonparametric scan statistic ( an aggregate measure of anomalousness ) over connected subgraphs of the heterogeneous network to identify the most anomalous network clusters . It summarizes each event with information such as type of event , geographical locations , time , and participants , and p ...
2K_dev_153	Kidney exchange provides a life-saving alternative to long waiting lists for patients in need of a new kidney . Fielded exchanges typically match under utilitarian or near-utilitarian rules ; this approach marginalizes certain classes of patients . In this paper , we focus on improving access to kidneys for highly-sensitized , or hard-to-match , patients . Toward this end , we formally adapt a recently introduced measure of the tradeoff between fairness and efficiency -- -the price of fairness -- -to the standard kidney exchange model . We show that the price of fairness in the standard theoretical model is small . We then introduce two natural definitions of fairness and empirically explore the tradeoff between matching more hard-to-match patients and the overall utility of a utilitarian matching , on real data from the UNOS nationwide kidney exchange and simulated data from each of the standard kidney exchange distributions .
2K_dev_154	This article summarizes and draws connections among diverse streams of theoretical and empirical research on the economics of privacy . We focus on the economic value and consequences of protecting and disclosing personal information , and on consumers ' understanding and decisions regarding the trade-offs associated with the privacy and the sharing of personal data . We highlight how the economic analysis of privacy evolved over time , as advancements in information technology raised increasingly nuanced and complex issues . We find and highlight three themes that connect diverse insights from the literature . First , characterizing a single unifying economic theory of privacy is hard , because privacy issues of economic relevance arise in widely diverse contexts . Second , there are theoretical and empirical situations where the protection of privacy can both enhance and detract from individual and societal welfare . Third , in digital economies , consumers ' ability to make informed decisions about their privacy is severely hindered because consumers are often in a position of imperfect or asymmetric information regarding when their data is collected , for what purposes , and with what consequences . We conclude the article by highlighting some of the ongoing issues in the privacy debate of interest to economists .
2K_dev_155	Abstract : Privacy policies in sectors as diverse as Web services , finance and healthcare often place restrictions on the purposes for which a governed entity may use personal information . Thus , automated methods for enforcing privacy policies require a semantics of purpose restrictions to determine whether a governed agent used information for a purpose . We provide such a semantics using a formalism based on planning . We model planning using Partially Observable Markov Decision Processes ( POMDPs ) , which supports an explicit model of information . We argue that information use is for a purpose if and only if the information is used while planning to optimize the satisfaction of that purpose under the POMDP model . We determine information use by simulating ignorance of the information prohibited by the purpose restriction , which we relate to noninterference . We use this semantics to develop a sound audit algorithm to automate the enforcement of purpose restrictions .
2K_dev_156	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context . Our method can be understood as a generalization of ngram modeling to non-integer n , and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases . PLRE training is efficient and our approach outperforms stateof-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .
2K_dev_157	We find large deviations rates for consensus-based distributed inference for directed networks . When the topology is deterministic , we establish the large deviations principle and find exactly the corresponding rate function , equal at all nodes . We show that the dependence of the rate function on the stochastic weight matrix associated with the network is fully captured by its left eigenvector corresponding to the unit eigenvalue . Further , when the sensors observations are Gaussian , the rate function admits a closed-form expression . Motivated by these observations , we formulate the optimal network design problem of finding the left eigenvector that achieves the highest value of the rate function , for a given target accuracy . This eigenvector therefore minimizes the time that the inference algorithm needs to reach the desired accuracy . For Gaussian observations , we show that the network design problem can be formulated as a semidefinite ( convex ) program , and hence can be solved efficiently . When observations are identically distributed across agents , the system exhibits an interesting property : the graph of the rate function always lies between the graphs of the rate function of an isolated node and the rate function of a fusion center that has access to all observations . We prove that this fundamental property holds even when the topology and the associated system matrices change randomly over time , with arbitrary distribution . Due to the generality of its assumptions , the latter result requires more subtle techniques than the standard large deviations tools , contributing to the general theory of large deviations .
2K_dev_158	In this paper , we explore how the contextual ambiguity of a search can affect a keyword 's performance . We propose an automatic way of categorizing keywords and examining keyword contextual ambiguity based on topic models from machine learning and computational linguistics . We quantify the effect of contextual ambiguity on keyword click-through performance using a hierarchical Bayesian model , and validate our study using a novel dataset from a major search engine containing information on click activities for 12,790 keywords across multiple categories from over 4.6 million impressions . We find that consumer click behaviors vary significantly across keywords , and keyword category and contextual ambiguity significantly affect such variation . Specifically , higher contextual ambiguity can lead to higher click-through rate ( CTR ) on top-positioned ads , but the CTR tends to decay faster with position . Our study has the potential to help advertisers design keyword portfolios , and help search engines improve the quality of sponsored ads .
2K_dev_159	We analyze the problem of regression when both input covariates and output responses are functions from a nonparametric function class . Function to function regression ( FFR ) covers a large range of interesting applications including timeseries prediction problems , and also more general tasks like studying a mapping between two separate types of distributions . However , previous nonparametric estimators for FFR type problems scale badly computationally with the number of input/output pairs in a data-set . Given the complexity of a mapping between general functions it may be necessary to consider large datasets in order to achieve a low estimation risk . To address this issue , we develop a novel scalable nonparametric estimator , the Triple-Basis Estimator ( 3BE ) , which is capable of operating over data-sets with many instances . To the best of our knowledge , the 3BE is the first nonparametric FFR estimator that can scale to massive data-sets . We analyze the 3BEs risk and derive an upperbound rate . Furthermore , we show an improvement of several orders of magnitude in terms of prediction speed and a reduction in error over previous estimators in various real-world datasets .
2K_dev_160	An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer . We propose several candidate authentication protocols for a setting in which the human user can only receive assistance from a semi-trusted computer -- - a computer that stores information and performs computations correctly but does not provide confidentiality . Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $ . The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate . We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $ , for a security parameter $ s ( f ) $ that depends on two key properties of $ f $ . To obtain our results , we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $ . Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate ) , and generalize recent results of Feldman et al . As an application , we propose a family of human computable password functions $ f_ { k_1 , k_2 } $ in which the user needs to perform $ 2k_1+2k_2+1 $ primitive operations ( e.g. , adding two digits or remembering $ \sigma ( i ) $ ) , and we show that $ s ( f ) 0 \min\ { k_1+1 , ( k_2+1 ) /2\ } $ . For these schemes , we prove that forging passwords is equivalent to recovering the secret mapping . Thus , our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts .
2K_dev_161	We study security games with multiple defenders . To achieve maximum security , defenders must perfectly synchronize their randomized allocations of resources . However , in real-life scenarios ( such as protection of the port of Boston ) this is not the case . Our goal is to quantify the loss incurred by miscoordination between defenders , both theoretically and empirically . We introduce two notions that capture this loss under different assumptions : the price of miscoordination , and the price of sequential commitment . Generally speaking , our theoretical bounds indicate that the loss may be extremely high in the worst case , while our simulations establish a smaller yet significant loss in practice .
2K_dev_162	In real world industrial applications of topic modeling , the ability to capture gigantic conceptual space by learning an ultra-high dimensional topical representation , i.e. , the so-called `` big model '' , is becoming the next desideratum after enthusiasms on `` big data '' , especially for fine-grained downstream tasks such as online advertising , where good performances are usually achieved by regression-based predictors built on millions if not billions of input features . The conventional data-parallel approach for training gigantic topic models turns out to be rather inefficient in utilizing the power of parallelism , due to the heavy dependency on a centralized image of `` model '' . Big model size also poses another challenge on the storage , where available model size is bounded by the smallest RAM of nodes . To address these issues , we explore another type of parallelism , namely model-parallelism , which enables training of disjoint blocks of a big topic model in parallel . By integrating data-parallelism with model-parallelism , we show that dependencies between distributed elements can be handled seamlessly , achieving not only faster convergence but also an ability to tackle significantly bigger model size . We describe an architecture for model-parallel inference of LDA , and present a variant of collapsed Gibbs sampling algorithm tailored for it . Experimental results demonstrate the ability of this system to handle topic modeling with unprecedented amount of 200 billion model variables only on a low-end cluster with very limited computational resources and bandwidth .
2K_dev_163	The proliferation of mobile devices that are capable of estimating their position , has lead to the emergence of a new class of social networks , namely location-based social networks ( LBSNs for short ) . The main interaction between users in an LBSN is location sharing . While the latter can be realized through continuous tracking of a user 's whereabouts from the service provider , the majority of LBSNs allow users to voluntarily share their location , through check-ins . LBSNs provide incentives to users to perform check-ins . However , these incentives can also lead to people faking their location , thus , generating false information . In this work , we propose the use of tensor decomposition for spotting anomalies in the check-in behavior of users . To the best of our knowledge , this is the first attempt to model this problem using tensor analysis .
2K_dev_164	The computational characterization of game-theoretic solution concepts is a central topic in artificial intelligence , with the aim of developing computationally efficient tools for finding optimal ways to behave in strategic interactions . The central solution concept in game theory is Nash equilibrium ( NE ) . However , it fails to capture the possibility that agents can form coalitions ( even in the 2-agent case ) . Strong Nash equilibrium ( SNE ) refines NE to this setting . It is known that finding an SNE is NP-complete when the number of agents is constant . This hardness is solely due to the existence of mixed-strategy SNEs , given that the problem of enumerating all pure-strategy SNEs is trivially in P. Our central result is that , in order for a game to have at least one non-pure-strategy SNE , the agents ' payoffs restricted to the agents ' supports must , in the case of 2 agents , lie on the same line , and , in the case of n agents , lie on an ( n - 1 ) -dimensional hyperplane . Leveraging this result , we provide two contributions . First , we develop worst-case instances for support-enumeration algorithms . These instances have only one SNE and the support size can be chosen to be of any size-in particular , arbitrarily large . Second , we prove that , unlike NE , finding an SNE is in smoothed polynomial time : generic game instances ( i.e. , all instances except knife-edge cases ) have only pure-strategy SNEs .
2K_dev_165	If we know most of Smith 's friends are from Boston , what can we say about the rest of Smith 's friends ? In this paper , we focus on the node classification problem on networks , which is one of the most important topics in AI and Web communities . Our proposed algorithm which is referred to as OMNI-Prop has the following properties : ( a ) seamless and accurate ; it works well on any label correlations ( i.e. , homophily , het-erophily , and mixture of them ) ( b ) fast ; it is efficient and guaranteed to converge on arbitrary graphs ( c ) quasi-parameter free ; it has just one well-interpretable parameter with heuristic default value of 1 . We also prove the theoretical connections of our algorithm to the semi-supervised learning ( SSL ) algorithms and to random-walks . Experiments on four real , different network datasets demonstrate the benefits of the proposed algorithm , where OMNI-Prop outperforms the top competitors .
2K_dev_166	Single virus epidemics over complete networks are widely explored in the literature as the fraction of infected nodes is , under appropriate microscopic modeling of the virus infection , a Markov process . With non-complete networks , this macroscopic variable is no longer Markov . In this paper , we study virus diffusion , in particular , multi-virus epidemics , over non-complete stochastic networks . We focus on multipartite networks . In companying work this http URL , we show that the peer-to-peer local random rules of virus infection lead , in the limit of large multipartite networks , to the emergence of structured dynamics at the macroscale . The exact fluid limit evolution of the fraction of nodes infected by each virus strain across islands obeys a set of nonlinear coupled differential equations , see this http URL In this paper , we develop methods to analyze the qualitative behavior of these limiting dynamics , establishing conditions on the virus micro characteristics and network structure under which a virus persists or a natural selection phenomenon is observed .
2K_dev_167	Many modern machine learning ( ML ) algorithms are iterative , converging on a final solution via many iterations over the input data . This paper explores approaches to exploiting these algorithms ' convergent nature to improve performance , by allowing parallel and distributed threads to use loose consistency models for shared algorithm state . Specifically , we focus on bounded staleness , in which each thread can see a view of the current intermediate solution that may be a limited number of iterations out-of-date . Allowing staleness reduces communication costs ( batched updates and cached reads ) and synchronization ( less waiting for locks or straggling threads ) . One approach is to increase the number of iterations between barriers in the oft-used Bulk Synchronous Parallel ( BSP ) model of parallelizing , which mitigates these costs when all threads proceed at the same speed . A more flexible approach , called Stale Synchronous Parallel ( SSP ) , avoids barriers and allows threads to be a bounded number of iterations ahead of the current slowest thread . Extensive experiments with ML algorithms for topic modeling , collaborative filtering , and PageRank show that both approaches significantly increase convergence speeds , behaving similarly when there are no stragglers , but SSP outperforms BSP in the presence of stragglers .
2K_dev_168	Determining a match between the subjects of first and second images as a function of decimal-number representations of regions of the first and second images . The decimal-number representations are generated by performing discrete transforms on the regions so as to obtain discrete-transform coefficients , performing local-bit-pattern encoding of the coefficients to create data streams , and converting the data streams to decimal numbers . In one embodiment , the first and second images depict periocular facial regions , and the disclosed techniques can be used for face recognition , even where a small portion of a person 's face is captured in an image . Subspace modeling may be used to improve accuracy .
2K_dev_169	Practical applications of Bayesian nonparametric ( BNP ) models have been limited , due to their high computational complexity and poor scaling on large data . In this paper , we consider dependent nonparametric trees ( DNTs ) , a powerful infinite model that captures time-evolving hierarchies , and develop a large-scale distributed training system . Our major contributions include : ( 1 ) an effective memoized variational inference for DNTs , with a novel birth-merge strategy for exploring the unbounded tree space ; ( 2 ) a model-parallel scheme for concurrent tree growing/pruning and efficient model alignment , through conflict-free model partitioning and lightweight synchronization ; ( 3 ) a data-parallel scheme for variational parameter updates that allows distributed processing of massive data . Using 64 cores in 36 hours , our system learns a 10K-node DNT topic model on 8M documents that captures both high-frequency and longtail topics . Our data and model scales are orders-of-magnitude larger than recent results on the hierarchical Dirichlet process , and the near-linear scalability indicates great potential for even bigger problem sizes .
2K_dev_170	Designing optimalthat is , revenue-maximizingcombinatorial auctions ( CAs ) is an important elusive problem . It is unsolved even for two bidders and two items for sale . Rather than pursuing the manual approach of attempting to characterize the optimal CA , we introduce a family of CAs and then seek a high-revenue auction within that family . The family is based on bidder weighting and allocation boosting ; we coin such CAs virtual valuations combinatorial auctions ( VVCAs ) . VVCAs are the Vickrey-Clarke-Groves ( VCG ) mechanism executed on virtual valuations that are affine transformations of the bidders valuations . The auction family is parameterized by the coefficients in the transformations . The problem of designing a CA is thereby reduced to search in the parameter space of VVCAor the more general space of affine maximizer auctions .We first construct VVCAs with logarithmic approximation guarantees in canonical special settings : ( 1 ) limited supply with additive valuations and ( 2 ) unlimited supply.In the main part of the paper , we develop algorithms that design high-revenue CAs for general valuations using samples from the prior distribution over bidders valuations . ( Priors turn out to be necessary for achieving high revenue . ) We prove properties of the problem that guide our design of algorithms . We then introduce a series of algorithms that use economic insights to guide the search and thus reduce the computational complexity . Experiments show that our algorithms create mechanisms that yield significantly higher revenue than the VCG and scale dramatically better than prior automated mechanism design algorithms . The algorithms yielded deterministic mechanisms with the highest known revenues for the settings tested , including the canonical setting with two bidders , two items , and uniform additive valuations . 1
2K_dev_171	Background Tumorigenesis is an evolutionary process by which tumor cells acquire mutations through successive diversification and differentiation . There is much interest in reconstructing this process of evolution due to its relevance to identifying drivers of mutation and predicting future prognosis and drug response . Efforts are challenged by high tumor heterogeneity , though , both within and among patients . In prior work , we showed that this heterogeneity could be turned into an advantage by computationally reconstructing models of cell populations mixed to different degrees in distinct tumors . Such mixed membership model approaches , however , are still limited in their ability to dissect more than a few well-conserved cell populations across a tumor data set .
2K_dev_172	How many listens will an artist receive on a online radio ? How about plays on a YouTube video ? How many of these visits are new or returning users ? Modeling and mining popularity dynamics of social activity has important implications for researchers , content creators and providers . We here investigate the effect of revisits ( successive visits from a single user ) on content popularity . Using four datasets of social activity , with up to tens of millions media objects ( e.g. , YouTube videos , Twitter hashtags or LastFM artists ) , we show the effect of revisits in the popularity evolution of such objects . Secondly , we propose the Phoenix-R model which captures the popularity dynamics of individual objects . Phoenix-R has the desired properties of being : ( 1 ) parsimonious , being based on the minimum description length principle , and achieving lower root mean squared error than state-of-the-art baselines ; ( 2 ) applicable , the model is effective for predicting future popularity values of objects .
2K_dev_173	We propose social microvolunteering , in which people can do charitable microwork themselves for free , but also grant access to their Facebook friends as additional volunteers to magnify their effort . Social microvolunteering lets people volunteer despite temporal , financial , or physical limitations .
2K_dev_174	In this paper , we use data collected from over 2000 non-residential electric vehicle supply equipments ( EVSEs ) located in Northern California for the year of 2013 to estimate the potential benefits of smart electric vehicle ( EV ) charging . We develop a smart charging framework to identify the benefits of non-residential EV charging to the load aggregators and the distribution grid . Using this extensive dataset , we aim to improve upon past studies focusing on the benefits of smart EV charging by relaxing the assumptions made in these studies regarding : ( i ) driving patterns , driver behavior and driver types ; ( ii ) the scalability of a limited number of simulated vehicles to represent different load aggregation points in the power system with different customer characteristics ; and ( iii ) the charging profile of EVs . First , we study the benefits of EV aggregations behind-the-meter , where a time-of-use pricing schema is used to understand the benefits to the owner when EV aggregations shift load from high cost periods to lower cost periods . For the year of 2013 , we show a reduction of up to 24.8 % in the monthly bill is possible . Then , following a similar aggregation strategy , we show that EV aggregations decrease their contribution to the system peak load by approximately 37 % ( median ) when charging is controlled within arrival and departure times . Our results also show that it could be expected to shift approximately 0.25kWh ( 2.8 % ) of energy per non-residential EV charging session from peak periods ( 12PM6PM ) to off-peak periods ( after 6PM ) in Northern California for the year of 2013 .
2K_dev_175	Given the pace of discovery in medicine , accessing the literature to make informed decisions at the point of care has become increasingly difficult . Although the Internet creates unprecedented access to information , gaps in the medical literature and inefficient searches often leave healthcare providers ' questions unanswered . Advances in social computation and human computer interactions offer a potential solution to this problem . We developed and piloted the mobile application DocCHIRP , which uses a system of point-to-multipoint push notifications designed to help providers problem solve by crowdsourcing from their peers . Over the 244-day pilot period , 85 registered users logged 1544 page views and sent 45 consult questions . The median initial first response from the crowd occurred within 19 minutes . Review of the transcripts revealed several dominant themes , including complex medical decision making and inquiries related to prescription medication use . Feedback from the post-trial survey identified potential hurdles related to medical crowdsourcing , including a reluctance to expose personal knowledge gaps and the potential risk for distracted doctoring . Users also suggested program modifications that could support future adoption , including changes to the mobile interface and mechanisms that could expand the crowd of participating healthcare providers . Journal of Hospital Medicine 2014 ; 9:451456 . 2014 Society of Hospital Medicine
2K_dev_176	In this paper , we describe a novel methodology , grounded in techniques from the field of machine learning , for modeling emerging social structure as it develops in threaded discussion forums , with an eye towards application in the threaded discussions of massive open online courses ( MOOCs ) . This modeling approach integrates two simpler , well established prior techniques , namely one related to social network structure and another related to thematic structure of text . As an illustrative application of the integrated techniques use and utility , we use it as a lens for exploring student dropout behavior in three different MOOCs . In particular , we use the model to identify twenty emerging subcommunities within the threaded discussions of each of the three MOOCs . We then use a survival model to measure the impact of participation in identified subcommunities on attrition along the way for students who have participated in the course discussion forums of the three courses . In each of three MOOCs we find evidence that participation in two to four subcommunities out of the twenty is associated with significantly higher or lower dropout rates than average . A qualitative post-hoc analysis illustrates how the learned models can be used as a lens for understanding the values and focus of discussions within the subcommunities , and in the illustrative example to think about the association between those and detected higher or lower dropout rates than average in the three courses . Our qualitative analysis demonstrates that the patterns that emerge make sense : It associates evidence of stronger expressed motivation to actively participate in the course as well as evidence of stronger cognitive engagement with the material in subcommunities associated with lower attrition , and the opposite in subcommunities associated with higher attrition . We conclude with a discussion of ways the modeling approach might be applied , along with caveats from limitations , and directions for future work .
2K_dev_177	Given a large number of taxi trajectories , we would like to find interesting and unexpected patterns from the data . How can we summarize the major trends , and how can we spot anomalies ? The anal- ysis of trajectories has been an issue of considerable interest with many applications such as tracking trails of migrating animals and predicting the path of hurricanes . Several recent works propose methods on clus- tering and indexing trajectories data . However , these approaches are not especially well suited to pattern discovery with respect to the dynamics of social and economic behavior . To further analyze a huge collection of taxi trajectories , we develop a novel method , called F-Trail , w hich al- lows us to find meaningful patterns and anomalies . Our approach has the following advantages : ( a ) it is fast , and scales linearly on the input size , ( b ) it is effective , leading to novel discoveries , and surprising outliers . We demonstrate the effectiveness of our approach , by performing exper- iments on real taxi trajectories . In fact , F-Trail does produce concise , informative and interesting patterns .
2K_dev_178	The present invention discloses CrowdScape , a system that supports the human evaluation of complex crowd work through interactive visualization and mixed initiative machine learning . The system combines information about worker behavior with worker outputs and aggregate worker behavioral traces to allow the isolation of target worker clusters . This approach allows users to develop and test their mental models of tasks and worker behaviors , and then ground those models in worker outputs and majority or gold standard verifications .
2K_dev_179	The amount and the complexity of the data gathered by current enterprises are increasing at an exponential rate . Consequently , the analysis of Big Data is nowadays a central challenge in Computer Science , especially for complex data . For example , given a satellite image database containing tens of Terabytes , how can we find regions aiming at identifying native rainforests , deforestation or reforestation ? Can it be made automatically ? Based on the work discussed in this book , the answers to both questions are a sound yes , and the results can be obtained in just minutes . In fact , results that used to require days or weeks of hard work from human specialists can now be obtained in minutes with high precision . Data Mining in Large Sets of Complex Data discusses new algorithms that take steps forward from traditional data mining ( especially for clustering ) by considering large , complex datasets . Usually , other works focus in one aspect , either data size or complexity . This work considers both : it enables mining complex data from high impact applications , such as breast cancer diagnosis , region classification in satellite images , assistance to climate change forecast , recommendation systems for the Web and social networks ; the data are large in the Terabyte-scale , not in Giga as usual ; and very accurate results are found in just minutes . Thus , it provides a crucial and well timed contribution for allowing the creation of real time applications that deal with Big Data of high complexity in which mining on the fly can make an immeasurable difference , such as supporting cancer diagnosis or detecting deforestation .
2K_dev_180	Structural Control and Health Monitoring Early View ( Online Version of Record published before inclusion in an issue )
2K_dev_181	The increasing performance of modern processors makes virtualization a viable solution for consolidating real-time systems into a single hardware platform . Although real-time task scheduling in a virtual machine can benefit from hierarchical scheduling , unbounded interrupt handling time and vulnerability to interrupt storms make practitioners hesitant to virtualize interrupt-driven real-time applications . In this paper , we propose vINT , an interrupt handling scheme designed for real-time system virtualization . vINT provides a pseudo-VCPU abstraction dedicated for interrupt handling , which overcomes the limits imposed by the timing parameters of virtual CPUs in an analyzable way . vINT also accounts for and enforces interrupt handling and resulting execution flows within a guest virtual machine . vINT does not require any change to the guest OS code , so it can be used for virtualizing proprietary , closed-source OSs . We analyze interrupt handling time as well as VCPU and task schedulability , with and without vINT . Our experimental results indicate that vINT achieves timely interrupt handling while providing as good task schedulability as when it is not used . Our case study based on a prototype implementation on the KVM hyper visor shows that vINT yields significant benefits in reducing interrupt handling time and in protecting real-time tasks against interrupt storms permeating into the virtual machine .
2K_dev_182	Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in incomplete-information games . However , imperfect-recall abstractions are poorly understood , and only weak algorithm-specific guarantees on solution quality are known . In this paper , we show the first general , algorithm-agnostic , solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions , when implemented in the original ( perfect-recall ) game . Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions where any results had been obtained . Further , our analysis is tighter in two ways , each of which can lead to an exponential reduction in the solution quality error bound . We then show that for extensive-form games that satisfy certain properties , the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem , where the increase in our bound is the distance function . This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds . We proceed to show a divide in the class of abstraction problems . If payoffs are at the same scale at all information sets considered for abstraction , the input forms a metric space . Conversely , if this condition is not satisfied , we show that the input does not form a metric space . Finally , we use these results to experimentally investigate the quality of our bound for single-level abstraction .
2K_dev_183	In this paper , we focus on complex event detection in internet videos while also providing the key evidences of the detection results . Convolutional Neural Networks ( CNNs ) have achieved promising performance in image classification and action recognition tasks . However , it remains an open problem how to use CNNs for video event detection and recounting , mainly due to the complexity and diversity of video events . In this work , we propose a flexible deep CNN infrastructure , namely Deep Event Network ( DevNet ) , that simultaneously detects pre-defined events and provides key spatial-temporal evidences . Taking key frames of videos as input , we first detect the event of interest at the video level by aggregating the CNN features of the key frames . The pieces of evidences which recount the detection results , are also automatically localized , both temporally and spatially . The challenge is that we only have video level labels , while the key evidences usually take place at the frame levels . Based on the intrinsic property of CNNs , we first generate a spatial-temporal saliency map by back passing through DevNet , which then can be used to find the key frames which are most indicative to the event , as well as to localize the specific spatial position , usually an object , in the frame of the highly indicative area . Experiments on the large scale TRECVID 2014 MEDTest dataset demonstrate the promising performance of our method , both for event detection and evidence recounting .
2K_dev_184	Formal verification of industrial systems is very challenging , due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams . More importantly , industrial systems are rarely designed for verification , but rather for operational needs . In this paper we present an overview of our experience using hybrid systems theorem proving to formally verify ACAS X , an airborne collision avoidance system for airliners scheduled to be operational around 2020 . The methods and proof techniques presented here are an overview of the work already presented in [ 8 ] , while the evaluation of ACAS X has been significantly expanded and updated to the most recent version of the system , run 13 . The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team .
2K_dev_185	Modern day law enforcement banks heavily on the use of commercial off-the-shelf ( COTS ) face recognition systems ( FRS ) as a tool for biometric evaluation and identification . However , in many real-world scenarios , when the face of an individual is occluded or degraded in some way , commercial recognition systems fail to accept the face for evaluation or simply return unusable matched faces . In these kinds of cases , forensic experts rely on image processing techniques and tools , to make the face fit to be processed by the commercial recognition systems ( e.g . use partial face images from another subject to fill in the occluded parts of the face of interest , or have a tight crop around the face ) . In this study , we evaluate the sensitivity of commercial recognition systems to such forensic techniques . More specifically , we study the change in the rank-1 identification result that is caused by forensic processing of faces-of-interest that are unusable by the commercial recognition systems . Further , forensic processing of such faces is more of an art and it is extremely difficult to process faces consistently such that there is a predictable effect on the rank-n identification result . This study is meant to serve as an evaluation of the effect of a few forensic techniques intended to allow commercial recognition systems to process and match face images that were otherwise unusable . Our results indicate that COTS FRS can be sensitive to the subjectivity in facial part swapping and cropping , resulting in inconsistencies in the identification rankings and similarity scores .
2K_dev_186	Third parties play a prominent role in network-based explanations for successful knowledge transfer . Third parties can be either shared or unshared . Shared third parties signal insider status and have a predictable positive effect on knowledge transfer . Unshared third parties , however , signal outsider status and are believed to undermine knowledge transfer . Surprisingly , unshared third parties have been ignored in empirical analysis , and so we do not know if or how much unshared third parties contribute to the process . Using knowledge transfer data from an online technical forum , we illustrate how unshared third parties affect the rate at which individuals initiate and sustain knowledge transfer relationships . Empirical results indicate that unshared third parties undermine knowledge sharing , and they also indicate that the magnitude of the negative unshared-third-party effect declines the more unshared third parties overlap in what they know . Our results provide a more complete view of how third parties contribute to knowledge sharing . The results also advance our understanding of network-based dynamics defined more broadly . By documenting how knowledge overlap among unshared third parties moderates their negative influence , our results show when the benefits provided by third parties and by bridges i.e. , relationships with outsiders will be opposed versus when both can be enjoyed .
2K_dev_187	How much did a network change since yesterday ? How different is the wiring between Bob 's brain ( a left-handed male ) and Alice 's brain ( a right-handed female ) ? Graph similarity with known node correspondence , i.e . the detection of changes in the connectivity of graphs , arises in numerous settings . In this work , we formally state the axioms and desired properties of the graph similarity functions , and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs . We propose DeltaCon , a principled , intuitive , and scalable algorithm that assesses the similarity between two graphs on the same nodes ( e.g . employees of a company , customers of a mobile carrier ) . Experiments on various synthetic and real graphs showcase the advantages of our method over existing similarity measures . Finally , we employ DeltaCon to real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs , and ( b ) do temporal anomaly detection in the who-emails-whom Enron graph .
2K_dev_188	Why does Smith follow Johnson on Twitter ? In most cases , the reason why users follow other users is unavailable . In this work , we answer this question by proposing TagF , which analyzes the who-follows-whom network ( matrix ) and the who-tags-whom network ( tensor ) simultaneously . Concretely , our method decomposes a coupled tensor constructed from these matrix and tensor . The experimental results on million-scale Twitter networks show that TagF uncovers different , but explainable reasons why users follow other users .
2K_dev_189	A lot of real-world data is spread across multiple domains . Handling such data has been a challenging task . Heterogeneous face biometrics has begun to receive attention in recent years . In real-world scenarios , many surveillance cameras capture data in the NIR ( near infrared ) spectrum . However , most datasets accessible to law enforcement have been collected in the VIS ( visible light ) domain . Thus , there exists a need to match NIR to VIS face images . In this paper , we approach the problem by developing a method to reconstruct VIS images in the NIR domain and vice-versa . This approach is more applicable to real-world scenarios since it does not involve having to project millions of VIS database images into learned common subspace for subsequent matching . We present a cross-spectral joint l 0 minimization based dictionary learning approach to learn a mapping function between the two domains . One can then use the function to reconstruct facial images between the domains . Our method is open set and can reconstruct any face not present in the training data . We present results on the CASIA NIR-VIS v2.0 database and report state-of-the-art results .
2K_dev_190	How often do individuals perform a given communication activity in the Web , such as posting comments on blogs or news ? Could we have a generative model to create communication events with realistic inter-event time distributions ( IEDs ) ? Which properties should we strive to match ? Current literature has seemingly contradictory results for IED : some studies claim good fits with power laws ; others with non-homogeneous Poisson processes . Given these two approaches , we ask : which is the correct one ? Can we reconcile them all ? We show here that , surprisingly , both approaches are correct , being corner cases of the proposed Self-Feeding Process ( SFP ) . We show that the SFP ( a ) exhibits a unifying power , which generates power law tails ( including the so-called `` top-concavity '' that real data exhibits ) , as well as short-term Poisson behavior ; ( b ) avoids the `` i.i.d . fallacy '' , which none of the prevailing models have studied before ; and ( c ) is extremely parsimonious , requiring usually only one , and in general , at most two parameters . Experiments conducted on eight large , diverse real datasets ( e.g. , Youtube and blog comments , e-mails , SMSs , etc ) reveal that the SFP mimics their properties very well .
2K_dev_191	Gaussian processes ( GPs ) are a flexible class of methods with state of the art performance on spatial statistics applications . However , GPs require O ( n3 ) computations and O ( n2 ) storage , and popular GP kernels are typically limited to smoothing and interpolation . To address these difficulties , Kronecker methods have been used to exploit structure in the GP covariance matrix for scalability , while allowing for expressive kernel learning ( Wilson et al. , 2014 ) . However , fast Kronecker methods have been confined to Gaussian likelihoods . We propose new scalable Kronecker methods for Gaussian processes with non-Gaussian likelihoods , using a Laplace approximation which involves linear conjugate gradients for inference , and a lower bound on the GP marginal likelihood for kernel learning . Our approach has near linear scaling , requiring O ( DnD+1/D ) operations and O ( Dn2/D ) storage , for n training data-points on a dense D > 1 dimensional grid . Moreover , we introduce a log Gaussian Cox process , with highly expressive kernels , for modelling spatiotemporal count processes , and apply it to a point pattern ( n 0 233,088 ) of a decade of crime events in Chicago . Using our model , we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts .
2K_dev_192	This article introduces differential hybrid games , which combine differential games with hybrid games . In both kinds of games , two players interact with continuous dynamics . The difference is that hybrid games also provide all the features of hybrid systems and discrete games , but only deterministic differential equations . Differential games , instead , provide differential equations with continuous-time game input by both players , but not the luxury of hybrid games , such as mode switches and discrete-time or alternating adversarial interaction . This article augments differential game logic with modalities for the combined dynamics of differential hybrid games . It shows how hybrid games subsume differential games and introduces differential game invariants and differential game variants for proving properties of differential games inductively .
2K_dev_193	Most state-of-the-art action feature extractors involve differential operators , which act as highpass filters and tend to attenuate low frequency action information . This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices . The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation . However , at the core of the Gaussian Pyramid is a convolutional smoothing operation , which makes it incapable of generating new features at coarse scales . In order to address this problem , we propose a novel feature enhancing technique called Multi-skIp Feature Stacking ( MIFS ) , which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space . MIFS compensates for information lost from using differential operators by recapturing information at coarse scales . This recaptured information allows us to match actions at different speeds and ranges of motion . We prove that MIFS enhances the learnability of differential-based features exponentially . The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods . Experimental results show significantly improved performance on challenging action recognition and event detection tasks . Specifically , our method exceeds the state-of-the-arts on Hollywood2 , UCF101 and UCF50 datasets and is comparable to state-of-the-arts on HMDB51 and Olympics Sports datasets . MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost .
2K_dev_194	Biometrics has come a long way over the past decade in terms of technologies and devices that are used to verify user identities . Three of the more well studied modalities in this field are the face , iris and fingerprint , with the latter two reporting very high user identification/verification rates . In the biometric community there has been little work in studying biomedical signals for user recognition purposes . In this paper , we propose using electromyograph ( EMG ) signals as a person 's biometric signature . The EMG records the motor unit action potentials ( MUAP ) during any physical motion . Our study is done within the context of a person using a keyboard to type a password or any other fixed phrase . Along with EMG signals , we log key press times for the user and study the feasibility of using this data too as a biometric feature . Keypress timings alone if used as a biometric , are very easy to spoof and hence we fuse this modality with EMG signals . In order to classify these features , we use subspace modeling as well as Bayesian classifiers . The experiments have been performed within the context of a user typing a fixed pass phrase at a workstation . The idea is to monitor both biometric modalities when this action is performed and study user verification across data capture sessions and within capture sessions . Our approach yields high values of verification rates , which shows the promise of using these modalities as user specific biometric signatures .
2K_dev_195	Generative score spaces provide a principled method to exploit generative information , e.g. , data distribution and hidden variables , in discriminative classifiers . The underlying methodology is to derive measures or score functions from generative models . The derived score functions , spanning the so-called score space , provide features of a fixed dimension for discriminative classification . In this paper , we propose a simple yet effective score space which is essentially the sufficient statistics of the adopted generative models and does not involve the parameters of generative models . We further propose a discriminative learning method for the score space that seeks to utilize label information by constraining the classification margin over the score space . The form of score function allows the formulation of simple learning rules , which are essentially the same learning rules for a generative model with an extra posterior imposed over its hidden variables . Experimental evaluation of this approach over two generative models shows that performance of the score space approach coupled with the proposed discriminative learning method is competitive with state-of-the-art classification methods .
2K_dev_196	A group 's collective action is an outcome of the group 's decision-making process , which may be reached by either averaging of the individual preferences or following the choices of certain members in the group . Our problem here is to decide which decision process the group has adopted given the data of the collective actions . We propose a generic statistical framework to infer the group 's decision process from the spatio-temporal data of group trajectories , where each `` trajectory '' is a sequence of group actions . This is achieved by systematically comparing each agent type 's influence on the group actions based on an array of spatio-temporal criteria . Results of those comparisons are then aggregated into a score to make inference about the group 's decision process .
2K_dev_197	Background Latent HIV-1 reservoirs are identified as one of the major challenges to achieve HIV-1 cure . Currently available strategies are associated with wide variability in outcomes both in patients and CD4+ T cell models . This underlines the critical need to develop innovative strategies to predict and recognize ways that could result in better reactivation and eventual elimination of latent HIV-1 reservoirs .
2K_dev_198	Most real-world games and many recreational games are games of incomplete information . Over the last dozen years , abstraction has emerged as a key enabler for solving large incomplete-information games . First , the game is abstracted to generate a smaller , abstract game that is strategically similar to the original game . Second , an approximate equilibrium is computed in the abstract game . Third , the strategy from the abstract game is mapped back to the original game . In this paper , I will review key developments in the field . I present reasons for abstracting games , and point out the issue of abstraction pathology . I then review the practical algorithms for information abstraction and action abstraction . I then cover recent theoretical breakthroughs that beget bounds on the quality of the strategy from the abstract game , when measured in the original game . I then discuss how to reverse map the opponent 's action into the abstraction if the opponent makes a move that is not in the abstraction . Finally , I discuss other topics of current and future research .
2K_dev_199	Bayesian nonparametric models , such as Gaussian processes , provide a compelling framework for automatic statistical modelling : these models have a high degree of flexibility , and automatically calibrated complexity . However , automating human expertise remains elusive ; for example , Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners . In this paper , we create function extrapolation problems and acquire human responses , and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments . We use the learned kernels to gain psychological insights and to extrapolate in humanlike ways that go beyond traditional stationary and polynomial kernels . Finally , we investigate Occam 's razor in human and Gaussian process based function learning .
2K_dev_200	This paper introduces a new proof calculus for differential dynamic logic ( dL ) that is entirely based on uniform substitution , a proof rule that substitutes a formula for a predicate symbol everywhere . Uniform substitutions make it possible to rely on axioms rather than axiom schemata , substantially simplifying implementations . Instead of nontrivial schema variables and soundness-critical side conditions on the occurrence patterns of variables , the resulting calculus adopts only a finite number of ordinary dL formulas as axioms . The static semantics of differential dynamic logic is captured exclusively in uniform substitutions and bound variable renamings as opposed to being spread in delicate ways across the prover implementation . In addition to sound uniform substitutions , this paper introduces differential forms for differential dynamic logic that make it possible to internalize differential invariants , differential substitutions , and derivations as first-class axioms in dL .
2K_dev_201	While the growth of the mobile apps market has created significant market opportunities and economic incentives for mobile app developers to innovate , it has also inevitably invited other developers to create rip-offs . Practitioners and developers of original apps claim that copycats steal the original apps idea and demand and have called for app platforms to take action against such copycats . Surprisingly , however , there has been little rigorous research analyzing whether and how copycats affect an original apps demand . The primary deterrent to such research is the lack of an objective way to identify similarities between different apps . Using a combination of machine learning techniques such as natural language processing , latent semantic analysis , network-based clustering and image analysis , we propose a method to compare apps and detect two types of copycats : deceptive and non-deceptive . Based on the detection results , we conduct an econometric analysis to determine the impact of copycat apps on the demand for the original apps on a sample of 10,100 action game apps by 5,141 developers that were released in the iOS App Store over five years . Our final results indicate that the effect of copycats on an original apps demand is determined by the quality and level of imitation of the copycat . High-quality , non-deceptive copycats negatively affect demand for the originals . In contrast , low-quality , deceptive copycats positively affect demand for the originals . Our study contributes to the growing literature on mobile app consumption by presenting a method to identify copycats and providing evidence of the impact of copycats on an original apps demand .
2K_dev_202	Hidden information derived from probabilistic generative models of data distributions can be used to construct features for discriminative classifiers . This observation has motivated the development of approaches that attempt to couple generative and discriminative models together for classification . However , existing approaches typically feed features derived from generative models to discriminative classifiers , and do not refine the generative models or the feature mapping functions based on classification results . In this paper , we propose a coupling mechanism developed under the PAC-Bayes framework that can fine-tune the generative models and the feature mapping functions iteratively to improve the classifier 's performance . In our approach , a stochastic feature mapping , which is a function over the random variables of a generative model , is derived to generate feature vectors for a stochastic classifier . We construct a stochastic classifier over the feature mapping and derive the PAC-Bayes generalization bound for the classifier , for both supervised and semi-supervised learning . This allows us to jointly learn the feature mapping and the classifier by minimizing the bound with an EM-like iterative algorithm using labeled and unlabeled data . The resulting framework integrates the learning of the discriminative classifier and the generative model and allows iterative fine-tuning of the generative models , and the feedforward feature mappings based on task performance feedback . Our experiments show , in three distinct applications , this new framework produces a general classification tool with state-of-the-art performance .
2K_dev_203	What do real communities in social networks look like ? Community detection plays a key role in understanding the structure of real-life graphs with impact on recommendation systems , load balancing and routing . Previous community detection methods look for uniform blocks in adjacency matrices . However , after studying four real networks with ground-truth communities , we provide empirical evidence that communities are best represented as having an hyperbolic structure . We detail HyCoM - the Hyperbolic Community Model - as a better representation of communities and the relationships between their members , and show improvements in compression compared to standard methods . We also introduce HyCoM-FIT , a fast , parameter free algorithm to detect communities with hyperbolic structure . We show that our method is effective in finding communities with a similar structure to self-declared ones . We report findings in real social networks , including a community in a blogging platform with over 34 million edges in which more than 1000 users established over 300 000 relations .
2K_dev_204	In this paper , we propose a discriminative video representation for event detection over a large scale video dataset when only limited hardware resources are available . The focus of this paper is to effectively leverage deep Convolutional Neural Networks ( CNNs ) to advance event detection , where only frame level static descriptors can be extracted by the existing CNN toolkits . This paper makes two contributions to the inference of CNN video representation . First , while average pooling and max pooling have long been the standard approaches to aggregating frame level static features , we show that performance can be significantly improved by taking advantage of an appropriate encoding method . Second , we propose using a set of latent concept descriptors as the frame descriptor , which enriches visual information while keeping it computationally affordable . The integration of the two contributions results in a new state-of-the-art performance in event detection over the largest video datasets . Compared to improved Dense Trajectories , which has been recognized as the best video representation for event detection , our new representation improves the Mean Average Precision ( mAP ) from 27.6 % to 36.8 % for the TRECVID MEDTest 14 dataset and from 34.0 % to 44.6 % for the TRECVID MEDTest 13 dataset .
2K_dev_205	We propose a new neurally-inspired model that can learn to encode the global relationship context of visual events across time and space and to use the contextual information to modulate the analysis by synthesis process in a predictive coding framework . The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes . In contrast to standard predictive coding models , the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer , and is thus more consistent with neurophysiological observations . We establish the computational feasibility of this model by demonstrating its ability in several aspects . We show that our model can outperform state-of-art performances of gated Boltzmann machines ( GBM ) in estimation of contextual information . Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information . We show it achieves state-of-art performances in terms of prediction accuracy in a variety of tasks and possesses the ability to interpolate missing frames , a function that is lacking in GBM .
2K_dev_206	We address the problem how high-fidelity verification results about the hybrid systems dynamics of cyber-physical flow systems can be provided at the scale of large ( traffic ) networks without prohibitive analytic cost . We propose the use of contracts for traffic flow components concisely capturing the conditions for a safe operation in the context of a traffic network . This reduces the analysis of flows in the full traffic network to simple arithmetic checks of the local compatibility of the traffic component contracts , while retaining higher-fidelity correctness guarantees of the global hybrid systems models that inherits from correct contracts of the hybrid system components . We evaluate our approach in a case study of a modular traffic network and a prototypical implementation in a model-based analysis and design tool for traffic flow networks .
2K_dev_207	Background Computer-assisted diagnosis of dermoscopic images of skin lesions has the potential to improve melanoma early detection . Objective We sought to evaluate the performance of a novel classifier that uses decision forest classification of dermoscopic images to generate a lesion severity score . Methods Severity scores were calculated for 173 dermoscopic images of skin lesions with known histologic diagnosis ( 39 melanomas , 14 nonmelanoma skin cancers , and 120 benign lesions ) . A threshold score was used to measure classifier sensitivity and specificity . A reader study was conducted to compare the sensitivity and specificity of the classifier with those of 30 dermatology clinicians . Results The classifier sensitivity for melanoma was 97.4 % ; specificity was 44.2 % in a test set of images . In the reader study , the classifier 's sensitivity to melanoma was higher ( P P Limitations This is a retrospective study using existing images primarily chosen for biopsy by a dermatologist . The size of the test set is small . Conclusions Our classifier may aid clinicians in deciding if a skin lesion should be biopsied and can easily be incorporated into a portable tool ( that uses no proprietary equipment ) that could aid clinicians in noninvasively evaluating cutaneous lesions .
2K_dev_208	Most Lamb wave localization techniques require that we know the waves velocity characteristics ; yet , in many practical scenarios , velocity estimates can be challenging to acquire , are unavailable , or are unreliable because of the complexity of Lamb waves . As a result , there is a significant need for new methods that can reduce a systems reliance on a priori velocity information . This paper addresses this challenge through two novel source localization methods designed for sparse sensor arrays in isotropic media . Both methods exploit the fundamental sparse structure of a Lamb wave 's frequencywavenumber representation . The first method uses sparse recovery techniques to extract velocities from calibration data . The second method uses kurtosis and the support earth movers distance to measure the sparseness of a Lamb waves approximate frequency-wavenumber representation . These measures are then used to locate acoustic sources with no prior calibration data . We experimentally study each method with a collection of acoustic emission data measured from a 1.22 m by 1.22 m isotropic aluminum plate . We show that both methods can achieve less than 1 cm localization error and have less systematic error than traditional time-of-arrival localization methods .
2K_dev_209	In this paper we introduce a paradigm for completing complex tasks from wearable devices by leveraging crowdsourcing , and demonstrate its validity for academic writing . We explore this paradigm using a collaborative authoring system , called WearWrite , which is designed to enable authors and crowd workers to work together using an Android smartwatch and Google Docs to produce academic papers , including this one . WearWrite allows expert authors who do not have access to large devices to contribute bits of expertise and big picture direction from their watch , while freeing them of the obligation of integrating their contributions into the overall document . Crowd workers on desktop computers actually write the document . We used this approach to write several simple papers , and found it was effective at producing reasonable drafts . However , the workers often needed more structure and the authors more context . WearWrite addresses these issues by focusing workers on specific tasks and providing select context to authors on the watch . We demonstrate the system 's feasibility by writing this paper using it .
2K_dev_210	Road intersections are considered to be serious bottlenecks in urban transportation . More than 44 % of all reported crashes in U.S . Occur within intersection areas , which in turn lead to 8,500 fatalities and approximately 1 million injuries every year . Furthermore , because traffic traveling in one direction is generally stopped at busy intersections to allow traffic to flow in another direction , an intersection creates traffic congestion and frustration . The impact of road intersections on traffic delays leads to enormous waste of human and natural resources . According to the 2011 Urban Mobility Report , the delay endured by the average commuter was 34 hours , which costs in aggregate more than $ 100 billion each year in the U.S. With the advances in Cyber-Physical Systems ( CPS ) , autonomous driving as a part of Intelligent Transportation Systems ( ITS ) is likely to be at the heart of urban transportation in the future . Autonomous vehicles have been demonstrated successfully at the DARPA Urban Challenge . General Motors ' Electrical-Networked Vehicle , CMU 's autonomous vehicle and Google 's car are just a few other recently unveiled examples . Therefore , it is critical to address safety and throughput concerns as one of the main challenges for autonomous driving through intersections . In this paper , we propose a spatio-temporal technique called the Ballroom Intersection Protocol ( BRIP ) to manage the safe and efficient passage of autonomous vehicles through intersections . To achieve high throughput at intersections , BRIP aims to maximize the utilization of the capacity of the intersection area by increasing parallelism . By enforcing a synchronized arrival of autonomous vehicles at intersections , BRIP allows vehicles approaching from all directions to simultaneously and continuously cross without stopping behind or inside the intersection area . Our simulation results show that we are able to avoid collisions and increase the throughput of the intersections by up to 96.24 % compared to common signalized intersections . Under BRIP , the optimal intersection capacity utilization of 100 % is achievable in certain cases .
2K_dev_211	The harmful effects of cell phone usage on driver behavior have been well investigated and the growing problem has motivated several several research efforts aimed at developing automated cell phone usage detection systems . Computer vision based approaches for dealing with this problem have only emerged in recent years . In this paper , we present a vision based method to automatically determine if a driver is holding a cell phone close to one of his/her ears ( thus keeping only one hand on the steering wheel ) and quantitatively demonstrate the method 's efficacy on challenging Strategic Highway Research Program ( SHRP2 ) face view videos from the head pose validation data that was acquired to monitor driver head pose variation under naturalistic driving conditions . To the best of our knowledge , this is the first such evaluation carried out using this relatively new data . Our approach utilizes the Supervised Descent Method ( SDM ) based facial landmark tracking algorithm to track the locations of facial landmarks in order to extract a crop of the region of interest . Following this , features are extracted from the crop and are classified using previously trained classifiers in order to determine if a driver is holding a cell phone . We adopt a through approach and benchmark the performance obtained using raw pixels and Histogram of Oriented Gradients ( HOG ) features in combination with various classifiers .
2K_dev_212	The use of deductive techniques , such as theorem provers , has several advantages in safety verification of hybrid systems ; however , state-of-the-art theorem provers require manual intervention to handle complex systems . Furthermore , there is often a gap between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide directly . This paper presents an extension to KeYmaera , a deductive verification tool for differential dynamic logic ; the new technique allows local reasoning using system designer intuition about performance within particular modes as part of a proof task . Our approach allows the theorem prover to leverage forward invariants , discovered using numerical techniques , as part of a proof of safety . We introduce a new inference rule into the proof calculus of KeYmaera , the forward invariant cut rule , and we present a methodology to discover useful forward invariants , which are then used with the new cut rule to complete verification tasks . We demonstrate how our new approach can be used to complete verification tasks that lie out of the reach of existing automatic verification approaches using several examples , including one involving an automotive powertrain control system .
2K_dev_213	In recent years , many lawsuits have been filed by individuals seeking legal redress for harms caused by the loss or theft of their personal information . However , very little is known about the drivers , mechanics , and outcomes of those lawsuits , making it difficult to assess the effectiveness of litigation at balancing organizations ' usage of personal data with individual privacy rights . Using a unique and manually collected database , we analyze court dockets for more than 230 federal data breach lawsuits from 2000 to 2010 . We investigate two questions : Which data breaches are being litigated ? and Which data breach lawsuits are settling ? Our results suggest that the odds of a firm being sued are 3.5 times greater when individuals suffer financial harm , but 6 times lower when the firm provides free credit monitoring . Moreover , defendants settle 30 percent more often when plaintiffs allege financial loss , or when faced with a certified class action suit . By providing the first comprehensive empirical analysis of data breach litigation , our findings offer insight into the debate over privacy litigation versus privacy regulation .
2K_dev_214	Epidemics in large complete networks is well established . In contrast , we consider epidemics in non-complete networks . We establish the fluid limit macroscopic dynamics of a multi-virus spread over a multipartite network as the number of nodes at each partite or island grows large . The virus spread follows a peer-to-peer random rule of infection in line with the Harris contact process . The model conforms to an SIS ( susceptible-infected-susceptible ) type , where a node is either infected or it is healthy and prone to be infected . The local ( at node level ) random infection model induces the emergence of structured dynamics at the macroscale . Namely , we prove that , as the multipartite network grows large , the normalized Markov jump vector process $ \left ( \bar { \mathbf { Y } } ^\mathbf { N } ( t ) \right ) 0 \left ( \bar { Y } _1^\mathbf { N } ( t ) , \ldots , \bar { Y } _M^\mathbf { N } ( t ) \right ) $ collecting the fraction of infected nodes at each island $ i=1 , \ldots , M $ , converges weakly ( with respect to the Skorokhod topology on the space of \emph { c\` { a } dl\` { a } g } sample paths ) to the solution of an $ M $ -dimensional vector nonlinear coupled ordinary differential equation . In the case of multi-virus diffusion with $ K\in\mathbb { N } $ distinct strains of virus , the Markov jurmp matrix process $ \left ( \bar { \mathbf { Y } } ^\mathbf { N } ( t ) \right ) $ , stacking the fraction of nodes infected with virus type $ j $ , $ j=1 , \ldots , K $ , at each island $ i=1 , \ldots , M $ , converges weakly as well to the solution of a $ \left ( K\times M\right ) $ -dimensional vector differential equation that is also characterized .
2K_dev_215	We apply a novel semantic scan statistic approach to solve a problem posed by the NC DETECT team , North Carolina Division of Public Health ( NC DPH ) and UNC Department of Emergency Medicine Carolina Center for Health Informatics , and facilitated by the ISDS Technical Conventions Committee . This use case identifies a need for methodology that detects emerging , potentially novel outbreaks in free-text emergency department ( ED ) chief complaint data . Our semantic scan approach successfully addresses this problem , eliminates the need for classifying cases into pre-defined syndromes and identifies emerging clusters that public health officials could not have predicted in advance .
2K_dev_216	Given a large cloud of multi-dimensional points , and an off-the shelf outlier detection method , why does it take a week to finish ? After careful analysis , we discovered that duplicate points create subtle issues , that the literature has ignored : if d max is the multiplicity of the most over-plotted point , typical algorithms are quadratic on d max . We propose several ways to eliminate the problem ; we report wall-clock times and our time savings ; and we show that our methods give either exact results , or highly accurate approximate ones .
2K_dev_217	Instant access to computing , when and where we need it , has long been one of the aims of research areas such as ubiquitous computing . In this paper , we describe the WorldKit system , which makes use of a paired depth camera and projector to make ordinary surfaces instantly interactive . Using this system , touch-based interactivity can , without prior calibration , be placed on nearly any unmodified surface literally with a wave of the hand , as can other new forms of sensed interaction . From a user perspective , such interfaces are easy enough to instantiate that they could , if desired , be recreated or modified `` each time we sat down '' by `` painting '' them next to us . From the programmer 's perspective , our system encapsulates these capabilities in a simple set of abstractions that make the creation of interfaces quick and easy . Further , it is extensible to new , custom interactors in a way that closely mimics conventional 2D graphical user interfaces , hiding much of the complexity of working in this new domain . We detail the hardware and software implementation of our system , and several example applications built using the library .
2K_dev_218	We propose using the statistical measurement of the sample skewness of the distribution of mean firing rates of a tuning curve to quantify sharpness of tuning . For some features , like binocular disparity , tuning curves are best described by relatively complex and sometimes diverse functions , making it difficult to quantify sharpness with a single function and parameter . Skewness provides a robust nonparametric measure of tuning curve sharpness that is invariant with respect to the mean and variance of the tuning curve and is straightforward to apply to a wide range of tuning , including simple orientation tuning curves and complex object tuning curves that often can not even be described parametrically . Because skewness does not depend on a specific model or function of tuning , it is especially appealing to cases of sharpening where recurrent interactions among neurons produce sharper tuning curves that deviate in a complex manner from the feedforward function of tuning . Since tuning curves for all neurons are not typically well described by a single parametric function , this model independence additionally allows skewness to be applied to all recorded neurons , maximizing the statistical power of a set of data . We also compare skewness with other nonparametric measures of tuning curve sharpness and selectivity . Compared to these other nonparametric measures tested , skewness is best used for capturing the sharpness of multimodal tuning curves defined by narrow peaks maximum and broad valleys minima . Finally , we provide a more formal definition of sharpness using a shape-based information gain measure and derive and show that skewness is correlated with this definition .
2K_dev_219	How can we describe a large , dynamic graph over time ? Is it random ? If not , what are the most apparent deviations from randomness -- a dense block of actors that persists over time , or perhaps a star with many satellite nodes that appears with some fixed periodicity ? In practice , these deviations indicate patterns -- for example , botnet attackers forming a bipartite core with their victims over the duration of an attack , family members bonding in a clique-like fashion over a difficult period of time , or research collaborations forming and fading away over the years . Which patterns exist in real-world dynamic graphs , and how can we find and rank them in terms of importance ? These are exactly the problems we focus on in this work . Our main contributions are ( a ) formulation : we show how to formalize this problem as minimizing the encoding cost in a data compression paradigm , ( b ) algorithm : we propose TIMECRUNCH , an effective , scalable and parameter-free method for finding coherent , temporal patterns in dynamic graphs and ( c ) practicality : we apply our method to several large , diverse real-world datasets with up to 36 million edges and 6.3 million nodes . We show that TIMECRUNCH is able to compress these graphs by summarizing important temporal structures and finds patterns that agree with intuition .
2K_dev_220	Generating three-dimensional ( 3D ) as-is Building Information Models ( BIMs ) , representative of the existing conditions of buildings , from point cloud data collected by laser scanners is becoming common practice . However , generation of such models currently is mostly performed manually , and errors can be introduced during data collection , pre-processing , and modeling . This paper presents a method for assessing the quality of as-is BIMs generated from point cloud data by analyzing the patterns of geometric deviations between the model and the point cloud data . The fundamental assumption is that the point cloud and the as-is BIM generated from the point cloud should corroborate in the depiction of the components and their spatial attributes . Major geometric deviations between as-is models and point clouds can indicate potential errors introduced during data collection , processing and/or model generation . The research described in this paper provides a taxonomy for patterns of deviations and sources of errors and demonstrates that it is possible to identify the source , magnitude , and nature of errors by analyzing the deviation patterns . The method is validated through a comparison with the currently adopted physical measurement method in a case study . The results show that the deviation analysis method is capable of identifying almost six times more errors with more than 40 % time savings compared to the physical measurement method .
2K_dev_221	Learning , whether motor , sensory or cognitive , requires networks of neurons to generate new activity patterns . As some behaviours are easier to learn than others1 , 2 , we asked if some neural activity patterns are easier to generate than others . Here we investigate whether an existing network constrains the patterns that a subset of its neurons is capable of exhibiting , and if so , what principles define this constraint . We employed a closed-loop intracortical braincomputer interface learning paradigm in which Rhesus macaques ( Macaca mulatta ) controlled a computer cursor by modulating neural activity patterns in the primary motor cortex . Using the braincomputer interface paradigm , we could specify and alter how neural activity mapped to cursor velocity . At the start of each session , we observed the characteristic activity patterns of the recorded neural population . The activity of a neural population can be represented in a high-dimensional space ( termed the neural space ) , wherein each dimension corresponds to the activity of one neuron . These characteristic activity patterns comprise a low-dimensional subspace ( termed the intrinsic manifold ) within the neural space . The intrinsic manifold presumably reflects constraints imposed by the underlying neural circuitry . Here we show that the animals could readily learn to proficiently control the cursor using neural activity patterns that were within the intrinsic manifold . However , animals were less able to learn to proficiently control the cursor using activity patterns that were outside of the intrinsic manifold . These results suggest that the existing structure of a network can shape learning . On a timescale of hours , it seems to be difficult to learn to generate neural activity patterns that are not consistent with the existing network structure . These findings offer a network-level explanation for the observation that we are more readily able to learn new skills when they are related to the skills that we already possess3 , 4 .
2K_dev_222	AbstractUnderstanding the value that individuals assign to the protection of their personal data is of great importance for business , law , and public policy . We use a field experiment informed by behavioral economics and decision research to investigate individual privacy valuations and find evidence of endowment and order effects . Individuals assigned markedly different values to the privacy of their data depending on ( 1 ) whether they were asked to consider how much money they would accept to disclose otherwise private information or how much they would pay to protect otherwise public information and ( 2 ) the order in which they considered different offers for their data . The gap between such values is large compared with that observed in comparable studies of consumer goods . The results highlight the sensitivity of privacy valuations to contextual , nonnormative factors .
2K_dev_223	Modern offices are crowded with personal computers . While studies have shown these to be idle most of the time , they remain powered , consuming up to 60p of their peak power . Hardware-based solutions engendered by PC vendors ( e.g. , low-power states , Wake-on-LAN ) have proved unsuccessful because , in spite of user inactivity , these machines often need to remain network active in support of background applications that maintain network presence . Recent proposals have advocated the use of consolidation of idle desktop Virtual Machines ( VMs ) . However , desktop VMs are often large , requiring gigabytes of memory . Consolidating such VMs creates large network transfers lasting in the order of minutes and utilizes server memory inefficiently . When multiple VMs migrate concurrently , networks become congested , and the resulting migration latencies are prohibitive . We present partial VM migration , an approach that transparently migrates only the working set of an idle VM . It creates a partial replica of the desktop VM on the consolidation server by copying only VM metadata , and it transfers pages to the server on-demand , as the VM accesses them . This approach places desktop PCs in low-power mode when inactive and switches them to running mode when pages are needed by the VM running on the consolidation server . To ensure that desktops save energy , we have developed sleep scheduling and prefetching algorithms , as well as the context-aware selective resume framework , a novel approach to reduce the latency of power mode transition operations in commodity PCs . Jettison , our software prototype of partial VM migration for off-the-shelf PCs , can deliver 44 -- 91p energy savings during idle periods of at least 10 minutes , while providing low migration latencies of about 4 seconds and migrating minimal state that is under an order of magnitude of the VMs memory footprint .
2K_dev_224	Event detection from real surveillance videos with complicated background environment is always a very hard task . Different from the traditional retrospective and interactive systems designed on this task , which are mainly executed on video fragments located within the event-occurrence time , in this paper we propose a new interactive system constructed on the mid-level discriminative representations ( patches/shots ) which are closely related to the event ( might occur beyond the event-occurrence period ) and are easier to be detected than video fragments . By virtue of such easily-distinguished mid-level patterns , our framework realizes an effective labor division between computers and human participants . The task of computers is to train classifiers on a bunch of mid-level discriminative representations , and to sort all the possible mid-level representations in the evaluation sets based on the classifier scores . The task of human participants is then to readily search the events based on the clues offered by these sorted mid-level representations . For computers , such mid-level representations , with more concise and consistent patterns , can be more accurately detected than video fragments utilized in the conventional framework , and on the other hand , a human participant can always much more easily search the events of interest implicated by these location-anchored mid-level representations than conventional video fragments containing entire scenes . Both of these two properties facilitate the availability of our framework in real surveillance event detection applications .
2K_dev_225	The Gartner 's 2014 Hype Cycle released last August moves Big Data technology from the Peak of Inflated Expectations to the beginning of the Trough of Disillusionment when interest starts to wane as reality does not live up to previous promises . As the hype is starting to dissipate it is worth asking what Big Data ( however defined ) means from a scientific perspective : Did the emergence of gigantic corpora exposed the limits of classical information retrieval and data mining and led to new concepts and challenges , the way say , the study of electromagnetism showed the limits of Newtonian mechanics and led to Relativity Theory , or is it all just `` sound and fury , signifying nothing '' , simply a matter of scaling up well understood technologies ? To answer this question , we have assembled a distinguished panel of eminent scientists , from both Industry and Academia : Lada Adamic ( Facebook ) , Michael Franklin ( University of California at Berkeley ) , Maarten de Rijke ( University of Amsterdam ) , Eric Xing ( Carnegie Mellon University ) , and Kai Yu ( Baidu ) will share their point of view and take questions from the moderator and the audience .
2K_dev_226	We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems . These problems are motivated by the LASSO framework and have applications in machine learning and computer vision . Our work shows a close connection between these problems and core questions in algorithmic graph theory . While this connection demonstrates the difficulties of obtaining runtime guarantees , it also suggests an approach of using techniques originally developed for graph algorithms . We then show that most of these problems can be formulated as a grouped least squares problem , and give efficient algorithms for this formulation . Our algorithms rely on routines for solving quadratic minimization problems , which in turn are equivalent to solving linear systems . Some preliminary experimental work on image processing tasks are also presented .
2K_dev_227	Fluctuations in the growth rate of a bacterial culture during unbalanced growth are generally considered undesirable in quantitative studies of bacterial physiology . Under well-controlled experimental conditions , however , these fluctuations are not random but instead reflect the interplay between intra-cellular networks underlying bacterial growth and the growth environment . Therefore , these fluctuations could be considered quantitative phenotypes of the bacteria under a specific growth condition . Here , we present a method to identify phenotypic signatures by time-frequency analysis of unbalanced growth curves measured with high temporal resolution . The signatures are then applied to differentiate amongst different bacterial strains or the same strain under different growth conditions , and to identify the essential architecture of the gene network underlying the observed growth dynamics . Our method has implications for both basic understanding of bacterial physiology and for the classification of bacterial strains .
2K_dev_228	At the core of Machine Learning ( ML ) analytics is often an expert-suggested model , whose parameters are refined by iteratively processing a training dataset until convergence . The completion time ( i.e . convergence time ) and quality of the learned model not only depends on the rate at which the refinements are generated but also the quality of each refinement . While data-parallel ML applications often employ a loose consistency model when updating shared model parameters to maximize parallelism , the accumulated error may seriously impact the quality of refinements and thus delay completion time , a problem that usually gets worse with scale . Although more immediate propagation of updates reduces the accumulated error , this strategy is limited by physical network bandwidth . Additionally , the performance of the widely used stochastic gradient descent ( SGD ) algorithm is sensitive to step size . Simply increasing communication often fails to bring improvement without tuning step size accordingly and tedious hand tuning is usually needed to achieve optimal performance . This paper presents Bosen , a system that maximizes the network communication efficiency under a given inter-machine network bandwidth budget to minimize parallel error , while ensuring theoretical convergence guarantees for large-scale data-parallel ML applications . Furthermore , Bosen prioritizes messages most significant to algorithm convergence , further enhancing algorithm convergence . Finally , Bosen is the first distributed implementation of the recently presented adaptive revision algorithm , which provides orders of magnitude improvement over a carefully tuned fixed schedule of step size refinements for some SGD algorithms . Experiments on two clusters with up to 1024 cores show that our mechanism significantly improves upon static communication schedules .
2K_dev_229	Autonomous driving will play an important role in the future of transportation . Various autonomous vehicles have been demonstrated at the DARPA Urban Challenge [ 3 ] . General Motors has recently unveiled their Electrical-Networked Vehicles ( EN-V ) in Shanghai , China [ 5 ] . One of the main challenges of autonomous driving in urban areas is transition through cross-roads and intersections . In addition to safety concerns , current intersection management technologies such as stop signs and traffic lights can introduce significant traffic delays even under light traffic conditions . Our goal is to design and develop efficient and reliable intersection protocols to avoid vehicle collisions at intersections and increase the traffic throughput . The focus of this paper is investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous vehicles . We study how our proposed V2V intersection protocols can be beneficial for autonomous driving , and show significant improvements in throughput . We also prove that our protocols avoid deadlock situations inside the intersection area . The simulation results show that our new proposed V2V intersection protocols provide both safe passage through the intersection and significantly decrease the delay at the intersection and our latest V2V intersection protocol yields over 85 % overall performance improvement over the common traffic light models .
2K_dev_230	Massive Open Online Courses ( MOOCs ) enable everyone to receive high-quality education . However , current MOOC creators can not provide an effective , economical , and scalable method to detect cheating on tests , which would be required for any certification . In this paper , we propose a Massive Open Online Proctoring ( MOOP ) framework , which combines both automatic and collaborative approaches to detect cheating behaviors in online tests . The MOOP framework consists of three major components : Automatic Cheating Detector ( ACD ) , Peer Cheating Detector ( PCD ) , and Final Review Committee ( FRC ) . ACD uses webcam video or other sensors to monitor students and automatically flag suspected cheating behavior . Ambiguous cases are then sent to the PCD , where students peer-review flagged webcam video to confirm suspicious cheating behaviors . Finally , the list of suspicious cheating behaviors is sent to the FRC to make the final punishing decision . Our experiment show that ACD and PCD can detect usage of a cheat sheet with good accuracy and can reduce the overall human resources required to monitor MOOCs for cheating .
2K_dev_231	Visible light communication ( VLC ) between LED light bulbs and smart-phone cameras has already begun to gain traction for identification and indoor localization applications . To support detection by cameras , the frequencies and data rates are typically limited to below 1kHz and tens of bytes per second ( Bps ) . In this paper , we present a technique for transmitting data from solid-state luminaries , used for interior ambient lighting , simultaneously to both cameras and low-power embedded devices in a manner that is imperceptible to occupants . This allows the camera communication VLC channel to also act as a higher speed downstream link and low-power wakeup mechanism for energy-constrained devices . Our approach uses Manchester encoding and Binary Frequency Shift Keying ( BFSK ) to modulate the high-speed data stream and applies duty-cycle adjustment to generate the slower camera communication signal . We explore the trade-off between the performance of the two communication channels . Our hybrid communication protocol is also compatible with existing IR receivers . This allows lights to communicate with low-cost commodity chipsets and control home appliances such as TVs , AV receivers , AC window units , etc . We show that we are able to reliably simultaneously transmit low-speed data at 1.3 Bps to camera enabled devices and higher-speed data at 104 Bps to low-power embedded devices . Since the majority of energy in many RF communication protocols often goes towards media access and receiving , VLC-triggered wakeup can significantly decrease system energy consumption . We also demonstrate a proof-of-concept wakeup circuit that consumes less then 204 uA and can be triggered in less then 10ms .
2K_dev_232	We present GraphScan , a novel method for detecting arbitrarily shaped connected clusters in graph or network data . Given a graph structure , data observed at each node , and a score function defining the anomalousness of a set of nodes , GraphScan can efficiently and exactly identify the most anomalous ( highest-scoring ) connected subgraph . Kulldorffs spatial scan , which searches over circles consisting of a center location and its k 1 nearest neighbors , has been extended to include connectivity constraints by FlexScan . However , FlexScan performs an exhaustive search over connected subsets and is computationally infeasible for k > 30 . Alternatively , the upper level set ( ULS ) scan scales well to large graphs but is not guaranteed to find the highest-scoring subset . We demonstrate that GraphScan is able to scale to graphs an order of magnitude larger than FlexScan , while guaranteeing that the highest-scoring subgraph will be identified . We evaluate GraphScan , Kulldorffs spatial scan ( searching over circles ) ...
2K_dev_233	Consider networks in harsh environments , where nodes may be lost due to failure , attack , or infection -- how is the topology affected by such events ? Can we mimic and measure the effect ? We propose a new generative model of network evolution in dynamic and harsh environments . Our model can reproduce the range of topologies observed across known robust and fragile biological networks , as well as several additional transport , communication , and social networks . We also develop a new optimization measure to evaluate robustness based on preserving high connectivity following random or adversarial bursty node loss . Using this measure , we evaluate the robustness of several real-world networks and propose a new distributed algorithm to construct secure networks operating within malicious environments .
2K_dev_234	We propose a novel discrete signal processing framework for the representation and analysis of datasets with complex structure . Such datasets arise in many social , economic , biological , and physical networks . Our framework extends traditional discrete signal processing theory to structured datasets by viewing them as signals represented by graphs , so that signal coefficients are indexed by graph nodes and relations between them are represented by weighted graph edges . We discuss the notions of signals and filters on graphs , and define the concepts of the spectrum and Fourier transform for graph signals . We demonstrate their relation to the generalized eigenvector basis of the graph adjacency matrix and study their properties . As a potential application of the graph Fourier transform , we consider the efficient representation of structured data that utilizes the sparseness of graph signals in the frequency domain .
2K_dev_235	Computational methods have been widely used to infer properties of complex systems that one can not directly observe experimentally . Viral capsid assembly is a key model system for complex self-assembly for which we lack direct experimental data on critical information , such as kinetic parameters , needed to build models and reveal detailed assembly pathways . We previously sought to learn such hidden parameters with a heuristic optimization approach using gradient and response surface methods applied to the light scattering measurements of three in vitro viral assembly systems : human papillomavirus ( HPV ) , hepatitis B virus ( HBV ) , and cowpea chlorotic mottle virus ( CCMV ) . This method successfully learned plausible kinetic parameters for all the three viruses leading to reconstruction of detailed models of assembly pathways . Significant computational challenges , however , hinder our ability to construct more precise or detailed models and reliably quantify uncertainty in the inferences . First , there is no closed form representation for the quality of fit of models to data , which therefore must be evaluated through computationally costly simulations . Second , the problem requires stochastic simulations , and the resulting simulation trajectories must be averaged over many replicates to suppress noise . Third , optimization of parameters must account for unknown factors and imprecision of experimental measurements . We explore here improvements based on the idea of derivative free optimization ( DFO ) , a class of optimization algorithm that can achieve faster and more accurate fitting , especially on systems characterized by costly , noisy evaluations of quality of fit . Preliminary tests show improvements over our custom gradient-based method using a DFO strategy . Work is continuing on evaluating different DFO methods and customizing them to inference of kinetic parameters in order to determine the best strategies for inferring unobservable physical parameters in complex biological self-assembly systems .
2K_dev_236	Background : Intracortical electrode arrays that can record extracellular action potentials from small , targeted groups of neurons are critical for basic neuroscience research and emerging clinical applications . In general , these electrode devices suffer from reliability and variability issues , which have led to comparative studies of existing and emerging electrode designs to optimize performance . Comparisons of different chronic recording devices have been limited to single-unit ( SU ) activity and employed a bulk averaging approach treating brain architecture as homogeneous with respect to electrode distribution . New method : In this study , we optimize the methods and parameters to quantify evoked multi-unit ( MU ) and local field potential ( LFP ) recordings in eight mice visual cortices . Results : These findings quantify the large recording differences stemming from anatomical differences in depth and the layer dependent relative changes to SU and MU recording performance over 6-months . For example , performance metrics in Layer V and stratum pyramidale were initially higher than Layer II/III , but decrease more rapidly . On the other hand , Layer II/III maintained recording metrics longer . In addition , chronic changes at the level of layer IV are evaluated using visually evoked current source density . Comparison with existing method ( s ) : The use of MU and LFP activity for evaluation and tracking biological depth provides a more comprehensive characterization of the electrophysiological performance landscape of microelectrodes . Conclusions : A more extensive spatial and temporal insight into the chronic electrophysiological performance over time will help uncover the biological and mechanical failure mechanisms of the neural electrodes and direct future research toward the elucidation of design optimization for specific applications .
2K_dev_237	Multimedia event detection ( MED ) is an emerging area of research . Previous work mainly focuses on simple event detection in sports and news videos , or abnormality detection in surveillance videos . In contrast , we focus on detecting more complicated and generic events that gain more users ' interest , and we explore an effective solution for MED . Moreover , our solution only uses few positive examples since precisely labeled multimedia content is scarce in the real world . As the information from these few positive examples is limited , we propose using knowledge adaptation to facilitate event detection . Different from the state of the art , our algorithm is able to adapt knowledge from another source for MED even if the features of the source and the target are partially different , but overlapping . Avoiding the requirement that the two domains are consistent in feature types is desirable as data collection platforms change or augment their capabilities and we should be able to respond to this with little or no effort . We perform extensive experiments on real-world multimedia archives consisting of several challenging events . The results show that our approach outperforms several other state-of-the-art detection algorithms .
2K_dev_238	ABSTRACT In spite of their many advantages , real -world application of guided -waves for structural health monitoring ( SHM ) of pipelines is still quite limited . The challenges can be discussed under three headings : ( 1 ) Multiple m odes , ( 2 ) Multi -path reflections , and ( 3 ) Sensitivity to environmental and operational conditions ( EOCs ) . These challenges are UHYLHZHG LQ WKH DXWKRUV SUHYLRXV ZRUN This paper is part of a study whose objective is to overcome these challenges for damage diagnosis of pipes , while addressing the limitations of the current approaches . That is , develop methods that simplify signal while retaining damage information , and perform well as EOC s vary . In this paper , a s upervised method is proposed to extract a sparse subset of the ultrasonic guided -wave signal s that contain optimal damage information for detection purposes . That is , a discriminant vector is calculated so that the projection s of undamaged and damaged pipes on this vector is separated . In the training stage , data is recorded from intact pipe , and from a pipe with an artificial structural abnormality ( to simulate any variation from intact condition ) . During the monitoring stage , test signals are projected on the discriminant vector , and these projections are used as damage -sensitive features for detection purposes . Being a supervised metho d , factors such as EOC variations , and difference in the characteristics of the structural abnormality in training and test data , may affect the detection performance . This paper reports the experiments investigating the extent to which the differences in damage size and damage location , as well as temperatures , can influence the discriminatory power of the extracted damage -sensitive features . The results suggest that , for practical ranges of monitoring and damage sizes of interest , the proposed method has low sensitivity to such training factors . High detection performances are obtained for temperature differences up to 14 ( . The finding s reported in this paper suggest that although the proposed method is a supervised approach , labeling of the training data does not require prior knowledge about the damage characteristics ( e.g. , size , location ) . Moreover , the potential of the proposed met hod for online monitoring is illustrated , for wide range of temperature variations and different damage scenarios . Keywords : Pipeline Monitoring , Guided -waves , Nondestructive Evaluation , Structural Health Monitoring , Temperature Effects , Damage Detection , Sparse Representation , Damage Characteristics .
2K_dev_239	We present an adaptive graph filtering approach to semi-supervised classification . Adaptive graph filters combine decisions from multiple graph filters using a weighting function that is optimized in a semi-supervised manner . We also demonstrate the multiresolution property of adaptive graph filters by connecting them to the diffusion wavelets . In our experiments , we apply the adaptive graph filters to the classification of online blogs and damage identification in indirect bridge structural health monitoring .
2K_dev_240	We address the problem of action recognition in unconstrained videos . We propose a novel content driven pooling that leverages space-time context while being robust toward global space-time transformations . Being robust to such transformations is of primary importance in unconstrained videos where the action localizations can drastically shift between frames . Our pooling identifies regions of interest using video structural cues estimated by different saliency functions . To combine the different structural information , we introduce an iterative structure learning algorithm , WSVM ( weighted SVM ) , that determines the optimal saliency layout of an action model through a sparse regularizer . A new optimization method is proposed to solve the WSVM ' highly non-smooth objective function . We evaluate our approach on standard action datasets ( KTH , UCF50 and HMDB ) . Most noticeably , the accuracy of our algorithm reaches 51.8 % on the challenging HMDB dataset which outperforms the state-of-the-art of 7.3 % relatively .
2K_dev_241	Large scale integration of stochastic energy resources in power systems requires probabilistic analysis approaches for comprehensive system analysis . The large-varying grid condition on the aging and stressed power system infrastructures also requires merging of offline security analyses into online operation . Meanwhile in computing , the recent rapid hardware performance growth comes from the more and more complicated architecture . Fully utilizing the computing power for specific applications becomes very difficult . Given the challenges and opportunities in both the power system and the computing fields , this paper presents the unique commodity high performance computing system solutions to the following fundamental tools for power system probabilistic and security analysis : 1 ) a high performance Monte Carlo simulation ( MCS ) based distribution probabilistic load flow solver for real-time distribution feeder probabilistic solutions . 2 ) A high performance MCS based transmission probabilistic load flow solver for transmission grid probabilistic analysis . 3 ) A SIMD accelerated AC contingency calculation solver based on Woodbury matrix identity on multi-core CPUs . By aggressive algorithm level and computer architecture level performance optimizations including optimized data structures , optimization for superscalar out-of-order execution , SIMDization , and multi-core scheduling , our software fully utilizes the modern commodity computing systems , makes the critical and computational intensive power system probabilistic and security analysis problems solvable in real-time on commodity computing systems .
2K_dev_242	Lamb waves are powerful tools in nondestructive evaluation and structural health monitoring . Researchers use Lamb waves to detect and locate damage across large areas . To best utilize Lamb waves , they are analyzed through two processing steps : baseline subtraction and velocity calibration . Baseline subtraction removes background information from our data and velocity calibration tunes our algorithms . Yet , in many scenarios , these steps are challenging to implement . Baseline subtraction is challenging due to variable environmental conditions . Velocity calibration is challenging due to multi-modal and dispersive velocity behavior in Lamb waves . To address both challenges , we present two approaches that combine environmental compensation with self-calibrating localization . We discuss temperature compensation strategies based on the scale transform and singular value decomposition . We then integrate these with a localization framework known as data-driven matched field processing . We show these combined approaches to be effective in a variety of scenarios .
2K_dev_243	Modern robots , like todays smartphones , are complex devices with intricate software systems . Introductory robot programming courses must evolve to reflect this reality , by teaching students to make use of the sophisticated tools their robots provide rather than reimplementing basic algorithms . This paper focuses on teaching with Tekkotsu , an open source robot application development framework designed specifically for education . But , the curriculum described here can also be taught using ROS , the Robot Operating System that is now widely used for robotics research .
2K_dev_244	As airspace becomes ever more crowded , air traffic management must reduce both space and time between aircraft to increase throughput , making on-board collision avoidance systems ever more important . These safety-critical systems must be extremely reliable , and as such , many resources are invested into ensuring that the protocols they implement are accurate . Still , it is challenging to guarantee that such a controller works properly under every circumstance . In tough scenarios where a large number of aircraft must execute a collision avoidance maneuver , a human pilot under stress is not necessarily able to understand the complexity of the distributed system and may not take the right course , especially if actions must be taken quickly . We consider a class of distributed collision avoidance controllers designed to work even in environments with arbitrarily many aircraft or UAVs . We prove that the controllers never allow the aircraft to get too close to one another , even when new planes approach an in-progress avoidance maneuver that the new plane may not be aware of . Because these safety guarantees always hold , the aircraft are protected against unexpected emergent behavior which simulation and testing may miss . This is an important step in formally verified , flyable , and distributed air traffic control .
2K_dev_245	There has been a longstanding concern within HCI that even though we are accumulating great innovations in the field , we rarely see these innovations develop into products . Our panel brings together HCI researchers from academia and industry who have been directly involved in technology transfer of one or more HCI innovations . They will share their experiences around what it takes to transition an HCI innovation from the lab to the market , including issues around time commitment , funding , resources , and business expertise . More importantly , our panelists will discuss and debate the tensions that we ( researchers ) face in choosing design and evaluation methods that help us make an HCI research contribution versus what actually matters when we go to market .
2K_dev_246	Brand Associations , one of central concepts in marketing , describe customers ' top-of-mind attitudes or feelings toward a brand . Thus , this consumer-driven brand equity often attains the grounds for purchasing products or services of the brand . Traditionally , brand associations are measured by analyzing the text data from consumers ' responses to the survey or their online conversation logs . In this paper , we propose to go beyond text data and leverage large-scale online photo collections contributed by the general public , which have not been explored so far . As a first technical step toward the study of photo-based brand associations , we aim to jointly achieve the following two visualization tasks in a mutually-rewarding way : ( i ) detecting and visualizing core visual concepts associated with brands , and ( ii ) localizing the regions of brand in the images . With experiments on about five millions of images of 48 brands crawled from five popular online photo sharing sites , we demonstrate that our approach can discover complementary views on the brand associations that are hardly mined from the text data . We also quantitatively show that our approach outperforms other candidate methods on the both visualization tasks .
2K_dev_247	How does malware propagate ? Does it form spikes over time ? Does it resemble the propagation pattern of benign files , such as software patches ? Does it spread uniformly over countries ? How long does it take for a URL that distributes malware to be detected and shut down ? In this work , we answer these questions by analyzing patterns from 22 million malicious ( and benign ) files , found on 1.6 million hosts worldwide during the month of June 2011 . We conduct this study using the WINE database available at Symantec Research Labs . Additionally , we explore the research questions raised by sampling on such large databases of executables ; the importance of studying the implications of sampling is twofold : First , sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second , because every such data collection can be perceived as a sample of the real world . We discover the SharkFin temporal propagation pattern of executable files , the GeoSplit pattern in the geographical spread of machines that report executables to Symantecs servers , the Periodic Power Law ( Ppl ) distribution of the lifetime of URLs , and we show how to efficiently extrapolate crucial properties of the data from a small sample . We further investigate the propagation pattern of benign and malicious executables , unveiling latent structures in the way these files spread . To the best of our knowledge , our work represents the largest study of propagation patterns of executables .
2K_dev_248	using an existing crowd marketplace ( Amazon Mechanical Turk ) , explore design considerations important for building systems that use crowd storage , and outline ideas for future research in this area . This paper introduces the concept of crowd storage , the idea that digital files can be stored and retrieved later from the memories of people in the crowd . Similar to human memory , crowd storage is ephemeral , which means that storage is temporary and the quality of the stored information degrades over time . Crowd storage may be preferred over storing information directly in the cloud , or when it is desirable for information to degrade inline with normal human memories . To explore and validate this idea , we created WeStore , a system that stores and then later retrieves digital files in the existing memories of crowd workers . WeStore does not store information directly , but rather encrypts the files using details of the existing memories elicited from individuals within the crowd as cryptographic keys . The fidelity of the retrieved information is tied to how well the crowd remembers the details of the memories they provided . We demonstrate that crowd storage is feasible
2K_dev_249	We present a cloud-based approach to opportunistic , near real-time search of untagged images on smartphones that is sensitive to bandwidth and energy constraints . Our approach is inspired by the long-established practice of photographers using contact sheets to rapidly visualize a new collection of photographs , and then selecting a subset on which to focus attention . On behalf of each smartphone , the cloud maintains a virtual contact sheet of images that have been captured but not yet uploaded . The virtual contact sheet consists of a set of low-fidelity images as well as full or partial meta-data associated with each image . If search processing on the cloud indicates that a particular low-fidelity object is relevant , then its full-fidelity image can be obtained just-in-time from the corresponding smartphone for further search processing or presentation to the user . Our approach is not limited to images , but they provide a convenient query space to test search optimizations .
2K_dev_250	In this demonstration , we show an energy measurement system that estimates the energy consumption of individual appliances using a wireless sensor network consisting of contactless electromagnetic field ( EMF ) sensors deployed near each appliance , and a whole-house power meter . The EMF sensor can detect appliance state transitions within close proximity based on magnetic field fluctuations . Data from these sensors are then relayed back to the main meter using a low-latency wireless sensor networking protocol , where changes in the total power consumption of the house are used to determine the power usage of individual appliances . The sensors are low-cost , easy to deploy and are able to detect current changes associated with the appliance from a few inches away making it possible to externally monitor in-wall wiring to devices like overhead lights or heavy machinery that might operate on multiple phases of the AC distribution system of the building . Appliance-level energy data provide continuous feedback to end users about their consumption patterns and provide building managers accurate information that can be used to target the most effective update and retrofit strategies .
2K_dev_251	Autonomous driving is likely to be the heart of urban transportation in the future . Autonomous vehicles have the potential to increase the safety of passengers and also to make road trips shorter and more enjoyable . As the first steps toward these goals , many car manufacturers are investing in designing and equipping their vehicles with advanced driver-assist systems . Road intersections are considered to be serious bottlenecks of urban transportation , as more than 44 % of all reported crashes in U.S. occur within intersection areas which in turn lead to 8,500 fatalities and approximately 1 million injuries every year . Furthermore , the impact of road intersections on traffic delays leads to enormous waste of human and natural resources . In this paper , we therefore focus on intersection management in Intelligent Transportation Systems ( ITS ) research . In the future , when dealing with autonomous vehicles , it is critical to address safety and throughput concerns that arise from autonomous driving through intersections and roundabouts . Our goal is to provide vehicles with a safe and efficient passage method through intersections and roundabouts . We have been investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous driving . We have designed and developed efficient and reliable intersection protocols to avoid vehicle collisions at intersections and increase traffic throughput . In this paper , we introduce new V2V intersection protocols to achieve the above goals . We show that , in addition to intersections , these protocols are also applicable to vehicle crossings at roundabouts . Additionally , we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols and suggest required modifications to guarantee their safety and efficiency despite these impairments . Our simulation results show that we are able to avoid collisions and also increase the throughput of the intersections up to 87.82 % compared to common traffic-light signalized intersections .
2K_dev_252	Parameterized probabilistic complex computational ( P 2 C 2 ) models are being increasingly used in computational systems biology for analyzing biological systems . A key challenge is to build mechanistic P 2 C 2 models by combining prior knowledge and empirical data , given that certain system properties are unknown . These unknown components are incorporated into a model as parameters and determining their values has traditionally been a process of trial and error . We present a new algorithmic procedure for discovering parameters in agent-based models of biological systems against behavioral specifications mined from large data-sets . Our approach uses Bayesian model checking , sequential hypothesis testing , and stochastic optimization to synthesize parameters of P 2 C 2 models . We demonstrate our algorithm by discovering the amount and schedule of doses of bacterial lipopolysaccharide in a clinical agent-based model of the dynamics of acute inflammation that guarantee a set of desired clinical outcomes with high probability .
2K_dev_253	In this paper , we address the problem of jointly summarizing large sets of Flickr images and YouTube videos . Starting from the intuition that the characteristics of the two media types are different yet complementary , we develop a fast and easily-parallelizable approach for creating not only high-quality video summaries but also novel structural summaries of online images as storyline graphs . The storyline graphs can illustrate various events or activities associated with the topic in a form of a branching network . The video summarization is achieved by diversity ranking on the similarity graphs between images and video frames . The reconstruction of storyline graphs is formulated as the inference of sparse time-varying directed graphs from a set of photo streams with assistance of videos . For evaluation , we collect the datasets of 20 outdoor activities , consisting of 2.7M Flickr images and 16K YouTube videos . Due to the large-scale nature of our problem , we evaluate our algorithm via crowdsourcing using Amazon Mechanical Turk . In our experiments , we demonstrate that the proposed joint summarization approach outperforms other baselines and our own methods using videos or images only .
2K_dev_254	Although widely touted as a replacement for glass slides and microscopes in pathology , digital slides present major challenges in data storage , transmission , processing and interoperability . Since no universal data format is in widespread use for these images today , each vendor defines its own proprietary data formats , analysis tools , viewers and software libraries . This creates issues not only for pathologists , but also for interoperability . In this paper , we present the design and implementation of OpenSlide , a vendor-neutral C library for reading and manipulating digital slides of diverse vendor formats . The library is extensible and easily interfaced to various programming languages . An application written to the OpenSlide interface can transparently handle multiple vendor formats . OpenSlide is in use today by many academic and industrial organizations world-wide , including many research sites in the United States that are funded by the National Institutes of Health .
2K_dev_255	Smartwatches are a promising new interactive platform , but their small size makes even basic actions cumbersome . Hence , there is a great need for approaches that expand the interactive envelope around smartwatches , allowing human input to escape the small physical confines of the device . We propose using tiny projectors integrated into the smartwatch to render icons on the user 's skin . These icons can be made touch sensitive , significantly expanding the interactive region without increasing device size . Through a series of experiments , we show that these 'skin buttons ' can have high touch accuracy and recognizability , while being low cost and power-efficient .
2K_dev_256	AbstractSharing research data is necessary for collaboration within a research network and is required by funding agencies , such as the National Science Foundation ( NSF ) , that enforce the scientific method and ethics associated with data management and sharing . However , methods and infrastructure for supporting construction research data management are currently underdeveloped ; emphasizing the need for developing effective and efficient means for managing and sharing research data . A review of existing data management models reveals that there is currently no effective universal system for sharing the data obtained from construction research endeavours . This paper presents electronic product and process management systems ( EPPMS ) as a construction research data management and sharing approach . The developed EPPMS is a web-based system that utilizes workflows that can automate the collection , authorization , and dissemination of construction research data . A comparative analysis of the developed system to t ...
2K_dev_257	In this paper we introduce High Assurance SPIRAL to solve the last mile problem for the synthesis of high assurance implementations of controllers for vehicular systems that are executed in todays and future embedded and high performance embedded system processors . High Assurance SPIRAL is a scalable methodology to translate a high level specification of a high assurance controller into a highly resource-efficient , platform-adapted , verified control software implementation for a given platform in a language like C or C++ . High Assurance SPIRAL proves that the implementation is equivalent to the specification written in the control engineers domain language . Our approach scales to problems involving floating-point calculations and provides highly optimized synthesized code . It is possible to estimate the available headroom to enable assurance/performance trade-offs under real-time constraints , and enables the synthesis of multiple implementation variants to make attacks harder . At the core of High Assurance SPIRAL is the Hybrid Control Operator Language ( HCOL ) that leverages advanced mathematical constructs expressing the controller specification to provide high quality translation capabilities . Combined with a verified/certified compiler , High Assurance SPIRAL provides a comprehensive complete solution to the efficient synthesis of verifiable high assurance controllers . We demonstrate High Assurance SPIRALs capability by co-synthesizing proofs and implementations for attack detection and sensor spoofing algorithms and deploy the code as ROS nodes on the Landshark unmanned ground vehicle and on a Synthetic Car in a real-time simulator .
2K_dev_258	A responsibility we have as researchers is to disseminate the results of our research widely . A primary way we do this is through research publications . When these publications are not accessible to everyone , some readers will be excluded and the impact of our research limited . In this paper , we explore this problem in two ways . First , we report on the accessibility of 1,811 papers in the technical program of several top conferences related to accessibility and human-computer interaction . Second , we reflect on our experience making papers accessible for any CHI 2015 author who requested it . We offer thoughts on research challenges and future work that may make our community 's research more accessible .
2K_dev_259	It is our view that the state of the art in constructing a large collection of graph algorithms in terms of linear algebraic operations is mature enough to support the emergence of a standard set of primitive building blocks . This paper is a position paper defining the problem and announcing our intention to launch an open effort to define this standard .
2K_dev_260	The robust detection of small targets is one of the key techniques in infrared search and tracking applications . A novel small target detection method in a single infrared image is proposed in this paper . Initially , the traditional infrared image model is generalized to a new infrared patch-image model using local patch construction . Then , because of the non-local self-correlation property of the infrared background image , based on the new model small target detection is formulated as an optimization problem of recovering low-rank and sparse matrices , which is effectively solved using stable principle component pursuit . Finally , a simple adaptive segmentation method is used to segment the target image and the segmentation result can be refined by post-processing . Extensive synthetic and real data experiments show that under different clutter backgrounds the proposed method not only works more stably for different target sizes and signal-to-clutter ratio values , but also has better detection performance compared with conventional baseline methods .
2K_dev_261	In this paper , we investigate a time-sensitive image retrieval problem , in which given a query keyword , a query time point , and optionally user information , we retrieve the most relevant and temporally suitable images from the database . Inspired by recently emerging interests on query dynamics in information retrieval research , our time-sensitive image retrieval algorithm can infer users ' implicit search intent better and provide more engaging and diverse search results according to temporal trends of Web user photos . We model observed image streams as instances of multivariate point processes represented by several different descriptors , and develop a regularized multi-task regression framework that automatically selects and learns stochastic parametric models to solve the relations between image occurrence probabilities and various temporal factors that influence them . Using Flickr datasets of more than seven million images of 30 topics , our experimental results show that the proposed algorithm is more successful in time-sensitive image retrieval than other candidate methods , including ranking SVM , a PageRank-based image ranking , and a generative temporal topic model .
2K_dev_262	As part of a collaboration with a major California school district , we study the problem of fairly allocating unused classrooms in public schools to charter schools . Our approach revolves around the randomized leximin mechanism . We extend previous work to the classroom allocation setting , showing that the leximin mechanism is proportional , envy-free , efficient , and group strategyproof . We also prove that the leximin mechanism provides a ( worst-case ) 4-approximation to the maximum number of classrooms that can possibly be allocated . Our experiments , which are based on real data , show that a nontrivial implementation of the leximin mechanism scales gracefully in terms of running time ( even though the problem is intractable in theory ) , and performs extremely well with respect to a number of efficiency objectives . We take great pains to establish the practicability of our approach , and discuss issues related to its deployment .
2K_dev_263	This paper studies the convergence of the estimation error process and the characterization of the corresponding invariant measure in distributed Kalman filtering for potentially unstable and large linear dynamic systems . A gossip network protocol termed Modified Gossip Interactive Kalman Filtering ( M-GIKF ) is proposed , where sensors exchange their filtered states ( estimates and error covariances ) and propagate their observations via inter-sensor communications of rate $ \overline { \gamma } $ ; $ \overline { \gamma } $ is defined as the averaged number of inter-sensor message passages per signal evolution epoch . The filtered states are interpreted as stochastic particles swapped through local interaction . The paper shows that the conditional estimation error covariance sequence at each sensor under M-GIKF evolves as a random Riccati equation ( RRE ) with Markov modulated switching . By formulating the RRE as a random dynamical system , it is shown that the network achieves weak consensus , i.e. , the conditional estimation error covariance at a randomly selected sensor converges weakly ( in distribution ) to a unique invariant measure . Further , it is proved that as $ \overline { \gamma } \rightarrow \infty $ this invariant measure satisfies the Large Deviation ( LD ) upper and lower bounds , implying that this measure converges exponentially fast ( in probability ) to the Dirac measure $ \delta_ { P^* } $ , where $ P^* $ is the stable error covariance of the centralized ( Kalman ) filtering setup . The LD results answer a fundamental question on how to quantify the rate at which the distributed scheme approaches the centralized performance as the inter-sensor communication rate increases .
2K_dev_264	Given a set of k networks , possibly with different sizes and no overlaps in nodes or links , how can we quickly assess similarity between them ? Analogously , are there a set of social theories which , when represented by a small number of descriptive , numerical features , effectively serve as a `` signature '' for the network ? Having such signatures will enable a wealth of graph mining and social network analysis tasks , including clustering , outlier detection , visualization , etc . We propose a novel , effective , and scalable method , called NETSIMILE , for solving the above problem . Our approach has the following desirable properties : ( a ) It is supported by a set of social theories . ( b ) It gives similarity scores that are size-invariant . ( c ) It is scalable , being linear on the number of links for graph signature extraction . In extensive experiments on numerous synthetic and real networks from disparate domains , NETSIMILE outperforms baseline competitors . We also demonstrate how our approach enables several mining tasks such as clustering , visualization , discontinuity detection , network transfer learning , and re-identification across networks .
2K_dev_265	Short-term forecasting is a ubiquitous practice in a wide range of energy systems , including forecasting demand , renewable generation , and electricity pricing . Although it is known that probabilistic forecasts ( which give a distribution over possible future outcomes ) can improve planning and control , many forecasting systems in practice are just used as point forecast tools , as it is challenging to represent high-dimensional non-Gaussian distributions over multiple spatial and temporal points . In this paper , we apply a recently-proposed algorithm for modeling high-dimensional conditional Gaussian distributions to forecasting wind power and extend it to the non-Gaussian case using the copula transform . On a wind power forecasting task , we show that this probabilistic model greatly outperforms other methods on the task of accurately modeling potential distributions of power ( as would be necessary in a stochastic dispatch problem , for example ) .
2K_dev_266	The omnipresence of indoor lighting makes it an ideal vehicle for pervasive communication with mobile devices . In this paper , we present a communication scheme that enables interior ambient LED lighting systems to send data to mobile devices using either cameras or light sensors . By exploiting rolling shutter camera sensors that are common on tablets , laptops and smartphones , it is possible to detect high-frequency changes in light intensity reflected off of surfaces and in direct line-of-sight of the camera . We present a demodulation approach that allows smartphones to accurately detect frequencies as high as 8kHz with 0.2kHz channel separation . In order to avoid humanly perceivable flicker in the lighting , our system operates at frequencies above 2kHz and compensates for the non-ideal frequency response of standard LED drivers by adjusting the light 's duty-cycle . By modulating the PWM signal commonly used to drive LED lighting systems , we are able to encode data that can be used as localization landmarks . We show through experiments how a binary frequency shift keying modulation scheme can be used to transmit data at 1.25 bytes per second ( fast enough to send an ID code ) from up to 29 unique light sources simultaneously in a single collision domain . We also show how tags can demodulate the same signals using a light sensor instead of a camera for low-power applications .
2K_dev_267	With the rapid increase in cloud services collecting and using user data to offer personalized experiences , ensuring that these services comply with their privacy policies has become a business imperative for building user trust . However , most compliance efforts in industry today rely on manual review processes and audits designed to safeguard user data , and therefore are resource intensive and lack coverage . In this paper , we present our experience building and operating a system to automate privacy policy compliance checking in Bing . Central to the design of the system are ( a ) Legal ease-a language that allows specification of privacy policies that impose restrictions on how user data is handled , and ( b ) Grok-a data inventory for Map-Reduce-like big data systems that tracks how user data flows among programs . Grok maps code-level schema elements to data types in Legal ease , in essence , annotating existing programs with information flow types with minimal human input . Compliance checking is thus reduced to information flow analysis of big data systems . The system , bootstrapped by a small team , checks compliance daily of millions of lines of ever-changing source code written by several thousand developers .
2K_dev_268	Question answering ( Q & A ) communities have been gaining popularity in the past few years . The success of such sites depends mainly on the contribution of a small number of expert users who provide a significant portion of the helpful answers , and so identifying users that have the potential of becoming strong contributers is an important task for owners of such communities . We present a study of the popular Q & A website StackOverflow ( SO ) , in which users ask and answer questions about software development , algorithms , math and other technical topics . The dataset includes information on 3.5 million questions and 6.9 million answers created by 1.3 million users in the years 2008 -- 2012 . Participation in activities on the site ( such as asking and answering questions ) earns users reputation , which is an indicator of the value of that user to the site . We describe an analysis of the SO reputation system , and the participation patterns of high and low reputation users . The contributions of very high reputation users to the site indicate that they are the primary source of answers , and especially of high quality answers . Interestingly , we find that while the majority of questions on the site are asked by low reputation users , on average a high reputation user asks more questions than a user with low reputation . We consider a number of graph analysis methods for detecting influential and anomalous users in the underlying user interaction network , and find they are effective in detecting extreme behaviors such as those of spam users . Lastly , we show an application of our analysis : by considering user contributions over first months of activity on the site , we predict who will become influential long-term contributors .
2K_dev_269	In a series of experiments , we examined how the timing impacts the salience of smartphone app privacy notices . In a web survey and a field experiment , we isolated different timing conditions for displaying privacy notices : in the app store , when an app is started , during app use , and after app use . Participants installed and played a history quiz app , either virtually or on their phone . After a distraction or delay they were asked to recall the privacy notice 's content . Recall was used as a proxy for the attention paid to and salience of the notice . Showing the notice during app use significantly increased recall rates over showing it in the app store . In a follow-up web survey , we tested alternative app store notices , which improved recall but did not perform as well as notices shown during app use . The results suggest that even if a notice contains information users care about , it is unlikely to be recalled if only shown in the app store .
2K_dev_270	In commercial-off-the-shelf ( COTS ) multi-core systems , a task running on one core can be delayed by other tasks running simultaneously on other cores due to interference in the shared DRAM main memory . Such memory interference delay can be large and highly variable , thereby posing a significant challenge for the design of predictable real-time systems . In this paper , we present techniques to provide a tight upper bound on the worst-case memory interference in a COTS-based multi-core system . We explicitly model the major resources in the DRAM system , including banks , buses and the memory controller . By considering their timing characteristics , we analyze the worst-case memory interference delay imposed on a task by other tasks running in parallel . To the best of our knowledge , this is the first work bounding the request re-ordering effect of COTS memory controllers . Our work also enables the quantification of the extent by which memory interference can be reduced by partitioning DRAM banks . We evaluate our approach on a commodity multi-core platform running Linux/RK . Experimental results show that our approach provides an upper bound very close to our measured worst-case interference .
2K_dev_271	Virus capsid assembly has been widely studied as a biophysical system , both for its biological and medical significance and as an important model for complex self-assembly processes . No current technology can monitor assembly in detail and what information we have on assembly kinetics comes exclusively from invitro studies . There are many differences between the intracellular environment and that of an invitro assembly assay , however , that might be expected to alter assembly pathways . Here , we explore one specific feature characteristic of the intracellular environment and known to have large effects on macromolecular assembly processes : molecular crowding . We combine prior particle simulation methods for estimating crowding effects with coarse-grained stochastic models of capsid assembly , using the crowding models to adjust kinetics of capsid simulations to examine possible effects of crowding on assembly pathways . Simulations suggest a striking difference depending on whether or not a system uses nucleation-limited assembly , with crowding tending to promote off-pathway growth in a nonnucleation-limited model but often enhancing assembly efficiency at high crowding levels even while impeding it at lower crowding levels in a nucleation-limited model . These models may help us understand how complicated assembly systems may have evolved to function with high efficiency and fidelity in the densely crowded environment of the cell .
2K_dev_272	Understanding and quantifying the impact of unobserved processes is one of the major challenges of analyzing multivariate time series data . In this paper , we analyze a flexible stochastic process model , the generalized linear auto-regressive process ( GLARP ) and identify the conditions under which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood . In particular , we examine three examples , including two popular models for count data , i.e , Poisson and Conwey-Maxwell Poisson vector auto-regressive processes , and one powerful model for extreme value data , i.e. , Gumbel vector auto-regressive processes . We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models . We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it . Experiments on two synthetic datasets , one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models .
2K_dev_273	Kidney exchanges allow incompatible donor-patient pairs to swap kidneys , but each donation must pass three tests : blood , tissue , and crossmatch . In practice a matching is computed based on the first two tests , and then a single crossmatch test is performed for each matched patient . However , if two crossmatches could be performed per patient , in principle significantly more successful exchanges could take place . In this paper , we ask : If we were allowed to perform two crossmatches per patient , could we harness this additional power optimally and efficiently ? Our main result is a polynomial time algorithm for this problem that almost surely computes optimal -- - up to lower order terms -- - solutions on random large kidney exchange instances .
2K_dev_274	The Internet has the potential to accelerate scientific problem solving by engaging a global pool of contributors . Existing approaches focus on broadcasting problems to many independent solvers . We investigate other approaches that may be advantageous by examining a community for mathematical problem solving -- MathOverflow -- in which contributors communicate and collaborate to solve new mathematical 'micro-problems ' online . We contribute a simple taxonomy of collaborative acts derived from a process-level examination of collaborations and a quantitative analysis relating collaborative acts to solution quality . Our results indicate a diversity of ways in which mathematicians are reaching a solution , including by iteratively advancing a solution . A better understanding of such collaborative strategies can inform the design of tools to support distributed collaboration on complex problems .
2K_dev_275	Recent computer systems research has proposed using redundant requests to reduce latency . The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request ) . However there is no exact analysis of systems with redundancy . This paper presents the first exact analysis of systems with redundancy . We allow for any number of classes of redundant requests , any number of classes of non-redundant requests , any degree of redundancy , and any number of heterogeneous servers . In all cases we derive the limiting distribution on the state of the system . In small ( two or three server ) systems , we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes , and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy . We find some surprising results . First , the response time of a fully redundant class follows a simple Exponential distribution and that of the non-redundant class follows a Generalized Hyperexponential . Second , fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant . We also compare redundancy with other approaches for reducing latency , such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and Join-the-Shortest-Queue ( JSQ ) routing of a class . We find that , in many cases , redundancy outperforms JSQ and Opt-Split with respect to overall response time , making it an attractive solution .
2K_dev_276	Detecting dyslexia is crucial so that people who have dyslexia can receive training to avoid associated high rates of academic failure . In this paper we present Dytective , a game designed to detect dyslexia . The results of a within-subjects experiment with 40 children ( 20 with dyslexia ) show significant differences between groups who played Dytective . These differences suggest that Dytective could be used to help identify those likely to have dyslexia .
2K_dev_277	How do anomalies , fraud , and spam effect our models of normal user behavior ? How can we modify our models to catch fraudsters ? In this tutorial we will answer these questions - connecting graph analysis tools for user behavior modeling to anomaly and fraud detection . In particular , we will focus on three data mining techniques : subgraph analysis , label propagation and latent factor models ; and their application to static graphs , e.g . social networks , evolving graphs , e.g . `` who-calls-whom '' networks , and attributed graphs , e.g . the `` who-reviews-what '' graphs of Amazon and Yelp . For each of these techniques we will give an explanation of the algorithms and the intuition behind them . We will then give brief examples of recent research using the techniques to model , understand and predict normal behavior . With this intuition for how these methods are applied to graphs and user behavior , we will focus on state-of-the-art research showing how the outcomes of these methods are effected by fraud , and how they have been used to catch fraudsters .
2K_dev_278	In this paper , we investigate an approach for reconstructing storyline graphs from large-scale collections of Internet images , and optionally other side information such as friendship graphs . The storyline graphs can be an effective summary that visualizes various branching narrative structure of events or activities recurring across the input photo sets of a topic class . In order to explore further the usefulness of the storyline graphs , we leverage them to perform the image sequential prediction tasks , from which photo recommendation applications can benefit . We formulate the storyline reconstruction problem as an inference of sparse time-varying directed graphs , and develop an optimization algorithm that successfully addresses a number of key challenges of Web-scale problems , including global optimality , linear complexity , and easy parallelization . With experiments on more than 3.3 millions of images of 24 classes and user studies via Amazon Mechanical Turk , we show that the proposed algorithm improves other candidate methods for both storyline reconstruction and image prediction tasks .
2K_dev_279	Given a large collection of epidemiological data consisting of the count of d contagious diseases for l locations of duration n , how can we find patterns , rules and outliers ? For example , the Project Tycho provides open access to the count infections for U.S. states from 1888 to 2013 , for 56 contagious diseases ( e.g. , measles , influenza ) , which include missing values , possible recording errors , sudden spikes ( or dives ) of infections , etc . So how can we find a combined model , for all these diseases , locations , and time-ticks ? In this paper , we present FUNNEL , a unifying analytical model for large scale epidemiological data , as well as a novel fitting algorithm , FUNNELFIT , which solves the above problem . Our method has the following properties : ( a ) Sense-making : it detects important patterns of epidemics , such as periodicities , the appearance of vaccines , external shock events , and more ; ( b ) Parameter-free : our modeling framework frees the user from providing parameter values ; ( c ) Scalable : FUNNELFIT is carefully designed to be linear on the input size ; ( d ) General : our model is general and practical , which can be applied to various types of epidemics , including computer-virus propagation , as well as human diseases . Extensive experiments on real data demonstrate that FUNNELFIT does indeed discover important properties of epidemics : ( P1 ) disease seasonality , e.g. , influenza spikes in January , Lyme disease spikes in July and the absence of yearly periodicity for gonorrhea ; ( P2 ) disease reduction effect , e.g. , the appearance of vaccines ; ( P3 ) local/state-level sensitivity , e.g. , many measles cases in NY ; ( P4 ) external shock events , e.g. , historical flu pandemics ; ( P5 ) detect incongruous values , i.e. , data reporting errors .
2K_dev_280	From Twitter to Facebook to Reddit , users have become accustomed to sharing the articles they read with friends or followers on their social networks . While previous work has modeled what these shared stories say about the user who shares them , the converse question remains unexplored : what can we learn about an article from the identities of its likely readers ? To address this question , we model the content of news articles and blog posts by attributes of the people who are likely to share them . For example , many Twitter users describe themselves in a short profile , labeling themselves with phrases such as `` vegetarian '' or `` liberal . '' By assuming that a user 's labels correspond to topics in the articles he shares , we can learn a labeled dictionary from a training corpus of articles shared on Twitter . Thereafter , we can code any new document as a sparse non-negative linear combination of user labels , where we encourage correlated labels to appear together in the output via a structured sparsity penalty . Finally , we show that our approach yields a novel document representation that can be effectively used in many problem settings , from recommendation to modeling news dynamics . For example , while the top politics stories will change drastically from one month to the next , the `` politics '' label will still be there to describe them . We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012 , demonstrating that our approach is effective .
2K_dev_281	The M/M/k/setup model , where there is a penalty for turning servers on , is common in data centers , call centers , and manufacturing systems . Setup costs take the form of a time delay , and sometimes there is additionally a power penalty , as in the case of data centers . While the M/M/1/setup was exactly analyzed in 1964 , no exact analysis exists to date for the M/M/k/setup with $ $ k > 1 $ $ k > 1 . In this paper , we provide the first exact , closed-form analysis for the M/M/k/setup and some of its important variants including systems in which idle servers delay for a period of time before turning off or can be put to sleep . Our analysis is made possible by a new way of combining renewal reward theory and recursive techniques to solve Markov chains with a repeating structure . Our renewal-based approach uses ideas from renewal reward theory and busy period analysis to obtain closed-form expressions for metrics of interest such as the transform of time in system and the transform of power consumed by the system . The simplicity , intuitiveness , and versatility of our renewal-based approach makes it useful for analyzing Markov chains far beyond the M/M/k/setup . In general , our renewal-based approach should be used to reduce the analysis of any 2-dimensional Markov chain which is infinite in at most one dimension and repeating to the problem of solving a system of polynomial equations . In the case where all transitions in the repeating portion of the Markov chain are skip-free and all up/down arrows are unidirectional , the resulting system of equations will yield a closed-form solution .
2K_dev_282	Multimedia event detection ( MED ) is an effective technique for video indexing and retrieval . Current classifier training for MED treats the negative videos equally . However , many negative videos may resemble the positive videos in different degrees . Intuitively , we may capture more informative cues from the negative videos if we assign them fine-grained labels , thus benefiting the classifier learning . Aiming for this , we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event . Based on these decisive attributes , we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation . The resulting fine-grained labels may be not accurate enough to characterize the negative videos . Hence , we propose to jointly optimize the fine-grained labels with the knowledge from the visual features and the attributes representations , which brings mutual reciprocality . Our model obtains two kinds of classifiers , one from the attributes and one from the features , which incorporate the informative cues from the fine-grained labels . The outputs of both classifiers on the testing videos are fused for detection . Extensive experiments on the challenging TRECVID MED 2012 development set have validated the efficacy of our proposed approach .
2K_dev_283	Autonomous driving technologies have been emerging over the past few years , and semi-autonomous driving functionalities have been deployed to vehicles available in the market . Since autonomous driving is realized by the intelligent processing of data from various types of sensors such as LIDAR , radar , camera , etc. , the complexity of designing a dependable real-time autonomous driving system is rather high . Although there has been much research on building a reliable real-time system using hardware replication , the resulting systems tend to add significant extra cost due to hardware replication . Therefore , an alternative solution would be helpful in building an autonomous vehicle in a cost-effective way . An autonomous driving system is different from the conventional reliable real-time system because it requires ( 1 ) flexible design , ( 2 ) adaptive graceful degradation and ( 3 ) effective use of different modalities of sensors and actuators . To address these characteristics , we summarize SAFER ( System-level Architecture for Failure Evasion in Real-time applications ) our previous work on flexible system design . We then present a conceptual framework for autonomous vehicles to provide adaptive graceful degradation and support for using different types of sensors/actuators when a failure happens . We motivate our proposed framework with various scenarios , and we describe how SAFER can be extended to support the proposed conceptual framework .
2K_dev_284	Coding behavioral video is an important method used by researchers to understand social phenomenon . Unfortunately , traditional hand-coding approaches can take days or weeks of time to complete . Recent work has shown that these tasks can be completed quickly by leveraging the parallelism of large online crowds , but using the crowd introduces new concerns about accuracy , reliability , privacy , and cost . To explore these issues , we conducted interviews with 12 researchers who frequently code behavioral video , to investigate common practices and challenges with video coding . We find accuracy and privacy to be the researchers ' primary concerns . To explore this more concretely , we used sample videos to investigate whether crowds can accurately recognize instances of commonly coded behaviors , and show that the crowd yields accurate results . Then , we demonstrate a method for obfuscating participant identity with a video blur filter , and find , as expected , that workers ' ability to identify participants decreases as blur level increases . The workers ' ability to accurately and reliably code behaviors also decreases , but not as steeply as the identity test . This trade-off between coding quality and privacy protection suggests that researchers can use online crowds to code for some key behaviors in video without compromising participant identity . We conclude with a discussion of how researchers can balance privacy and accuracy on their own data using a system we introduce called Incognito .
2K_dev_285	For pipe guided wave inspection systems , it can often be difficult to achieve accurate localization performance due to the pipe 's geometry . Many localization techniques focus on the first arrival for processing , but this often results in a poor circumferential resolution . Furthermore , the pipe 's circular geometry generates multipath arrivals that make data interpretation difficult . In this paper , however , we utilize this multipath behavior by combining the standard delay-and-sum localization method with a simple multipath model for a pipe . Using experimental data from a transmitting source , we show that our method significantly improves circumferential resolution and reduces localization artifacts when compared with the standard delay-and-sum method .
2K_dev_286	If Lisa visits Dr. Brown , and there is no record of the drug he prescribed her , can we find it ? Data sources , much to analysts ' dismay , are too often plagued with incompleteness , making business analytics over the data difficult . Data entries with incomplete values are ignored , making some analytic queries fail to accurately describe how an organization is performing . We introduce a principled way of performing value imputation on missing values , allowing a user to choose a correct value after viewing possible values and why they were inferred . We achieve this by turning our data into a graph network and performing link prediction on nodes of interest using the belief propagation algorithm .
2K_dev_287	A cognitive assistance application combines a wearable device such as Google Glass with cloudlet processing to provide step-by-step guidance on a complex task . In this paper , we focus on user assistance for narrow and well-defined tasks that require specialized knowledge and/or skills . We describe proof-of-concept implementations for four different tasks : assembling 2D Lego models , freehand sketching , playing ping-pong , and recommending context-relevant YouTube tutorials . We then reflect on the difficulties we faced in building these applications , and suggest future research that could simplify the creation of similar applications .
2K_dev_288	The utilization of Building Information Modeling ( BIM ) has been growing significantly and translating into the support of various tasks within the construction industry . In relation to such a growth , many approaches that leverage dimensions of information stored in BIM model are being developed . Through this , it is possible to allow all stakeholders to retrieve and generate information from the same model , enabling them to work cohesively . To identify gaps of existing work and evaluate new studies in this area , a BIM application framework is developed and discussed in this paper . Such a framework gives an overview of BIM applications in the construction industry . A literature review , within this framework , has been conducted and the result reveals a research gap for BIM applications in the project domains of quality , safety and environmental management . A computable multi-dimensional ( nD ) model is difficult to establish in these areas because with continuously changing conditions , the decision making rules for evaluating whether an individual component is considered good quality , or whether a construction site is safe , also vary as the construction progresses . A process of expanding from 3D to computable nD models , specifically , a possible way to integrate safety , quality and carbon emission variables into BIM during the construction phase of a project is explained in this paper . As examples , the processes of utilizing nD models on real construction sites are described . It is believed to benefit the industry by providing a computable BIM and enabling all project participants to extract any information required for decision making . Finally , the framework is used to identify areas to extend BIM research .
2K_dev_289	Let us consider that someone is starting a research on a topic that is unfamiliar to them . Which seminal papers have influenced the topic the most ? What is the genealogy of the seminal papers in this topic ? These are the questions that they can raise , which we try to answer in this paper . First , we propose an algorithm that finds a set of seminal papers on a given topic . We also address the performance and scalability issues of this sophisticated algorithm . Next , we discuss the measures to decide how much a paper is influenced by another paper . Then , we propose an algorithm that constructs a genealogy of the seminal papers by using the influence measure and citation information . Finally , through extensive experiments with a large volume of a real-world academic literature data , we show the effectiveness and efficiency of our approach .
2K_dev_290	Online social networks and the World Wide Web lead to large underlying graphs that might not be completely known because of their size . To compute reliable statistics , we have to resort to sampling the network . In this paper , we investigate four network sampling methods to estimate the network degree distribution and the so-called biased degree distribution of a 3.7 million wireless subscriber network . We measure the quality of our estimates of the degree distributions by using the Kolmogorov-Smirnov statistic . Among all four sampling methods , node sampling yields Pareto optimal sample sizes in terms of the Kolomogorov-Smirnov statistic for the degree distribution , while node-by-edge sampling yields optimal sample sizes for the biased distribution . We also find that random walk sampling performs better than the Metropolis-Hastings random walk .
2K_dev_291	As kidney exchange programs are growing , manipulation by hospitals becomes more of an issue . Assuming that hospitals wish to maximize the number of their own patients who receive a kidney , they may have an incentive to withhold some of their incompatible donorpatient pairs and match them internally , thus harming social welfare . We study mechanisms for two-way exchanges that are strategyproof , i.e. , make it a dominant strategy for hospitals to report all their incompatible pairs . We establish lower bounds on the welfare loss of strategyproof mechanisms , both deterministic and randomized , and propose a randomized mechanism that guarantees at least half of the maximum social welfare in the worst case . Simulations using realistic distributions for blood types and other parameters suggest that in practice our mechanism performs much closer to optimal .
2K_dev_292	AbstractVulnerability assessment serves to identify vulnerabilities , develop responses , and drive the risk-management process . In identifying vulnerabilities , it is fundamental to identify and rank critical assets , which include vital systems , facilities , processes , and information necessary to maintain continuity of service . During emergencies in the facility management domain , first responders typically search for critical assets , both related to business continuity and value to the organization . This paper presents a formalized approach to reason about building systems and content to support vulnerability assessment in building emergencies caused by failures in building systems ( e.g. , sprinkler line leak , power outage ) . The developed reasoning approach enables a first responder to perform flexible searches and prioritize critical spaces and pieces of equipment that need to be protected in an emergency by leveraging existing building and content representations found in building information models ( BIM ) ...
2K_dev_293	Data locality and parallelism are critical optimization objectives for performance on modern multi-core machines . Both coarse-grain parallelism ( e.g. , multi-core ) and fine-grain parallelism ( e.g. , vector SIMD ) must be effectively exploited , but despite decades of progress at both ends , current compiler optimization schemes that attempt to address data locality and both kinds of parallelism often fail at one of the three objectives . We address this problem by proposing a 3-step framework , which aims for integrated data locality , multi-core parallelism and SIMD execution of programs . We define the concept of vectorizable codelets , with properties tailored to achieve effective SIMD code generation for the codelets . We leverage the power of a modern high-level transformation framework to restructure a program to expose good ISA-independent vectorizable codelets , exploiting multi-dimensional data reuse . Then , we generate ISA-specific customized code for the codelets , using a collection of lower-level SIMD-focused optimizations . We demonstrate our approach on a collection of numerical kernels that we automatically tile , parallelize and vectorize , exhibiting significant performance improvements over existing compilers .
2K_dev_294	Abstract Guided wave ultrasonics is an attractive monitoring technique for damage diagnosis in large-scale plate and pipe structures . Damage can be detected by comparing incoming records with baseline records collected on intact structure . However , during long-term monitoring , environmental and operational conditions often vary significantly and produce large changes in the ultrasonic signals , thereby challenging the baseline comparison based damage detection . Researchers developed temperature compensation methods to eliminate the effects of temperature variation , but they have limitations in practical implementations . In this paper , we develop a robust damage detection method based on singular value decomposition ( SVD ) . We show that the orthogonality of singular vectors ensures that the effect of damage and that of environmental and operational variations are separated into different singular vectors . We report on our field ultrasonic monitoring of a 273.05mm outer diameter pipe segment , which belongs to a hot water piping system in continuous operation . We demonstrate the efficacy of our method on experimental pitchcatch records collected during seven months . We show that our method accurately detects the presence of a mass scatterer , and is robust to the environmental and operational variations exhibited in the practical system .
2K_dev_295	We propose a non-intrusive approach for monitoring virtual machines ( VMs ) in the cloud . At the core of this approach is a mechanism for selective real-time monitoring of guest file updates within VM instances . This mechanism is agentless , requiring no guest VM support . It has low virtual I/O overhead , low latency for emitting file updates , and a scalable design . Its central design principle is distributed streaming of file updates inferred from introspected disk sector writes . The mechanism , called DS-VMI , enables many system administration tasks that involve monitoring files to be performed outside VMs .
2K_dev_296	The 13th Workshop on Privacy in the Electronic Society is held on November 3 , 2014 in Scottsdale , Arizona , USA in conjunction with the 21st ACM Conference on Computer and Communications Security . The goal of this workshop is to discuss the problems of privacy in global interconnected societies and possible solutions to them . The workshop program includes 17 full papers and 9 short papers on a diverse set of exciting privacy topics selected from a set of 67 total submissions . Specific areas covered include but are not limited to healthcare privacy , censorship circumvention , anonymous communication , web tracking , location and social network privacy .
2K_dev_297	In social settings , individuals interact through webs of relationships . Each individual is a node in a complex network ( or graph ) of interdependencies and generates data , lots of data . We label the data by its source , or formally stated , we index the data by the nodes of the graph . The resulting signals ( data indexed by the nodes ) are far removed from time or image signals indexed by well ordered time samples or pixels . DSP , discrete signal processing , provides a comprehensive , elegant , and efficient methodology to describe , represent , transform , analyze , process , or synthesize these well ordered time or image signals . This paper extends to signals on graphs DSP and its basic tenets , including filters , convolution , z -transform , impulse response , spectral representation , Fourier transform , frequency response , and illustrates DSP on graphs by classifying blogs , linear predicting and compressing data from irregularly located weather stations , or predicting behavior of customers of a mobile service provider .
2K_dev_298	Spatial Pyramid Matching ( SPM ) assumes that the spatial Bag-of-Words ( BoW ) representation is independent of data . However , evidence has shown that the assumption usually leads to a suboptimal representation . In this paper , we propose a novel method called Jensen-Shannon ( JS ) Tiling to learn the BoW representation from data directly at the BoW level . The proposed JS Tiling is especially appropriate for large-scale datasets as it is orders of magnitude faster than existing methods , but with comparable or even better classification precision . Experimental results on four benchmarks including two TRECVID12 datasets validate that JS Tiling outperforms the SPM and the state-of-the-art methods . The runtime comparison demonstrates that selecting BoW representations by JS Tiling is more than 1,000 times faster than running classifiers . Besides , JS Tiling is an important component contributing to CMU Teams ' final submission in TRECVID 2012 Multimedia Event Detection .
2K_dev_299	We present the design and analysis of a nearly-linear work parallel algorithm for solving symmetric diagonally dominant ( SDD ) linear systems . On input an SDD n-by-n matrix A with m nonzero entries and a vector b , our algorithm computes a vector $ \tilde { x } $ such that $ \|\tilde { x } - A^ { + } b\|_ { A } \leq\varepsilon\cdot\| { A^ { + } b } \|_ { A } $ in $ O ( m\log^ { O ( 1 ) } { n } \log { \frac { 1 } { \varepsilon } } ) $ work and $ O ( m^ { 1/3+\theta } \log\frac { 1 } { \varepsilon } ) $ depth for any ? > 0 , where A + denotes the Moore-Penrose pseudoinverse of A . The algorithm relies on a parallel algorithm for generating low-stretch spanning trees or spanning subgraphs . To this end , we first develop a parallel decomposition algorithm that in O ( mlog O ( 1 ) n ) work and polylogarithmic depth , partitions a graph with n nodes and m edges into components with polylogarithmic diameter such that only a small fraction of the original edges are between the components . This can be used to generate low-stretch spanning trees with average stretch O ( n ? ) in O ( mlog O ( 1 ) n ) work and O ( n ? ) depth for any ? > 0 . Alternatively , it can be used to generate spanning subgraphs with polylogarithmic average stretch in O ( mlog O ( 1 ) n ) work and polylogarithmic depth . We apply this subgraph construction to derive a parallel linear solver . By using this solver in known applications , our results imply improved parallel randomized algorithms for several problems , including single-source shortest paths , maximum flow , minimum-cost flow , and approximate maximum flow .
2K_dev_300	Heating , Ventilation and Air-Conditioning ( HVAC ) systems account for more than 15 % of the total energy consumption in the US . In order to improve the energy efficiency of HVAC systems , researchers have developed hundreds of algorithms to automatically analyze their performance . However , the complex information , such as configurations of HVAC systems , layouts and materials of building elements and dynamic data from the control systems , required by these algorithms inhibits the process of deploying them in real-world facilities . To address this challenge , we envision a framework that automatically integrates the required information items and provides them to the performance analysis algorithms for HVAC systems . This paper presents an approach to identify and document the information requirements from the publications that describe these algorithms . We extend the Information Delivery Manual ( IDM ) approach so that the identified information requirements can be mapped to multiple information sources that use various formats and schemas . This paper presents the extensions to the IDM approach and the results of using it to identify information requirements for performance analysis algorithms of HVAC systems .
2K_dev_301	A password composition policy restricts the space of allowable passwords to eliminate weak passwords that are vulnerable to statistical guessing attacks . Usability studies have demonstrated that existing password composition policies can sometimes result in weaker password distributions ; hence a more principled approach is needed . We introduce the first theoretical model for optimizing password composition policies . We study the computational and sample complexity of this problem under different assumptions on the structure of policies and on users ' preferences over passwords . Our main positive result is an algorithm that -- with high probability -- - constructs almost optimal policies ( which are specified as a union of subsets of allowed passwords ) , and requires only a small number of samples of users ' preferred passwords . We complement our theoretical results with simulations using a real-world dataset of 32 million passwords .
2K_dev_302	Time synchronization in wireless sensor networks is important for event ordering and efficient communication scheduling . In this paper , we introduce an external hardwarebased clock tuning circuit that can be used to improve synchronization and significantly reduce clock drift over long periods of time without waking up the host MCU . This is accomplished through two main hardware sub-systems . First , we improve upon the circuit presented in [ 1 ] that synchronizes clocks using the ambient magnetic fields emitted from power lines . The new circuit uses an electric field front-end as opposed to the original magnetic-field sensor , which makes the design more compact , lower-power , lower-cost , exhibit less jitter and improves robustness to noise generated by nearby appliances . Second , we present a low-cost hardware tuning circuit that can be used to continuously trim a micro-controller 's low-power clock at runtime . Most time synchronization approaches require a CPU to periodically adjust internal counters to accommodate for clock drift . Periodic discrete updates can introduce interpolation errors as compared to continuous update approaches and they require the CPU to expend energy during these wake up periods . Our hardware-based external clock tuning circuit allows the main CPU to remain in a deep-sleep mode for extended periods while an external circuit compensates for clock drift . We show that our new synchronization circuit consumes 60 % less power than the original design and is able to correct clock drift rates to within 0.01 ppm without power hungry and expensive precision clocks .
2K_dev_303	Split fabrication , the process of splitting an IC into an untrusted and trusted tier , facilitates access to the most advanced semiconductor manufacturing capabilities available in the world without requiring disclosure of design intent . While researchers have investigated the security of logic blocks in the context of split fabrication , the security of IP blocks , another key component of an SoC , has not been examined . Our security analysis of IP block designs , specifically embedded memory and analog components , shows that they are vulnerable to recognition attacks at the untrusted foundry due to the use of standardized floorplans and leaf cell layouts . We propose methodologies to design these blocks efficiently and securely , and demonstrate their effectiveness using 130nm split fabricated testchips .
2K_dev_304	In this paper we address the distributed estimation of a dynamic ( time varying ) random field . The dynamic field is globally observable ( by the entire sensor network ) , but not locally observable ( at each sensor ) . We present a distributed Kalman-type estimator such that the estimate at each sensor is unbiased with bounded mean-squared estimation error . The challenges with distributed estimation by a network of sensors lie in the estimation of fields with unstable dynamics . Our distributed Kalman filter type estimator , which includes a consensus step on the pseudo-innovations , a modified version of the filter innovations , is able to track arbitrary unstable dynamics , as long as the sensor network connectivity is above a threshold determined by the degree of instability of the field dynamics , regardless of the specifics of the local observations .
2K_dev_305	Self-powered vehicles that interact with the physical world , such as spacecraft , require computing platforms with predictable timing behavior and a low energy demand . Energy consumption can be reduced by choosing energy-efficient designs for both hardware and software components of the platform . We leverage the state-of-the-art in energy-efficient hardware design by adopting Heterogeneous Multi-core Processors with support for Dynamic Voltage and Frequency Scaling and Dynamic Power Management . We address the problem of allocating real-time software components onto heterogeneous cores such that total energy is minimized . Our approach is to start from an analytically justified target load distribution and find a task assignment heuristic that approximates it . Our analysis shows that neither balancing the load nor assigning all load to the cheapest core is the best load distribution strategy , unless the cores are extremely alike or extremely different . The optimal load distribution is then formulated as a solution to a convex optimization problem . A heuristic that approximates this load distribution and an alternative method that leverages the solution explicitly are proposed as viable task assignment methods . The proposed methods are compared to state-of-the-art on simulated problem instances and in a case study of a soft-real-time application on an off-the-shelf ARM big.LITTLE heterogeneous processor .
2K_dev_306	Harnessing crowds can be a powerful mechanism for increasing innovation . However , current approaches to crowd innovation rely on large numbers of contributors generating ideas independently in an unstructured way . We introduce a new approach called distributed analogical idea generation , which aims to make idea generation more effective and less reliant on chance . Drawing from the literature in cognitive science on analogy and schema induction , our approach decomposes the creative process in a structured way amenable to using crowds . In three experiments we show that distributed analogical idea generation leads to better ideas than example-based approaches , and investigate the conditions under which crowds generate good schemas and ideas . Our results have implications for improving creativity and building systems for distributed crowd innovation .
2K_dev_307	How to fairly allocate divisible resources , and why computer scientists should take notice .
2K_dev_308	Reranking has been a focal technique in multimedia retrieval due to its efficacy in improving initial retrieval results . Current reranking methods , however , mainly rely on the heuristic weighting . In this paper , we propose a novel reranking approach called Self-Paced Reranking ( SPaR ) for multimodal data . As its name suggests , SPaR utilizes samples from easy to more complex ones in a self-paced fashion . SPaR is special in that it has a concise mathematical objective to optimize and useful properties that can be theoretically verified . It on one hand offers a unified framework providing theoretical justifications for current reranking methods , and on the other hand generates a spectrum of new reranking schemes . This paper also advances the state-of-the-art self-paced learning research which potentially benefits applications in other fields . Experimental results validate the efficacy and the efficiency of the proposed method on both image and video search tasks . Notably , SPaR achieves by far the best result on the challenging TRECVID multimedia event search task .
2K_dev_309	Memory layout transformations via data reorganization are very common operations , which occur as a part of the computation or as a performance optimization in data-intensive applications . These operations require inefficient memory access patterns and roundtrip data movement through the memory hierarchy , failing to utilize the performance and energy-efficiency potentials of the memory subsystem . This paper proposes a high-bandwidth and energy-efficient hardware accelerated memory layout transform ( HAMLeT ) system integrated within a 3D-stacked DRAM . HAMLeT uses a low-overhead hardware that exploits the existing infrastructure in the logic layer of 3D-stacked DRAMs , and does not require any changes to the DRAM layers , yet it can fully exploit the locality and parallelism within the stack by implementing efficient layout transform algorithms . We analyze matrix layout transform operations ( such as matrix transpose , matrix blocking and 3D matrix rotation ) and demonstrate that HAMLeT can achieve close to peak system utilization , offering up to an order of magnitude performance improvement compared to the CPU and GPU memory subsystems which does not employ HAMLeT .
2K_dev_310	This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop , integrate , and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations ' information systems . Our system combines structural and semantic information from a real corporate database of monitored activity on their users ' computers to detect independently developed red team inserts of malicious insider activities . We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior , indicators of unusual activities , high-dimensional statistical patterns , temporal sequences , and normal graph evolution . Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams . We have also developed a visual language for specifying combinations of features , baselines , peer groups , time periods , and algorithms to detect anomalies suggestive of instances of insider threat behavior . We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users . We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data .
2K_dev_311	Support for multiple concurrent applications is an important enabler for promoting the use of sensor networks as an infrastructure technology , where multiple users can deploy their applications independently . In such a scenario , different applications on a node may transmit packets at distinct periods , causing the node to change from sleep to active state more often , which negatively impacts the energy consumption of the whole network . In this paper , we propose to batch the transmissions together by defining a harmonizing period to align the transmissions from multiple applications at periodic boundaries . This harmonizing period is then leveraged to design a protocol that coordinates the transmissions across nodes and provides real-time guarantees in a multi-hop network . This protocol , which we call Network- Harmonized Scheduling ( NHS ) , takes advantage of the periodicity introduced to assign offsets to nodes at different hop-levels such that collisions are always avoided , and deterministic behavior is enforced . NHS is a light-weight and distributed protocol that does not require any global state-keeping mechanism . We implemented NHS on the Contiki operating system and show how it can achieve a duty-cycle comparable to an ideal TDMA approach .
2K_dev_312	We describe a new algorithm for computing the Voronoi diagram of a set of n points in constant-dimensional Euclidean space . The running time of our algorithm is O ( f log n log ) where f is the output complexity of the Voronoi diagram and is the spread of the input , the ratio of largest to smallest pairwise distances . Despite the simplicity of the algorithm and its analysis , it improves on the state of the art for all inputs with polynomial spread and near-linear output size . The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation . Then , the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity , yielding the output sensitive bound . The removal only involves local flips and is inspired by kinetic data structures .
2K_dev_313	Color descriptors are one of the important features used in content-based image retrieval . The dominant color descriptor ( DCD ) represents a few perceptually dominant colors in an image through color quantization . For image retrieval based on DCD , the earth movers distance ( EMD ) and the optimal color composition distance were proposed to measure the dissimilarity between two images . Although providing good retrieval results , both methods are too time-consuming to be used in a large image database . To solve the problem , we propose a new distance function that calculates an approximate earth movers distance in linear time . To calculate the dissimilarity in linear time , the proposed approach employs the space-filling curve for multidimensional color space . To improve the accuracy , the proposed approach uses multiple curves and adjusts the color positions . As a result , our approach achieves order-of-magnitude time improvement but incurs small errors . We have performed extensive experiments to show the effectiveness and efficiency of the proposed approach . The results reveal that our approach achieves almost the same results with the EMD in linear time .
2K_dev_314	In previous work , we developed the scaled SIS process , which models the dynamics of SIS epidemics over networks . With the scaled SIS process , we can consider networks that are finite-sized and of arbitrary topology ( i.e. , we are not restricted to specific classes of networks ) . We derived for the scaled SIS process a closed-form expression for the time-asymptotic probability distribution of the states of all the agents in the network . This closed-form solution of the equilibrium distribution explicitly exhibits the underlying network topology through its adjacency matrix . This paper determines which network configuration is the most probable . We prove that , for a range of epidemics parameters , this combinatorial problem leads to a submodular optimization problem , which is exactly solvable in polynomial time . We relate the most-probable configuration to the network structure , in particular , to the existence of high density subgraphs . Depending on the epidemics parameters , subset of agents may be more likely to be infected than others ; these more-vulnerable agents form subgraphs that are denser than the overall network . We illustrate our results with a 193 node social network and the 4941 node Western US power grid under different epidemics parameters .
2K_dev_315	We propose a novel discrete signal processing framework for structured datasets that arise from social , economic , biological , and physical networks . Our framework extends traditional discrete signal processing theory to datasets with complex structure that can be represented by graphs , so that data elements are indexed by graph nodes and relations between elements are represented by weighted graph edges . We interpret such datasets as signals on graphs , introduce the concept of graph filters for processing such signals , and discuss important properties of graph filters , including linearity , shift-invariance , and invertibility . We then demonstrate the application of graph filters to data classification by demonstrating that a classifier can be interpreted as an adaptive graph filter . Our experiments demonstrate that the proposed approach achieves high classification accuracy .
2K_dev_316	Semantic search in video is a novel and challenging problem in information and multimedia retrieval . Existing solutions are mainly limited to text matching , in which the query words are matched against the textual metadata generated by users . This paper presents a state-of-the-art system for event search without any textual metadata or example videos . The system relies on substantial video content understanding and allows for semantic search over a large collection of videos . The novelty and practicality is demonstrated by the evaluation in NIST TRECVID 2014 , where the proposed system achieves the best performance . We share our observations and lessons in building such a state-of-the-art system , which may be instrumental in guiding the design of the future system for semantic search in video .
2K_dev_317	AbstractDuring planning and execution of construction projects , project planners and managers make various assumptions with respect to execution of construction activities , availability of resources , suitability of construction methods , and status of preceding activities . However , not all of these assumptions are explicitly documented and verified before the construction activities start . Decisions made based on invalid assumptions can negatively impact the outcomes of construction projects , such as rework , activity delays , and extra material cost . To address the problems caused by invalid assumptions , this paper proposed to develop a formal approach to capture and represent assumptions and proactively verify assumptions to reduce the uncertainties associated with construction projects . To develop such a formal approach , an initial step is to identify characteristics of assumptions . The research team conducted two detailed case studies ( i.e. , a bridge rehabilitation project and a research lab renovation p ...
2K_dev_318	In this work , we have proposed an Augmented Linear Discriminant Analysis ( ALDA ) approach to identify identical twins . It learns a common subspace that not only can identify from which family the individual comes , but also can distinguish between individuals within the same family . We evaluate the ALDA against the traditional LDA approach for subspace learning on the Notre Dame twin database . We have shown that the proposed ALDA method with the aid of facial asymmetry features significantly outperforms other well-established facial descriptors ( LBP , LTP , LTrP ) , and the ALDA subspace method does a much better job in distinguishing identical twins than LDA . We are able to achieve 48.50 % VR at 0.1 % FAR for identifying family membership of identical twin individuals in the crowd and an averaged 82.58 % VR at 0.1 % FAR for verifying identical twin individuals within the same family , a significant improvement over traditional descriptors and traditional LDA method .
2K_dev_319	Accurate inference of molecular and functional interactions among genes , especially in multicellular organisms such as Drosophila , often requires statistical analysis of correlations not only between the magnitudes of gene expressions , but also between their temporal-spatial patterns . The ISH ( in-situ-hybridization ) -based gene expression micro-imaging technology offers an effective approach to perform large-scale spatial-temporal profiling of whole-body mRNA abundance . However , analytical tools for discovering gene interactions from such data remain an open challenge due to various reasons , including difficulties in extracting canonical representations of gene activities from images , and in inference of statistically meaningful networks from such representations . In this paper , we present GINI , a machine learning system for inferring gene interaction networks from Drosophila embryonic ISH images . GINI builds on a computer-vision-inspired vector-space representation of the spatial pattern of gene expression in ISH images , enabled by our recently developed system ; and a new multi-instance-kernel algorithm that learns a sparse Markov network model , in which , every gene ( i.e. , node ) in the network is represented by a vector-valued spatial pattern rather than a scalar-valued gene intensity as in conventional approaches such as a Gaussian graphical model . By capturing the notion of spatial similarity of gene expression , and at the same time properly taking into account the presence of multiple images per gene via multi-instance kernels , GINI is well-positioned to infer statistically sound , and biologically meaningful gene interaction networks from image data . Using both synthetic data and a small manually curated data set , we demonstrate the effectiveness of our approach in network building . Furthermore , we report results on a large publicly available collection of Drosophila embryonic ISH images from the Berkeley Drosophila Genome Project , where GINI makes novel and interesting predictions of gene interactions . Software for GINI is available at http : //sailing.cs.cmu.edu/Drosophila_ISH_images/
2K_dev_320	Background : Association analysis using genome-wide expression quantitative trait locus ( eQTL ) data investigates the effect that genetic variation has on cellular pathways and leads to the discovery of candidate regulators . Traditional analysis of eQTL data via pairwise statistical significance tests or linear regression does not leverage the availability of the structural information of the transcriptome , such as presence of gene networks that reveal correlation and potentially regulatory relationships among the study genes . We employ a new eQTL mapping algorithm , GFlasso , which we have previously developed for sparse structured regression , to reanalyze a genome-wide yeast dataset . GFlasso fully takes into account the dependencies among expression traits to suppress false positives and to enhance the signal/noise ratio . Thus , GFlasso leverages the gene-interaction network to discover the pleiotropic effects of genetic loci that perturb the expression level of multiple ( rather than individual ) genes , which enables us to gain more power in detecting previously neglected signals that are marginally weak but pleiotropically significant . Results : While eQTL hotspots in yeast have been reported previously as genomic regions controlling multiple genes , our analysis reveals additional novel eQTL hotspots and , more interestingly , uncovers groups of multiple contributing eQTL hotspots that affect the expression level of functional gene modules . To our knowledge , our study is the first to report this type of gene regulation stemming from multiple eQTL hotspots . Additionally , we report the results from in-depth bioinformatics analysis for three groups of these eQTL hotspots : ribosome biogenesis , telomere silencing , and retrotransposon biology . We suggest candidate regulators for the functional gene modules that map to each group of hotspots . Not only do we find that many of these candidate regulators contain mutations in the promoter and coding regions of the genes , in the case of the Ribi group , we provide experimental evidence suggesting that the identified candidates do regulate the target genes predicted by GFlasso . Conclusions : Thus , this structured association analysis of a yeast eQTL dataset via GFlasso , coupled with extensive bioinformatics analysis , discovers a novel regulation pattern between multiple eQTL hotspots and functional gene modules . Furthermore , this analysis demonstrates the potential of GFlasso as a powerful computational tool for eQTL studies that exploit the rich structural information among expression traits due to correlation , regulation , or other forms of biological dependencies .
2K_dev_321	Human action may be observed from multi-view , which are highly related but sometimes look different from each other . Traditional metric learning algorithms have achieved satisfactory performance in single-view , but they often fail or do not satisfy when they are utilized to fuse different views . Thus , multi-view discriminative and structured dictionary learning with group sparsity and graph model ( GM-GS-DSDL ) is proposed to fuse different views and recognize human actions . First , spatio-temporal interest points are extracted for each view , and then multi-view bag of words ( MVBoW ) representation is employed , at the same time , the graph model is also utilized to fuse different views , which will remove overlapped interest points to explore their consistency properties . Furthermore , GM-GS-DSDL is formulated to discover the latent correlation among multiple views . In addition , we also issue a new multi-view action dataset with RGB , depth and skeleton data ( called CVS-MV-RGBD ) . Large-scale experimental results on multi-view IXMAX and CVS-MV-RGBD datasets show that the exploring of consistency properties of different views by graph model is very useful , moreover , GM-GS-DSDL for each view , which are learnt simultaneously , can further improve the fusion performance . Comparative experiments demonstrate that our proposed algorithm can obtain competing performance against the state-of-the-art methods . Multi-view bag of words ( MVBoW ) representation is employed.The graph model is also utilized to fuse different views.We formulate the multi-view action recognition task as a joint dictionary learning ( DL ) problem.We release a new multi-view action dataset which contains RGB , depth and skeleton data .
2K_dev_322	Objective . Analyzing and interpreting the activity of a heterogeneous population of neurons can be challenging , especially as the number of neurons , experimental trials , and experimental conditions increases . One approach is to extract a set of latent variables that succinctly captures the prominent co-fluctuation patterns across the neural population . A key problem is that the number of latent variables needed to adequately describe the population activity is often greater than 3 , thereby preventing direct visualization of the latent space . By visualizing a small number of 2-d projections of the latent space or each latent variable individually , it is easy to miss salient features of the population activity . Approach . To address this limitation , we developed a Matlab graphical user interface ( called DataHigh ) that allows the user to quickly and smoothly navigate through a continuum of different 2-d projections of the latent space . We also implemented a suite of additional visualization tools ( including playing out population activity timecourses as a movie and displaying summary statistics , such as covariance ellipses and average timecourses ) and an optional tool for performing dimensionality reduction . Main results . To demonstrate the utility and versatility of DataHigh , we used it to analyze single-trial spike count and single-trial timecourse population activity recorded using a multi-electrode array , as well as trial-averaged population activity recorded using single electrodes . Significance . DataHigh was developed to fulfil a need for visualization in exploratory neural data analysis , which can provide intuition that is critical for building scientific hypotheses and models of population activity .
2K_dev_323	Current applications have produced graphs on the order of hundreds of thousands of nodes and millions of edges . To take advantage of such graphs , one must be able to find patterns , outliers , and communities . These tasks are better performed in an interactive environment , where human expertise can guide the process . For large graphs , though , there are some challenges : the excessive processing requirements are prohibitive , and drawing hundred-thousand nodes results in cluttered images hard to comprehend . To cope with these problems , we propose an innovative framework suited for any kind of tree-like graph visual design . GMine integrates 1 ) a representation for graphs organized as hierarchies of partitions-the concepts of SuperGraph and Graph-Tree ; and 2 ) a graph summarization methodology-CEPS . Our graph representation deals with the problem of tracing the connection aspects of a graph hierarchy with sub linear complexity , allowing one to grasp the neighborhood of a single node or of a group of nodes in a single click . As a proof of concept , the visual environment of GMine is instantiated as a system in which large graphs can be investigated globally and locally .
2K_dev_324	Many content-based video search ( CBVS ) systems have been proposed to analyze the rapidly-increasing amount of user-generated videos on the Internet . Though the accuracy of CBVS systems have drastically improved , these high accuracy systems tend to be too inefficient for interactive search . Therefore , to strive for real-time web-scale CBVS , we perform a comprehensive study on the different components in a CBVS system to understand the trade-offs between accuracy and speed of each component . Directions investigated include exploring different low-level and semantics-based features , testing different compression factors and approximations during video search , and understanding the time v.s . accuracy trade-off of reranking . Extensive experiments on data sets consisting of more than 1,000 hours of video showed that through a combination of effective features , highly compressed representations , and one iteration of reranking , our proposed system can achieve an 10,000-fold speedup while retaining 80 % accuracy of a state-of-the-art CBVS system . We further performed search over 1 million videos and demonstrated that our system can complete the search in 0.975 seconds with a single core , which potentially opens the door to interactive web-scale CBVS for the general public .
2K_dev_325	Even simpler or more usable privacy controls and notices might not improve users ' decision-making regarding sharing of personal information . Control might paradoxically increase riskier disclosure by soothing privacy concerns . Transparency might be easily muted , and its effect arbitrarily controlled , through simple framing or misdirections .
2K_dev_326	We ( the authors of CSCWs program ) have finite time and energy that can be invested into our publications and the research communities we value . While we want our work to have the most impact possible , we also want to grow and support productive research communities within which to have this impact . This panel discussion explores the costs and benefits of submitting papers to various tiers of conferences and journals surrounding CSCW and reflects on the value of investing hours into building up a research community .
2K_dev_327	We demonstrate a Visual Light Communication ( VLC ) system that enables LED lighting luminaires to communicate with cameras on mobile devices . Each LED pulses at a frequency above the humanly perceivable flicker threshold where cameras and photodiodes can still detect changes in light intensity . Our modulation scheme supports multiple light sources in a single collision domain , and works for both , line-of-sight ( LOS ) operation as well as from reflected surfaces like those found in architectural lighting . The spatial confinement of light makes this system ideal for use as localization landmarks . Our demonstration includes four LED ambient lights acting as location landmarks transmitting modulated data . A mobile device receiving and processing the signal displays the ID and RSSI of the closest landmark . Interacting with the system will allow users to see the practical effects of multiple-access , frequency of operation , distance from the lights , camera parameters and camera motion .
2K_dev_328	The relationship between occupant activity and electricity consumption is inextricably linked . It has been difficult to both gather detailed energy data and information about occupants ' daily lives as well as understand their relationship quantitatively . There is significant past work on activity recognition in homes and load prediction , but there is limited understanding of how activities can inform consumption or vice versa . Our work begins by characterizing power data as provided by plug-level meters from one household . Association and sequential rule mining techniques are applied to extract explicit rules that may be useful for forming the basis of demand patterns . Initial findings include the identification of device groups but highlight the challenges of modeling complex patterns and event rarity .
2K_dev_329	To reduce costs , organizations may outsource data storage and data processing to third-party clouds . This raises confidentiality concerns , since the outsourced data may have sensitive information . Although semantically secure encryption of the data prior to outsourcing alleviates these concerns , it also renders the outsourced data useless for any relational processing . Motivated by this problem , we present two database encryption schemes that reveal just enough information about structured data to support a wide-range of relational queries . Our main contribution is a definition and proof of security for the two schemes . This definition captures confidentiality offered by the schemes using a novel notion of equivalence of databases from the adversary 's perspective . As a specific application , we adapt an existing algorithm for finding violations of a rich class of privacy policies to run on logs encrypted under our schemes and observe low to moderate overheads .
2K_dev_330	Developers use cryptographic APIs in Android with the intent of securing data such as passwords and personal information on mobile devices . In this paper , we ask whether developers use the cryptographic APIs in a fashion that provides typical cryptographic notions of security , e.g. , IND-CPA security . We develop program analysis techniques to automatically check programs on the Google Play marketplace , and find that 10.327 out of 11,748 applications that use cryptographic APIs -- 88 % overall -- make at least one mistake . These numbers show that applications do not use cryptographic APIs in a fashion that maximizes overall security . We then suggest specific remediations based on our analysis towards improving overall cryptographic security in Android applications .
2K_dev_331	As a way to relieve the tedious work of manual annotation , active learning plays important roles in many applications of visual concept recognition . In typical active learning scenarios , the number of labelled data in the seed set is usually small . However , most existing active learning algorithms only exploit the labelled data , which often suffers from over-fitting due to the small number of labelled examples . Besides , while much progress has been made in binary class active learning , little research attention has been focused on multi-class active learning . In this paper , we propose a semi-supervised batch mode multi-class active learning algorithm for visual concept recognition . Our algorithm exploits the whole active pool to evaluate the uncertainty of the data . Considering that uncertain data are always similar to each other , we propose to make the selected data as diverse as possible , for which we explicitly impose a diversity constraint on the objective function . As a multi-class active learning algorithm , our algorithm is able to exploit uncertainty across multiple classes . An efficient algorithm is used to optimize the objective function . Extensive experiments on action recognition , object classification , scene recognition , and event detection demonstrate its advantages .
2K_dev_332	AbstractWith the increased usage of building information models ( BIMs ) during construction , has BIM become a medium for delivering as-built building information . It is important to maintain accurate and up-to-date information stored in a BIM so that it can become a reliable data source throughout the service life of a facility . Laser scanning technology is able to capture accurate geometric data in the form of a point cloud and to depict the existing condition of a building . Hence , point cloud data captured by laser scans can be used as references to update a given BIM . An important step during the update process is to match segments of elements captured by a point cloud to building components modeled in a BIM , so that the discrepancies between the two data sets can be identified . Typically , features depicted within point cloud segments and BIM components are used in the matching process . However , understanding is limited regarding which features enable the matching process and how these features perform ... .
2K_dev_333	This paper proposes Halite , a novel , fast , and scalable clustering method that looks for clusters in subspaces of multidimensional data . Existing methods are typically superlinear in space or execution time . Halite 's strengths are that it is fast and scalable , while still giving highly accurate results . Specifically the main contributions of Halite are : 1 ) Scalability : it is linear or quasi linear in time and space regarding the data size and dimensionality , and the dimensionality of the clusters ' subspaces ; 2 ) Usability : it is deterministic , robust to noise , does n't take the number of clusters as an input parameter , and detects clusters in subspaces generated by original axes or by their linear combinations , including space rotation ; 3 ) Effectiveness : it is accurate , providing results with equal or better quality compared to top related works ; and 4 ) Generality : it includes a soft clustering approach . Experiments on synthetic data ranging from five to 30 axes and up to 1 \rm million points were performed . Halite was in average at least 12 times faster than seven representative works , and always presented highly accurate results . On real data , Halite was at least 11 times faster than others , increasing their accuracy in up to 35 percent . Finally , we report experiments in a real scenario where soft clustering is desirable .
2K_dev_334	Analysts synthesize complex , qualitative data to uncover themes and concepts , but the process is time-consuming , cognitively taxing , and automated techniques show mixed success . Crowdsourcing could help this process through on-demand harnessing of flexible and powerful human cognition , but incurs other challenges including limited attention and expertise . Further , text data can be complex , high-dimensional , and ill-structured . We address two major challenges unsolved in prior crowd clustering work : scaffolding expertise for novice crowd workers , and creating consistent and accurate categories when each worker only sees a small portion of the data . To address these challenges we present an empirical study of a two-stage approach to enable crowds to create an accurate and useful overview of a dataset : A ) we draw on cognitive theory to assess how re-representing data can shorten and focus the data on salient dimensions ; and B ) introduce an iterative clustering approach that provides workers a global overview of data . We demonstrate a classification-plus-context approach elicits the most accurate categories at the most useful level of abstraction .
2K_dev_335	Sequencing of RNAs ( RNA-Seq ) has revolutionized the field of transcriptomics , but the reads obtained often contain errors . Read error correction can have a large impact on our ability to accurately assemble transcripts . This is especially true for de novo transcriptome analysis , where a reference genome is not available . Current read error correction methods , developed for DNA sequence data , can not handle the overlapping effects of non-uniform abundance , polymorphisms and alternative splicing . Here we present SEquencing Error CorrEction in Rna-seq data ( SEECER ) , a hidden Markov Model ( HMM ) based method , which is the first to successfully address these problems . SEECER efficiently learns hundreds of thousands of HMMs and uses these to correct sequencing errors . Using human RNA-Seq data , we show that SEECER greatly improves on previous methods in terms of quality of read alignment to the genome and assembly accuracy . To illustrate the usefulness of SEECER for de novo transcriptome studies , we generated new RNA-Seq data to study the development of the sea cucumber Parastichopus parvimensis . Our corrected assembled transcripts shed new light on two important stages in sea cucumber development . Comparison of the assembled transcripts to known transcripts in other species has also revealed novel transcripts that are unique to sea cucumber , some of which we have experimentally validated . Supporting website : http : //sb.cs.cmu.edu/seecer/ .
2K_dev_336	The HMT3522 progression series of human breast cells have been used to discover how tissue architecture , microenvironment and signaling molecules affect breast cell growth and behaviors . However , much remains to be elucidated about malignant and phenotypic reversion behaviors of the HMT3522-T4-2 cells of this series . We employed a pan-cell-state strategy , and analyzed jointly microarray profiles obtained from different state-specific cell populations from this progression and reversion model of the breast cells using a tree-lineage multi-network inference algorithm , Treegl . We found that different breast cell states contain distinct gene networks . The network specific to non-malignant HMT3522-S1 cells is dominated by genes involved in normal processes , whereas the T4-2-specific network is enriched with cancer-related genes . The networks specific to various conditions of the reverted T4-2 cells are enriched with pathways suggestive of compensatory effects , consistent with clinical data showing patient resistance to anticancer drugs . We validated the findings using an external dataset , and showed that aberrant expression values of certain hubs in the identified networks are associated with poor clinical outcomes . Thus , analysis of various reversion conditions ( including non-reverted ) of HMT3522 cells using Treegl can be a good model system to study drug effects on breast cancer .
2K_dev_337	For urban driving , knowledge of ego-vehicle 's position is a critical piece of information that enables ad- vanced driver-assistance systems or self-driving cars to execute safety-related , autonomous driving maneuvers . This is because , without knowing the current location , it is very hard to autonomously execute any driving maneuvers for the future . The existing solutions for localization rely on a combination of Global Navigation Satellite System ( GNSS ) , an inertial mea- surement unit , and a digital map . However , on urban driving environments , due to poor satellite geometry and disruption of radio signal reception , their longitudinal and lateral errors are too significant to be used to guide an autonomous system . To enhance the existing system 's localization capability , this work presents an effort of developing a vision-based lateral localization algorithm . The algorithm aims at reliably counting , with or without observations of lane-markings , the number road-lanes and identifying the index of the road-lane on the roadway that our vehicle happens to be driving on . Testings the proposed algorithms against inter-city and inter-state highway videos showed promising results in terms of counting the number of road-lanes and the indices of the current road-lanes . I . INTRODUCTION
2K_dev_338	We present a generic event detection system evaluated in the Surveillance Event Detection ( SED ) task of TRECVID 2012 . We investigate a statistical approach with spatio-temporal features applied to seven event classes , which were defined by the SED task . This approach is based on local spatio-temporal descriptors , called MoSIFT and generated by pair-wise video frames . A Gaussian Mixture Model ( GMM ) is learned to model the distribution of the low level features . Then for each sliding window , the Fisher vector encoding [ improvedFV ] is used to generate the sample representation . The model is learnt using a Linear SVM for each event . The main novelty of our system is the introduction of Fisher vector encoding into video event detection . Fisher vector encoding has demonstrated great success in image classification . The key idea is to model the low level visual features as a Gaussian Mixture Model and to generate an intermediate vector representation for bag of features . FV encoding uses higher order statistics in place of histograms in the standard BoW . FV has several good properties : ( a ) it can naturally separate the video specific information from the noisy local features and ( b ) we can use a linear model for this representation . We build an efficient implementation for FV encoding which can attain a 10 times speed-up over real-time . We also take advantage of non-trivial object localization techniques to feed into the video event detection , e.g . multi-scale detection and non-maximum suppression . This approach outperformed the results of all other teams submissions in TRECVID SED 2012 on four of the seven event types .
2K_dev_339	Background : Drug discovery and development has been aided by high throughput screening methods that detect compound effects on a single target . However , when using focused initial screening , undesirable secondary effects are often detected late in the development process after significant investment has been made . An alternative approach would be to screen against undesired effects early in the process , but the number of possible secondary targets makes this prohibitively expensive . Results : This paper describes methods for making this global approach practical by constructing predictive models for many target responses to many compounds and using them to guide experimentation . We demonstrate for the first time that by jointly modeling targets and compounds using descriptive features and using active machine learning methods , accurate models can be built by doing only a small fraction of possible experiments . The methods were evaluated by computational experiments using a dataset of 177 assays and 20,000 compounds constructed from the PubChem database . Conclusions : An average of nearly 60 % of all hits in the dataset were found after exploring only 3 % of the experimental space which suggests that active learning can be used to enable more complete characterization of compound effects than otherwise affordable . The methods described are also likely to find widespread application outside drug discovery , such as for characterizing the effects of a large number of compounds or inhibitory RNAs on a large number of cell or tissue phenotypes .
2K_dev_340	With the increasing availability of metropolitan transportation data , such as those from vehicle Global Positioning Systems ( GPSs ) and road-side sensors , it has become viable for authorities , operators , and individuals to analyze the data for better understanding of the transportation system and , possibly , improved utilization and planning of the system . We report our experience in building the Visual Analytics for Intelligent Transportation ( VAIT ) system , which is the first system on real-life large-scale data sets for intelligent transportation . Our key observation is that metropolitan transportation data are inherently visual as they are spatio-temporal around road networks . Therefore , we visualize and manage traffic data , together with digital maps , and support analytical queries through this interactive visual interface . As a case study , we demonstrate VAIT on real-world taxi GPS and meter data sets from 15 000 taxis running for two months in a Chinese city of over 10 million people . We discuss the technical challenges in data calibration , storage , visualization , and query processing and offer first-hand lessons learned from developing the system . Based on our extensive empirical experiment results , VAIT beats state-of-the-art methods and systems in terms of scalability , efficiency , and effectiveness and offers us an easy-to-use , efficient , and scalable platform to shed more light on intelligent transportation research .
2K_dev_341	Computers are often used in performance of popular music , but most often in very restricted ways , such as keyboard synthesizers where musicians are in complete control , or pre-recorded or sequenced music where musicians follow the computer 's drums or click track . An interesting and yet little-explored possibility is the computer as highly autonomous performer of popular music , capable of joining a mixed ensemble of computers and humans . Considering the skills and functional requirements of musicians leads to a number of predictions about future humancomputer music performance ( HCMP ) systems for popular music . We describe a general architecture for such systems and describe some early implementations and our experience with them .
2K_dev_342	Fast Fourier transform algorithms on large data sets achieve poor performance on various platforms because of the inefficient strided memory access patterns . These inefficient access patterns need to be reshaped to achieve high performance implementations . In this paper we formally restructure 1D , 2D and 3D FFTs targeting a generic machine model with a two-level memory hierarchy requiring block data transfers , and derive memory access pattern efficient algorithms using custom block data layouts . Using the Kronecker product formalism , we integrate our optimizations into Spiral framework . In our evaluations we demonstrate that Spiral generated hardware designs achieve close to theoretical peak performance of the targeted platform and offer significant speed-up ( up to 6.5x ) compared to naive baseline algorithms .
2K_dev_343	How can we detect suspicious users in large online networks ? Online popularity of a user or product ( via follows , page-likes , etc . ) can be monetized on the premise of higher ad click-through rates or increased sales . Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck . Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent ( but sometimes honest ) users . However , small-scale , stealthy attacks may go unnoticed due to the nature of low-rank Eigen analysis used in practice . In this work , we take an adversarial approach to find and prove claims about the weaknesses of modern , state-of-the-art spectral methods and propose fBox , an algorithm designed to catch small-scale , stealth attacks that slip below the radar . Our algorithm has the following desirable properties : ( a ) it has theoretical underpinnings , ( b ) it is shown to be highly effective on real data and ( c ) it is scalable ( linear on the input size ) . We evaluate fBox on a large , public 41.7 million node , 1.5 billion edge who-follows-whom social graph from Twitter in 2010 and with high precision identify many suspicious accounts which have persisted without suspension even to this day .
2K_dev_344	As the complexity of software for Cyber-Physical Systems ( CPS ) rapidly increases , multi-core processors and parallel programming models such as OpenMP become appealing to CPS developers for guaranteeing timeliness . Hence , a parallel task on multi-core processors is expected to become a vital component in CPS such as a self-driving car , where tasks must be scheduled in real-time . In this paper , we extend the fork-join parallel task model to be scheduled in real-time , where the number of parallel threads can vary depending on the physical attributes of the system . To efficiently schedule the proposed task model , we develop the task stretch transform . Using this transform for global Deadline Monotonic scheduling for fork-join real-time tasks , we achieve a resource augmentation bound of 3.73 . In other words , any task set that is feasible on m unit-speed processors can be scheduled by the proposed algorithm on m processors that are 3.73 times faster . The proposed scheme is implemented on Linux/RK as a proof of concept , and ported to Boss , the self-driving vehicle that won the 2007 DARPA Urban Challenge . We evaluate our scheme on Boss by showing its driving quality , i.e. , curvature and velocity profiles of the vehicle .
2K_dev_345	A well-studied approach to the design of voting rules views them as maximum likelihood estimators ; given votes that are seen as noisy estimates of a true ranking of the alternatives , the rule must reconstruct the most likely true ranking . We argue that this is too stringent a requirement , and instead ask : How many votes does a voting rule need to reconstruct the true ranking ? We define the family of pairwise-majority consistent rules , and show that for all rules in this family the number of samples required from the Mallows noise model is logarithmic in the number of alternatives , and that no rule can do asymptotically better ( while some rules like plurality do much worse ) . Taking a more normative point of view , we consider voting rules that surely return the true ranking as the number of samples tends to infinity ( we call this property accuracy in the limit ) ; this allows us to move to a higher level of abstraction . We study families of noise models that are parametrized by distance functions , and find voting rules that are accurate in the limit for all noise models in such general families . We characterize the distance functions that induce noise models for which pairwise-majority consistent rules are accurate in the limit , and provide a similar result for another novel family of position-dominance consistent rules . These characterizations capture three well-known distance functions .
2K_dev_346	The quality and effectiveness of the load following services provided by centralized control of thermostatically controlled loads depend highly on the communication requirements and the underlying cyberinfrastructure characteristics . Specifically , ensuring end-user comfort while providing real-time demand response services depends on the availability of the information provided from the thermostatically controlled loads to the main controller regarding their operating statuses and internal temperatures . State estimation techniques can be used to infer the necessary information from the aggregate power consumption of these loads , replacing the need for an upstream communication platform carrying information from appliances to the main controller in real-time . In this paper , we introduce a moving horizon mean squared error state estimator with constraints as an alternative to a Kalman filter approach , which assumes a linear model without constraints . The results show that some improvement is possible for scenarios when loads are expected to be toggled frequently .
2K_dev_347	Reading text on the Web is a challenging task for many people , such as those with cognitive impairments , reading difficulties or people who are learning a new language . In this paper we present a web browser plug-in to help with reading Spanish text on the Web . The plug-in is freely available for Chrome and presents definitions and simpler synonyms on demand for the selected web text . The tool was modified following the suggestions of 5 people ( 2 with diagnosed dyslexia ) who tested the tool using the think aloud protocol and undertook a subsequent interview .
2K_dev_348	We propose a novel method MultiModal Pseudo Relevance Feedback ( MMPRF ) for event search in video , which requires no search examples from the user . Pseudo Relevance Feedback has shown great potential in retrieval tasks , but previous works are limited to unimodal tasks with only a single ranked list . To tackle the event search task which is inherently multimodal , our proposed MMPRF takes advantage of multiple modalities and multiple ranked lists to enhance event search performance in a principled way . The approach is unique in that it leverages not only semantic features , but also non-semantic low-level features for event search in the absence of training data . Evaluated on the TRECVID MEDTest dataset , the approach improves the baseline by up to 158 % in terms of the mean average precision . It also significantly contributes to CMU Team 's final submission in TRECVID-13 Multimedia Event Detection .
2K_dev_349	How do we find patterns and anomalies in very large graphs with billions of nodes and edges ? How to mine such big graphs efficiently ? Big graphs are everywhere , ranging from social networks and mobile call networks to biological networks and the World Wide Web . Mining big graphs leads to many interesting applications including cyber security , fraud detection , Web search , recommendation , and many more . In this paper we describe Pegasus , a big graph mining system built on top of MapReduce , a modern distributed data processing platform . We introduce GIM-V , an important primitive that Pegasus uses for its algorithms to analyze structures of large graphs . We also introduce HEigen , a large scale eigensolver which is also a part of Pegasus . Both GIM-V and HEigen are highly optimized , achieving linear scale up on the number of machines and edges , and providing 9.2x and 76x faster performance than their naive counterparts , respectively . Using Pegasus , we analyze very large , real world graphs with billions of nodes and edges . Our findings include anomalous spikes in the connected component size distribution , the 7 degrees of separation in a Web graph , and anomalous adult advertisers in the who-follows-whom Twitter social network .
2K_dev_350	We address the challenging problem of utilizing related exemplars for complex event detection while multiple features are available . Related exemplars share certain positive elements of the event , but have no uniform pattern due to the huge variance of relevance levels among different related exemplars . None of the existing multiple feature fusion methods can deal with the related exemplars . In this paper , we propose an algorithm which adaptively utilizes the related exemplars by cross-feature learning . Ordinal labels are used to represent the multiple relevance levels of the related videos . Label candidates of related exemplars are generated by exploring the possible relevance levels of each related exemplar via a cross-feature voting strategy . Maximum margin criterion is then applied in our framework to discriminate the positive and negative exemplars , as well as the related exemplars from different relevance levels . We test our algorithm using the large scale TRECVID 2011 dataset and it gains promising performance .
2K_dev_351	Multi-core CPUs with multiple levels of parallelism ( i.e . data level , instruction level and task/core level ) have become the mainstream CPUs for commodity computing systems . Based on the multi-core CPUs , in this paper we developed a high performance computing framework for AC contingency calculation ( ACCC ) to fully utilize the computing power of commodity systems for online and real time applications . Using Woodbury matrix identity based compensation method , we transform and pack multiple contingency cases of different outages into a fine grained vectorized data parallel programming model . We implement the data parallel programming model using SIMD instruction extension on x86 CPUs , therefore , fully taking advantages of the CPU core with SIMD floating point capability . We also implement a thread pool scheduler for ACCC on multi-core CPUs which automatically balances the computing loads across CPU cores to fully utilize the multi-core capability . We test the ACCC solver on the IEEE test systems and on the Polish 3000-bus system using a quad-core Intel Sandy Bridge CPU . The optimized ACCC solver achieves close to linear speedup ( SIMD width multiply core numbers ) comparing to scalar implementation and is able to solve a complete N-1 line outage AC contingency calculation of the Polish grid within one second on a commodity CPU . It enables the complete ACCC as a real-time application on commodity computing systems .
2K_dev_352	This paper introduces a 3D-stacked logic-in-memory ( LiM ) system that integrates the 3D die-stacked DRAM architecture with the application-specific LiM IC to accelerate important data-intensive computing . The proposed system comprises a fine-grained rank-level 3D die-stacked DRAM device and extra LiM layers implementing logic-enhanced SRAM blocks that are dedicated to a particular application . Through silicon vias ( TSVs ) are used for vertical interconnections providing the required bandwidth to support the high performance LiM computing . We performed a comprehensive 3D DRAM design space exploration and exploit the efficient architectures to accelerate the computing that can balance the performance and power . Our experiments demonstrate orders of magnitude of performance and power efficiency improvements compared with the traditional multithreaded software implementation on modern CPU .
2K_dev_353	Since the dawn of mobile computing two decades ago , the unique constraints of mobility have shaped the software architectures of systems . We now stand at the threshold of the next major transformation in computing : one in which the rich sensing and interaction capabilities of mobile devices are seamlessly fused with compute-intensive and data-intensive processing in the cloud . This heralds a new genre of software that augments human perception and cognition in a mobile context . A major obstacle to realizing this vision is the large and variable end-to-end WAN latency between mobile device and cloud , and the possibility of WAN disruptions . Cloudlets have emerged as an architectural solution to this problem . A cloudlet represents the middle tier of a 3-tier hierarchy : mobile device -- cloudlet -- cloud , and can be viewed as a `` data center in a box '' whose goal is to `` bring the cloud closer '' . A cloudlet-based hardware/software ecosystem inspires futuristic visions such as cognitive assistance for attention-challenged mobile users , scalable crowd-sourcing of first-person video , and ubiquitous mobile access to one 's legacy world . Realizing these visions will require many technical challenges to be overcome . It will also require us to rethink a wide range of issues in areas such as privacy , software licensing , and business models .
2K_dev_354	This paper presents explicit convergence rates for a class of deterministic distributed augmented Lagrangian methods . The expressions for the convergence rates show the dependence on the underlying network parameters . Simulations illustrate the analytical results .
2K_dev_355	Objective . Brain ? computer interfaces ( BCIs ) are being developed to assist paralyzed people and amputees by translating neural activity into movements of a computer cursor or prosthetic limb . Here we introduce a novel BCI task paradigm , intended to help accelerate improvements to BCI systems . Through this task , we can push the performance limits of BCI systems , we can quantify more accurately how well a BCI system captures the user ? s intent , and we can increase the richness of the BCI movement repertoire . Approach . We have implemented an instructed path task , wherein the user must drive a cursor along a visible path . The instructed path task provides a versatile framework to increase the difficulty of the task and thereby push the limits of performance . Relative to traditional point-to-point tasks , the instructed path task allows more thorough analysis of decoding performance and greater richness of movement kinematics . Main results . We demonstrate that monkeys are able to perform the instructed path task in a closed-loop BCI setting . We further investigate how the performance under BCI control compares to native arm control , whether users can decrease their movement variability in the face of a more demanding task , and how the kinematic richness is enhanced in this task . Significance . The use of the instructed path task has the potential to accelerate the development of BCI systems and their clinical translation .
2K_dev_356	Guided waves , such as Lamb waves , are attractive tools for monitoring large civil infrastructures due to their sensitivity to damage . Yet , interpreting guided wave data and identifying effects resulting from damage is often complicated by the multimodal and dispersive characteristics of guided waves and multipath interference from the medium 's boundaries . In this paper , we present a method to decompose guided waves into a collection of multipath arrivals by combining sparse wavenumber analysis , a methodology for accurately recovering multimodal and dispersive properties , with additional l 1 minimization techniques . Its application to experimental Lamb wave data shows that the estimates all correspond to expected paths .
2K_dev_357	Pipes carrying fluids under pressure are critical components in infrastructure and industry . Changes in ultrasonic signals detected by piezoelectric transducers can indicate scattering from flaws , but signals also change dramatically from environmental and operational variations . Extensive pitch-catch tests are performed on pressurized pipe segments in a working hot-water supply system that experiences ongoing variations in pressure , temperature , and flow rate . Singular value decomposition is applied to differentiate the change caused by scatterer from the changes produced by benign variations . We build a singular value decomposition ( SVD ) based change detector that is sensitive to the mass scatterer but insensitive to the changes produced by operational and environmental variations , and we show examples of its successful performance on field experiments data .
2K_dev_358	Understanding the dynamics of biochemical networks is a major goal of systems biology . Due to the heterogeneity of cells and the low copy numbers of key molecules , spatially resolved approaches are required to fully understand and model these systems . Until recently , most spatial modeling was performed using geometries obtained either through manual segmentation or manual fabrication both of which are time-consuming and tedious . Similarly , the system of reactions associated with the model had to be manually defined , a process that is both tedious and error-prone for large networks . As a result , spatially resolved simulations have typically only been performed in a limited number of geometries , which are often highly simplified , and with small reaction networks .
2K_dev_359	The convergence of mobile computing and cloud computing is predicated on a reliable , high-bandwidth end-to-end network . This basic requirement is hard to guarantee in hostile environments such as military operations and disaster recovery . In this article , the authors examine how VM-based cloudlets that are located in close proximity to associated mobile devices can overcome this challenge . This article is part of a special issue on the edge of the cloud .
2K_dev_360	ABSTRACT One of the main challenges in real-world application of guided-waves based nondestructive evaluation ( NDE ) of pipelines is their sensitivity to changes in environmental and operational conditions ( EOC ) that these structures are subject to . In spite of many favorable characteristics of gu ided-waves for NDE of pipes , their multi-modal , dispersive , and multi-path characteristics result in complex signa ls whose interpretation is a difficult task . Studies that have considered the effects of EOC varia tions either fail to reflect realistic EOC scenarios ( e.g. , limited to particular effects of specific EOCs , like time shifting effects of temperature in plates ) or lack the necessary understanding of the effects of EOC variations on different aspects of the developed damage detection approaches . Such gaps limit the extensibility of these approaches to pipeline applications outside of controlled environments . This paper motivates the idea of analytically incorporating the effects of temperature and flow rate variations into damage diagnosis of pipes , through a number of case studie s. A review of the existing literature on guided-wave based testing is also provided . For damage detection , a linear supervised classification method , namely linear discriminant analysis ( LDA ) , is applied to experimental guided-wave data recorded from a hot water piping system under regular operation . Principal components , obtained through principal component analysis ( PCA ) , and Fourier transforms of the signals are two sets of damage-sensitive features ( DSF ) that are examined for LDA-based classification . The effects of temperature and flow rate difference among testing and training datasets on ( A ) detection performance and ( B ) goodness of fit of the method to the data are investigated . Keywords : Pipeline Monitoring , Guided-waves , Nondestructive Eval uation , Structural Health Monitoring , Effects of Temperature and Flow Rate , Damage Detection , Linear Discriminant Analysis .
2K_dev_361	As threats from malicious attackers increase , integrity approaches in networked embedded systems will have to evolve to provide both the security and safety aspects of integrity in a unified approach . And they 'll have to do it on a shoestring , using only a few bits per message . To help achieve this , you can exploit two embedded-systems characteristics : the periodic sampling of messages and the system 's inertia .
2K_dev_362	Over the last several years there has been an explosion of powerful , affordable , multi-touch devices . This provides an outstanding opportunity for novel data visualization techniques that leverage new interaction methods and minimize their barriers to entry . In this paper we describe an approach for multivariate data visualization that uses physics-based affordances that are easy to intuit , constraints that are easy to apply and visualize , and a consistent view as data is manipulated in order to promote data exploration and interrogation . We provide a framework for exploring this problem space , and an example proof of concept system called Kinetica . We describe the results of a user study that suggest users of Kinetica were able to explore multiple dimensions of data at once , identify outliers , and discover trends with minimal training .
2K_dev_363	To explore the exciting new domain of brain informatics , we invited several well-known experts to discuss the state of the art , the challenges , the opportunities , and the trends . In `` Creating Human-Level AI by Educating a Child Machine , '' Raj Reddy proposes an architecture for a `` child machine '' that can learn and is teachable . In `` Cyborg Intelligence , '' Zhaohui Wu , Gang Pan , and Nenggan Zheng describe a biological-machine system consisting of both an organic and a computing part . In `` Formal Minds and Biological Brains II : From the Mirage of Intelligence to a Science and Engineering of Consciousness , '' Paul F.M.J . Verschure discusses human-like cognitive architectures and describes the Distributed Adaptive Control ( DAC ) architecture for perception , cognition , and action . In `` The Challenges of Closed-Loop Invasive Brain-Machine Interfaces , '' Qiaosheng Zhang and Xiaoxiang Zheng discuss the challenges and trends in closed-loop brain-machine interfaces . In `` Neural Signal Processing in Brain-Machine Interfaces , '' Jose C. Principe takes a critical look at the challenges and opportunities of performing computation with pulses , as neurons do . In `` Neuroprosthesis Control via a Noninvasive Hybrid Brain-Computer Interface , '' Alex Kreilinger , Martin Rohm , Vera Kaiser , Robert Leeb , Rudiger Rupp , and Gernot R. Muller-Putz describe an example of the convergence of biological intelligence and machine intelligence in a hand-elbow neuroprosthesis control unit .
2K_dev_364	Signal recovery from noisy measurements is an important task that arises in many areas of signal processing . In this paper , we consider this problem for signals represented with graphs using a recently developed framework of discrete signal processing on graphs . We formulate graph signal denoising as an optimization problem and derive an exact closed-form solution expressed by an inverse graph filter , as well as an approximate iterative solution expressed by a standard graph filter . We evaluate the obtained algorithms by applying them to measurement denoising for temperature sensors and opinion combination for multiple experts .
2K_dev_365	At the 38th International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) in Vancouver , Canada , Jose M.F . Moura , in his acceptance speech for the IEEE Signal Processing Society Award , shared a number of thoughts that I found worthy of further reflection because they are an intricate part of the identity of signal processing as a technical discipline . The first of these thoughts was the difficulties many of us have experienced trying to explain what signal processing is to someone with no significant background in technical disciplines . The second of these thoughts was a recollection of the idea of `` Signal Processing Inside , '' presented in his column by then editor-in-chief of IEEE Signal Processing Magazine and current IEEE Signal Processing Society president , K.J . Ray Liu , [ S1 ] , and highlighted during ICASSP 2007 as a motto describing signal processing as a `` phantom technology , '' that was pervasive but at the same time a key contributor to technological advances from a position that is not readily apparent to the general public .
2K_dev_366	In this paper , we present the architecture , design and experiences from a wirelessly managed microgrid deployment in rural Les Anglais , Haiti . The system consists of a three-tiered architecture with a cloud-based monitoring and control service , a local embedded gateway infrastructure and a mesh network of wireless smart meters deployed at 52 buildings . Each smart meter device has an 802.15.4 radio that enables remote monitoring and control of electrical service . The meters communicate over a scalable multi-hop TDMA network back to a central gateway that manages load within the system . The gateway also provides an 802.11 interface for an on-site operator and a cellular modem connection to a cloud-backend that manages and stores billing and usage data . The cloud backend allows occupants in each home to pre-pay for electricity at a particular peak power limit using a text messaging service . The system activates each meter within seconds and locally enforces power limits with provisioning for theft detection . We believe that this fine-grained micro-payment model can enable sustainable power in otherwise unfeasible areas . This paper provides a chronology of our deployment and installation strategy that involved GPS-based site mapping along with various network conditioning actions required as the network evolved . Finally , we summarize key lessons learned and hypothesis about additional hardware that could be used to ease the tracing of faults like short circuits and downed lines within microgrids .
2K_dev_367	Proceedings : AACR 104th Annual Meeting 2013 ; Apr 6-10 , 2013 ; Washington , DC Understanding tumors as evolutionary systems is an important area of study with far-reaching implications in diagnostic and treatment paradigms . Computational phylogenetics is a valuable method for inferring tumor evolution in terms of evolutionary trees , phylogenies , where paths in a tree correspond to possible tumor progression pathways . The location of specific cell-types and patient samples in the tree provide information on tumor sub-types and development of heterogeneity . We previously developed a tumor phylogeny inference pipeline for array comparative genome hybridization ( aCGH ) -based tumor copy number profiles . Steps in the pipeline included extraction of robust progression markers from the data , which could differentiate stages of tumor evolution or the different paths in the tree , and assigning amplification states to the inferred markers in those stages . We introduced a novel multi-sample model for amplicon identification and calling , HMMCNA , which jointly extracted markers from and assigned amplification states to small sets of tumor aCGH profiles . HMMCNA employs a Hidden Markov Model ( HMM ) , a probabilistic model , to classify data into normal and amplified states based on an underlying distribution for the two copy number states and a hidden state space of possible amplification states . We assumed two possible amplification states per sample : normal ( 0 ) or amplified ( 1 ) . Joint segmentation and calling is performed by identifying a most likely sequence of amplification states across all genomic sites probes and samples . This approach limits in the number of samples the HMM can handle since the number of possible hidden amplification states increases exponentially with the number of samples . Here , we present an extension of the approach to handle large datasets . We incorporate a heuristic prior to the HMM classification to reduce the hidden state space by first screening out amplification states not strongly supported at any individual genome coordinates . The introduction of this heuristic reduces the state space on average by 99 % . We further reduce the set of possible amplification states based on the frequency of occurrence of the states by only allowing those states occuring at multiple aCGH probes or array genome coordinate . This step accounts for the presence of random noise in the data and gives a further reduction of 80 % . We demonstrate the method on a breast tumor aCGH dataset comprising copy number profiles derived from sectioned biopsy samples ( NCBI GEO [ GSE16672 ] [ 1 ] , Navin et al. , 2010 ) . Our method was able to quickly segment the data into sets of robust normal and amplified segments suitable for downstream phylogeny building . The amplicons inferred carried several known markers of tumor progression . Further steps include tuning the parameters of the HMM to handle noise-levels across different datasets . Citation Format : Ayshwarya Subramanian , Stanley Shackney , Russell Schwartz . Inference of tumor phylogenetic markers from large copy number datasets . [ abstract ] . In : Proceedings of the 104th Annual Meeting of the American Association for Cancer Research ; 2013 Apr 6-10 ; Washington , DC . Philadelphia ( PA ) : AACR ; Cancer Res 2013 ; 73 ( 8 Suppl ) : Abstract nr 5133. doi:10.1158/1538-7445.AM2013-5133 [ 1 ] : /lookup/external-ref ? link_type=NCBIGEO & access_num=GSE16672 & atom= % 2Fcanres % 2F73 % 2F8_Supplement % 2F5133.atom
2K_dev_368	Most sensory , cognitive and motor functions depend on the interactions of many neurons . In recent years , there has been rapid development and increasing use of technologies for recording from large numbers of neurons , either sequentially or simultaneously . A key question is what scientific insight can be gained by studying a population of recorded neurons beyond studying each neuron individually . Here , we examine three important motivations for population studies : single-trial hypotheses requiring statistical power , hypotheses of population response structure and exploratory analyses of large data sets . Many recent studies have adopted dimensionality reduction to analyze these populations and to find features that are not apparent at the level of individual neurons . We describe the dimensionality reduction methods commonly applied to population activity and offer practical advice about selecting methods and interpreting their outputs . This review is intended for experimental and computational researchers who seek to understand the role dimensionality reduction has had and can have in systems neuroscience , and who seek to apply these methods to their own data .
2K_dev_369	Spliddit is a first-of-its-kind fair division website , which offers provably fair solutions for the division of rent , goods , and credit . In this note , we discuss Spliddit 's goals , methods , and implementation .
2K_dev_370	Hybrid systems with both discrete and continuous dynamics are an important model for real-world cyber-physical systems . The key challenge is to ensure their correct functioning w.r.t . safety requirements . Promising techniques to ensure safety seem to be model-driven engineering to develop hybrid systems in a well-defined and traceable manner , and formal verification to prove their correctness . Their combination forms the vision of verification-driven engineering . Often , hybrid systems are rather complex in that they require expertise from many domains ( e. g. , robotics , control systems , computer science , software engineering , and mechanical engineering ) . Moreover , despite the remarkable progress in automating formal verification of hybrid systems , the construction of proofs of complex systems often requires nontrivial human guidance , since hybrid systems verification tools solve undecidable problems . It is , thus , not uncommon for development and verification teams to consist of many players with diverse expertise . This paper introduces a verification-driven engineering toolset that extends our previous work on hybrid and arithmetic verification with tools for ( 1 ) graphical ( UML ) and textual modeling of hybrid systems , ( 2 ) exchanging and comparing models and proofs , and ( 3 ) managing verification tasks . This toolset makes it easier to tackle large-scale verification tasks .
2K_dev_371	Data imbalance problem often exists in our real life dataset , especial for massive video dataset , however , the balanced data distribution and the same misclassification cost are assumed in traditional machine learning algorithms , thus , it will be difficult for them to accurately describe the true data distribution , and resulting in misclassification . In this paper , the data imbalance problem in semantic extraction under massive video dataset is exploited , and enhanced and hierarchical structure ( called EHS ) algorithm is proposed . In proposed algorithm , data sampling , filtering and model training are considered and integrated together compactly via hierarchical structure algorithm , thus , the performance of model can be improved step by step , and is robust and stability with the change of features and datasets . Experiments on TRECVID2010 Semantic Indexing demonstrate that our proposed algorithm has much more powerful performance than that of traditional machine learning algorithms , and keeps stable and robust when different kinds of features are employed . Extended experiments on TRECVID2010 Surveillance Event Detection also prove that our EHS algorithm is efficient and effective , and reaches top performance in four of seven events .
2K_dev_372	Cake cutting is a common metaphor for the division of a heterogeneous divisible good . There are numerous papers that study the problem of fairly dividing a cake ; a small number of them also take into account self-interested agents and consequent strategic issues , but these papers focus on fairness and consider a strikingly weak notion of truthfulness . In this paper we investigate the problem of cutting a cake in a way that is truthful , Pareto-efficient , and fair , where for the first time our notion of dominant strategy truthfulness is the ubiquitous one in social choice and computer science . We design both deterministic and randomized cake cutting mechanisms that are truthful and fair under different assumptions with respect to the valuation functions of the agents .
2K_dev_373	Mobile crowdsensing is becoming a vital technique for environment monitoring , infrastructure management , and social computing . However , deploying mobile crowdsensing applications in large-scale environments is not a trivial task . It creates a tremendous burden on application developers as well as mobile users . In this paper we try to reveal the barriers hampering the scale-up of mobile crowdsensing applications , and to offer our initial thoughts on the potential solutions to lowering the barriers .
2K_dev_374	We have been investigating vehicle-to-vehicle ( V2V ) communications as a part of co-operative driving in the context of autonomous driving . In this work , we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols and suggest required modifications to guarantee their safety and efficiency despite these impairments .
2K_dev_375	Smart metering systems in distribution networks provide near real-time , two-way information exchange between end users and utilities , enabling many advanced smart grid technologies . However , the fine grained real-time data as well as the various market functionalities also pose great risks to customer privacy . In this work we propose a secure multi-party computation ( SMC ) based privacy preserving smart metering system . Using the proposed SMC protocol , a utility is able to perform advanced market based demand management algorithms without knowing the actual values of private end user consumption and configuration data . Using homomorphic encryption , billing is secure and verifiable . We implemented a demonstration system that includes a graphical user interface and simulates real-world network communication of the proposed SMC-enabled smart meters . The demonstration shows the feasibility of our proposed privacy preserving protocol for advanced smart grid technologies which includes load management and retail level electricity market support .
2K_dev_376	Rating data is ubiquitous on websites such as Amazon , TripAdvisor , or Yelp . Since ratings are not static but given at various points in time , a temporal analysis of rating data provides deeper insights into the evolution of a product 's quality . In this work , we tackle the following question : Given the time stamped rating data for a product or service , how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous ? We propose a Bayesian model that represents the rating data as sequence of categorical mixture models . In contrast to existing methods , our method does not require any aggregation of the input but it operates on the original time stamped data . To capture the dynamic effects of the ratings , the categorical mixtures are temporally constrained : Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time . Our method automatically determines the intervals where anomalies occur , and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions . For learning our model , we propose an efficient algorithm combining principles from variational inference and dynamic programming . In our experimental study we show the effectiveness of our method and we present interesting discoveries on multiple real world datasets .
2K_dev_377	We introduce the Plug-Level Appliance Identification Dataset ( PLAID ) , a public and crowd-sourced dataset for load identification research consisting of short voltage and current measurements ( in the order of a few seconds ) for different residential appliances . The goal of PLAID is to provide a public library for high-resolution appliance measurements that can be integrated into existing or novel appliance identification algorithms . PLAID currently contains measurements for more than 200 different appliance instances , representing 11 appliance classes , and totaling more than a thousand records . In this demo we summarize the existing dataset , demonstrate how new records can be added to the library using a web interface and , finally , walk through a live example of how the library can be integrated into an existing non-intrusive load monitoring ( NILM ) algorithm framework .
2K_dev_378	Abstraction has emerged as a key component in solving extensive-form games of incomplete information . However , lossless abstractions are typically too large to solve , so lossy abstraction is needed . All prior lossy abstraction algorithms for extensive-form games either 1 ) had no bounds on solution quality or 2 ) depended on specific equilibrium computation approaches , limited forms of abstraction , and only decreased the number of information sets rather than nodes in the game tree . We introduce a theoretical framework that can be used to give bounds on solution quality for any perfect-recall extensive-form game . The framework uses a new notion for mapping abstract strategies to the original game , and it leverages a new equilibrium refinement for analysis . Using this framework , we develop the first general lossy extensive-form game abstraction method with bounds . Experiments show that it finds a lossless abstraction when one is available and lossy abstractions when smaller abstractions are desired . While our framework can be used for lossy abstraction , it is also a powerful tool for lossless abstraction if we set the bound to zero . Prior abstraction algorithms typically operate level by level in the game tree . We introduce the extensive-form game tree isomorphism and action subset selection problems , both important problems for computing abstractions on a level-by-level basis . We show that the former is graph isomorphism complete , and the latter NP-complete . We also prove that level-by-level abstraction can be too myopic and thus fail to find even obvious lossless abstractions .
2K_dev_379	Abstract Image patterns at different spatial levels are well organized , such as regions within one image and feature points within one region . These classes of spatial structures are hierarchical in nature . The appropriate integration and utilization of such relationship are important to improve the performance of region tagging . Inspired by the recent advances of sparse coding methods , we propose an approach , called Unified Dictionary Learning and Region Tagging with Hierarchical Sparse Representation . This approach consists of two steps : region representation and region reconstruction . In the first step , rather than using the l 1 -norm as it is commonly done in sparse coding , we add a hierarchical structure to the process of sparse coding and form a framework of tree-guided dictionary learning . In this framework , the hierarchical structures among feature points , regions , and images are encoded by forming a tree-guided multi-task learning process . With the learned dictionary , we obtain a better representation of training and testing regions . In the second step , we propose to use a sub-hierarchical structure to guide the sparse reconstruction for testing regions , i.e. , the structure between regions and images . Thanks to this hierarchy , the obtained reconstruction coefficients are more discriminate . Finally , tags are propagated to testing regions by the learned reconstruction coefficients . Extensive experiments on three public benchmark image data sets demonstrate that the proposed approach has better performance of region tagging than the current state of the art methods .
2K_dev_380	This paper describes the objectives and work developed in the project New Technologies and Interfaces for Music Education and Production by Universitat Politecnica de Valencia and the Computer Music Group from Carnegie Mellon University : Several education scenarios and application typologies are designed , the use of collaborative creation with Web 2.0 is proposed , and the first implemented applications are described .
2K_dev_381	ABSTRACT Guided wave ultrasonics is an attractive technique for structural health monitoring , especially on pressurized pipes . However , civil infrastructure components , including pipes , are often subject to large environmental and operational variations that prevent traditional baseline subtraction-based approaches from detecting damage . We collect ultrasonic data on a large-scale pipe segment in its normal operating conditions and observe large environmental variations . We developed a damage detection method based on singular value decomposition ( SVD ) that is robust to those benign variations . We further develop an online novelty detection framework based on our SVD method to detect the presence of a mass scatterer on the pipe at the same time that we co llect the data . We examine the framework with both synthetic simulations and field experimental data . The results show that the framework can effectively detect the presence of a scatterer and is robust to large environmental and operational variations . Keywords : Guided waves , pipe structural health monitoring , environmental and operational variations , singular value decomposition , online novelty detection
2K_dev_382	The execution of an agent 's complex activities , comprising sequences of simpler actions , sometimes leads to the clash of conflicting functions that must be optimized . These functions represent satisfaction , short-term as well as long-term objectives , costs and individual preferences . The way that these functions are weighted is usually unknown even to the decision maker . But if we were able to understand the individual motivations and compare such motivations among individuals , then we would be able to actively change the environment so as to increase satisfaction and/or improve performance . In this work , we approach the problem of providing highlevel and intelligible descriptions of the motivations of an agent , based on observations of such an agent during the fulfillment of a series of complex activities ( called sequential decisions in our work ) . A novel algorithm for the analysis of observational records is proposed . We also present a methodology that allows researchers to converge towards a summary description of an agent 's behaviors , through the minimization of an error measure between the current description and the observed behaviors . This work was validated using not only a synthetic dataset representing the motivations of a passenger in a public transportation network , but also real taxi drivers ' behaviors from their trips in an urban network . Our results show that our method is not only useful , but also performs much better than the previous methods , in terms of accuracy , efficiency and scalability .
2K_dev_383	We propose a new framework for distributed computation of average consensus . The presented framework leads to a systematic design of iterative algorithms that compute the consensus exactly , are guaranteed to converge in finite time , are computationally efficient , and require no online memory . We demonstrate that our approach is applicable to a broad class of networks . For remaining networks , our framework leads to the construction of approximating algorithms for consensus that are also guaranteed to compute in finite time . Our approach is inspired by graph filters introduced by the theoretical framework of signal processing on graphs .
2K_dev_384	The primary goal of a vehicular headlight is to improve safety in low-light and poor weather conditions . The typical headlight however has very limited flexibility - switching between high and low beams , turning off beams toward the opposing lane or rotating the beam as the vehicle turns - and is not designed for all driving environments . Thus , despite decades of innovation in light source technology , more than half of the vehicular accidents still happen at night even with much less traffic on the road . We will describe a new DMD-based design for a headlight that can be programmed to perform several tasks simultaneously and that can sense , react and adapt quickly to any environment with the goal of increasing safety for all drivers on the road . For example , we will be able to drive with high-beams without glaring any other driver and we will be able to see better during rain and snowstorms when the road is most treacherous to drive . The headlight can also increase contrast of lanes , markings and sidewalks and can alert drivers to sudden obstacles . In this talk , we will lay out the engineering challenges in building this headlight and share our experiences with the prototypes developed over the past two years . ( 2014 ) COPYRIGHT Society of Photo-Optical Instrumentation Engineers ( SPIE ) . Downloading of the abstract is permitted for personal use only .
2K_dev_385	Estimating the number of people within a room is important for a wide variety of applications including : HVAC load management , scheduling room allocations and guiding first responders to areas with trapped people . In this paper , we present an active sensing technique that uses changes in a room 's acoustic properties to estimate the number of occupants . Frequency dependent models of reverberation and room capacity are often used when designing auditoriums and concert halls . We leverage this property by using measured changes in the ultrasonic spectrum reflected back from a wide-band transmitter to estimate occupancy . A centrally located beacon transmits an ultrasonic chirp and then records how the signal dissipates over time . By analyzing the frequency response over the chirp 's bandwidth at a few known occupancy levels , we are able to extrapolate the response as the number of people in the room changes . We explore the design of an excitation signal that best senses the environment with the fewest number of training samples . Through experimentation , we show that our approach is able to capture the number of people in a wide-variety of room configurations with people counting accuracy below 10 % of the maximum room capacity count with as few as two training points . Finally , we provide a simple mechanism that allows our system to recalibrate when we know the room is empty so that it can adapt dynamically over time .
2K_dev_386	Organizations collect personal information from individuals to carry out their business functions . Federal privacy regulations , such as the Health Insurance Portability and Accountability Act ( HIPAA ) , mandate how this collected information can be shared by the organizations . It is thus incumbent upon the organizations to have means to check compliance with the applicable regulations . Prior work by Barth et . al . introduces two notions of compliance , weak compliance ( WC ) and strong compliance ( SC ) . WC ensures that present requirements of the policy can be met whereas SC also ensures obligations can be met . An action is compliant with a privacy policy if it is both weakly and strongly compliant . However , their definitions of compliance are restricted to only propositional linear temporal logic ( pLTL ) , which can not feasibly specify HIPAA . To this end , we present a policy specification language based on a restricted subset of first order temporal logic ( FOTL ) which can capture the privacy requirements of HIPAA . We then formally specify WC and SC for policies of our form . We prove that checking WC is feasible whereas checking SC is undecidable . We then formally specify the property WC entails SC , denoted by , which requires that each weakly compliant action is also strongly compliant . To check whether an action is compliant with such a policy , it is sufficient to only check whether the action is weakly compliant with that policy . We also prove that when a policy has the -property , the present requirements of the policy reduce to the safety requirements imposed by . We then develop a sound , semi-automated technique for checking whether practical policies have the -property . We finally use HIPAA as a case study to demonstrate the efficacy of our policy analysis technique .
2K_dev_387	The increase in the number of bloggers and the amount of information diffused in the blogosphere makes the blogosphere an important medium through which to communicate and exchange information . Accordingly , the interest in understanding the nature of the information diffusion in the blogosphere has also been increased . Existing studies in social networks have mainly focused on the information diffusion through explicit relationships between members . In this paper , we analyze the causes for the information diffusion without explicit relationships in the blogosphere . BlogCast , a functionality provided by blog-service providers to expose a high quality post on the portal main page , is found to be one of the main causes of the information diffusion without explicit relationships . We analyze the characteristics of the information diffusion through the BlogCast and its halo effect on the bloggers whose post has been exposed on the portal main page . In addition , we examine the sustainability of the halo effect of the BlogCast over time .
2K_dev_388	Many of the visual questions that blind people ask can not be easily answered with a single image or a short response , especially when questions are of an exploratory nature , e.g . what is in this area , or what tools are available on this work bench ? We introduce RegionSpeak to allow blind users to capture large areas of visual information , identify all of the objects within them , and explore their spatial layout with fewer interactions . RegionSpeak helps blind users capture all of the relevant visual information using an interface designed to support stitching multiple images together . We use a parallel crowdsourcing workflow that asks workers to define and describe regions of interest , allowing even complex images to be described quickly . The regions and descriptions are displayed on an auditory touchscreen interface , allowing users to know what is in a scene and how it is laid out .
2K_dev_389	The proliferation of Bluetooth Low-Energy ( BLE ) chipsets on mobile devices has lead to a wide variety of user-installable tags and beacons designed for location-aware applications . In this paper , we present the Acoustic Location Processing System ( ALPS ) , a platform that augments BLE transmitters with ultrasound in a manner that improves ranging accuracy and can help users configure indoor localization systems with minimal effort . A user places three or more beacons in an environment and then walks through a calibration sequence with their mobile device where they touch key points in the environment like the floor and the corners of the room . This process automatically computes the room geometry as well as the precise beacon locations without needing auxiliary measurements . Once configured , the system can track a user 's location referenced to a map . The platform consists of time-synchronized ultrasonic transmitters that utilize the bandwidth just above the human hearing limit , where mobile devices are still sensitive and can detect ranging signals . To aid in the mapping process , the beacons perform inter-beacon ranging during setup . Each beacon includes a BLE radio that can identify and trigger the ultrasonic signals . By using differences in propagation characteristics between ultrasound and radio , the system can classify if beacons are within Line-Of-Sight ( LOS ) to the mobile phone . In cases where beacons are blocked , we show how the phone 's inertial measurement sensors can be used to supplement localization data . We experimentally evaluate that our system can estimate three-dimensional beacon location with a Euclidean distance error of 16.1cm , and can generate maps with room measurements with a two-dimensional Euclidean distance error of 19.8cm . When tested in six different environments , we saw that the system can identify Non-Line-Of-Sight ( NLOS ) signals with over 80 % accuracy and track a user 's location to within less than 100cm .
2K_dev_390	We explore methods for improving the readability of real- time captions by allowing users to more easily switch their gaze between multiple visual information sources . Real-time captioning provides deaf and hard of hearing ( DHH ) users with access to spoken content during live events , and the web has allowed these services to be provided via remotely- located captioning services , and for web content itself . However , despite caption benefits , spoken language reading rates often result in DHH users falling behind spoken content , especially when the audio is paired with visual references . This is particularly true in classroom settings , where multi-modal content is the norm , and captions are often poorly positioned in the room , relative to speakers . Additionally , this accommodation can benefit other students who face temporary or `` situational '' disabilities such as listening to unfamiliar speech accents , or if a student is in a location with poor acoustics . In this paper , we explore pausing and highlighting as a means of helping DHH students keep up with live classroom content by helping them track their place when reading text involving visual references . Our experiments show that by providing users with a tool to more easily track their place in a transcript while viewing live video , it is possible for them to follow visual content that might otherwise have been missed . Both pausing and highlighting have a positive impact on students ' scores on comprehension tests , but highlighting is preferred to pausing , and yields nearly twice as large of an improvement . We then discuss several issues with captioning that we observed during our design process and user study , and then suggest future work that builds on these insights .
2K_dev_391	This paper introduces a 3D-stacked logic-in-memory ( LiM ) system to accelerate the processing of sparse matrix data that is held in a 3D DRAM system . We build a customized content addressable memory ( CAM ) hardware structure to exploit the inherent sparse data patterns and model the LiM based hardware accelerator layers that are stacked in between DRAM dies for the efficient sparse matrix operations . Through silicon vias ( TSVs ) are used to provide the required high inter-layer bandwidth . Furthermore , we adapt the algorithm and data structure to fully leverage the underlying hardware capabilities , and develop the necessary design framework to facilitate the design space evaluation and LiM hardware synthesis . Our simulation demonstrates more than two orders of magnitude of performance and energy efficiency improvements compared with the traditional multithreaded software implementation on modern processors .
2K_dev_392	AbstractIn order to save energy and improve the control of indoor environments , researchers have developed hundreds of computer algorithms that can automatically and continuously analyze the conditions of HVAC systems . However , the complex information requirements of these algorithms inhibit deploying them in real-world facilities . This paper proposes an integrated performance analysis framework that automatically collects , merges , and provides the information required by them . The information items that are used to represent the information needs are categorized , a domain-specific query language that can formally represent the query statements is formalized , and a library of mechanisms that can automatically reason about and retrieve the needed information is developed . In order to validate the performance of the query language and mechanisms , a prototype was also developed , which includes a graphic user interface that helps users to define the queries , and the reasoning mechanisms that process the queri ...
2K_dev_393	Automatic face recognition performance has been steadily improving over years of research , however it remains significantly affected by a number of factors such as illumination , pose , expression , resolution and other factors that can impact matching scores . The focus of this paper is the pose problem which remains largely overlooked in most real-world applications . Specifically , we focus on one-to-one matching scenarios where a query face image of a random pose is matched against a set of gallery images . We propose a method that relies on two fundamental components : ( a ) A 3D modeling step to geometrically correct the viewpoint of the face . For this purpose , we extend a recent technique for efficient synthesis of 3D face models called 3D Generic Elastic Model . ( b ) A sparse feature extraction step using subspace modeling and l1-minimization to induce pose-tolerance in coefficient space . This in return enables the synthesis of an equivalent frontal-looking face , which can be used towards recognition . We show significant performance improvements in verification rates compared to commercial matchers , and also demonstrate the resilience of the proposed method with respect to degrading input quality . We find that the proposed technique is able to match non-frontal images to other non-frontal images of varying angles .
2K_dev_394	Autonomous agents that operate as components of dynamic spatial systems are becoming increasingly popular and mainstream . Applications can be found in consumer robotics , in road , rail , and air transportation , manufacturing , and military operations . Unfortunately , the approaches to modeling and analyzing the behavior of dynamic spatial systems are just as diverse as these application domains . In this article , we discuss reasoning approaches for the medium-term control of autonomous agents in dynamic spatial systems , which requires a sufficiently detailed description of the agents behavior and environment but may still be conducted in a qualitative manner . We survey logic-based qualitative and hybrid modeling and commonsense reasoning approaches with respect to their features for describing and analyzing dynamic spatial systems in general , and the actions of autonomous agents operating therein in particular . We introduce a conceptual reference model , which summarizes the current understanding of the characteristics of dynamic spatial systems based on a catalog of evaluation criteria derived from the model . We assess the modeling features provided by logic-based qualitative commonsense and hybrid approaches for projection , planning , simulation , and verification of dynamic spatial systems . We provide a comparative summary of the modeling features , discuss lessons learned , and introduce a research roadmap for integrating different approaches of dynamic spatial system analysis to achieve coverage of all required features .
2K_dev_395	We show an improved parallel algorithm for decomposing an undirected unweighted graph into small diameter pieces with a small fraction of the edges in between . These decompositions form critical subroutines in a number of graph algorithms . Our algorithm builds upon the shifted shortest path approach introduced in [ Blelloch , Gupta , Koutis , Miller , Peng , Tangwongsan , SPAA 2011 ] . By combining various stages of the previous algorithm , we obtain a significantly simpler algorithm with the same asymptotic guarantees as the best sequential algorithm .
2K_dev_396	Human computation allows computer systems to leverage human intelligence in computational processes . While it has primarily been used for tasks that are not time-sensitive , recent systems use crowdsourcing to get on-demand , real-time , and even interactive results . In this paper , we present techniques for building real-time crowdsourcing systems , and then discuss how and when to use them . Our goal is to provide system builders with the tools and insights they need to replicate the success of modern systems in order to further explore this new space .
2K_dev_397	Large-scale collaboration systems often separate their content from the deliberation around how that content was produced . Surfacing this deliberation may engender trust in the content generation process if the deliberation process appears fair , well-reasoned , and thorough . Alternatively , it could encourage doubts about content quality , especially if the process appears messy or biased . In this paper we report the results of an experiment where we found that surfacing deliberation generally led to decreases in perceptions of quality for the article under consideration , especially - but not only - if the discussion revealed conflict . The effect size depends on the type of editors ' interactions . Finally , this decrease in actual article quality rating was accompanied by self-reported improved perceptions of the article and Wikipedia overall .
2K_dev_398	We describe a three-stage model of computing instruction beginning with a simple , highly scaffolded programming environment ( Kodu ) and progressing to more challenging frameworks ( Alice and Lego NXT-G ) . In moving between frameworks , students explore the similarities and differences in how concepts such as variables , conditionals , and looping are realized . This can potentially lead to a deeper understanding of programming , bringing students closer to true computational thinking . Some novel strategies for teaching with Kodu are outlined . Finally , we briefly report on our methodology and select preliminary results from a pilot study using this curriculum with students ages 10-17 , including several with disabilities .
2K_dev_399	Proceedings : AACR Annual Meeting 2014 ; April 5-9 , 2014 ; San Diego , CA We describe computational methods to compute likely evolutionary histories from tumor single-cell copy number data and next generation sequencing data and apply the methods to data collected from diverse types of tumors . Experimental techniques for assessing heterogeneity in tumor cell populations have undergone great advances , but these improvements have created a great need for more sophisticated computer algorithms capable of making sense of these data sources in terms of coherent models of tumor evolution . We have addressed this problem by developing computer algorithms for building phylogenetic trees describing evolution of individual tumors based on copy numbers of fluorescence in situ hybridization ( FISH ) probes from single cells in these tumors . These algorithms reconstruct evolutionary trees for observed cell populations so as to heuristically minimize the number of mutational events needed to explain the observed combinations of probe counts by evolution from a common diploid ancestral cell . We have extended this work from initial simple evolutionary models of evolution by single copy number changes to account for distinct mechanisms of evolution at the gene , chromosome , or whole-genome scale , with potentially different rates of evolution by mutation type . We have applied these algorithms to several FISH data sets , including cervical cancers probed for four genes ( LAMP3 , PROX1 , PRKAA1 and CCND1 ) measured for up to 250 cells of paired primary and metastatic samples from 16 patients , head-and-neck cancers probed for four genes ( TERC , CCND1 , EGFR and TP53 ) measured on up to 250 cells per patient for 65 patients at four tumor stages , prostate cancers probed for six genes ( TBL1XR1 , CTTNBP2 , MYC , PTEN , MEN1 and PDGFB ) measured for up to 407 cells in 6 non-progressive and 7 progressive carcinomas , and breast cancers probed for eight genes ( COX-2 , MYC , CCND1 , HER-2 , ZNF217 , DBC2 , CDH1 and TP53 ) measured on up to 220 cells of paired of ductal carcinoma in situ and invasive ductal carcinoma samples from 13 patients . We have then applied statistical and machine learning analysis to examine the ability of these trees to classify tumors by stage or potential for progression . The evolutionary tree models reveal robust features of evolutionary processes distinguishing progression stages and predicting future progression that lead to improved classification accuracy relative to predictions from cellular heterogeneity data alone . Our software is freely available at ftp : //ftp.ncbi.nlm.nih.gov/pub/FISHtrees . In continuing work , we are exploring extension of these approaches to better modeling and analysis of tumor evolution using single-cell sequencing data and to more detailed models of tumor evolution . Citation Format : Salim A. Chowdhury , Ayshwarya Subramanian , Alejandro A. Schaffer , Stanley E. Shackney , Darawalee Wangsa , Kerstin Heselmeyer-Haddad , Thomas Ried , Russell Schwartz . Inferring evolutionary models of tumor progression from single-cell heterogeneity data . [ abstract ] . In : Proceedings of the 105th Annual Meeting of the American Association for Cancer Research ; 2014 Apr 5-9 ; San Diego , CA . Philadelphia ( PA ) : AACR ; Cancer Res 2014 ; 74 ( 19 Suppl ) : Abstract nr 5338. doi:10.1158/1538-7445.AM2014-5338
2K_dev_400	AbstractWe report on a user study that provides evidencethat spaced repetition and a specic mnemonic technique enableusers to successfully recall multiple strong passwords over time.Remote research participants were asked to memorize 4 Person-Action-Object ( PAO ) stories where they chose a famous personfrom a drop-down list and were given machine-generated randomaction-object pairs . Users were also shown a photo of a scene andasked to imagine the PAO story taking place in the scene ( e.g. , Bill Gatesswallowingbike on a beach ) . Subsequently , theywere asked to recall the action-object pairs when prompted withthe associated scene-person pairs following a spaced repetitionschedule over a period of 127+ days . While we evaluated severalspaced repetition schedules , the best results were obtained whenusers initially returned after 12 hours and then in 1:5 increasingintervals : 77 % of the participants successfully recalled all 4stories in 10 tests over a period of 158 days . Much of theforgetting happened in the rst test period ( 12 hours ) : 89 % of participants who remembered their stories during the rsttest period successfully remembered them in every subsequentround . These ndings , coupled with recent results on naturallyrehearsing password schemes , suggest that 4 PAO stories couldbe used to create usable and strong passwords for 14 sensitiveaccounts following this spaced repetition schedule , possibly witha few extra upfront rehearsals . In addition , we nd statisticallysignicant evidence that with 8 tests over 64 days users whowere asked to memorize 4 PAO stories outperform users whoare given 4 random action-object pairs , but with 9 tests over 128days the advantage is not signicant . Furthermore , there is aninterference effect across multiple PAO stories : the recall rate of100 % ( resp . 90 % ) for participants who were asked to memorize1 PAO story ( resp . 2 PAO stories ) is signicantly better than therate for participants who were asked to memorize 4 PAO stories.These ndings yield concrete advice for improving constructionsof password management schemes and future user studies .
2K_dev_401	Non-Intrusive Load Monitoring ( NILM ) is a set of techniques used to estimate the electricity consumed by individual appliances in a building from measurements of the total electrical consumption . Most commonly , NILM works by first attributing any significant change in the total power consumption ( also known as an event ) to a specific load and subsequently using these attributions ( i.e . the labels for the events ) to estimate energy for each load . For this last step , most published work in the field makes simplifying assumptions to make the problem more tractable . In this paper , we present a framework for creating appliance models based on classification labels and aggregate power measurements that can help to relax many of these assumptions . Our framework automatically builds models for appliances to perform energy estimation . The model relies on feature extraction , clustering via affinity propagation , perturbation of extracted states to ensure that they mimic appliance behavior , creation of finite state models , correction of any errors in classification that might violate the model , and estimation of energy based on corrected labels . We evaluate our framework on 3 houses from standard datasets in the field and show that the framework can learn data-driven models based on event labels and use that to estimate energy with lower error margins ( e.g. , 1.142.3 % ) than when using the heuristic models used by others .
2K_dev_402	It is becoming more and more evident that the mechanical forces previously taken for granted actually play a pivotal role in influencing biological phenomenon from cancer metastases to vasculogenesis . Recent literature provides strong evidence for a causative link between the mechanical stretching of the cytoskeleton and the release of mechanotransductive signaling molecules . Understanding the links between mechanical input , the corresponding morphological changes in the actin cytoskeleton , and the resulting biochemical response is not well understood yet is a significant challenge in the field of mechanotransduction . Here , we present a model that integrates actin filament network remodeling under stretch with a novel biophysical model of molecular release to further elucidate the interplay between actin network morphology and resultant biochemical signaling.As stretch is applied to a model of the actin filament network , the distribution of bond angles in the network transitions from a more peaked to a flatter distribution . High variability is observed from site-to-site within the network upon an applied stretch , with a nearly uniform distribution of difference between stretched and unstretched angles ( delta angles ) at high levels of stretching . We used our approach to explore various thresholding models of how actin filament network deformations might influence rates of release of bound signaling molecules . These models allow us to project how a biochemical response might appear from a given applied mechanical stimulus . We validate these simulations using experimental data and use our model to then test different predictive capabilities of how mechanotransduction may function . Our model for the mechanotransductive release of signaling factors represents a potentially versatile mechanistic platform for examining biophysical interactions that link mechanical stimulus at the cellular level to response at the protein level .
2K_dev_403	Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval . Existing methods index a video by the raw concept detection score that is dense and inconsistent , and thus can not scale to `` big data '' that are readily available on the Internet . This paper proposes a scalable solution . The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index . The proposed adjustment model relies on a concise optimization framework with interpretations . The proposed index leverages the text-based inverted index for video retrieval . Experimental results validate the efficacy and the efficiency of the proposed method . The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance . Specifically , the proposed method ( with reranking ) achieves the best result on the challenging TRECVID Multimedia Event Detection ( MED ) zero-example task . It only takes 0.2 second on a single CPU core to search a collection of 100 million Internet videos .
2K_dev_404	Detecting anomalies and events in data is a vital task , with numerous applications in security , finance , health care , law enforcement , and many others . While many techniques have been developed in past years for spotting outliers and anomalies in unstructured collections of multi-dimensional points , with graph data becoming ubiquitous , techniques for structured graph data have been of focus recently . As objects in graphs have long-range correlations , novel technology has been developed for abnormality detection in graph data . The goal of this tutorial is to provide a general , comprehensive overview of the state-of-the-art methods for anomaly , event , and fraud detection in data represented as graphs . As a key contribution , we provide a thorough exploration of both data mining and machine learning algorithms for these detection tasks . We give a general framework for the algorithms , categorized under various settings : unsupervised vs. ( semi- ) supervised , for static vs. dynamic data . We focus on the scalability and effectiveness aspects of the methods , and highlight results on crucial real-world applications , including accounting fraud and opinion spam detection .
2K_dev_405	The paper develops $ { { \cal Q } { \cal D } } $ -learning , a distributed version of reinforcement $ Q $ -learning , for multi-agent Markov decision processes ( MDPs ) ; the agents have no prior information on the global state transition and on the local agent cost statistics . The network agents minimize a network-averaged infinite horizon discounted cost , by local processing and by collaborating through mutual information exchange over a sparse ( possibly stochastic ) communication network . The agents respond differently ( depending on their instantaneous one-stage random costs ) to a global controlled state and the control actions of a remote controller . When each agent is aware only of its local online cost data and the inter-agent communication network is weakly connected , we prove that $ { { \cal Q } { \cal D } } $ -learning , a $ \rm consensus + innovations $ algorithm with mixed time-scale stochastic dynamics , converges asymptotically almost surely to the desired value function and to the optimal stationary control policy at each network agent .
2K_dev_406	In a Stackelberg Security Game , a defender commits to a randomized deployment of security resources , and an attacker best-responds by attacking a target that maximizes his utility . While algorithms for computing an optimal strategy for the defender to commit to have had a striking real-world impact , deployed applications require significant information about potential attackers , leading to inefficiencies . We address this problem via an online learning approach . We are interested in algorithms that prescribe a randomized strategy for the defender at each step against an adversarially chosen sequence of attackers , and obtain feedback on their choices ( observing either the current attacker type or merely which target was attacked ) . We design no-regret algorithms whose regret ( when compared to the best fixed strategy in hindsight ) is polynomial in the parameters of the game , and sublinear in the number of times steps .
2K_dev_407	Detection of neuronal cell differentiation is essential to study cell fate decisions under various stimuli and/or environmental conditions . Many tools exist that quantify differentiation by neurite length measurements of single cells . However , quantification of differentiation in whole cell populations remains elusive so far . Because such populations can consist of both proliferating and differentiating cells , the task to assess the overall differentiation status is not trivial and requires a high-throughput , fully automated approach to analyze sufficient data for a statistically significant discrimination to determine cell differentiation . We address the problem of detecting differentiation in a mixed population of proliferating and differentiating cells over time by supervised classification . Using nerve growth factor induced differentiation of PC12 cells , we monitor the changes in cell morphology over days by phase-contrast live-cell imaging . For general applicability , the classification procedure starts out with many features to identify those that maximize discrimination of differentiated and undifferentiated cells and to eliminate features sensitive to systematic measurement artifacts . The resulting image analysis determines the optimal post treatment day for training and achieves a near perfect classification of differentiation , which we confirmed in technically and biologically independent as well as differently designed experiments . Our approach allows to monitor neuronal cell populations repeatedly over days without any interference . It requires only an initial calibration and training step and is thereafter capable to discriminate further experiments . In conclusion , this enables long-term , large-scale studies of cell populations with minimized costs and efforts for detecting effects of external manipulation of neuronal cell differentiation .
2K_dev_408	Mechanotransduction has been divided into mechanotransmission , mechanosensing , and mechanoresponse , although how a cell performs all three functions using the same set of structural components is still highly debated . Here , we bridge the gap between emerging molecular and systems-level understandings of mechanotransduction through a multiscale model linking these three phases . Our model incorporates a discrete network of actin filaments and associated proteins that responds to stretching through geometric relaxation . We assess three potential activating mechanisms at mechanosensitive crosslinks as inputs to a mixture model of molecular release and benchmark each using experimental data of mechanically-induced Rho GTPase FilGAP release from actin-filamin crosslinks . Our results suggest that filamin-FilGAP mechanotransduction response is best explained by a bandpass mechanism favoring release when crosslinking angles fall outside of a specific range . Our model further investigates the difference between ordered versus disordered networks and finds that a more disordered actin network may allow a cell to more finely tune control of molecular release enabling a more robust response .
2K_dev_409	Understanding the unique biochemical and physical differences between typical in vitro experimental systems and the in vivo environment of a living cell is a question of great importance in building and interpreting reliable models of complex reaction systems . Virus capsids make an excellent model system for such questions because they tend to have few components , making them amenable to in vitro and modeling studies , yet their assembly can be described by enormously complex networks of possible reactions that can not be resolved by any current experimental technology . We have previously attempted to bridge the gap between the complexity of the system and the limitations of data for tracking detailed assembly pathways using simulation-based model inference , learning kinetic parameters of coarse-grained rule models by fitting simulations to light scattering data from in vitro capsid assembly systems . Here , we describe extensions of that work to attempt to understand the influence of specific features of the cellular environment , individually or in concert , on assembly pathway selection . We specifically focus on the effects of macromolecular crowding and nucleic acid on capsid assembly , using coarse-grained biophysical models to adjust rate parameters learned from the in vitro system and suggest how these adjustments to fine-scale interactions may alter high-level pathway selection . Results from a series of virus capsid models suggest surprisingly complex and often counterintuitive mechanisms by which crowding or nucleic acids can alternately promote or inhibit assembly for different virus and assembly conditions .
2K_dev_410	Atomistic simulations of the conformational dynamics of proteins can be performed using either Molecular Dynamics or Monte Carlo procedures . The ensembles of three-dimensional structures produced during simulation can be analyzed in a number of ways to elucidate the thermodynamic and kinetic properties of the system . The goal of this chapter is to review both traditional and emerging methods for learning generative models from atomistic simulation data . Here , the term generative refers to a model of the joint probability distribution over the behaviors of the constituent atoms . In the context of molecular modeling , generative models reveal the correlation structure between the atoms , and may be used to predict how the system will respond to structural perturbations . We begin by discussing traditional methods , which produce multivariate Gaussian models . We then discuss GAMELAN ( GrAphical Models of Energy LANdscapes ) , which produces generative models of complex , non-Gaussian conformational dynamics ( e.g. , allostery , binding , folding , etc ) from long timescale simulation data .
2K_dev_411	Abstract Residential electricity users need more detail than monthly bills to reduce consumption . With the emergence of technologies that provide detailed usage estimates for energy consumption , two questions arise . First , how many different energy-consuming appliances contribute to household electricity load , and secondly which appliances ? Using national average penetration rates , the Residential Energy Consumption Survey ( RECS ) , estimates that 42 unique appliances account for 93 % of electricity consumption , while 12 appliances account for 80 % of average household electric load . A typical scenario is developed from national and regional penetration rates and find that eight appliances are responsible for 80 % of a household 's electric load in the United States . Four household scenarios are developed : a house that uses electric appliances , gas appliances , the average household , and typical household . It is concluded that RECS can not be used as a representative household as it overestimates the number of appliances that contribute to a household electric load . The number of significant appliances is affected by appliance ownership and use , which is more variable between homes than between census divisions . These results can be used to design and maximize the value of residential energy information and management systems .
2K_dev_412	A genome-wide association study involves examining a large number of single-nucleotide polymorphisms ( SNPs ) to identify SNPs that are significantly associated with the given phenotype , while trying to reduce the false positive rate . Although haplotype-based association methods have been proposed to accommodate correlation information across nearby SNPs that are in linkage disequilibrium , none of these methods directly incorporated the structural information such as recombination events along chromosome . In this paper , we propose a new approach called stochastic block lasso for association mapping that exploits prior knowledge on linkage disequilibrium structure in the genome such as recombination rates and distances between adjacent SNPs in order to increase the power of detecting true associations while reducing false positives . Following a typical linear regression framework with the genotypes as inputs and the phenotype as output , our proposed method employs a sparsity-enforcing Laplacian prior for the regression coefficients , augmented by a first-order Markov process along the sequence of SNPs that incorporates the prior information on the linkage disequilibrium structure . The Markov-chain prior models the structural dependencies between a pair of adjacent SNPs , and allows us to look for association SNPs in a coupled manner , combining strength from multiple nearby SNPs . Our results on HapMap-simulated datasets and mouse datasets show that there is a significant advantage in incorporating the prior knowledge on linkage disequilibrium structure for marker identification under whole-genome association .
2K_dev_413	In this paper , we consider the problem of state estimation of a dynamical system in a multi-agent network . The agents are sparsely connected and each of them observes a strict subset of the state vector . The distributed algorithm that we propose enables each agent to estimate any arbitrary linear dynamical system with bounded mean-squared error . To achieve this , the ratio of the algebraic connectivity and the largest eigenvalue of the graph Laplacian has to be larger than a lower bound determined by the spectral radius of the system 's dynamics matrix . This extends the notion of Network Tracking Capacity introduced by other authors in prior work . We accomplish this by introducing a new class of estimation algorithm of dynamical systems that , besides a ( consensus + innovations ) term , also includes consensus on the innovations .
2K_dev_414	Healthcare systems are depending on increasingly sophisticated and ubiquitous technology , while telehealth is rapidly gaining importance with the advent of low-cost and effective technological solutions in medicine . The increase in the worldwide elderly population and the burden this is inflicting upon the workforce , societies and economies are making remote care and independent living at home a necessity . MIIRH is the first workshop on multimedia analysis for remote care of and assisted living solutions which enable people that are incapacitated in some regard to continue living independently at home and remain active members of society . The topics addressed in MIIRH are extremely timely , as multitudes of cost-effective and high quality care solutions are already being developed and used , rendering the examination of new medical , healthcare paradigms an absolute necessity .
2K_dev_415	Guided waves in plates , known as Lamb waves , are characterized by complex , multimodal , and frequency dispersive wave propagation , which distort signals and make their analysis difficult . Estimating these multimodal and dispersive characteristics from experimental data becomes a difficult , underdetermined inverse problem . To accurately and robustly recover these multimodal and dispersive properties , this paper presents a methodology referred to as sparse wavenumber analysis based on sparse recovery methods . By utilizing a general model for Lamb waves , waves propagating in a plate structure , and robust l1 optimization strategies , sparse wavenumber analysis accurately recovers the Lamb wave 's frequency-wavenumber representation with a limited number of surface mounted transducers . This is demonstrated with both simulated and experimental data in the presence of multipath reflections . With accurate frequency-wavenumber representations , sparse wavenumber synthesis is then used to accurately remove multipath in ...
2K_dev_416	We introduce Acoustruments : low-cost , passive , and power-less mechanisms , made from plastic , that can bring rich , tangible functionality to handheld devices . Through a structured exploration , we identified an expansive vocabulary of design primitives , providing building blocks for the construction of tangible interfaces utilizing smartphones ' existing audio functionality . By combining design primitives , familiar physical mechanisms can all be constructed from passive elements . On top of these , we can create end-user applications with rich , tangible interactive functionalities . Our experiments show that Acoustruments can achieve 99 % accuracy with minimal training , is robust to noise , and can be rapidly prototyped . Acoustruments adds a new method to the toolbox HCI practitioners and researchers can draw upon , while introducing a cheap and passive method for adding interactive controls to consumer products .
2K_dev_417	With the widespread availability of video cameras , we are facing an ever-growing enormous collection of unedited and unstructured video data . Due to lack of an automatic way to generate summaries from this large collection of consumer videos , they can be tedious and time consuming to index or search . In this work , we propose online video highlighting , a principled way of generating short video summarizing the most important and interesting contents of an unedited and unstructured video , costly both time-wise and financially for manual processing . Specifically , our method learns a dictionary from given video using group sparse coding , and updates atoms in the dictionary on-the-fly . A summary video is then generated by combining segments that can not be sparsely reconstructed using the learned dictionary . The online fashion of our proposed method enables it to process arbitrarily long videos and start generating summaries before seeing the end of the video . Moreover , the processing time required by our proposed method is close to the original video length , achieving quasi real-time summarization speed . Theoretical analysis , together with experimental results on more than 12 hours of surveillance and YouTube videos are provided , demonstrating the effectiveness of online video highlighting .
2K_dev_418	This paper describes an empirical study of 1.6M deleted tweets collected over a continuous one-week period from a set of 292K Twitter users . We examine several aggregate properties of deleted tweets , including their connections to other tweets ( e.g. , whether they are replies or retweets ) , the clients used to produce them , temporal aspects of deletion , and the presence of geotagging information . Some significant differences were discovered between the two collections , namely in the clients used to post them , their conversational aspects , the sentiment vocabulary present in them , and the days of the week they were posted . However , in other dimensions for which analysis was possible , no substantial differences were found . Finally , we discuss some ramifications of this work for understanding Twitter usage and management of one 's privacy .
2K_dev_419	This paper reviews signal processing research for applications in the future electric power grid , commonly referred to as smart grid . Generally , it is expected that the grid of the future would differ from the current system by the increased integration of distributed generation , distributed storage , demand response , power electronics , and communications and sensing technologies . The consequence is that the physical structure of the system becomes significantly more distributed . The existing centralized control structure is not suitable any more to operate such a highly distributed system . Hence , in this paper , we overview distributed approaches , all based on consensus $ { + } $ innovations , for three common energy management functions : state estimation , economic dispatch , and optimal power flow . We survey the pertinent literature and summarize our work . Simulation results illustrate tradeoffs and the performance of consensus $ { + } $ innovations for these three applications .
2K_dev_420	Background Disease progression in the absence of therapy varies significantly in HIV-1 infected individuals . Both viral and host cellular molecules are implicated ; however , the exact role of these factors and/or the mechanism involved remains elusive . To understand how microRNAs ( miRNAs ) , which are regulators of transcription and translation , influence host cellular gene expression ( mRNA ) during HIV-1 infection , we performed a comparative miRNA and mRNA microarray analysis using PBMCs obtained from infected individuals with distinct viral load and CD4 counts .
2K_dev_421	In the Architecture , Engineering , and Construction ( AEC ) domain , semantically rich 3D information models are increasingly used throughout a facility 's life cycle for diverse applications , such as planning renovations , space usage planning , and managing building maintenance . These models , which are known as building information models ( BIMs ) , are often constructed using dense , three dimensional ( 3D ) point measurements obtained from laser scanners . Laser scanners can rapidly capture the as-is conditions of a facility , which may differ significantly from the design drawings . Currently , the conversion from laser scan data to BIM is primarily a manual operation , and it is labor-intensive and can be error-prone . This paper presents a method to automatically convert the raw 3D point data from a laser scanner positioned at multiple locations throughout a facility into a compact , semantically rich information model . Our algorithm is capable of identifying and modeling the main visible structural components of an indoor environment ( walls , floors , ceilings , windows , and doorways ) despite the presence of significant clutter and occlusion , which occur frequently in natural indoor environments . Our method begins by extracting planar patches from a voxelized version of the input point cloud . The algorithm learns the unique features of different types of surfaces and the contextual relationships between them and uses this knowledge to automatically label patches as walls , ceilings , or floors . Then , we perform a detailed analysis of the recognized surfaces to locate openings , such as windows and doorways . This process uses visibility reasoning to fuse measurements from different scan locations and to identify occluded regions and holes in the surface . Next , we use a learning algorithm to intelligently estimate the shape of window and doorway openings even when partially occluded . Finally , occluded surface regions are filled in using a 3D inpainting algorithm . We evaluated the method on a large , highly cluttered data set of a building with forty separate rooms .
2K_dev_422	This article investigates the transient use of free local storage for improving performance in VM-based mobile computing systems implemented as thick clients on host PCs . We use the term TransientPC systems to refer to these types of systems . The solution we propose , called TransPart , uses the higher-performing local storage of host hardware to speed up performance-critical operations . Our solution constructs a virtual storage device on demand ( which we call transient storage ) by borrowing free disk blocks from the hosts storage . In this article , we present the design , implementation , and evaluation of a TransPart prototype , which requires no modifications to the software or hardware of a host computer . Experimental results confirm that TransPart offers low overhead and startup cost , while improving user experience .
2K_dev_423	Real-time system level implementations of complex Synthetic Aperture Radar ( SAR ) image reconstruction algorithms have always been challenging due to their data intensive characteristics . In this paper , we propose a basis vector transform based novel algorithm to alleviate the data intensity and a 3D-stacked logic in memory based hardware accelerator as the implementation platform . Experimental results indicate that this proposed algorithm/hardware co-optimized system can achieve an accuracy of 91 dB PSNR compared to a reference algorithm implemented in Matlab and energy efficiency of 72 GFLOPS/W for a 8k8k SAR image reconstruction .
2K_dev_424	A significant challenge for crowdsourcing has been increasing worker engagement and output quality . We explore the effects of social , learning , and financial strategies , and their combinations , on increasing worker retention across tasks and change in the quality of worker output . Through three experiments , we show that 1 ) using these strategies together increased workers ' engagement and the quality of their work ; 2 ) a social strategy was most effective for increasing engagement ; 3 ) a learning strategy was most effective in improving quality . The findings of this paper provide strategies for harnessing the crowd to perform complex tasks , as well as insight into crowd workers ' motivation .
2K_dev_425	This paper is motivated by major needs for fast and accurate on-line data analysis tools in the emerging electric energy systems , due to the recent penetration of distributed green energy , distributed intelligence , and plug-in electric vehicles . Instead of taking the traditional complex physical model based approach , this paper proposes a data-driven method , leading to an effective topology estimation approach for the smart grid . Specifically , we first introduce the data-driven topology estimation problem . Then , a novel Logistic Kernel Regression is proposed in a Bayesian framework based on Nearest Neighbors search . Notably , unlike many machine learning approaches that do not account for physical constraints , and distinctive from deterministic engineering modeling defined solely by physical laws , this paper for the first time combines the two into one single regression modeling for topology estimation . Simulation results of the proposed method show that the new method produces a topology estimate excelling the current industrial approach . Finally , the proposed method can be implemented given recent advances in machine learning , which are becoming drivers and sources of data previously unavailable in the electric power industry .
2K_dev_426	We present the first formal study of crowdworkers who have disabilities via in-depth open-ended interviews of 17 people ( disabled crowdworkers and job coaches for people with disabilities ) and a survey of 631 adults with disabilities . Our findings establish that people with a variety of disabilities currently participate in the crowd labor marketplace , despite challenges such as crowdsourcing workflow designs that inadvertently prohibit participation by , and may negatively affect the worker reputations of , people with disabilities . Despite such challenges , we find that crowdwork potentially offers different opportunities for people with disabilities relative to the normative office environment , such as job flexibility and lack of a need to rely on public transit . We close by identifying several ways in which crowd labor platform operators and/or individual task requestors could improve the accessibility of this increasingly important form of employment .
2K_dev_427	Effortless one-touch capture of video is a unique capability of wearable devices such as Google Glass . We use this capability to create a new type of crowd-sourced system in which users receive queries relevant to their current location and opt-in preferences . In response , they can send back live video snippets of their surroundings . A system of result caching , geolocation and query similarity detection shields users from being overwhelmed by a flood of queries .
2K_dev_428	In this paper we propose a common file format and API for public Non-Intrusive Load Monitoring ( NILM ) datasets such that researchers can easily evaluate their approaches across the different datasets and benchmark their results against prior work . The proposed file format enables storing the power demand of the whole house along with individual appliance consumption , and other relevant metadata in a single compact file , whereas the API supports the creation and manipulation of individual files and datasets in the proposed format .
2K_dev_429	We consider the social welfare model of Naor [ 20 ] and revenue-maximization model of Chen and Frank [ 7 ] , where a single class of delay-sensitive customers seek service from a server with an observable queue , under state dependent pricing . It is known that in this setting both revenue and social welfare can be maximized by a threshold policy , whereby customers are barred from entry once the queue length reaches a certain threshold . However , no explicit expression for this threshold has been found . This paper presents the first derivation of the optimal threshold in closed form , and a surprisingly simple formula for the ( maximum ) revenue under this optimal threshold . Utilizing properties of the Lambert W function , we also provide explicit scaling results of the optimal threshold as the customer valuation grows . Finally , we present a generalization of our results , allowing for settings with multiple servers .
2K_dev_430	The virtualization of real-time systems has received much attention for its many benefits , such as the consolidation of individually developed real-time applications while maintaining their implementations . However , the current state of the art still lacks properties required for resource sharing among real-time application tasks in a multi-core virtualization environment . In this paper , we propose vMPCP , a synchronization framework for the virtualization of multi-core real-time systems . Vmpcp exposes the executions of critical sections of tasks in a guest virtual machine to the hyper visor . Using this approach , vMPCP reduces and bounds blocking time on accessing resources shared within and across virtual CPUs ( VCPUs ) assigned on different physical CPU cores . Vmpcp supports periodic server and deferrable server policies for the VCPU budget replenish policy , with an optional budget overrun to reduce blocking times . We provide the VCPU and task schedulability analyses under vMPCP , with different VCPU budget supply policies , with and without overrun . Experimental results indicate that , under vMPCP , deferrable server outperforms periodic server when overrun is used , with as much as 80 % more task sets being schedulable . The case study using our hyper visor implementation shows that vMPCP yields significant benefits compared to a virtualization-unaware multi-core synchronization protocol , with 29 % shorter response time on average .
2K_dev_431	The 19th IEEE Real-Time and Embedded Technology and Applications Symposium ( RTAS 2013 ) was held in Philadelphia , Pennsylvania , USA , as part of the Cyber-Physical Systems Week . The scope of RTAS has been defined by its four tracks : Track 1 on Applications , Systems , RTOSs , and Tools ; Track 2 on Applied Methodologies and Foundations , that is , on basic methodologies and algorithms that are applicable to real systems to solve specific problems ; Track 3 on Wireless Sensor Networks ; and Track 4 on Hardware-Software Co-design , including design methodologies and tools for hardware/software integration . Papers submitted to all tracks were required to clearly define motivating application examples and include a section reporting experimental results with a real implementation of the proposed system , or showing the applicability to an industrial case study or working system . This year 's edition continues a steady growth in attractiveness and quality . Overall , 96 submissions were received . It was a hard and long process to select 28 papers , which are compiled in this volume . All submissions received at least four reviews , resulting in an overall number of more than 400 reviews - involving more than 160 reviewers - being conducted . An important piece of the process was the Program Committee meeting , which was held in San Juan , Puerto Rico , in conjunction with RTSS 2012 .
2K_dev_432	Real-time captioning enables deaf and hard of hearing ( DHH ) people to follow classroom lectures and other aural speech by converting it into visual text with less than a five second delay . Keeping the delay short allows end-users to follow and participate in conversations . This article focuses on the fundamental problem that makes real-time captioning difficult : sequential keyboard typing is much slower than speaking . We first surveyed the audio characteristics of 240 one-hour-long captioned lectures on YouTube , such as speed and duration of speaking bursts . We then analyzed how these characteristics impact caption generation and readability , considering specifically our human-powered collaborative captioning approach . We note that most of these characteristics are also present in more general domains . For our caption comparison evaluation , we transcribed a classroom lecture in real-time using all three captioning approaches . We recruited 48 participants ( 24 DHH ) to watch these classroom transcripts in an eye-tracking laboratory . We presented these captions in a randomized , balanced order . We show that both hearing and DHH participants preferred and followed collaborative captions better than those generated by automatic speech recognition ( ASR ) or professionals due to the more consistent flow of the resulting captions . These results show the potential to reliably capture speech even during sudden bursts of speed , as well as for generating enhanced captions , unlike other human-powered captioning approaches .
2K_dev_433	Event detection in social media is an important but challenging problem . Most existing approaches are based on burst detection , topic modeling , or clustering techniques , which can not naturally model the implicit heterogeneous network structure in social media . As a result , only limited information , such as terms and geographic locations , can be used . This paper presents Non-Parametric Heterogeneous Graph Scan ( NPHGS ) , a new approach that considers the entire heterogeneous network for event detection : we first model the network as a `` sensor '' network , in which each node senses its `` neighborhood environment '' and reports an empirical p-value measuring its current level of anomalousness for each time interval ( e.g. , hour or day ) . Then , we efficiently maximize a nonparametric scan statistic over connected subgraphs to identify the most anomalous network clusters . Finally , the event represented by each cluster is summarized with information such as type of event , geographical locations , time , and participants . As a case study , we consider two applications using Twitter data , civil unrest event detection and rare disease outbreak detection , and present empirical evaluations illustrating the effectiveness and efficiency of our proposed approach .
2K_dev_434	Clustering is one of the fundamental data mining tasks . While traditional clustering techniques assign each object to a single cluster only , in many applications it has been observed that objects might belong to multiple clusters with different degrees . In this work , we present a Bayesian framework to tackle the challenge of mixed membership clustering for vector data . We exploit the ideas of subspace clustering where the relevance of dimensions might be different for each cluster . Combining the relevance of the dimensions with the cluster membership degree of the objects , we propose a novel type of mixture model able to represent data containing mixed membership subspace clusters . For learning our model , we develop an efficient algorithm based on variational inference allowing easy parallelization . In our empirical study on synthetic and real data we show the strengths of our novel clustering technique .
2K_dev_435	Monte Carlo simulation ( MCS ) is a numerical method to solve the probabilistic load flow ( PLF ) problem . Comparing to analytical methods , MCS for PLF has advantages such as flexibility , general purpose , able to deal with large nonlinearity and large variances , and embarrassingly parallelizable . However , MCS also suffers from low convergence speed and high computational burden , especially for problems with multiple random variables . In this paper , we proposed a Quasi-Monte Carlo ( QMC ) based method to solve the PLF for radial distribution network . QMC uses samples from low-discrepancy sequence intended to cover the high dimension random sample space as uniformly as possible . The QMC method is particularly suitable for the high dimension problems with low effective dimensions , and has been successfully used to solve large scale problems in econometrics and statistical circuit design . In this paper , we showed that the PLF for radial distribution system has the similar properties and can be a good candidate for QMC method . The proposed method possesses the advantage of MCS method and significantly increases the convergence rate and overall speed . Numerical experiment results on IEEE test feeders have shown the effectiveness of the proposed method .
2K_dev_436	People spend an enormous amount of time searching for and saving information online . Existing tools capture only a small portion of the cognitive processing a user engages in while making sense of a new domain . In this paper we introduce a novel interface for capturing online information in a structured but lightweight way . We use this interface as a platform to experimentally characterize the costs and benefits of structuring information during the sensemaking process . Our results contribute empirical knowledge relevant to theories of information seeking and sensemaking , and practical implications for the development of tools to capture and share online information .
2K_dev_437	ABSTRACT Multiple ultrasonic guided -wave modes propagating along a pipe travel with different velocities which are themselves a function of frequency . Reflections from the features of the structure ( e.g. , boundaries , pipe welding , damage , etc . ) , and their complex s uperposition , adds to the complexity of guided -waves . Guided -wave based damage d iagnosis of pipelines becomes even more challenging when environmental and operational conditions ( EOCs ) vary ( e.g. , temperature , flow rate , inner pressure , etc . ) . These complexities make guided -wave based damage diagnosis of operating pipelines a challenging task . This paper reviews the approaches to -date addressing these challenges , and highlights the preferred characteristics of a method that simplifies guided -wa ve signals for damage diagnosis purposes . A method is proposed to extract a sparse subset of guided -wave signals in time -domain , while retaining optimal damage information for detection purpose . In this paper , the general concept of this method is proved t hrough an extensive set of experiments . Effects of temperature variation on detection performnce of the proposed method , and on discriminatory power of the extracted damage -sensitive features are investigated . The potential of the proposed method for real -time damage detection is illustrated , for wide range of temperature variation scenarios ( i.e. , temperature difference between training and test data varying between -2 ( and 13 ( ) . Keywords : Pipeline Monitoring , Guided -waves , Nondestructive Evaluation , Structural Health Monitoring , Temperature Effects , Damage Detection , Sparse Representation .
2K_dev_438	This paper reports the design and development of an HTML5-empowered Virtual Sensor Editor ( VSE ) over Internet of Things cloud . VSE is a scalable tool that allows users to design virtual sensors with user-defined dataflow logic , by visually aggregating existing sensors , either physical sensors or user-defined virtual sensors . VSE supports a real-time and historical visualization of sensor values and analytical studies , and is a cross-platform and customizable tool equipped with ability to support verifiable sensor data service compos ability . A discussion on design decisions is presented . Our preliminary work has been applied to NASA Sustainability Base for Smart Building monitoring . Preliminary performance and scalability study is also reported .
2K_dev_439	Sensor selection in nonparametric decentralized detection is investigated . Kernel-based minimization framework with a weighted kernel is adopted , where the kernel weight parameters represent sensors ' contributions to decision making . L 1 regularization on weight parameters is introduced into the risk function so that the resulting optimal decision rule contains a sparse vector of nonzero weight parameters . In this way , sensor selection is naturally performed because only sensors corresponding to nonzero weight parameters contribute to decision making . A gradient projection algorithm and a Gauss-Seidel algorithm are developed to jointly perform weight selection ( i.e. , sensor selection ) and optimize decision rules . Both algorithms are shown to converge to critical points for this non-convex optimization problem . Numerical results are provided to demonstrate the advantages and properties of the proposed sensor selection approach .
2K_dev_440	AbstractWind energy is a key renewable source , yet wind farms have relatively high cost compared with many traditional energy sources . Among the life cycle costs of wind farms , operation and maintenance ( O & M ) accounts for 2530 % , and an efficient strategy for management of turbines can significantly reduce the O & M cost . Wind turbines are subject to fatigue-induced degradation and need periodic inspections and repairs , which are usually performed through semiannual scheduled maintenance . However , better maintenance can be achieved by flexible policies based on prior knowledge of the degradation process and on data collected in the field by sensors and visual inspections . Traditional methods to model the O & M process , such as Markov decision processes ( MDPs ) and partially observable MDPs ( POMDPs ) , have limitations that do not allow the model to properly include the knowledge available and that may result in nonoptimal strategies for management of the farm . Specifically , the conditional probabilities for mode ...
2K_dev_441	We present the penalized fast subset scan ( PFSS ) , a new and general framework for scalable and accurate pattern detection . PFSS enables exact and efficient identification of the most anomalous subsets of the data , as measured by a likelihood ratio scan statistic . However , PFSS also allows incorporation of prior information about each data elements probability of inclusion , which was not previously possible within the subset scan framework . PFSS builds on two main results : first , we prove that a large class of likelihood ratio statistics satisfy a property that allows additional , element-specific penalty terms to be included while maintaining efficient computation . Second , we prove that the penalized statistic can be maximized exactly by evaluating only O ( N ) subsets . As a concrete example of the PFSS framework , we incorporate soft constraints on spatial proximity into the spatial event detection task , enabling more accurate detection of irregularly shaped spatial clusters of varying sparsity . To do so , ...
2K_dev_442	ABSTRACT Guided waves can propagate long distances and are sensitive to subtle structural damage . Guided-wave based damage localization often requires extracting the scatter signal ( s ) produced by damage , which is typically obtained by subtracting an intact baseline record from a record to be tested . However , in practical applications , environmental and operational conditions ( EOC ) dramatically affect guided wave si gnals . In this case , the baseline subtraction process can no longer perfectly remove the baseline , thereby defeating localization algorithms . In previous work , we showed that singular value decomposition ( SVD ) can be used to detect the presence of damage under large EOC variations , because it can differentiate the tr ends of damage from other EO C variations . This capability of differentiation implies that SVD can also robustly extract a scatter signal , originating from damage in the structure , that is not affected by temperature vari ation . This process allows us to extract a scatterer signal without the challenges associated with traditional temperature compensation and baseline subtraction routines . . In this work , we use to approach to localize structural damage in large , spatially and temporally varying EOCs . We collect pitch-catch records from randomly placed PZT transducers on an aluminum plate while undergoing temperature variations . Damage is introduced to the plate during the monitoring period . We then use our SVD method to extract the scatter signal from the records , and use the scatter signal to localize damage using the delay-and-sum method . To compare results , we also apply several temperature compensation methods to the records and then perform baseline subtraction . We show that our SVD-based approach successfully localize damage while current temperature-compensated baseline subtraction methods fail . Keywords : Guided waves , structural health monitoring , environmental and operational variations , singular value decomposition , damage detection and localization
2K_dev_443	We use exponential start time clustering to design faster parallel graph algorithms involving distances . Previous algorithms usually rely on graph decomposition routines with strict restrictions on the diameters of the decomposed pieces . We weaken these bounds in favor of stronger local probabilistic guarantees . This allows more direct analyses of the overall process , giving : Linear work parallel algorithms that construct spanners with O ( k ) stretch and size O ( n 1+1/ k ) in unweighted graphs , and size O ( n 1+1/ k log k ) in weighted graphs . Hopsets that lead to the first parallel algorithm for approximating shortest paths in undirected graphs with O ( m poly log n ) work .
2K_dev_444	Testing Cyber-Physical Systems is becoming increasingly challenging as they incorporate advanced autonomy features . We investigate using an external runtime monitor as a partial test oracle to detect violations of critical system behavioral requirements on an automotive development platform . Despite limited source code access and using only existing network messages , we were able to monitor a hardware-in-the-loop vehicle simulator and analyze prototype vehicle log data to detect violations of high-level critical properties . Interface robustness testing was useful to further exercise the monitors . Beyond demonstrating feasibility , the experience emphasized a number of remaining research challenges , including : approximating system intent based on limited system state observability , how to best balance the simplicity and expressiveness of the specification language used to define monitored properties , how to warm up monitoring of system variable state after mode change discontinuities , and managing the differences between simulation and real vehicles when conducting such tests .
2K_dev_445	Disparity tuning measured in the primary visual cortex ( V1 ) is described well by the disparity energy model , but not all aspects of disparity tuning are fully explained by the model . Such deviations from the disparity energy model provide us with insight into how network interactions may play a role in disparity processing and help to solve the stereo correspondence problem . Here , we propose a neuronal circuit model with recurrent connections that provides a simple account of the observed deviations . The model is based on recurrent connections inferred from neurophysiological observations on spike timing correlations , and is in good accord with existing data on disparity tuning dynamics . We further performed two additional experiments to test predictions of the model . First , we increased the size of stimuli to drive more neurons and provide a stronger recurrent input . Our model predicted sharper disparity tuning for larger stimuli . Second , we displayed anti-correlated stereograms , where dots of opposite luminance polarity are matched between the left- and right-eye images and result in inverted disparity tuning in the disparity energy model . In this case , our model predicted reduced sharpening and strength of inverted disparity tuning . For both experiments , the dynamics of disparity tuning observed from the neurophysiological recordings in macaque V1 matched model simulation predictions . Overall , the results of this study support the notion that , while the disparity energy model provides a primary account of disparity tuning in V1 neurons , neural disparity processing in V1 neurons is refined by recurrent interactions among elements in the neural circuit .
2K_dev_446	Behavioral researchers spend considerable amount of time coding video data to systematically extract meaning from subtle human actions and emotions . In this paper , we present Glance , a tool that allows researchers to rapidly query , sample , and analyze large video datasets for behavioral events that are hard to detect automatically . Glance takes advantage of the parallelism available in paid online crowds to interpret natural language queries and then aggregates responses in a summary view of the video data . Glance provides analysts with rapid responses when initially exploring a dataset , and reliable codings when refining an analysis . Our experiments show that Glance can code nearly 50 minutes of video in 5 minutes by recruiting over 60 workers simultaneously , and can get initial feedback to analysts in under 10 seconds for most clips . We present and compare new methods for accurately aggregating the input of multiple workers marking the spans of events in video data , and for measuring the quality of their coding in real-time before a baseline is established by measuring the variance between workers . Glance 's rapid responses to natural language queries , feedback regarding question ambiguity and anomalies in the data , and ability to build on prior context in followup queries allow users to have a conversation-like interaction with their data - opening up new possibilities for naturally exploring video data .
2K_dev_447	Inference of gene interaction networks from expression data usually focuses on either supervised or unsupervised edge prediction from a single data source . However , in many real world applications , multiple data sources , such as microarray and ISH ( in situ hybridization ) measurements of mRNA abundances , are available to offer multiview information about the same set of genes . We propose ISH to estimate a gene interaction network that is consistent with such multiple data sources , which are expected to reflect the same underlying relationships between the genes . NP-MuScL casts the network estimation problem as estimating the structure of a sparse undirected graphical model . We use the semiparametric Gaussian copula to model the distribution of the different data sources , with the different copulas sharing the same precision ( i.e. , inverse covariance ) matrix , and we present an efficient algorithm to estimate such a model in the high-dimensional scenario . Results are reported on synthetic data , where NP-MuScL outperforms baseline algorithms significantly , even in the presence of noisy data sources . Experiments are also run on two real-world scenarios : two yeast microarray datasets and three Drosophila embryonic gene expression datasets , where NP-MuScL predicts a higher number of known gene interactions than existing techniques .
2K_dev_448	Objective . Intracortical braincomputer interface ( BCI ) decoders are typically retrained daily to maintain stable performance . Self-recalibrating decoders aim to remove the burden this may present in the clinic by training themselves autonomously during normal use but have only been developed for continuous control . Here we address the problem for discrete decoding ( classifiers ) . Approach . We recorded threshold crossings from 96-electrode arrays implanted in the motor cortex of two rhesus macaques performing center-out reaches in 7 directions over 41 and 36 separate days spanning 48 and 58days in total for offline analysis . Main results . We show that for the purposes of developing a self-recalibrating classifier , tuning parameters can be considered as fixed within days and that parameters on the same electrode move up and down together between days . Further , drift is constrained across time , which is reflected in the performance of a standard classifier which does not progressively worsen if it is not retrained daily , though overall performance is reduced by more than 10 % compared to a daily retrained classifier . Two novel self-recalibrating classifiers produce a increase in classification accuracy over that achieved by the non-retrained classifier to nearly recover the performance of the daily retrained classifier . Significance . We believe that the development of classifiers that require no daily retraining will accelerate the clinical translation of BCI systems . Future work should test these results in a closed-loop setting .
2K_dev_449	Low-cost genetic sequencing , coupled with novel social media platforms and visualization techniques , present a new frontier for scientific participation , whereby people can learn , share , and act on data embedded within their own bodies . Our study of 23andMe , a popular genetic testing service , reveals how users make sense of and contextualize their genetic results , critique and evaluate the underlying research , and reflect on the broader implications of genetic testing . We frame user groups as citizen science publics groups that coalesce around scientific issues and work towards resolving shared concerns . Our findings show that personal genetics serves as a site for public engagement with science , whereby communities of biological citizens creatively interpret , debate , and act on professional research . We conclude with design trajectories at the intersection of genetics and creativity support tools : platforms for aggregating hybrid knowledge ; tools for creative reflection on professional science ; and strategies for supporting collaborations across communities .
2K_dev_450	In the real-world unconstrained face recognition scenarios , automatic facial landmarking scheme using the active shape model ( ASM ) usually gives non-ideal results , especially at the facial boundary . This is because the local subspace methods such as the principal component analysis ( PCA ) used in the ASM does not properly discern skin texture and background with very similar photometric and textual properties , thus fails to accurately locate the facial boundary . In this work , we have novelly developed a robust image statistics approach to efficiently refine the landmarks on facial boundary . Moreover , with the aid of banana wavelets to highlight the facial boundary , our proposed approach can deal with even more difficult task . This algorithm can dramatically increase the accuracy of landmarks on facial boundary for unconstrained facial images with minimum computational expense since this method is purely based on image statistics with no training stages involved at all . We have shown the effectiveness of our proposed methods on the GBU database where the refined landmarks yield much lower MSE from the ground truth .
2K_dev_451	One of the most significant challenges for many online communities is increasing members ' contributions over time . Prior studies on peer feedback in online communities have suggested its impact on contribution , but have been limited by their correlational nature . In this paper , we conducted a field experiment on Wikipedia to test the effects of different feedback types ( positive feedback , negative feedback , directive feedback , and social feedback ) on members ' contribution . Our results characterize the effects of different feedback types , and suggest trade-offs in the effects of feedback between the focal task and general motivation , as well as differences in how newcomers and experienced editors respond to peer feedback . This research provides insights into the mechanisms underlying peer feedback in online communities and practical guidance to design more effective peer feedback systems .
2K_dev_452	In commercial-off-the-shelf ( COTS ) multi-core systems , the execution times of tasks become hard to predict because of contention on shared resources in the memory hierarchy . In particular , a task running in one processor core can delay the execution of another task running in another processor core . This is due to the fact that tasks can access data in the same cache set shared among processor cores or in the same memory bank in the DRAM memory ( or both ) . Such cache and bank interference effects have motivated the need to create isolation mechanisms for resources accessed by more than one task . One popular isolation mechanism is cache coloring that divides the cache into multiple partitions . With cache coloring , each task can be assigned exclusive cache partitions , thereby preventing cache interference from other tasks . Similarly , bank coloring allows assigning exclusive bank partitions to tasks . While cache coloring and some bank coloring mechanisms have been studied separately , interactions between the two schemes have not been studied . Specifically , while memory accesses to two different bank colors do not interfere with each other at the bank level , they may interact at the cache level . Similarly , two different cache colors avoid cache interference but may not prevent bank interference . Therefore it is necessary to coordinate cache and bank coloring approaches . In this paper , we present a coordinated cache and bank coloring scheme that is designed to prevent cache and bank interference simultaneously . We also developed color allocation algorithms for configuring a virtual memory system to support our scheme which has been implemented in the Linux kernel . In our experiments , we observed that the execution time can increase by 60 % due to inter-task interference when we use only cache coloring . Our coordinated approach can reduce this figure down to 12 % ( an 80 % reduction ) .
2K_dev_453	Anecdotal evidence and scholarly research have shown that Internet users may regret some of their online disclosures . To help individuals avoid such regrets , we designed two modifications to the Facebook web interface that nudge users to consider the content and audience of their online disclosures more carefully . We implemented and evaluated these two nudges in a 6-week field trial with 28 Facebook users . We analyzed participants ' interactions with the nudges , the content of their posts , and opinions collected through surveys . We found that reminders about the audience of posts can prevent unintended disclosures without major burden ; however , introducing a time delay before publishing users ' posts can be perceived as both beneficial and annoying . On balance , some participants found the nudges helpful while others found them unnecessary or overly intrusive . We discuss implications and challenges for designing and evaluating systems to assist users with online disclosures .
2K_dev_454	Energy-efficient control mechanisms are necessary to manage the ever increasing energy demand . Recently several tools for building energy consumption control have been proposed for small ( e.g . homes ) [ 8 ] and large ( e.g . offices ) buildings [ 3 ] [ 6 ] [ 1 ] . The mechanism each tool uses is different , e.g . HVAC control [ 3 ] and appliance rescheduling [ 8 ] , but they share the goal of improving consumption of the buildings with respect to a given cost function . Some examples of cost functions are reduced energy consumption , reduced electricity bill , lower peak power , and increased ancillary service participation . The tools however do not capture the impacts of their control actions on the grid . These actions can lead to supply/demand imbalance and voltage/frequency deviation and thus , threaten grid stability . Utilities can take protective actions against those who cause instability by increasing electricity price or even momentarily disconnecting them from the grid . The effects of these protective actions can be so severe that the savings obtained by building management tools might disappear .
2K_dev_455	Given a large collection of time series , such as web-click logs , electric medical records and motion capture sensors , how can we efficiently and effectively find typical patterns ? How can we statistically summarize all the sequences , and achieve a meaningful segmentation ? What are the major tools for forecasting and outlier detection ? Time-series data analysis is becoming of increasingly high importance , thanks to the decreasing cost of hardware and the increasing on-line processing capability . The objective of this tutorial is to provide a concise and intuitive overview of the most important tools that can help us find patterns in large-scale time-series sequences . We review the state of the art in four related fields : ( 1 ) similarity search and pattern discovery , ( 2 ) linear modeling and summarization , ( 3 ) non-linear modeling and forecasting , and ( 4 ) the extension of time-series mining and tensor analysis . The emphasis of the tutorial is to provide the intuition behind these powerful tools , which is usually lost in the technical literature , as well as to introduce case studies that illustrate their practical use .
2K_dev_456	Virus capsid assembly has been a powerful model system for biological self-assembly in general due to the combination of experimental tractability but complicated pathway space . Detailed experimental resolution of viral assembly processes , however , has so far proven impossible . Computational approaches have provided a solution , allowing us to learn models of assembly consistent with indirect experimental measures of bulk in vitro assembly and thus fill the gaps between coarse-grained experimental measurements and detailed theoretical models . Nonetheless , accurate simulation predictions rely on building accurate models , which has proven to be a challenging data-fitting problem due to the high computational cost of simulating capsid assembly trajectories , high stochastic noise inherent to the system , and limited and generally noisy experimental data available . Here , we describe progress in learning accurate kinetic models of capsid assembly systems by computationally fitting assembly simulations to experimental data . We previously developed a heuristic optimization approach to learn rate parameters of coat-coat interactions by minimizing the deviation between real and simulated static light scattering measurements . We now show that one can substantially improve fitting to light scattering data using an alternative class of methods called derivative-free optimization , designed to deal with challenges of costly , noisy computations . Simultaneously , simulated exploration of potential alternative sources of experimental data for monitoring bulk assembly ( e.g , non-covalent mass spectrometry ) suggest that other feasible technologies providing richer data on assembly progress can more precisely pin down true parameters and assembly pathways . Advancing such simulation-based data fitting methods provides a general technology for greatly enhancing our ability to learn fine-scale details of complex assembly processes from experimental data , a strategy with potential application to developing accurate quantitative models of numerous other assembly systems found through biology .
2K_dev_457	This paper is a tutorial on how to model hybrid systems as hybrid programs in differential dynamic logic and how to prove complex properties about these complex hybrid systems in KeYmaera , an automatic and interactive formal verification tool for hybrid systems . Hybrid systems can model highly nontrivial controllers of physical plants , whose behaviors are often safety critical such as trains , cars , airplanes , or medical devices . Formal methods can help design systems that work correctly . This paper illustrates how KeYmaera can be used to systematically model , validate , and verify hybrid systems . We develop tutorial examples that illustrate challenges arising in many real-world systems . In the context of this tutorial , we identify the impact that modeling decisions have on the suitability of the model for verification purposes . We show how the interactive features of KeYmaera can help users understand their system designs better and prove complex properties for which the automatic prover of KeYmaera still takes an impractical amount of time . We hope this paper is a helpful resource for designers of embedded and cyber -- -physical systems and that it illustrates how to master common practical challenges in hybrid systems verification .
2K_dev_458	Automatic understanding of human activities is a huge challenge in multimedia analysis field . This challenge is especially critical in small-scale activities , such as finger motions , and activities in complex scenes . For typical camera views , both global feature and local feature analysis methods are unsuitable . To solve this problem , many studies focus on using spatio-temporal features and feature selection methods to get video representation . However , these spatio-temporal features are problematic for two reasons . First , we are not sure whether these features are meaningful foreground or noise . Second , we are unable to foresee where an activity will occur based on these features . Therefore , a biological feature selection method is needed to reorganize these spatio-temporal features and represent the video in a feature space . In this paper , we propose a graph based Co-Attention model to select more efficient features for activity analysis . Without reducing the dimensionality , our Co-Attention model considers the number of interest points . Our model is derived from correlations among individual tiny activities , whose salient regions are identified by combining an integrated top-down and bottom-up visual attention model , and a motion attention model built by spatio-temporal features instead of optical flow directly . Different from typical attention models , the Co-Attention model allows multiple regions of interest in video co-existing for further analysis . Experimental results on the KTH dataset , YouTube dataset and a new tiny activity dataset , Pump dataset which consist of visual observation data from patients operating an infusion pump , validate our activity analysis approach is more effective than state-of-the-art methods .
2K_dev_459	Machine-learning ( ML ) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices , making medical diagnoses , and facial recognition . In a model inversion attack , recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al. , adversarial access to an ML model is abused to learn sensitive genomic information about individuals . Whether model inversion attacks apply to settings outside theirs , however , is unknown . We develop a new class of model inversion attack that exploits confidence values revealed along with predictions . Our new attacks are applicable in a variety of settings , and we explore two in depth : decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition . In both cases confidence values are revealed to those with the ability to make prediction queries to models . We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and , in the other context , show how to recover recognizable images of people 's faces given only their name and access to the ML model . We also initiate experimental exploration of natural countermeasures , investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning , as well as revealing only rounded confidence values . The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility .
2K_dev_460	We model a SIS ( susceptible-infected-susceptible ) epidemics over as tatic , fi nite-sized network as ac ontinuous-time Markov process using the scaled SIS epidemics model . In our previous work , we derived the closed form description of the equilibrium distribution that explicitly accounts for the network topology and showe dt hat themost probable equilibrium statedemonstrates threshol db ehavior . In this paper , we will show how subgraph structures in the network topology impact the most probable state of the long run behavior of aS IS epidemics ( i.e. , stochastic diffusion process ) over any static , finite-sized , network .
2K_dev_461	Multimedia Event Detection ( MED ) is a multimedia retrieval task with the goal of finding videos of a particular event in a large-scale Internet video archive , given example videos and text descriptions . In this paper , we mainly focus on an 'ad-hoc ' scenario in MED where we do not use any example video . We aim to retrieve test videos based on their visual semantics using a Visual Concept Signature ( VCS ) generated for each event only derived from the event description provided as the query . Visual semantics are described using the Semantic INdexing ( SIN ) feature which represents the likelihood of predefined visual concepts in a video . To generate a VCS for an event , we project the given event description to a visual concept list using the proposed textual semantic similarity . Exploring SIN feature properties , we harmonize the generated visual concept signature and the SIN feature to improve retrieval performance . We conduct different experiments to assess the quality of generated visual concept signatures with respect to human expectation , and in the context of the MED task to retrieve the SIN feature of videos in the test dataset when we have no or only very few training videos .
2K_dev_462	Given a large graph , like who-calls-whom , or who-likes-whom , what behavior is normal and what should be surprising , possibly due to fraudulent activity ? How do graphs evolve over time ? How does influence/news/viruses propagate , over time ? We focus on three topics : ( a ) anomaly detection in large static graphs ( b ) patterns and anomalies in large time-evolving graphs and ( c ) cascades and immunization . For the first , we present a list of static and temporal laws , including advances patterns like 'eigenspokes ' ; we show how to use them to spot suspicious activities , in on-line buyer-and-seller settings , in FaceBook , in twitter-like networks . For the second , we show how to handle time-evolving graphs as tensors , how to handle large tensors in map-reduce environments , as well as some discoveries such settings . For the third , we show that for virus propagation , a single number is enough to characterize the connectivity of graph , and thus we show how to do efficient immunization for almost any type of virus ( SIS - no immunity ; SIR - lifetime immunity ; etc ) We conclude with some open research questions for graph mining .
2K_dev_463	The idea is to identify security-critical software bugs so they can be fixed first .
2K_dev_464	The space around the body provides a large interaction volume that can allow for big interactions on small mobile devices . However , interaction techniques making use of this opportunity are underexplored , primarily focusing on distributing information in the space around the body . We demonstrate three types of around-body interaction including canvas , modal and context-aware interactions in six demonstration applications . We also present a sensing solution using standard smartphone hardware : a phone 's front camera , accelerometer and inertia measurement units . Our solution allows a person to interact with a mobile device by holding and positioning it between a normal field of view and its vicinity around the body . By leveraging a user 's proprioceptive sense , around-body Interaction opens a new input channel that enhances conventional interaction on a mobile device without requiring additional hardware .
2K_dev_465	Recent improvements in content-based video search have led to systems with promising accuracy , thus opening up the possibility for interactive content-based video search to the general public . We present an interactive system based on a state-of-the-art content-based video search pipeline which enables users to do multimodal text-to-video and video-to-video search in large video collections , and to incrementally refine queries through relevance feedback and model visualization . Also , the comprehensive functionalities enhance a flexible formulation of multimodal queries with different characteristics . Quantitative and qualitative analysis shows that our system is capable of assisting users to incrementally build effective queries over complex event topics .
2K_dev_466	When offloading computation from a mobile device , we show that it can pay to perform additional on-device work in order to reduce the offloading workload . We call this offload shaping , and demonstrate its application at many different levels of abstraction using a variety of techniques . We show that offload shaping can produce significant reduction in resource demand , with little loss of application-level fidelity .
2K_dev_467	In recent years , progresses in data mining and business analytics have fostered the advent of recommender systems , behavioral advertising , and other ways of using consumer data to personalize offers and products . We investigate the incentives for sellers to invest in systems that allow the tracking of consumers and then to truthfully report whether potential buyers will enjoy yet untried products . We find that there are two types of equilibria : For some parameter values , sellers will target all potential buyers , hence their targeted ads or purchase recommendations provide no benefit to the consumer . But for other values , ads and recommendations will be accurate . In particular , the incentive for the seller to provide accurate ads and recommendations will be inversely related to the difference between the cost of producing the good and its average market evaluation . Copyright Springer Science+Business Media New York 2014
2K_dev_468	We investigate strong normalization , confluence , and behavioral equality in the realm of session-based concurrency . These interrelated issues underpin advanced correctness analysis in models of structured communications . The starting point for our study is an interpretation of linear logic propositions as session types for communicating processes , proposed in prior work . Strong normalization and confluence are established by developing a theory of logical relations . Defined upon a linear type structure , our logical relations remain remarkably similar to those for functional languages . We also introduce a natural notion of observational equivalence for session-typed processes . Strong normalization and confluence come in handy in the associated coinductive reasoning : as applications , we prove that all proof conversions induced by the logic interpretation actually express observational equivalences , and explain how type isomorphisms resulting from linear logic equivalences are realized by coercions between interface types of session-based concurrent systems .
2K_dev_469	Given a large social graph , what can we say about its robustness ? Broadly speaking , the property of robustness is crucial in real graphs , since it is related to the structural behavior of graphs to retain their connectivity properties after losing a portion of their edges/nodes . Can we estimate a robustness index for a graph quickly ? Additionally , if the graph evolves over time , how this property changes ? In this work , we are trying to answer the above questions studying the expansion properties of large social graphs . First , we present a measure that characterizes the robustness properties of a graph and also serves as global measure of the community structure ( or lack thereof ) . We show how to compute this measure efficiently by exploiting the special spectral properties of real-world networks . We apply our method on several diverse real networks with millions of nodes , and we observe interesting properties for both static and time-evolving social graphs . As an application example , we show how to spot outliers and anomalies in graphs over time . Finally , we examine how graph generating models that mimic several properties of real-world graphs and behave in terms of robustness dynamics .
2K_dev_470	User provided rating data about products and services is one key feature of websites such as Amazon , TripAdvisor , or Yelp . Since these ratings are rather static but might change over time , a temporal analysis of rating distributions provides deeper insights into the evolution of a products ' quality . Given a time-series of rating distributions , in this work , we answer the following questions : ( 1 ) How to detect the base behavior of users regarding a product 's evaluation over time ? ( 2 ) How to detect points in time where the rating distribution differs from this base behavior , e.g. , due to attacks or spontaneous changes in the product 's quality ? To achieve these goals , we model the base behavior of users regarding a product as a latent multivariate autoregressive process . This latent behavior is mixed with a sparse anomaly signal finally leading to the observed data . We propose an efficient algorithm solving our objective and we present interesting findings on various real world datasets .
2K_dev_471	Self-reported behavioral data is frequently relied upon to understand the population of social network users . These data often consist of self-reported posting , commenting or general engagement frequency within the social network over the last few days or a month . Using a sample of 397 Google+ users , we show that these data can be quite inaccurate when asking users to report on actions that tend to occur infrequently and irregularly ( e.g . profile field editing ) . Indeed , the regularity of the behavior is a strong predictor of self-report accuracy . Users who exhibit a behavior either very frequently or very infrequently are the most accurate in their reporting . For social networks , in which it is often the case that most users are `` lurkers '' who do not post or comment much , our study suggests that questions should only refer to a very narrow and recent time window to improve response accuracy . Our study also highlights the importance of considering the granularity of privacy concern measurements when investigating the so-called privacy paradox . Within our sample , those users who report that the ability to control post visibility and/or delete posts are more important than other , non-privacy related , features , do take more privacy actions . In particular , this group is less likely to enter profile information , more likely to limit the visibility of their posts and more likely to delete posts .
2K_dev_472	We study the problem of large-scale social identity linkage across different social media platforms , which is of critical importance to business intelligence by gaining from social data a deeper understanding and more accurate profiling of users . This paper proposes HYDRA , a solution framework which consists of three key steps : ( I ) modeling heterogeneous behavior by long-term behavior distribution analysis and multi-resolution temporal information matching ; ( II ) constructing structural consistency graph to measure the high-order structure consistency on users ' core social structures across different platforms ; and ( III ) learning the mapping function by multi-objective optimization composed of both the supervised learning on pair-wise ID linkage information and the cross-platform structure consistency maximization . Extensive experiments on 10 million users across seven popular social network platforms demonstrate that HYDRA correctly identifies real user linkage across different platforms , and outperforms existing state-of-the-art algorithms by at least 20 % under different settings , and 4 times better in most settings .
2K_dev_473	While much progress has been made to multi-task classification and subspace learning , multi-task feature selection has long been largely unaddressed . In this paper , we propose a new multi-task feature selection algorithm and apply it to multimedia ( e.g. , video and image ) analysis . Instead of evaluating the importance of each feature individually , our algorithm selects features in a batch mode , by which the feature correlation is considered . While feature selection has received much research attention , less effort has been made on improving the performance of feature selection by leveraging the shared knowledge from multiple related tasks . Our algorithm builds upon the assumption that different related tasks have common structures . Multiple feature selection functions of different tasks are simultaneously learned in a joint framework , which enables our algorithm to utilize the common knowledge of multiple tasks as supplementary information to facilitate decision making . An efficient iterative algorithm is proposed to optimize it , whose convergence is guaranteed . Experiments on different databases have demonstrated the effectiveness of the proposed algorithm .
2K_dev_474	We present an architecture for the Security Behavior Observatory ( SBO ) , a client-server infrastructure designed to collect a wide array of data on user and computer behavior from hundreds of participants over several years . The SBO infrastructure had to be carefully designed to fulfill several requirements . First , the SBO must scale with the desired length , breadth , and depth of data collection . Second , we must take extraordinary care to ensure the security of the collected data , which will inevitably include intimate participant behavioral data . Third , the SBO must serve our research interests , which will inevitably change as collected data is analyzed and interpreted . This short paper summarizes some of our design and implementation benefits and discusses a few hurdles and trade-offs to consider when designing such a data collection system .
2K_dev_475	Very recently , there has been a perfect storm of technical advances that has culminated in the emergence of a new interaction modality : on-body interfaces . Such systems enable the wearer to use their body as an input and output platform with interactive graphics . Projects such as PALMbit and Skinput sought to answer the initial and fundamental question : whether or not on-body interfaces were technologically possible . Although considerable technical work remains , we believe it is important to begin shifting the question away from how and what , and towards where , and ultimately why . These are the class of questions that inform the design of next generation systems . To better understand and explore this expansive space , we employed a mixed-methods research process involving more than two thousand individuals . This started with high-resolution , but low-detail crowdsourced data . We then combined this with rich , expert interviews , exploring aspects ranging from aesthetics to kinesthetics . The results of this complimentary , structured exploration , point the way towards more comfortable , efficacious , and enjoyable on-body user experiences .
2K_dev_476	Given a large image set , in which very few images have labels , how to guess labels for the remaining majority ? How to spot images that need brand new labels different from the predefined ones ? How to summarize these data to route the user 's attention to what really matters ? Here we answer all these questions . Specifically , we propose QuMinS , a fast , scalable solution to two problems : ( i ) Low-labor labeling ( LLL ) - given an image set , very few images have labels , find the most appropriate labels for the rest ; and ( ii ) Mining and attention routing - in the same setting , find clusters , the top-N '' O outlier images , and the N '' R images that best represent the data . Experiments on satellite images spanning up to 2.25 GB show that , contrasting to the state-of-the-art labeling techniques , QuMinS scales linearly on the data size , being up to 40 times faster than top competitors ( GCap ) , still achieving better or equal accuracy , it spots images that potentially require unpredicted labels , and it works even with tiny initial label sets , i.e. , nearly five examples . We also report a case study of our method 's practical usage to show that QuMinS is a viable tool for automatic coffee crop detection from remote sensing images .
2K_dev_477	We show how a disruptive force in mobile computing can be created by extending todays unmodified cloud to a second level consisting of self-managed data centers with no hard state called cloudlets . These are located at the edge of the Internet , just one wireless hop away from associated mobile devices . By leveraging lowlatency offload , cloudlets enable a new class of real-time cognitive assistive applications on wearable devices . By processing high data rate sensor inputs such as video close to the point of capture , cloudlets can reduce ingress bandwidth demand into the cloud . By serving as proxies for distant cloud services that are unavailable due to failures or cyberattacks , cloudlets can improve robustness and availability . We caution that proprietary software ecosytems surrounding cloudlets will lead to a fragmented marketplace that fails to realize the full business potential of mobile-cloud convergence . Instead , we urge that the software ecosystem surrounding cloudlets be based on the same principles of openness and end-to-end design that have made the Internet so successful .
2K_dev_478	Introductory programming activities for students often include graphical user interfaces or other visual media that are inaccessible to students with visual impairments . Digital fabrication techniques such as 3D printing offer an opportunity for students to write programs that produce tactile objects , providing an accessible way of exploring program output . This paper describes the planning and execution of a four-day computer science education workshop in which blind and visually impaired students wrote Ruby programs to analyze data from Twitter regarding a fictional ecological crisis . Students then wrote code to produce accessible tactile visualizations of that data . This paper describes outcomes from our workshop and suggests future directions for integrating data analysis and 3D printing into programming instruction for blind students .
2K_dev_479	Botnets are large networks of bots ( compromised machines ) that are under the control of a small number of bot masters . They pose a significant threat to Internets communications and applications . A botnet relies on command and control ( C2 ) communications channels traffic between its members for its attack execution . C2 traffic occurs prior to any attack ; hence , the detection of botnets C2 traffic enables the detection of members of the botnet before any real harm happens . We analyze C2 traffic and find that it exhibits a periodic behavior . This is due to the pre-programmed behavior of bots that check for updates to download them every T seconds . We exploit this periodic behavior to detect C2 traffic . The detection involves evaluating the periodogram of the monitored traffic . Then applying Walkers large sample test to the periodograms maximum ordinate in order to determine if it is due to a periodic component or not . If the periodogram of the monitored traffic contains a periodic component , then it is highly likely that it is due to a bots C2 traffic . The test looks only at aggregate control plane traffic behavior , which makes it more scalable than techniques that involve deep packet inspection ( DPI ) or tracking the communication flows of different hosts . We apply the test to two types of botnet , tinyP2P and IRC that are generated by SLINGbot . We verify the periodic behavior of their C2 traffic and compare it to the results we get on real traffic that is obtained from a secured enterprise network . We further study the characteristics of the test in the presence of injected HTTP background traffic and the effect of the duty cycle on the periodic behavior .
2K_dev_480	Memory reservations are used to provide real-time tasks with guaranteed memory access to a specified amount of physical memory . However , previous work on memory reservation primarily focused on private pages , and did not pay attention to shared pages , which are widely used in current operating systems . With previous schemes , a real-time task may experience unexpected timing delays from other tasks through shared pages that are shared by another process , even though the task has enough free pages in its own reservation . In this paper , we first describe the problems that arise when real-time tasks share pages . We then propose a shared-page management framework which enhances the temporal isolation provided by memory reservations in resource kernels that use the resource reservation approach . Our proposed solution consists of two schemes , Shared-Page Conservation ( SPC ) and Shared-Page Eviction Lock ( SPEL ) , each of which prevents timing penalties caused by the seemingly arbitrary eviction of shared pages . The framework can manage shared data for inter-process communication and shared libraries , as well as pages shared by the kernel 's copy-on-write technique and file caches . We have implemented and evaluated our schemes on the Linux/RK platform , but it can also be applied to other operating systems with paged virtual memory .
2K_dev_481	The promise of affordable , automatic approaches to real-time captioning imagines a future in which deaf and hard of hearing ( DHH ) users have immediate access to speech in the world around them my simply picking up their phone or other mobile device . While the challenges of processing highly variable natural language has prevented automated approaches from completing this task reliably enough for use in settings such as classrooms or workplaces [ 4 ] , recent work in crowd-powered approaches have allowed groups of non-expert captionists to provide a similarly-flexible source of captions for DHH users . This is in contrast to current human-powered approaches , which use highly-trained professional captionists who can type up to 250 words per minute ( WPM ) , but also can cost over $ 100/hr . In this paper , we describe a real-time demo of Legion : Scribe ( or just `` Scribe '' ) , a crowd-powered captioning system that allows untrained participants and volunteers to provide reliable captions with less than 5 seconds of latency by computationally merging their input into a single collective answer that is more accurate and more complete than any one worker could have generated alone .
2K_dev_482	We study distributed optimization where nodes cooperatively minimize the sum of their individual , locally known , convex costs $ f_ { i } ( x ) $ 's ; $ x\in\BBR^ { d } $ is global . Distributed augmented Lagrangian ( AL ) methods have good empirical performance on several signal processing and learning applications , but there is limited understanding of their convergence rates and how it depends on the underlying network . This paper establishes globally linear ( geometric ) convergence rates of a class of deterministic and randomized distributed AL methods , when the $ f_ { i } $ 's are twice continuously differentiable and have a bounded Hessian . We give explicit dependence of the convergence rates on the underlying network parameters . Simulations illustrate our analytical findings .
2K_dev_483	Dispersion curves characterize many propagation mediums . When known , many methods use these curves to analyze waves . Yet , in many scenarios , their exact values are unknown due to material and environmental uncertainty . This paper presents a fast implementation of sparse wavenumber analysis , a method for recovering dispersion curves from data . This approach , based on orthogonal matching pursuit , is compared with a prior implementation , based on basis pursuit denoising . In the results , orthogonal matching pursuit provides two to three orders of magnitude improvement in speed and a small average reduction in prediction capability . The analysis is demonstrated across multiple scenarios and parameters .
2K_dev_484	This paper presents a semi-supervised method for categorizing human actions using multiple visual features . The proposed algorithm simultaneously learns multiple features from a small number of labeled videos , and automatically utilizes data distributions between labeled and unlabeled data to boost the recognition performance . Shared structural analysis is applied in our approach to discover a common subspace shared by each type of feature . In the subspace , the proposed algorithm is able to characterize more discriminative information of each feature type . Additionally , data distribution information of each type of feature has been preserved . The aforementioned attributes make our algorithm robust for action recognition , especially when only limited labeled training samples are provided . Extensive experiments have been conducted on both the choreographed and the realistic video datasets , including KTH , Youtube action and UCF50 . Experimental results show that our method outperforms several state-of-the-art algorithms . Most notably , much better performances have been achieved when there are only a few labeled training samples .
2K_dev_485	This paper presents an information-theoretic approach to address the phasor measurement unit ( PMU ) placement problem in electric power systems . Different from the conventional `topological observability ' based approaches , this paper advocates a much more refined , information-theoretic criterion , namely the mutual information ( MI ) between PMU measurements and power system states . The proposed MI criterion not only includes observability as a special case , but also rigorously models the uncertainty reduction on power system states from PMU measurements . Thus , it can generate highly informative PMU configurations . The MI criterion can also facilitate robust PMU placement by explicitly modeling probabilistic PMU outages . We propose a greedy PMU placement algorithm , and show that it achieves an approximation ratio of ( 1-1/ e ) for any PMU placement budget . We further show that the performance is the best that one can achieve , in the sense that it is NP-hard to achieve any approximation ratio beyond ( 1-1/ e ) . Such performance guarantee makes the greedy algorithm very attractive in the practical scenario of multi-stage installations for utilities with limited budgets . Finally , simulation results demonstrate near-optimal performance of the proposed PMU placement algorithm .
2K_dev_486	Robust facial hair detection and segmentation is a highly valued soft biometric attribute for carrying out forensic facial analysis . In this paper , we propose a novel and fully automatic system , called SparCLeS , for beard/moustache detection and segmentation in challenging facial images . SparCLeS uses the multiscale self-quotient ( MSQ ) algorithm to preprocess facial images and deal with illumination variation . Histogram of oriented gradients ( HOG ) features are extracted from the preprocessed images and a dynamic sparse classifier is built using these features to classify a facial region as either containing skin or facial hair . A level set based approach , which makes use of the advantages of both global and local information , is then used to segment the regions of a face containing facial hair . Experimental results demonstrate the effectiveness of our proposed system in detecting and segmenting facial hair regions in images drawn from three databases , i.e. , the NIST Multiple Biometric Grand Challenge ( MBGC ) still face database , the NIST Color Facial Recognition Technology FERET database , and the Labeled Faces in the Wild ( LFW ) database .
2K_dev_487	A descending ( multi-item ) clock auction ( DCA ) is a mechanism for buying items from multiple potential sellers . In the DCA , bidder-specific prices are decremented over the course of the auction . In each round , each bidder might accept or decline his offer price . Accepting means the bidder is willing to sell at that price . Rejecting means the bidder will not sell at that price or a lower price . DCAs have been proposed as the method for procuring spectrum from existing holders in the FCC 's imminent incentive auctions so spectrum can be repurposed to higher-value uses . However , the DCA design has lacked a way to determine the prices to offer the bidders in each round . This is a recognized , important , and timely problem . We present , to our knowledge , the first techniques for this . We develop a percentile-based approach which provides a means to naturally reduce the offer prices to the bidders through the bidding rounds . We also develop an optimization model for setting prices so as to minimize expected payment while stochastically satisfying the feasibility constraint . ( The DCA has a final adjustment round that obtains feasibility after feasibility has been lost in the final round of the main DCA . ) We prove attractive properties of this , such as symmetry and monotonicity . We develop computational methods for solving the model . ( We also develop optimization models with recourse , but they are not computationally practical . ) We present experiments both on the homogeneous items case and the case of FCC incentive auctions , where we use real interference constraint data to get a fully faithful model of feasibility . An unexpected paradox about DCAs is that sometimes when the number of rounds allowed increases , the final payment increases . We provide an explanation for this .
2K_dev_488	Aggregations of thermostatically controlled loads ( TCLs ) have been shown to hold promise as demand response resources . However , the evaluation of these promises has relied on simulations of individual TCLs that make important assumptions about the thermal dynamics and properties of the loads , the end-users interactions with individual TCLs and the disturbances to their operation . In this paper , we first propose a data-driven modeling strategy to simulate individual TCLsspecifically , household refrigeration units ( HRUs ) that allows us to relax some of these assumptions and evaluate the validity of the approaches proposed to date . Specifically , we fit probability distributions to a year-long dataset of power measurements for HRUs and use these models to create more realistic simulations . We then derive the aggregate system equations using a bottom-up approach that results in a more flexible [ linear time invariant ( LTI ) ] system . Finally , we quantify the plant-model mismatch and evaluate the proposed strategy with the more realistic simulation . Our results show that the effects of invalid assumptions about the disturbances and time-invariant properties of individual HRUs may be mitigated by a faster sampling of the state variables and that , when this is not possible , the proposed LTI system reduces the plant-model mismatch .
2K_dev_489	We show an algorithm for solving symmetric diagonally dominant ( SDD ) linear systems with m non-zero entries to a relative error of e in O ( m log 1/2 n log c n log ( 1/ e ) ) time . Our approach follows the recursive preconditioning framework , which aims to reduce graphs to trees using iterative methods . We improve two key components of this framework : random sampling and tree embeddings . Both of these components are used in a variety of other algorithms , and our approach also extends to the dual problem of computing electrical flows . We show that preconditioners constructed by random sampling can perform well without meeting the standard requirements of iterative methods . In the graph setting , this leads to ultra-sparsifiers that have optimal behavior in expectation . The improved running time makes previous low stretch embedding algorithms the running time bottleneck in this framework . In our analysis , we relax the requirement of these embeddings to snowflake spaces . We then obtain a two-pass approach algorithm for constructing optimal embeddings in snowflake spaces that runs in O ( m log log n ) time . This algorithm is also readily parallelizable .
2K_dev_490	Social media offers a targeted way for mainstream technology companies to communicate with people with disabilities about the accessibility problems that they face . While companies have started to engage with users on social media about accessibility , they differ greatly in terms of their approach and how well they support the ways in which their users want to engage . In this paper , we describe current use patterns of six corporate accessibility teams and their users on Twitter , and present an analysis of these interactions . We find that while many users want to interact directly with companies about accessibility , companies prefer to redirect them to other channels and use Twitter for broadcast messages promoting their accessibility work instead . Our analysis demonstrates that users want to use social media to become part of the process of improving accessibility of mainstream technology , and suggests the extent to which a company is able to leverage this input depends greatly on how they choose to present themselves and interact on social media .
2K_dev_491	In this paper , we present two conceptual frameworks for GPU applications to adjust their task execution times based on total workload . These frameworks enable smart GPU resource management when many applications share GPU resources while the workloads of those applications vary . Application developers can explicitly adjust the number of GPU cores depending on their needs . An implicit adjustment will be supported by a run-time framework , which dynamically allocates the number of cores to tasks based on the total workload . The runtime support of the proposed system can be realized using functions which measure the execution times of the tasks on GPU and change the number of GPU cores . We motivate the necessity of this framework in the context of self-driving technologies , and we believe that our frameworks for GPU programming are useful contributions given the increasing emphasis on parallel heterogeneous computing .
2K_dev_492	For very large dynamic networks , monitoring the behavior of a subset of agents provides an efficient framework for detecting changes in network topology . For example , in mobile caller networks with millions of subscribers , we would like to monitor the dynamics of the smallest possible set of subscribers and still be able to infer abnormal events that occur over the entire network . In general , we assume that the temporal behavior of a network agent is captured by a ( local ) dynamic state , which may reflect either a physical property such as the number of connections or an abstract quantity such as opinions or beliefs . Further , assuming coupled linear inter-agent dynamics in which the local agent states evolve as weighted linear combinations of the neighboring agents ' states , we focus on tracking network-wide agent dynamics . Due to the large-scale nature of the problem , directly monitoring data streams of the state dynamics for every individual agent is infeasible . To address this issue , we propose a method that identifies a relatively small subset of agents whose state streams enable us to reconstruct the dynamic state evolution of all the network agents at any given time and , simultaneously , detect agent departure events . Using structural properties of the coupled inter-agent dynamics , we provide an algorithm , which is polynomial in the number of agents , to identify a small subset of agents that ensures such network observability regardless of any agent leaving . In addition , we show how well-known tools in dynamic control systems may be useful for identifying abnormal events ; in particular , we use a fault detection and isolation scheme to identify agent departures . Finally , we illustrate our method and algorithms in a small test network as a proof of concept .
2K_dev_493	This paper presents a computer vision algorithm that detects , by analyzing lane-marking detection results , stop-lines and tracks , using an unscented Kalman filter , the detected stop-line over time . To detect lateral and longitudinal lane-markings , this method applies a spatial filter emphasizing the intensity contrast between lane-marking pixels and their neighboring pixels . The authors then examine the detected lane-markings to identify perpendicular , geometry layouts between longitudinal and lateral lane-markings for stop-line detection . To provide reliable stop-line recognition , the authors developed an unscented Kalman filter to track the detected stop-line over frames . Through the tests with real-world , busy urban street videos , this method demonstrated promising results , in terms of the accuracy of the initial detection accuracy and the reliability of the tracking .
2K_dev_494	Despite the popularity of home medical devices , serious safety concerns have been raised , because the use-errors of home medical devices have linked to a large number of fatal hazards . To resolve the problem , we introduce a cognitive assistive system to automatically monitor the use of home medical devices . Being able to accurately recognize user operations is one of the most important functionalities of the proposed system . However , even though various action recognition algorithms have been proposed in recent years , it is still unknown whether they are adequate for recognizing operations in using home medical devices . Since the lack of the corresponding database is the main reason causing the situation , at the first part of this paper , we present a database specially designed for studying the use of home medical devices . Then , we evaluate the performance of the existing approaches on the proposed database . Although using state-of-art approaches which have demonstrated near perfect performance in recognizing certain general human actions , we observe significant performance drop when applying it to recognize device operations . We conclude that the tiny action involved in using devices is one of the most important reasons leading to the performance decrease . To accurately recognize tiny actions , it 's critical to focus on where the target action happens , namely the region of interest ( ROI ) and have more elaborate action modeling based on the ROI . Therefore , in the second part of this paper , we introduce a simple but effective approach to estimating ROI for recognizing tiny actions . The key idea of this method is to analyze the correlation between an action and the sub-regions of a frame . The estimated ROI is then used as a filter for building more accurate action representations . Experimental results show significant performance improvements over the baseline methods by using the estimated ROI for action recognition .
2K_dev_495	Given a real world graph , how should we lay-out its edges ? How can we compress it ? These questions are closely related , and the typical approach so far is to find clique-like communities , like the cavemen graph , and compress them . We show that the block-diagonal mental image of the cavemen graph is the wrong paradigm , in full agreement with earlier results that real world graphs have no good cuts . Instead , we propose to envision graphs as a collection of hubs connecting spokes , with super-hubs connecting the hubs , and so on , recursively . Based on the idea , we propose the SlashBurn method to recursively split a graph into hubs and spokes connected only by the hubs . We also propose techniques to select the hubs and give an ordering to the spokes , in addition to the basic SlashBurn . We give theoretical analysis of the proposed hub selection methods . Our view point has several advantages : ( a ) it avoids the no good cuts problem , ( b ) it gives better compression , and ( c ) it leads to faster execution times for matrix-vector operations , which are the back-bone of most graph processing tools . Through experiments , we show that SlashBurn consistently outperforms other methods for all data sets , resulting in better compression and faster running time . Moreover , we show that SlashBurn with the appropriate spokes ordering can further improve compression while hardly sacrificing the running time .
2K_dev_496	Exploring the similarities and differences between distributed computations in biological and computational systems .
2K_dev_497	We propose and study a new distributed Kalman filter algorithm that can track unstable dynamics with bounded mean-squared error ( MSE ) . The Network Tracking Capacity ( NTC ) of this algorithm depends only on the diffusion rate of the network and is independent of the local observation patterns , only requiring global observability . We analyze and compare the NTC for different network models .
2K_dev_498	Big data has been becoming ubiquitous and applied in numerous fields recently . The challenges to solve a large-scale machine learning problem in big data scenario generally lie in three aspects . Firstly , a proposed machine learning algorithm has to be appropriated for the distributed optimization problem . Secondly , it needs a platform for the distributed implementation . Finally , the communication delays different machines may cause problems in convergence even though the non-distributed algorithm shows a good convergence rate . In order to solve these challenges , we propose a new machine learning approach named Distributed Class-dependent Feature Analysis ( DCFA ) , to combine the advantages of sparse representation in an over-complete dictionary . The classifier is based on the estimation of class-specific optimal filters , by solving an i-norm optimization problem . We demonstrate how this problem is solved using the Alternating Direction Method of Multipliers and also explore relevant convergency details . More importantly , our proposed framework can be efficiently implemented on a robust distributed framework . Thus , it improves both accuracy and computational time in large-scale databases . Our method achieves very high classification accuracies in face recognition in the presence of occlusions on AR database . It also outperforms the state of the art methods in object recognition on two challenging large-scale object databases , i.e . Caltech101 and Caltech256 . It hence shows its applicability to general computer vision and pattern recognition problems . In addition , computational time experiments show our distributed method achieves high speedup of 7.85x on Caltech256 databases with just 10 machine nodes compared to the non-distributed version and can gain even more with more computing resources .
2K_dev_499	In this paper , we study the intertwined propagation of two competing `` memes '' ( or data , rumors , etc . ) in a composite network . Within the constraints of this scenario , we ask two key questions : ( a ) which meme will prevail ? and ( b ) can one influence the outcome of the propagations ? Our model is underpinned by two key concepts , a structural graph model ( composite network ) and a viral propagation model ( SI 1 I 2 S ) . Using this framework , we formulate a non-linear dynamic system and perform an eigenvalue analysis to identify the tipping point of the epidemic behavior . Based on insights gained from this analysis , we demonstrate an effective and accurate prediction method to determine viral dominance , which we call the EigenPredictor . Next , using a combination of synthetic and real composite networks , we evaluate the effectiveness of various viral suppression techniques by either a ) concurrently suppressing both memes or b ) unilaterally suppressing a single meme while leaving the other relatively unaffected .
2K_dev_500	The Gates Hillman prediction market ( GHPM ) was an internet prediction market designed to predict the opening day of the Gates and Hillman Centers , the new computer science complex at Carnegie Mellon University . Unlike a traditional continuous double auction format , the GHPM was mediated by an automated market maker , a central agent responsible for pricing transactions with traders over the possible opening days . The GHPMs event partition was , at the time , the largest ever elicited in any prediction market by an order of magnitude , and dealing with the markets size required new advances , including a novel span-based elicitation interface that simplified interactions with the market maker . We use the large set of identity-linked trades generated by the GHPM to examine issues of trader performance and market microstructure , including how the market both reacted to and anticipated official news releases about the buildings opening day .
2K_dev_501	Objective . Braincomputer interfaces ( BCIs ) are a promising technology for restoring motor ability to paralyzed patients . Spiking-based BCIs have successfully been used in clinical trials to control multi-degree-of-freedom robotic devices . Current implementations of these devices require a lengthy spike-sorting step , which is an obstacle to moving this technology from the lab to the clinic . A viable alternative is to avoid spike-sorting , treating all threshold crossings of the voltage waveform on an electrode as coming from one putative neuron . It is not known , however , how much decoding information might be lost by ignoring spike identity . Approach . We present a full analysis of the effects of spike-sorting schemes on decoding performance . Specifically , we compare how well two common decoders , the optimal linear estimator and the Kalman filter , reconstruct the arm movements of non-human primates performing reaching tasks , when receiving input from various sorting schemes . The schemes we tested included : using threshold crossings without spike-sorting ; expert-sorting discarding the noise ; expert-sorting , including the noise as if it were another neuron ; and automatic spike-sorting using waveform features . We also decoded from a joint statistical model for the waveforms and tuning curves , which does not involve an explicit spike-sorting step . Main results . Discarding the threshold crossings that can not be assigned to neurons degrades decoding : no spikes should be discarded . Decoding based on spike-sorted units outperforms decoding based on electrodes voltage crossings : spike-sorting is useful . The four waveform based spike-sorting methods tested here yield similar decoding efficiencies : a fast and simple method is competitive . Decoding using the joint waveform and tuning model shows promise but is not consistently superior . Significance . Our results indicate that simple automated spike-sorting performs as well as the more computationally or manually intensive methods used here . Even basic spike-sorting adds value to the low-threshold waveform-crossing methods often employed in BCI decoding .
2K_dev_502	A common approach in crowdsourcing is to break large tasks into small microtasks so that they can be parallelized across many crowd workers and so that redundant work can be more easily compared for quality control . In practice , this can result in the microtasks being presented out of their natural order and often introduces delays between individual microtasks . In this paper , we demonstrate in a study of 338 crowd workers that non-sequential microtasks and the introduction of delays significantly decreases worker performance . We show that interruptions where a large delay occurs between two related tasks can cause up to a 102 % slowdown in completion time , and interruptions where workers are asked to perform different tasks in sequence can slow down completion time by 57 % . We conclude with a set of design guidelines to improve both worker performance and realized pay , and instructions for implementing these changes in existing interfaces for crowd work .
2K_dev_503	We describe a new algorithm for computing the Voronoi diagram of a set of $ $ n $ $ n points in constant-dimensional Euclidean space . The running time of our algorithm is $ $ O ( f \log n \log \varDelta ) $ $ O ( flognlog ) where $ $ f $ $ f is the output complexity of the Voronoi diagram and $ $ \varDelta $ $ is the spread of the input , the ratio of largest to smallest pairwise distances . Despite the simplicity of the algorithm and its analysis , it improves on the state of the art for all inputs with polynomial spread and near-linear output size . The key idea is to first build the Voronoi diagram of a superset of the input points using ideas from Voronoi refinement mesh generation . Then , the extra points are removed in a straightforward way that allows the total work to be bounded in terms of the output complexity , yielding the output sensitive bound . The removal only involves local flips and is inspired by kinetic data structures .
2K_dev_504	The paper studies the qualitative behavior of a set of ordinary differential equations ( ODE ) that models the dynamics of bi-virus epidemics over bilayer networks . Each layer is a weighted digraph associated with a strain of virus ; the weights $ \gamma ^ { z } _ { ij } $ represent the rates of infection from node $ i $ to node $ j $ of strain $ z $ . We establish a sufficient condition on the $ \gamma $ s that guarantees survival of the fittestonly one strain survives . We propose an ordering of the weighted digraphs , the $ \star $ -order , and show that if the weighted digraph of strain $ y $ is $ \star $ -dominated by the weighted digraph of strain $ x $ , then $ y $ dies out in the long run . We prove that the orbits of the ODE accumulate to an attractor that captures the survival of the fittest phenomenon . Due to the coupled nonlinear high-dimension nature of the ODEs , there is no natural Lyapunov function to study their global qualitative behavior . We prove our results by combining two important properties of these ODEs : ( i ) monotonicity under a partial ordering on the set of graphs ; and ( ii ) dimension-reduction under symmetry of the graphs . Property ( ii ) allows us to fully address the survival of the fittest for regular graphs . Then , by bounding the epidemics dynamics for generic networks by the dynamics on regular networks , we prove the result for general networks .
2K_dev_505	The Internet of Things ( IoT ) offers the promise of integrating the digital world of the Internet with the physical world in which we live . But realizing this promise necessitates a systematic approach to integrating the sensors , actuators , and information on which they operate into the Internet we know today . This paper reports the design and development of an open community-oriented platform aiming to support federated sensor data as a service , featuring interoperability and reusability of heterogeneous sensor data and data services . The concepts of virtual sensors and virtual devices are identified as central autonomic units to model scalable and context-aware configurable/reconfigurable sensor data and services . The decoupling of the storage and management of sensor data and platform-oriented metadata enables the handling of both discrete and streaming sensor data . A cloud computing-empowered prototyping system has been established as a proof of concept to host smart community-oriented sensor data and services .
2K_dev_506	AbstractWith rising complexity of indoor environments and growing demand for positioning and tracking of people ( such as occupants and field workers ) indoors , there has been an increasing need to have accurate and reliable indoor positioning . In this paper , the effects of ( 1 ) the quality of positioning data and ( 2 ) the types of navigation models on the accuracy of map-matching of indoor positioning data are evaluated . Sensitivity analyses on the quality of two different types of positioning data , namely , ( 1 ) absolute point-positioning data and ( 2 ) relative point-positioning data have been carried out . Two different types of navigation models , namely , ( 1 ) network models and ( 2 ) metric models , have been evaluated to examine their effect on the accuracy of the map-matching results . Eight different map-matching algorithms were selected and their accuracies were assessed in six different indoor data-collection routes containing spaces with varying density , sizes , and shapes . The results show different ways thr ...
2K_dev_507	We study a mechanism design version of matching computation in graphs that models the game played by hospitals participating in pairwise kidney exchange programs . We present a new randomized matching mechanism for two agents which is truthful in expectation and has an approximation ratio of 3/2 to the maximum cardinality matching . This is an improvement over a recent upper bound of 2 ( Ashlagi et al. , 2010 2 ] ) and , furthermore , our mechanism beats for the first time the lower bound on the approximation ratio of deterministic truthful mechanisms . We complement our positive result with new lower bounds . Among other statements , we prove that the weaker incentive compatibility property of truthfulness in expectation in our mechanism is necessary ; universally truthful mechanisms that have an inclusion-maximality property have an approximation ratio of at least 2 .
2K_dev_508	Self-powered systems that interact with the physical world require computing platforms with predictable timing behavior and a low energy demand . Energy consumption can be reduced by choosing energy-efficient designs for both hardware and software components of the platform . We leverage the state-of-the-art in hardware design by adopting Heterogeneous Multi-core Processors with support for Dynamic Voltage and Frequency Scaling and Dynamic Power Management . Through experiments on one such platform , we expose the hardware characteristics that violate assumptions of conventional energy models and propose a revised model suitable for identifying the energy-efficient frequency range . We then address the problem of allocating real-time software components onto heterogeneous cores such that total energy is minimized . Our approach is to start from an analytically justified target load distribution and find a task assignment heuristic that approximates it . Our analysis shows that neither balancing the load nor assigning all load to the `` cheapest '' core is the best load distribution strategy , unless the cores are extremely alike or extremely different . The optimal load distribution is then formulated as a solution to a convex optimization problem . A heuristic that approximates this load distribution and an alternative method that leverages the solution explicitly are proposed as viable task assignment methods . The proposed methods are compared to state-of-the-art on simulated problem instances and in a case study of a soft-real-time application on an off-the-shelf ARM big.LITTLE heterogeneous processor .
2K_dev_509	Poor posture and incorrect muscle usage are a leading cause of many injuries in sports and fitness . For this reason , non- invasive , fine-grained sensing and monitoring of human motion and muscles is important for mitigating injury and improving fitness efficacy . Current sensing systems either de- pend on invasive techniques or unscalable approaches whose accuracy is highly dependent on body sensor placement . As a result these systems are not suitable for use in active sports or fitness training where sensing needs to be scalable , accurate and un-inhibitive to the activity being performed . We present MARS , a system that detects both body motion and individual muscle group activity during physical human activity by only using unobtrusive , non-invasive in- ertial sensors . MARS not only accurately senses and recreates human motion down to the muscles , but also allows for fast personalized system setup by determining the individual identities of the instrumented muscles , obtained with minimal system training . In a real world human study con- ducted to evaluate MARS , the system achieves greater than 95 % accuracy in identifying muscle groups .
2K_dev_510	The openness of wireless communication and the recent development of software-defined radio technology , respectively , provide a low barrier and a wide range of capabilities for misbehavior , attacks , and defenses against attacks . In this work we present finite-energy jamming games , a game model that allows a jammer and sender to choose ( 1 ) whether to transmit or sleep , ( 2 ) a power level to transmit with , and ( 3 ) what channel to transmit on . We also allow the jammer to choose on how many channels it simultaneously attacks . A major addition in finite-energy jamming games is that the jammer and sender both have a limited amount of energy which is drained according to the actions a player takes . We develop a model of our system as a zero-sum finite-horizon stochastic game with deterministic transitions . We leverage the zero-sum and finite-horizon properties of our model to design a simple polynomial-time algorithm to compute optimal randomized strategies for both players . The utility function of our game model can be decoupled into a recursive equation . Our algorithm exploits this fact to use dynamic programming to construct solutions in a bottom-up fashion . For each state of energy levels , a linear program is solved to find Nash equilibrium strategies for the subgame . With these techniques , our algorithm has only a linear dependence on the number of states , and quadratic dependence on the number of actions , allowing us to solve very large instances . By computing Nash equilibria for our game models , we explore what kind of performance guarantees can be achieved both for the sender and jammer , when playing against an optimal opponent . We also use the optimal strategies to simulate finite-energy jamming games and provide insights into robust communication among reconfigurable , yet energy-limited , radio systems . To test the performance of the optimal strategies we compare their performance with a random and adaptive strategy . Matching our intuition , the aggressiveness of an attacker is related to how much of a discount is placed on data delay . This results in the defender often choosing to sleep despite the latency implication , because the threat of jamming is high . We also present several other findings from simulations where we vary the strategies for one or both of the players .
2K_dev_511	Multimedia event detection ( MED ) and multimedia event recounting ( MER ) are fundamental tasks in managing large amounts of unconstrained web videos , and have attracted a lot of attention in recent years . Most existing systems perform MER as a post-processing step on top of the MED results . In order to leverage the mutual benefits of the two tasks , we propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events . Our premise is that a good recounting algorithm should not only explain the detection result , but should also be able to assist detection in the first place . Coupled in a joint optimization framework , recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences . To better utilize the powerful and interpretable semantic video representation , we segment each video into several shots and exploit the rich temporal structures at shot level . The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm , which , after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps , enables us to efficiently process extremely large video corpora . We test the proposed method on the large scale TRECVID MEDTest 2014 and MEDTest 2013 datasets , and obtain very promising results for both MED and MER .
2K_dev_512	Despite persistent effort , many web pages are still not accessible to everyone . Fixing web accessibility problems can be complicated . Developers need to have extensive knowledge not only of possible accessibility problems but also of approaches for fixing them . This paper is about using the large number of accessibility issues on real websites and crowd-sourced fixes for them as a unique source of learning materials for web developers to learn how to build accessible components in a cost-efficient manner . In this paper , we present the design , development and study of CAN ( Composable Accessibility Infrastructure ) , a crowdsourcing infrastructure that collects web accessibility issues and their fixes , dynamically composes solutions on-the-fly , and delivers the crowd-sourced content as teaching materials . Our unique CAN user interaction and system design enables end users with disabilities to both benefit from and contribute to the system without additional effort in their daily web browsing , and allows web developers to experience real accessibility issues and initiate a learning process with first-hand materials . CAN also provides an opportunity for data-driven discovery of the common implementation practices that cause accessibility issues . We show how CAN addresses a set of accessibility issues on the top 100 popular websites . We also present our user study results where web developers who had varying knowledge of web accessibility all found our system an effective and interesting platform to learning web accessibility .
2K_dev_513	Over the last several years there have been major breakthroughs in the design of approximation algorithms for such classic problems as finding the maximum flow in a graph . Maximum flow for undirected graphs can now be approximately solved in almost linear time . This result by researchers at Berkeley and MIT , I claim , is only the beginning of a new era in efficient algorithm design . Graph theoretic optimization problems , that have been dormant for fifty years are now seeing new and exciting algorithms . These advances have been made possible by Spectral Graph Theory , the interplay between linear algebra and combinatorial graph theory . One application of this interplay is a nearly linear time solver for Symmetric Diagonally Dominate systems ( SDD ) . This seemingly restrictive class of linear systems has received substantial interest in the last 15 years . Both algorithm design theory and practical implementations have made major progress . Surprisingly , there is an ever growing list of problems that can be efficiently solved using SDD solvers including image segmentation , image denoising , finding solutions to elliptic equations , computing maximum flow in a graph , graph sparsification , and graphics . All these examples can be viewed as special cases of convex optimization problems that arise from graph problems . I can imagine a world where such optimization problems that seem to require at least quadratic work will all be solvable by practical algorithms guaranteed to run in near linear work and are very parallel .
2K_dev_514	If the people belong to multiple online communities , their joint membership can influence the survival of each of the communities to which they belong . Communities with many joint memberships may struggle to get enough of their members ' time and attention , but find it easy to import best practices from other communities . In this paper , we study the effects of membership overlap on the survival of online communities . By analyzing the historical data of 5673 Wikia communities , we find that higher levels of membership overlap are positively associated with higher survival rates of online communities . Furthermore , we find that it is beneficial for young communities to have shared members who play a central role in other mature communities . Our contributions are two-fold . Theoretically , by examining the impact of membership overlap on the survival of online communities we identified an important mechanism underlying the success of online communities . Practically , our findings may guide community creators on how to effectively manage their members , and tool designers on how to support this task .
2K_dev_515	Self-driving vehicles seem to have become quite the rage in popular culture over the past 3 years or so . Jumpstarted by the DARPA Grand Challenges , the promise of self-driving vehicles does have the potential to revolutionize modern transportation . This talk will provide some insights on many basic questions that , however , still remain unanswered . What are the technological barriers ? What can or can not be sensed ? Can vehicles recognize and comprehend as good as ( or better than ) humans ? What role does connectivity play ( if any ) ? Will the technology be affordable for the masses ? How do issues like liability , insurance , regulations and societal acceptance impact deployment ? The talk will be based on road experiences interspersed with some speculation .
2K_dev_516	Imperfect-information games model settings where players have private information . Tremendous progress has been made in solving such games over the past 20 years , especially since the Annual Computer Poker Competition was established in 2006 , where programs play each other . This progress can fuel the operationalization of seminal game-theoretic solution concepts into detailed game models , powering a host of applications in business ( e.g. , auctions and negotiations ) , medicine ( e.g. , making sophisticated sequential plans against diseases ) , ( cyber ) security , and other domains . On page 145 of this issue , Bowling et al . ( 1 ) report on having computed a strategy for two-player limit Texas Hold'em poker that is so close to optimal that , at the pace a human plays poker , it can not be beaten with statistical significance in a lifetime . While strong strategies have been computed for larger imperfect-information games as well ( 2 6 ) , this is , to my knowledge , the largest imperfect-information game essentially solved to date , and the first one competitively played by humans that has now been essentially solved .
2K_dev_517	We demonstrate the Acoustic Location Processing System ( ALPS ) , a platform that augments BLE proximity beacons with ultrasonic transmitters in a manner that allows for precise and robust indoor localization . { \em ALPS } uses Time-Difference-Of-Arrival ( TDOA ) and Time-Of-Flight ( TOF ) ranging to accurately localize mobile devices such as off-the-shelf smartphones and tablets in 2D space . Users inside the demo area will be able to determine their location and can directly plot it relatively to a map of the area using our app on a smartphone . Once a receiving device has determined its initial position , it can synchronize its audio clock with the transmission infrastructure to perform TOF-based localization , which provides similar position accuracy to TDOA based localization with fewer beacons . Multilateration and trilateration processing for each device 's location is offloaded onto a cloud-based solver that can provide localization as a service to ALPS and similar TOF/TDOA based systems .
2K_dev_518	Motor cortex plays a substantial role in driving movement , yet the details underlying this control remain unresolved . We analyzed the extent to which movement-related information could be extracted from single-trial motor cortical activity recorded while monkeys performed center-out reaching . Using information theoretic techniques , we found that single units carry relatively little speed-related information compared with direction-related information . This result is not mitigated at the population level : simultaneously recorded population activity predicted speed with significantly lower accuracy relative to direction predictions . Furthermore , a unit-dropping analysis revealed that speed accuracy would likely remain lower than direction accuracy , even given larger populations . These results suggest that the instantaneous details of single-trial movement speed are difficult to extract using commonly assumed coding schemes . This apparent paucity of speed information takes particular importance in the context of brain-machine interfaces ( BMIs ) , which rely on extracting kinematic information from motor cortex . Previous studies have highlighted subjects ' difficulties in holding a BMI cursor stable at targets . These studies , along with our finding of relatively little speed information in motor cortex , inspired a speed-dampening Kalman filter ( SDKF ) that automatically slows the cursor upon detecting changes in decoded movement direction . Effectively , SDKF enhances speed control by using prevalent directional signals , rather than requiring speed to be directly decoded from neural activity . SDKF improved success rates by a factor of 1.7 relative to a standard Kalman filter in a closed-loop BMI task requiring stable stops at targets . BMI systems enabling stable stops will be more effective and user-friendly when translated into clinical applications .
2K_dev_519	Traditional hard real-time scheduling algorithms require the use of the worst-case execution times to guarantee that deadlines will be met . Unfortunately , many algorithms with parameters derived from sensing the physical world suffer large variations in execution time , leading to pessimistic overall utilization , such as visual recognition tasks . In this article , we present ZS-QRAM , a scheduling approach that enables the use of flexible execution times and application-derived utility to tasks in order to maximize total system utility . In particular , we provide a detailed description of the algorithm , the formal proofs for its temporal protection , and a detailed , evaluation . Our evaluation uses the Utility Degradation Resilience ( UDR ) showing that ZS-QRAM is able to obtain 4 as much UDR as ZSRM , a previous overbooking approach , and almost 2 as much UDR as Rate-Monotonic with Period Transformation ( RM/TP ) . We then evaluate a Linux kernel module implementation of our scheduler on an Unmanned Air Vehicle ( UAV ) platform . We show that , by using our approach , we are able to keep the tasks that render the most utility by degrading lower-utility ones even in the presence of highly dynamic execution times .
2K_dev_520	Complex events essentially include human , scenes , objects and actions that can be summarized by visual attributes , so leveraging relevant attributes properly could be helpful for event detection . Many works have exploited attributes at image level for various applications . However , attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data . Hence , we propose to leverage attributes at video level ( named as video attributes in this work ) , i.e. , the semantic labels of external videos are used as attributes . Compared to complex event videos , these external videos contain simple contents such as objects , scenes and actions which are the basic elements of complex events . Specifically , building upon a correlation vector which correlates the attributes and the complex event , we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos . Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach .
2K_dev_521	Traditional power system state estimation methods lack the ability to track and manage increasing uncertainties inherent in the new technologies , such as recent and ongoing massive penetration of renewable energy , distribution intelligence , and plug-in electric vehicles . To deal with the inability , a recent work proposes to utilize the unused historical data for power system state estimation . Although able to achieve much higher accuracy , the new approach is slow due to the burden by sequential similarity check over large volumes of high dimensional historical measurements , making it unsuitable for online services . This calls for a general approach to preprocess the historical data . In this paper , we propose to achieve such a goal with three steps . First , because the power systems are with periodic patterns , which create clustered measurement data , dimension reduction is proposed to remove redundancy , but still able to retrieve similar measurements . To further reduce the computational time , the k-dimensional tree indexing approach is employed in step two to group the clustered power system data into a tree structure , resulting in a log-reduction over searching time . Finally , we verify the obtained historical power system states via AC power system model and the current measurements to filter out bad historical data . Simulation results show that the new method can dramatically reduce the necessary computational time for online data-driven state estimation , while producing a highly accurate state estimate .
2K_dev_522	This article presents consensus + innovations inference algorithms that intertwine consensus ( local averaging among agents ) and innovations ( sensing and assimilation of new observations ) . These algorithms are of importance in many scenarios that involve cooperation and interaction among a large number of agents with no centralized coordination . The agents only communicate locally over sparse topologies and sense new observations at the same rate as they communicate . This stands in sharp contrast with other distributed inference approaches , in which interagent communications are assumed to occur at a much faster rate than agents can sense ( sample ) the environment so that , in between measurements , agents may iterate enough times to reach a decision-consensus before a new measurement is made and assimilated . While optimal design of distributed inference algorithms in stochastic time-varying scenarios is a hard ( often intractable ) problem , this article emphasizes the design of asymptotically ( in time ) optimal distributed inference approaches , i.e. , distributed algorithms that achieve the asymptotic performance of the corresponding optimal centralized inference approach ( with instantaneous access to the entire network sensed information at all times ) . Consensus + innovations algorithms extend consensus in nontrivial ways to mixed-scale stochastic approximation algorithms , in which the time scales ( or weighting ) of the consensus potential ( the potential for distributed agent collaboration ) and of the innovation potential ( the potential for local innovations ) are suitably traded for optimal performance . This article shows why this is needed and what the implications are , giving the reader pointers to new methodologies that are useful in their own right and in many other contexts .
2K_dev_523	The Internet of Things ( IoT ) aims to integrate the digital world of the Internet with our encompassing physical world . However , existing IoT lacks to provide considerate services , meaning that sensors dynamically `` collaborate '' to provide context-aware federated sensor data . This paper reports our on-going work developing a sensor service federation and provisioning infrastructure . A novel approach has been presented to build social sensor networks to record and study historical interaction patterns among sensors . A case study is reported to leverage in-memory database to monitor and manage real-time sensor service provisioning . A two-way publish/subscribe pattern on top of a message bus is established to assure scalability of sensor service communication . Workflow provenance is carried by a dynamic virtual device concept that we have introduced . A case study is reported to leverage in-memory database to monitor and manage real-time sensor service provisioning .
2K_dev_524	A key idea in object-oriented programming is that objects encapsulate state and interact with each other by message exchange . This perspective suggests a model of computation that is inherently concurrent ( to facilitate simultaneous message exchange ) and that accounts for the effect of message exchange on an object 's state ( to express valid sequences of state transitions ) . In this paper we show that such a model of computation arises naturally from session-based communication . We introduce an object-oriented programming language that has processes as its only objects and employs linear session types to express the protocols of message exchange and to reason about concurrency and state . Based on various examples we show that our language supports the typical patterns of object-oriented programming ( e.g. , encapsulation , dynamic dispatch , and subtyping ) while guaranteeing session fidelity in a concurrent setting . In addition , we show that our language facilitates new forms of expression ( e.g. , type-directed reuse , internal choice ) , which are not available in current object-oriented languages . We have implemented our language in a prototype compiler .
2K_dev_525	The Pascaline was the first working mechanical calculator , created in 1642 by the French polymath Blaise Pascal . Over the next two decades Pascal built 40 of these machines , of which nine survive today . Several good web resources describe the Pascaline , but to properly appreciate the sautoir , Pascal 's kinetic energy solution to jam-free ripple carry , building a working replica is invaluable . Thanks to the growing availability of rapid prototyping tools , it has become relatively easy for CS educators to fabricate physical artifacts to help students explore computational ideas . I 've created a Pascaline kit using laser-cut acrylic and standard fasteners that can be assembled with just a screwdriver , pliers , and Loctite . High school or college students with minimal skills can put it together in a few hours and have a functioning calculator . Exploring the Pascaline 's design is an engaging way to connect a milestone in the early history of computing with more modern theoretical concepts . Students can investigate questions such as : What makes a device `` digital '' ? ( Slide rules have numeric scales but are analog devices . ) How does nonlinearity produce discrete states in a continuous world ? How are nonlinearities induced in the Pascaline vs. in digital electronics ? How do the logic design concepts `` half adder '' and `` full adder '' map onto the components of the Pascaline ? Is the Pascaline really adding , or merely counting ? How does the Pascaline use nines complement arithmetic to perform subtraction , and why is n't it tens complement ? The Pascaline kit , designed in SolidWorks , is open source and available at http : //www.cs.cmu.edu/~dst/Pascaline .
2K_dev_526	User review is a crucial component of open mobile app markets such as the Google Play Store . How do we automatically summarize millions of user reviews and make sense out of them ? Unfortunately , beyond simple summaries such as histograms of user ratings , there are few analytic tools that can provide insights into user reviews . In this paper , we propose Wiscom , a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail . Our system is able to ( a ) discover inconsistencies in reviews ; ( b ) identify reasons why users like or dislike a given app , and provide an interactive , zoomable view of how users ' reviews evolve over time ; and ( c ) provide valuable insights into the entire app market , identifying users ' major concerns and preferences of different types of apps . Results using our techniques are reported on a 32GB dataset consisting of over 13 million user reviews of 171,493 Android apps in the Google Play Store . We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users .
2K_dev_527	In an effort to address persistent consumer privacy concerns , policy makers and the data industry seem to have found common grounds in proposals that aim at making online privacy more `` transparent . '' Such self-regulatory approaches rely on , among other things , providing more and better information to users of Internet services about how their data is used . However , we illustrate in a series of experiments that even simple privacy notices do not consistently impact disclosure behavior , and may in fact be used to nudge individuals to disclose variable amounts of personal information . In a first experiment , we demonstrate that the impact of privacy notices on disclosure is sensitive to relative judgments , even when the objective risks of disclosure actually stay constant . In a second experiment , we show that the impact of privacy notices on disclosure can be muted by introducing simple misdirections that do not alter the objective risk of disclosure . These findings cast doubts on the likelihood of initiatives predicated around notices and transparency to address , by themselves , online privacy concerns .
2K_dev_528	The commoditization of wireless sensing systems makes it feasible to include BAS functionality in small and medium-sized buildings . The configuration complexity and cost of installation is now the dominant barrier to adoption . In this demo we introduce a platform called Mortar.io , which focuses on ease-of-installation , secure configuration , and management of BAS sub-systems in a manner that can scale from small to large installations . Unlike cloud-reliant systems , Mortar.io distributes storage and control functionality across end devices making it robust to network and internet outages . The system , once initialized , can run autonomously on a low-cost controller within a building or connect to the cloud for remote monitoring and configuration . We will also show our efficient multi-resolution data store that buffers data locally and replicates aggregate data across devices for reliability . A publish-subscribe model built on top of XMPP is used for messaging with per-device access control and a transducer schema . Finally , a web portal provides an interface to monitor and schedule lighting , plug-loads , environmental sensors and HVAC from a single uniform interface .
2K_dev_529	Visually impaired people can struggle to use everyday appliances with inaccessible control panels . To address this problem , we present ApplianceReader - a system that combines a wearable point-of-view camera with on-demand crowdsourcing and computer vision to make appliance interfaces accessible . ApplianceReader sends photos of appliance interfaces that it has not seen previously to the crowd , who work in parallel to quickly label and describe elements of the interface . Computer vision techniques then track the user 's finger pointing at the controls and read out the labels previously provided by the crowd . This enables visually impaired users to interactively explore and use appliances without asking the crowd repetitively . ApplianceReader broadly demonstrates the potential of hybrid approaches that combine human and machine intelligence to effectively realize intelligent , interactive access technology today .
2K_dev_530	Advances in real-time , embedded and distributed systems along with control and communication theory have catalyzed the rapid emergence of cyber-physical systems such as a self-driving car . The importance of fault-tolerance support on a cyber-physical system ( CPS ) has been greatly emphasized by recent research due to the nature of CPS that senses its surroundings , processes sensor data , and reacts using its actuators . In order to tackle this challenge , we proposed SAFER ( System-level Architecture for Failure Evasion in Real-time Applications ) in our previous work . SAFER is able to tolerate fail-stop processor and/or task failures for distributed embedded real-time systems . One of its limitations , however , is that SAFER is not capable of tolerating a failure of a processor with a dedicated connection to an actuator . This paper provides a method that relaxes this limitation by ( 1 ) deploying a small piece of hardware to avoid a dedicated connection between a processor and an actuator , ( 2 ) adding a software module that monitors and controls the hardware , and ( 3 ) enhancing the failure detection and recovery mechanisms of SAFER to support these changes . The detailed implementation and evaluation of the SAFER extension is on-going work .
2K_dev_531	Behavioral coding is a common technique in the social sciences and human computer interaction for extracting meaning from video data [ 3 ] . Since computer vision can not yet reliably interpret human actions and emotions , video coding remains a time-consuming manual process done by a small team of researchers . We present Glance , a tool that allows researchers to rapidly analyze video datasets for behavioral events that are difficult to detect automatically . Glance uses the crowd to interpret natural language queries , and then aggregates and summarizes the content of the video . We show that Glance can accurately code events in video in a fraction of the time it would take a single person . We also investigate speed improvements made possible by recruiting large crowds , showing that Glance is able to code 80 % of an hour-long video in just 5 minutes . Rapid coding allows participants to have a `` conversation with their data '' to rapidly develop and refine research hypotheses in ways not previously possible .
2K_dev_532	We present the design , implementation , and verification of XMHF- an eXtensible and Modular Hypervisor Framework . XMHF is designed to achieve three goals -- modular extensibility , automated verification , and high performance . XMHF includes a core that provides functionality common to many hypervisor-based security architectures and supports extensions that augment the core with additional security or functional properties while preserving the fundamental hypervisor security property of memory integrity ( i.e. , ensuring that the hypervisor 's memory is not modified by software running at a lower privilege level ) . We verify the memory integrity of the XMHF core -- 6018 lines of code -- using a combination of automated and manual techniques . The model checker CBMC automatically verifies 5208 lines of C code in about 80 seconds using less than 2GB of RAM . We manually audit the remaining 422 lines of C code and 388 lines of assembly language code that are stable and unlikely to change as development proceeds . Our experiments indicate that XMHF 's performance is comparable to popular high-performance general-purpose hypervisors for the single guest that it supports .
2K_dev_533	The promise of `` smart '' homes , workplaces , schools , and other environments has long been championed . Unattractive , however , has been the cost to run wires and install sensors . More critically , raw sensor data tends not to align with the types of questions humans wish to ask , e.g. , do I need to restock my pantry ? Although techniques like computer vision can answer some of these questions , it requires significant effort to build and train appropriate classifiers . Even then , these systems are often brittle , with limited ability to handle new or unexpected situations , including being repositioned and environmental changes ( e.g. , lighting , furniture , seasons ) . We propose Zensors , a new sensing approach that fuses real-time human intelligence from online crowd workers with automatic approaches to provide robust , adaptive , and readily deployable intelligent sensors . With Zensors , users can go from question to live sensor feed in less than 60 seconds . Through our API , Zensors can enable a variety of rich end-user applications and moves us closer to the vision of responsive , intelligent environments .
2K_dev_534	Video analysis has been attracting increasing research due to the proliferation of internet videos . In this paper , we investigate how to improve the performance on internet quality video analysis . Particularly , we work on the scenario of few labeled training videos being provided , which is less focused in multimedia . To being with , we consider how to more effectively harness the evidences from the low-level features . Researchers have developed several promising features to represent videos to capture the semantic information . However , as videos usually characterize rich semantic contents , the analysis performance by using one single feature is potentially limited . Simply combining multiple features through early fusion or late fusion to incorporate more informative cues is doable but not optimal due to the heterogeneity and different predicting capability of these features . For better exploitation of multiple features , we propose to mine the importance of different features and cast it into the learning of the classification model . Our method is based on multiple graphs from different features and uses the Riemannian metric to evaluate the feature importance . On the other hand , to be able to use limited labeled training videos for a respectable accuracy we formulate our method in a semi-supervised way . The main contribution of this paper is a novel scheme of evaluating the feature importance that is further casted into a unified framework of harnessing multiple weighted features with limited labeled training videos . We perform extensive experiments on video action recognition and multimedia event recognition and the comparison to other state-of-the-art multi-feature learning algorithms has validated the efficacy of our framework .
2K_dev_535	Distributed reinforcement learning algorithms for collaborative multi-agent Markov decision processes ( MDPs ) are presented and analyzed . The networked setup consists of a collection of agents ( learners ) which respond differently ( depending on their instantaneous one-stage random costs ) to a global controlled state and the control actions of a remote controller . With the objective of jointly learning the optimal stationary control policy ( in the absence of global state transition and local agent cost statistics ) that minimizes network-averaged infinite horizon discounted cost , the paper presents distributed variants of Q-learning of the consensus + innovations type in which each agent sequentially refines its learning parameters by locally processing its instantaneous payoff data and the information received from neighboring agents . Under broad conditions on the multi-agent decision model and mean connectivity of the inter-agent communication network , the proposed distributed algorithms are shown to achieve optimal learning asymptotically , i.e. , almost surely ( a.s. ) each network agent is shown to learn the value function and the optimal stationary control policy of the collaborative MDP asymptotically . Further , convergence rate estimates for the proposed class of distributed learning algorithms are obtained .
2K_dev_536	Matched field processing is a model-based framework for localizing targets in complex propagation environments . In underwater acoustics , it has been extensively studied for improving localization performance in multimodal and multipath media . For guided wave structural health monitoring problems , matched field processing has not been widely applied but is an attractive option for damage localization due to equally complex propagation environments . Although effective , matched field processing is often challenging to implement because it requires accurate models of the propagation environment , and the optimization methods used to generate these models are often unreliable and computationally expensive . To address these obstacles , this paper introduces data-driven matched field processing , a framework to build models of multimodal propagation environments directly from measured data , and then use these models for localization . This paper presents the data-driven framework , analyzes its behavior under unmodel ...
2K_dev_537	With the advancement of information systems , means of communications are becoming cheaper , faster , and more available . Today , millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want . They can access their e-mails , comment on weblogs , watch and post videos and photos ( as well as comment on them ) , and make phone calls or text messages almost ubiquitously . Given this scenario , in this article , we tackle a fundamental aspect of this new era of communication : How the time intervals between communication events behave for different technologies and means of communications . Are there universal patterns for the Inter-Event Time Distribution ( IED ) q How do inter-event times behave differently among particular technologiesq To answer these questions , we analyzed eight different datasets from real and modern communication data and found four well-defined patterns seen in all the eight datasets . Moreover , we propose the use of the Self-Feeding Process ( SFP ) to generate inter-event times between communications . The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data . We also show three potential applications of the SFP : as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications , as a technique to detect anomalies , and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems .
2K_dev_538	In this video we show TouchViz , a software system for visualizing multivariate data that harnesses the physical , embodied nature of tablet computers and physical models such as gravity and force to allow users to explore data along many dimensions at once . Data are represented as actual physical objects that can be manipulated through user touches , tilts , and finger gestures . TouchViz provides an open sandbox for user interaction , supplying an array of force-based tools for structuring and manipulating data made physical . These tools promote curiosity , play , and exploration , leading users to trends and actionable findings encoded in data . By closely mimicking real-world force , gravity , and momentum , TouchViz allows users to explore many dimensions at once through multitouch interactions .
2K_dev_539	Multimedia Event Detection ( MED ) is a multimedia retrieval task with the goal of finding videos of a particular event in video archives , given example videos and event descriptions ; different from MED , multimedia classification is a task that classifies given videos into specified classes . Both tasks require mining features of example videos to learn the most discriminative features , with best performance resulting from a combination of multiple complementary features . How to combine different features is the focus of this paper . Generally , early fusion and late fusion are two popular combination strategies . The former one fuses features before performing classification and the latter one combines output of classifiers from different features . Early fusion can better capture the relationship among features yet is prone to over-fit the training data . Late fusion deals with the over-fitting problem better but does not allow classifiers to train on all the data at the same time . In this paper , we introduce a fusion scheme named double fusion , which simply combines early fusion and late fusion together to incorporate their advantages . Results are reported on the TRECVID MED 2010 , MED 2011 , UCF50 and HMDB51 datasets . For the MED 2010 dataset , we get a mean minimal normalized detection cost ( MMNDC ) of 0.49 , which exceeds the state-of-the-art performance by more than 12 percent . On the TRECVID MED 2011 test dataset , we achieve a MMNDC of 0.51 , which is the second best among all 19 participants . On UCF50 and HMDB51 , we obtain classification accuracy of 88.1 % and 48.7 % respectively , which are the best reported results to date .
2K_dev_540	Given a large collection of co-evolving multiple time-series , which contains an unknown number of patterns of different durations , how can we efficiently and effectively find typical patterns and the points of variation ? How can we statistically summarize all the sequences , and achieve a meaningful segmentation ? In this paper we present AutoPlait , a fully automatic mining algorithm for co-evolving time sequences . Our method has the following properties : ( a ) effectiveness : it operates on large collections of time-series , and finds similar segment groups that agree with human intuition ; ( b ) scalability : it is linear with the input size , and thus scales up very well ; and ( c ) AutoPlait is parameter-free , and requires no user intervention , no prior training , and no parameter tuning . Extensive experiments on 67GB of real datasets demonstrate that AutoPlait does indeed detect meaningful patterns correctly , and it outperforms state-of-the-art competitors as regards accuracy and speed : AutoPlait achieves near-perfect , over 95 % precision and recall , and it is up to 472 times faster than its competitors .
2K_dev_541	In this paper we focus on common data reorganization operations such as shuffle , pack/unpack , swap , transpose , and layout transformations . Although these operations simply relocate the data in the memory , they are costly on conventional systems mainly due to inefficient access patterns , limited data reuse and roundtrip data traversal throughout the memory hierarchy . This paper presents a two pronged approach for efficient data reorganization , which combines ( i ) a proposed DRAM-aware reshape accelerator integrated within 3D-stacked DRAM , and ( ii ) a mathematical framework that is used to represent and optimize the reorganization operations . We evaluate our proposed system through two major use cases . First , we demonstrate the reshape accelerator in performing a physical address remapping via data layout transform to utilize the internal parallelism/locality of the 3D-stacked DRAM structure more efficiently for general purpose workloads . Then , we focus on offloading and accelerating commonly used data reorganization routines selected from the Intel Math Kernel Library package . We evaluate the energy and performance benefits of our approach by comparing it against existing optimized implementations on state-of-the-art GPUs and CPUs . For the various test cases , in-memory data reorganization provides orders of magnitude performance and energy efficiency improvements via low overhead hardware .
2K_dev_542	Can we identify patterns of temporal activities caused by human communications in social media ? Is it possible to model these patterns and tell if a user is a human or a bot based only on the timing of their postings ? Social media services allow users to make postings , generating large datasets of human activity time-stamps . In this paper we analyze time-stamp data from social media services and find that the distribution of postings inter-arrival times ( IAT ) is characterized by four patterns : ( i ) positive correlation between consecutive IATs , ( ii ) heavy tails , ( iii ) periodic spikes and ( iv ) bimodal distribution . Based on our findings , we propose Rest-Sleep-and-Comment ( RSC ) , a generative model that is able to match all four discovered patterns . We demonstrate the utility of RSC by showing that it can accurately fit real time-stamp data from Reddit and Twitter . We also show that RSC can be used to spot outliers and detect users with non-human behavior , such as bots . We validate RSC using real data consisting of over 35 million postings from Twitter and Reddit . RSC consistently provides a better fit to real data and clearly outperform existing models for human dynamics . RSC was also able to detect bots with a precision higher than 94 % .
2K_dev_543	Detecting and quantifying the timing and the genetic contributions of parental populations to a hybrid population is an important but challenging problem in reconstructing evolutionary histories from genetic variation data . With the advent of high throughput genotyping technologies , new methods suitable for large-scale data are especially needed . Furthermore , existing methods typically assume the assignment of individuals into subpopulations is known , when that itself is a difficult problem often unresolved for real data . Here , we propose a novel method that combines prior work for inferring nonreticulate population structures with an MCMC scheme for sampling over admixture scenarios to both identify population assignments and learn divergence times and admixture proportions for those populations using genome-scale admixed genetic variation data . We validated our method using coalescent simulations and a collection of real bovine and human variation data . On simulated sequences , our methods show better accuracy and faster runtime than leading competitive methods in estimating admixture fractions and divergence times . Analysis on the real data further shows our methods to be effective at matching our best current knowledge about the relevant populations .
2K_dev_544	The paper studies the problem of distributed parameter estimation in multi-agent networks with exponential family observation statistics . A certainty-equivalence type distributed estimator of the consensus + innovations form is proposed in which , at each each observation sampling epoch agents update their local parameter estimates by appropriately combining the data received from their neighbors and the locally sensed new information ( innovation ) . Under global observability of the networked sensing model , i.e. , the ability to distinguish between different instances of the parameter value based on the joint observation statistics , and mean connectivity of the inter-agent communication network , the proposed estimator is shown to yield consistent parameter estimates at each network agent . Further , it is shown that the distributed estimator is asymptotically efficient , in that , the asymptotic covariances of the agent estimates coincide with that of the optimal centralized estimator , i.e. , the inverse of the centralized Fisher information rate . From a technical viewpoint , the proposed distributed estimator leads to non-Markovian mixed timescale stochastic recursions and the analytical methods developed in the paper contribute to the general theory of distributed stochastic approximation .
2K_dev_545	Reasoning that overexpression of multiple E2F-responsive genes might be a useful marker for RB1 dysfunction , we compiled a list of E2F-responsive genes from the literature and evaluated their expression in publicly available gene expression microarray data of patients with breast cancer , serous ovarian cancer , and prostate cancer . In breast cancer , a group of tumors was identified , each of which simultaneously overexpressed multiple E2F-responsive genes . Seventy percent of these genes were concerned with cell cycle progression , DNA repair , or mitosis . These E2F-responsive gene overexpressing ( ERGO ) tumors frequently exhibited additional evidence of Rb/E2F axis dysfunction , were mostly triple negative , and preferentially overexpressed multiple basal cytokeratins , suggesting that they overlapped substantially with the basal-like tumor subset . ERGO tumors were also identified in serous ovarian cancer and prostate cancer . In these cancer types , there was no evidence for a tumor subset comparable to the breast cancer basal-like subset . A core group of about 30 E2F-responsive genes were overexpressed in all three cancer types . Thus , it appears that disorders of the Rb/E2F axis can arise at multiple organ sites and produce tumors that simultaneously overexpress multiple E2F-responsive genes .
2K_dev_546	We explore 3D printing physical controls whose tactile response can be manipulated programmatically through pneumatic actuation . In particular , by manipulating the internal air pressure of various pneumatic elements , we can create mechanisms that require different levels of actuation force and can also change their shape . We introduce and discuss a series of example 3D printed pneumatic controls , which demonstrate the feasibility of our approach . This includes conventional controls , such as buttons , knobs and sliders , but also extends to domains such as toys and deformable interfaces . We describe the challenges that we faced and the methods that we used to overcome some of the limitations of current 3D printing technology . We conclude with example applications and thoughts on future avenues of research .
2K_dev_547	Many people would find the Web easier to use if content was a little bigger , even those who already find the Web possible to use now . This paper introduces the idea of opportunistic accessibility improvement in which improvements intended to make a web page easier to access , such as magnification , are automatically applied to the extent that they can be without causing negative side effects . We explore this idea with oppaccess.js , an easily-deployed system for magnifying web pages that iteratively increases magnification until it notices negative side effects , such as horizontal scrolling or overlapping text . We validate this approach by magnifying existing web pages 1.6x on average without introducing negative side effects . We believe this concept applies generally across a wide range of accessibility improvements designed to help people with diverse abilities .
2K_dev_548	We consider the problem of fairly allocating indivisible goods , focusing on a recently-introduced notion of fairness called maximin share guarantee : Each player 's value for his allocation should be at least as high as what he can guarantee by dividing the items into as many bundles as there are players and receiving his least desirable bundle . Assuming additive valuation functions , we show that such allocations may not exist , but allocations guaranteeing each player 2/3 of the above value always exist , and can be computed in polynomial time when the number of players is constant . These theoretical results have direct practical implications .
2K_dev_549	How can we model users ' preferences ? How do anomalies , fraud , and spam effect our models of normal users ? How can we modify our models to catch fraudsters ? In this tutorial we will answer these questions - connecting graph analysis tools for user behavior modeling to anomaly and fraud detection . In particular , we will focus on the application of subgraph analysis , label propagation , and latent factor models to static , evolving , and attributed graphs . For each of these techniques we will give a brief explanation of the algorithms and the intuition behind them . We will then give examples of recent research using the techniques to model , understand and predict normal behavior . With this intuition for how these methods are applied to graphs and user behavior , we will focus on state-of-the-art research showing how the outcomes of these methods are effected by fraud , and how they have been used to catch fraudsters .
2K_dev_550	This Review summarizes and draws connections between diverse streams of empirical research on privacy behavior . We use three themes to connect insights from social and behavioral sciences : peoples uncertainty about the consequences of privacy-related behaviors and their own preferences over those consequences ; the context-dependence of peoples concern , or lack thereof , about privacy ; and the degree to which privacy concerns are malleablemanipulable by commercial and governmental interests . Organizing our discussion by these themes , we offer observations concerning the role of public policy in the protection of privacy in the information age .
2K_dev_551	We introduce the notion of restricted sensitivity as an alternative to global and smooth sensitivity to improve accuracy in differentially private data analysis . The definition of restricted sensitivity is similar to that of global sensitivity except that instead of quantifying over all possible datasets , we take advantage of any beliefs about the dataset that a querier may have , to quantify over a restricted class of datasets . Specifically , given a query f and a hypothesis HH about the structure of a dataset D , we show generically how to transform f into a new query f HH whose global sensitivity ( over all datasets including those that do not satisfy HH ) matches the restricted sensitivity of the query f. Moreover , if the belief of the querier is correct ( i.e. , D HH ) then f HH ( D ) 0 f ( D ) . If the belief is incorrect , then f HH ( D ) may be inaccurate . We demonstrate the usefulness of this notion by considering the task of answering queries regarding social-networks , which we model as a combination of a graph and a labeling of its vertices . In particular , while our generic procedure is computationally inefficient , for the specific definition of H as graphs of bounded degree , we exhibit efficient ways of constructing f H using different projection-based techniques . We then analyze two important query classes : subgraph counting queries ( e.g. , number of triangles ) and local profile queries ( e.g. , number of people who know a spy and a computer-scientist who know each other ) . We demonstrate that the restricted sensitivity of such queries can be significantly lower than their smooth sensitivity . Thus , using restricted sensitivity we can maintain privacy whether or not D HH , while providing more accurate results in the event that HH holds true .
2K_dev_552	Activity recognition can provide computers with the context underlying user inputs , enabling more relevant responses and more fluid interaction . However , training these systems is difficult because it requires observing every possible sequence of actions that comprise a given activity . Prior work has enabled the crowd to provide labels in real-time to train automated systems on-the-fly , but numerous examples are still needed before the system can recognize an activity on its own . To reduce the need to collect this data by observing users , we introduce ARchitect , a system that uses the crowd to capture the dependency structure of the actions that make up activities . Our tests show that over seven times as many examples can be collected using our approach versus relying on direct observation alone , demonstrating that by leveraging the understanding of the crowd , it is possible to more easily train automated systems .
2K_dev_553	Multimedia event detection ( MED ) plays an important role in many applications such as video indexing and retrieval . Current event detection works mainly focus on sports and news event detection or abnormality detection in surveillance videos . Differently , our research aims to detect more complicated and generic events within a longer video sequence . In the past , researchers have proposed using intermediate concept classifiers with concept lexica to help understand the videos . Yet it is difficult to judge how many and what concepts would be sufficient for the particular video analysis task . Additionally , obtaining robust semantic concept classifiers requires a large number of positive training examples , which in turn has high human annotation cost . In this paper , we propose an approach that exploits the external concepts-based videos and event-based videos simultaneously to learn an intermediate representation from video features . Our algorithm integrates the classifier inference and latent intermediate representation into a joint framework . The joint optimization of the intermediate representation and the classifier makes them mutually beneficial and reciprocal . Effectively , the intermediate representation and the classifier are tightly correlated . The classifier dependent intermediate representation not only accurately reflects the task semantics but is also more suitable for the specific classifier . Thus we have created a discriminative semantic analysis framework based on a tightly coupled intermediate representation . Extensive experiments on multimedia event detection using real-world videos demonstrate the effectiveness of the proposed approach .
2K_dev_554	Ultrasonic guided-waves propagating in pipes with varying environmental and operational conditions ( EOCs ) are usually the results of complex superposition of multiple modes travelling in multiple paths . Among all of the components forming a complex guided-wave signal , the arrivals scattered by damage ( so called scatter signal ) are of importance for damage diagnosis purposes . This paper evaluates the potentials of nonlinear decomposition methods for extracting the scatter signal from a multi-modal signal recorded from a pipe under varying temperatures . Current approaches for extracting scatter signal can be categorized as ( A ) baseline subtraction methods , and ( B ) linear decomposition methods . In this paper , we first illustrate , experimentally , the challenges for applying these methods on multi-modal signals at varying temperatures . To better analyze the experimental results , the effects of temperature on multi-modal signals are simulated . The simulation results show that different wave modes may have significantly different sensitivities to temperature variations . This brings about challenges such as shape distortion and nonlinear relations between the signals recorded at different temperatures , which prevent the aforementioned methods to be extensible to wide range of temperatures . In this paper , we examine the potential of a nonlinear decomposition method , namely nonlinear principal component analysis ( NLPCA ) , for removing the nonlinear relation between the components of a multi-modal guided-wave signal , and thus , extracting the scatter signal . Ultrasonic pitch-catch measurements from an aluminum pipe segment in a thermally controlled laboratory are used to evaluate the detection performance of the damage-sensitive features extracted by the proposed approach . It is observed that NLPCA can successfully remove nonlinear relations between the signal bases , hence extract scatter signal , for temperature variations up to 10 , with detection accuracies above 99 % . ( 2015 ) COPYRIGHT Society of Photo-Optical Instrumentation Engineers ( SPIE ) . Downloading of the abstract is permitted for personal use only .
2K_dev_555	Abstract In case of an emergency in a building , first responders need to know current blockages in the building ( e.g. , blocked passageways and exits ) and safe evacuating paths so that the occupants can be guided to the unblocked exits and safe paths toward those exits . To automatically determine blockage levels at buildings , a system that fuses data from multiple sensors and video camera was proposed . A prototype was developed and tested on an experimental model of a pilot building 's hallway . A series of damage tests were conducted on the hallway model and recorded by the sensors and the video camera . Individual performances of sensors and video camera were evaluated , and a decision tree method was used to fuse sensor and video camera data for estimating the level of blockage in the hallway for different damage combinations applied on building components . The results demonstrated the technical feasibility of the proposed system and the findings of the decision tree highlight that by using less number of sensors , a cost-effective configuration can be achieved . The estimated blockage information can be used to create a topological map of the damaged building , indicating safe paths toward available unblocked exits .
2K_dev_556	Topic models are effective probabilistic tools for processing large collections of unstructured data . With the exponential growth of modern industrial data , and consequentially also with our ambition to explore much bigger models , there is a real pressing need to significantly scale up topic modeling algorithms , which has been taken up in lots of previous works , culminating in the recent fast Markov chain Monte Carlo sampling algorithms in [ 10 , 23 ] for the unsupervised latent Dirichlet allocation ( LDA ) formulations . In this work we extend the recent sampling advances for unsupervised LDA models to supervised tasks . We focus on the Gibbs MedLDA model [ 27 ] that is able to simultaneously discover latent structures and make accurate predictions . By combining a set of sampling techniques we are able to reduce the O ( K 3 + DK 2 + DNK complexity in [ 27 ] to O ( DK + DN ) when there are K topics and D documents with average length N . To our best knowledge , this is the first linear time sampling algorithm for supervised topic models . Our algorithm requires minimal modifications to incorporate most loss functions in a variety of supervised tasks , and we observe in our experiments an order of magnitude speedup over the current state-of-the-art implementation , while achieving similar prediction performances . The open-source C++ implementation of the proposed algorithm is available at https : //github.com/xunzheng/light_medlda .
2K_dev_557	We derive the time reversal likelihood ratio optimal detector for a target in stationary random multipath clutter , extending prior work where both are assumed to be deterministic . We suppress the stationary clutter through adaptive power allocation . We show that the Time Reversal Likelihood Ratio Test performs much better than the conventional Weighted Energy Detector . We provide analytical results on the performance of the time reversal detector by approximating it with the Time Reversal Linear Quadratic detector , which models the received signal as a Complex Gaussian . Our simulations show that the Linear Quadratic detector is a good approximation to the Time Reversal Likelihood Ratio Test and that both show a significant improvement of 4 to 7 dB effective signal-to-noise ratio gain over the Weighted Energy Detector . This gain is dependent on the target and clutter power spectral densities .
2K_dev_558	Brain-computer interfaces ( BCIs ) and optical imaging have both undergone impressive technological growth in recent years . A study in which mice learn to modulate neural activity merges these technologies to investigate the neural basis of BCI learning with unprecedented spatial detail .
2K_dev_559	Text can often be complex and difficult to read , especially for people with cognitive impairments or low literacy skills . Text simplification is a process that reduces the complexity of both wording and structure in a sentence , while retaining its meaning . However , this is currently a challenging task for machines , and thus , providing effective on-demand text simplification to those who need it remains an unsolved problem . Even evaluating the simplicity of text remains a challenging problem for both computers , which can not understand the meaning of text , and humans , who often struggle to agree on what constitutes a good simplification . This paper focuses on the evaluation of English text simplification using the crowd . We show that leveraging crowds can result in a collective decision that is accurate and converges to a consensus rating . Our results from 2,500 crowd annotations show that the crowd can effectively rate levels of simplicity . This may allow simplification systems and system builders to get better feedback about how well content is being simplified , as compared to standard measures which classify content into 'simplified ' or 'not simplified ' categories . Our study provides evidence that the crowd could be used to evaluate English text simplification , as well as to create simplified text in future work .
2K_dev_560	Seeking solutions from one domain to solve problems in another is an effective process of innovation . This process of analogy searching is difficult for both humans and machines . In this paper , we present a novel approach for re-presenting a problem in terms of its abstract structure , and then allowing people to use this structural representation to find analogies . We propose a crowdsourcing process that helps people navigate a large dataset to find analogies . Through two experiments , we show the benefits of using abstract structural representations to search for ideas that are analogous to a source problem , and that these analogies result in better solutions than alternative approaches . This work provides a useful method for finding analogies , and can streamline innovation for both novices and professional designers .
2K_dev_561	Welcome to SIGMETRICS 2013 . SIGMETRICS is the flagship conference of the ACM special interest group for the computer systems performance evaluation community . This year marks the fortieth anniversary since SIGMETRICS ( under its prior name , SIGME ) held the First National SIGME Symposium on Measurement and Evaluation in 1973 . The past four decades have seen enormous changes in the field of computer science , but the importance of measurement , modeling , and performance evaluation remains as critical as ever . This year 's conference includes papers on topics that have been a mainstay since the founding of our SIG , including queuing , scheduling , resource allocation , and performance measurement . Application areas that have emerged in recent years , such as multicore systems , cellular networks , and energy optimization , continue to be represented in our program . Papers on solid-state storage have seen a significant uptick this year , and we have papers on some topics that are new to SIGMETRICS , including crowdsourcing and RFID systems . Interestingly , the program also shows a drop-off in topics that were hot just a brief while ago , such as social networks , BitTorrent , swarms , peer-to-peer , and MapReduce . We received 196 submissions to this year 's conference , of which 26 appear in the program as full papers , which is a highly competitive acceptance ratio below 14 % . An additional 28 submissions appear in the abbreviated form of poster presentations with brief summaries in the proceedings . As in some prior years , we performed reviews in two rounds . In the first round , each paper was assigned to four reviewers . In the second round , additional reviews were assigned to papers with fewer than three completed reviews and papers with highly divergent review opinions and fewer than two high-confidence reviews . We experimented with two changes to the review process this year . The first change to the review process was the addition of a rebuttal phase between the first and second review rounds , to give authors an opportunity to respond to questions raised in first-round reviews . To impede the addition of new substantive material in the rebuttals , and instead reserve rebuttals for merely highlighting information already contained in the submission , we strictly limited each rebuttal to 500 characters . It is not easy to gauge the effectiveness of the rebuttal process : There were many occasions during the PC meeting when reviewers commented on items in authors ' rebuttals , which suggests that the rebuttals provided additional information ; however , reviewers mostly found that their opinions were unchanged by what they read in the rebuttals . The second change to the review process was the use of rankings rather than ratings . Instead of rating their assigned papers with accept/reject recommendations , each PC member was asked to produce a list of assigned papers ordered by the reviewer 's assessment of each paper 's overall quality . Our intent was to eliminate the bias that is inherent in accept/reject recommendations because each reviewer has only a narrow view of the conference 's submissions . Reviewers ' individual rankings were combined into a global ranking using an algorithm similar to PageRank , and the top 60 papers were discussed at the PC meeting . During the PC meeting , whenever a paper was accepted , we identified any paper with global rank below 60 that at least one reviewer had ranked substantially higher than the accepted paper . The reviewer was given the option to add this other paper to the discussion list . About a dozen such additional papers were discussed , although none were accepted as full papers . We are pleased to present three awards to two of this year 's papers . The SIGMETRICS Best Paper Award honors the overall best paper in each year 's conference , and the Kenneth C. Sevcik Outstanding Student Paper Award honors an outstanding paper whose primary author is a student . This year , both awards are presented to an outstanding student paper that is also the overall best paper in the conference : `` Queueing System Topologies with Limited Flexibility , '' by John N. Tsitsiklis and Kuang Xu . We are inaugurating a new award this year , the SIGMETRICS Best Practical Paper Award , to honor the best paper from among those whose research has the most direct practical applicability . This award is presented to `` Practical Conflict Graphs for Dynamic Spectrum Distribution , '' by Xia Zhou , Zengbin Zhang , Gang Wang , Xiaoxiao Yu , Ben Y. Zhao , and Haitao Zheng .
2K_dev_562	In 1876 , Charles Lutwidge Dodgson suggested the intriguing voting rule that today bears his name . Although Dodgsons rule is one of the most well-studied voting rules , it suffers from serious deficiencies , both from the computational point of viewit is NP-hard even to approximate the Dodgson score within sublogarithmic factorsand from the social choice point of viewit fails basic social choice desiderata such as monotonicity and homogeneity . However , this does not preclude the existence of approximation algorithms for Dodgson that are monotonic or homogeneous , and indeed it is natural to ask whether such algorithms exist . In this article , we give definitive answers to these questions . We design a monotonic exponential-time algorithm that yields a 2-approximation to the Dodgson score , while matching this result with a tight lower bound . We also present a monotonic polynomial-time O ( log m ) -approximation algorithm ( where m is the number of alternatives ) ; this result is tight as well due to a complexity-theoretic lower bound . Furthermore , we show that a slight variation on a known voting rule yields a monotonic , homogeneous , polynomial-time O ( m log m ) -approximation algorithm and establish that it is impossible to achieve a better approximation ratio even if one just asks for homogeneity . We complete the picture by studying several additional social choice properties ; for these properties , we prove that algorithms with an approximation ratio that depends only on m do not exist .
2K_dev_563	Given a snapshot of a large graph , in which an infection has been spreading for some time , can we identify those nodes from which the infection started to spread ? In other words , can we reliably tell who the culprits are ? In this paper , we answer this question affirmatively and give an efficient method called NetSleuth for the well-known susceptible-infected virus propagation model . Essentially , we are after that set of seed nodes that best explain the given snapshot . We propose to employ the minimum description length principle to identify the best set of seed nodes and virus propagation ripple , as the one by which we can most succinctly describe the infected graph . We give an highly efficient algorithm to identify likely sets of seed nodes given a snapshot . Then , given these seed nodes , we show we can optimize the virus propagation ripple in a principled way by maximizing likelihood . With all three combined , NetSleuth can automatically identify the correct number of seed nodes , as well as which nodes are the culprits . Experimentation on our method shows high accuracy in the detection of seed nodes , in addition to the correct automatic identification of their number . Moreover , NetSleuth scales linearly in the number of nodes of the graph .
2K_dev_564	Computer accompaniment began in the eighties as a technology to synchronize computers to live musicians by sensing , following , and adapting to expressive musical performances . The technology has progressed from systems where performances were modeled as sequences of discrete symbols , i.e. , pitches , to modern systems that use continuous probabilistic models . Although score following techniques have been a common focus , computer accompaniment research has addressed many other interesting topics , including the musical adjustment of tempo , the problem of following an ensemble of musicians , and making systems more robust to unexpected mistakes by performers . Looking toward the future , we find that score following is only one of many ways musicians use to synchronize . Score following is appropriate when scores exist and describe the performance accurately , and where timing deviations are to be followed rather than ignored . In many cases , however , especially in popular music forms , tempo is rather steady , and pe ...
2K_dev_565	The two letters in this special issue cover two critical topics for future vehicles : security layers for smartphone-to-vehicle communication and the design of new applications for vehicle sharing .
2K_dev_566	As sensor networks gain traction and begin to scale , we will be increasingly faced with challenges associated with managing large-scale time-series data . In this paper , we present a cloud-to-edge partitioned architecture called Respawn that is capable of serving large amounts of time-series data from a continuously updating datastore with access latencies low enough to support interactive real-time visualization . Respawn targets sensing systems where resource-constrained edge node devices may only have limited or intermittent network connections linking them to a cloud-backend . The cloud-backend provides aggregate storage and transparent dispatching of data queries to edge node devices . Data is downsampled as it enters the system creating a multi-resolution representation capable of lowlatency range-base queries . Lower-resolution aggregate data is automatically migrated from edge nodes to the cloud-backend both for improved consistency and caching . In order to further mask latency from users , edge nodes automatically identify and migrate blocks of data that contain statistically interesting features . We show through simulation and micro-benchmarking that Respawn is able to run on ARM-based edge node devices connected to a cloud-backend with the ability to serve thousands of clients and terabytes of data with sub-second latencies .
2K_dev_567	Online instructional videos have become a popular way for people to learn new skills encompassing art , cooking and sports . As watching instructional videos is a natural way for humans to learn , analogously , machines can also gain knowledge from these videos . We propose to utilize the large amount of instructional videos available online to harvest examples of various actions in an unsupervised fashion . The key observation is that in instructional videos , the instructor 's action is highly correlated with the instructor 's narration . By leveraging this correlation , we can exploit the timing of action corresponding terms in the speech transcript to temporally localize actions in the video and harvest action examples . The proposed method is scalable as it requires no human intervention . Experiments show that the examples harvested are of reasonably good quality , and action detectors trained on data collected by our unsupervised method yields comparable performance with detectors trained with manually collected data on the TRECVID Multimedia Event Detection task .
2K_dev_568	We introduce GOTCHAs ( Generating panOptic Turing Tests to Tell Computers and Humans Apart ) as a way of preventing automated offline dictionary attacks against user selected passwords . A GOTCHA is a randomized puzzle generation protocol , which involves interaction between a computer and a human . Informally , a GOTCHA should satisfy two key properties : ( 1 ) The puzzles are easy for the human to solve . ( 2 ) The puzzles are hard for a computer to solve even if it has the random bits used by the computer to generate the final puzzle -- - unlike a CAPTCHA [ 44 ] . Our main theorem demonstrates that GOTCHAs can be used to mitigate the threat of offline dictionary attacks against passwords by ensuring that a password cracker must receive constant feedback from a human being while mounting an attack . Finally , we provide a candidate construction of GOTCHAs based on Inkblot images . Our construction relies on the usability assumption that users can recognize the phrases that they originally used to describe each Inkblot image -- - a much weaker usability assumption than previous password systems based on Inkblots which required users to recall their phrase exactly . We conduct a user study to evaluate the usability of our GOTCHA construction . We also generate a GOTCHA challenge where we encourage artificial intelligence and security researchers to try to crack several passwords protected with our scheme .
2K_dev_569	Viral videos that gain popularity through the process of Internet sharing are having a profound impact on society . Existing studies on viral videos have only been on small or confidential datasets . We collect by far the largest open benchmark for viral video study called CMU Viral Video Dataset , and share it with researchers from both academia and industry . Having verified existing observations on the dataset , we discover some interesting characteristics of viral videos . Based on our analysis , in the second half of the paper , we propose a model to forecast the future peak day of viral videos . The application of our work is not only important for advertising agencies to plan advertising campaigns and estimate costs , but also for companies to be able to quickly respond to rivals in viral marketing campaigns . The proposed method is unique in that it is the first attempt to incorporate video metadata into the peak day prediction . The empirical results demonstrate that the proposed method outperforms the state-of-the-art methods , with statistically significant differences .
2K_dev_570	Space-Time Adaptive Processing ( STAP ) is a technique for processing signals from multiple antenna elements over multiple time periods for target detection . As STAP algorithms are typical run on airborne platforms , they need to be both high performance and energy-efficient . Due to the high rate of processing required , many existing algorithms focus on reducing the dimensionality of the data , or exploiting structure in the underlying mathematical formulation in order to reduce the total number of floating-point operations ( FLOPs ) , and consequently , the time for computation . While such algorithms target the FLOPs-intensive operations within the STAP algorithm , a significant portion of the compute time for most STAP algorithms is actually spent in low-FLOPs , memory-bounded operations . In this paper , we address the computation of these memory-bounded operations within the STAP algorithm using a 3D stacked Logic-in-Memory system . The imminent arrival of 3D stacked memory makes avail high memory bandwidth , which opens up a new and orthogonal dimension for optimizing STAP algorithms . We show that more than 11x improvement in time , and 77x improvement in energy efficiency can be expected when a 3D stack is used together with memory-side accelerators to target the memory-bounded operations within STAP .
2K_dev_571	For deeply scaled digital integrated systems , the power required for transporting data between memory and logic can exceed the power needed for computation , thereby limiting the efficacy of synthesizing logic and compiling memory independently . Logic-in-Memory ( LiM ) architectures address this challenge by embedding logic within the memory block to perform basic operations on data locally for specific functions . While custom smart memories have been successfully constructed for various applications , a fully automated LiM synthesis flow enables architectural exploration that has heretofore not been possible . In this paper we present a tool and design methodology for LiM physical synthesis that performs co-design of algorithms and architectures to explore system level trade-offs . The resulting layouts and timing models can be incorporated within any physical synthesis tool . Silicon results shown in this paper demonstrate a 250x performance improvement and 310x energy savings for a data-intensive application example .
2K_dev_572	We consider an adaptive cruise control system in which control decisions are made based on position and velocity information received from other vehicles via V2V wireless communication . If the vehicles follow each other at a close distance , they have better wireless reception but collisions may occur when a follower car does not receive notice about the decelerations of the leader car fast enough to react before it is too late . If the vehicles are farther apart , they would have a bigger safety margin , but the wireless communication drops out more often , so that the follower car no longer receives what the leader car is doing . In order to guarantee safety , such a system must return control to the driver if it does not receive an update from a nearby vehicle within some timeout period . The value of this timeout parameter encodes a tradeoff between the likelihood that an update is received and the maximum safe acceleration . Combining formal verification techniques for hybrid systems with a wireless communication model , we analyze how the expected efficiency of a provably-safe adaptive cruise control system is affected by the value of this timeout .
2K_dev_573	Accurate models of the cross-talk between signaling pathways and transcriptional regulatory networks within cells are essential to understand complex response programs . We present a new computational method that combines condition-specific time-series expression data with general protein interaction data to reconstruct dynamic and causal stress response networks . These networks characterize the pathways involved in the response , their time of activation , and the affected genes . The signaling and regulatory components of our networks are linked via a set of common transcription factors that serve as targets in the signaling network and as regulators of the transcriptional response network . Detailed case studies of stress responses in budding yeast demonstrate the predictive power of our method . Our method correctly identifies the core signaling proteins and transcription factors of the response programs . It further predicts the involvement of additional transcription factors and other proteins not previously implicated in the response pathways . We experimentally verify several of these predictions for the osmotic stress response network . Our approach requires little condition-specific data : only a partial set of upstream initiators and time-series gene expression data , which are readily available for many conditions and species . Consequently , our method is widely applicable and can be used to derive accurate , dynamic response models in several species .
2K_dev_574	People spend an enormous amount of time searching for complex information online ; for example , consumers researching new purchases or patients learning about their conditions . As they search , people build up rich mental schemas about their target domains ; which , if effectively shared , could accelerate learning for others with similar interests . In this paper we introduce a novel approach for integrating the schemas individuals develop as they gather information online and surfacing them for others with similar interests . Through a controlled experiment we show that having access to others ' schemas while foraging for information helps new users to induce more useful , prototypical , and better-structured schemas than gathering information alone .
2K_dev_575	Multimedia event detection ( MED ) is a retrieval task with the goal of finding videos of a particular event in a large scale internet video archive , given example videos and text descriptions . Nowadays , different multimodal fusion schemes of low-level and high-level features are extensively investigated and evaluated for MED . For most of events in MED , people are usually the central subjects in videos . The face of a person can be considered as the most important factor which brings a lot of information describing the video events . However , face information has not been systematically investigated in the previous research for MED . In this paper , we investigate the possibility of using the high-level face information to assist multimedia event detection . Moreover , since the labeled data in TRECVID MED dataset are limited , we propose a semi-supervised kernel ridge regression which works well in practice to explore the useful information from unlabeled data to assist the event detection . Extensive experimental results on TRECVID MED dataset show that our proposed method outperforms the state-of-the-art methods by up to 4 % .
2K_dev_576	Current research is interested in identifying how topology impacts epidemics in networks . In this paper , we model SIS ( susceptible-infected-susceptible ) epidemics as a continuous-time Markov process and for which we can obtain a closed form description of the equilibrium distribution . Such distribution describes the long-run behavior of the epidemics . The adjacency matrix of the network topology is reflected explicitly in the formulation of the equilibrium distribution . Secondly , we are interested in analyzing the model in the regime where the topology dependent infection process opposes the topology independent healing process . Specifically , how will network topology affect the most probable long-run network state ? We show that for k-regular graph topologies , the most probable network state transitions from the state where everyone is healthy to one where everyone is infected at a threshold that depends on k but not on the size of the graph .
2K_dev_577	Provision of training data sets is one of the core requirements for event-based supervised NILM ( Non-Intrusive Load Monitoring ) algorithms . Due to diversity in appliances ' technologies , in-situ training by users is often required . This process might require continuous user-interaction to ensure that a high quality training data set is provided . Pre-populating a training data set could potentially reduce the need for user-system interaction . In this study , a heuristic unsupervised clustering algorithm is presented and evaluated to enable autonomous partitioning of appliances signature space ( i.e . feature space ) for applications in electricity consumption disaggregation . The algorithm is based on hierarchical clustering and uses the characteristics of a cluster binary tree to determine the distance threshold for pruning the tree without a priori information . The algorithm determines the partition of a feature space recursively to account for multi-scale nature of the binary cluster tree . Evaluation of the algorithm was carried out using metrics for accuracy and cluster quality ( proposed in this study ) on a fully labeled data set that was collected and processed in a real residential setting . The algorithm performance in accurate partitioning of the feature space and the effect of different feature extraction techniques were presented and discussed .
2K_dev_578	Given a table where rows correspond to records and columns correspond to attributes , we want to find a small number of patterns that succinctly summarize the dataset . For example , given a set of patient records with several attributes each , how can we find ( a ) that the `` most representative '' pattern is , say , ( male , adult , * ) , followed by ( * , child , low-cholesterol ) , etc ? We propose TSum , a method that provides a sequence of patterns ordered by their `` representativeness . '' It can decide both which these patterns are , as well as how many are necessary to properly summarize the data . Our main contribution is formulating a general framework , TSum , using compression principles . TSum can easily accommodate different optimization strategies for selecting and refining patterns . The discovered patterns can be used to both represent the data efficiently , as well as interpret it quickly . Extensive experiments demonstrate the effectiveness and intuitiveness of our discovered patterns .
2K_dev_579	Matched field processing is a powerful tool for accurately localizing targets in dispersive media . However , matched field processing requires a precise model of the medium under test . In underwater acoustics , where matched field processing has been extensively studied , authors often resort to extremely detailed numerical models of the propagation medium , which are computationally expensive and impractical for many applications . As an alternative , this paper uses convex sparse recovery techniques to construct , directly from measured data , an accurate model of a plate medium based on its dispersion characteristics . From this data-driven model , the Green 's function between two points can be readily predicted . We demonstrate the effectiveness of this model by localizing a source in a dispersive plate medium . The results visually illustrate our approach to significantly improve localization accuracy and reduce artifacts when compared to a conventional narrowband technique .
2K_dev_580	Restricted Boltzmann Machine ( RBM ) has shown great effectiveness in document modeling . It utilizes hidden units to discover the latent topics and can learn compact semantic representations for documents which greatly facilitate document retrieval , clustering and classification . The popularity ( or frequency ) of topics in text corpora usually follow a power-law distribution where a few dominant topics occur very frequently while most topics ( in the long-tail region ) have low probabilities . Due to this imbalance , RBM tends to learn multiple redundant hidden units to best represent dominant topics and ignore those in the long-tail region , which renders the learned representations to be redundant and non-informative . To solve this problem , we propose Diversified RBM ( DRBM ) which diversifies the hidden units , to make them cover not only the dominant topics , but also those in the long-tail region . We define a diversity metric and use it as a regularizer to encourage the hidden units to be diverse . Since the diversity metric is hard to optimize directly , we instead optimize its lower bound and prove that maximizing the lower bound with projected gradient ascent can increase this diversity metric . Experiments on document retrieval and clustering demonstrate that with diversification , the document modeling power of DRBM can be greatly improved .
2K_dev_581	Anomaly detection is an important problem with multiple applications , and thus has been studied for decades in various research domains . In the past decade there has been a growing interest in anomaly detection in data represented as networks , or graphs , largely because of their robust expressiveness and their natural ability to represent complex relationships . Originally , techniques focused on anomaly detection in static graphs , which do not change and are capable of representing only a single snapshot of data . As real-world networks are constantly changing , there has been a shift in focus to dynamic graphs , which evolve over time . In this survey , we aim to provide a comprehensive overview of anomaly detection in dynamic networks , concentrating on the state-of-the-art methods . We first describe four types of anomalies that arise in dynamic networks , providing an intuitive explanation , applications , and a concrete example for each . Having established an idea for what constitutes an anomaly , a general two-stage approach to anomaly detection in dynamic networks that is common among the methods is presented . We then construct a two-tiered taxonomy , first partitioning the methods based on the intuition behind their approach , and subsequently subdividing them based on the types of anomalies they detect . Within each of the tier one categoriescommunity , compression , decomposition , distance , and probabilistic model basedwe highlight the major similarities and differences , showing the wealth of techniques derived from similar conceptual approaches . 2015 The Authors .
2K_dev_582	Despite benefits and uses of social networking sites ( SNSs ) users are not always satisfied with their behaviors on the sites . These desires for behavior change both provide insight into users ' perceptions of how SNSs impact their lives ( positively or negatively ) and can inform tools for helping users achieve desired behavior changes . We use a 604-participant online survey to explore SNS users ' behavior-change goals for Facebook , Instagram , and Twitter . While some participants want to reduce site use , others want to improve their use or increase a range of behaviors . These desired changes differ by SNS , and , for Twitter , by participants ' levels of site use . Participants also expect a range of benefits from these goals , including increased time , contact with others , intrinsic benefits , better security/privacy , and improved self presentation . Based on these results we provide insights both into how participants perceive different SNSs , as well as potential designs for behavior-change mechanisms to target SNS behaviors .
2K_dev_583	Non-Intrusive Load Monitoring ( NILM ) has been studied for a few decades now as a method of disaggregating information about appliance level power consumption in a building from aggregate measurements of voltage and/or current obtained at a centralized location in the electrical system . When such information is provided to the electricity consumer as feedback , they can then take the necessary steps to modify their behavior and conserve electricity . Research has shown potential for savings of up to 20 % through this kind of feedback . The training phase required to allow the algorithms to recognize appliances in the home at the beginning of a NILM setup is a big hindrance to wide adoption of the technique . One of the recent advances in this research area includes the addition of an Electro-Magnetic Field ( EMF ) sensor that measures the electric and magnetic field nearby an appliance to detect its operational state . This information , when coupled with the aggregate power consumption data for the home , can help to train a NILM system , which is a significant step forward in automating the training phase . This paper explores the theory behind the operation of the EMF sensor and discusses the feasibility of automating the training and classification process using these devices . A case study is presented , where magnetic field measurements of eight appliances are analyzed to determine the viability of using these signals alone to determine the type of appliance that the EMF sensor has been placed next to . Various dimensionality reduction techniques are applied to the collected data , and the resulting feature vectors are used to train a variety of common machine learning classifiers . A vector subspace obtained using Independent Component Analysis ( ICA ) , along with a k-NN classifier , was found to perform best among the different alternatives explored . Possible reasons behind the findings are discussed and areas for further exploration are proposed .
2K_dev_584	High-data-rate sensors , such as video cameras , are becoming ubiquitous in the Internet of Things . This article describes GigaSight , an Internet-scale repository of crowd-sourced video content , with strong enforcement of privacy preferences and access controls . The GigaSight architecture is a federated system of VM-based cloudlets that perform video analytics at the edge of the Internet , thus reducing the demand for ingress bandwidth into the cloud . Denaturing , which is an owner-specific reduction in fidelity of video content to preserve privacy , is one form of analytics on cloudlets . Content-based indexing for search is another form of cloudlet-based analytics . This article is part of a special issue on smart spaces .
2K_dev_585	Summary form only given . Multimedia is being increasingly used in embedded real-time systems not just for supporting friendly user interfaces but also for vision processing . Real-time compression , decompression , logging and synchronization of several multimedia streams are increasingly common . This talk will present operating system abstractions and analytical models that provide sound foundations for manipulating multimedia streams . A compelling application will then be used to discuss how these abstractions could be applied in real-life situations .
2K_dev_586	Many vision tasks require a multi-class classifier to discriminate multiple categories , on the order of hundreds or thousands . In this paper , we propose sparse output coding , a principled way for large-scale multi-class classification , by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem . Specifically , sparse output coding is composed of two steps : efficient coding matrix learning with scalability to thousands of classes , and probabilistic decoding . Empirical results on object recognition and scene classification demonstrate the effectiveness of our proposed approach .
2K_dev_587	Computers have the potential to significantly extend the practice of popular music based on steady tempo and mostly determined form . There are significant challenges to overcome , however , due to constraints including accurate timing based on beats and adherence to a form or structure despite possible changes that may occur , possibly even during performance . We describe an approach to synchronization across media that takes into account latency due to communication delays and audio buffering . We also address the problem of mapping from a conventional score with repeats and other structures to an actual performance , which can involve both flattening the score and rearranging it , as is common in popular music . Finally , we illustrate the possibilities of the score as a bidirectional user interface in a real-time system for music performance , allowing the user to direct the computer through a digitally displayed score , and allowing the computer to indicate score position back to human performers .
2K_dev_588	Noisy , high-dimensional time series observations can often be described by a set of low-dimensional latent variables . Commonly used methods to extract these latent variables typically assume instantaneous relationships between the latent and observed variables . In many physical systems , changes in the latent variables manifest as changes in the observed variables after time delays . Techniques that do not account for these delays can recover a larger number of latent variables than are present in the system , thereby making the latent representation more difficult to interpret . In this work , we introduce a novel probabilistic technique , time-delay gaussian-process factor analysis TD-GPFA , that performs dimensionality reduction in the presence of a different time delay between each pair of latent and observed variables . We demonstrate how using a gaussian process to model the evolution of each latent variable allows us to tractably learn these delays over a continuous domain . Additionally , we show how TD-GPFA combines temporal smoothing and dimensionality reduction into a common probabilistic framework . We present an expectation/conditional maximization either ECME algorithm to learn the model parameters . Our simulations demonstrate that when time delays are present , TD-GPFA is able to correctly identify these delays and recover the latent space . We then applied TD-GPFA to the activity of tens of neurons recorded simultaneously in the macaque motor cortex during a reaching task . TD-GPFA is able to better describe the neural activity using a more parsimonious latent space than GPFA , a method that has been used to interpret motor cortex data but does not account for time delays . More broadly , TD-GPFA can help to unravel the mechanisms underlying high-dimensional time series data by taking into account physical delays in the system .
2K_dev_589	There has been significant interest and progress recently in algorithms that solve regression problems involving tall and thin matrices in input sparsity time . Given a n * d matrix where n g d , these algorithms find an approximation with fewer rows , allowing one to solve a poly ( d ) sized problem instead . In practice , the best performances are often obtained by invoking these routines in an iterative fashion . We show these iterative methods can be adapted to give theoretical guarantees comparable to and better than the current state of the art . Our approaches are based on computing the importances of the rows , known as leverage scores , in an iterative manner . We show that alternating between computing a short matrix estimate and finding more accurate approximate leverage scores leads to a series of geometrically smaller instances . This gives an algorithm whose runtime is input sparsity plus an overhead comparable to the cost of solving a regression problem on the smaller approximation . Our results build upon the close connection between randomized matrix algorithms , iterative methods , and graph sparsification .
2K_dev_590	Abstract Genes are often combinatorially regulated by multiple transcription factors ( TFs ) . Such combinatorial regulation plays an important role in development and facilitates the ability of cells to respond to different stresses . While a number of approaches have utilized sequence and ChIP-based datasets to study combinational regulation , these have often ignored the combinational logic and the dynamics associated with such regulation . Here we present cDREM , a new method for reconstructing dynamic models of combinatorial regulation . cDREM integrates time series gene expression data with ( static ) protein interaction data . The method is based on a hidden Markov model and utilizes the sparse group Lasso to identify small subsets of combinatorially active TFs , their time of activation , and the logical function they implement . We tested cDREM on yeast and human data sets . Using yeast we show that the predicted combinatorial sets agree with other high throughput genomic datasets and improve upon prior methods ...
2K_dev_591	This technical note studies the impact of side initial state information on the detectability of data deception attacks against cyber-physical systems . We assume the attack detector has access to a linear function of the initial system state that can not be altered by an attacker . First , we provide a necessary and sufficient condition for an attack to be undetectable by any dynamic attack detector under each specific side information pattern . Second , we characterize attacks that can be sustained for arbitrarily long periods without being detected . Third , we define the zero state inducing attack , the only type of attack that remains dynamically undetectable regardless of the side initial state information available to the attack detector . Finally , we design a dynamic attack detector that detects detectable attacks .
2K_dev_592	Rapid advances in the life sciences and in related information technologies necessitate the ongoing refinement of bioinformatics educational programs in order to maintain their relevance . As the discipline of bioinformatics and computational biology expands and matures , it is important to characterize the elements that contribute to the success of professionals in this field . These individuals work in a wide variety of settings , including bioinformatics core facilities , biological and medical research laboratories , software development organizations , pharmaceutical and instrument development companies , and institutions that provide education , service , and training . In response to this need , the Curriculum Task Force of the International Society for Computational Biology ( ISCB ) Education Committee seeks to define curricular guidelines for those who train and educate bioinformaticians . The previous report of the task force summarized a survey that was conducted to gather input regarding the skill set needed by bioinformaticians [ 1 ] . The current article details a subsequent effort , wherein the task force broadened its perspectives by examining bioinformatics career opportunities , surveying directors of bioinformatics core facilities , and reviewing bioinformatics education programs . The bioinformatics literature provides valuable perspectives on bioinformatics education by defining skill sets needed by bioinformaticians , presenting approaches for providing informatics training to biologists , and discussing the roles of bioinformatics core facilities in training and education . The skill sets required for success in the field of bioinformatics are considered by several authors : Altman [ 2 ] defines five broad areas of competency and lists key technologies ; Ranganathan [ 3 ] presents highlights from the Workshops on Education in Bioinformatics , discussing challenges and possible solutions ; Yale 's interdepartmental PhD program in computational biology and bioinformatics is described in [ 4 ] , which lists the general areas of knowledge of bioinformatics ; in a related article , a graduate of Yale 's PhD program reflects on the skills needed by a bioinformatician [ 5 ] ; Altman and Klein [ 6 ] describe the Stanford Biomedical Informatics ( BMI ) Training Program , presenting observed trends among BMI students ; the American Medical Informatics Association defines competencies in the related field of biomedical informatics in [ 7 ] ; and the approaches used in several German universities to implement bioinformatics education are described in [ 8 ] . Several approaches to providing bioinformatics training for biologists are described in the literature . Tan et al . [ 9 ] report on workshops conducted to identify a minimum skill set for biologists to be able to address the informatics challenges of the -omics era . They define a requisite skill set by analyzing responses to questions about the knowledge , skills , and abilities that biologists should possess . The authors in [ 10 ] present examples of strategies and methods for incorporating bioinformatics content into undergraduate life sciences curricula . Pevzner and Shamir [ 11 ] propose that undergraduate biology curricula should contain an additional course , Algorithmic , Mathematical , and Statistical Concepts in Biology . Wingren and Botstein [ 12 ] present a graduate course in quantitative biology that is based on original , pathbreaking papers in diverse areas of biology . Johnson and Friedman [ 13 ] evaluate the effectiveness of incorporating biological informatics into a clinical informatics program . The results reported are based on interviews of four students and informal assessments of bioinformatics faculty . The challenges and opportunities relevant to training and education in the context of bioinformatics core facilities are discussed by Lewitter et al . [ 14 ] . Relatedly , Lewitter and Rebhan [ 15 ] provide guidance regarding the role of a bioinformatics core facility in hiring biologists and in furthering their education in bioinformatics . Richter and Sexton [ 16 ] describe a need for highly trained bioinformaticians in core facilities and provide a list of requisite skills . Similarly , Kallioniemi et al . [ 17 ] highlight the roles of bioinformatics core units in education and training . This manuscript expands the body of knowledge pertaining to bioinformatics curriculum guidelines by presenting the results from a broad set of surveys ( of core facility directors , of career opportunities , and of existing curricula ) . Although there is some overlap in the findings of the surveys , they are reported separately , in order to avoid masking the unique aspects of each of the perspectives and to demonstrate that the same themes arise , even when different perspectives are considered . The authors derive from their surveys an initial set of core competencies and relate the competencies to three different categories of professions that have a need for bioinformatics training .
2K_dev_593	Several researchers proposed using non-Euclidean metrics on point sets in Euclidean space for clustering noisy data . Almost always , a distance function is desired that recognizes the closeness of the points in the same cluster , even if the Euclidean cluster diameter is large . There- fore , it is preferred to assign smaller costs to the paths that stay close to the input points . In this paper , we consider a natural metric with this property , which we call the nearest neighbor metric . Given a point set P and a path , t his metric is the integral of the distance to P along .W e describe a ( 3 +e ) - approximation algorithm and a more intricate ( 1 + e ) -approximation algorithm to compute the nearest neighbor metric . Both approximation algorithms work in near-linear time . The former uses shortest paths on a sparse graph defined over the input points . The latter uses a sparse sample of the ambient space , to find good approximate geodesic paths .
2K_dev_594	We present an algorithm that on input of an $ n $ -vertex $ m $ -edge weighted graph $ G $ and a value $ k $ produces an incremental sparsifier $ \hat { G } $ with $ n-1 + m/k $ edges , such that the relative condition number of $ G $ with $ \hat { G } $ is bounded above by $ \tilde { O } ( k\log^2 n ) $ , with probability $ 1-p $ ( we use the $ \tilde { O } ( ) $ notation to hide a factor of at most $ ( \log\log n ) ^4 $ ) . The algorithm runs in time $ \tilde { O } ( ( m \log { n } + n\log^2 { n } ) \log ( 1/p ) ) . $ As a result , we obtain an algorithm that on input of an $ n\times n $ symmetric diagonally dominant matrix $ A $ with $ m $ nonzero entries and a vector $ b $ computes a vector $ { x } $ satisfying $ || { x } -A^ { + } b||_A < \epsilon ||A^ { + } b||_A $ , in expected time $ \tilde { O } ( m\log^2 { n } \log ( 1/\epsilon ) ) . $ The solver is based on repeated applications of the incremental sparsifier that produces a chain of graphs which is then used as input to the recursive preconditioned Chebyshev iteration .
2K_dev_595	Linear subspace learning methods such as Fisher 's Linear Discriminant Analysis ( LDA ) , Unsupervised Discriminant Projection ( UDP ) , and Locality Preserving Projections ( LPP ) have been widely used in face recognition applications as a tool to capture low dimensional discriminant information . However , when these methods are applied in the context of face recognition , they often encounter the small-sample-size problem . In order to overcome this problem , a separate Principal Component Analysis ( PCA ) step is usually adopted to reduce the dimensionality of the data . However , such a step may discard dimensions that contain important discriminative information that can aid classification performance . In this work , we propose a new idea which we named Multi-class Fukunaga Koontz Discriminant Analysis ( FKDA ) by incorporating the Fukunaga Koontz Transform within the optimization for maximizing class separation criteria in LDA , UDP , and LPP . In contrast to traditional LDA , UDP , and LPP , our approach can work with very high dimensional data as input , without requiring a separate dimensionality reduction step to make the scatter matrices full rank . In addition , the FKDA formulation seeks optimal projection direction vectors that are orthogonal which the existing methods can not guarantee , and it has the capability of finding the exact solutions to the `` trace ratio '' objective in discriminant analysis problems while traditional methods can only deal with a relaxed and inexact `` ratio trace '' objective . We have shown using six face database , in the context of large scale unconstrained face recognition , face recognition with occlusions , and illumination invariant face recognition , under `` closed set '' , `` semi-open set '' , and `` open set '' recognition scenarios , that our proposed FKDA significantly outperforms traditional linear discriminant subspace learning methods as well as five other competing algorithms . HighlightsSolve small-sample-size problem in LDA , UDP , LPP using FKT formulation.Can work with high dimensional data without inverting any scatter matrices.Finds optimal projection direction vectors that are orthogonal.Finds exact solutions to the objective in the form of trace ratio.Improvement in unconstrained face recognition scenarios .
2K_dev_596	Online consumers are uncertain about subjective product quality e.g. , fit and feel of clothing and texture of materials because of the absence of experiential information . In this paper , we examine the dynamic change of the products purchased online over time in the presence of this type of uncertainty . Using individual-level transaction data , we find that consumers purchase products with a high degree of product uncertainty as their online shopping experiences help them better estimate product quality . Our results also show that the average and highest prices of market baskets decrease around 1 % when online shopping experience increases 10 % . This implies that online consumers are reluctant to buy expensive products with only digitally transferred information , whereas they tend to purchase more of the cheaper products online along with their accumulated online shopping experience . We also verify the interaction effects of product uncertainty and product price on online consumers ' purchase decision . When online consumers buy products priced under $ 50 , they readily buy products with a high degree of product uncertainty regardless of their online shopping experience . But consumers are unlikely to buy expensive products online if there is a high degree of product uncertainty , even when they have accumulated much online shopping experience . In addition , we find that online vendors can effectively overcome product-level uncertainty by taking advantage of retailer reputation in the physical world and through the use of digitized video commercials . Our study on the dynamics in the set of products purchased online expands the understanding of consumer purchase behavior under uncertainty . This paper was accepted by Lorin Hitt , information systems .
2K_dev_597	Systems and methods for performing hybrid symbolic execution to detect exploitable bugs in binary code are described . In some example embodiments , the systems and methods determine that resources associated with an execution client performing symbolic execution of a target program are below , at , or above a threshold performance level , generate checkpoints for active executing paths of the online symbolic execution , and cause the execution client to perform symbolic execution in response to the determination that the resources are at or above the threshold performance level .
2K_dev_598	We propose a scalable Internet system for continuous collection of crowd-sourced video from devices such as Google Glass . Our hybrid cloud architecture , GigaSight , is effectively a Content Delivery Network ( CDN ) in reverse . It achieves scalability by decentralizing the collection infrastructure using cloudlets based on virtual machines~ ( VMs ) . Based on time , location , and content , privacy sensitive information is automatically removed from the video . This process , which we refer to as denaturing , is executed in a user-specific VM on the cloudlet . Users can perform content-based searches on the total catalog of denatured videos . Our experiments reveal the bottlenecks for video upload , denaturing , indexing , and content-based search . They also provide insight on how parameters such as frame rate and resolution impact scalability .
2K_dev_599	Self-paced learning ( SPL ) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training . Existing methods are limited in that they ignore an important aspect in learning : diversity . To incorporate this information , we propose an approach called self-paced learning with diversity ( SPLD ) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective , and thus can be easily generalized into various learning tasks . Albeit non-convex , the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time . We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets . Specifically , SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets .
2K_dev_600	Given a directed graph of millions of nodes , how can we automatically spot anomalous , suspicious nodes , judging only from their connectivity patterns ? Suspicious graph patterns show up in many applications , from Twitter users who buy fake followers , manipulating the social network , to botnet members performing distributed denial of service attacks , disturbing the network traffic graph . We propose a fast and effective method , CatchSync , which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior pattern , because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority . We introduce novel measures to quantify both concepts ( `` synchronicity '' and `` normality '' ) and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots . Thanks to careful design , CatchSync has the following desirable properties : ( a ) it is scalable to large datasets , being linear on the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology , without needing labeled data , nor timing information , etc. , while still capable of using side information , if available . We applied CatchSync on two large , real datasets 1-billion-edge Twitter social graph and 3-billion-edge Tencent Weibo social graph , and several synthetic ones ; CatchSync consistently outperforms existing competitors , both in detection accuracy by 36 % on Twitter and 20 % on Tencent Weibo , as well as in speed .
2K_dev_601	Regularization has played a key role in deriving sensible estimators in high dimensional statistical inference . A substantial amount of recent works has argued for nonconvex regularizers in favor of their superior theoretical properties and excellent practical performances . In a dierent but analogous vein , nonconvex loss functions are promoted because of their robustness against \outliers '' . However , these nonconvex formulations are computationally more challenging , especially in the presence of nonsmoothness and nonseparability . To address this issue , we propose a new proximal gradient meta-algorithm by rigorously extending the proximal average to the nonconvex setting . We formally prove its nice convergence properties , and illustrate its eectiveness on two applications : multi-task graph-guided fused lasso and robust support vector machines . Experiments demonstrate that our method compares favorably against other alternatives .
2K_dev_602	We investigate the power of voting among diverse , randomized software agents . With teams of computer Go agents in mind , we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning . This model allows us to reason about a collection of agents with different biases ( determined by the first-stage noise models ) , which , furthermore , apply randomized algorithms to evaluate alternatives and produce votes ( captured by the second-stage noise models ) . We analytically demonstrate that a uniform team , consisting of multiple instances of any single agent , must make a significant number of mistakes , whereas a diverse team converges to perfection as the number of agents grows . Our experiments , which pit teams of computer Go agents against strong agents , provide evidence for the effectiveness of voting when agents are diverse .
2K_dev_603	Navigation models are explicit representations of geometrical and topological information of physical environments that can be utilized for map-matching of indoor positioning data . This research paper presents algorithms for automated generation of three different types of navigation models , namely , centerline-based network , metric-based and grid-based navigation models , for map-matching of indoor positioning data . The abovementioned navigation models have been generated in an automated fashion from Industry Foundation Classes ( IFC ) -based building information models ( BIM ) . Specifically , we have 1 ) built on and targeted addressing limitations of existing algorithms that generate centerline-based network navigation models for polygonal shapes , 2 ) developed an approach to extract 2D geometry and topology from IFC-based BIM for creating metric-based navigation models , and 3 ) modified an existing algorithm to generate grid-based navigation models using geometry and topology extracted from BIM . The abovementioned three types of navigation models have been generated for six different testbeds with varying shape , size and density of spaces . We have validated the generality of the developed algorithms by evaluating the accuracy of geometrical and topological information contained within the three types of navigation models generated from testbeds with varying spatial characteristics .
2K_dev_604	Economics and behavioral economics offer different but complementary approaches to understanding privacy and security . This article explains briefly their differences and similarities , and why they matter in our thinking about security and privacy .
2K_dev_605	The use of fluorescence microscopy has undergone a major revolution over the past twenty years , both with the development of dramatic new technologies and with the widespread adoption of image analysis and machine learning methods . Many open source software tools provide the ability to use these methods in a wide range of studies , and many molecular and cellular phenotypes can now be automatically distinguished . This article presents the next major challenge in microscopy automation , the creation of accurate models of cell organization directly from images , and reviews the progress that has been made towards this challenge .
2K_dev_606	Motivation : Synaptic connections underlie learning and memory in the brain and are dynamically formed and eliminated during development and in response to stimuli . Quantifying changes in overall density and strength of synapses is an important pre-requisite for studying connectivity and plasticity in these cases or in diseased conditions . Unfortunately , most techniques to detect such changes are either low-throughput ( e.g . electrophysiology ) , prone to error and difficult to automate ( e.g . standard electron microscopy ) or too coarse ( e.g . magnetic resonance imaging ) to provide accurate and large-scale measurements . Results : To facilitate high-throughput analyses , we used a 50-year-old experimental technique to selectively stain for synapses in electron microscopy images , and we developed a machine-learning framework to automatically detect synapses in these images . To validate our method , we experimentally imaged brain tissue of the somatosensory cortex in six mice . We detected thousands of synapses in these images and demonstrate the accuracy of our approach using crossvalidation with manually labeled data and by comparing against existing algorithms and against tools that process standard electron microscopy images . We also used a semi-supervised algorithm that leverages unlabeled data to overcome sample heterogeneity and improve performance . Our algorithms are highly efficient and scalable and are freely available for others to use . Availability : Code is available at http : //www.cs.cmu.edu/ saketn/detect_synapses/
2K_dev_607	We present a scheme for fast , distributed learning on big ( i.e . high-dimensional ) models applied to big datasets . Unlike algorithms that focus on distributed learning in either the big data or big model setting ( but not both ) , our scheme partitions both the data and model variables simultaneously . This not only leads to faster learning on distributed clusters , but also enables machine learning applications where both data and model are too large to fit within the memory of a single machine . Furthermore , our scheme allows worker machines to perform additional updates while waiting for slow workers to finish , which provides users with a tunable synchronization strategy that can be set based on learning needs and cluster conditions . We prove the correctness of such strategies , as well as provide bounds on the variance of the model variables under our scheme . Finally , we present empirical results for latent space models such as topic models , which demonstrate that our method scales well with large data and model sizes , while beating learning strategies that fail to take both data and model partitioning into account .
2K_dev_608	The integration of synthetic and cell-free biology has made tremendous strides towards creating artificial cellular nanosystems using concepts from solution-based chemistry : only the concentrations of reacting species modulate gene expression rates . However , it is known that macromolecular crowding , a key feature of natural cells , can dramatically influence biochemical kinetics by volume exclusion effects that reduce diffusion rates and enhance binding rates of macromolecules . Here , we demonstrate that macromolecular crowding can increase the robustness of gene expression through integrating synthetic cellular components of biological circuits and artificial cellular nanosystems . In addition , we reveal how ubiquitous cellular modules , including genetic components , a negative feedback loop , and the size of crowding molecules , can fine tune gene circuit response to molecular crowding . By bridging a key gap between artificial and living cells , our work has implications for efficient and robust control of both synthetic and natural cellular circuits .
2K_dev_609	Motivation : Phylogenetic algorithms have begun to see widespread use in cancer research to reconstruct processes of evolution in tumor progression . Developing reliable phylogenies for tumor data requires quantitative models of cancer evolution that include the unusual genetic mechanisms by which tumors evolve , such as chromosome abnormalities , and allow for heterogeneity between tumor types and individual patients . Previous work on inferring phylogenies of single tumors by copy number evolution assumed models of uniform rates of genomic gain and loss across different genomic sites and scales , a substantial oversimplification necessitated by a lack of algorithms and quantitative parameters for fitting to more realistic tumor evolution models . Results : We propose a framework for inferring models of tumor progression from single-cell gene copy number data , including variable rates for different gain and loss events . We propose a new algorithm for identification of most parsimonious combinations of single gene and single chromosome events . We extend it via dynamic programming to include genome duplications . We implement an expectation maximization ( EM ) -like method to estimate mutation-specific and tumor-specific event rates concurrently with tree reconstruction . Application of our algorithms to real cervical cancer data identifies key genomic events in disease progression consistent with prior literature . Classification experiments on cervical and tongue cancer datasets lead to improved prediction accuracy for the metastasis of primary cervical cancers and for tongue cancer survival . Availability and implementation : Our software ( FISHtrees ) and two datasets are available at ftp : //ftp.ncbi.nlm.nih.gov/pub/FISHtrees . Contact : ude.umc.werdna @ sllessur Supplementary information : Supplementary data are available at Bioinformatics online .
2K_dev_610	Studying temporal dynamics of topics in social media is very useful to understand online user behaviors . Most of the existing work on this subject usually monitors the global trends , ignoring variation among communities . Since users from different communities tend to have varying tastes and interests , capturing communitylevel temporal change can improve the understanding and management of social content . Additionally , it can further facilitate the applications such as community discovery , temporal prediction and online marketing . However , this kind of extraction becomes challenging due to the intricate interactions between community and topic , and intractable computational complexity . In this paper , we take a unified solution towards the communitylevel topic dynamic extraction . A probabilistic model , CosTot ( Community Specific Topics-over-Time ) is proposed to uncover the hidden topics and communities , as well as capture community-specific temporal dynamics . Specifically , CosTot considers text , time , and network information simultaneously , and well discovers the interactions between community and topic over time . We then discuss the approximate inference implementation to enable scalable computation of model parameters , especially for large social data . Based on this , the application layer support for multi-scale temporal analysis and community exploration is also investigated . We conduct extensive experimental studies on a large real microblog dataset , and demonstrate the superiority of proposed model on tasks of time stamp prediction , link prediction and topic perplexity .
2K_dev_611	The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values . The least absolute shrinkage and selection operator ( Lasso ) allows computationally efficient feature selection based on linear dependency between input features and output values . In this paper , we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency . We first show that , with particular choices of kernel functions , non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures such as the Hilbert-Schmidt independence criterion ( HSIC ) . We then show that the globally optimal solution can be efficiently computed ; this makes the approach scalable to high-dimensional problems . The effectiveness of the proposed method is demonstrated through feature selection experiments for classification and regression with thousands of features .
2K_dev_612	As our society is increasingly aging , it is urgent to develop computer aided techniques to improve the quality-of-care ( QoC ) and quality-of-life ( QoL ) of geriatric patients . In this paper , we focus on automatic human activities analysis in video surveillance recorded in complicated environments at a nursing home . This will enable the automatic exploration of the statistical patterns between patients ' daily activities and their clinical diagnosis . We also discuss potential future research directions in this area . Experiment demonstrate the proposed approach is effective for human activity analysis .
2K_dev_613	We present an autonomous driving research vehicle with minimal appearance modifications that is capable of a wide range of autonomous and intelligent behaviors , including smooth and comfortable trajectory generation and following ; lane keeping and lane changing ; intersection handling with or without V2I and V2V ; and pedestrian , bicyclist , and workzone detection . Safety and reliability features include a fault-tolerant computing system ; smooth and intuitive autonomous-manual switching ; and the ability to fully disengage and power down the drive-by-wire and computing system upon E-stop . The vehicle has been tested extensively on both a closed test field and public roads .
2K_dev_614	Game-theoretic algorithms for physical security have made an impressive real-world impact . These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game , where the attacker observes the defender 's strategy and best-responds . In order to build the game model , though , the payoffs of potential attackers for various outcomes must be estimated ; inaccurate estimates can lead to significant inefficiencies . We design an algorithm that optimizes the defender 's strategy with no prior information , by observing the attacker 's responses to randomized deployments of resources and learning his priorities . In contrast to previous work , our algorithm requires a number of queries that is polynomial in the representation of the game .
2K_dev_615	The paper explains how to use sensors as the eyes , ears , hands , and feet for the cloud . This paper describes the opportunities and challenges when integrating sensors and cloud computing .
2K_dev_616	Tablet computers are often called upon to emulate classical pen-and-paper input . However , touchscreens typically lack the means to distinguish between legitimate stylus and finger touches and touches with the palm or other parts of the hand . This forces users to rest their palms elsewhere or hover above the screen , resulting in ergonomic and usability problems . We present a probabilistic touch filtering approach that uses the temporal evolution of touch contacts to reject palms . Our system improves upon previous approaches , reducing accidental palm inputs to 0.016 per pen stroke , while correctly passing 98 % of stylus inputs .
2K_dev_617	Most algorithmic matches in fielded kidney exchanges do not result in an actual transplant . In this paper , we address the problem of cycles and chains in a proposed match failing after the matching algorithm has committed to them . We show that failure-aware kidney exchange can significantly increase the expected number of lives saved ( i ) in theory , on random graph models ; ( ii ) on real data from kidney exchange match runs between 2010 and 2012 ; ( iii ) on synthetic data generated via a model of dynamic kidney exchange . From the computational viewpoint , we design a branch-and-price-based optimal clearing algorithm specifically for the probabilistic exchange clearing problem and show that this new solver scales well on large simulated data , unlike prior clearing algorithms .
2K_dev_618	Cloud offload is an important technique in mobile computing . VM-based cloudlets have been proposed as offload sites for the resource-intensive and latency-sensitive computations typically associated with mobile multimedia applications . Since cloud offload relies on precisely-configured back-end software , it is difficult to support at global scale across cloudlets in multiple domains . To address this problem , we describe just-in-time ( JIT ) provisioning of cloudlets under the control of an associated mobile device . Using a suite of five representative mobile applications , we demonstrate a prototype system that is capable of provisioning a cloudlet with a non-trivial VM image in 10 seconds . This speed is achieved through dynamic VM synthesis and a series of optimizations to aggressively reduce transfer costs and startup latency .
2K_dev_619	The average person can skillfully manipulate a plethora of tools , from hammers to tweezers . However , despite this remarkable dexterity , gestures on today 's touch devices are simplistic , relying primarily on the chording of fingers : one -finger pan , two -finger pinch , four -finger swipe and similar . We propose that touch gesture design be inspired by the manipulation of physical tools from the real world . In this way , we can leverage user familiarity and fluency with such tools to build a rich set of gestures for touch interaction . With only a few minutes of training on a proof-of-concept system , users were able to summon a variety of virtual tools by replicating their corresponding real-world grasps .
2K_dev_620	Location sharing is a popular feature of online social networks , but challenges remain in the effective presentation of privacy choices to users , whose location sharing preferences are complex and diverse . One proposed approach for capturing these nuances builds on the observation that key attributes of users ' location sharing preferences can be represented by a small number of privacy profiles , which can provide a basis for configuring individual preferences . However , the impact of this approach on how users view their privacy is relatively unknown . We present a study evaluating the impact of this approach on users ' location sharing preferences and their satisfaction with the decisions made by their resulting settings . The results suggest that this approach can influence users to share significantly more without a substantial difference in comfort . This further suggests that the provision of profiles for privacy settings must be carefully considered , as they can substantially alter sharing behavior .
2K_dev_621	The proliferation of touchscreen devices has made soft keyboards a routine part of life . However , ultra-small computing platforms like the Sony SmartWatch and Apple iPod Nano lack a means of text entry . This limits their potential , despite the fact they are quite capable computers . In this work , we present a soft keyboard interaction technique called ZoomBoard that enables text entry on ultra-small devices . Our approach uses iterative zooming to enlarge otherwise impossibly tiny keys to comfortable size . We based our design on a QWERTY layout , so that it is immediately familiar to users and leverages existing skill . As the ultimate test , we ran a text entry experiment on a keyboard measuring just 16 x 6mm - smaller than a US penny . After eight practice trials , users achieved an average of 9.3 words per minute , with accuracy comparable to a full-sized physical keyboard . This compares favorably to existing mobile text input methods .
2K_dev_622	Meeting service level objectives ( SLOs ) for tail latency is an important and challenging open problem in cloud computing infrastructures . The challenges are exacerbated by burstiness in the workloads . This paper describes PriorityMeister -- a system that employs a combination of per-workload priorities and rate limits to provide tail latency QoS for shared networked storage , even with bursty workloads . PriorityMeister automatically and proactively configures workload priorities and rate limits across multiple stages ( e.g. , a shared storage stage followed by a shared network stage ) to meet end-to-end tail latency SLOs . In real system experiments and under production trace workloads , PriorityMeister outperforms most recent reactive request scheduling approaches , with more workloads satisfying latency SLOs at higher latency percentiles . PriorityMeister is also robust to mis-estimation of underlying storage device performance and contains the effect of misbehaving workloads .
2K_dev_623	With an explosion of popularity of online photo sharing , we can trivially collect a huge number of photo streams for any interesting topics such as scuba diving as an outdoor recreational activity class . Obviously , the retrieved photo streams are neither aligned nor calibrated since they are taken in different temporal , spatial , and personal perspectives . However , at the same time , they are likely to share common storylines that consist of sequences of events and activities frequently recurred within the topic . In this paper , as a first technical step to detect such collective storylines , we propose an approach to jointly aligning and segmenting uncalibrated multiple photo streams . The alignment task discovers the matched images between different photo streams , and the image segmentation task parses each image into multiple meaningful regions to facilitate the image understanding . We close a loop between the two tasks so that solving one task helps enhance the performance of the other in a mutually rewarding way . To this end , we design a scalable message-passing based optimization framework to jointly achieve both tasks for the whole input image set at once . With evaluation on the new Flickr dataset of 15 outdoor activities that consist of 1.5 millions of images of 13 thousands of photo streams , our empirical results show that the proposed algorithms are more successful than other candidate methods for both tasks .
2K_dev_624	We study the problem of computing a Nash equilibrium in large-scale two-player zero-sum extensive-form games . While this problem can be solved in polynomial time , first-order or regret-based methods are usually preferred for large games . Regret-based methods have largely been favored in practice , in spite of their theoretically inferior convergence rates . In this paper we investigate the acceleration of first-order methods both theoretically and experimentally . An important component of many first-order methods is a distance-generating function . Motivated by this , we investigate a specific distance-generating function , namely the dilated entropy function , over treeplexes , which are convex polytopes that encompass the strategy spaces of perfect-recall extensive-form games . We develop significantly stronger bounds on the associated strong convexity parameter . In terms of extensive-form game solving , this improves the convergence rate of several first-order methods by a factor of O ( ( # information sets depth M ) / ( 2 depth ) ) where M is the maximum value of the l 1 norm over the treeplex encoding the strategy spaces . Experimentally , we investigate the performance of three first-order methods ( the excessive gap technique , mirror prox , and stochastic mirror prox ) and compare their performance to the regret-based algorithms . In order to instantiate stochastic mirror prox , we develop a class of gradient sampling schemes for game trees . Equipped with our distance-generating function and sampling scheme , we find that mirror prox and the excessive gap technique outperform the prior regret-based methods for finding medium accuracy solutions
2K_dev_625	How does a new startup drive the popularity of competing websites into oblivion like Facebook famously did to MySpace ? This question is of great interest to academics , technologists , and financial investors alike . In this work we exploit the singular way in which Facebook wiped out the popularity of MySpace , Hi5 , Friendster , and Multiply to guide the design of a new popularity competition model . Our model provides new insights into what Nobel Laure- ate Herbert A. Simon called the `` marketplace of attention , '' which we recast as the attention-activity marketplace . Our model design is further substantiated by user-level activity of 250,000 MySpace users obtained between 2004 and 2009 . The resulting model not only accurately fits the observed Daily Active Users ( DAU ) of Facebook and its competitors but also predicts their fate four years into the future .
2K_dev_626	We present a new method for disease outbreak detection , the `` Non-Parametric Heterogeneous Graph Scan ( NPHGS ) '' . NPHGS enables fast and accurate detection of emerging space-time clusters using Twitter and other social media streams where standard parametric model assumptions are incorrect .
2K_dev_627	Distributed machine learning has typically been approached from a data parallel perspective , where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness . A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms , where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates . We argue that model and data parallelisms impose rather different challenges for system design , algorithmic adjustment , and theoretical analysis . In this paper , we develop a system for model-parallelism , STRADS , that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs . STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models , and improves memory efficiency of distributed ML . We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling , matrix factorization , and Lasso .
2K_dev_628	Computational cancer phylogenetics seeks to enumerate the temporal sequences of aberrations in tumor evolution , thereby delineating the evolution of possible tumor progression pathways , molecular subtypes , and mechanisms of action . We previously developed a pipeline for constructing phylogenies describing evolution between major recurring cell types computationally inferred from whole-genome tumor profiles . The accuracy and detail of the phylogenies , however , depend on the identification of accurate , high-resolution molecular markers of progression , i.e. , reproducible regions of aberration that robustly differentiate different subtypes and stages of progression . Here , we present a novel hidden Markov model ( HMM ) scheme for the problem of inferring such phylogenetically significant markers through joint segmentation and calling of multisample tumor data . Our method classifies sets of genome-wide DNA copy number measurements into a partitioning of samples into normal ( diploid ) or amplified at each probe . It differs from other similar HMM methods in its design specifically for the needs of tumor phylogenetics , by seeking to identify robust markers of progression conserved across a set of copy number profiles . We show an analysis of our method in comparison to other methods on both synthetic and real tumor data , which confirms its effectiveness for tumor phylogeny inference and suggests avenues for future advances .
2K_dev_629	Enormous databases of short-read RNA-seq sequencing experiments such as the NIH Sequence Read Archive ( SRA ) are now available . However , these collections remain difficult to use due to the inability to search for a particular expressed sequence . A natural question is which of these experiments contain sequences that indicate the expression of a particular sequence such as a gene isoform , lncRNA , or uORF . However , at present this is a computationally demanding question at the scale of these databases . We introduce an indexing scheme , the Sequence Bloom Tree ( SBT ) , to support sequence-based querying of terabase-scale collections of thousands of short-read sequencing experiments . We apply SBT to the problem of finding conditions under which query transcripts are expressed . Our experiments are conducted on a set of 2652 publicly available RNA-seq experiments contained in the NIH for the breast , blood , and brain tissues , comprising 5 terabytes of sequence . SBTs of this size can be queried for a 1000 nt sequence in 19 minutes using less than 300 MB of RAM , over 100 times faster than standard usage of SRA-BLAST and 119 times faster than STAR . SBTs allow for fast identification of experiments with expressed novel isoforms , even if these isoforms were unknown at the time the SBT was built . We also provide some theoretical guidance about appropriate parameter selection in SBT and propose a sampling-based scheme for potentially scaling SBT to even larger collections of files . While SBT can handle any set of reads , we demonstrate the effectiveness of SBT by searching a large
2K_dev_630	We present Air+Touch , a new class of interactions that interweave touch events with in-air gestures , offering a unified input modality with expressiveness greater than each input modality alone . We demonstrate how air and touch are highly complementary : touch is used to designate targets and segment in-air gestures , while in-air gestures add expressivity to touch events . For example , a user can draw a circle in the air and tap to trigger a context menu , do a finger 'high jump ' between two touches to select a region of text , or drag and in-air 'pigtail ' to copy text to the clipboard . Through an observational study , we devised a basic taxonomy of Air+Touch interactions , based on whether the in-air component occurs before , between or after touches . To illustrate the potential of our approach , we built four applications that showcase seven exemplar Air+Touch interactions we created .
2K_dev_631	Recent trends in System-on-a-Chip show that an increasing number of special-purpose processors are being added to improve the efficiency of common operations . Unfortunately , the use of these processors may introduce suspension delays incurred by communication , synchronization and external I/O operations . When these processors are used in real-time systems , conventional schedulability analyses incorporate these delays in the worst-case execution/response time , hence significantly reducing the schedulable utilization . In this paper , we provide schedulability analyses and propose segment-fixed priority scheduling for self-suspending tasks . We model the tasks as segments of execution separated by suspensions . We start from providing response-time analyses for self-suspending tasks under Rate Monotonic Scheduling ( RMS ) . While RMS is shown to not be optimal , it can be used effectively in some special cases that we have identified . We then derive a utilization bound for the cases as a function of the ratio of the suspension duration to the period of the tasks . For general cases , we develop a segment-fixed priority scheduling scheme . Our scheme assigns individual segments different priorities and phase offsets that are used for phase enforcement to control the unexpected self-suspending nature . With the exact schedulability analysis designed for our scheme , our experiments show that the proposed scheme provides up to 40 times more schedulable utilization than RMS .
2K_dev_632	Cloud-sourced virtual appliances ( VAs ) have been touted as powerful solutions for many software maintenance , mobility , backward compatibility , and security challenges . In this paper , we ask whether it is possible to create a VA cloud service that supports fluid , interactive user experience even over mobile networks . More specifically , we wish to support a YouTube-like streaming service for executable content , such as games , interactive books , research artifacts , etc . Users should be able to post , browse through , and interact with executable content swiftly and without long interruptions . Intuitively , this seems impossible ; the bandwidths , latencies , and costs of last-mile networks would be prohibitive given the sheer sizes of virtual machines ! Yet , we show that a set of carefully crafted , novel prefetching and streaming techniques can bring this goal surprisingly close to reality . We show that vTube , a VA streaming system that incorporates our techniques , supports fluid interaction even in challenging network conditions , such as 4G LTE .
2K_dev_633	This paper considers the sparse Gaussian conditional random field , a discriminative extension of sparse inverse covariance estimation , where we use convex methods to learn a high-dimensional conditional distribution of outputs given inputs . The model has been proposed by multiple researchers within the past year , yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems . In this paper , we make three contributions : 1 ) we develop a second-order active-set method which is several orders of magnitude faster than previously proposed optimization approaches for this problem , 2 ) we analyze the model from a theoretical standpoint , improving upon past bounds with convergence rates that depend logarithmically on the data dimension , and 3 ) we apply the method to large-scale energy forecasting problems , demonstrating state-of-the-art performance on two real-world tasks .
2K_dev_634	We study the phase transition of the coalitional manipulation problem for generalized scoring rules . Previously it has been shown that , under some conditions on the distribution of votes , if the number of manipulators is o ( n ) , where n is the number of voters , then the probability that a random profile is manipulable by the coalition goes to zero as the number of voters goes to infinity , whereas if the number of manipulators is ( n ) , then the probability that a random profile is manipulable goes to one . Here we consider the critical window , where a coalition has size cn , and we show that as c goes from zero to infinity , the limiting probability that a random profile is manipulable goes from zero to one in a smooth fashion , i.e. , there is a smooth phase transition between the two regimes . This result analytically validates recent empirical results , and suggests that deciding the coalitional manipulation problem may be of limited computational hardness in practice .
2K_dev_635	Real-time captioning provides people who are deaf or hard of hearing access to speech in settings such as classrooms and live events . The most reliable approach to provide these captions is to recruit an expert stenographer who is able to type at natural speaking rates , but they charge more than $ 100 USD per hour and must be scheduled in advance . We introduce Legion Scribe ( Scribe ) , a system that allows 3-5 ordinary people who can hear and type to jointly caption speech in real-time . Each person is unable to type at natural speaking rates , and so is asked only to type part of what they hear . Scribe automatically stitches all of the partial captions together to form a complete caption stream . We have shown that the accuracy of Scribe captions approaches that of a professional stenographer , while its latency and cost is dramatically lower .
2K_dev_636	Distributed online groups have great potential for generating interdependent and complex products like encyclopedia articles or product design . However , coordinating multiple group members to work together effectively while minimizing process losses remains an open challenge . We conducted an experiment comparing the effectiveness of two coordination strategies ( simultaneous vs. sequential work ) on a complex creative task as the number of group members increased . Our results indicate that , contrary to prior work , a sequential work structure was more effective than a simultaneous work structure as the size of the group increased . A mediation analysis suggests that social processes such as territoriality partially accounts for these results . A follow up experiment giving workers specific roles mitigated the detrimental effects of the simultaneous work structure . These results have implications for small group theory and crowdsourcing research .
2K_dev_637	In a multimillion-node network of who-follows-whom like Twitter , since a high count of followers leads to higher profits , users have the incentive to boost their in-degree . Can we spot the suspicious following behavior , which may indicate zombie followers and suspicious followees ? To answer the above question , we propose CatchSync , which exploits two tell-tale signs of the suspicious behavior : ( a ) synchronized behavior : the zombie followers have extremely similar following behavior pattern , because , say , they are generated by a script ; and ( b ) abnormal behavior : their behavior pattern is very different from the majority . Our CatchSync introduces novel measures to quantify both concepts and catches the suspicious behavior . Moreover , we show it is effective in a real-world social network .
2K_dev_638	How can one build a distributed framework that allows efficient deployment of a wide spectrum of modern advanced machine learning ( ML ) programs for industrial-scale problems using Big Models ( 100s of billions of parameters ) on Big Data ( terabytes or petabytes ) - Contemporary parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce , or even specialized operators relying on graphical representations of ML programs . The variety of approaches tends to pull systems and algorithms design in different directions , and it remains difficult to find a universal platform applicable to a wide range of different ML programs at scale . We propose a general-purpose framework that systematically addresses data- and model-parallel challenges in large-scale ML , by leveraging several fundamental properties underlying ML programs that make them different from conventional operation-centric programs : error tolerance , dynamic structure , and nonuniform convergence ; all stem from the optimization-centric nature shared in ML programs ' mathematical definitions , and the iterative-convergent behavior of their algorithmic solutions . These properties present unique opportunities for an integrative system design , built on bounded-latency network synchronization and dynamic load-balancing scheduling , which is efficient , programmable , and enjoys provable correctness guarantees . We demonstrate how such a design in light of ML-first principles leads to significant performance improvements versus well-known implementations of several ML programs , allowing them to run in much less time and at considerably larger model sizes , on modestly-sized computer clusters .
2K_dev_639	In this paper , we address the problem of discovering topically meaningful , yet compact ( densely connected ) communities in a social network . Assuming the social network to be an integer-weighted graph ( where the weights can be intuitively defined as the number of common friends , followers , documents exchanged , etc . ) , we transform the social network to a more efficient representation . In this new representation , each user is a bag of her one-hop neighbors . We propose a mixed-membership model to identify compact communities using this transformation . Next , we augment the representation and the model to incorporate user-content information imposing topical consistency in the communities . In our model a user can belong to multiple communities and a community can participate in multiple topics . This allows us to discover community memberships as well as community and user interests . Our method outperforms other well known baselines on two real-world social networks . Finally , we also provide a fast , parallel approximation of the same .
2K_dev_640	Recently , data with complex characteristics such as epilepsy electroencephalography ( EEG ) time series has emerged . Epilepsy EEG data has special characteristics including nonlinearity , nonnormality , and nonperiodicity . Therefore , it is important to find a suitable forecasting method that covers these special characteristics . In this paper , we propose a coercively adjusted autoregression ( CA-AR ) method that forecasts future values from a multivariable epilepsy EEG time series . We use the technique of random coefficients , which forcefully adjusts the coefficients with 1 and 1 . The fractal dimension is used to determine the order of the CA-AR model . We applied the CA-AR method reflecting special characteristics of data to forecast the future value of epilepsy EEG data . Experimental results show that when compared to previous methods , the proposed method can forecast faster and accurately .
2K_dev_641	With the continuous improvement in genotyping and molecular phenotyping technology and the decreasing typing cost , it is expected that in a few years , more and more clinical studies of complex diseases will recruit thousands of individuals for pan-omic genetic association analyses . Hence , there is a great need for algorithms and software tools that could scale up to the whole omic level , integrate different omic data , leverage rich structure information , and be easily accessible to non-technical users . We present GenAMap , an interactive analytics software platform that 1 ) automates the execution of principled machine learning methods that detect genome- and phenome-wide associations among genotypes , gene expression data , and clinical or other macroscopic traits , and 2 ) provides new visualization tools specifically designed to aid in the exploration of association mapping results . Algorithmically , GenAMap is based on a new paradigm for GWAS and PheWAS analysis , termed structured association mapping , which leverages various structures in the omic data . We demonstrate the function of GenAMap via a case study of the Brem and Kruglyak yeast dataset , and then apply it on a comprehensive eQTL analysis of the NIH heterogeneous stock mice dataset and report some interesting findings . GenAMap is available from http : //sailing.cs.cmu.edu/genamap .
2K_dev_642	Machine learning , particularly kernel methods , has been demonstrated as a promising new tool to tackle the challenges imposed by todays explosive data growth in genomics . They provide a practical and principled approach to learning how a large number of genetic variants are associated with complex phenotypes , to help reveal the complexity in the relationship between the genetic markers and the outcome of interest . In this review , we highlight the potential key role it will have in modern genomic data processing , especially with regard to integration with classical methods for gene prioritizing , prediction and data fusion .
2K_dev_643	Oral tongue squamous cell carcinoma ( OTSCC ) is associated with poor prognosis . To improve prognostication , we analyzed four gene probes ( TERC , CCND1 , EGFR and TP53 ) and the centromere probe CEP4 as a marker of chromosomal instability , using fluorescence in situ hybridization ( FISH ) in single cells from the tumors of sixty-five OTSCC patients ( Stage I , n=15 ; Stage II , n=30 ; Stage III , n=7 ; Stage IV , n=13 ) . Unsupervised hierarchical clustering of the FISH data distinguished three clusters related to smoking status . Copy number increases of all five markers were found to be correlated to non-smoking habits , while smokers in this cohort had low-level copy number gains . Using the phylogenetic modeling software FISHtrees , we constructed models of tumor progression for each patient based on the four gene probes . Then , we derived test statistics on the models that are significant predictors of disease-free and overall survival , independent of tumor stage and smoking status in multivariate analysis . The patients whose tumors were modeled as progressing by a more diverse distribution of copy number changes across the four genes have poorer prognosis . This is consistent with the view that multiple genetic pathways need to become deregulated in order for cancer to progress . Whats new ? Oral tongue squamous cell carcinoma ( OTSCC ) is a rare head and neck cancer that typically is asymptomatic in early stages . Hence , in order to improve prognosis in OTSCC , predictive biomarkers that are independent of tumor stage must be identified . Here , using four fluorescence in situ hybridization ( FISH ) gene probes and the software FISHtrees , phylogenetic tree models of tumor progression in OTSCC patients were constructed . Analyses of the models showed that the more diverse the changes within the four marker genes , the worse the outcome in OTSCC . The markers predicted survival independent of smoking behavior and tumor stage .
2K_dev_644	participants reassessing their permissions , and 58 % of them further restricting some of their permissions . We discuss how participants interacted both with the permission manager and the privacy nudges , analyze the effectiveness of both solutions , and derive some recommendations . Smartphone users are often unaware of the data collected by apps running on their devices . We report on a study that evaluates the benefits of giving users an app permission manager and sending them nudges intended to raise their awareness of the data collected by their apps . Our study provides both qualitative and quantitative evidence that these approaches are complementary and can each play a significant role in empowering users to more effectively control their privacy . For instance , even after a week with access to the permission manager , participants benefited from nudges showing them how often some of their sensitive data was being accessed by apps , with 95 % of
2K_dev_645	When a free , catchy application shows up , how quickly will people notify their friends about it ? Will the enthusiasm drop exponentially with time , or oscillate ? What other patterns emerge ? Here we answer these questions using data from the Polly telephone-based application , a large influence network of 72,000 people , with about 173,000 in- teractions , spanning 500MB of log data and 200 GB of audio data . We report surprising patterns , the most striking of which are : ( a ) the FIZZLE pattern , i.e. , excitement about Polly shows a power-law decay over time with ex- ponent of -1.2 ; ( b ) the RENDEZVOUS pattern , that obeys a power law ( we explain RENDEZVOUS in the text ) ; ( c ) the DISPERSION pattern , we find that the more a person uses Polly , the fewer friends he will use it with , but in a reciprocal fashion . Finally , we also propose a generator of influence networks , which generate networks that mimic our discovered patterns .
2K_dev_646	Inadequate information interoperability in facility management ( FM ) activities costs time and money being wasted for searching for the needed information in many different data sources . An integrated building information model ( BIM ) , depicting as-is conditions , has a high potential to minimize such wastes . However , facility managers still face the challenge of generating as-is building information models for existing facilities . A main problem with current approaches for generating as-is BIMs is that they mainly focus on capturing and providing geometric information . Many other types of information are missing , such as equipment warranty and technical parameters . The research described in this paper targets the generation of accurate and semantically-rich as-is BIMs by leveraging heterogeneous existing information sources , such as drawings and operation and maintenance manuals . This approach was investigated through detailed case studies done in two old academic buildings . Existing information obtained from documents has been compared to Industry Foundation Classes ( IFC ) , COBie and the data generated from a BIM authoring system . The comparative analysis results reveal several information gaps among different sources .
2K_dev_647	We approach the problem of estimating the parameters of a latent tree graphical model from a hierarchical tensor decomposition point of view . In this new view , the marginal probability table of the observed variables is treated as a tensor , and we show that : ( i ) the latent variables induce low rank structures in various matricizations of the tensor ; ( ii ) this collection of low rank matricizations induces a hierarchical low rank decomposition of the tensor . We further derive an optimization problem for estimating ( alternative ) parameters of a latent tree graphical model , allowing us to represent the marginal probability table of the observed variables in a compact and robust way . The optimization problem aims to find the best hierarchical low rank approximation of a tensor in Frobenius norm . For correctly specified latent tree graphical models , we show that a global optimum of the optimization problem can be obtained via a recursive decomposition algorithm . This algorithm recovers previous spectral algorithms for hidden Markov models ( Hsu et al. , 2009 ; Foster et al. , 2012 ) and latent tree graphical models ( Parikh et al. , 2011 ; Song et al. , 2011 ) as special cases , elucidating the global objective these algorithms are optimizing . For misspecified latent tree graphical models , we derive a novel decomposition based on our framework , and provide approximation guarantee and computational complexity analysis . In both synthetic and real world data , this new estimator significantly improves over the state-of-the-art .
2K_dev_648	Much work in optimal control and inverse control has assumed that the controller has perfect knowledge of plant dynamics . However , if the controller is a human or animal subject , the subject 's internal dynamics model may differ from the true plant dynamics . Here , we consider the problem of learning the subject 's internal model from demonstrations of control and knowledge of task goals . Due to sensory feedback delay , the subject uses an internal model to generate an internal prediction of the current plant state , which may differ from the actual plant state . We develop a probabilistic framework and exact EM algorithm to jointly estimate the internal model , internal state trajectories , and feedback delay . We applied this framework to demonstrations by a nonhuman primate of brain-machine interface ( BMI ) control . We discovered that the subject 's internal model deviated from the true BMI plant dynamics and provided significantly better explanation of the recorded neural control signals than did the true plant dynamics .
2K_dev_649	We describe the architecture and prototype implementation of an assistive system based on Google Glass devices for users in cognitive decline . It combines the first-person image capture and sensing capabilities of Glass with remote processing to perform real-time scene interpretation . The system architecture is multi-tiered . It offers tight end-to-end latency bounds on compute-intensive operations , while addressing concerns such as limited battery capacity and limited processing capability of wearable devices . The system gracefully degrades services in the face of network failures and unavailability of distant architectural tiers .
2K_dev_650	Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown , but inference in such models can be slow . Existing attempts to parallelize inference in such models have relied on introducing approximations , which can lead to inaccuracies in the posterior estimate . In this paper , we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to perform MCMC using the correct equilibrium distribution , in a distributed manner . We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods .
2K_dev_651	Nowadays , facility management ( FM ) teams are facing challenges to generate accurate and semantically-rich as-is BIMs for existing buildings . Current model creation approaches , such as model generation based on point cloud data , mainly capture geometric information of a building and lack to provide additional semantic information about components and other project information . This paper provides the results of a detailed case study that aimed at leveraging existing data sources ( e.g. , archived documents and data in FM systems ) to generate accurate and semanticallyrich as-is BIMs . The initial findings from the case study highlighted two main challenges associated with model generation from existing data sources : information extraction and integration . Existing information for different components is typically stored in heterogeneous data sources with various formats and quality , and hence requires different approaches to extract information . The findings also showed that almost 40 % of the component attributes investigated had conflicting values in existing sources . In order to address these challenges , formalized approaches are required to support conflict resolution , data extraction and integration .
2K_dev_652	Formal verification and validation play a crucial role in making cyber-physical systems ( CPS ) safe . Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained , including models of the controller and of the physical dynamics . In CPS , models are essential ; but any model we could possibly build necessarily deviates from the real world . If the real system fits to the model , its behavior is guaranteed to satisfy the correctness properties verified with respect to the model . Otherwise , all bets are off . This article introduces ModelPlex , a method ensuring that verification results about models apply to CPS implementations . ModelPlex provides correctness guarantees for CPS executions at runtime : it combines offline verification of CPS models with runtime validation of system executions for compliance with the model . ModelPlex ensures in a provably correct way that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model . If , at some point , the observed behavior no longer complies with the model so that offline verification results no longer apply , ModelPlex initiates provably safe fallback actions , assuming the system dynamics deviation is bounded . This article , furthermore , develops a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic by a correct-by-construction approach , leading to verifiably correct runtime model validation . Overall , ModelPlex generates provably correct monitor conditions that , if checked to hold at runtime , are provably guaranteed to imply that the offline safety verification results about the CPS model apply to the present run of the actual CPS implementation .
2K_dev_653	This paper describes the experiences and lessons learned by a team of Civil and Environmental Engineering graduate students during a capstone project-based course , which orients students to acquire knowledge on sensing technologies , data management , systems engineering , and visualization concepts in order to design and implement an energy monitoring system in a three room lab space . The team , mentored by faculty members , was responsible for the deployment of hardware , establishing communications , database design and implementation , and developing visualizations to communicate the relevant information based on the requirements of a client role-played by a faculty member . Lessons learned from this project include the importance of applying a systems engineering approach during the iterative scope definition and design processes , and the use of alternative communications , learning , and problem-solving methods in order to tackle challenges of larger scope and complexity than presented in classroom-setting coursework , while working on a team
2K_dev_654	Demand response has gained significant attention in recent years as it demonstrates potentials to enhance the power system 's operational flexibility in a cost-effective way . Industrial loads such as steel manufacturing plants consume large amounts of electric energy , and their electricity bills account for a remarkable percentage of their total operation cost . Meanwhile , lots of industrial loads are very flexible in terms of adjusting their power consumption rate , e.g . through switching the transformer tap position . Hence , industrial loads such as the steel plants have both the motivation and the ability to support power system operation through demand response . In this paper , we focus on the steel plant and optimize its scheduling to maximize its profits from both the energy and the spinning reserve markets .
2K_dev_655	Black-box mutational fuzzing is a simple yet effective technique to find bugs in software . Given a set of program-seed pairs , we ask how to schedule the fuzzings of these pairs in order to maximize the number of unique bugs found at any point in time . We develop an analytic framework using a mathematical model of black-box mutational fuzzing and use it to evaluate 26 existing and new randomized online scheduling algorithms . Our experiments show that one of our new scheduling algorithms outperforms the multi-armed bandit algorithm in the current version of the CERT Basic Fuzzing Framework ( BFF ) by finding 1.5x more unique bugs in the same amount of time .
2K_dev_656	Much research on human action recognition has been oriented toward the performance gain on lab-collected datasets . Yet real-world videos are more diverse , with more complicated actions and often only a few of them are precisely labeled . Thus , recognizing actions from these videos is a tough mission . The paucity of labeled real-world videos motivates us to `` borrow '' strength from other resources . Specifically , considering that many lab datasets are available , we propose to harness lab datasets to facilitate the action recognition in real-world videos given that the lab and real-world datasets are related . As their action categories are usually inconsistent , we design a multi-task learning framework to jointly optimize the classifiers for both sides . The general Schatten $ $ p $ $ p -norm is exerted on the two classifiers to explore the shared knowledge between them . In this way , our framework is able to mine the shared knowledge between two datasets even if the two have different action categories , which is a major virtue of our method . The shared knowledge is further used to improve the action recognition in the real-world videos . Extensive experiments are performed on real-world datasets with promising results .
2K_dev_657	Visual inspection of civil infrastructure is essential for condition assessment.We focus on concrete bridges , tunnels , underground pipes , and asphalt pavements.Accordingly , we review the latest computer vision based defect detection methods.Using computer vision most relevant types of defects can be automatically detected.Automatic defect properties retrieval and assessment has not been achieved yet . To ensure the safety and the serviceability of civil infrastructure it is essential to visually inspect and assess its physical and functional condition . This review paper presents the current state of practice of assessing the visual condition of vertical and horizontal civil infrastructure ; in particular of reinforced concrete bridges , precast concrete tunnels , underground concrete pipes , and asphalt pavements . Since the rate of creation and deployment of computer vision methods for civil engineering applications has been exponentially increasing , the main part of the paper presents a comprehensive synthesis of the state of the art in computer vision based defect detection and condition assessment related to concrete and asphalt civil infrastructure . Finally , the current achievements and limitations of existing methods as well as open research challenges are outlined to assist both the civil engineering and the computer science research community in setting an agenda for future research .
2K_dev_658	We study distributed optimization problems when N nodes minimize the sum of their individual costs subject to a common vector variable . The costs are convex , have Lipschitz continuous gradient ( with constant L ) , and bounded gradient . We propose two fast distributed gradient algorithms based on the centralized Nesterov gradient algorithm and establish their convergence rates in terms of the per-node communications K and the per-node gradient evaluations k. Our first method , Distributed Nesterov Gradient , achieves rates O ( logK/K ) and O ( logk/k ) . Our second method , Distributed Nesterov gradient with Consensus iterations , assumes at all nodes knowledge of L and ( W ) - the second largest singular value of the N N doubly stochastic weight matrix W. It achieves rates O ( 1/ K 2- ) and O ( 1/k 2 ) ( > 0 arbitrarily small ) . Further , we give for both methods explicit dependence of the convergence constants on N and W. Simulation examples illustrate our findings .
2K_dev_659	In spite of many favorable characteristics of guided-waves for Nondestructive Evaluation ( NDE ) of pipes , real-world application of these systems is still quite limited . Beside the complexities derived from multi-modal , dispersive and multi-path characteristics of guided-waves , one of the main challenges in guided-wave based NDE of pipelines is sensitivity of these systems to variations of environmental and operational conditions ( EOC ) . This paper investigates the effects of varying EOCs on guilded-wave based NDE of pipelines . We first provide a review of the studies to date in the field of guided-wave based testing to identify research gaps for enhancing the application of these systems in pipeline NDE . To study the identified gaps , guided-wave data from a fully operational piping system , with continuously varying flow rate and temperature , is used . Time-shift and amplitude drift effects due to flow rate variations are evaluated along with those of temperature . It is observed that masking effects of flow rate for damage detection can be at least as significant as temperature effects , and that such effects become more dominant when flow rate and temperature variations co-occur .
2K_dev_660	Given a graph with billions of nodes and edges , how can we find patterns and anomalies ? Are there nodes that participate in too many or too few triangles ? Are there close-knit near-cliques ? These questions are expensive to answer unless we have the first several eigenvalues and eigenvectors of the graph adjacency matrix . However , eigensolvers suffer from subtle problems ( e.g. , convergence ) for large sparse matrices , let alone for billion-scale ones . We address this problem with the proposed HEIGEN algorithm , which we carefully design to be accurate , efficient , and able to run on the highly scalable MAPREDUCE ( HADOOP ) environment . This enables HEIGEN to handle matrices more than 1 ; 000 larger than those which can be analyzed by existing algorithms . We implement HEIGEN and run it on the M45 cluster , one of the top 50 supercomputers in the world . We report important discoveries about nearcliques and triangles on several real-world graphs , including a snapshot of the Twitter social network ( 56 Gb , 2 billion edges ) and the YahooWeb data set , one of the largest publicly available graphs ( 120 Gb , 1.4 billion nodes , 6.6 billion edges ) .
2K_dev_661	In this paper we present a local interpolation-based variant of the well-known polar format algorithm used for synthetic aperture radar ( SAR ) image formation . We develop the algorithm to match the capabilities of the application-specific logic-in-memory processing paradigm , which off-loads lightweight computation directly into the SRAM and DRAM . Our proposed algorithm performs filtering , an image perspective transformation , and a local 2D interpolation , and supports partial and low-resolution reconstruction . We implement our customized SAR grid interpolation logic-in-memory hardware in advanced 14 nm silicon technology . Our high-level design tools allow to instantiate various optimized design choices to fit image processing and hardware needs of application designers . Our simulation results show that the logic-in-memory approach has the potential to enable substantial improvements in energy efficiency without sacrificing image quality .
2K_dev_662	Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets . However , the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time : We expect both the structure of our hierarchy , and the parameters of the clusters , to evolve with time . In this paper , we present a distribution over collections of time-dependent , infinite-dimensional trees that can be used to model evolving hierarchies , and present an efficient and scalable algorithm for performing approximate inference in such a model . We demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora .
2K_dev_663	In modern crowdsourcing markets , requesters face the challenge of training and managing large transient workforces . Requesters can hire peer workers to review others ' work , but the value may be marginal , especially if the reviewers lack requisite knowledge . Our research explores if and how workers learn and improve their performance in a task domain by serving as peer reviewers . Further , we investigate whether peer reviewing may be more effective in teams where the reviewers can reach consensus through discussion . An online between-subjects experiment compares the trade-offs of reviewing versus producing work using three different organization strategies : working individually , working as an interactive team , and aggregating individuals into nominal groups . The results show that workers who review others ' work perform better on subsequent tasks than workers who just produce . We also find that interactive reviewer teams outperform individual reviewers on all quality measures . However , aggregating individual reviewers into nominal groups produces better quality assessments than interactive teams , except in task domains where discussion helps overcome individual misconceptions .
2K_dev_664	Understanding where electricity is being used in buildings is an important tool for Cyber-Physical Systems ( CPS ) used in building energy conservation and efficiency . Current approaches for appliance-level energy metering typically require the installation of plug-through power meters , which is often difficult and costly for devices with inaccessible wires or outlets , or appliances that draw large amounts of current . In this paper , we present an energy measurement system that estimates the energy consumption of individual appliances using a wireless sensor network consisting of contactless electromagnetic field ( EMF ) sensors deployed near each appliance , and a whole-house power meter . We present the design of a battery-operated EMF sensor , which can detect appliance state transitions within close proximity based on magnetic and electric field fluctuations . Each detector wirelessly transmits state change events to a circuit-panel energy meter , in a time-synchronized fashion , so that the overall power measurements can be used to estimate appliance-level energy usage . Our EMF sensors are able to detect significant power state changes from a few inches away , thus making it possible to externally monitor in-wall wiring to devices . We experimentally evaluate our proposed EMF sensor , three-phase power meter and communication protocol in a residential building collecting data for over a week . The system is able to detect appliance state transitions with an accuracy of 95.8 % and estimate the overall energy with an accuracy of 98.1 % .
2K_dev_665	Methods for the analysis of chromatin immunoprecipitation sequencing ( ChIP-seq ) data start by aligning the short reads to a reference genome . While often successful , they are not appropriate for cases where a reference genome is not available . Here we develop methods for de novo analysis of ChIP-seq data . Our methods combine de novo assembly with statistical tests enabling motif discovery without the use of a reference genome . We validate the performance of our method using human and mouse data . Analysis of fly data indicates that our method outperforms alignment based methods that utilize closely related species .
2K_dev_666	Consumers who are close to one another in a social network often make similar purchase decisions . This similarity can result from latent homophily or social influence , as well as common exogenous factors . Latent homophily means consumers who are connected to one another are likely to have similar characteristics and product preferences . Social influence refers to the ability of one consumer to directly influence another consumer 's decision based upon their communication . We present an empirical study of purchases of caller ring-back tones using data from an Asian mobile network that predicts consumers ' purchase timing and choice decisions . We simultaneously measure latent homophily and social influence , while also accounting for exogenous factors . Identification is achieved due to our dynamic , panel data structure and the availability of detailed communication data . We find strong influence effects and latent homophily effects in both the purchase timing and product choice decisions of consumers . This paper was accepted by Sandra Slaughter , information systems .
2K_dev_667	How can web services that depend on user generated content discern fraudulent input by spammers from legitimate input ? In this paper we focus on the social network Facebook and the problem of discerning ill-gotten Page Likes , made by spammers hoping to turn a profit , from legitimate Page Likes . Our method , which we refer to as CopyCatch , detects lockstep Page Like patterns on Facebook by analyzing only the social graph between users and Pages and the times at which the edges in the graph ( the Likes ) were created . We offer the following contributions : ( 1 ) We give a novel problem formulation , with a simple concrete definition of suspicious behavior in terms of graph structure and edge constraints . ( 2 ) We offer two algorithms to find such suspicious lockstep behavior - one provably-convergent iterative algorithm and one approximate , scalable MapReduce implementation . ( 3 ) We show that our method severely limits `` greedy attacks '' and analyze the bounds from the application of the Zarankiewicz problem to our setting . Finally , we demonstrate and discuss the effectiveness of CopyCatch at Facebook and on synthetic data , as well as potential extensions to anomaly detection problems in other domains . CopyCatch is actively in use at Facebook , searching for attacks on Facebook 's social graph of over a billion users , many millions of Pages , and billions of Page Likes .
2K_dev_668	Iris masks play an important role in iris recognition . They indicate which part of the iris texture map is useful and which part is occluded or contaminated by noisy image artifacts such as eyelashes , eyelids , eyeglasses frames , and specular reflections . The accuracy of the iris mask is extremely important . The performance of the iris recognition system will decrease dramatically when the iris mask is inaccurate , even when the best recognition algorithm is used . Traditionally , people used the rule-based algorithms to estimate iris masks from iris images . However , the accuracy of the iris masks generated this way is questionable . In this work , we propose to use Figueiredo and Jain 's Gaussian Mixture Models ( FJ-GMMs ) to model the underlying probabilistic distributions of both valid and invalid regions on iris images . We also explored possible features and found that Gabor Filter Bank ( GFB ) provides the most discriminative information for our goal . Finally , we applied Simulated Annealing ( SA ) technique to optimize the parameters of GFB in order to achieve the best recognition rate . Experimental results show that the masks generated by the proposed algorithm increase the iris recognition rate on both ICE2 and UBIRIS dataset , verifying the effectiveness and importance of our proposed method for iris occlusion estimation .
2K_dev_669	Given a simple noun such as { \em apple } , and a question such as `` is it edible ? `` , what processes take place in the human brain ? More specifically , given the stimulus , what are the interactions between ( groups of ) neurons ( also known as functional connectivity ) and how can we automatically infer those interactions , given measurements of the brain activity ? Furthermore , how does this connectivity differ across different human subjects ? In this work we present a simple , novel good-enough brain model , or GeBM in short , and a novel algorithm Sparse-SysId , which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity . Moreover , GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text ) . We evaluate GeBM by using both synthetic and real brain data . Using the real data , GeBM produces brain activity patterns that are strikingly similar to the real ones , and the inferred functional connectivity is able to provide neuroscientific insights towards a better understanding of the way that neurons interact with each other , as well as detect regularities and outliers in multi-subject brain activity measurements .
2K_dev_670	Given a network with attributed edges , how can we identify anomalous behavior ? Networks with edge attributes are ubiquitous , and capture rich information about interactions between nodes . In this paper , we aim to utilize exactly this information to discern suspicious from typical behavior in an unsupervised fashion , lending well to the traditional scarcity of ground-truth labels in practical anomaly detection scenarios . Our work has a number of notable contributions , including ( a ) formulation : while most other graph-based anomaly detection works use structural graph connectivity or node information , we focus on the new problem of leveraging edge information , ( b ) methodology : we introduce EdgeCentric , an intuitive and scalable compression-based approach for detecting edge-attributed graph anomalies , and ( c ) practicality : we show that EdgeCentric successfully spots numerous such anomalies in several large , edge-attributed real-world graphs , including the Flipkart e-commerce graph with over 3 million product reviews between 1.1 million users and 545 thousand products , where it achieved 0.87 precision over the top 100 results .
2K_dev_671	We consider the problem of recovering a symmetric , positive semidefinite ( SPSD ) matrix from a subset of its entries , possibly corrupted by noise . In contrast to previous matrix recovery work , we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix . We develop a set of sufficient conditions for the recovery of a SPSD matrix from a set of its principal submatrices , present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met . The proposed algorithm is naturally generalized to the problem of noisy matrix recovery , and we provide a worst-case bound on reconstruction error for this scenario . Finally , we demonstrate the algorithm 's utility on noiseless and noisy simulated datasets .
2K_dev_672	This paper explores a PAC ( probably approximately correct ) learning model in cooperative games . Specifically , we are given m random samples of coalitions and their values , taken from some unknown cooperative game ; can we predict the values of unseen coalitions ? We study the PAC learnability of several well-known classes of cooperative games , such as network flow games , threshold task games , and induced subgraph games . We also establish a novel connection between PAC learnability and core stability : for games that are efficiently learnable , it is possible to find payoff divisions that are likely to be stable using a polynomial number of samples .
2K_dev_673	In this paper , we study an approach for discovering brand associations by leveraging large-scale online photo collections contributed by the general public . Brand Associations , one of central concepts in marketing , describe customers ' top-of-mind attitudes or feelings toward a brand . ( e.g . what comes to mind when you think of Burberry ? ) Traditionally , brand associations are measured by analyzing the text data from consumers ' responses to the survey or their online conversation logs . In this paper , we go beyond textual media and take advantage of large-scale photos shared on the Web . More specifically , we jointly achieve the following two fundamental tasks in a mutually-rewarding way : ( i ) detecting exemplar images as key visual concepts associated with brands , and ( ii ) localizing the regions of brand in images . For experiments we collect about five millions of images of 48 brands crawled from five popular online photo sharing sites . We then demonstrate that our approach can discover complementary views on the brand associations that are hardly obtained from text data . We also quantitatively show the superior performance of our algorithm for the two tasks over other candidate methods .
2K_dev_674	Abstract : Security-sensitive applications that execute untrusted code often check the codes integrity by comparing its syntax to a known good value or sandbox the code to contain its effects . System M is a new program logic for reasoning about such security-sensitive applications . System M extends Hoare Type Theory ( HTT ) to trace safety properties and , additionally , contains two new reasoning principles . First , its type system internalizes logical equality , facilitating reasoning about applications that check code integrity . Second , a confinement rule assigns an effect type to a computation based solely on knowledge of the computations sandbox . We prove the sound-ness of System M relative to a step-indexed trace-based semantic model . We illustrate both new reasoning principles of System M by verifying the main integrity property of the design of Memoir , a previously proposed trusted computing system for ensuring state continuity of isolated security-sensitive applications .
2K_dev_675	There has been recent interest in applying Stackelberg games to infrastructure security , in which a defender must protect targets from attack by an adaptive adversary . In real-world security settings the adversaries are humans and are thus boundedly rational . Most existing approaches for computing defender strategies against boundedly rational adversaries try to optimize against specific behavioral models of adversaries , and provide no quality guarantee when the estimated model is inaccurate . We propose a new solution concept , monotonic maximin , which provides guarantees against all adversary behavior models satisfying monotonicity , including all in the family of Regular Quantal Response functions . We propose a mixed-integer linear program formulation for computing monotonic maximin . We also consider top-monotonic maximin , a related solution concept that is more conservative , and propose a polynomial-time algorithm for top-monotonic maximin .
2K_dev_676	The development of accurate clinical biomarkers has been challenging in part due to the diversity between patients and diseases . One approach to account for the diversity is to use multiple markers to classify patients , based on the concept that each individual marker contributes information from its respective subclass of patients . Here we present a new strategy for developing biomarker panels that accounts for completely distinct patient subclasses . Marker State Space ( MSS ) defines marker states based on all possible patterns of high and low values among a panel of markers . Each marker state is defined as either a case state or a control state , and a sample is classified as case or control based on the state it occupies . MSS was used to define multi-marker panels that were robust in cross validation and training-set/test-set analyses and that yielded similar classification accuracy to several other classification algorithms . A three-marker panel for discriminating pancreatic cancer patients from control subjects revealed subclasses of patients based on distinct marker states . MSS provides a straightforward approach for modeling highly divergent subclasses of patients , which may be adaptable for diverse applications .
2K_dev_677	Background Serum albumin is a major pharmacokinetic effector of drugs . To gain further insight into albumin binding chemistry , the crystal structures of six oncology agents were determined in complex with human serum albumin at resolutions of 2.8 to 2.0 A : camptothecin , 9-amino-camptothecin , etoposide , teniposide , bicalutamide and idarubicin .
2K_dev_678	Multimedia data are usually represented by multiple features . In this paper , we propose a new algorithm , namely Multi-feature Learning via Hierarchical Regression for multimedia semantics understanding , where two issues are considered . First , labeling large amount of training data is labor-intensive . It is meaningful to effectively leverage unlabeled data to facilitate multimedia semantics understanding . Second , given that multimedia data can be represented by multiple features , it is advantageous to develop an algorithm which combines evidence obtained from different features to infer reliable multimedia semantic concept classifiers . We design a hierarchical regression model to exploit the information derived from each type of feature , which is then collaboratively fused to obtain a multimedia semantic concept classifier . Both label information and data distribution of different features representing multimedia data are considered . The algorithm can be applied to a wide range of multimedia applications and experiments are conducted on video data for video concept annotation and action recognition . Using Trecvid and CareMedia video datasets , the experimental results show that it is beneficial to combine multiple features . The performance of the proposed algorithm is remarkable when only a small amount of labeled training data are available .
2K_dev_679	Matrix-parametrized models , including multiclass logistic regression and sparse coding , are used in machine learning ( ML ) applications ranging from computer vision to computational biology . When these models are applied to large-scale ML problems starting at millions of samples and tens of thousands of classes , their parameter matrix can grow at an unexpected rate , resulting in high parameter synchronization costs that greatly slow down distributed learning . To address this issue , we propose a Sufficient Factor Broadcasting ( SFB ) computation model for efficient distributed learning of a large family of matrix-parameterized models , which share the following property : the parameter update computed on each data sample is a rank-1 matrix , i.e. , the outer product of two `` sufficient factors '' ( SFs ) . By broadcasting the SFs among worker machines and reconstructing the update matrices locally at each worker , SFB improves communication efficiency -- - communication costs are linear in the parameter matrix 's dimensions , rather than quadratic -- - without affecting computational correctness . We present a theoretical convergence analysis of SFB , and empirically corroborate its efficiency on four different matrix-parametrized ML models .
2K_dev_680	BACKGROUND The purpose of this study was to identify genome-wide single nucleotide variants and mutations in African American patients with colorectal cancer ( CRC ) . There is a need of such studies in African Americans , because they display a higher incidence of aggressive CRC tumors . METHODS We performed whole exome sequencing ( WES ) on DNA from 12 normal/tumor pairs of African American CRC patient tissues . Data analysis was performed using the software package GATK ( Genome Analysis Tool Kit ) . Normative population databases ( eg , 1000 Genomes SNP database , dbSNP , and HapMap ) were used for comparison . Variants were annotated using analysis of variance and were validated via Sanger sequencing . RESULTS We identified somatic mutations in genes that are known targets in CRC such as APC , BRAF , KRAS , and PIK3CA . We detected novel alterations in the Wnt pathway gene , APC , within its exon 15 , of which mutations are highly associated with CRC . CONCLUSIONS This WES study in African American patients with CRC provides insight into the identification of novel somatic mutations in APC . Our data suggest an association between specific mutations in the Wnt signaling pathway and an increased risk of CRC . The analysis of the pathogenicity of these novel variants may shed light on the aggressive nature of CRC in African Americans . Cancer 2015 ; 121:3442 . 2014 American Cancer Society .
2K_dev_681	Numeric time series data has unique storage requirements and access patterns that can benefit from specialized support , given its importance in Big Data analyses . Popular frameworks and databases focus on addressing other needs , making them a suboptimal fit . This paper describes the support needed for numeric time series , suggests an architecture for efficient time series storage , and illustrates its potential for satisfying key requirements .
2K_dev_682	We present a new algorithm that produces a well-spaced superset of points conforming to a given input set in any dimension with guaranteed optimal output size . We also provide an approximate Delaunay graph on the output points . Our algorithm runs in expected time O ( 2 O ( d ) ( n log n + m ) ) , where n is the input size , m is the output point set size , and d is the ambient dimension . The constants only depend on the desired element quality bounds . To gain this new efficiency , the algorithm approximately maintains the Voronoi diagram of the current set of points by storing a superset of the Delaunay neighbors of each point . By retaining quality of the Voronoi diagram and avoiding the storage of the full Voronoi diagram , a simple exponential dependence on d is obtained in the running time . Thus , if one only wants the approximate neighbors structure of a refined Delaunay mesh conforming to a set of input points , the algorithm will return a size 2 O ( d ) m graph in 2 O ( d ) ( n log n + m ) expected time . If m is superlinear in n , then we can produce a hierarchically well-spaced superset of size 2 O ( d ) n in 2 O ( d ) n log n expected time .
2K_dev_683	Distributions over matrices with exchangeable rows and infinitely many columns are useful in constructing nonparametric latent variable models . However , the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks . In this paper , we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models . Such models allow us to specify the distribution over the number of features per data point , and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution .
2K_dev_684	Stochastic Differential Equation SDE models are used to describe the dynamics of complex systems with inherent randomness . The primary purpose of these models is to study rare but interesting or important behaviours , such as the formation of a tumour . Stochastic simulations are the most common means for estimating or bounding the probability of rare behaviours , but the cost of simulations increases with the rarity of events . To address this problem , we introduce a new algorithm specifically designed to quantify the likelihood of rare behaviours in SDE models . Our approach relies on temporal logics for specifying rare behaviours of interest , and on the ability of bit-vector decision procedures to reason exhaustively about fixed-precision arithmetic . We apply our algorithm to a minimal parameterised model of the cell cycle , and take Brownian noise into account while investigating the likelihood of irregularities in cell size and time between cell divisions .
2K_dev_685	We propose a scalable approach for making inference about latent spaces of large networks . With a succinct representation of networks as a bag of triangular motifs , a parsimonious statistical model , and an efficient stochastic variational inference algorithm , we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours , a setting that is out of reach for many existing methods . When compared to the state-of-the-art probabilistic approaches , our method is several orders of magnitude faster , with competitive or improved accuracy for latent space recovery and link prediction .
2K_dev_686	As renewable generation capacity in the power grid increases , keeping the balance between the supply and demand becomes difficult . This threatens the grids stability and security . Existing power reserve assets and regulation methodologies fail to provide the short-term responses required to keep the load and generation balanced as the amount of renewable generation increases . Hence , researchers proposed to increase the information exchange within the power network and to introduce realtime demand control to ensure robustness while accommodating the intermittent nature of these generation resources . Constituting a significant portion of the electrical demand of buildings , thermostatically controlled loads ( TCLs ) are wellsuited to provide real-time demand control . In this paper , we shed light on challenges associated with engaging TCLs to the power grid using a centralized control strategy . We focus on the challenges associated with simulating a realistic TCL population using the models that are proposed in the literature . Specifically , we use data collected from residential refrigeration units operating in 214 different households to propose a strategy to select parameters when simulating a TCL population .
2K_dev_687	Fusion of multiple features can boost the performance of large-scale visual classification and detection tasks like TRECVID Multimedia Event Detection ( MED ) competition [ 1 ] . In this paper , we propose a novel feature fusion approach , namely Feature Weighting via Optimal Thresholding ( FWOT ) to effectively fuse various features . FWOT learns the weights , thresholding and smoothing parameters in a joint framework to combine the decision values obtained from all the individual features and the early fusion . To the best of our knowledge , this is the first work to consider the weight and threshold factors of fusion problem simultaneously . Compared to state-of-the-art fusion algorithms , our approach achieves promising improvements on HMDB [ 8 ] action recognition dataset and CCV [ 5 ] video classification dataset . In addition , experiments on two TRECVID MED 2011 collections show that our approach outperforms the state-of-the-art fusion methods for complex event detection .
2K_dev_688	Background Aging infrastructure in the US has gained quite a bit of attention in the past decade . Being one type of a critical infrastructure , embankment dams in the US require significant investment to upgrade the deteriorated parts . Due to limited budgets , understanding the behavior of structures over time through risk assessment is essential to prioritize dams . During the risk assessment for embankment dams , engineers utilize current and historical data from the design , construction , and operation phases of these structures . The challenge is that during risk assessment , various engineers from different disciplines ( e.g. , geotechnical , hydraulics ) come together , and how they would like to visualize the available datasets changes based on the discipline-specific analyses they need to perform . The objective of this research study is to understand the discipline-specific visualization needs of engineers from US Army Corps of Engineers ( USACE ) who are involved in risk assessment of embankment dams when they deal with large set of data accumulated since the inception of dams .
2K_dev_689	Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously , as well as the identification of the types of neurons being recorded ( e.g. , excitatory vs. inhibitory ) . There is a growing need for statistical methods to study the interaction among multiple , labeled populations of neurons . Rather than attempting to identify direct interactions between neurons ( where the number of interactions grows with the number of neurons squared ) , we propose to extract a smaller number of latent variables from each population and study how these latent variables interact . Specifically , we propose extensions to probabilistic canonical correlation analysis ( pCCA ) to capture the temporal structure of the latent variables , as well as to distinguish within-population dynamics from across-population interactions ( termed Group Latent Auto-Regressive Analysis , gLARA ) . We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2 , and found that gLARA provides a better description of the recordings than pCCA . This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function .
2K_dev_690	We introduce and test the hypothesis that control over publication of private information may influence individuals privacy concerns and affect their propensity to disclose sensitive information , even when the objective risks associated with such disclosures do not change or worsen . We designed three experiments in the form of online surveys administered to students at a North-American University . In all experiments we manipulated the participants control over information publication , but not their control over the actual access to and usage by others of the published information . Our findings suggest , paradoxically , that more control over the publication of their private information decreases individuals privacy concerns and increases their willingness to publish sensitive information , even when the probability that strangers will access and use that information stays the same or , in fact , increases . On the other hand , less control over the publication of personal information increases individuals privacy concerns and decreases their willingness to publish sensitive information , even when the probability that strangers will access and use that information actually decreases . Our findings have both behavioral and policy implications , as they highlight how technologies that make individuals feel more in control over the publication of personal information may have the paradoxical and unintended consequence of eliciting their disclosure of more sensitive information .
2K_dev_691	A large number of facility management tasks relying on sensor measurements require knowledge of the context under which the readings were collected . However , this context information ( i.e . 'spatial metadata ' ) is generally recorded manually , a process that is error-prone and time consuming considering the number of sensors located in a building . Therefore , as other researchers have pointed out , there is a need to automatically determine the location information . Inferring the relative locations of sensors with respect to each other is arguably the first step to take and previous work in this area has already shown promising initial results . In this paper , we explore whether linear correlation or a statistical dependency measure ( in this case distance correlation ) are better suited to infer spatial relations between a pair of temperature sensors in a commercial building . We conducted analyses on three different test beds where temperature measurements from 10 sensors were collected every minute . We consider every possible size of data subsets within a year to explore time-windowing effects . We also examine how different physical distances between sensors affect the results . We conclude that a linear correlation ( the normalized covariance matrix ) captures the spatial relationship in most situations although it is significantly sensitive to choosing the appropriate window size .
2K_dev_692	Most everyday electrical and electromechanical objects emit small amounts of electromagnetic ( EM ) noise during regular operation . When a user makes physical contact with such an object , this EM signal propagates through the user , owing to the conductivity of the human body . By modifying a small , low-cost , software-defined radio , we can detect and classify these signals in real-time , enabling robust on-touch object detection . Unlike prior work , our approach requires no instrumentation of objects or the environment ; our sensor is self-contained and can be worn unobtrusively on the body . We call our technique EM-Sense and built a proof-of-concept smartwatch implementation . Our studies show that discrimination between dozens of objects is feasible , independent of wearer , time and local environment .
2K_dev_693	Imaging techniques such as immunofluorescence ( IF ) and the expression of fluorescent protein ( FP ) fusions are widely used to investigate the subcellular distribution of proteins . Here we report a s ...
2K_dev_694	We propose StarScan , a new star-shaped scan statistic for detecting irregularly-shaped spatial clusters . StarScan generalizes the traditional , circular spatial scan statistic by allowing the radius of the cluster around a center location to vary continuously with the angle , but penalizes the log-likelihood ratio score proportional to the total change in radius . StarScan was compared with circular scan and fast subset scan on simulated respiratory outbreaks and bioterrorist anthrax attacks injected into real-world Emergency Department data . Given a small amount of labeled training data , StarScan learns appropriate penalties for both compact and elongated clusters , resulting in improved detection performance .
2K_dev_695	Crowd-powered systems that help people are difficult to scale and sustain because human labor is expensive and worker pools are difficult to grow . To address this problem we introduce the idea of social microvolunteering , a type of intermediated friendsourcing in which a person can provide access to their friends as potential workers for microtasks supporting causes that they care about . We explore this idea by creating Visual Answers , an exemplar social microvolunteering application for Facebook that posts visual questions from people who are blind . We present results of a survey of 350 participants on the concept of social microvolunteering , and a deployment of the Visual Answers application with 91 participants , which collected 618 high-quality answers to questions asked over 12 days , illustrating the feasibility of the approach .
2K_dev_696	This paper considers the problem of distributed adaptive linear parameter estimation in multiagent inference networks . Local sensing model information is only partially available at the agents , and interagent communication is assumed to be unpredictable . The paper develops a generic mixed time-scale stochastic procedure consisting of simultaneous distributed learning and estimation , in which the agents adaptively assess their relative observation quality over time and fuse the innovations accordingly . Under rather weak assumptions on the statistical model and the interagent communication , it is shown that , by properly tuning the consensus potential with respect to the innovation potential , the asymptotic information rate loss incurred in the learning process may be made negligible . As such , it is shown that the agent estimates are asymptotically efficient , in that their asymptotic covariance coincides with that of a centralized estimator ( the inverse of the centralized Fisher information rate for Gaussian ...
2K_dev_697	Communication costs , resulting from synchronization requirements during learning , can greatly slow down many parallel machine learning algorithms . In this paper , we present a parallel Markov chain Monte Carlo ( MCMC ) algorithm in which subsets of data are processed independently , with very little communication . First , we arbitrarily partition data onto multiple machines . Then , on each machine , any classical MCMC method ( e.g. , Gibbs sampling ) may be used to draw samples from a posterior distribution given the data subset . Finally , the samples from each machine are combined to form samples from the full posterior . This embarrassingly parallel algorithm allows each machine to act independently on a subset of the data ( without communication ) until the final combination stage . We prove that our algorithm generates asymptotically exact samples and empirically demonstrate its ability to parallelize burn-in and sampling in several models .
2K_dev_698	Stochastic gradient optimization is a class of widely used algorithms for training machine learning models . To optimize an objective , it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset . However , when the variance of the noisy gradient is large , the algorithm might spend much time bouncing around , leading to slower convergence and worse performance . In this paper , we develop a general approach of using control variate for variance reduction in stochastic gradient . Data statistics such as low-order moments ( pre-computed or estimated online ) is used to form the control variate . We demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization . One is convexthe MAP estimation for logistic regression , and the other is non-convexstochastic variational inference for latent Dirichlet allocation . On both problems , our approach shows faster convergence and better performance than the classical approach .
2K_dev_699	We propose Fast Generalized Subset Scan ( FGSS ) , a new method for detecting anomalous patterns in general categorical data sets . We frame the pattern detection problem as a search over subsets of data records and attributes , maximizing a nonparametric scan statistic over all such subsets . We prove that the nonparametric scan statistics possess a novel property that allows for efficient optimization over the exponentially many subsets of the data without an exhaustive search , enabling FGSS to scale to massive and high-dimensional data sets . We evaluate the performance of FGSS in three real-world application domains ( customs monitoring , disease surveillance , and network intrusion detection ) , and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain . As compared to three other recently proposed detection algorithms , FGSS substantially decreased run time and improved detection power for massive multivariate data sets .
2K_dev_700	Paid crowd work offers remarkable opportunities for improving productivity , social mobility , and the global economy by engaging a geographically distributed workforce to complete complex tasks on demand and at scale . But it is also possible that crowd work will fail to achieve its potential , focusing on assembly-line piecework . Can we foresee a future crowd workplace in which we would want our children to participate ? This paper frames the major challenges that stand in the way of this goal . Drawing on theory from organizational behavior and distributed computing , as well as direct feedback from workers , we outline a framework that will enable crowd work that is complex , collaborative , and sustainable . The framework lays out research challenges in twelve major areas : workflow , task assignment , hierarchy , real-time response , synchronous collaboration , quality control , crowds guiding AIs , AIs guiding crowds , platforms , job design , reputation , and motivation .
2K_dev_701	We consider the problem of adaptively routing a fleet of cooperative vehicles within a road network in the presence of uncertain and dynamic congestion conditions . To tackle this problem , we first propose a Gaussian Process Dynamic Congestion Model that can effectively characterize both the dynamics and the uncertainty of congestion conditions . Our model is efficient and thus facilitates real-time adaptive routing in the face of uncertainty . Using this congestion model , we develop an efficient algorithm for non-myopic adaptive routing to minimize the collective travel time of all vehicles in the system . A key property of our approach is the ability to efficiently reason about the long-term value of exploration , which enables collectively balancing the exploration/exploitation trade-off for entire fleets of vehicles . We validate our approach based on traffic data from two large Asian cities . We show that our congestion model is effective in modeling dynamic congestion conditions . We also show that our routing algorithm generates significantly faster routes compared to standard baselines , and achieves near-optimal performance compared to an omniscient routing algorithm . We also present the results from a preliminary field study , which showcases the efficacy of our approach .
2K_dev_702	The simple fact that human fingers are large and mobile devices are small has led to the perennial issue of limited surface area for touch-based interactive tasks . In response , we have developed Toffee , a sensing approach that extends touch interaction beyond the small confines of a mobile device and onto ad hoc adjacent surfaces , most notably tabletops . This is achieved using a novel application of acoustic time differences of arrival ( TDOA ) correlation . Previous time-of-arrival based systems have required semi-permanent instrumentation of the surface and were too large for use in mobile devices . Our approach requires only a hard tabletop and gravity -- the latter acoustically couples mobile devices to surfaces . We conducted an evaluation , which shows that Toffee can accurately resolve the bearings of touch events ( mean error of 4.3 with a laptop prototype ) . This enables radial interactions in an area many times larger than a mobile device ; for example , virtual buttons that lie above , below and to the left and right .
2K_dev_703	A method activates different interactive functions based on a classification of vibro-acoustic signals resulting from touch events with finger parts of a user . A primary function is trigger when an interactive element on a touch screen is touched with a finger tip of the user . An auxiliary function is launched when the interactive element is touched with a knuckle or fingernail of the user . The touch events result in generating the vibro-acoustic signals . The vibro-acoustic signals are classified and used to distinguish what finger part was used based on the classification of the vibro-acoustic signals .
2K_dev_704	We present Lumitrack , a novel motion tracking technology that uses projected structured patterns and linear optical sensors . Each sensor unit is capable of recovering 2D location within the projection area , while multiple sensors can be combined for up to six degree of freedom ( DOF ) tracking . Our structured light approach is based on special patterns , called m-sequences , in which any consecutive sub-sequence of m bits is unique . Lumitrack can utilize both digital and static projectors , as well as scalable embedded sensing configurations . The resulting system enables high-speed , high precision , and low-cost motion tracking for a wide range of interactive applications . We detail the hardware , operation , and performance characteristics of our approach , as well as a series of example applications that highlight its immediate feasibility and utility .
2K_dev_705	There are many security tools and techniques for analyzing software , but many of them require access to source code . We propose leveraging decompilation , the study of recovering abstractions from compiled code , to apply existing source-based tools and techniques to compiled programs . A decompiler should focus on two properties to be used for security . First , it should recover abstractions as much as possible to minimize the complexity that must be handled by the security analysis that follows . Second , it should aim to recover these abstractions correctly . Previous work in control-flow structuring , an abstraction recovery problem used in decompilers , does not provide either of these properties . Specifically , existing structuring algorithms are not semantics-preserving , which means that they can not safely be used for decompilation without modification . Existing structural algorithms also miss opportunities for recovering control flow structure . We propose a new structuring algorithm in this paper that addresses these problems . We evaluate our decompiler , Phoenix , and our new structuring algorithm , on a set of 107 real world programs from GNU coreutils . Our evaluation is an order of magnitude larger than previous systematic studies of endto-end decompilers . We show that our decompiler outperforms the de facto industry standard decompiler Hex-Rays in correctness by 114 % , and recovers 30 more control-flow structure than existing structuring algorithms in the literature .
2K_dev_706	Identifying a suspect wearing a mask ( where only the suspect 's periocular region is visible ) is one of the toughest real-world challenges in biometrics that exist . In this paper , we present a practical method to hallucinate the full frontal face given only the periocular region of a face . This is an important problem faced in many law-enforcement applications on almost a daily basis . In such real-world situations , we only have access to the periocular region of a person 's face . Unfortunately commercial matchers are unable to process these images successfully . We propose in this paper , an approach that will reconstruct the entire frontal face using just the periocular region . We empirically show that our reconstruction technique , based on a modified sparsifying dictionary learning algorithm , can effectively reconstruct faces which we show are actually very similar to the original ground-truth faces . Further , our method is open set , thus can reconstruct any face not seen in training . We show the real-world applicability of method by benchmarking face verification results using the reconstructed faces to show that they still match competitively compared to the original faces when evaluated under a large-scale face verification protocol such as NIST 's FRGC protocol where over 256 million face matches are made .
2K_dev_707	A device just like Harry Potter 's Marauder 's Map , which pinpoints the location of each person-of-interest at all times , provides invaluable information for analysis of surveillance videos . To make this device real , a system would be required to perform robust person localization and tracking in real world surveillance scenarios , especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage . We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem . Given a set of person detection outputs , our framework takes advantage of all important cues such as color , person detection , face recognition and non-background information to perform tracking . Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints . Nonnegative discretization is used to enforce the mutual exclusion constraint , which guarantees a person detection output to only belong to exactly one individual . Experiments show that our algorithm performs robust localization and tracking of persons-of-interest not only in outdoor scenes , but also in a complex indoor real-world nursing home environment .
2K_dev_708	This article investigates the economic consequences of data errors in the information flows associated with business processes . We develop a process modeling-based methodology for managing the risks associated with such data errors . Our method focuses on the topological structure of a process and takes into account its effect on error propagation and risk mitigation using both expected loss and conditional value-at-risk risk measures . Using this method , optimal strategies can be designed for control resource allocation to manage risk in a business process . Our work contributes to the literature on both ex ante risk management-based business process design and ex post risk assessments of existing business processes and control models . This research applies not only to the literature on and practice of process design and risk management but also to business decision support systems in general . An order-fulfillment process of an online pharmacy is used to illustrate the methodology .
2K_dev_709	Design , construction and operation of building heating , ventilation and air conditioning ( HVAC ) systems are complicated processes that generally involve several stakeholders , such as mechanical designers , control system integrators , commissioning agents and facilities managers . It is important for all these stakeholders at various phases of the project to have a thorough understanding of the system components as well as the control strategy according to the design intent of the mechanical designers . For example , when assessing the behavior of a HVAC system during operation phase , it is important for facilities managers to check for the correctness of every components behavior and its control logic against the design specifications . The control sequences and logic of HVAC systems are primarily conveyed through schematic diagrams and textual descriptions called sequence of operations ( SOOs ) in construction documents ( ASHRAE , 2004 ) . Several challenges are associated with extracting and interpreting the information contained in these SOOs . Through a detailed analysis of a case-study conducted in relation to the information provided in the SOOs for the air handling unit ( AHU ) in a building , the research described in this paper highlights these challenges . Challenges such as missing information for controlled parameters as well as textual descriptions that are open to interpretations are common and result in inaccurate interpretation of the system behavior . This may adversely affect the overall performance of systems and lead to energy inefficiencies .
2K_dev_710	Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets . It optimizes the variational objective with stochastic optimization , following noisy estimates of the natural gradient . Operationally , stochastic inference iteratively subsamples from the data , analyzes the subsample , and updates parameters with a decreasing learning rate . However , the algorithm is sensitive to that rate , which usually requires hand-tuning to each application . We solve this problem by developing an adaptive learning rate for stochastic variational inference . Our method requires no tuning and is easily implemented with computations already made in the algorithm . We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora . Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates .
2K_dev_711	Confessions are people 's way of coming clean , sharing unethical acts with others . Although confessions are traditionally viewed as categorical one either comes clean or not people often confess to only part of their transgression . Such partial confessions may seem attractive , because they offer an opportunity to relieve ones guilt without having to own up to the full consequences of the transgression . In this paper , we explored the occurrence , antecedents , consequences , and everyday prevalence of partial confessions . Using a novel experimental design , we found a high frequency of partial confessions , especially among people cheating to the full extent possible . People found partial confessions attractive because they ( correctly ) expected partial confessions to be more believable than not confessing . People failed , however , to anticipate the emotional costs associated with partially confessing . In fact , partial confessions made people feel worse than not confessing or fully confessing , a finding corroborated in a laboratory setting as well as in a study assessing peoples everyday confessions . It seems that although partial confessions seem attractive , they come at an emotional cost .
2K_dev_712	Compared to visual concepts such as actions , scenes and objects , complex event is a higher level abstraction of longer video sequences . For example , a `` marriage proposal '' event is described by multiple objects ( e.g. , ring , faces ) , scenes ( e.g. , in a restaurant , outdoor ) and actions ( e.g. , kneeling down ) . The positive exemplars which exactly convey the precise semantic of an event are hard to obtain . It would be beneficial to utilize the related exemplars for complex event detection . However , the semantic correlations between related exemplars and the target event vary substantially as relatedness assessment is subjective . Two related exemplars can be about completely different events , e.g. , in the TRECVID MED dataset , both bicycle riding and equestrianism are labeled as related to `` attempting a bike trick '' event . To tackle the subjectiveness of human assessment , our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis . Experiments demonstrate that our algorithm is able to utilize related exemplars adaptively , and the algorithm gains good performance for complex event detection .
2K_dev_713	Summary : T cells are activated through interaction with antigen-presenting cells ( APCs ) . During activation , receptors and signalingintermediates accumulate in diverse spatiotemporal distributions . Thesedistributions control the probability of signaling interactions and thusgovern information ow through the signaling system . Spatiotemporal-ly resolved system-scale investigation of signaling can extract the regu-latory information thus encoded , allowing unique insight into thecontrol of T-cell function . Substantial technical challenges exist , andthese are briey discussed herein . While much of the work assessingT-cell spatiotemporal organization uses planar APC substitutes , wefocus here on B-cell APCs with often stark differences . Spatiotemporalsignaling distributions are driven by cell biologically distinct structures , a large protein assembly at the interface center , a large invagination , the actin-supported interface periphery as extended by smaller indi-vidual lamella , and a newly discovered whole-interface actin-drivenlamellum . The more than 60 elements of T-cell activation studied todate are dynamically distributed between these structures , generating acomplex organization of the signaling system . Signal initiation and coresignaling prefer the interface center , while signal amplication islocalized in the transient lamellum . Actin dynamics control signalingdistributions through regulation of the underlying structures and drivea highly undulating T-cell/APC interface that imposes substantialconstraints on T-cell organization . We suggest that the regulation ofactin dynamics , by controlling signaling distributions and membranetopology , is an important rheostat of T-cell signaling.Keywords : T-cell signaling , systems biology , imaging , actin
2K_dev_714	We study the problem of distribution to real regression , where one aims to regress a mapping f that takes in a distribution input covariate P 2 I ( for a non-parametric family of distributionsI ) and outputs a real-valued response Y 0 f ( P ) + . This setting was recently studied in [ 15 ] , where the \KernelKernel '' estimator was introduced and shown to have a polynomial rate of convergence . However , evaluating a new prediction with the Kernel-Kernel estimator scales as ( N ) . This causes the dicult situation where a large amount of data may be necessary for a low estimation risk , but the computation cost of estimation becomes infeasible when the data-set is too large . To this end , we propose the Double-Basis estimator , which looks to alleviate this big data problem in two ways : rst , the Double-Basis estimator is shown to have a computation complexity that is independent of the number of of instances N when evaluating new predictions after training ; secondly , the Double-Basis estimator is shown to have a fast rate of convergence for a general class of mappings f2F .
2K_dev_715	Primary motor-cortex multi-unit activity ( MUA ) and local-field potentials ( LFPs ) have both been suggested as potential control signals for brain-computer interfaces ( BCIs ) aimed at movement restoration . Some studies report that LFP-based decoding is comparable to spiking-based decoding , while others offer contradicting evidence . Differences in experimental paradigms , tuning models and decoding techniques make it hard to directly compare these results . Here , we use regression and mutual information analyses to study how MUA and LFP encode various kinematic parameters during reaching movements . We find that in addition to previously reported directional tuning , MUA also contains prominent speed tuning . LFP activity in low-frequency bands ( 15-40Hz , LFP L ) is primarily speed tuned , and contains more speed information than both high-frequency LFP ( 100-300Hz , LFP H ) and MUA . LFP H contains more directional information compared to LFP L , but less information when compared with MUA . Our results suggest that a velocity and speed encoding model is most appropriate for both MUA and LFP H , whereas a speed only encoding model is adequate for LFP L .
2K_dev_716	Stencil computations are an integral component of applications in a number of scientific computing domains . Short-vector SIMD instruction sets are ubiquitous on modern processors and can be used to significantly increase the performance of stencil computations . Traditional approaches to optimizing stencils on these platforms have focused on either short-vector SIMD or data locality optimizations . In this paper , we propose a domain specific language and compiler for stencil computations that allows specification of stencils in a concise manner and automates both locality and short-vector SIMD optimizations , along with effective utilization of multi-core parallelism . Loop transformations to enhance data locality and enable load-balanced parallelism are combined with a data layout transformation to effectively increase the performance of stencil computations . Performance increases are demonstrated for a number of stencils on several modern SIMD architectures .
2K_dev_717	Prototyping allows designers to quickly iterate and gather feedback , but the time it takes to create even a Wizard-of-Oz prototype reduces the utility of the process . In this paper , we introduce crowdsourcing techniques and tools for prototyping interactive systems in the time it takes to describe the idea . Our Apparition system uses paid microtask crowds to make even hard-to-automate functions work immediately , allowing more fluid prototyping of interfaces that contain interactive elements and complex behaviors . As users sketch their interface and describe it aloud in natural language , crowd workers and sketch recognition algorithms translate the input into user interface elements , add animations , and provide Wizard-of-Oz functionality . We discuss how design teams can use our approach to reflect on prototypes or begin user studies within seconds , and how , over time , Apparition prototypes can become fully-implemented versions of the systems they simulate . Powering Apparition is the first self-coordinated , real-time crowdsourcing infrastructure . We anchor this infrastructure on a new , lightweight write-locking mechanism that workers can use to signal their intentions to each other .
2K_dev_718	We present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features . These context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself . We learn our model in an efficient online fashion that is scalable for large , streaming data . With five streaming datasets from two different genres economics news articles and social mediawe evaluate our model on the task of sequential language modeling . Our model consistently outperforms competing models .
2K_dev_719	Embankment dams , like most other civil infrastructure systems , are exposed to harsh and largely unpredictable environments . However , unlike bridges , buildings and other structures , their design specifications and as-is properties are not generally known in the same level of detail due to , among other things , their age and the difficulties associated with assessing their internal structure . Hence , making sense of measurements collected from instruments used to monitor their behavior requires sound engineering judgment and analysis , as well as robust statistical analysis techniques to prevent misinterpretation . In the United States ( US ) , the current practice of analyzing the structural integrity of embankment dams relies primarily on manual a posteriori analysis of instrument data by engineers , leaving much room for improvement through the application of automated data analysis techniques . In our previous work , we presented the effectiveness of applying statistical anomaly detection techniques such as Principal Component Analysis and Robust Regression Analysis when analyzing piezometer data collected from embankment dams . In this paper , we present how we could improve our work by testing with simulated anomalies that are indicative of internal erosion problems . In order to closely replicate more realistic anomalous scenarios , a physics-based model of an embankment dam was developed . By varying a hydraulic conductivity of a soil material in the model , corresponding detection accuracies and sensitivities of the statistical anomaly detection algorithm were evaluated . When we applied our proposed anomaly detection on more realistically simulated anomalous data using the numerical model , the detection accuracy came out to be 98.5 % .
2K_dev_720	It is typically expected that if a mechanism is truthful , then the agents would , indeed , truthfully report their private information . But why would an agent believe that the mechanism is truthful ? We wish to design truthful mechanisms that are simple , that is whose truthfulness can be verified efficiently ( in the computational sense ) . Our approach involves three steps : ( i ) specifying the structure of mechanisms , ( ii ) constructing a verification algorithm , and ( iii ) measuring the quality of verifiably truthful mechanisms . We demonstrate this approach using a case study : approximate mechanism design without money for facility location .
2K_dev_721	Given the deluge of multimedia content that is becoming available over the Internet , it is increasingly important to be able to effectively examine and organize these large stores of information in ways that go beyond browsing or collaborative filtering . In this paper , we review previous work on audio and video processing , and define the task of topic-oriented multimedia summarization ( TOMS ) using natural language generation ( NLG ) : given a set of automatically extracted features from a video , a TOMS system will automatically generate a paragraph of natural language , which summarizes the important information in a video belonging to a certain topic , and for example provides explanations for why a video was matched and retrieved . Possible features include visual semantic concepts , objects , and actions , environmental sounds , and transcripts from automatic speech recognition ( ASR ) . We see this as a first step towards systems that will be able to discriminate visually similar , but semantically different videos , compare two videos and provide textual output or summarize a large number of videos at once . In this paper , we introduce our approach of solving the TOMS problem . We extract various visual concept features , environmental sounds and ASR transcription features from a given video , and develop a template-based NLG system to produce a textual recounting based on the extracted features . We also propose possible experimental designs for continuously evaluating and improving TOMS systems , and present results of a pilot evaluation of our initial system .
2K_dev_722	We present the results of an online survey of 1,221 Twitter users , comparing messages individuals regretted either saying during in-person conversations or posting on Twitter . Participants generally reported similar types of regrets in person and on Twitter . In particular , they often regretted messages that were critical of others . However , regretted messages that were cathartic/expressive or revealed too much information were reported at a higher rate for Twitter . Regretted messages on Twitter also reached broader audiences . In addition , we found that participants who posted on Twitter became aware of , and tried to repair , regret more slowly than those reporting in-person regrets . From this comparison of Twitter and in-person regrets , we provide preliminary ideas for tools to help Twitter users avoid and cope with regret .
2K_dev_723	Fragments of first-order temporal logic are useful for representing many practical privacy and security policies . Past work has proposed two strategies for checking event trace ( audit log ) compliance with policies : online monitoring and offline audit . Although online monitoring is space- and time-efficient , existing techniques insist that satisfying instances of all subformulas of the policy be amenable to caching , which limits expressiveness when some subformulas have infinite support . In contrast , offline audit is brute force and can handle more policies but is not as efficient . This paper proposes a new online monitoring algorithm that caches satisfying instances when it can , and falls back to the brute force search when it can not . Our key technical insight is a new flow- and time-sensitive static check of variable groundedness , called the temporal mode check , which determines subformulas for which such caching is feasible and those for which it is not and , hence , guides our algorithm . We prove the correctness of our algorithm and evaluate its performance over synthetic traces and realistic policies .
2K_dev_724	Abstract Host factor protein Cyclophilin A ( CypA ) regulates HIV-1 viral infectivity through direct interactions with the viral capsid , by an unknown mechanism . CypA can either promote or inhibit viral infection , depending on host cell type and HIV-1 capsid ( CA ) protein sequence . We have examined the role of conformational dynamics on the nanosecond to millisecond timescale in HIV-1 CA assemblies in the escape from CypA dependence , by magic-angle spinning ( MAS ) NMR and molecular dynamics ( MD ) . Through the analysis of backbone 1H-15N and 1H-13C dipolar tensors and peak intensities from 3D MAS NMR spectra of wild-type and the A92E and G94D CypA escape mutants , we demonstrate that assembled CA is dynamic , particularly in loop regions . The CypA loop in assembled wild-type CA from two strains exhibits unprecedented mobility on the nanosecond to microsecond timescales , and the experimental NMR dipolar order parameters are in quantitative agreement with those calculated from MD trajectories . Remarkably , the CypA loop dynamics of wild-type CA HXB2 assembly is significantly attenuated upon CypA binding , and the dynamics profiles of the A92E and G94D CypA escape mutants closely resemble that of wild-type CA assembly in complex with CypA . These results suggest that CypA loop dynamics is a determining factor in HIV-1 's escape from CypA dependence .
2K_dev_725	We applied quantified differential-dynamic logic ( QdL ) to analyze a control algorithm designed to provide directional force feedback for a surgical robot . We identified problems with the algorithm , proved that it was in general unsafe , and described exactly what could go wrong . We then applied QdL to guide the development of a new algorithm that provides safe operation along with directional force feedback . Using \KeYmaeraD ( a tool that mechanizes QdL ) , we created a machine-checked proof that guarantees the new algorithm is safe for all possible inputs .
2K_dev_726	The rise of socially targeted marketing suggests that decisions made by consumers can be predicted not only from their personal tastes and characteristics , but also from the decisions of people who are close to them in their networks . One obstacle to consider is that there may be several different measures for closeness that are appropriate , either through different types of friendships , or different functions of distance on one kind of friendship , where only a subset of these networks may actually be relevant . Another is that these decisions are often binary and more difficult to model with conventional approaches , both conceptually and computationally . To address these issues , we present a hierarchical auto-probit model for individual binary outcomes that uses and extends the machinery of the auto-probit method for binary data . We demonstrate the behavior of the parameters estimated by the multiple network-regime auto-probit model ( m-NAP ) under various sensitivity conditions , such as the impact of the prior distribution and the nature of the structure of the network . We also demonstrate several examples of correlated binary data outcomes in networks of interest to information systems , including the adoption of caller ring-back tones , whose use is governed by direct connection but explained by additional network topologies .
2K_dev_727	The multivariate multi-response ( MVMR ) linear regression problem is investigated , in which design matrices are Gaussian with covariance matrices ( 1 : K ) 0 ( 1 ) ; : : : ; ( K ) for K linear regressions . The support union of K p-dimensional regression vectors ( collected as columns of matrix B ) are recovered using l1=l2-regularized Lasso . Sucient and necessary conditions to guarantee successful recovery of the support union are characterized via a threshold . More specifically , it is shown that under certain conditions on the distributions of design matri
2K_dev_728	DOI : 10.2514/1.I010178 Complex software systems are becoming increasingly prevalent in aerospace applications : in particular , to accomplish critical tasks . Ensuring the safety of these systems is crucial , as they can have subtly different behaviors under slight variations in operating conditions.This paper advocates the use of formal verification techniques and in particulartheoremprovingfor hybridsoftware-intensivesystemsasawell-foundedcomplementaryapproachtothe classical aerospace verification and validation techniques , such as testing or simulation . As an illustration of these techniques , a novel lateral midair collision-avoidance maneuver is studied in an ideal setting , without accounting for the uncertainties of the physical reality . The challenges that naturally arise when applying such technology to industrial-scale applications is then detailed , and proposals are given on how to address these issues .
2K_dev_729	Existing Bayesian models , especially nonparametric Bayesian methods , rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations . While priors affect posterior distributions through Bayes ' rule , imposing posterior regularization is arguably more direct and in some cases more natural and general . In this paper , we present regularized Bayesian inference ( RegBayes ) , a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation . RegBayes is more flexible than the procedure that elicits expert knowledge via priors , and it covers both directed Bayesian networks and undirected Markov networks . When the regularization is induced from a linear operator on the posterior distributions , such as the expectation operator , we present a general convex-analysis theorem to characterize the solution of RegBayes . Furthermore , we present two concrete examples of RegBayes , infinite latent support vector machines ( iLSVM ) and multi-task infinite latent support vector machines ( MT-iLSVM ) , which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning , respectively . We present efficient inference methods and report empirical studies on several benchmark data sets , which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics . Such results contribute to push forward the interface between these two important subfields , which have been largely treated as isolated in the community .
2K_dev_730	Network robustness is an important principle in biology and engineering . Previous studies of global networks have identified both redundancy and sparseness as topological properties used by robust networks . By focusing on molecular subnetworks , or modules , we show that module topology is tightly linked to the level of environmental variability ( noise ) the module expects to encounter . Modules internal to the cell that are less exposed to environmental noise are more connected and less robust than external modules . A similar design principle is used by several other biological networks . We propose a simple change to the evolutionary gene duplication model which gives rise to the rich range of module topologies observed within real networks . We apply these observations to evaluate and design communication networks that are specifically optimized for noisy or malicious environments . Combined , joint analysis of biological and computational networks leads to novel algorithms and insights benefiting both fields .
2K_dev_731	The convergence of mobile computing and cloud computing enables new multimedia applications that are both resource-intensive and interaction-intensive . For these applications , end-to-end network bandwidth and latency matter greatly when cloud resources are used to augment the computational power and battery life of a mobile device . We first present quantitative evidence that this crucial design consideration to meet interactive performance criteria limits data center consolidation . We then describe an architectural solution that is a seamless extension of today 's cloud computing infrastructure .
2K_dev_732	We develop mini-batched parallel Frank-Wolfe ( conditional gradient ) methods for smooth convex optimization subject to block-separable constraints . Our work includes the basic ( batch ) Frank-Wolfe algorithm as well as the recently proposed Block-Coordinate Frank-Wolfe ( BCFW ) method\citep { lacoste2012block } as special cases . Our algorithm permits asynchronous updates within the minibatch , and is robust to stragglers and faulty worker threads . Our analysis reveals how the potential speedups over BCFW depend on the minibatch size and how one can provably obtain large problem dependent speedups . We present several experiments to indicate empirical behavior of our methods , obtaining significant speedups over competing state-of-the-art ( and synchronous ) methods on structural SVMs .
2K_dev_733	Radiation oncology has always been deeply rooted in modeling , from the early days of isoeffect curves to the contemporary Quantitative Analysis of Normal Tissue Effects in the Clinic ( QUANTEC ) initiative . In recent years , medical modeling for both prognostic and therapeutic purposes has exploded thanks to increasing availability of electronic data and genomics . One promising direction that medical modeling is moving toward is adopting the same machine learning methods used by companies such as Google and Facebook to combat disease . Broadly defined , machine learning is a branch of computer science that deals with making predictions from complex data through statistical models . These methods serve to uncover patterns in data and are actively used in areas such as speech recognition , handwriting recognition , face recognition , `` spam '' filtering ( junk email ) , and targeted advertising . Although multiple radiation oncology research groups have shown the value of applied machine learning ( ML ) , clinical adoption has been slow due to the high barrier to understanding these complex models by clinicians . Here , we present a review of the use of ML to predict radiation therapy outcomes from the clinician 's point of view with the hope that it lowers the `` barrier to entry '' for those without formal training in ML . We begin by describing 7 principles that one should consider when evaluating ( or creating ) an ML model in radiation oncology . We next introduce 3 popular ML methodslogistic regression ( LR ) , support vector machine ( SVM ) , and artificial neural network ( ANN ) and critique 3 seminal papers in the context of these principles . Although current studies are in exploratory stages , the overall methodology has progressively matured , and the field is ready for larger-scale further investigation .
2K_dev_734	How can we tell when accounts are fake or real in a social network ? And how can we tell which accounts belong to liberal , conservative or centrist users ? Often , we can answer such questions and label nodes in a network based on the labels of their neighbors and appropriate assumptions of homophily ( `` birds of a feather flock together '' ) or heterophily ( `` opposites attract '' ) . One of the most widely used methods for this kind of inference is Belief Propagation ( BP ) which iteratively propagates the information from a few nodes with explicit labels throughout a network until convergence . A well-known problem with BP , however , is that there are no known exact guarantees of convergence in graphs with loops . This paper introduces Linearized Belief Propagation ( LinBP ) , a linearization of BP that allows a closed-form solution via intuitive matrix equations and , thus , comes with exact convergence guarantees . It handles homophily , heterophily , and more general cases that arise in multi-class settings . Plus , it allows a compact implementation in SQL . The paper also introduces Single-pass Belief Propagation ( SBP ) , a localized ( or `` myopic '' ) version of LinBP that propagates information across every edge at most once and for which the final class assignments depend only on the nearest labeled neighbors . In addition , SBP allows fast incremental updates in dynamic networks . Our runtime experiments show that LinBP and SBP are orders of magnitude faster than standard BP , while leading to almost identical node labels .
2K_dev_735	We propose a dynamic topic model for monitoring temporal evolution of market competition by jointly leveraging tweets and their associated images . For a market of interest ( e.g . luxury goods ) , we aim at automatically detecting the latent topics ( e.g . bags , clothes , luxurious ) that are competitively shared by multiple brands ( e.g . Burberry , Prada , and Chanel ) , and tracking temporal evolution of the brands ' stakes over the shared topics . One of key applications of our work is social media monitoring that can provide companies with temporal summaries of highly overlapped or discriminative topics with their major competitors . We design our model to correctly address three major challenges : multiview representation of text and images , modeling of competitiveness of multiple brands over shared topics , and tracking their temporal evolution . As far as we know , no previous model can satisfy all the three challenges . For evaluation , we analyze about 10 millions of tweets and 8 millions of associated images of the 23 brands in the two categories of luxury and beer . Through experiments , we show that the proposed approach is more successful than other candidate methods for the topic modeling of competition . We also quantitatively demonstrate the generalization power of the proposed method for three prediction tasks .
2K_dev_736	The stochastic matching problem deals with finding a maximum matching in a graph whose edges are unknown but can be accessed via queries . This is a special case of stochastic k-set packing , where the problem is to find a maximum packing of sets , each of which exists with some probability . In this paper , we provide edge and set query algorithms for these two problems , respectively , that provably achieve some fraction of the omniscient optimal solution . Our main theoretical result for the stochastic matching ( i.e. , 2-set packing ) problem is the design of an adaptive algorithm that queries only a constant number of edges per vertex and achieves a ( 1-e ) fraction of the omniscient optimal solution , for an arbitrarily small e > 0 . Moreover , this adaptive algorithm performs the queries in only a constant number of rounds . We complement this result with a non-adaptive ( i.e. , one round of queries ) algorithm that achieves a ( 0.5 - e ) fraction of the omniscient optimum . We also extend both our results to stochastic k-set packing by designing an adaptive algorithm that achieves a ( 2/k - e ) fraction of the omniscient optimal solution , again with only O ( 1 ) queries per element . This guarantee is close to the best known polynomial-time approximation ratio of 3/k+1 -e for the deterministic k-set packing problem [ Furer 2013 ] . We empirically explore the application of ( adaptations of ) these algorithms to the kidney exchange problem , where patients with end-stage renal failure swap willing but incompatible donors . We show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches .
2K_dev_737	The generalized conductance ( G , H ) between two graphsG and H on the same vertex set V is defined as the ratio ( G , H ) 0 min SV capG ( S , S ) capH ( S , S ) , where capG ( S , S ) is the total weight of the edges crossing from S to S 0 V S. We show that the minimum generalized eigenvalue ( LG , LH ) of the pair of Laplacians LG and LH satisfies ( LG , LH ) ( G , H ) ( G ) /8 , where ( G ) is the usual conductance of G. A generalized cut that meets this bound can be obtained from the generalized eigenvector corresponding to ( LG , LH ) . The inequality complements a result of Trevisan ( Tre13 ) which shows that ( G ) can not be replaced by ( ( G , H ) ) in the above inequality , unless the Unique Games Conjecture is false .
2K_dev_738	Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data . For example , they are often used to explore complex systems where connections between entities are not well understood , such as in functional brain networks or genetic networks . Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable , such as a binary neural activation state or a continuous mRNA abundance measurement , even though in many real world problems , nodes can represent multivariate variables with much richer meanings , such as whole images , text documents , or multi-view feature vectors . In this paper , we propose a new principled framework for estimating the structure of undirected graphical models from such multivariate ( or multi-attribute ) nodal data . The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes . Under a Gaussian model , this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection ( Dempster , 1972 ) . We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective . Extensive simulation studies demonstrate the effectiveness of the method under various conditions . We provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles , and uncovering brain connectivity graph from positron emission tomography data . Finally , we provide sufficient conditions under which the true graphical structure can be recovered correctly .
2K_dev_739	Many large-scale machine learning ( ML ) applications use iterative algorithms to converge on parameter values that make the chosen model fit the input data . Often , this approach results in the same sequence of accesses to parameters repeating each iteration . This paper shows that these repeating patterns can and should be exploited to improve the efficiency of the parallel and distributed ML applications that will be a mainstay in cloud computing environments . Focusing on the increasingly popular `` parameter server '' approach to sharing model parameters among worker threads , we describe and demonstrate how the repeating patterns can be exploited . Examples include replacing dynamic cache and server structures with static pre-serialized structures , informing prefetch and partitioning decisions , and determining which data should be cached at each thread to avoid both contention and slow accesses to memory banks attached to other sockets . Experiments show that such exploitation reduces per-iteration time by 33 -- 98 % , for three real ML workloads , and that these improvements are robust to variation in the patterns over time .
2K_dev_740	Dashboards are increasingly being used in commercial buildings to show building data in an intuitive way to occupants and facility operators . Such dashboards make relevant parties aware of the impact that they have on a buildings behavior and enable them to understand the dynamics of building systems and current/historical energy use , and as a result , support reduction in energy use and improvement of operations of such systems . This paper gives an overview of an approach for designing and implementing an energy dashboard for a monitored building that includes highly sensed building automation systems and sensors for energy consumption . This project is part of the Energy Efficient Buildings Hub in the Philadelphia Navy Yard . The developed approach incorporates formalization of a taxonomy for building energy dashboards , identification of visualization aids and requirements through a questionnaire , and a prototype implementation . The findings show that effective building energy dashboards should contain query-based and quick-access based functionalities for showing building energy data through the use of various widgets and user interactions . Such dashboards should also enable decomposing data within spatial and temporal dimensions , interacting with static and dynamic data sources , and providing information about directly measurable energy usage vs. resulting energy use indicators .
2K_dev_741	The median survival of patients with idiopathic pulmonary fibrosis ( IPF ) continues to be approximately 3 years from the time of diagnosis , underscoring the lack of effective medical therapies for this disease . In the United States alone , approximately 40,000 patients die of this disease annually . In November 2012 , the NHLBI held a workshop aimed at coordinating research efforts and accelerating the development of IPF therapies . Basic , translational , and clinical researchers gathered with representatives from the NHLBI , patient advocacy groups , pharmaceutical companies , and the U.S. Food and Drug Administration to review the current state of IPF research and identify priority areas , opportunities for collaborations , and directions for future research . The workshop was organized into groups that were tasked with assessing and making recommendations to promote progress in one of the following six critical areas of research : ( 1 ) biology of alveolar epithelial injury and aberrant repair ; ( 2 ) role of extracellular matrix ; ( 3 ) preclinical modeling ; ( 4 ) role of inflammation and immunity ; ( 5 ) genetic , epigenetic , and environmental determinants ; ( 6 ) translation of discoveries into diagnostics and therapeutics . The workshop recommendations provide a basis for directing future research and strategic planning by scientific , professional , and patient communities and the NHLBI .
2K_dev_742	Many modern multi-core processors sport a large shared cache with the primary goal of enhancing the statistic performance of computing workloads . However , due to resulting cache interference among tasks , the uncontrolled use of such a shared cache can significantly hamper the predictability and analyzability of multi-core real-time systems . Software cache partitioning has been considered as an attractive approach to address this issue because it does not require any hardware support beyond that available on many modern processors . However , the state-of-the-art software cache partitioning techniques face two challenges : ( 1 ) the memory co-partitioning problem , which results in page swapping or waste of memory , and ( 2 ) the availability of a limited number of cache partitions , which causes degraded performance . These are major impediments to the practical adoption of software cache partitioning . In this paper , we propose a practical OS-level cache management scheme for multi-core real-time systems . Our scheme provides predictable cache performance , addresses the aforementioned problems of existing software cache partitioning , and efficiently allocates cache partitions to schedule a given task set . We have implemented and evaluated our scheme in Linux/RK running on the Intel Core i7 quad-core processor . Experimental results indicate that , compared to the traditional approaches , our scheme is up to 39 % more memory space efficient and consumes up to 25 % less cache partitions while maintaining cache predictability . Our scheme also yields a significant utilization benefit that increases with the number of tasks .
2K_dev_743	Existing algorithms for trajectory-based clustering usually rely on simplex representation and a single proximity-related distance ( or similarity ) measure . Consequently , additional information markers ( e.g. , social interactions or the semantics of the spatial layout ) are usually ignored , leading to the inability to fully discover the communities in the trajectory database . This is especially true for human-generated trajectories , where additional fine-grained markers ( e.g. , movement velocity at certain locations , or the sequence of semantic spaces visited ) can help capture latent relationships between cluster members . To address this limitation , we propose TODMIS : a general framework for Trajectory cOmmunity Discovery using Multiple Information Sources . TODMIS combines additional information with raw trajectory data and creates multiple similarity metrics . In our proposed approach , we first develop a novel approach for computing semantic level similarity by constructing a Markov Random Walk model from the semantically-labeled trajectory data , and then measuring similarity at the distribution level . In addition , we also extract and compute pair-wise similarity measures related to three additional markers , namely trajectory level spatial alignment ( proximity ) , temporal patterns and multi-scale velocity statistics . Finally , after creating a single similarity metric from the weighted combination of these multiple measures , we apply dense sub-graph detection to discover the set of distinct communities . We evaluated TODMIS extensively using traces of ( i ) student movement data in a campus , ( ii ) customer trajectories in a shopping mall , and ( iii ) city-scale taxi movement data . Experimental results demonstrate that TODMIS correctly and efficiently discovers the real grouping behaviors in these diverse settings .
2K_dev_744	This article summarizes the outcome of the 2012 Shonan Meeting Future of Multimedia Analysis and Mining.The meeting was really interesting , and the participants had a fun time with an Kamakura excursion and fine dinners , in addition to in-depth discussions on ready-to-go hot research topics ( see Figure 4 ) . We have enjoyed sharing even part of our experiences with readers here .
2K_dev_745	Despite decades of research attempting to establish conversational interaction between humans and computers , the capabilities of automated conversational systems are still limited . In this paper , we introduce Chorus , a crowd-powered conversational assistant . When using Chorus , end users converse continuously with what appears to be a single conversational partner . Behind the scenes , Chorus leverages multiple crowd workers to propose and vote on responses . A shared memory space helps the dynamic crowd workforce maintain consistency , and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting . Studies with 12 end users and 100 crowd workers demonstrate that Chorus can provide accurate , topical responses , answering nearly 93 % of user queries appropriately , and staying on-topic in over 95 % of responses . We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed , quality , and breadth of assistance . Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence , and may enable a useful new way of interacting with the crowds powering other systems .
2K_dev_746	Organizations that collect and use large volumes of personal information often use security audits to protect data subjects from inappropriate uses of this information by authorized insiders . In face of unknown incentives of employees , a reasonable audit strategy for the organization is one that minimizes its regret . While regret minimization has been extensively studied in repeated games , the standard notion of regret for repeated games can not capture the complexity of the interaction between the organization ( defender ) and an adversary , which arises from dependence of rewards and actions on history . To account for this generality , we introduce a richer class of games called bounded-memory games , which can provide a more accurate model of the audit process . We introduce the notion of k-adaptive regret , which compares the reward obtained by playing actions prescribed by the algorithm against a hypothetical k-adaptive adversary with the reward obtained by the best expert in hindsight against the same adversary . Roughly , a hypothetical k-adaptive adversary adapts her strategy to the defender 's actions exactly as the real adversary would within each window of karounds . A k-adaptive adversary is a natural model for temporary adversaries ( e.g. , company employees ) who stay for a certain number of audit cycles and are then replaced by a different person . Our definition is parameterized by a set of experts , which can include both fixed and adaptive defender strategies . We investigate the inherent complexity of and design algorithms for adaptive regret minimization in bounded-memory games of perfect and imperfect information . We prove a hardness result showing that , with imperfect information , any k-adaptive regret minimizing algorithm ( with fixed strategies as experts ) must be inefficient unless NP 0 RP even when playing against an oblivious adversary . In contrast , for bounded-memory games of perfect and imperfect information we present approximate 0-adaptive regret minimization algorithms against an oblivious adversary running in time $ n^ { O\left ( 1\right ) } $ .
2K_dev_747	A 2.91-billion base pair ( bp ) consensus sequence of the euchromatic portion of the human genome was generated by the whole-genome shotgun sequencing method . The 14.8-billion bp DNA sequence was generated over 9 months from 27,271,853 high-quality sequence reads ( 5.11-fold coverage of the genome ) from both ends of plasmid clones made from the DNA of five individuals . Two assembly strategiesa whole-genome assembly and a regional chromosome assemblywere used , each combining sequence data from Celera and the publicly funded genome effort . The public data were shredded into 550-bp segments to create a 2.9-fold coverage of those genome regions that had been sequenced , without including biases inherent in the cloning and assembly procedure used by the publicly funded group . This brought the effective coverage in the assemblies to eightfold , reducing the number and size of gaps in the final assembly over what would be obtained with 5.11-fold coverage . The two assembly strategies yielded very similar results that largely agree with independent mapping data . The assemblies effectively cover the euchromatic regions of the human chromosomes . More than 90 % of the genome is in scaffold assemblies of 100,000 bp or more , and 25 % of the genome is in scaffolds of 10 million bp or larger . Analysis of the genome sequence revealed 26,588 protein-encoding transcripts for which there was strong corroborating evidence and an additional 12,000 computationally derived genes with mouse matches or other weak supporting evidence . Although gene-dense clusters are obvious , almost half the genes are dispersed in low G+C sequence separated by large tracts of apparently noncoding sequence . Only 1.1 % of the genome is spanned by exons , whereas 24 % is in introns , with 75 % of the genome being intergenic DNA . Duplications of segmental blocks , ranging in size up to chromosomal lengths , are abundant throughout the genome and reveal a complex evolutionary history . Comparative genomic analysis indicates vertebrate expansions of genes associated with neuronal function , with tissue-specific developmental regulation , and with the hemostasis and immune systems . DNA sequence comparisons between the consensus sequence and publicly funded genome data provided locations of 2.1 million single-nucleotide polymorphisms ( SNPs ) . A random pair of human haploid genomes differed at a rate of 1 bp per 1250 on average , but there was marked heterogeneity in the level of polymorphism across the genome . Less than 1 % of all SNPs resulted in variation in proteins , but the task of determining which SNPs have functional consequences remains an open challenge .
2K_dev_748	Developments in health information technology have encouraged the establishment of distributed systems known as Health Information Exchanges ( HIEs ) to enable the sharing of patient records between institutions . In many cases , the parties running these exchanges wish to limit the amount of information they are responsible for holding because of sensitivities about patient information . Hence , there is an interest in broker-based HIEs that keep limited information in the exchange repositories . However , it is essential to audit these exchanges carefully due to risks of inappropriate data sharing . In this paper , we consider some of the requirements and present a design for auditing broker-based HIEs in a way that controls the information available in audit logs and regulates their release for investigations . Our approach is based on formal rules for audit and the use of Hierarchical Identity-Based Encryption ( HIBE ) to support staged release of data needed in audits and a balance between automated and manual reviews . We test our methodology via an extension of a standard for auditing HIEs called the Audit Trail and Node Authentication Profile ( ATNA ) protocol .
2K_dev_749	We introduce scalable deep kernels , which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods . Specifically , we transform the inputs of a spectral mixture base kernel with a deep architecture , using local kernel interpolation , inducing points , and structure exploiting ( Kronecker and Toeplitz ) algebra for a scalable kernel representation . These closed-form kernels can be used as drop-in replacements for standard kernels , with benefits in expressive power and scalability . We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process . Inference and learning cost $ O ( n ) $ for $ n $ training points , and predictions cost $ O ( 1 ) $ per test point . On a large and diverse collection of applications , including a dataset with 2 million examples , we show improved performance over scalable Gaussian processes with flexible kernel learning models , and stand-alone deep architectures .
2K_dev_750	Significant efforts are being made in the improvement of building automation and control systems in order to optimize the performance of buildings ( e.g . reduction of energy consumption ) . As sensor networks in buildings increase , the complexity of managing them also increases . For instance , the generation and maintenance of metadata about sensors , such as their location within a building , currently requires significant manual labor . The research described in this paper explores the relationship between different HVAC system sensor measurements and physical characteristics of spaces , and its potential application in streamlining the identification of sensor location within a facility . The energy contained in the conditioned air delivered to each room is presented as a characteristic feature in order to understand the differences between rooms . Being able to understand the relationships between different measurement types and building characteristics is fundamental in achieving an automatic mapping of sensors in buildings , and this paper describes initial observations and results towards this goal .
2K_dev_751	Given a large dataset of users ' ratings of movies , what is the best model to accurately predict which movies a person will like ? And how can we prevent spammers from tricking our algorithms into suggesting a bad movie ? Is it possible to infer structure between movies simultaneously ? In this paper we describe a unified Bayesian approach to Collaborative Filtering that accomplishes all of these goals . It models the discrete structure of ratings and is flexible to the often non-Gaussian shape of the distribution . Additionally , our method finds a co-clustering of the users and items , which improves the model 's accuracy and makes the model robust to fraud . We offer three main contributions : ( 1 ) We provide a novel model and Gibbs sampling algorithm that accurately models the quirks of real world ratings , such as convex ratings distributions . ( 2 ) We provide proof of our model 's robustness to spam and anomalous behavior . ( 3 ) We use several real world datasets to demonstrate the model 's effectiveness in accurately predicting user 's ratings , avoiding prediction skew in the face of injected spam , and finding interesting patterns in real world ratings data .
2K_dev_752	This book offers detailed surveys and systematic discussion of models , algorithms and applications for link mining , focusing on theory and technique , and related applications : text mining , social network analysis , collaborative filtering and bioinformatics .
2K_dev_753	We address the problem of model checking stochastic systems , i.e. , checking whether a stochastic system satisfies a certain temporal property with a probability greater ( or smaller ) than a fixed threshold . In particular , we present a Statistical Model Checking ( SMC ) approach based on Bayesian statistics . We show that our approach is feasible for a certain class of hybrid systems with stochastic transitions , a generalization of Simulink/Stateflow models . Standard approaches to stochastic discrete systems require numerical solutions for large optimization problems and quickly become infeasible with larger state spaces . Generalizations of these techniques to hybrid systems with stochastic effects are even more challenging . The SMC approach was pioneered by Younes and Simmons in the discrete and non-Bayesian case . It solves the verification problem by combining randomized sampling of system traces ( which is very efficient for Simulink/Stateflow ) with hypothesis testing ( i.e. , testing against a probability threshold ) or estimation ( i.e. , computing with high probability a value close to the true probability ) . We believe SMC is essential for scaling up to large Stateflow/Simulink models . While the answer to the verification problem is not guaranteed to be correct , we prove that Bayesian SMC can make the probability of giving a wrong answer arbitrarily small . The advantage is that answers can usually be obtained much faster than with standard , exhaustive model checking techniques . We apply our Bayesian SMC approach to a representative example of stochastic discrete-time hybrid system models in Stateflow/Simulink : a fuel control system featuring hybrid behavior and fault tolerance . We show that our technique enables faster verification than state-of-the-art statistical techniques . We emphasize that Bayesian SMC is by no means restricted to Stateflow/Simulink models . It is in principle applicable to a variety of stochastic models from other domains , e.g. , systems biology .
2K_dev_754	We consider distributed optimization in random networks where N nodes cooperatively minimize the sum i=1 N f i ( x ) of their individual convex costs . Existing literature proposes distributed gradient-like methods that are computationally cheap and resilient to link failures , but have slow convergence rates . In this paper , we propose accelerated distributed gradient methods that 1 ) are resilient to link failures ; 2 ) computationally cheap ; and 3 ) improve convergence rates over other gradient methods . We model the network by a sequence of independent , identically distributed random matrices { W ( k ) } drawn from the set of symmetric , stochastic matrices with positive diagonals . The network is connected on average and the cost functions are convex , differentiable , with Lipschitz continuous and bounded gradients . We design two distributed Nesterov-like gradient methods that modify the D-NG and D-NC methods that we proposed for static networks . We prove their convergence rates in terms of the expected optimality gap at the cost function . Let k and K be the number of per-node gradient evaluations and per-node communications , respectively . Then the modified D-NG achieves rates O ( logk/k ) and O ( logK/ K ) , and the modified D-NC rates O ( 1/k 2 ) and O ( 1/ K 2- ) , where > 0 is arbitrarily small . For comparison , the standard distributed gradient method can not do better than ( 1/k 2/3 ) and ( 1/ K 2/3 ) , on the same class of cost functions ( even for static networks ) . Simulation examples illustrate our analytical findings .
2K_dev_755	Localization , finding the coordinates of an object with respect to other objects with known coordinateshereinafter , referred to as anchors , is a nonlinear problem , as it involves solving circle equations when relating distances to Cartesian coordinates , or , computing Cartesian coordinates from angles using the law of sines . This nonlinear problem has been a focus of significant attention over the past two centuries and the progress follows closely with the advances in instrumentation as well as applied mathematics , geometry , statistics , and signal processing . The Internet-of-Things ( IoT ) , with massive deployment of wireless tagged things , has renewed the interest and activity in finding novel , expert , and accurate indoor self-localization methods , where a particular emphasis is on distributed approaches . This paper is dedicated to reviewing a notable alternative to the nonlinear localization problem , i.e. , a linear-convex method , based on Khan et al . s work . This linear solution utilizes relatively unknown geometric concepts in the context of localization problems , i.e. , the barycentric coordinates and the CayleyMenger determinants . Specifically , in an $ m $ -dimensional Euclidean space , a set of $ m+1 $ anchors , objects with known locations , is sufficient ( and necessary ) to localize an arbitrary collection of objects with unknown locationshereinafter , referred to as sensors , with a linear-iterative algorithm . To ease the presentation , we discuss the solution under a structural convexity condition , namely , the sensors lie inside the convex hull of at least $ m+1 $ anchors . Although rigorous results are included , several remarks and discussion throughout this paper provide the intuition behind the solution and are primarily aimed toward researchers and practitioners interested in learning about this challenging field of research . Additional figures and demos have been added as auxiliary material to support this aim .
2K_dev_756	In search advertising , the search engine needs to select the most profitable advertisements to display , which can be formulated as an instance of online learning with partial feedback , also known as the stochastic multi-armed bandit ( MAB ) problem . In this paper , we show that the naive application of MAB algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and `` estimation of the largest mean '' ( ELM ) bias that harms the advertisers by increasing game-theoretic player-regret . We then propose simple bias-correction methods with benefits to both the search engine and the advertisers .
2K_dev_757	graph theory . One application of this interplay is a nearly linear time solver for Symmetric Diagonally Dominate systems ( SDD ) . This seemingly restrictive class of systems has received much interest in the last 15 years . Both algorithm design theory and practical implementations have made substantial progress . There is also a growing number of problems that can be efficiently solved using SDD solvers including : image segmentation , image denoising , finding solutions to elliptic equations , computing maximum flow in a graph , graph sparsification , and graphics . All these examples can be viewed as special case of convex optimization problems . Spectral Graph Theory is the interplay between linear algebra and combinatorial
2K_dev_758	Smartwatches promise to bring enhanced convenience to common communication , creation and information retrieval tasks . Due to their prominent placement on the wrist , they must be small and otherwise unobtrusive , which limits the sophistication of interactions we can perform . This problem is particularly acute if the smartwatch relies on a touchscreen for input , as the display is small and our fingers are relatively large . In this work , we propose a complementary input approach : using the watch face as a multi-degree-of-freedom , mechanical interface . We developed a proof of concept smartwatch that supports continuous 2D panning and twist , as well as binary tilt and click . To illustrate the potential of our approach , we developed a series of example applications , many of which are cumbersome -- or even impossible -- on today 's smartwatch devices .
2K_dev_759	Which song will Smith listen to next ? Which restaurant will Alice go to tomorrow ? Which product will John click next ? These applications have in common the prediction of user trajectories that are in a constant state of flux over a hidden network ( e.g . website links , geographic location ) . Moreover , what users are doing now may be unrelated to what they will be doing in an hour from now . Mindful of these challenges we propose TribeFlow , a method designed to cope with the complex challenges of learning personalized predictive models of non-stationary , transient , and time-heterogeneous user trajectories . TribeFlow is a general method that can perform next product recommendation , next song recommendation , next location prediction , and general arbitrary-length user trajectory prediction without domain-specific knowledge . TribeFlow is more accurate and up to 413x faster than top competitors .
2K_dev_760	We propose a model to learn visually grounded word embeddings ( vis-w2v ) to capture visual notions of semantic relatedness . While word embeddings trained using text have been extremely successful , they can not uncover notions of semantic relatedness implicit in our visual world . For instance , although `` eats '' and `` stares at '' seem unrelated in text , they share semantics visually . When people are eating something , they also tend to stare at the food . Grounding diverse relations like `` eats '' and `` stares at '' into vision remains challenging , despite recent progress in vision . We note that the visual grounding of words depends on semantics , and not the literal pixels . We thus use abstract scenes created from clipart to provide the visual grounding . We find that the embeddings we learn capture fine-grained , visually grounded notions of semantic relatedness . We show improvements over text-only word embeddings ( word2vec ) on three tasks : common-sense assertion classification , visual paraphrasing and text-based image retrieval . Our code and datasets are available online .
2K_dev_761	Learning about a new area of knowledge is challenging for novices partly because they are not yet aware of which topics are most important . The Internet contains a wealth of information for learning the underlying structure of a domain , but relevant sources often have diverse structures and emphases , making it hard to discern what is widely considered essential knowledge vs. what is idiosyncratic . Crowdsourcing offers a potential solution because humans are skilled at evaluating high-level structure , but most crowd micro-tasks provide limited context and time . To address these challenges , we present Crowdlines , a system that uses crowdsourcing to help people synthesize diverse online information . Crowdworkers make connections across sources to produce a rich outline that surfaces diverse perspectives within important topics . We evaluate Crowdlines with two experiments . The first experiment shows that a high context , low structure interface helps crowdworkers perform faster , higher quality synthesis , while the second experiment shows that a tournament-style ( parallelized ) crowd workflow produces faster , higher quality , more diverse outlines than a linear ( serial/iterative ) workflow .
2K_dev_762	Review fraud is a pervasive problem in online commerce , in which fraudulent sellers write or purchase fake reviews to manipulate perception of their products and services . Fake reviews are often detected based on several signs , including 1 ) they occur in short bursts of time ; 2 ) fraudulent user accounts have skewed rating distributions . However , these may both be true in any given dataset . Hence , in this paper , we propose an approach for detecting fraudulent reviews which combines these 2 approaches in a principled manner , allowing successful detection even when one of these signs is not present . To combine these 2 approaches , we formulate our Bayesian Inference for Rating Data ( BIRD ) model , a flexible Bayesian model of user rating behavior . Based on our model we formulate a likelihood-based suspiciousness metric , Normalized Expected Surprise Total ( NEST ) . We propose a linear-time algorithm for performing Bayesian inference using our model and computing the metric . Experiments on real data show that BIRDNEST successfully spots review fraud in large , real-world graphs : the 50 most suspicious users of the Flipkart platform flagged by our algorithm were investigated and all identified as fraudulent by domain experts at Flipkart .
2K_dev_763	We describe a novel approach for estimating the pitch and yaw of fingers relative to a touchscreen 's surface , offering two additional , analog degrees of freedom for interactive functions . Further , we show that our approach can be achieved on off-the-shelf consumer touchscreen devices : a smartphone and smartwatch . We validate our technique though a user study on both devices and conclude with several demo applications that illustrate the value and immediate feasibility of our approach .
2K_dev_764	User identification and differentiation have implications in many application domains , including security , personalization , and co-located multiuser systems . In response , dozens of approaches have been developed , from fingerprint and retinal scans , to hand gestures and RFID tags . In this work , we propose CapAuth , a technique that uses existing , low-level touchscreen data , combined with machine learning classifiers , to provide real-time authentication and even identification of users . As a proof-of-concept , we ran our software on an off-the-shelf Nexus 5 smartphone . Our user study demonstrates twenty-participant authentication accuracies of 99.6 % . For twenty-user identification , our software achieved 94.0 % accuracy and 98.2 % on groups of four , simulating family use .
2K_dev_765	Health information exchanges ( HIEs ) are healthcare information technology efforts designed to foster coordination of patient care across the fragmented U.S. healthcare system . Their purpose is to improve efficiency and quality of care through enhanced sharing of patient data . Across the United States , numerous states have enacted laws that provide various forms of incentives for HIEs and address growing privacy concerns associated with the sharing of patient data . We investigate the impact on the emergence of HIEs of state laws that incentivize HIE efforts and state laws that include different types of privacy requirements for sharing healthcare data , focusing on the impact of laws that include requirements for patient consent . Although we observe that privacy regulation alone can result in a decrease in planning and operational HIEs , we also find that , when coupled with incentives , privacy regulation with requirements for patient consent can actually positively impact the development of HIE efforts . Among all states with laws creating HIE incentives , only states that combined incentives with consent requirements saw a net increase in operational HIEs ; HIEs in those states also reported decreased levels of privacy concern relative to HIEs in states with other legislative approaches . Our results contribute to the burgeoning literature on health information technology and the debate on the impact of privacy regulation on technology innovation . In particular , they show that the impact of privacy regulation on the success of information technology efforts is heterogeneous : both positive and negative effects can arise from regulation , depending on the specific attributes of privacy laws . This paper was accepted by Anandhi Bharadwaj , information systems .
2K_dev_766	Natural language dialog is an important and intuitive way for people to access information and services . However , current dialog systems are limited in scope , brittle to the richness of natural language , and expensive to produce . This paper introduces Guardian , a crowdpowered framework that wraps existing Web APIs into immediately usable spoken dialog systems . Guardian takes as input the Web API and desired task , and the crowd determines the parameters necessary to complete it , how to ask for them , and interprets the responses from the API . The system is structured so that , over time , it can learn to take over for the crowd . This hybrid systems approach will help make dialog systems both more general and more robust going forward .
2K_dev_767	Recent research has highlighted the need for upstream planning in healthcare service delivery systems , patient scheduling , and resource allocation in the hospital inpatient setting . This study examines the value of upstream planning within hospital-wide resource allocation decisions based on machine learning ( ML ) and mixed-integer programming ( MIP ) , focusing on prediction of diagnosis-related groups ( DRGs ) and the use of these predictions for allocating scarce hospital resources . DRGs are a payment scheme employed at patients discharge , where the DRG and length of stay determine the revenue that the hospital obtains . We show that early and accurate DRG classification using ML methods , incorporated into an MIP-based resource allocation model , can increase the hospitals contribution margin , the number of admitted patients , and the utilization of resources such as operating rooms and beds . We test these methods on hospital data containing more than 16,000 inpatient records and demonstrate improved DRG classification accuracy as compared to the hospitals current approach . The largest improvements were observed at and before admission , when information such as procedures and diagnoses is typically incomplete , but performance was improved even after a substantial portion of the patients length of stay , and under multiple scenarios making different assumptions about the available information . Using the improved DRG predictions within our resource allocation model improves contribution margin by 2.9 % and the utilization of scarce resources such as operating rooms and beds from 66.3 % to 67.3 % and from 70.7 % to 71.7 % , respectively . This enables 9.0 % more nonurgent elective patients to be admitted as compared to the baseline .
2K_dev_768	Touchscreens with dynamic electrostatic friction are a com-pelling , low-latency and solid-state haptic feedback technology . Work to date has focused on minimum perceptual difference , texture rendering , and fingertip-surface models . However , no work to date has quantified how electrostatic feedback can be used to improve user performance , in par-ticular targeting , where virtual objects rendered on touchscreens can offer tactile feedback . Our results show that electrostatic haptic feedback can improve targeting speed by 7.5 % compared to conventional flat touchscreens .
2K_dev_769	Background Effective management and treatment of cancer continues to be complicated by the rapid evolution and resulting heterogeneity of tumors . Phylogenetic study of cell populations in single tumors provides a way to delineate intra-tumoral heterogeneity and identify robust features of evolutionary processes . The introduction of single-cell sequencing has shown great promise for advancing single-tumor phylogenetics ; however , the volume and high noise in these data present challenges for inference , especially with regard to chromosome abnormalities that typically dominate tumor evolution . Here , we investigate a strategy to use such data to track differences in tumor cell genomic content during progression .
2K_dev_770	When training large machine learning models with many variables or parameters , a single machine is often inadequate since the model may be too large to fit in memory , while training can take a long time even with stochastic updates . A natural recourse is to turn to distributed cluster computing , in order to harness additional memory and processors . However , naive , unstructured parallelization of ML algorithms can make inefficient use of distributed memory , while failing to obtain proportional convergence speedups or can even result in divergence . We develop a framework of primitives for dynamic model-parallelism , STRADS , in order to explore partitioning and update scheduling of model variables in distributed ML algorithms thus improving their memory efficiency while presenting new opportunities to speed up convergence without compromising inference correctness . We demonstrate the efficacy of model-parallel algorithms implemented in STRADS versus popular implementations for Topic Modeling , Matrix Factorization and Lasso .
2K_dev_771	We prove that any invariant algebraic set of a given polynomial vector field can be algebraically represented by one polynomial and a finite set of its successive Lie derivatives . This so-called differential radical characterization re- lies on a sound abstraction of the reachable set of solutions by the smallest variety that contains it . The characterization leads to a differential radical invariant proof rule that is sound and complete , which implies that invariance of algebraic equa- tions over real-closed fields is decidable . Furthermore , the problem of generating invariant varieties is shown to be as hard as minimizing the rank of a symbolic matrix , and is therefore NP-hard . We investigate symbolic linear algebra tools based on Gaussian elimination to efficiently automate the generation . The ap- proach can , e.g. , generate nontrivial algebraic invariant equations capturing the airplane behavior during take-off or landing in longitudinal motion .
2K_dev_772	Given multimillion-node graphs such as `` who-follows-whom '' , `` patent-cites-patent '' , `` user-likes-page '' and `` actor/director-makes-movie '' networks , how can we find unexpected behaviors ? When companies operate on the graphs with monetary incentives to sell Twitter `` Followers '' and Facebook page `` Likes '' , the graphs show strange connectivity patterns . In this paper , we study a complete graph from a large Twitter-style social network , spanning up to 3.33 billion edges . We report strange deviations from typical patterns like smooth degree distributions . We find that such deviations are often due to `` lockstep behavior '' that large groups of followers connect to the same groups of followees . Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep . We discover that ( a ) the lockstep behaviors on the graph shape dense `` block '' in its adjacency matrix and creates `` rays '' in spectral subspaces , and ( b ) partially overlapping of the behaviors shape `` staircase '' in its adjacency matrix and creates `` pearls '' in spectral subspaces . The second contribution is that we provide a fast algorithm , using the discovery as a guide for practitioners , to detect users who offer the lockstep behaviors in undirected/directed/bipartite graphs . We carry out extensive experiments on both synthetic and real datasets , as well as public datasets from IMDb and US Patent . The results demonstrate the scalability and effectiveness of our proposed algorithm .
2K_dev_773	Characterizing the spatial distribution of proteins directly from microscopy images is a difficult problem with numerous applications in cell biology ( e.g . identifying motor-related proteins ) and clinical research ( e.g . identification of cancer biomarkers ) . Here we describe the design of a system that provides automated analysis of punctate protein patterns in microscope images , including quantification of their relationships to microtubules . We constructed the system using confocal immunofluorescence microscopy images from the Human Protein Atlas project for 11 punctate proteins in three cultured cell lines . These proteins have previously been characterized as being primarily located in punctate structures , but their images had all been annotated by visual examination as being simply vesicular . We were able to show that these patterns could be distinguished from each other with high accuracy , and we were able to assign to one of these subclasses hundreds of proteins whose subcellular localization had not previously been well defined . In addition to providing these novel annotations , we built a generative approach to modeling of punctate distributions that captures the essential characteristics of the distinct patterns . Such models are expected to be valuable for representing and summarizing each pattern and for constructing systems biology simulations of cell behaviors .
2K_dev_774	Revenue maximization in combinatorial auctions ( and other multidimensional selling settings ) is one of the most important and elusive problems in mechanism design . The optimal design is unknown , and is known to include features that are not acceptable in many applications , such as favoring some bidders over others and randomization . In this paper , we instead study a common revenue-enhancement approach - bundling - in the context of the most commonly studied combinatorial auction mechanism , the Vickrey-Clarke-Groves ( VCG ) mechanism . A second challenge in mechanism design for combinatorial auctions is that the prior distribution on each bidder 's valuation can be doubly exponential . Such priors do not exist in most applications . Rather , in many applications ( such as premium display advertising markets ) , there is essentially a point prior , which may not be accurate . We adopt the point prior model , and prove robustness to inaccuracy in the prior . Then , we present a branch-and-bound framework for finding the optimal bundling . We introduce several techniques for branching , upper bounding , lower bounding , and lazy bounding . Experiments on CATS distributions validate the approach and show that our techniques dramatically improve scalability over a leading general-purpose MIP solver .
2K_dev_775	We address the problem of action recognition in unconstrained videos . We propose a novel content driven pooling that leverages space-time context while being robust toward global space-time transformations . Being robust to such transformations is of primary importance in unconstrained videos where the action localizations can drastically shift between frames . Our pooling identifies regions of interest using video structural cues estimated by different saliency functions . To combine the different structural information , we introduce an iterative structure learning algorithm , WSVM ( weighted SVM ) , that determines the optimal saliency layout of an action model through a sparse regularizer . A new optimization method is proposed to solve the WSVM highly non-smooth objective function . We evaluate our approach on standard action datasets ( KTH , UCF50 and HMDB ) . Most noticeably , the accuracy of our algorithm reaches 51.8 % on the challenging HMDB dataset which outperforms the state-of-the-art of 7.3 % relatively .
2K_dev_776	It is well known that strategic behavior in elections is essentially unavoidable ; we therefore ask : how bad can the rational outcome be ? We answer this question via the notion of the price of anarchy , using the scores of alternatives as a proxy for their quality and bounding the ratio between the score of the optimal alternative and the score of the winning alternative in Nash equilibrium . Specifically , we are interested in Nash equilibria that are obtained via sequences of rational strategic moves . Focusing on three common voting rules -- plurality , veto , and Borda -- we provide very positive results for plurality and very negative results for Borda , and place veto in the middle of this spectrum .
2K_dev_777	Given a large graph , like a computer communication network , which $ k $ nodes should we immunize ( or monitor , or remove ) , to make it as robust as possible against a computer virus attack ? This problem , referred to as the node immunization problem , is the core building block in many high-impact applications , ranging from public health , cybersecurity to viral marketing . A central component in node immunization is to find the best $ k $ bridges of a given graph . In this setting , we typically want to determine the relative importance of a node ( or a set of nodes ) within the graph , for example , how valuable ( as a bridge ) a person or a group of persons is in a social network . First of all , we propose a novel bridging score $ \Delta \lambda $ , inspired by immunology , and we show that its results agree with intuition for several realistic settings . Since the straightforward way to compute $ \Delta \lambda $ is computationally intractable , we then focus on the computational issues and propose a surprisingly efficient way ( $ O ( nk^2+m ) $ ) to estimate it . Experimental results on real graphs show that ( 1 ) the proposed bridging score gives mining results consistent with intuition ; and ( 2 ) the proposed fast solution is up to seven orders of magnitude faster than straightforward alternatives .
2K_dev_778	How does malware propagate ? Does it form spikes over time ? Does it resemble the propagation pattern of benign files , such as software patches ? Does it spread uniformly over countries ? How long does it take for a URL that distributes malware to be detected and shut down ? In this work , we answer these questions by analyzing patterns from 22 million malicious ( and benign ) files , found on 1.6 million hosts worldwide during the month of June 2011 . We conduct this study using the WINE database available at Symantec Research Labs . Additionally , we explore the research questions raised by sampling on such large databases of executables ; the importance of studying the implications of sampling is twofold : First , sampling is a means of reducing the size of the database hence making it more accessible to researchers ; second , because every such data collection can be perceived as a sample of the real world . Finally , we discover the SharkFin temporal propagation pattern of executable files , the GeoSplit pattern in the geographical spread of machines that report executables to Symantec 's servers , the Periodic Power Law ( Ppl ) distribution of the life-time of URLs , and we show how to efficiently extrapolate crucial properties of the data from a small sample . To the best of our knowledge , our work represents the largest study of propagation patterns of executables .
2K_dev_779	Given a large cloud of multi-dimensional points , and an off-theshelf outlier detection method , why does it take a week to finish ? After careful analysis , we discovered that duplicate points create subtle issues , that the literature has ignored : if dmax is the multiplicity of the most over-plotted point , typical algorithms are quadratic on dmax . We propose several ways to eliminate the problem ; we report wall-clock times and our time savings ; and we show that our methods give either exact results , or highly accurate approximate ones .
2K_dev_780	Counterfactual Regret Minimization ( CFR ) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games . CFR is an iterative algorithm that repeatedly traverses the game tree , updating regrets at each information set . We introduce an improvement to CFR that prunes any path of play in the tree , and its descendants , that has negative regret . It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive , had that path been explored on every iteration . The new algorithm maintains CFR 's convergence guarantees while making iterations significantly fastereven if previously known pruning techniques are used in the comparison . This improvement carries over to CFR+ , a recent variant of CFR . Experiments show an order of magnitude speed improvement , and the relative speed improvement increases with the size of the game .
2K_dev_781	Influence maximization is a problem of maximizing the aggregate adoption of products , technologies , or even beliefs . Most past algorithms leveraged an assumption of submodularity that captures diminishing returns to scale . While submodularity is natural in many domains , early stages of innovation adoption are often better characterized by convexity , which is evident for renewable technologies , such as rooftop solar . We formulate a dynamic influence maximization problem under increasing returns to scale over a finite time horizon , in which the decision maker faces a budget constraint . We propose a simple algorithm in this model which chooses the best time period to use up the entire budget ( called Best-Stage ) , and prove that this policy is optimal in a very general setting . We also propose a heuristic algorithm for this problem of which Best-Stage decision is a special case . Additionally , we experimentally verify that the proposed `` best-time '' algorithm remains quite effective even as we relax the assumptions under which optimality can be proved . However , we find that when we add a `` learning-by-doing '' effect , in which the adoption costs decrease not as a function of time , but as a function of aggregate adoption , the `` best-time '' policy becomes suboptimal , and is significantly outperformed by our more general heuristic .
2K_dev_782	Pareto efficiency is a widely used property in solution concepts for cooperative and non -- cooperative game -- theoretic settings and , more generally , in multi -- objective problems . However , finding or even approximating ( when the objective functions are not convex ) the Pareto curve is hard . Most of the literature focuses on computing concise representations to approximate the Pareto curve or on exploiting evolutionary approaches to generate approximately Pareto efficient samples of the curve . In this paper , we show that the Pareto curve of a bimatrix game can be found exactly in polynomial time and that it is composed of a polynomial number of pieces . Furthermore , each piece is a quadratic function . We use this result to provide algorithms for game-theoretic solution concepts that incorporate Pareto efficiency .
2K_dev_783	Extensive-form games are a powerful tool for modeling a large range of multiagent scenarios . However , most solution algorithms require discrete , finite games . In contrast , many real-world domains require modeling with continuous action spaces . This is usually handled by heuristically discretizing the continuous action space without solution quality bounds . In this paper we address this issue . Leveraging recent results on abstraction solution quality , we develop the first framework for providing bounds on solution quality for discretization of continuous action spaces in extensive-form games . For games where the error is Lipschitz-continuous in the distance of a continuous point to its nearest discrete point , we show that a uniform discretization of the space is optimal . When the error is monotonically increasing in distance to nearest discrete point , we develop an integer program for finding the optimal discretization when the error is described by piecewise linear functions . This result can further be used to approximate optimal solutions to general monotonic error functions . Finally we discuss how our theory applies to several practical problems for which no solution quality bounds could be derived before .
2K_dev_784	Kidney exchange , where candidates with organ failure trade incompatible but willing donors , is a life-saving alternative to the deceased donor waitlist , which has inadequate supply to meet demand . While fielded kidney exchanges see huge benefit from altruistic kidney donors ( who give an organ without a paired needy candidate ) , a significantly higher medical risk to the donor deters similar altruism with livers . In this paper , we begin by proposing the idea of liver exchange , and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level . We then explore cross-organ donation where kidneys and livers can be bartered for each other . We show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges ; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool . We support this result experimentally on demographically accurate multi-organ exchanges . We conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view .
2K_dev_785	The human brain is widely hypothesized to construct inner beliefs about how the world works . It is thought that we need this conception to coordinate our movements and anticipate rapid events that go on around us . A driver , for example , needs to predict how the car should behave in response to every turn of the steering wheel and every tap on the brake . But on icy roads , these predictions will often not reflect how the car would behave . Applying the brakes sharply in these conditions could send the car skidding uncontrollably rather than stopping . In general , a mismatch between ones inner beliefs and reality is thought to cause errors and accidents . Yet this compelling hypothesis has not yet been fully investigated . Golub et al . investigated this hypothesis by conducting a brain-machine interface experiment . In this experiment , neural signals from the brains of two rhesus macaques were recorded using arrays of electrodes and translated into movements of a cursor on a computer screen . The monkeys were then trained to mentally move the cursor to hit targets on the screen . The monkeys cursor movements were remarkably precise . In fact , the experiment showed that the monkeys could internally predict their cursor movements just as a driver predicts how a car will move when turning the steering wheel . These findings indicate that the monkeys have likely developed inner beliefs to predict how their neural signals drive the cursor , and that these beliefs helped coordinate their performance . In addition , when the monkeys did make mistakes , their neural signals were not entirely wrongin fact they were typically consistent with the monkeys inner beliefs about how the cursor moves . A mismatch between these inner beliefs and reality explained most of the monkeys mistakes . The brain constructs such inner beliefs over time through experience and learning . To study this learning process , Golub et al . next conducted an experiment in which the cursor moved in a way that was substantially different from the monkeys inner beliefs . This experiment uncovered that , during the course of learning , the monkeys inner beliefs realigned to better match the movements of the new cursor . Taken together , this work provides a framework for understanding how the brain transforms sensory information into instructions for movement . The findings could also help improve the performance of brain-machine interfaces and suggest how we can learn new skills more rapidly and proficiently in everyday life .
2K_dev_786	A multi-faceted graph defines several facets on a set of nodes . Each facet is a set of edges that represent the relationships between the nodes in a specific context . Mining multi-faceted graphs have several applications , including finding fraudster rings that launch advertising traffic fraud attacks , tracking IP addresses of botnets over time , analyzing interactions on social networks and co-authorship of scientific papers . We propose NeSim , a distributed efficient clustering algorithm that does soft clustering on individual facets . We also propose optimizations to further improve the scalability , the efficiency and the clusters quality . We employ generalpurpose graph-clustering algorithms in a novel way to discover communities across facets . Due to the qualities of NeSim , we employ it as a backbone in the distributed MuFace algorithm , which discovers multi-faceted communities . We evaluate the proposed algorithms on several real and synthetic datasets , where NeSim is shown to be superior to MCL , JP and AP , the well-established clustering algorithms . We also report the success stories of MuFace in finding advertisement click rings .
2K_dev_787	The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm . We introduce a distributed version of the most commonly used equilibrium-finding algorithm , counterfactual regret minimization ( CFR ) , which enables CFR to scale to dramatically larger abstractions and numbers of cores . The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint . We introduce an algorithm for generating such abstractions while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and the earth-mover's-distance similarity metric . Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency . Prior approaches run slowly on this architecture . Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency . Finally , we introduce a family of post-processing techniques that outperform prior ones . We applied these techniques to generate an agent for two-player no-limit Texas Hold'em . It won the 2014 Annual Computer Poker Competition , beating each opponent with statistical significance .
2K_dev_788	How can we succinctly describe a million-node graph with a few simple sentences ? Given a large graph , how can we find its most `` important '' structures , so that we can summarize it and easily visualize it ? How can we measure the `` importance '' of a set of discovered subgraphs in a large graph ? Starting with the observation that real graphs often consist of stars , bipartite cores , cliques , and chains , our main idea is to find the most succinct description of a graph in these `` vocabulary '' terms . To this end , we first mine candidate subgraphs using one or more graph partitioning algorithms . Next , we identify the optimal summarization using the minimum description length MDL principle , picking only those subgraphs from the candidates that together yield the best lossless compression of the graph-or , equivalently , that most succinctly describe its adjacency matrix .
2K_dev_789	We propose a method for generating interpretable descriptions of inputs that cause faults in high-dimensional software interfaces . Our method models the set of fault-triggering inputs as a Cartesian product and identifies this set by actively querying the system under test . The active sampling scheme is very efficient in the common case that few fields in the interface are relevant to causing the fault . This scheme also solves the problem of efficiently finding sufficient examples to model rare faults , which is problematic for other learning-based methods . Compared to other techniques , ours requires no parameter turning or post-processing in order to produce useful results . We analyze the method qualitatively , theoretically , and empirically . An experimental evaluation demonstrates superior performance and reliability compared to a basic decision tree approach . We also briefly discuss how the method has assisted in debugging a commercial autonomous ground vehicle system .
2K_dev_790	Latent Variable Models ( LVMs ) are a large family of machine learning models providing a principled and effective way to extract underlying patterns , structure and knowledge from observed data . Due to the dramatic growth of volume and complexity of data , several new challenges have emerged and can not be effectively addressed by existing LVMs : ( 1 ) How to capture long-tail patterns that carry crucial information when the popularity of patterns is distributed in a power-law fashion ? ( 2 ) How to reduce model complexity and computational cost without compromising the modeling power of LVMs ? ( 3 ) How to improve the interpretability and reduce the redundancy of discovered patterns ? To addresses the three challenges discussed above , we develop a novel regularization technique for LVMs , which controls the geometry of the latent space during learning to enable the learned latent components of LVMs to be diverse in the sense that they are favored to be mutually different from each other , to accomplish long-tail coverage , low redundancy , and better interpretability . We propose a mutual angular regularizer ( MAR ) to encourage the components in LVMs to have larger mutual angles . The MAR is non-convex and non-smooth , entailing great challenges for optimization . To cope with this issue , we derive a smooth lower bound of the MAR and optimize the lower bound instead . We show that the monotonicity of the lower bound is closely aligned with the MAR to qualify the lower bound as a desirable surrogate of the MAR . Using neural network ( NN ) as an instance , we analyze how the MAR affects the generalization performance of NN . On two popular latent variable models -- - restricted Boltzmann machine and distance metric learning , we demonstrate that MAR can effectively capture long-tail patterns , reduce model complexity without sacrificing expressivity and improve interpretability .
2K_dev_791	We propose a facial alignment algorithm that is able to jointly deal with the presence of facial pose variation , partial occlusion of the face , and varying illumination and expressions . Our approach proceeds from sparse to dense landmarking steps using a set of specific models trained to best account for the shape and texture variation manifested by facial landmarks and facial shapes across pose and various expressions . We also propose the use of a novel $ \ell _1 $ -regularized least squares approach that we incorporate into our shape model , which is an improvement over the shape model used by several prior Active Shape Model ( ASM ) based facial landmark localization algorithms . Our approach is compared against several state-of-the-art methods on many challenging test datasets and exhibits a higher fitting accuracy on all of them .
2K_dev_792	Motivated by problems such as molecular energy prediction , we derive an ( improper ) kernel between geometric inputs , that is able to capture the relevant rotational and translation invariances in geometric data . Since many physical simulations based upon geometric data produce derivatives of the output quantity with respect to the input positions , we derive an approach that incorporates derivative information into our kernel learning . We further show how to exploit the low rank structure of the resulting kernel matrices to speed up learning . Finally , we evaluated the method in the context of molecular energy prediction , showing good performance for modeling previously unseen molecular configurations . Integrating the approach into a Bayesian optimization , we show substantial improvement over the state of the art in molecular energy optimization .
2K_dev_793	The leading approach for computing strong game-theoretic strategies in large imperfect-information games is to first solve an abstracted version of the game offline , then perform a table lookup during game play . We consider a modification to this approach where we solve the portion of the game that we have actually reached in real time to a greater degree of accuracy than in the initial computation . We call this approach endgame solving . Theoretically , we show that endgame solving can produce highly exploitable strategies in some games ; however , we show that it can guarantee a low exploitability in certain games where the opponent is given sufficient exploitative power within the endgame . Furthermore , despite the lack of a general worst-case guarantee , we describe many benefits of endgame solving . We present an efficient algorithm for performing endgame solving in large imperfect-information games , and present a new variance-reduction technique for evaluating the performance of an agent that uses endgame solving . Experiments on no-limit Texas Hold'em show that our algorithm leads to significantly stronger performance against the strongest agents from the 2013 AAAI Annual Computer Poker Competition .
2K_dev_794	The proliferation of mobile technologies makes it possible for mobile advertisers to go beyond the realtime snapshot of the static location and contextual information about consumers . In this study , we propose a novel mobile advertising strategy that leverages full information on consumers offline moving trajectories . To evaluate the effectiveness of this strategy , we design a large-scale randomized field experiment in a large shopping mall in Asia based on 83,370 unique user responses for two weeks in 2014 . We found the new mobile trajectory-based advertising is significantly more effective for focal advertising store compared to several existing baselines . It is especially effective in attracting highincome consumers . Interestingly , it becomes less effective during the weekend . This indicates closely targeted mobile ads may constrict consumer focus and significantly reduce the impulsive purchase behavior . Our finding suggests marketers should carefully design mobile advertising strategy , depending on different business contexts .
2K_dev_795	We propose a new method called the Pokerface for extreme face illumination normalization . The Pokerface is a two-phase approach . It first aims at maximizing the minimum gap between adjacently-valued pixels while keeping the partial ordering of the pixels in the face image under extreme illumination condition , an intuitive effort based on order theory to unveil the underlying structure of a dark image . This optimization can be formulated as a feasibility search problem and can be efficiently solved by linear programming . It then smooths the intermediate representation by repressing the energy of the gradient map . The smoothing step is carried out by total variation minimization and sparse approximation . The illumination normalized faces using our proposed Pokerface not only exhibit very high fidelity against neutrally illuminated face , but also allow for a significant improvement in face verification experiments using even the simplest classifier . Simultaneously achieving high level of faithfulness and expressiveness is very rare among other methods . These conclusions are drawn after benchmarking our algorithm against 22 prevailing illumination normalization techniques on both the CMU Multi-PIE database and Extended YaleB database that are widely adopted for face illumination problems .
2K_dev_796	Living organisms adapt to challenges through evolution and adaptation . This has proven to be a key difficulty in developing therapies , since the organisms develop resistance . I propose the wild idea of steering evolution/adaptation strategicallyusing computational game theory for ( typically incomplete-information ) multistage games and opponent exploitation techniques . A sequential contingency plan for steering is constructed computationally for the setting at hand . In the biological context , the opponent ( e.g. , a disease ) has a systematic handicap because it evolves myopically . This can be exploited by computing trapping strategies that cause the opponent to evolve into states where it can be handled effectively . Potential application classes include therapeutics at the population , individual , and molecular levels ( drug design ) , as well as cell repurposing and synthetic biology .
2K_dev_797	Content-based medical image retrieval ( CBMIR ) is an active research area for disease diagnosis and treatment but it can be problematic given the small visual variations between anatomical structures . We propose a retrieval method based on a bag-of-visual-words ( BoVW ) to identify discriminative characteristics between different medical images with Pruned Dictionary based on Latent Semantic Topic description . We refer to this as the PD-LST retrieval . Our method has two main components . First , we calculate a topic-word significance value for each visual word given a certain latent topic to evaluate how the word is connected to this latent topic . The latent topics are learnt , based on the relationship between the images and words , and are employed to bridge the gap between low-level visual features and high-level semantics . These latent topics describe the images and words semantically and can thus facilitate more meaningful comparisons between the words . Second , we compute an overall-word significance value to evaluate the significance of a visual word within the entire dictionary . We designed an iterative ranking method to measure overall-word significance by considering the relationship between all latent topics and words . The words with higher values are considered meaningful with more significant discriminative power in differentiating medical images . We evaluated our method on two public medical imaging datasets and it showed improved retrieval accuracy and efficiency .
2K_dev_798	A system and method for locating , tracking , and monitoring resource in large-scale facilities is disclosed herein . The system is based on a sensor network and is efficient , scalable , and requires only short-range communication . The system allows for sensor-to-sensor communication as well as the traditional sensor-to-anchor communication to effectively eliminate long-range communications . In order to perform resource localization and tracking , the present invention pairs each resource with an inexpensive , low-powered sensor possessing minimal communication and computation capabilities . The sensors communicate with only nearby resources or anchors and those resources communicate with their nearby resources or anchors until a wireless , linked network of resources and anchors is formed .
2K_dev_799	Deep learning ( DL ) has achieved notable successes in many machine learning tasks . A number of frameworks have been developed to expedite the process of designing and training deep neural networks ( DNNs ) , such as Caffe , Torch and Theano . Currently they can harness multiple GPUs on a single machine , but are unable to use GPUs that are distributed across multiple machines ; as even average-sized DNNs can take days to train on a single GPU with 100s of GBs to TBs of data , distributed GPUs present a prime opportunity for scaling up DL . However , the limited bandwidth available on commodity Ethernet networks presents a bottleneck to distributed GPU training , and prevents its trivial realization . To investigate how to adapt existing frameworks to efficiently support distributed GPUs , we propose Poseidon , a scalable system architecture for distributed inter-machine communication in existing DL frameworks . We integrate Poseidon with Caffe and evaluate its performance at training DNNs for object recognition . Poseidon features three key contributions that accelerate DNN training on clusters : ( 1 ) a three-level hybrid architecture that allows Poseidon to support both CPU-only and GPU-equipped clusters , ( 2 ) a distributed wait-free backpropagation ( DWBP ) algorithm to improve GPU utilization and to balance communication , and ( 3 ) a structure-aware communication protocol ( SACP ) to minimize communication overheads . We empirically show that Poseidon converges to same objectives as a single machine , and achieves state-of-art training speedup across multiple models and well-established datasets using a commodity GPU cluster of 8 nodes ( e.g . 4.5x speedup on AlexNet , 4x on GoogLeNet , 4x on CIFAR-10 ) . On the much larger ImageNet22K dataset , Poseidon with 8 nodes achieves better speedup and competitive accuracy to recent CPU-based distributed systems such as Adam and Le et al. , which use 10s to 1000s of nodes .
2K_dev_800	We investigate the welfare implications and the allocative effects of different consumer data '' handling regimes in online targeted advertisin g. We develop a three '' players model that includes firms , consumers , and an intermediary `` the ad exchange and analyze three scenarios that differ in the type of consumers ' data available during the targeting : a case in which only the horizontal information ( consumers ' brand preferences ) is available ; a case in which only vertical information ( consumers ' purchasing power ) is available ; a case in which both pieces of information are available . We find that there exist conditions under which the intermediary obtains the highest proportion of benefits from targeting and , in general , the intermediary 's incentives regarding the type of consumer information to be used for targeting are misaligned with the incentives of firms and/or consumers . Furthermore , consumers ' surplus from targeting is higher when only specific types of personal information are made available during the targeting process .
2K_dev_801	The kernel-based nonparametric approach proposed by Nguyen , Wainwright , and Jordan is further investigated for decentralized detection . In contrast with the uniform kernel used in the previous work , a weighted kernel is proposed , where weight parameters serve to selectively incorporate sensors information into the fusion centers decision rule based on quality of sensors observations . Furthermore , weight parameters also serve as sensor selection parameters with nonzero parameters corresponding to sensors being selected . By introducing the $ l_1 $ regularization on weight parameters into the risk minimization framework , sensor selection is jointly performed with decision rules for sensors and the fusion center with the resulting optimal decision rule having only sparse nonzero weight parameters . A gradient projection algorithm and a Gauss-Seidel algorithm are developed to solve the risk minimization problem , which is nonconvex , and both algorithms are shown to converge to critical points . Conditions on the sample complexity to guarantee asymptotically small estimation error are characterized based on analysis of Rademacher complexity . Connection between the probability of error and the risk function is also studied . Numerical results are provided to demonstrate the advantages and properties of the proposed approach based on weighted kernel .
2K_dev_802	We present Tomo , a wearable , low-cost system using Electrical Impedance Tomography ( EIT ) to recover the interior impedance geometry of a user 's arm . This is achieved by measuring the cross-sectional impedances between all pairs of eight electrodes resting on a user 's skin . Our approach is sufficiently compact and low-powered that we integrated the technology into a prototype wrist- and armband , which can monitor and classify gestures in real-time . We conducted a user study that evaluated two gesture sets , one focused on gross hand gestures and another using thumb-to-finger pinches . Our wrist location achieved 97 % and 87 % accuracies on these gesture sets respectively , while our arm location achieved 93 % and 81 % . We ultimately envision this technique being integrated into future smartwatches , allowing hand gestures and direct touch manipulation to work synergistically to support interactive tasks on small screens .
2K_dev_803	Next-generation information technologies will process unprecedented amounts of loosely structured data that overwhelm existing computing systems . N3XT improves the energy efficiency of abundant-data applications 1,000-fold by using new logic and memory technologies , 3D integration with fine-grained connectivity , and new architectures for computation immersed in memory .
2K_dev_804	The rise of big data has led to new demands for machine learning ( ML ) systems to learn complex models , with millions to billions of parameters , that promise adequate capacity to digest massive datasets and offer powerful predictive analytics ( such as high-dimensional latent features , intermediate representations , and decision functions ) thereupon . In order to run ML algorithms at such scales , on a distributed cluster with tens to thousands of machines , it is often the case that significant engineering efforts are requiredand one might fairly ask whether such engineering truly falls within the domain of ML research . Taking the view that big ML systems can benefit greatly from ML-rooted statistical and algorithmic insightsand that ML researchers should therefore not shy away from such systems designwe discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions . These principles and strategies span a continuum from application , to engineering , and to theoretical research and development of big ML systems and architectures , with the goal of understanding how to make them efficient , generally applicable , and supported with convergence and scaling guarantees . They concern four key questions that traditionally receive little attention in ML research : How can an ML program be distributed over a cluster ? How can ML computation be bridged with inter-machine communication ? How can such communication be performed ? What should be communicated between machines ? By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs , and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks , we present opportunities for ML researchers and practitioners to further shape and enlarge the area that lies between ML and systems .
2K_dev_805	Semantic search or text-to-video search in video is a novel and challenging problem in information and multimedia retrieval . Existing solutions are mainly limited to text-to-text matching , in which the query words are matched against the user-generated metadata . This kind of text-to-text search , though simple , is of limited functionality as it provides no understanding about the video content . This paper presents a state-of-the-art system for event search without any user-generated metadata or example videos , known as text-to-video search . The system relies on substantial video content understanding and allows for searching complex events over a large collection of videos . The proposed text-to-video search can be used to augment the existing text-to-text search for video . The novelty and practicality are demonstrated by the evaluation in NIST TRECVID 2014 , where the proposed system achieves the best performance . We share our observations and lessons in building such a state-of-the-art system , which may be instrumental in guiding the design of the future system for video search and analysis .
2K_dev_806	We present a credit-based matching mechanism for dynamic barter marketsand kidney exchange in particularthat is both strategy proof and efficient , that is , it guarantees truthful disclosure of donor-patient pairs from the transplant centers and results in the maximum global matching . Furthermore , the mechanism is individually rational in the sense that , in the long run , it guarantees each transplant center more matches than the center could have achieved alone . The mechanism does not require assumptions about the underlying distribution of compatibility graphsa nuance that has previously produced conflicting results in other aspects of theoretical kidney exchange . Our results apply not only to matching via 2-cycles : the matchings can also include cycles of any length and altruist-initiated chains , which is important at least in kidney exchanges . The mechanism can also be adjusted to guarantee immediate individual rationality at the expense of economic efficiency , while preserving strategy proofness via the credits . This circumvents a well-known impossibility result in static kidney exchange concerning the existence of an individually rational , strategy-proof , and maximal mechanism . We show empirically that the mechanism results in significant gains on data from a national kidney exchange that includes 59 % of all US transplant centers .
2K_dev_807	The leading approach for solving large imperfect-information games is automated abstraction followed by running an equilibrium-finding algorithm . We introduce a distributed version of the most commonly used equilibrium-finding algorithm , counterfactual regret minimization ( CFR ) , which enables CFR to scale to dramatically larger abstractions and numbers of cores . The new algorithm begets constraints on the abstraction so as to make the pieces running on different computers disjoint . We introduce an algorithm for generating such abstractions while capitalizing on state-of-the-art abstraction ideas such as imperfect recall and earth-mover 's distance . Our techniques enabled an equilibrium computation of unprecedented size on a supercomputer with a high inter-blade memory latency . Prior approaches run slowly on this architecture . Our approach also leads to a significant improvement over using the prior best approach on a large shared-memory server with low memory latency . Finally , we introduce a family of post-processing techniques that outperform prior ones . We applied these techniques to generate an agent for two-player no-limit Texas Hold'em , called Tartanian7 , that won the 2014 Annual Computer Poker Competition , beating each opponent with statistical significance .
2K_dev_808	In this paper , we propose a novel joint formulation of feature-based active contour ( FAC ) and prior shape constraints ( CS ) for lips/mouth segmentation in the wild . Our proposed SC-FAC model is able to robustly segment the lips/mouth that belongs to a given mouth shape space while minimizing the energy functional . The shape space is defined by a 2D Modified Active Shape Model ( MASM ) whereas the active contour model is based on the Chan-Vese functional . Our SC-FAC energy functional is able to overcome the drawback of noise while minimizing the fitting forces under the shape constraints . We conducted our experiments on images captured under challenging conditions such as varying illumination , low contrast , facial expression , low resolution , blurring , wearing beard/moustache and cosmetic affection from the MBGC , VidTIMIT , JAFFE , and LFW databases . The results from our studies indicate that the proposed SC-FAC model is reliable and accurately perform prior shape weak object segmentation . The average performance of the mouth segmentation using proposed SC-FAC on 1918 images from the MBGC database under different illuminations , expressions , and complex background reaches to a Precision of 91.30 % , a Recall of 91.32 % and an F-measure of 90.62 % . HighlightsPropose a novel joint formulation of contour and shape for lips segmentation.Robustly segment lips under challenging environment and complex background.The energy functional is composed of 5 terms based on the Chan-Vese functional.The shape space is defined by a 2D.Experiments are conducted from the MBGC , VidTIMIT , JAFFE , and LFW databases .
2K_dev_809	Bayesian theory has provided a compelling conceptualization for perceptual inference in the brain . Central to Bayesian inference is the notion of statistical priors . To understand the neural mechanisms of Bayesian inference , we need to understand the neural representation of statistical regularities in the natural environment . In this paper , we investigated empirically how statistical regularities in natural 3D scenes are represented in the functional connectivity of disparity-tuned neurons in the primary visual cortex of primates . We applied a Boltzmann machine model to learn from 3D natural scenes , and found that the units in the model exhibited cooperative and competitive interactions , forming a disparity association field , analogous to the contour association field . The cooperative and competitive interactions in the disparity association field are consistent with constraints of computational models for stereo matching . In addition , we simulated neurophysiological experiments on the model , and found the results to be consistent with neurophysiological data in terms of the functional connectivity measurements between disparity-tuned neurons in the macaque primary visual cortex . These findings demonstrate that there is a relationship between the functional connectivity observed in the visual cortex and the statistics of natural scenes . They also suggest that the Boltzmann machine can be a viable model for conceptualizing computations in the visual cortex and , as such , can be used to predict neural circuits in the visual cortex from natural scene statistics .
2K_dev_810	AbstractEngineering analysis to quantify the effects of earthquake forces on the structural strength of components requires determining the damage mode and severity of the components . The analysis requires strength computations and visual damage assessments , which are information intensive , potentially error-prone , and slow . This study develops a building-information-modeling ( BIM ) based approach to support the engineering analysis of reinforced concrete structures . In the proposed approach , the damage information is represented along with the geometric , topological , and structural information . Transformation and reasoning mechanisms are proposed to utilize the information contained in the BIM to perform strength analysis and visual assessment tasks . The approach is validated on a case study building , which contains 42 damaged piers and spandrels .
2K_dev_811	We present the first model of optimal voting under adversarial noise . From this viewpoint , voting rules are seen as error-correcting codes : their goal is to correct errors in the input rankings and recover a ranking that is close to the ground truth . We derive worst-case bounds on the relation between the average accuracy of the input votes , and the accuracy of the output ranking . Empirical results from real data show that our approach produces significantly more accurate rankings than alternative approaches .
2K_dev_812	The use of deductive techniques , such as theorem provers , has several advantages in safety verification of hybrid systems . There is often a gap , however , between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide . To address this deficiency we present an extension to the deductive verification framework of differential dynamic logic that allows the theorem prover KeYmaera to locally reason about behaviors by leveraging forward invariant sets provided by external methods , such as numerical techniques and designer insights . Our key contribution is a new inference rule , the forward invariant cut rule , introduced into the proof calculus of KeYmaera . We demonstrate the cut rule in action on an example involving an automotive powertrain control systems , in which we make use of a simulation-driven numerical technique to compute a local barrier function .
2K_dev_813	Kernel methods are ubiquitous tools in machine learning . They have proven to be effective in many domains and tasks . Yet , kernel methods often require the user to select a predefined kernel to build an estimator with . However , there is often little reason for the a priori selection of a kernel . Even if a universal approximating kernel is selected , the quality of the finite sample estimator may be greatly effected by the choice of kernel . Furthermore , when directly applying kernel methods , one typically needs to compute a $ N \times N $ Gram matrix of pairwise kernel evaluations to work with a dataset of $ N $ instances . The computation of this Gram matrix precludes the direct application of kernel methods on large datasets . In this paper we introduce Bayesian nonparmetric kernel ( BaNK ) learning , a generic , data-driven framework for scalable learning of kernels . We show that this framework can be used for performing both regression and classification tasks and scale to large datasets . Furthermore , we show that BaNK outperforms several other scalable approaches for kernel learning on a variety of real world datasets .
2K_dev_814	When solving extensive-form games with large action spaces , typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective . When this occurs , a procedure is needed to interpret actions of the opponent that fall outside of our abstraction ( by mapping them to actions in our abstraction ) . This is called an action translation mapping . Prior action translation mappings have been based on heuristics without theoretical justification . We show that the prior mappings are highly exploitable and that most of them violate certain natural desiderata . We present a new mapping that satisfies these desiderata and has significantly lower exploitability than the prior mappings . Furthermore , we observe that the cost of this worst-case performance benefit ( low exploitability ) is not high in practice ; our mapping performs competitively with the prior mappings against no-limit Texas Hold'em agents submitted to the 2012 Annual Computer Poker Competition . We also observe several paradoxes that can arise when performing action abstraction and translation ; for example , we show that it is possible to improve performance by including suboptimal actions in our abstraction and excluding optimal actions .
2K_dev_815	This paper studies sound proof rules for checking positive invariance of algebraic and semi-algebraic sets , that is , sets satisfying polynomial equalities and those satisfying finite boolean combinations of polynomial equalities and inequalities , under the flow of polynomial ordinary differential equations . Problems of this nature arise in formal verification of continuous and hybrid dynamical systems , where there is an increasing need for methods to expedite formal proofs . We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of benchmarks . The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting .
2K_dev_816	Dr. Eric Xing is a Professor of Machine Learning in the School of Computer Science at Carnegie Mellon University , and Director of the CMU/UPMC Center for Machine Learning and Health . His principal research interests lie in the development of machine learning and statistical methodology , and large-scale computational system and architecture ; especially for solving problems involving automated learning , reasoning , and decision-making in high-dimensional , multimodal , and dynamic possible worlds in artificial , biological , and social systems . Professor Xing received a Ph.D. in Molecular Biology from Rutgers University , and another Ph.D. in Computer Science from UC Berkeley . He servers ( or served ) as an associate editor of the Annals of Applied Statistics ( AOAS ) , the Journal of American Statistical Association ( JASA ) , the IEEE Transaction of Pattern Analysis and Machine Intelligence ( PAMI ) , the PLoS Journal of Computational Biology , and an Action Editor of the Machine Learning Journal ( MLJ ) , the Journal of Machine Learning Research ( JMLR ) . He was a member of the DARPA Information Science and Technology ( ISAT ) Advisory Group , a recipient of the NSF Career Award , the Sloan Fellowship , the United States Air Force Young Investigator Award , and the IBM Open Collaborative Research Award . He was the Program Chair of ICML 2014 .
2K_dev_817	How can we efficiently decompose a tensor into sparse factors , when the data do not fit in memory ? Tensor decompositions have gained a steadily increasing popularity in data-mining applications ; however , the current state-of-art decomposition algorithms operate on main memory and do not scale to truly large datasets . In this work , we propose P ar C ube , a new and highly parallelizable method for speeding up tensor decompositions that is well suited to produce sparse approximations . Experiments with even moderately large data indicate over 90p sparser outputs and 14 times faster execution , with approximation error close to the current state of the art irrespective of computation and memory requirements . We provide theoretical guarantees for the algorithms correctness and we experimentally validate our claims through extensive experiments , including four different real world datasets ( E nron , L bnl , F acebook and N ell ) , demonstrating its effectiveness for data-mining practitioners . In particular , we are the first to analyze the very large N ell dataset using a sparse tensor decomposition , demonstrating that P ar C ube enables us to handle effectively and efficiently very large datasets . Finally , we make our highly scalable parallel implementation publicly available , enabling reproducibility of our work .
2K_dev_818	In the U.S. , the current practice of analyzing the structural integrity of embankment dams relies primarily on manual a posteriori analysis of instrument data by engineers , leaving much room for improvement through the application of advanced data analysis techniques . In this research , different types of anomaly detection techniques are examined in an effort to propose which data analytics are appropriate for various anomaly scenarios as well as piezometer locations . Moreover , both the parametric ( Auto Regressive AR and Moving Principal Component Analysis MPCA ) and nonparametric ( Kullback-Leibler Divergence KL ) techniques are applied in order to test if the widely-held assumptions about piezometer data , i.e. , linearity between piezometer data and pool levels , as well as normally distributed piezometer data , are necessary in the anomaly detection task . In general , KL performs better than MPCA and AR , and delivers more consistent results throughout the different piezometers and anomaly scenarios . Given that KL is a nonparametric technique , the authors conclude that the prior assumptions about piezometer data do not always provide the best performance for anomaly prediction .
2K_dev_819	Modeling cell shape variation is critical to our understanding of cell biology . Previous work has demonstrated the utility of nonrigid image registration methods for the construction of nonparametric nuclear shape models in which pairwise deformation distances are measured between all shapes and are embedded into a low-dimensional shape space . Using these methods , we explore the relationship between cell shape and nuclear shape . We find that these are frequently dependent on each other and use this as the motivation for the development of combined cell and nuclear shape space models , extending nonparametric cell representations to multiple-component three-dimensional cellular shapes and identifying modes of joint shape variation . We learn a first-order dynamics model to predict cell and nuclear shapes , given shapes at a previous time point . We use this to determine the effects of endogenous protein tags or drugs on the shape dynamics of cell lines and show that tagged C1QBP reduces the correlation between cell and nuclear shape . To reduce the computational cost of learning these models , we demonstrate the ability to reconstruct shape spaces using a fraction of computed pairwise distances . The open-source tools provide a powerful basis for future studies of the molecular basis of cell organization .
2K_dev_820	An electronic device includes a touch-sensitive surface , for example a touch pad or touch screen . The user physically interacts with the touch-sensitive surface , producing touch events . The resulting interface actions taken depend at least in part on the touch type . The type of touch is determined in part based on capacitive image data produced by the touch event .
2K_dev_821	Building automation systems are believed to hold the key to significantly reducing the average energy consumption of our residential and commercial building stock , which in the U.S. is responsible for 41 % of the total annual energy use in 2014 . As these systems become more widespread and inexpensive , the complexity and challenges associated with their installation , maintenance and upkeep will increase . One of the primary challenges is the generation and update of the meta-data associated with the sensors and control points distributed throughout the facility . Previous research has attempted to reduce the human input required to perform these activities , by leveraging different signal processing and statistical analysis approaches to infer the sensor types and locations from measurements and/or tags obtained through a BAS . However , because of the relatively small sample size , the feasibility of applying these type approaches on large buildings , as well as their generalizability , remain as unsolved questions . In this paper , we propose a meta-data inference framework to learn from BAS measurement data in a semi-automated way . Furthermore , we evaluate the framework on two large buildings instrumented with thousands sensors and show the feasibility of applying data driven approaches in the real world . We present the results of our study and provide recommendations for future work in this area .
2K_dev_822	This paper considers the dynamics of edges in a network . The Dynamic Bond Percolation ( DBP ) process models , through stochastic local rules , the dependence of an edge $ ( a , b ) $ in a network on the states of its neighboring edges . Unlike previous models , DBP does not assume statistical independence between different edges . In applications , this means for example that failures of transmission lines in a power grid are not statistically independent , or alternatively , relationships between individuals ( dyads ) can lead to changes in other dyads in a social network . We consider the time evolution of the probability distribution of the network state , the collective states of all the edges ( bonds ) , and show that it converges to a stationary distribution . We use this distribution to study the emergence of global behaviors like consensus ( i.e. , catastrophic failure or full recovery of the entire grid ) or coexistence ( i.e. , some failed and some operating substructures in the grid ) . In particular , we show that , depending on the local dynamical rule , different network substructures , such as hub or triangle subgraphs , are more prone to failure .
2K_dev_823	Image clustering and visual codebook learning are two fundamental problems in computer vision and they are tightly related . On one hand , a good codebook can generate effective feature representations which largely affect clustering performance . On the other hand , class labels obtained from image clustering can serve as supervised information to guide codebook learning . Traditionally , these two processes are conducted separately and their correlation is generally ignored . In this paper , we propose a Double Layer Gaussian Mixture Model ( DLGMM ) to simultaneously perform image clustering and codebook learning . In DLGMM , two tasks are seamlessly coupled and can mutually promote each other . Cluster labels and codebook are jointly estimated to achieve the overall best performance . To incorporate the spatial coherence between neighboring visual patches , we propose a Spatially Coherent DL-GMM which uses a Markov Random Field to encourage neighboring patches to share the same visual word label . We use variational inference to approximate the posterior of latent variables and learn model parameters . Experiments on two datasets demonstrate the effectiveness of two models .
2K_dev_824	While prior research has studied the motivations of individuals to consume content on social media platforms , limited work exists on how contributors are motivated to create content . We examine the role of peer influence in content production on YouTube , where content creators are competing for attention . Given that content creation efforts are driven not only by their personal preferences , but also by the content creation decisions of others in the network neighbors , we develop a new method to analyze discrete choice decisions ( such as creating content or not ) in a networked environment with panel data . We face a novel set of big data challenges , i.e. , both statistical and quantitative , in estimating peer influence . We face computational challenges in that we can not reasonably estimate peer influence over the entire YouTube network , which has billions of nodes . We employ graph sampling methods to address this issue . Identification of social influence in large-scale social networks such as YouTube is difficult due to the interdependence in decisions of users , correlations between the video 's observable and unobservable characteristics and attributes over time . These patterns can not be modeled with existing autocorrelation models . We design a new method , the Network Auto-Probit Model with Fixed Effects ( NAFE ) , to identify peer influence among content creators on YouTube . Implications for research and practice are also discussed .
2K_dev_825	Despite the enormous medical impact of cancers and intensive study of their biology , detailed characterization of tumor growth and development remains elusive . This difficulty occurs in large part because of enormous heterogeneity in the molecular mechanisms of cancer progression , both tumor-to-tumor and cell-to-cell in single tumors . Advances in genomic technologies , especially at the single-cell level , are improving the situation , but these approaches are held back by limitations of the biotechnologies for gathering genomic data from heterogeneous cell populations and the computational methods for making sense of those data . One popular way to gain the advantages of whole-genome methods without the cost of single-cell genomics has been the use of computational deconvolution ( unmixing ) methods to reconstruct clonal heterogeneity from bulk genomic data . These methods , too , are limited by the difficulty of inferring genomic profiles of rare or subtly varying clonal subpopulations from bulk data , a problem that can be computationally reduced to that of reconstructing the geometry of point clouds of tumor samples in a genome space . Here , we present a new method to improve that reconstruction by better identifying subspaces corresponding to tumors produced from mixtures of distinct combinations of clonal subpopulations . We develop a nonparametric clustering method based on medoidshift clustering for identifying subgroups of tumors expected to correspond to distinct trajectories of evolutionary progression . We show on synthetic and real tumor copy-number data that this new method substantially improves our ability to resolve discrete tumor subgroups , a key step in the process of accurately deconvolving tumor genomic data and inferring clonal heterogeneity from bulk data .
2K_dev_826	Over the last decade , the looming power wall has spurred a flurry of interest in developing heterogeneous systems with hardware accelerators . The questions , then , are what and how accelerators should be designed , and what software support is required . Our accelerator design approach stems from the observation that many efficient and portable software implementations rely on high performance software libraries with well-established application programming interfaces ( APIs ) . We propose the integration of hardware accelerators on 3D-stacked memory that explicitly targets the memory-bounded operations within high performance libraries . The fixed APIs with limited configurability simplify the design of the accelerators , while ensuring that the accelerators have wide applicability . With our software support that automatically converts library APIs to accelerator invocations , an additional advantage of our approach is that library-based legacy code automatically gains the benefit of memory-side accelerators without requiring a reimplementation . On average , the legacy code using our proposed MEmory Accelerated Library ( MEALib ) improves performance and energy efficiency for individual operations in Intel 's Math Kernel Library ( MKL ) by 38 and 75 , respectively . For a real-world signal processing application that employs Intel MKL , MEALib attains more than 10 better energy efficiency .
2K_dev_827	The study of representations invariant to common transformations of the data is important to learning . Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees . In this paper , we study kernels that are invariant to the unitary group while having theoretical guarantees in addressing practical issues such as ( 1 ) unavailability of transformed versions of labelled data and ( 2 ) not observing all transformations . We present a theoretically motivated alternate approach to the invariant kernel SVM . Unlike previous approaches to the invariant SVM , the proposed formulation solves both issues mentioned . We also present a kernel extension of a recent technique to extract linear unitary-group invariant features addressing both issues and extend some guarantees regarding invariance and stability . We present experiments on the UCI ML datasets to illustrate and validate our methods .
2K_dev_828	The assembly of virus capsids proceeds by a complicated cascade of association and dissociation steps , the great majority of which can not be directly experimentally observed . This has made capsid assembly a rich field for computational models , but there are substantial obstacles to model inference for such systems . Here , we describe progress on fitting kinetic rate constants defining capsid assembly models to experimental data , a difficult data-fitting problem because of the high computational cost of simulating assembly trajectories , the stochastic noise inherent to the models , and the limited and noisy data available for fitting . We evaluate the merits of data-fitting methods based on derivative-free optimization ( DFO ) relative to gradient-based methods used in prior work . We further explore the advantages of alternative data sources through simulation of a model of time-resolved mass spectrometry data , a technology for monitoring bulk capsid assembly that can be expected to provide much richer data than previously used static light scattering approaches . The results show that advances in both the data and the algorithms can improve model inference . More informative data sources lead to high-quality fits for all methods , but DFO methods show substantial advantages on less informative data sources that better represent current experimental practice .
2K_dev_829	We present a convex approach to probabilistic segmentation and modeling of time series data . Our approach builds upon recent advances in multivariate total variation regularization , and seeks to learn a separate set of parameters for the distribution over the observations at each time point , but with an additional penalty that encourages the parameters to remain constant over time . We propose efficient optimization methods for solving the resulting ( large ) optimization problems , and a two-stage procedure for estimating recurring clusters under such models , based upon kernel density estimation . Finally , we show on a number of real-world segmentation tasks , the resulting methods often perform as well or better than existing latent variable models , while being substantially easier to train .
2K_dev_830	We propose the problem of predicting a bundle of goods , where the goods considered is a set of spatial locations that an agent wishes to visit . This typically arises in the tourism setting where attractions can often be bundled and sold as a package to visitors . While the problem of predicting future locations given the current and past trajectories is well-established , we take a radical approach by looking at it from an economic point of view . We view an agent 's past trajectories as revealed preference ( RP ) data , where the choice of locations is a solution to an optimisation problem according to some unknown utility function and subject to the prevailing prices and budget constraint . We approximate the prices and budget constraint as the time costs to finish visiting the chosen locations . We leverage on a recent line of work that has established algorithms to efficiently learn from RP data ( i.e. , recover the utility functions ) and make predictions of future purchasing behaviours . We adopt and adapt those work to our original setting while incorporating techniques from spatiotemporal analysis . We experiment with real-world trajectory data collected from a theme park . Our predictions show improved accuracies in comparison with the baseline methods by at least 20 , one of which comes from the spatiotemporal analysis domain .
2K_dev_831	Given a large collection of co-evolving online activities , such as searches for the keywords `` Xbox '' , `` PlayStation '' and `` Wii '' , how can we find patterns and rules ? Are these keywords related ? If so , are they competing against each other ? Can we forecast the volume of user activity for the coming month ? We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food . We present ECOWEB , ( i.e. , Ecosystem on the Web ) , which is an intuitive model designed as a non-linear dynamical system for mining large-scale co-evolving online activities . Our second contribution is a novel , parameter-free , and scalable fitting algorithm , ECOWEB-FIT , that estimates the parameters of ECOWEB . Extensive experiments on real data show that ECOWEB is effective , in that it can capture long-range dynamics and meaningful patterns such as seasonalities , and practical , in that it can provide accurate long-range forecasts . ECOWEB consistently outperforms existing methods in terms of both accuracy and execution speed .
2K_dev_832	Software lineage refers to the evolutionary relationship among a collection of software . The goal of software lineage inference is to recover the lineage given a set of program binaries . Software lineage can provide extremely useful information in many security scenarios such as malware triage and software vulnerability tracking . In this paper , we systematically study software lineage inference by exploring four fundamental questions not addressed by prior work . First , how do we automatically infer software lineage from program binaries ? Second , how do we measure the quality of lineage inference algorithms ? Third , how useful are existing approaches to binary similarity analysis for inferring lineage in reality , and how about in an idealized setting ? Fourth , what are the limitations that any software lineage inference algorithm must cope with ? Towards these goals we build ILINE , a system for automatic software lineage inference of program binaries , and also IEVAL , a system for scientific assessment of lineage quality . We evaluated ILINE on two types of lineage -- straight line and directed acyclic graph -- with large-scale real-world programs : 1,777 goodware spanning over a combined 110 years of development history and 114 malware with known lineage collected by the DARPA Cyber Genome program . We used IEVAL to study seven metrics to assess the diverse properties of lineage . Our results reveal that partial order mismatches and graph arc edit distance often yield the most meaningful comparisons in our experiments . Even without assuming any prior information about the data sets , ILINE proved to be effective in lineage inference -- it achieves a mean accuracy of over 84 % for goodware and over 72 % for malware in our data sets .
2K_dev_833	With the rise of online social networks and smartphones that record the user 's location , a new type of online social network has gained popularity during the last few years , the so called Location-based Social Networks ( LBSNs ) . In such networks , users voluntarily share their location with their friends via a check-in . In exchange they get recommendations tailored to their particular location as well as special deals that businesses offer when users check-in frequently . LBSNs started as specialized platforms such as Gowalla and Foursquare , however their immense popularity has led online social networking giants like Facebook to adopt this functionality . The spatial aspect of LBSNs directly ties the physical with the online world , creating a very rich ecosystem where users interact with their friends both online as well as declare their physical ( co- ) presence in various locations . Such a rich environment calls for novel analytic tools that can model the aforementioned types of interactions . In this work , we propose to model and analyze LBSNs using Tensors and Tensor Decompositions , powerful analytical tools that have enjoyed great growth and success in fields like Machine Learning , Data Mining , and Signal Processing alike . By doing so , we identify tightly knit , hidden communities of users and locations which they frequent . In addition to Tensor Decompositions , we use Signal Processing tools that have been previously used in Direction of Arrival ( DOA ) estimations , in order to study the temporal dynamics of hidden communities in LBSNs .
2K_dev_834	This work focuses on the task of finding latent vector representations of the words in a corpus . In particular , we address the issue of what to do when there are multiple languages in the corpus . Prior work has , among other techniques , used canonical correlation analysis to project pre-trained vectors in two languages into a common space . We propose a simple and scalable method that is inspired by the notion that the learned vector representations should be invariant to translation between languages . We show empirically that our method outperforms prior work on multilingual tasks , matches the performance of prior work on monolingual tasks , and scales linearly with the size of the input data ( and thus the number of languages being embedded ) .
2K_dev_835	It is our great pleasure to welcome you to the 17th International ACM SIGACCESS Conference on Computers and Accessibility ( ASSETS 2015 ) . The ASSETS conference has a long-standing tradition of exploring the design , evaluation , and use of computing and information technologies to benefit people with disabilities and older adults . This year , in its 17th edition , ASSETS 2015 is again the premier forum for presenting innovative research on mainstream and specialized assistive technologies , accessible computing , and assistive applications of computer , network , and information technologies . The call for papers attracted submissions from Australia ( 1 ) , Austria ( 4 ) , Belgium ( 2 ) , Brazil ( 11 ) , Canada ( 2 ) , China ( 1 ) , France ( 8 ) , Germany ( 4 ) , India ( 2 ) , Ireland ( 1 ) , Israel ( 2 ) , Italy ( 3 ) , Japan ( 1 ) , Netherlands ( 2 ) , Portugal ( 8 ) , Puerto Rico ( 1 ) , Republic of Korea ( 1 ) , Saudi Arabia ( 1 ) , Singapore ( 1 ) , Spain ( 4 ) , Sweden ( 1 ) , Switzerland ( 1 ) , United Kingdom ( 12 ) , and the United States ( 53 ) . Our growing community generated more technical paper submissions than ever before at an ASSETS conference , and we had the immense pleasure to work with our technical program committee to decide which would be accepted to the conference . We received 127 submissions from which we selected 30 for inclusion in the program , for an acceptance rate of 23 % . For the first time at ASSETS , decisions were made by consensus of the program committee members , who discussed the papers extensively both online and via video calls . The program committee also reviewed 81 posters submissions ( 49 accepted ) and 18 demo submissions ( 15 accepted ) . The program committee consisted of 42 senior members of our committee who worked with 5 advanced doctoral systems to produce a total of 381 reviews . We would like to thank them for their incredibly diligent and considered work . The ASSETS 2015 program is organized as follows : On Monday ( Day 1 ) , the program starts with a keynote presentation by Dr. Jon Schull who is the President of the Enable Community Foundation . Dr. Schull 's keynote is titled as `` Enabling the future : Crowdsourced 3D-printed prosthetics as model for open source assistive technology innovation and mutual aid '' and he will tell us about the fantastic work that he and e-NABLE are doing in the area of crowdsourcing 3D-printed prosthetic hands . Following the keynote , we will have a series of technical paper sessions organized around Speech Technologies , Reading and Language , and New Perspectives on Accessibility Research . The day will end with a Reception and hands-on Demo Session with awards for the best demos ! On Tuesday ( Day 2 ) , the program will continue with sessions on Non-Visual Access to Graphics , Sign Language , 3D Interaction , Accessibility and Work , and Exercise and Physical Activity . We will also hear short talks from the finalists in the ACM Student Research Competition . The day will conclude with a special reception at a historic Lisbon Museum supported by the Paciello Group . Awards for the Best Paper and the Best Student Paper will be given at this reception . On Wednesday ( Day 3 ) , the program will conclude with technical sessions on Making Speech Accessible and Usable , Accessibility and Cognition , Text Input , and Improving Non-Visual Access . We will then turn over the reigns to next year 's chairs to tell you about ASSETS 2016 .
2K_dev_836	We propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery . Our approach is grammarless we directly learn the bracketing structure of a given sentence without using a grammar model . The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples . Although finding the minimal latent tree is NP-hard in general , for the case of projective trees we find that it can be found using bilexical parsing algorithms . Empirically , our algorithm performs favorably compared to the constituent context model of Klein and Manning ( 2002 ) without the need for careful initialization .
2K_dev_837	Mahadev Satyanarayanan ( Satya ) presented his thoughts on `` The Evolution of Memory and File Systems '' . He observed that over a 60-year period , there have been four drivers of progress : the quests for scale , performance , transparency , and robustness . At the dawn of computing , the quest for scale was dominant . Easing the memory limitations of early computers was crucial to the growth of computing and the creation of new applications , because memory was so scarce and so expensive . That quest has been phenomenally successful . On a cost per bit basis , volatile and persistent memory technologies have improved by nearly 13 orders of magnitude . The quest for performance has been dominated by the growing gap between processor performance and memory performance . This gap has been most apparent since the use of DRAM technology by the early 1980s , but it was already a serious issue 20 years before that in the era of core memory . Over time , memory hierarchies of increasing depth have improved average case performance by exploiting temporal and spatial locality . These have been crucial in overcoming the processor-memory performance gap , with clever prefetching and write-back techniques also playing important roles . For the first decade or so , the price of improving scale and performance was the need to rewrite software as computers were replaced by new ones . By the early 1960s , this cost was becoming significant . Over time , as people costs have increased relative to hardware costs , disruptive software changes have become unacceptable . This has led to the quest for transparency . In its System/360 , IBM pioneered the concept of an invariant architecture with multiple implementations at different price/performance points . The principle of transparent management of data across levels of a memory hierarchy , which we broadly term `` caching '' , was pioneered at the software level by the Atlas computer in the early 1960s . At the hardware level , it was demonstrated first in the IBM System 360 Model 85 in 1968 . Since then , caching has been applied at virtually every system level and is today perhaps the most ubiquitous and powerful systems technique for achieving scale , performance and transparency . By the late 1960s , as computers began to used in mission-critical contexts , the negative impact of hardware and software failures escalated . This led to he emergence of techniques to improve robustness even at the possible cost of performance or storage efficiency . The concept of separate address spaces emerged partly because it isolated the consequences of buggy software . Improved resilience to buggy sofware has also been one of the reasons that memory and file systems have remained distinct , even though systems based on the single-level storage concept have been proposed and experimentally demonstrated . In addition , to cope with hardware , software and networking failures , technqiues such as RAID , software replication , and disconnected operation emerged . The quest for robustness continues to rise in importance as the cost of failures increases relative to memory and storage costs . In closing , Satya commented on recent predictions that the classic hierarchical file system will soon be extinct . He observed that such predictions are not new . Classic file systems may be overlaid by non-hierarchical interfaces that uses different abstractions ( such as the Android interface for Java applications ) . However , they will continue to be important for unstructured data that must be preserved for very long periods of time . Satya observed that the deep reasons for the longevity of the hierarchical file system model were articulated in broad terms by Herb Simon in his 1962 work , `` The Architecture of Complexity '' . Essentially , hierarchy arises due to the cognitive limitations of the human mind . File system implementations have evolved to be a good fit for these cognitive limitations . They are likely to be with us for a very long time .
2K_dev_838	We present Epsilon , a system for general convex programming using fast linear and proximal operators . As with existing convex programming frameworks , users specify convex optimization problems using a natural grammar for mathematical expressions , composing functions in a way that is guaranteed to be convex by the rules of disciplined convex programming . Given such an input , the Epsilon compiler transforms the optimization problem into a mathematically equivalent form consisting only of functions with ecient proximal operators|an intermediate representation we refer to as prox-ane form . By reducing problems to this form , Epsilon enables solving general convex problems using a large library of fast proximal and linear operators ; numerical examples on many popular problems from statistics and machine learning show that this often improves running times by an order of magnitude or more vs. existing approaches based on conic solvers .
2K_dev_839	This paper focuses on recursive nonlinear least-squares parameter estimation in multiagent networks , where the individual agents observe sequentially over time an independent and identically distributed time-series consisting of a nonlinear function of the true but unknown parameter corrupted by noise . A distributed recursive estimator of the consensus+innovations type , namely $ \mathcal { CIWNLS } $ , is proposed , in which the agents update their parameter estimates at each observation sampling epoch in a collaborative way by simultaneously processing the latest locally sensed information ( innovations ) and the parameter estimates from other agents ( consensus ) in the local neighborhood conforming to a prespecified interagent communication topology . Under rather weak conditions on the connectivity of the interagent communication and a global observability criterion , it is shown that , at every network agent , $ \mathcal { CIWNLS } $ leads to consistent parameter estimates . Furthermore , under standard smoothness assumptions on the local observation functions , the distributed estimator is shown to yield order-optimal convergence rates , i.e. , as far as the order of pathwise convergence is concerned , the local parameter estimates at each agent are as good as the optimal centralized nonlinear least-squares estimator that requires access to all the observations across all the agents at all times . To benchmark the performance of the $ \mathcal { CIWNLS } $ estimator with that of the centralized nonlinear least-squares estimator , the asymptotic normality of the estimate sequence is established , and the asymptotic covariance of the distributed estimator is evaluated .
2K_dev_840	Curriculum learning ( CL ) or self-paced learning ( SPL ) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training . The two methods share a similar conceptual learning paradigm , but differ in specific learning schemes . In CL , the curriculum is predetermined by prior knowledge , and remain fixed thereafter . Therefore , this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner . In SPL , the curriculum is dynamically determined to adjust to the learning pace of the leaner . However , SPL is unable to deal with prior knowledge , rendering it prone to overfitting . In this paper , we discover the missing link between CL and SPL , and propose a unified framework named self-paced curriculum leaning ( SPCL ) . SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training . In comparison to human education , SPCL is analogous to `` instructor-student-collaborative '' learning mode , as opposed to `` instructor-driven '' in CL or `` student-driven '' in SPL . Empirically , we show that the advantage of SPCL on two tasks .
2K_dev_841	Image and video classification research has made great progress through the development of handcrafted local features and learning based features . These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history . However , they are typically viewed as distinct approaches . In this paper , we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness . As an example , we study the problem of designing efficient video feature learning algorithms for action recognition . We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks ( CNNs ) share the same convolution-pooling network structure . We then propose a two-stream Convolutional ISA ( ConvISA ) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm . Through custom designed network structures for pixels and optical flow , our method also reflects distinctive characteristics of these two data sources . Our experimental results on standard action recognition benchmarks show that by focusing on the structure of CNNs , rather than end-to-end training methods , we are able to design an efficient and powerful video feature learning algorithm .
2K_dev_842	Early detection and precise characterization of emerging topics in text streams can be highly useful in applications such as timely and targeted public health interventions and discovering evolving regional business trends . Many methods have been proposed for detecting emerging events in text streams using topic modeling . However , these methods have numerous shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams . In this paper , we describe Semantic Scan ( SS ) that has been developed specifically to overcome these shortcomings in detecting new spatially compact events in text streams . Semantic Scan integrates novel contrastive topic modeling with online document assignment and principled likelihood ratio-based spatial scanning to identify emerging events with unexpected patterns of keywords hidden in text streams . This enables more timely and accurate detection and characterization of anomalous , spatially localized emerging events . Semantic Scan does not require manual intervention or labeled training data , and is robust to noise in real-world text data since it identifies anomalous text patterns that occur in a cluster of new documents rather than an anomaly in a single new document . We compare Semantic Scan to alternative state-of-the-art methods such as Topics over Time , Online LDA , and Labeled LDA on two real-world tasks : ( i ) a disease surveillance task monitoring free-text Emergency Department chief complaints in Allegheny County , and ( ii ) an emerging business trend detection task based on Yelp reviews . On both tasks , we find that Semantic Scan provides significantly better event detection and characterization accuracy than competing approaches , while providing up to an order of magnitude speedup .
2K_dev_843	This paper addresses the problem of scalable optimization for L1-regularized conditional Gaussian graphical models . Conditional Gaussian graphical models generalize the well-known Gaussian graphical models to conditional distributions to model the output network influenced by conditioning input variables . While highly scalable optimization methods exist for sparse Gaussian graphical model estimation , state-of-the-art methods for conditional Gaussian graphical models are not efficient enough and more importantly , fail due to memory constraints for very large problems . In this paper , we propose a new optimization procedure based on a Newton method that efficiently iterates over two sub-problems , leading to drastic improvement in computation time compared to the previous methods . We then extend our method to scale to large problems under memory constraints , using block coordinate descent to limit memory usage while achieving fast convergence . Using synthetic and genomic data , we show that our methods can solve one million dimensional problems to high accuracy in a little over a day on a single machine .
2K_dev_844	Unease over data privacy will retard consumer acceptance of IoT deployments . The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud . This is a direct consequence of the over-centralization of today 's cloud-based IoT hub designs . We propose a solution that interposes a locally-controlled software component called a privacy mediator on every raw sensor stream . Each mediator is in the same administrative domain as the sensors whose data is being collected , and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain . This solution necessitates a logical point of presence for mediators within the administrative boundaries of each organization . Such points of presence are provided by cloudlets , which are small locally-administered data centers at the edge of the Internet that can support code mobility . The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility .
2K_dev_845	We develop a parallel variational inference ( VI ) procedure for use in data-distributed settings , where each machine only has access to a subset of data and runs VI independently , without communicating with other machines . This type of `` embarrassingly parallel '' procedure has recently been developed for MCMC inference algorithms ; however , in many cases it is not possible to directly extend this procedure to VI methods without requiring certain restrictive exponential family conditions on the form of the model . Furthermore , most existing ( nonparallel ) VI methods are restricted to use on conditionally conjugate models , which limits their applicability . To combat these issues , we make use of the recently proposed nonparametric VI to facilitate an embarrassingly parallel VI procedure that can be applied to a wider scope of models , including to nonconjugate models . We derive our embarrassingly parallel VI algorithm , analyze our method theoretically , and demonstrate our method empirically on a few nonconjugate models .
2K_dev_846	Introduction : We document a funded effort to bridge the gap between constrained scientific challenges of public health surveillance and methodologies from academia and industry . Component tasks are the collection of epidemiologists use case problems , multidisciplinary consultancies to refine them , and dissemination of problem requirements and shareable datasets . We describe an initial use case and consultancy as a concrete example and challenge to developers . Materials and Methods : Supported by the Defense Threat Reduction Agency Biosurveillance Ecosystem project , the International Society for Disease Surveillance formed an advisory group to select tractable use case problems and convene inter-disciplinary consultancies to translate analytic needs into well-defined problems and to promote development of applicable solution methods . The initial consultancys focus was a problem originated by the North Carolina Department of Health and its NC DETECT surveillance system : Derive a method for detection of patient record clusters worthy of follow-up based on free-text chief complaints and without syndromic classification . Results : Direct communication between public health problem owners and analytic developers was informative to both groups and constructive for the solution development process . The consultancy achieved refinement of the asyndromic detection challenge and of solution requirements . Participants summarized and evaluated solution approaches and discussed dissemination and collaboration strategies . Conclusions : A solution meeting the specification of the use case described above could improve human monitoring efficiency with expedited warning of events requiring follow-up , including otherwise overlooked events with no syndromic indicators . This approach can remove obstacles to collaboration with efficient , minimal data-sharing and without costly overhead .
2K_dev_847	In this paper , we define the problem of coreference resolution in text as one of clustering with pairwise constraints where human experts are asked to provide pairwise constraints ( pairwise judgments of coreferentiality ) to guide the clustering process . Positing that these pairwise judgments are easy to obtain from humans given the right context , we show that with significantly lower number of pairwise judgments and feature-engineering effort , we can achieve competitive coreference performance . Further , we describe an active learning strategy that minimizes the overall number of such pairwise judgments needed by asking the most informative questions to human experts at each step of coreference resolution . We evaluate this hypothesis and our algorithms on both entity and event coreference tasks and on two languages .
2K_dev_848	The Restricted Isometric Property ( R.I.P . ) is a very important condition for recovering sparse vectors from high dimensional space . Traditional methods often rely on R.I.P or its relaxed variants . However , in real applications , features are often correlated to each other , which makes these assumptions too strong to be useful . In this paper , we study the sparse recovery problem in which the feature matrix is strictly non-R.I.P . We prove that when features exhibit cluster structures , which often happens in real applications , we are able to recover the sparse vector consistently . The consistency comes from our proposed density correction algorithm , which removes the variance of estimated cluster centers using cluster density . The proposed algorithm converges geometrically , achieves nearly optimal recovery bound O ( s2 log ( d ) ) where s is the sparsity and d is the nominal dimension .
2K_dev_849	We propose two well-motivated ranking-based methods to enhance the performance of current state-of-the-art human activity recognition systems . First , as an improvement over the classic power normalization method , we propose a parameter-free ranking technique called rank normalization ( RaN ) . RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD . Second , inspired by curriculum learning , we introduce a training-free re-ranking technique called multi-class iterative re-ranking ( MIR ) . MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly . We demonstrate that our methods significantly improve the performance of state-of-the-art motion features on six real-world datasets .
2K_dev_850	Welcome to the 14th IEEE/ACM International Conference on Information Processing in Sensor Networks ( IPSN 2015 ) in Seattle , USA . IPSN is one of the flagship research conferences of the networked sensing and control community , and continues to serve as a major forum for cuttingedge research results .
2K_dev_851	Despite their small size , mobile devices are able to perform tasks of creation , information and communication with unprecedented ease . However , diminutive screens and buttons mar the user experience , and otherwise prevent us from realizing the full potential of computing on the go . In this dissertation , I will first discuss strategies I have pursued to expand and enrich interaction . For example , fingers have many `` modes '' - they do not just poke , as contemporary touchscreen interaction would suggest , but also scratch , flick , knock , rub , and grasp , to name a few . I will then highlight an emergent shift in computing : from mobile devices we carry to using everyday surfaces for interaction , including tables , walls , furniture and even our skin , bringing computational power ever closer to users . This evolution brings significant new challenges in sensing and interaction design . For example , the human body is not only incredibly irregular and dynamic , but also comes in more than six billion different models . However , along with these challenges also come exciting new opportunities for more powerful , intuitive and intimate computing experiences .
2K_dev_852	Propagation of contagion in networks depends on the graph topology . This paper is concerned with studying the time-asymptotic behavior of the extended contact processes on static , undirected , finite-size networks . This is a contact process with nonzero exogenous infection rate ( also known as the { \epsilon } -SIS , { \epsilon } susceptible-infected-susceptible , model [ 1 ] ) . The only known analytical characterization of the equilibrium distribution of this process is for complete networks . For large networks with arbitrary topology , it is infeasible to numerically solve for the equilibrium distribution since it requires solving the eigenvalue-eigenvector problem of a matrix that is exponential in N , the size of the network . We show that , for a certain range of the network process parameters , the equilibrium distribution of the extended contact process on arbitrary , finite-size networks is well approximated by the equilibrium distribution of the scaled SIS process , which we derived in closed-form in prior work . We confirm this result with numerical simulations comparing the equilibrium distribution of the extended contact process with that of a scaled SIS process . We use this approximation to decide , in polynomial-time , which agents and network substructures are more susceptible to infection by the extended contact process .
2K_dev_853	The widespread use of social networks enables the rapid diffusion of information , e.g. , news , among users in very large communities . It is a substantial challenge to be able to observe and understand such diffusion processes , which may be modeled as networks that are both large and dynamic . A key tool in this regard is data summarization . However , few existing studies aim to summarize graphs/networks for dynamics . Dynamic networks raise new challenges not found in static settings , including time sensitivity and the needs for online interestingness evaluation and summary traceability , which render existing techniques inapplicable . We study the topic of dynamic network summarization : how to summarize dynamic networks with millions of nodes by only capturing the few most interesting nodes or edges over time , and we address the problem by finding interestingness-driven diffusion processes . Based on the concepts of diffusion radius and scope , we define interestingness measures for dynamic networks , and we propose OSNet , an online summarization framework for dynamic networks . We report on extensive experiments with both synthetic and real-life data . The study offers insight into the effectiveness and design properties of OSNet .
2K_dev_854	Session types provide a means to prescribe the communication behavior between concurrent message-passing processes . However , in a distributed setting , some processes may be written in languages that do not support static typing of sessions or may be compromised by a malicious intruder , violating invariants of the session types . In such a setting , dynamically monitoring communication between processes becomes a necessity for identifying undesirable actions . In this paper , we show how to dynamically monitor communication to enforce adherence to session types in a higher-order setting . We present a system of blame assignment in the case when the monitor detects an undesirable action and an alarm is raised . We prove that dynamic monitoring does not change system behavior for welltyped processes , and that one of an indicated set of possible culprits must have been compromised in case of an alarm .
2K_dev_855	The focus of HCOMP 2014 was the crowd worker . While crowdsourcing is motivated by the promise of leveraging people 's intelligence and diverse skillsets in computational processes , the human aspects of this workforce are all too often overlooked . Instead , workers are frequently viewed as interchangeable components that can be statistically managed to eek out reasonable outputs . We are quickly moving past and rejecting these notions , and beginning to understand that it is sometimes the very abstractions that we introduce to make human computation feasible , e.g. , abstracting humans behind APIs or isolating workers from others in order to ensure independent input , that can lead to the problems that we then set about trying to solve , e.g. , poor or inconsistent quality work . Creating a brighter future for crowd work will require new socio-technical systems that not only decompose tasks , recruit and coordinate workers , and make sense of results , but also find interesting tasks for people to contribute to , structure tasks so that workers learn from them as they go , and eventually automate mundane parts of work . Research in artificial intelligence will be vital for achieving this future .
2K_dev_856	We propose a distributed approach to train deep neural networks ( DNNs ) , which has guaranteed convergence theoretically and great scalability empirically : close to 6 times faster on instance of ImageNet data set when run with 6 machines . The proposed scheme is close to optimally scalable in terms of number of machines , and guaranteed to converge to the same optima as the undistributed setting . The convergence and scalability of the distributed setting is shown empirically across di ? erent datasets ( TIMIT and ImageNet ) and machine learning tasks ( image classi ? cation and phoneme extraction ) . The convergence analysis provides novel insights into this complex learning scheme , including : 1 ) layerwise convergence , and 2 ) convergence of the weights in probability .
2K_dev_857	We compute approximate solutions to L0 regularized linear regression problems using convex relaxation of L1 regularization , also known as the Lasso , as an initialization step . Our algorithm , the Lass-0 ( `` Lass-zero '' ) , uses a computationally efficient stepwise search to determine a locally optimal L0 solution given any L1 regularization solution . We present theoretical results of consistency under orthogonality and appropriate handling of redundant features . Empirically , we demonstrate on synthetic data that the Lass-0 solutions are closer to the true sparse support than L1 regularization models . Additionally , in real-world data Lass-0 finds more parsimonious solutions that L1 regularization while maintaining similar predictive accuracy .
2K_dev_858	Methods and corresponding software for allowing a user to manipulate and interactively explore data intuitively by objectifying the data and allowing the user to apply any one or more simulated physical tools to the objectified data . The data can be any suitable type of data , including multivariate data and graph ( network ) data . In some embodiments , the method displays user-selected charts , such as histograms , scattergrams , and network graphs , in which objectified data points , or simulated physical objects , are attracted to their proper charted locations . In some embodiments , the user can apply one or more simulated physical tools and/or other tools , such as physical-barrier-type filter tools ( e.g. , sieves ) and/or optical filter lens tools , to the simulated physical objects to filter the data . In some embodiments , the user can apply multiple tools , with each tool leaving a visual trace that allows the user to easily retrace their data manipulations .
2K_dev_859	We introduce a technique for furbricating 3D printed hair , fibers and bristles , by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling . Our approach offers a range of design parameters for controlling the properties of single strands and also of hair bundles . We further detail a list of post-processing techniques for refining the behavior and appearance of printed strands . We provide several examples of output , demonstrating the immediate feasibility of our approach using a low cost , commodity printer . Overall , this technique extends the capabilities of 3D printing in a new and interesting way , without requiring any new hardware .
2K_dev_860	One typically proves infeasibility in satisfiability/ constraint satisfaction ( or optimality in integer programming ) by constructing a tree certificate . However , deciding how to branch in the search tree is hard , and impacts search time drastically . We explore the power of a simple paradigm , that of throwing random darts into the assignment space and then using information gathered by that dart to guide what to do next . This method seems to work well when the number of short certificates of infeasibility is moderate , suggesting that the overhead of throwing darts is more than paid for by the information gained by these darts .
2K_dev_861	This volume contains the papers selected for presentation at the 13th Workshop on Privacy in the Electronic Society ( WPES 2014 ) , held in Scottsdale , Arizona , USA , on November 3 , 2014 , in conjunction with the ACM Conference on Computer and Communications Security ( ACM CCS ) . In response to the workshop 's call for papers , 67 papers were submitted to the conference from 25 different countries . Each paper has been reviewed by three members of the program committee , who considered its significance , novelty , technical quality , and interest for the research and industy communities in their evaluation . The program committee 's work was carried out electronically , yielding intensive discussion . Of the submitted papers , the program committee accepted 17 full papers and 9 short papers ( an acceptance rate of 39 % ) for presentation at the workshop .
2K_dev_862	People with disabilities have always engaged the people around them inorder to circumvent inaccessible situations , allowing them to live moreindependently and get things done in their everyday lives . Increasingconnectivity is allowing this approach to be extended to wherever andwhenever it is needed . Technology can leverage this human work forceto accomplish tasks beyond the capabilities of computers , increasinghow accessible the world is for people with disabilities . This articleoutlines the growth of online human support , outlines a number ofprojects in this space , and presents a set of challenges and opportunitiesfor this work going forward .
2K_dev_863	Smartwatches are becoming increasingly powerful , but limited input makes completing complex tasks impractical . Our WearWrite system introduces a new paradigm for enabling a watch user to contribute to complex tasks , not through new hardware or input methods , but by directing a crowd to work on their behalf from their wearable device . WearWrite lets authors give writing instructions and provide bits of expertise and big picture directions from their smartwatch , while crowd workers actually write the document on more powerful devices . We used this approach to write three academic papers , and found it was effective at producing reasonable drafts .
2K_dev_864	In applied fields , practitioners hoping to apply causal structure learning or causal orientation algorithms face an important question : which independence test is appropriate for my data ? In the case of real-valued iid data , linear dependencies , and Gaussian error terms , partial correlation is sufficient . But once any of these assumptions is modified , the situation becomes more complex . Kernel-based tests of independence have gained popularity to deal with nonlinear dependencies in recent years , but testing for conditional independence remains a challenging problem . We highlight the important issue of non-iid observations : when data are observed in space , time , or on a network , nearby observations are likely to be similar . This fact biases estimates of dependence between variables . Inspired by the success of Gaussian process regression for handling non-iid observations in a wide variety of areas and by the usefulness of the Hilbert-Schmidt Independence Criterion ( HSIC ) , a kernel-based independence test , we propose a simple framework to address all of these issues : first , use Gaussian process regression to control for certain variables and to obtain residuals . Second , use HSIC to test for independence . We illustrate this on two classic datasets , one spatial , the other temporal , that are usually treated as iid . We show how properly accounting for spatial and temporal variation can lead to more reasonable causal graphs . We also show how highly structured data , like images and text , can be used in a causal inference framework using a novel structured input/output Gaussian process formulation . We demonstrate this idea on a dataset of translated sentences , trying to predict the source language .
2K_dev_865	A computer-implemented method includes , in one aspect , accessing historical purchase data that is indicative of items that have been purchased and prices at which each of the items are purchased ; for each of the items specified in the historical purchase data , fitting a customer valuation model to a portion of the historical purchase data that pertains to the item , with the customer valuation model specifying purchase preferences , and with a purchase preference specifying a probability a customer will purchase a specific item at a specific price ; based on fitted customer valuation models for the items in the historical purchase data , identifying a priceable bundle , with the priceable bundle including at least two of the items specified in the historical purchase data ; and applying updated prices to one or more of ( i ) the priceable bundle , and ( ii ) the items included in the priceable bundle .
2K_dev_866	In computer architecture research and development , simulation is a powerful way of acquiring and predicting processor behaviors . While architectural simulation has been extensively utilized for computer performance evaluation , design space exploration , and computer architecture assessment , it still suffers from the high computational costs in practice . Specifically , the total simulation time is determined by the simulators raw speed and the total number of simulated instructions . The simulators speed can be improved by enhanced simulation infrastructures ( e.g. , simulators with high-level abstraction , parallel simulators , and hardware-assisted simulators ) . Orthogonal to these work , recent studies also managed to significantly reduce the total number of simulated instructions with a slight loss of accuracy . Interestingly , we observe that most of these work are built upon statistical techniques . This survey presents a comprehensive review to such studies and proposes a taxonomy based on the sources of reduction . In addition to identifying the similarities and differences of state-of-the-art approaches , we further discuss insights gained from these studies as well as implications for future research .
2K_dev_867	An energy efficient task scheduler for use with a processor that provides multiple reduced energy use modes . In one embodiment , a system for executing tasks includes a processor and a task scheduler . The processor provides a plurality of different reduced energy use modes . The task scheduler is executable by the processor to schedule execution a plurality of sleep tasks . Each of the sleep tasks corresponds to a different one of the reduced energy use modes . The task scheduler is executable by the processor to execute each of the sleep tasks , and as part of the execution of the sleep task to : place the processor in the reduced energy use mode corresponding to the sleep task , and exit the corresponding reduced energy use mode at suspension of the sleep task .
2K_dev_868	An indoor ultrasonic location tracking system that can utilize standard audio speakers to provide indoor ranging information to modern mobile devices like smartphones and tablets . The method uses a communication scheme based on linearly increasing frequency modulated chirps in the audio bandwidth just above the human hearing frequency range where mobile devices are still sensitive . The method uses gradual frequency and amplitude changes that minimize human perceivable ( psychoacoustic ) artifacts derived from the non-ideal impulse response of audio speakers . Chirps also benefit from Pulse Compression , which improves ranging resolution and resilience to both Doppler shifts and multi-path propagation that plague indoor environments . The method supports the decoding of multiple unique identifier packets simultaneously . A Time-Difference-of-Arrival pseudo-ranging technique allows for localization without explicit synchronization with the broadcasting infrastructure . An alternate received signal strength indicator based localization technique allows less accurate localization at the benefit of sparser transmission infrastructure .
2K_dev_869	Online discussion forums are complex webs of overlapping subcommunities ( macrolevel structure , across threads ) in which users enact different roles depending on which subcommunity they are participating in within a particular time point ( microlevel structure , within threads ) . This sub-network structure is implicit in massive collections of threads . To uncover this structure , we develop a scalable algorithm based on stochastic variational inference and leverage topic models ( LDA ) along with mixed membership stochastic block ( MMSB ) models . We evaluate our model on three large-scale datasets , Cancer-ThreadStarter ( 22K users and 14.4K threads ) , Cancer-NameMention ( 15.1K users and 12.4K threads ) and StackOverFlow ( 1.19 million users and 4.55 million threads ) . Qualitatively , we demonstrate that our model can provide useful explanations of microlevel and macrolevel user presentation characteristics in different communities using the topics discovered from posts . Quantitatively , we show that our model does better than MMSB and LDA in predicting user reply structure within threads . In addition , we demonstrate via synthetic data experiments that the proposed active sub-network discovery model is stable and recovers the original parameters of the experimental setup with high probability .
2K_dev_870	neurons , the arm has nonlinear dynamics , and multiple modalities of sensory feedback contribute to control . A braincomputer interface ( BCI ) is a well-defined sensorimotor loop with key simplifying advantages that address each of these challenges , while engaging similar cognitive processes . As a result , BCI is becoming recognized as a powerful tool for basic scientific studies of sensorimotor control . Here , we describe the benefits of BCI for basic scientific inquiries and review recent BCI studies that have uncovered new insights into the neural mechanisms underlying sensorimotor control . Sensorimotor control engages cognitive processes such as prediction , learning , and multisensory integration . Understanding the neural mechanisms underlying these cognitive processes with arm reaching is challenging because we currently record only a fraction of the relevant
2K_dev_871	This paper introduces reasoning about lawful behavior as an important computational thinking skill and provides examples from a novel introductory programming curriculum using Microsoft 's Kodu Game Lab . We present an analysis of assessment data showing that rising 5th and 6th graders can understand the lawfulness of Kodu programs . We also discuss some misconceptions students may develop about Kodu , their causes , and potential remedies .
2K_dev_872	We introduce LegionTools , a toolkit and interface for managing large , synchronous crowds of online workers for experiments . This poster contributes the design and implementation of a state-of-the-art crowd management tool , along with a publicly-available , open-source toolkit that future system builders can use to coordinate synchronous crowds of online workers for their systems and studies . We describe the toolkit itself , along with the underlying design rationale , in order to make it clear to the community of system builders at UIST when and how this tool may be beneficial to their project . We also describe initial deployments of the system in which workers were synchronously recruited to support real-time crowdsourcing systems , including the largest synchronous recruitment and routing of workers from Mechanical Turk that we are aware of . While the version of LegionTools discussed here focuses on Amazon 's Mechanical Turk platform , it can be easily extended to other platforms as APIs become available .
2K_dev_873	Obtaining per-device energy consumption estimates in Non-Intrusive Load Monitoring ( NILM ) has proven to be a challenging task . We present Power Consumption Clustered Non-Intrusive Load Monitoring ( PCC-NILM ) , a relaxation of the NILM problem that estimates the energy consumed by devices operating in different power ranges . The Approximate Power Trace Decomposition Algorithm ( APTDA ) is presented as an unsupervised , data-driven solution to the PCC-NILM problem . We show that reliable energy estimates can be obtained by crowdsourcing the results from using 1,456 event detectors applied to the publicly available BLUED dataset .
2K_dev_874	Recently diversity-inducing regularization methods for latent variable models ( LVMs ) , which encourage the components in LVMs to be diverse , have been studied to address several issues involved in latent variable modeling : ( 1 ) how to capture long-tail patterns underlying data ; ( 2 ) how to reduce model complexity without sacrificing expressivity ; ( 3 ) how to improve the interpretability of learned patterns . While the effectiveness of diversity-inducing regularizers such as the mutual angular regularizer has been demonstrated empirically , a rigorous theoretical analysis of them is still missing . In this paper , we aim to bridge this gap and analyze how the mutual angular regularizer ( MAR ) affects the generalization performance of supervised LVMs . We use neural network ( NN ) as a model instance to carry out the study and the analysis shows that increasing the diversity of hidden units in NN would reduce estimation error and increase approximation error . In addition to theoretical analysis , we also present empirical study which demonstrates that the MAR can greatly improve the performance of NN and the empirical observations are in accordance with the theoretical analysis .
2K_dev_875	We present a scalable Gaussian process model for identifying and characterizing smooth multidimensional changepoints , and automatically learning changes in expressive covariance structure . We use Random Kitchen Sink features to flexibly define a change surface in combination with expressive spectral mixture kernels to capture the complex statistical structure . Finally , through the use of novel methods for additive non-separable kernels , we can scale the model to large datasets . We demonstrate the model on numerical and real world data , including a large spatio-temporal disease dataset where we identify previously unknown heterogeneous changes in space and time .
2K_dev_876	User-generated online reviews can play a significant role in the success of retail products , hotels , restaurants , etc . However , review systems are often targeted by opinion spammers who seek to distort the perceived quality of a product by creating fraudulent reviews . We propose a fast and effective framework , FRAUDEAGLE , for spotting fraudsters and fake reviews in online review datasets . Our method has several advantages : ( 1 ) it exploits the network effect among reviewers and products , unlike the vast majority of existing methods that focus on review text or behavioral analysis , ( 2 ) it consists of two complementary steps ; scoring users and reviews for fraud detection , and grouping for visualization and sensemaking , ( 3 ) it operates in a completely unsupervised fashion requiring no labeled data , while still incorporating side information if available , and ( 4 ) it is scalable to large datasets as its run time grows linearly with network size . We demonstrate the effectiveness of our framework on syntheticand real datasets ; where FRAUDEAGLE successfully reveals fraud-bots in a large online app review database .
2K_dev_877	Networked or telematic music performances take many forms , ranging from small laptop ensembles using local area networks to long-distance musical collaborations using audio and video links . Two important concerns for any networked performance are : ( 1 ) what is the role of communication in the music performance ? In particular , what are the esthetic and pragmatic justifications for performing music at a distance , and ( 2 ) how are the effects of communication latency ameliorated or incorporated into the performance ? A recent project , the Global Net Orchestra , is described . In addition to addressing these two concerns , the technical aspects of the project , which achieved a coordinated performance involving 68 computer musicians , each with their own connection to the network , are described .
2K_dev_878	Given a large graph with several millions or billions of nodes and edges , such as a social network , how can we explore it efficiently and find out what is in the data ? In this demo we present Perseus , a large-scale system that enables the comprehensive analysis of large graphs by supporting the coupled summarization of graph properties and structures , guiding attention to outliers , and allowing the user to interactively explore normal and anomalous node behaviors . Specifically , Perseus provides for the following operations : 1 ) It automatically extracts graph invariants ( e.g. , degree , PageRank , real eigenvectors ) by performing scalable , offline batch processing on Hadoop ; 2 ) It interactively visualizes univariate and bivariate distributions for those invariants ; 3 ) It summarizes the properties of the nodes that the user selects ; 4 ) It efficiently visualizes the induced subgraph of a selected node and its neighbors , by incrementally revealing its neighbors . In our demonstration , we invite the audience to interact with Perseus to explore a variety of multi-million-edge social networks including a Wikipedia vote network , a friendship/foeship network in Slashdot , and a trust network based on the consumer review website Epinions.com .
2K_dev_879	The long-term goal of connecting scales in biological simulation can be facilitated by scale-agnostic methods . We demonstrate that the weighted ensemble ( WE ) strategy , initially developed for molecular simulations , applies effectively to spatially resolved cell-scale simulations . The WE approach runs an ensemble of parallel trajectories with assigned weights and uses a statistical resampling strategy of replicating and pruning trajectories to focus computational effort on difficult-to-sample regions . The method can also generate unbiased estimates of non-equilibrium and equilibrium observables , sometimes with significantly less aggregate computing time than would be possible using standard parallelization . Here , we use WE to orchestrate particle-based kinetic Monte Carlo simulations , which include spatial geometry ( e.g. , of organelles , plasma membrane ) and biochemical interactions among mobile molecular species . We study a series of models exhibiting spatial , temporal and biochemical complexity and show that although WE has important limitations , it can achieve performance significantly exceeding standard parallel simulationby orders of magnitude for some observables .
2K_dev_880	What can a human compute in his/her head that a powerful adversary can not infer ? To answer this question , we define a model of human computation and a measure of security . Then , motivated by the special case of password creation , we propose a collection of well-defined password-generation methods . We show that our password generation methods are humanly computable and , to a well-defined extent , machine uncrackable . For the proof of security , we posit that password generation methods are public , but that the humans privately chosen seed is not , and that the adversary will have observed only a few input-output pairs . Besides the application to password generation , our proposed Human Usability Model ( HUM ) will have other applications .
2K_dev_881	We address the problem of generating video features for action recognition . The spatial pyramid and its variants have been very popular feature models due to their success in balancing spatial location encoding and spatial invariance . Although it seems straightforward to extend spatial pyramid to the temporal domain ( spatio-temporal pyramid ) , the large spatio-temporal diversity of unconstrained videos and the resulting significantly higher dimensional representations make it less appealing . This paper introduces the space-time extended descriptor , a simple but efficient alternative way to include the spatio-temporal location into the video features . Instead of only coding motion information and leaving the spatio-temporal location to be represented at the pooling stage , location information is used as part of the encoding step . This method is a much more effective and efficient location encoding method as compared to the fixed grid model because it avoids the danger of over committing to artificial boundaries and its dimension is relatively low . Experimental results on several benchmark datasets show that , despite its simplicity , this method achieves comparable or better results than spatio-temporal pyramid .
2K_dev_882	Suppose you are a teacher , and have to convey a set of object-property pairs ( 'lions eat meat ' , or 'aspirin is a blood-thinner ' ) . A good teacher will convey a lot of information , with little effort on the student side . Specifically , given a list of objects ( like animals or medical drugs ) and their associated properties , what is the best and most intuitive way to convey this information to the student , without the student being overwhelmed ? A related , harder problem is : how can we assign a numerical score to each lesson plan ( i.e . way of conveying information ) ? Here , we give a formal definition of this problem of forming learning units and we provide a metric for comparing different approaches based on information theory . We also design a multi-pronged algorithm , HYTRA , for this problem . Our proposed HYTRA is scalable ( near-linear in the dataset size ) , it is effective , achieving excellent results on real data , both with respect to our proposed metric , but also with respect to encoding length , and it is intuitive , conforming to well-known educational principles , such as grouping related concepts , and `` comparing '' and `` contrasting '' . Experiments on real and synthetic datasets demonstrate the effectiveness of HYTRA .
2K_dev_883	The engineering analysis for determining the remaining seismic capacity of buildings following earthquakes requires performing structural calculations , observations of the actual damage , and applying extensive engineering judgment . Additionally , the analysis should often be performed under stringent time requirements . This study identifies the information requirements for representing the damage information and performing the visual damage assessment of structural walls . The damage descriptions for seven common damage modes of structural walls were studied by employing the affinity diagramming method . The study showed that the information required to represent the damaged conditions can be grouped under five broad categories and using seventeen damage parameters . A sensitivity analysis showed that the damage parameters have varying degrees of importance . The results of the study can be used to develop formal representation of damage information in information models and potentially allow better allocation of data collection time in the field .
2K_dev_884	Differential dynamic logic is a logic for specifying and verifying safety , liveness , and other properties about models of cyber-physical systems . Theorem provers based on differential dynamic logic have been used to verify safety properties for models of self-driving cars and collision avoidance protocols for aircraft . Unfortunately , these theorem provers do not have explicit proof terms , which makes the implementation of a number of important features unnecessarily complicated without soundness-critical and extra-logical extensions to the theorem prover . Examples include : an unambiguous separation between proof checking and proof search , the ability to extract program traces corresponding to counter-examples , and synthesis of surely-live deterministic programs from liveness proofs for nondeterministic programs . This paper presents a differential dynamic logic with such an explicit representation of proofs . The resulting logic extends both the syntax and semantics of differential dynamic logic with proof terms -- syntactic representations of logical deductions . To support axiomatic theorem proving , the logic allows equivalence rewriting deep within formulas and supports both uniform renaming and uniform substitutions .
2K_dev_885	Different real-world applications have varying definitions of suspicious behaviors . Detection methods often look for the most suspicious parts of the data by optimizing scores , but quantifying the suspiciousness of a behavioral pattern is still an open issue .
2K_dev_886	We study parallel and distributed Frank-Wolfe algorithms ; the former on shared memory machines with mini-batching , and the latter in a delayed update framework . In both cases , we perform computations asynchronously whenever possible . We assume block-separable constraints as in Block-Coordinate Frank-Wolfe ( BCFW ) method ( Lacoste-Julien et al. , 2013 ) , but our analysis subsumes BCFW and reveals problemdependent quantities that govern the speedups of our methods over BCFW . A notable feature of our algorithms is that they do not depend on worst-case bounded delays , but only ( mildly ) on expected delays , making them robust to stragglers and faulty worker threads . We present experiments on structural SVM and Group Fused Lasso , and observe significant speedups over competing state-of-the-art ( and synchronous ) methods .
2K_dev_887	In this work , we introduce Video Question Answering in temporal domain to infer the past , describe the present and predict the future . We present an encoder-decoder approach using Recurrent Neural Networks to learn temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions . We explore approaches for finer understanding of video content using question form of `` fill-in-the-blank '' , and managed to collect 109,895 video clips with duration over 1,000 hours from TACoS , MPII-MD , MEDTest 14 datasets , while the corresponding 390,744 questions are generated from annotations . Extensive experiments demonstrate that our approach significantly outperforms the compared baselines .
2K_dev_888	Mining knowledge from a multimedia database has received increasing attentions recently since huge repositories are made available by the development of the Internet . In this article , we exploit the relations among different modalities in a multimedia database and present a framework for general multimodal data mining problem where image annotation and image retrieval are considered as the special cases . Specifically , the multimodal data mining problem can be formulated as a structured prediction problem where we learn the mapping from an input to the structured and interdependent output variables . In addition , in order to reduce the demanding computation , we propose a new max margin structure learning approach called Enhanced Max Margin Learning ( EMML ) framework , which is much more efficient with a much faster convergence rate than the existing max margin learning methods , as verified through empirical evaluations . Furthermore , we apply EMML framework to develop an effective and efficient solution to the multimodal data mining problem that is highly scalable in the sense that the query response time is independent of the database scale . The EMML framework allows an efficient multimodal data mining query in a very large scale multimedia database , and excels many existing multimodal data mining methods in the literature that do not scale up at all . The performance comparison with a state-of-the-art multimodal data mining method is reported for the real-world image databases .
2K_dev_889	The differential temporal dynamic logic dTL 2 is a logic to specify temporal properties of hybrid systems . It combines differential dynamic logic with temporal logic to reason about the intermediate states reached by a hybrid system . The logic dTL 2 supports some linear time temporal properties of LTL . It extends differential temporal dynamic logic dTL with nested temporalities . We provide a semantics and a proof system for the logic dTL 2 , and show its use- fulness for nontrivial temporal properties of hybrid systems . We take particular care to handle the case of alternating universal dynamic and existential temporal modalities and its dual , solving an open problem formulated in previous work .
2K_dev_890	The interest in distributed control methods for power systems is motivated by the need for scalable solutions to handle the coordination of an increasing number of distributed resources . This paper presents a fully distributed multilevel method to solve the DC Optimal Power Flow problem ( DC-OPF ) . Our proposed approach constitutes a distributed iterative mechanism to solve the first order optimality conditions of the DC-OPF problem using the fact that optimality conditions involve local variable couplings . The proposed distributed structure requires each bus to update a few local variables and exchange information with neighboring buses . Our multilevel distributed approach distributes the computation at several levels , i.e. , nodes , subareas and areas . It allows for synchronous information exchanges , i.e. , after each iteration , at the nodal level and asynchronous communication , i.e. , after multiple iterations , between subareas and areas . To define meaningful subareas , we are using a graph theoretic partitioning method derived from an epidemics model . We compare the performance of the proposed partitioning method over a random partitioning method using the IEEE 118-bus system .
2K_dev_891	We work with the `` NYC Taxi Data Set , '' a historical repository of 750 million rides of taxi medallions over a period of four years ( 20102013 ) . This data set provides rich ( batch ) information on the movements in an urban network as its citizens go about their daily life . We present a spectral analysis of taxi movement based on the graph Fourier transform , which necessitates the spectral decomposition of a large directed , sparse matrix . Important considerations toward handling this matrix are discussed . Preliminary results show that our method allows us to pinpoint locations of co-behavior for traffic in the Manhattan road network .
2K_dev_892	Plug-meters benefit many grid and building-level energy management applications like automated load control and load scheduling . However , installing and maintaining large and/or long term deployments of such meters requires assignment and updating of the identity ( labels ) of electrical loads connected to them . Although the literature on electricity disaggregation and appliance identification is extensive , there is no consensus on the generalizability of the proposed solutions , especially with respect to the features that are extracted from voltage and current measurements . In this paper , we begin to address this problem by comparing the discriminative power of commonly used features . Specifically , we carry out tests on PLAID , a publicly available high-frequency dataset of hundreds of residential appliances . By examining how the classification accuracy changes with sampling frequency , we also explore the computational complexity of these techniques to understand the feasibility and design of a hardware setup that can perform these calculations in near real-time .
2K_dev_893	Many big data applications collect large numbers of time series . A first task in analyzing such data is to find a low- dimensional representation , a graph , which faithfully describes relations among the measured processes and through time . The processes are often affected by a relatively small number of unmeasured trends . This paper presents a computationally tractable algorithm for jointly estimating these trends and underlying weighted , directed graph structure from the collected data . The algorithm is demonstrated on simulated time series datasets .
2K_dev_894	Metering of electricity consumption , both at the building-level and appliance-level , provides stakeholders like residents , facility managers , building owners , etc . with information requisite to engage in energy efficient practices . Currently available solutions for appliance-level energy metering require the installation of plug-through power meters ; this is often difficult and costly for appliances with inaccessible wires/outlets , or for appliances that draw large amounts of current . In this paper , we utilize a framework that performs the step of energy estimation for load disaggregation , in order to substitute the need of plug-level sensors with cheap and easily deployable contactless sensors ( e.g. , light , sound , magnetic field sensors ) . The framework combines data from these contactless sensors and aggregate metering , in order to virtually meter the electricity consumption of specific appliances . The solution requires minimal calibration , and is easily performed using commercially available sensors . We test it in a commercial building with 6 appliances that are monitored using magnetic field sensors and find that the inferred energy values have an average error of 10.9 % .
2K_dev_895	This paper presents the first reported in-situ reconfiguration of a narrowband CMOS low noise amplifier ( LNA ) over two widely separated frequency bands using a GeTe phase-change ( PC ) switch . Previous work has demonstrated the attractiveness of CMOS-PC integration to realize high-performance reconfigurable RF front-end circuits [ 1-2 ] . Four-terminal PC switches with small form factor have been recently shown to possess close-to-ideal properties of an RF switch : a high OFF/ON resistance ratio and extremely high figure-of-merit for RF switches ( FCO 0 1/ ( 2RONCOFF ) ) [ 3-4 ] . In this work , we present a robust realization of a reconfigurable 3/5 GHz LNA designed and fabricated in a 0.13 m CMOS process and flip-chip integrated with a four-terminal PC switch fabricated using an in-house process .
2K_dev_896	Devices can be made more intelligent if they have the ability to sense their surroundings and physical configuration . However , adding extra , special purpose sensors increases size , price and build complexity . Instead , we use speakers and microphones already present in a wide variety of devices to open new sensing opportunities . Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment , chiefly the materials and geometry of proximate surfaces . We offer several example uses , two of which we implemented as self-contained demos , and conclude with an evaluation that quantifies their performance and demonstrates high accuracy .
2K_dev_897	People are more creative at solving difficult design problems when they use relevant examples from outside of the problem s domain as inspirations . However , finding such outside-the-box inspirations is difficult , particularly in large idea repositories such as the web , because without guidance people select domains to search based on surface similarity to the problem s domain . In this paper , we demonstrate an approach in which non-experts identify domains that have the potential to yield useful and non-obvious inspirations for solutions . We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains . Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own , or drew inspiration from domains closer to or not sharing structural correspondence with the original problem .
2K_dev_898	To study signals on networks , to detect epidemics , or to predict blackouts , we need to understand network topology and its impact on the behavior of network processes . The high dimensionality of large networks presents significant analytical and computational challenges ; only specific network structures have been studied without approximation . We consider the impact of network topology on the limiting behavior of a dynamical process obeying the stochastic rules of SIS ( susceptible-infected-susceptible ) epidemics using the scaled SIS process . We introduce the network effect ratio , which captures the preference of individual agents versus the preference of society ( i.e. , network ) and investigate its effects .
2K_dev_899	3D-stacked integration of DRAM and logic layers using through-silicon via ( TSV ) technology has given rise to a new interpretation of near-data processing ( NDP ) concepts that were proposed decades ago . However , processing capability within the stack is limited by stringent power and thermal constraints . Simple processing mechanisms with intensive memory accesses , such as data reorganization , are an effective means of exploiting 3D stacking-based NDP . Data reorganization handled completely in memory improves the host processor 's memory access performance . However , in-memory data reorganization performed in parallel with host memory accesses raises issues , including interference , bandwidth allocation , and coherence . Previous work has mainly focused on performing data reorganization while blocking host accesses . This article details data reorganization performed in parallel with host memory accesses , providing mechanisms to address host/NDP interference , flexible bandwidth allocation , and in-memory coherence .
2K_dev_900	How do people interact with their Facebook wall ? At a high level , this question captures the essence of our work . While most prior efforts focus on Twitter , the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study . In this work , we model Facebook user behavior : we analyze the wall activities of users focusing on identifying common patterns and surprising phenomena . We conduct an extensive study of roughly 7K users over three years during four month intervals each year . We propose PowerWall , a lesser known heavy-tailed distribution to fit our data . Our key results can be summarized in the following points . First , we find that many wall activities , including number of posts , number of likes , number of posts of type photo , etc. , can be described by the PowerWall distribution . What is more surprising is that most of these distributions have similar slope , with a value close to 1 ! Second , we show how our patterns and metrics can help us spot surprising behaviors and anomalies . For example , we find a user posting every two days , exactly the same count of posts ; another user posting at midnight , with no other activity before or after . Our work provides a solid step towards a systematic and quantitative wall-centric profiling of Facebook user activity .
2K_dev_901	Biomedical scientists have invested significant effort into making it easy to perform lots of experiments quickly and cheaply . These high throughput methods are the workhorses of modern systems biology efforts . However , we simply can not perform an experiment for every possible combination of different cell type , genetic mutation and other conditions . In practice this has led researchers to either exhaustively test a few conditions or targets , or to try to pick the experiments that best allow a particular problem to be explored . But which experiments should we pick ? The ones we think we can predict the outcome of accurately , the ones for which we are uncertain what the results will be , or a combination of the two ? Humans are not particularly well suited for this task because it requires reasoning about many possible outcomes at the same time . However , computers are much better at handling statistics for many experiments , and machine learning algorithms allow computers to learn how to make predictions and decisions based on the data theyve previously processed . Previous computer simulations showed that a machine learning approach termed active learning could do a good job of picking a series of experiments to perform in order to efficiently learn a model that predicts the results of experiments that were not done . Now , Naik et al . have performed cell biology experiments in which experiments were chosen by an active learning algorithm and then performed using liquid handling robots and an automated microscope . The key idea behind the approach is that you learn more from an experiment you cant predict ( or that you predicted incorrectly ) than from just confirming your confident predictions . The results of the robot-driven experiments showed that the active learning approach outperforms strategies a human might use , even when the potential outcomes of individual experiments are not known beforehand . The next challenge is to apply these methods to reduce the cost of achieving the goals of large projects , such as The Cancer Genome Atlas .
2K_dev_902	We study the spread of two strains of virus competing for space in a network modeled by the classical logistic ordinary differential equations . In large-scale complex networks , the underlying nonlinear dynamical system is high-dimensional and performing qualitative analysis of the differential equation becomes prohibitive . The study of such systems is often deferred to numerical simulations or local analysis about equilibrium points of the system . In this paper , we extend the work developed in [ 1 ] , to formally establish a simple sufficient condition for ( exponentially fast ) survival of the fittest in a bi-layer weighted digraph : the weaker strain dies out regardless of the initial conditions if its maximum in-flow rate of infection across nodes is smaller than the minimum in-flow rate of the stronger strain . We bound any solution of the logistic ODE by one- dimensional solutions over certain homogeneous networks , for which the system is well understood . Our global stability approach via bounds readily applies to the discrete-time logistic model counterpart .
2K_dev_903	The preferred treatment for kidney failure is a transplant ; however , demand for donor kidneys far outstrips supply . Kidney exchange , an innovation where willing but incompatible patient-donor pairs can exchange organsvia barter cycles and altruist-initiated chainsprovides a life-saving alternative . Typically , fielded exchanges act myopically , considering only the current pool of pairs when planning the cycles and chains . Yet kidney exchange is inherently dynamic , with participants arriving and departing . Also , many planned exchange transplants do not go to surgery due to various failures . So , it is important to consider the future when matching . Motivated by our experience running the computational side of a large nationwide kidney exchange , we present FUTURE-MATCH , a framework for learning to match in a general dynamic model . FUTUREMATCH takes as input a high-level objective ( e.g. , `` maximize graft survival of transplants over time '' ) decided on by experts , then automatically ( i ) learns based on data how to make this objective concrete and ( ii ) learns the `` means '' to accomplish this goala task , in our experience , that humans handle poorly . It uses data from all live kidney transplants in the US since 1987 to learn the quality of each possible match ; it then learns the potentials of elements of the current input graph offline ( e.g. , potentials of pairs based on features such as donor and patient blood types ) , translates these to weights , and performs a computationally feasible batch matching that incorporates dynamic , failure-aware considerations through the weights . We validate FUTUREMATCH on real fielded exchange data . It results in higher values of the objective . Furthermore , even under economically inefficient objectives that enforce equity , it yields better solutions for the efficient objective ( which does not incorporate equity ) than traditional myopic matching that uses the efficiency objective .
2K_dev_904	Humans rely on eye gaze and hand manipulations extensively in their everyday activities . Most often , users gaze at an object to perceive it and then use their hands to manipulate it . We propose applying a multimodal , gaze plus free-space gesture approach to enable rapid , precise and expressive touch-free interactions . We show the input methods are highly complementary , mitigating issues of imprecision and limited expressivity in gaze-alone systems , and issues of targeting speed in gesture-alone systems . We extend an existing interaction taxonomy that naturally divides the gaze+gesture interaction space , which we then populate with a series of example interaction techniques to illustrate the character and utility of each method . We contextualize these interaction techniques in three example scenarios . In our user study , we pit our approach against five contemporary approaches ; results show that gaze+gesture can outperform systems using gaze or gesture alone , and in general , approach the performance of `` gold standard '' input systems , such as the mouse and trackpad .
2K_dev_905	Dagstuhl Seminar 15201 was conducted on `` Cross-Lingual Cross-Media Content Linking : Annotations and Joint Representations '' . Participants from around the world participated in the seminar and presented state-of-the-art and ongoing research related to the seminar topic . An executive summary of the seminar , abstracts of the talks from participants and working group discussions are presented in the forthcoming sections .
2K_dev_906	There is often a large disparity between the size of a game we wish to solve and the size of the largest instances solvable by the best algorithms ; for example , a popular variant of poker has about 10165 nodes in its game tree , while the currently best approximate equilibrium-finding algorithms scale to games with around 1012 nodes . In order to approximate equilibrium strategies in these games , the leading approach is to create a sufficiently small strategic approximation of the full game , called an abstraction , and to solve that smaller game instead . The leading abstraction algorithm for imperfect-information games generates abstractions that have imperfect recall and are distribution aware , using k-means with the earth mover 's distance metric to cluster similar states together . A distribution-aware abstraction groups states together at a given round if their full distributions over future strength are similar ( as opposed to , for example , just the expectation of their strength ) . The leading algorithm considers distributions over future strength at the final round of the game . However , one might benefit by considering the trajectory of distributions over strength in all future rounds , not just the final round . An abstraction algorithm that takes all future rounds into account is called potential aware . We present the first algorithm for computing potential-aware imperfect-recall abstractions using earth mover 's distance . Experiments on no-limit Texas Hold'em show that our algorithm improves performance over the previously best approach .
2K_dev_907	Non-technical loss ( NTL ) represents a major challenge when providing reliable electrical service in developing countries , where it often accounts for 11-15 % of total generation capacity [ 1 ] . NTL is caused by a variety of factors such as theft , unmetered homes , and inability to pay which at volume can lead to system instability , grid failure , and major financial losses for providers . In this paper , we investigate error sources and techniques for separating NTL from total losses in microgrids . Our approach models the primary sources of state uncertainty including line losses , transformer losses , meter calibration error , packet loss , and sample synchronization error . We conduct an extensive data-driven simulation on 72 days of wireless meter data from a 430-home microgrid deployed in Les Anglais , Haiti . We show that the model can be used to determine uncertainty bounds that can help in separating NTL from total losses .
2K_dev_908	For the important task of binocular depth perception from complex natural-image stimuli , the neurophysiological basis for disambiguating multiple matches between the eyes across similar features has remained a long-standing problem . Recurrent interactions among binocular disparity-tuned neurons in the primary visual cortex ( V1 ) could play a role in stereoscopic computationsbyalteringresponsesto favorthemost likelydepthinterpretation fora givenimagepair.Psychophysicalresearch has shown that binocular disparity stimuli displayed in 1 region of the visualfield can be extrapolated into neighboring regions that contain ambiguous depth information . We tested whether neurons in macaque V1 interact in a similar manner and found that unambiguous binocular disparity stimuli displayed in the surrounding visualfields of disparity-selective V1 neurons indeed modified their responses when either bistable stereoscopic or uniform featureless stimuli were presented within their receptivefield centers . The delayed timing of the response behavior compared with the timing of classical surround suppression and multiple control experiments suggests that these modulations are carried out by slower disparity-specific recurrentconnectionsamongV1neurons.Theseresultsprovideexplicitevidencethatthespatialinteractionsthatarepredicted by cooperative algorithms play an important role in solving the stereo correspondence problem .
2K_dev_909	Regret matching is a widely-used algorithm for learning how to act . We begin by proving that regrets on actions in one setting ( game ) can be transferred to warm start the regrets for solving a different setting with same structure but different payoffs that can be written as a function of parameters . We prove how this can be done by carefully discounting the prior regrets . This provides , to our knowledge , the first principled warm-starting method for no-regret learning . It also extends to warm-starting the widely-adopted counterfactual regret minimization ( CFR ) algorithm for large incomplete-information games ; we show this experimentally as well . We then study optimizing a parameter vector for a player in a two-player zero-sum game ( e.g. , optimizing bet sizes to use in poker ) . We propose a custom gradient descent algorithm that provably finds a locally optimal parameter vector while leveraging our warm-start theory to significantly save regret-matching iterations at each step . It optimizes the parameter vector while simultaneously finding an equilibrium . We present experiments in no-limit Leduc Hold'em and nolimit Texas Hold'em to optimize bet sizing . This amounts to the first action abstraction algorithm ( algorithm for selecting a small number of discrete actions to use from a continuum of actions -- a key preprocessing step for solving large games using current equilibrium-finding algorithms ) with convergence guarantees for extensive-form games .
2K_dev_910	We adopt a utilitarian perspective on social choice , assuming that agents have ( possibly latent ) utility functions over some space of alternatives . For many reasons one might consider mechanisms , or social choice functions , that only have access to the ordinal rankings of alternatives by the individual agents rather than their utility functions . In this context , one possible objective for a social choice function is the maximization of ( expected ) social welfare relative to the information contained in these rankings . We study such optimal social choice functions under three different models , and underscore the important role played by scoring functions . In our worst-case model , no assumptions are made about the underlying distribution and we analyze the worst-case distortion-or degree to which the selected alternative does not maximize social welfare-of optimal ( randomized ) social choice functions . In our average-case model , we derive optimal functions under neutral ( or impartial culture ) probabilistic models . Finally , a very general learning-theoretic model allows for the computation of optimal social choice functions ( i.e. , ones that maximize expected social welfare ) under arbitrary , sampleable distributions . In the latter case , we provide both algorithms and sample complexity results for the class of scoring functions , and further validate the approach empirically .
2K_dev_911	We provide a solution for elementary science test using instructional materials . We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures ( given a corpus of question-answer pairs and instructional materials ) , and uses what it learns to answer novel elementary science questions . Our evaluation shows that our framework outperforms several strong baselines .
2K_dev_912	In Massively Open Online Courses ( MOOCs ) TA resources are limited ; most MOOCs use peer assessments to grade assignments . Students have to divide up their time between working on their own homework and grading others . If there is no risk of being caught and penalized , students have no reason to spend any time grading others . Course staff want to incentivize students to balance their time between course work and peer grading . They may do so by auditing students , ensuring that they perform grading correctly . One would not want students to invest too much time on peer grading , as this would result in poor course performance . We present the first model of strategic auditing in peer grading , modeling the student 's choice of effort in response to a grader 's audit levels as a Stackelberg game with multiple followers . We demonstrate that computing the equilibrium for this game is computationally hard . We then provide a PTAS in order to compute an approximate solution to the problem of allocating audit levels . However , we show that this allocation does not necessarily maximize social welfare ; in fact , there exist settings where course auditor utility is arbitrarily far from optimal under an approximately optimal allocation . To circumvent this issue , we present a natural condition that guarantees that approximately optimal TA allocations guarantee approximately optimal welfare for the course auditors .
2K_dev_913	For decades researchers have struggled with the problem of envy-free cake cutting : how to divide a divisible good between multiple agents so that each agent likes his own allocation best . Although an envy-free cake cutting protocol was ultimately devised , it is unbounded , in the sense that the number of operations can be arbitrarily large , depending on the preferences of the agents . We ask whether bounded protocols exist when the agents ' preferences are restricted . Our main result is an envy-free cake cutting protocol for agents with piecewise linear valuations , which requires a number of operations that is polynomial in natural parameters of the given instance .
2K_dev_914	Achieving high performance for compute bounded numerical kernels typically requires an expert to hand select an appropriate set of Single-instruction multiple-data ( SIMD ) instructions , then statically scheduling them in order to hide their latency while avoiding register spilling in the process . Unfortunately , this level of control over the code forces the expert to trade programming abstraction for performance which is why many performance critical kernels are written in assembly language . An alternative is to either resort to auto-vectorization ( see Figure 1 ) or to use intrinsic functions , both features offered by compilers . However , in both scenarios the expert loses control over which instructions are selected , which optimizations are applied to the code and moreover how the instructions are scheduled for a target architecture . Ideally , the expert would need assembly-like control over their SIMD instructions beyond what intrinsics provide while maintaining a C-level abstraction for the non-performance critical parts . In this paper , we bridge the gap between performance and abstraction for SIMD instructions through the use of custom macro intrinsics that provide the programmer control over the instruction selection , and scheduling , while leveraging the compiler to manage the registers . This provides the best of both assembly and vector intrinsics programming so that a programmer can obtain high performance implementations within the C programming language .
2K_dev_915	This article introduces a relatively complete proof calculus for differential dynamic logic ( dL ) that is entirely based on uniform substitution , a proof rule that substitutes a formula for a predicate symbol everywhere . Uniform substitutions make it possible to use axioms instead of axiom schemata , thereby substantially simplifying implementations . Instead of subtle schema variables and soundness-critical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones , the resulting calculus adopts only a finite number of ordinary dLformulas as axioms , which uniform substitutions instantiate soundly . The static semantics of differential dynamic logic and the soundness-critical restrictions it imposes on proof steps is captured exclusively in uniform substitutions and variable renamings as opposed to being spread in delicate ways across the prover implementation . In addition to sound uniform substitutions , this article introduces differential forms for differential dynamic logic that make it possible to internalize differential invariants , differential substitutions , and derivatives as first-class axioms to reason about differential equations axiomatically . The resulting axiomatization of differential dynamic logic is proved to be sound and relatively complete .
2K_dev_916	Complex event detection is a retrieval task with the goal of finding videos of a particular event in a large-scale unconstrained internet video archive , given example videos and text descriptions . Nowadays , different multimodal fusion schemes of low-level and high-level features are extensively investigated and evaluated for the complex event detection task . However , how to effectively select the high-level semantic meaningful concepts from a large pool to assist complex event detection is rarely studied in the literature . In this paper , we propose two novel strategies to automatically select semantic meaningful concepts for the event detection task based on both the events-kit text descriptions and the concepts high-level feature descriptions . Moreover , we introduce a novel event oriented dictionary representation based on the selected semantic concepts . Towards this goal , we leverage training samples of selected concepts from the Semantic Indexing ( SIN ) dataset with a pool of 346 concepts , into a novel supervised multitask dictionary learning framework . Extensive experimental results on TRECVID Multimedia Event Detection ( MED ) dataset demonstrate the efficacy of our proposed method .
2K_dev_917	This paper considers the development of information flow analyses to support resilient design and active detection of adversaries in cyber physical systems ( CPS ) . CPS security , though well studied , suffers from fragmentation . In this paper , we consider control systems as an abstraction of CPS . Here , we use information flow analysis , a well established set of methods developed in software security , to obtain a unified framework that captures and extends results in control system security . Specifically , we propose the Kullback Liebler ( KL ) divergence as a causal measure of information flow , which quantifies the effect of adversarial inputs on sensor outputs . We show that the proposed measure characterizes the resilience of control systems to specific attack strategies by relating the KL divergence to optimal detection . We then relate information flows to stealthy attack scenarios where an adversary can bypass detection . Finally , this article examines active detection mechanisms where a defender intelligently manipulates control inputs or the system itself to elicit information flows from an attacker 's malicious behavior . In all previous cases , we demonstrate an ability to investigate and extend existing results through the proposed information flow analyses .
2K_dev_918	An automatic software testing machine may be configured to provide an advanced symbolic execution approach to software testing that combines dynamic symbolic execution and static symbolic execution , leveraging the strengths of each and avoiding the vulnerabilities of each . One or more software testing machines within a software testing system may be configured to automatically and dynamically alternate between dynamic symbolic execution and static symbolic execution , based on partial control flow graphs of portions of the software code to be tested . In some example embodiments , a software testing machine begins with dynamic symbolic execution , but switches to static symbolic execution opportunistically . In static mode , instead of checking entire programs for verification , the software testing machine may only check one or more program fragments for testing purposes . Thus , the software testing machine may benefit from the strengths of both dynamic and static symbolic execution .
2K_dev_919	Some embodiments of the present invention include a method of differentiating touch screen users based on characterization of features derived from the touch event acoustics and mechanical impact and includes detecting a touch event on a touch sensitive surface , generating a vibro-acoustic waveform signal using at least one sensor detecting such touch event , converting the waveform signal into at least a domain signal , extracting distinguishing features from said domain signal , and classifying said features to associate the features of the domain signal with a particular user .
2K_dev_920	According to embodiments of the present invention are a system and method that use projected structured patterns of light and linear optical sensors for motion tracking . Sensors are capable of recovering two-dimensional location within the projection area , while several sensors can be combined for up to six degrees of freedom tracking . The structure patterns are based on m-sequences , in which any consecutive subsequence of m bits is unique . Both digital and static light sources can be used . The system and method of the present invention enables high-speed , high precision , and low-cost motion tracking for a wide range of applications .
2K_dev_921	A computer-implemented method includes , in one aspect , retrieving HVAC system information ; converting the retrieved HVAC system information from one or more first data formats to a second data format ; storing the converted HVAC system information ; identifying first portions of the stored HVAC system information that pertain to a particular component ; and generating one or more associations among the identified , first portions of the stored HVAC system information ; receiving a request for HVAC system information that is used by a performance analysis algorithm ; determining a type of component of the HVAC system that is related to the requested HVAC system information ; and identifying one or more items of the stored HVAC system information that is of the requested HVAC system information ; and transmitting the identified one or more items of the stored HVAC system information for use in execution of the performance analysis algorithm .
2K_dev_922	In the article Advanced Engineering Informatics 29 ( 2 ) ( 2015 ) 196-210 in Section 3.2 , there is unattributed use of materials from a prior publication , namely Chaiyasarn ( 2011 ) . This material has been improperly cited and the authors sincerely apologize for this . This corrigendum contains a revised version of Section 3.2 with corrected attributions and citations .
2K_dev_923	Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models . We propose a general framework capable of enhancing various types of neural networks ( e.g. , CNNs and RNNs ) with declarative first-order logic rules . Specifically , we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks . We deploy the framework on a CNN for sentiment analysis , and an RNN for named entity recognition . With a few highly intuitive rules , we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems .
2K_dev_924	We demonstrate four-terminal GeTe-based RF switches with independent thermal actuation ( switching ) . These devices incorporate an AlN-based dielectric separating high-conductivity W micro-heaters from the RF signal path . With dc , pulsed , and RF testing , we show that an AlN barrier decreases the switch parasitic capacitance with minimal increases in the switching power . Decoupling these design variables , with the high thermal conductivity of the AlN , makes it possible to increase the electrical separation with a thicker AlN film for lower parasitic capacitance with minimal decrease in desirable thermal coupling . Increasing the AlN thickness from 105 to 170 nm results in switches with an improvement in cutoff frequency , $ f_ { \mathrm { CO } } $ from 5.3 to 8 THz , a $ C_ { \mathrm { \scriptscriptstyle OFF } } $ improvement from 15 to 10 fF , while maintaining the $ R_ { \mathrm { \scriptscriptstyle ON } } $ at 2 $ \Omega $ . This improvement was accompanied by normalized minimum power to amorphize increases of only 14 % ( from 1.5 to 1.7 W ) for a 100-ns heater pulse .
2K_dev_925	This paper studies the convergence of the estimation error process and the characterization of the corresponding invariant measure in distributed Kalman filtering for poten tially unstable and large linear dynamic systems . A gossip network protocol termed Modified Gossip Interactive Kalman Filteri ng ( M-GIKF ) is proposed , where sensors exchange their filtered states ( estimates and error covariances ) and propagate their observations via inter-sensor communications of rate ; is defined as the averaged number of inter-sensor message passa ges per signal evolution epoch . The filtered states are interpre ted as stochastic particles swapped through local interaction . The paper shows that the conditional estimation error covariance sequence at each sensor under M-GIKF evolves as a random Riccati equa- tion ( RRE ) with Markov modulated switching . By formulating the RRE as a random dynamical system , it is shown that the network achieves weak consensus , i.e. , the conditional estimation error covariance at a randomly selected sensor converges weakly ( in distribution ) to a unique invariant measure . Further , it is proved that as ! 1 this invariant measure satisfies the Large Deviation ( LD ) upper and lower bounds , implying that this measure converges exponentially fast ( in probability ) to the Dirac measureP , where Pis the stable error covariance of the centralized ( Kalman ) filtering setup . The LD results ans wer a fundamental question on how to quantify the rate at which the distributed scheme approaches the centralized performance as the inter-sensor communication rate increases .
2K_dev_926	How can we analyze large-scale real-world data with various attributes ? Many real-world data ( e.g. , network traffic logs , web data , social networks , knowledge bases , and sensor streams ) with multiple attributes are represented as multi-dimensional arrays , called tensors . For analyzing a tensor , tensor decompositions are widely used in many data mining applications : detecting malicious attackers in network traffic logs ( with source IP , destination IP , port-number , timestamp ) , finding telemarketers in a phone call history ( with sender , receiver , date ) , and identifying interesting concepts in a knowledge base ( with subject , object , relation ) . However , current tensor decomposition methods do not scale to large and sparse real-world tensors with millions of rows and columns and `fibers . ' In this paper , we propose HaTen2 , a distributed method for large-scale tensor decompositions that runs on the MapReduce framework . Our careful design and implementation of HaTen2 dramatically reduce the size of intermediate data and the number of jobs leading to achieve high scalability compared with the state-of-the-art method . Thanks to HaTen2 , we analyze big real-world sparse tensors that can not be handled by the current state of the art , and discover hidden concepts .
2K_dev_927	Mental simulation is an important skill for program understanding and prediction of program behavior . Assessing students ' ability to mentally simulate program execution can be challenging in graphical programming environments and on paper-based assessments . This poster presents the iterative design and refinement process for assessing students ' ability to mentally simulate and predict code behavior using a novel introductory computational thinking curriculum for Microsoft 's Kodu Game Lab . We present an analysis of question prompts and student responses from data collected from three rising 3rd - 6th graders where the curriculum was implemented . Analysis of student responses suggest that this type of question can be used to identify misconceptions and misinterpretation of instructions . Finally , we present recommendations for question prompt design to foster better student simulation of program execution .
2K_dev_928	We introduce the notion of balance for directed graphs : aweighted directed graph is -balanced if for every cut S V , the total weight of edges going from S to V S is within factor of the total weight of edges going from V S to S . Several important families of graphs are nearly balanced , in particular , Eulerian graphs ( with 0 1 ) and residual graphs of ( 1+ ) -approximate undirected maximum flows ( with 0 O ( 1/ ) ) . We use the notion of balance to give a more fine-grained understanding of several well-studied routing questions that are considerably harder in directed graphs . We first revisit oblivious routings in directed graphs . Our main algorithmic result is an oblivious routing scheme for single-source instances that achieve an O ( log 3 n / loglog n ) competitive ratio . In the process , we make several technical contributions which may be of independent interest . In particular , we give an efficient algorithm for computing low-radius decompositions of directed graphs parameterized by balance . We also define and construct low-stretch arborescences , a generalization of low-stretch spanning trees to directed graphs . On the negative side , we present new lower bounds for oblivious routing problems on directed graphs . We show that the competitive ratio of oblivious routing algorithms for directed graphs is ( n ) in general ; this result improves upon the long-standing best known lower bound of ( n ) by Hajiaghayi et al . We also show that our restriction to single-source instances is necessary by showing an ( n ) lower bound for multiple-source oblivious routing in Eulerian graphs . We also study the maximum flow problem in balanced directed graphs with arbitrary capacities . We develop an efficient algorithm that finds an ( 1+ ) -approximate maximum flows in -balanced graphs in time O ( m 2 / 2 ) . We show that , using our approximate maximum flow algorithm , we can efficiently determine whether a given directed graph is -balanced . Additionally , we give an application to the directed sparsest cut problem .
2K_dev_929	Virtual machine ( VM ) migration demands distinct properties under resource oversubscription and workload surges . We present enlightened post-copy , a new mechanism for VMs under contention that evicts the target VM with fast execution transfer and short total duration . This design contrasts with common live migration , which uses the down time of the migrated VM as its primary metric ; it instead focuses on recovering the aggregate performance of the VMs being affected . In enlightened post-copy , the guest OS identifies memory state that is expected to encompass the VM 's working set . The hypervisor accordingly transfers its state , mitigating the performance impact on the migrated VM resulting from post-copy transfer . We show that our implementation , with modest instrumentation in guest Linux , resolves VM contention up to several times faster than live migration .
2K_dev_930	In this paper , we explore how the contextual ambiguity of a search can affect a keyword 's performance . The context of consumer search is often unobserved and the prediction of it can be nontrivial . Consumers arrive at search engines with diverse interests , and their search context may vary even when they are searching using the same keyword . In our study , we propose an automatic way of examining keyword contextual ambiguity based on probabilistic topic models from machine learning and computational linguistics . We examine the effect of contextual ambiguity on keyword performance using a hierarchical Bayesian approach that allows for topic-specific effects and nonlinear position effects , and jointly models click-through rate ( CTR ) and ad position ( rank ) . We validate our study using a novel data set from a major search engine that contains information on consumer click activities for 2,625 distinct keywords across multiple product categories from 10,000 impressions . We find that consumer click behavior varies significantly across keywords , and such variation can be partially explained by keyword category and the contextual ambiguity of keywords . Specifically , higher contextual ambiguity is associated with higher CTR on top-positioned ads , but also a faster decay in CTR with screen position . Therefore , the overall effect of contextual ambiguity on CTR varies across positions . Our study has the potential to help advertisers design keyword portfolios and bidding strategy by extracting contextual ambiguity and other semantic characteristics of keywords based on large-scale analytics from unstructured data . It can also help search engines improve the quality of displayed ads in response to a consumer search query .
2K_dev_931	Pipes carrying pressurized fluids are an important part of the civil infrastructure , and structural health monitoring ( SHM ) could ensure structural integrity by predicting and preventing structural failures . Guided wave ultrasonics is a good candidate for use in pipe SHM because guided waves can propagate long distances and are sensitive to structural damage such as cracks and corrosion losses . However , the multi-modal and dispersive characteristics of guided waves make it difficult to interpret their arrival records . Moreover , guided waves are also sensitive to environmental and operational variations , limiting the effectiveness of ultrasonic methods to detect pipe damage in a real environment . We introduce a damage detector based on singular value decomposition ( SVD ) that can identify a change of interest , caused by a mass scatterer that simulates subtle damage , under realistic environmental variations . We show the effectiveness and robustness of this method on experimental data collected on a pipe segment under realistic environmental and operational variations over a time period of several months .
2K_dev_932	Today several daily activities such as communication , education , E-commerce , Entertainment and tasks are carried out by using the internet . To perform such web activities users have to register regarding the websites . In registering websites , some intruders write malicious programs that waste the website resources by making automatic false enrolments that are called as bots . These false enrolments may adversely affect the working of websites . So , it becomes necessary to differentiate between human users and Web bots ( or computer programs ) is known as CAPTCHA . CAPTCHA is based on identifying the distorted text , the color of image , object or the background . This paper examines CAPTCHAs and its working and literature Review . This paper also provides classification of CAPTCHAs , its application areas and guidelines for generating a captcha .
2K_dev_933	Multimedia event detection has been one of the major endeavors in video event analysis . A variety of approaches have been proposed recently to tackle this problem . Among others , using semantic representation has been accredited for its promising performance and desirable ability for human-understandable reasoning . To generate semantic representation , we usually utilize several external image/video archives and apply the concept detectors trained on them to the event videos . Due to the intrinsic difference of these archives , the resulted representation is presumable to have different predicting capabilities for a certain event . Notwithstanding , not much work is available for assessing the efficacy of semantic representation from the source-level . On the other hand , it is plausible to perceive that some concepts are noisy for detecting a specific event . Motivated by these two shortcomings , we propose a bi-level semantic representation analyzing method . Regarding source-level , our method learns weights of semantic representation attained from different multimedia archives . Meanwhile , it restrains the negative influence of noisy or irrelevant concepts in the overall concept-level . In addition , we particularly focus on efficient multimedia event detection with few positive examples , which is highly appreciated in the real-world scenario . We perform extensive experiments on the challenging TRECVID MED 2013 and 2014 datasets with encouraging results that validate the efficacy of our proposed approach .
2K_dev_934	Software testing is all too often simply a bug hunt rather than a wellconsidered exercise in ensuring quality . A more methodical approach than a simple cycle of system-level test-fail-patch-test will be required to deploy safe autonomous vehicles at scale . The ISO 26262 development V process sets up a framework that ties each type of testing to a corresponding design or requirement document , but presents challenges when adapted to deal with the sorts of novel testing problems that face autonomous vehicles . This paper identifies five major challenge areas in testing according to the V model for autonomous vehicles : driver out of the loop , complex requirements , non-deterministic algorithms , inductive learning algorithms , and failoperational systems . General solution approaches that seem promising across these different challenge areas include : phased deployment using successively relaxed operational scenarios , use of a monitor/actuator pair architecture to separate the most complex autonomy functions from simpler safety functions , and fault injection as a way to perform more efficient edge case testing . While significant challenges remain in safety-certifying the type of algorithms that provide high-level autonomy themselves , it seems within reach to instead architect the system and its accompanying design process to be able to employ existing software safety approaches .
2K_dev_935	In multi-core systems , main memory is a major shared resource among processor cores . A task running on one core can be delayed by other tasks running simultaneously on other cores due to interference in the shared main memory system . Such memory interference delay can be large and highly variable , thereby posing a significant challenge for the design of predictable real-time systems . In this paper , we present techniques to reduce this interference and provide an upper bound on the worst-case interference on a multi-core platform that uses a commercial-off-the-shelf ( COTS ) DRAM system . We explicitly model the major resources in the DRAM system , including banks , buses , and the memory controller . By considering their timing characteristics , we analyze the worst-case memory interference delay imposed on a task by other tasks running in parallel . We find that memory interference can be significantly reduced by ( i ) partitioning DRAM banks , and ( ii ) co-locating memory-intensive tasks on the same processing core . Based on these observations , we develop a memory interference-aware task allocation algorithm for reducing memory interference . We evaluate our approach on a COTS-based multi-core platform running Linux/RK . Experimental results show that the predictions made by our approach are close to the measured worst-case interference under workloads with both high and low memory contention . In addition , our memory interference-aware task allocation algorithm provides a significant improvement in task schedulability over previous work , with as much as 96 % more tasksets being schedulable .
2K_dev_936	Sustainable building system design techniques aim to find an optimal balance between occupant comfort and the energy performance of HVAC systems . Design and implementation of effective heating ventilating and air conditioning ( HVAC ) controls is the key to achieve these optimal design conditions . Any anomalies in the functioning of a system component or a control system would result in occupant discomfort and/or energy wastage . While occupant discomfort can be directly sensed by occupants , measurement of waste in energy use would require additional sensing and analysis infrastructure . One way of identifying such a waste is to compare asdesigned system requirements with the actual performance of the systems . This paper presents an analysis of an air handling unit ( AHU ) in a five story office building and provides the comparison results of design requirements against the sensor data corresponding to the AHU parameters . One year sensor data for the AHU parameters was analyzed to assess the correctness of the implementation of the design intent . The design intent was interpreted from the sequence of operations ( SOOs ) and confirmed with a commissioning engineer , who worked on the project . The design intent was then graphically represented as a pattern that the sensor data corresponding to the controls is expected to follow if it follows the design intent . Any deviation in the sensor data as compared to the expected operation pattern of the design intent indicated incorrect operation of the system with incorrectly implemented controls . The findings in this paper substantiate the need to formally define the sequence of operations and also point to the need to verify the implemented controls in a given project to detect any deviations from the actual design intent .
2K_dev_937	Elucidating assembly pathways of complex macromolecular structures , such as virus capsids , is an important problem for understanding the many cellular processes dependent on self-assembly but also challenging given limited experimental technologies for observing such systems . We have previously addressed this problem through simulation-based data fitting , learning rate parameters of coarse-grained stochastic simulation models to match light scattering data from bulk assembly of purified coat protein in vitro providing an unprecedented view of the fine-scale reaction pathways that might have produced those data . A key question raised by such models , though , is how well they might reflect assembly under more natural cellular conditions where factors such as local concentration changes , non-specific crowding , and often the influence of nucleic acid during assembly become relevant . In the present study , we examine the latter issue , how using analytical models of various contributions of RNA folding to assembly would influence overall pathways and kinetics , primarily with reference to cowpea chlorotic mottle virus ( CCMV ) . We find a surprising complexity and synergy of interaction effects . Energetic effects that gain or lower free energy tend to disrupt successful assembly relative to the in vitro model individually , while the full combination of positive and negative effects collectively promotes greatly accelerated assembly without loss of yield . Furthermore , it accomplishes this change in kinetics while substantially altering the ensemble of assembly pathways open to the system . These simulation results help us understand how RNA viral coat and genome may interact in assembly to promote rapid growth while avoiding kinetic traps expected from much prior theory , bringing us a step closer to the goal of understanding how viral assembly in the cell may differ from our current conception based largely on in vitro models .
2K_dev_938	This year was the 17th edition of the ACM SIGACCESS International Conference on Computers and Accessibility ( ASSETS 2015 ) 1 . ASSETS 2015 took place in Lisbon , which is the capital of Portugal and is known as the city of seven hills ( A Cidade das Sete Colinas ) . The conference was very well attended around 170 participants from all over the world . There was one keynote speech by Jon Schull from Rochester Institute of Technology and e-NABLE . The best paper award went to `` Understanding the Challenges Faced by Neurodiverse Software Engineering Employees : Towards a More Inclusive and Productive Technical Workforce '' by Meredith Ringel Morris , Andrew Begel and Ben Weidermann . The best student paper award went to `` Social Media Platforms for Low-Income Blind People in India '' by Aditya Vashistha , Edward Cutrell , Nicola Dell , and Richard Anderson . The conference also had an excellent social program having drinks and nice food in the evenings , which was great to meet people and socialise with the accessibility community .
2K_dev_939	TetriSched is a scheduler that works in tandem with a calendaring reservation system to continuously re-evaluate the immediate-term scheduling plan for all pending jobs ( including those with reservations and best-effort jobs ) on each scheduling cycle . TetriSched leverages information supplied by the reservation system about jobs ' deadlines and estimated runtimes to plan ahead in deciding whether to wait for a busy preferred resource type ( e.g. , machine with a GPU ) or fall back to less preferred placement options . Plan-ahead affords significant flexibility in handling mis-estimates in job runtimes specified at reservation time . Integrated with the main reservation system in Hadoop YARN , TetriSched is experimentally shown to achieve significantly higher SLO attainment and cluster utilization than the best-configured YARN reservation and CapacityScheduler stack deployed on a real 256 node cluster .
2K_dev_940	Machine learning ( ML ) algorithms are commonly applied to big data , using distributed systems that partition the data across machines and allow each machine to read and update all ML model parameters -- - a strategy known as data parallelism . An alternative and complimentary strategy , model parallelism , partitions the model parameters for non-shared parallel access and updates , and may periodically repartition the parameters to facilitate communication . Model parallelism is motivated by two challenges that data-parallelism does not usually address : ( 1 ) parameters may be dependent , thus naive concurrent updates can introduce errors that slow convergence or even cause algorithm failure ; ( 2 ) model parameters converge at different rates , thus a small subset of parameters can bottleneck ML algorithm completion . We propose scheduled model parallelism ( SchMP ) , a programming approach that improves ML algorithm convergence speed by efficiently scheduling parameter updates , taking into account parameter dependencies and uneven convergence . To support SchMP at scale , we develop a distributed framework STRADS which optimizes the throughput of SchMP programs , and benchmark four common ML applications written as SchMP programs : LDA topic modeling , matrix factorization , sparse least-squares ( Lasso ) regression and sparse logistic regression . By improving ML progress per iteration through SchMP programming whilst improving iteration throughput through STRADS we show that SchMP programs running on STRADS outperform non-model-parallel ML implementations : for example , SchMP LDA and SchMP Lasso respectively achieve 10x and 5x faster convergence than recent , well-established baselines .
2K_dev_941	Sequential games of perfect information can be solved by backward induction , where solutions to endgames are propagated up the game tree . However , this does not work in imperfect-information games because different endgames can contain states that belong to the same information set and can not be treated independently . In fact , we show that this approach can fail even in a simple game with a unique equilibrium and a single endgame . Nonetheless , we show that endgame solving can have significant benefits in imperfectinformation games with large state and action spaces : computation of exact ( rather than approximate ) equilibrium strategies , computation of relevant equilibrium refinements , significantly finer-grained action and information abstraction , new information abstraction algorithms that take into account the relevant distribution of players types entering the endgame , being able to select the coarseness of the action abstraction dynamically , additional abstraction techniques for speeding up endgame solving , a solution to the off-tree problem , and using different degrees of probability thresholding in modeling versus playing . We discuss each of these topics in detail , and introduce techniques that enable one to conduct endgame solving in a scalable way even when the number of states and actions in the game is large . Our experiments on two-player no-limit Texas Holdem poker show that our approach leads to significant performance improvements in practice .
2K_dev_942	School shootings tear the fabric of society . In the wake of a school shooting , parents , pediatricians , policymakers , politicians , and the public search for `` the '' cause of the shooting . But there is no single cause . The causes of school shootings are extremely complex . After the Sandy Hook Elementary School rampage shooting in Newtown , Connecticut , we wrote a report for the National Science Foundation on what is known and not known about youth violence . This article summarizes and updates that report . After distinguishing violent behavior from aggressive behavior , we describe the prevalence of gun violence in the United States and age-related risks for violence . We delineate important differences between violence in the context of rare rampage school shootings , and much more common urban street violence . Acts of violence are influenced by multiple factors , often acting together . We summarize evidence on some major risk factors and protective factors for youth violence , highlighting individual and contextual factors , which often interact . We consider new quantitative `` data mining '' procedures that can be used to predict youth violence perpetrated by groups and individuals , recognizing critical issues of privacy and ethical concerns that arise in the prediction of violence . We also discuss implications of the current evidence for reducing youth violence , and we offer suggestions for future research . We conclude by arguing that the prevention of youth violence should be a national priority . ( PsycINFO Database Record Language : en
2K_dev_943	Privacy decision making has been examined from various perspectives . A dominant normative perspective has focused on rational processes by which consumers with stable preferences for privacy weigh the expected benefits of privacy choices against their potential costs . More recently , an alternate behavioral perspective has leveraged theories from behavioral decision research to construe privacy decision making as a process in which cognitive heuristics and biases predictably occur . In a series of experiments , we compare the predictive power of these two perspectives by evaluating the impact of changes in objective risk of disclosure and the impact of changes in relative perceptions of risk of disclosure on both hypothetical and actual consumer privacy choices . We find that both relative and objective risks can , in fact , impact consumer privacy decisions . However , and surprisingly , the impact of objective changes in risk diminishes between hypothetical and actual choice settings . Vice versa , the impact of relative risk is more pronounced going from hypothetical to actual choice settings . Our results suggest a way to integrate diverse streams of IS literature on privacy decision making : consumers may both over-estimate their response to normative factors and under-estimate their response to behavioral factors in hypothetical choice contexts relative to actual choice contexts .
2K_dev_944	Communication and coordination play a major role in the ability of bacterial cells to adapt to ever changing environments and conditions . Recent work has shown that such coordination underlies several aspects of bacterial responses including their ability to develop antibiotic resistance . Here we develop a new distributed gradient descent method that helps explain how bacterial cells collectively search for food in harsh environments using extremely limited communication and com- putational complexity . This method can also be used for computational tasks when agents are facing similarly restricted conditions . We formalize the communication and computation assumptions re- quired for successful coordination and prove that the method we propose leads to convergence even when using a dynamically changing interaction network . The proposed method improves upon prior models suggested for bacterial foraging despite making fewer assumptions . Simulation studies and analysis of experimental data illustrate the ability of the method to explain and further predict several aspects of bacterial swarm food search .
2K_dev_945	ABSTRACT Clad steel refers to a thick carbon steel structural plate bonded to a corrosion resistant alloy ( CRA ) plate , such as stainless steel or titanium , and is widely used in industry to construct pressure vessels . The CRA resists the chemically aggressive environment on the interior , but can not prevent the development of corrosion losses and cracks that limit the continued safe operation of such vessels . At present there are no practical methods to detect such defects from the exposed outer surface of the thick carbon steel plate , often necessitating removing such vessels from service and inspecting them visually from the interior . In previous resear ch , sponsored by industry to detect and localize damage in pressurized piping systems under operational and environmental changes , we investigated a number of data-driven signal processing methods to extract damage information from ultrasonic guided wave pitch-catch records . We now apply those methods to relatively large clad steel plate specimens . We study a sparse array of wafer-type ultrasonic transducers adhered to the carbon steel surface , attempting to localize mass scatterers grease-coupled to the stainless steel surface . We discuss conditions under which localization is achieved by relatively simple first-arrival methods , and other conditions for which data-driven methods are needed ; we also discuss observations of plate-like mode properties implied by these results . Keywords : aaeaa a
2K_dev_946	Many commercial products and academic research activities are embracing behavior analysis as a technique for improving detection of attacks of many sortsfrom retweet boosting , hashtag hijacking to link advertising . Traditional approaches focus on detecting dense blocks in the adjacency matrix of graph data , and recently , the tensors of multimodal data . No method gives a principled way to score the suspiciousness of dense blocks with different numbers of modes and rank them to draw human attention accordingly . In this paper , we first give a list of axioms that any metric of suspiciousness should satisfy ; we propose an intuitive , principled metric that satisfies the axioms , and is fast to compute ; moreover , we propose CrossSpot , an algorithm to spot dense blocks that are worth inspecting , typically indicating fraud or some other noteworthy deviation from the usual , and sort them in the order of importance ( suspiciousness ) . Finally , we apply CrossSpot to the real data , where it improves the F1 score over previous techniques by 68percent and finds suspicious behavioral patterns in social datasets spanning 0.3 billion posts .
2K_dev_947	Objective . A traditional goal of neural recording with extracellular electrodes is to isolate action potential waveforms of an individual neuron . Recently , in braincomputer interfaces ( BCIs ) , it has been recognized that threshold crossing events of the voltage waveform also convey rich information . To date , the threshold for detecting threshold crossings has been selected to preserve single-neuron isolation . However , the optimal threshold for single-neuron identification is not necessarily the optimal threshold for information extraction . Here we introduce a procedure to determine the best threshold for extracting information from extracellular recordings . We apply this procedure in two distinct contexts : the encoding of kinematic parameters from neural activity in primary motor cortex ( M1 ) , and visual stimulus parameters from neural activity in primary visual cortex ( V1 ) . Approach . We record extracellularly from multi-electrode arrays implanted in M1 or V1 in monkeys . Then , we systematically sweep the voltage detection threshold and quantify the information conveyed by the corresponding threshold crossings . Main Results . The optimal threshold depends on the desired information . In M1 , velocity is optimally encoded at higher thresholds than speed ; in both cases the optimal thresholds are lower than are typically used in BCI applications . In V1 , information about the orientation of a visual stimulus is optimally encoded at higher thresholds than is visual contrast . A conceptual model explains these results as a consequence of cortical topography . Significance . How neural signals are processed impacts the information that can be extracted from them . Both the type and quality of information contained in threshold crossings depend on the threshold setting . There is more information available in these signals than is typically extracted . Adjusting the detection threshold to the parameter of interest in a BCI context should improve our ability to decode motor intent , and thus enhance BCI control . Further , by sweeping the detection threshold , one can gain insights into the topographic organization of the nearby neural tissue .
2K_dev_948	Although widely used , Multilinear PCA ( MPCA ) , one of the leading multilinear analysis methods , still suffers from four major drawbacks . First , it is very sensitive to outliers and noise . Second , it is unable to cope with missing values . Third , it is computationally expensive since MPCA deals with large multi-dimensional datasets . Finally , it is unable to maintain the local geometrical structures due to the averaging process . This paper proposes a novel approach named Compressed Submanifold Multifactor Analysis ( CSMA ) to solve the four problems mentioned above . Our approach can deal with the problem of missing values and outliers via SVD-L1 . The Random Projection method is used to obtain the fast low-rank approximation of a given multifactor dataset . In addition , it is able to preserve the geometry of the original data . Our CSMA method can be used efficiently for multiple purposes , e.g. , noise and outlier removal , estimation of missing values , biometric applications . We show that CSMA method can achieve good results and is very efficient in the inpainting problem as compared to [ 1 ] , [ 2 ] . Our method also achieves higher face recognition rates compared to LRTC , SPMA , MPCA and some other methods , i.e. , PCA , LDA and LPP , on three challenging face databases , i.e. , CMU-MPIE , CMU-PIE and Extended YALE-B .
2K_dev_949	Consumer privacy decision making is often layered : different interrelated decisions determine , together , a final privacy outcome and its associated benefits and costs . Layered privacy choices are particularly common online , where consumers are frequently tasked with multiple , sequential choices ( such as first selecting a services privacy settings , and then engaging in privacy-sensitive behaviors ) that will ultimately impact their privacy trade-offs . The layered nature of online privacy choices has important implications for models of privacy decision making and for consumers assumption of privacy risks . In this manuscript , we investigate how changes in the architecture of privacy choices affect an initial layer of privacy choice , and how that effect percolates through subsequent layers of privacy choices . Specifically , in a series of experiments , we investigate the impact of framing on participants initial privacy choices , and whether participants subsequent behaviors take account of , and neutralize , that impact . We find that various manipulations of decision frames , common to privacy contexts , can significantly alter individual choice of privacy protective options . Further , and importantly , we find that participants subsequent disclosure behavior stays constant despite the shifts in chosen privacy protections induced by choice framing . Implications for privacy decision research as well as policy makers are discussed .
2K_dev_950	Motivation : As cancer researchers have come to appreciate the importance of intratumor heterogeneity , much attention has focused on the challenges of accurately profiling heterogeneity in individual patients . Experimental technologies for directly profiling genomes of single cells are rapidly improving , but they are still impractical for large-scale sampling . Bulk genomic assays remain the standard for population-scale studies , but conflate the influences of mixtures of genetically distinct tumor , stromal , and infiltrating immune cells . Many computational approaches have been developed to deconvolute these mixed samples and reconstruct the genomics of genetically homogeneous clonal subpopulations . All such methods , however , are limited to reconstructing only coarse approximations to a few major subpopulations . In prior work , we showed that one can improve deconvolution of genomic data by leveraging substructure in cellular mixtures through a strategy called simplicial complex inference . This strategy , however , is also limited by the difficulty of inferring mixture structure from sparse , noisy assays . Results : We improve on past work by introducing enhancements to automate learning of substructured genomic mixtures , with specific emphasis on genome-wide copy number variation ( CNV ) data . We introduce methods for dimensionality estimation to better decompose mixture model substructure ; fuzzy clustering to better identify substructure in sparse , noisy data ; and automated model inference methods for other key model parameters . We show that these improvements lead to more accurate inference of cell populations and mixture proportions in simulated scenarios . We further demonstrate their effectiveness in identifying mixture substructure in real tumor CNV data . Availability : Source code is available at this http URL
2K_dev_951	The pervasiveness of mobile technologies today have facilitated the creation of massive crowdsourced and geotagged data from individual users in real time and at different locations in the city . Such ubiquitous user-generated data allow us to infer various patterns of human behavior , which help us understand the interactions between humans and cities . In this study , we focus on understanding users economic behavior in the city by examining the economic value from crowdsourced and geotaggged data . Specifically , we extract multiple traffic and human mobility features from publicly available data sources using NLP and geo-mapping techniques , and examine the effects of both static and dynamic features on economic outcome of local businesses . Our study is instantiated on a unique dataset of restaurant bookings from OpenTable for 3,187 restaurants in New York City from November 2013 to March 2014 . Our results suggest that foot traffic can increase local popularity and business performance , while mobility and traffic from automobiles may hurt local businesses , especially the well-established chains and high-end restaurants . We also find that on average one more street closure nearby leads to a 4.7 % decrease in the probability of a restaurant being fully booked during the dinner peak . Our study demonstrates the potential of how to best make use of the large volumes and diverse sources of crowdsourced and geotagged user-generated data to create matrices to predict local economic demand in a manner that is fast , cheap , accurate , and meaningful .
2K_dev_952	How much has a network changed since yesterday ? How different is the wiring of Bobs brain ( a left-handed male ) and Alices brain ( a right-handed female ) , and how is it different ? Graph similarity with given node correspondence , i.e. , the detection of changes in the connectivity of graphs , arises in numerous settings . In this work , we formally state the axioms and desired properties of the graph similarity functions , and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs . We propose D elta C on , a principled , intuitive , and scalable algorithm that assesses the similarity between two graphs on the same nodes ( e.g. , employees of a company , customers of a mobile carrier ) . In conjunction , we propose D elta C on -A ttr , a related approach that enables attribution of change or dissimilarity to responsible nodes and edges . Experiments on various synthetic and real graphs showcase the advantages of our method over existing similarity measures . Finally , we employ D elta C on and D elta C on -A ttr on real applications : ( a ) we classify people to groups of high and low creativity based on their brain connectivity graphs , ( b ) do temporal anomaly detection in the who-emails-whom Enron graph and find the top culprits for the changes in the temporal corporate email graph , and ( c ) recover pairs of test-retest large brain scans ( 17M edges , up to 90M edges ) for 21 subjects .
2K_dev_953	Effective enforcement of laws and policies requires expending resources to prevent and detect offenders , as well as appropriate punishment schemes to deter violators . In particular , enforcement of privacy laws and policies in modern organizations that hold large volumes of personal information ( e.g. , hospitals , banks ) relies heavily on internal audit mechanisms . We study economic considerations in the design of these mechanisms , focusing in particular on effective resource allocation and appropriate punishment schemes . We present an audit game model that is a natural generalization of a standard security game model for resource allocation with an additional punishment parameter . Computing the Stackelberg equilibrium for this game is challenging because it involves solving an optimization problem with non-convex quadratic constraints . We present an additive FPTAS that efficiently computes the solution .
2K_dev_954	Admixture-introduced linkage disequilibrium ( LD ) has recently been introduced into the inference of the histories of complex admixtures . However , the influence of ancestral source populations on the LD pattern in admixed populations is not properly taken into consideration by currently available methods , which affects the estimation of several gene flow parameters from empirical data . We first illustrated the dynamic changes of LD in admixed populations and mathematically formulated the LD under a generalized admixture model with finite population size . We next developed a new method , MALDmef , by fitting LD with multiple exponential functions for inferring and dating multiple-wave admixtures . MALDmef takes into account the effects of source populations which substantially affect modeling LD in admixed population , which renders it capable of efficiently detecting and dating multiple-wave admixture events . The performance of MALDmef was evaluated by simulation and it was shown to be more accurate than MALDER , a state-of-the-art method that was recently developed for similar purposes , under various admixture models . We further applied MALDmef to analyzing genome-wide data from the Human Genome Diversity Project ( HGDP ) and the HapMap Project . Interestingly , we were able to identify more than one admixture events in several populations , which have yet to be reported . For example , two major admixture events were identified in the Xinjiang Uyghur , occurring around 27 ? ? ? 30 generations ago and 182 ? ? ? 195 generations ago , respectively . In an African population ( MKK ) , three recent major admixtures occurring 13 ? ? ? 16 , 50 ? ? ? 67 , and 107 ? ? ? 139 generations ago were detected . Our method is a considerable improvement over other current methods and further facilitates the inference of the histories of complex population admixtures .
2K_dev_955	Large-scale deep learning requires huge computational resources to train a multi-layer neural network . Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections . While the computation involved can be done more efficiently on GPUs than on more traditional CPU cores , training such networks on a single GPU is too slow and training on distributed GPUs can be inefficient , due to data movement overheads , GPU stalls , and limited GPU memory . This paper describes a new parameter server , called GeePS , that supports scalable deep learning across GPUs distributed among multiple machines , overcoming these obstacles . We show that GeePS enables a state-of-the-art single-node GPU implementation to scale well , such as to 13 times the number of training images processed per second on 16 machines ( relative to the original optimized single-node code ) . Moreover , GeePS achieves a higher training throughput with just four GPU machines than that a state-of-the-art CPU-only system achieves with 108 machines .
2K_dev_956	Often , Big Data applications collect a large number of time series , for example , the financial data of companies quoted in a stock exchange , the health care data of all patients that visit the emergency room of a hospital , or the temperature sequences continuously measured by weather stations across the US . A first task in the analytics of these data is to derive a low dimensional representation , a graph or discrete manifold , that describes well the interrelations among the time series and their intrarelations across time . This paper presents a computationally tractable algorithm for estimating this graph structure from the available data . This graph is directed and weighted , possibly representing causal relations , not just reciprocal correlations as in many existing approaches in the literature . A detailed convergence analysis is carried out . The algorithm is demonstrated on random graph and real network time series datasets , and its performance is compared to that of related methods . The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested .
2K_dev_957	Given a large collection of time-evolving activities , such as Google search queries , which consist of d keywords/activities for m locations of duration n , how can we analyze temporal patterns and relationships among all these activities and find location-specific trends ? How do we go about capturing non-linear evolutions of local activities and forecasting future patterns ? For example , assume that we have the online search volume for multiple keywords , e.g. , `` Nokia/Nexus/Kindle '' or `` CNN/BBC '' for 236 countries/territories , from 2004 to 2015 . Our goal is to analyze a large collection of multi-evolving activities , and specifically , to answer the following questions : ( a ) Is there any sign of interaction/competition between two different keywords ? If so , who competes with whom ? ( b ) In which country is the competition strong ? ( c ) Are there any seasonal/annual activities ? ( d ) How can we automatically detect important world-wide ( or local ) events ? We present COMPCUBE , a unifying non-linear model , which provides a compact and powerful representation of co-evolving activities ; and also a novel fitting algorithm , COMPCUBE-FIT , which is parameter-free and scalable . Our method captures the following important patterns : ( B ) asic trends , i.e. , non-linear dynamics of co-evolving activities , signs of ( C ) ompetition and latent interaction , e.g. , Nokia vs. Nexus , ( S ) easonality , e.g. , a Christmas spike for iPod in the U.S. and Europe , and ( D ) eltas , e.g. , unrepeated local events such as the U.S. election in 2008 . Thanks to its concise but effective summarization , COMPCUBE can also forecast long-range future activities . Extensive experiments on real datasets demonstrate that COMPCUBE consistently outperforms the best state-of- the-art methods in terms of both accuracy and execution speed .
2K_dev_958	If stored information is erased from neural circuits in one brain hemisphere in mice , the lost data can be recovered from the other . This finding highlights a safeguarding mechanism at work in the brain . See Article p.459
2K_dev_959	Given a large collection of co-evolving online activities , such as searches for the keywords `` Xbox '' , `` PlayStation '' and `` Wii '' , how can we find patterns and rules ? Are these keywords related ? If so , are they competing against each other ? Can we forecast the volume of user activity for the coming month ? We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food . We present EcoWeb , ( i.e. , Ecosystem on the Web ) , which is an intuitive model designed as a non-linear dynamical system for mining large-scale co-evolving online activities . Our second contribution is a novel , parameter-free , and scalable fitting algorithm , EcoWeb-Fit , that estimates the parameters of EcoWeb . Extensive experiments on real data show that EcoWeb is effective , in that it can capture long-range dynamics and meaningful patterns such as seasonalities , and practical , in that it can provide accurate long-range forecasts . EcoWeb consistently outperforms existing methods in terms of both accuracy and execution speed .
2K_dev_960	Online news , blogs , SNS and many other Web-based services has been attracting considerable interest for business and marketing purposes . Given a large collection of time series , such as web-click logs , online search queries , blog and review entries , how can we efficiently and effectively find typical time-series patterns ? What are the major tools for mining , forecasting and outlier detection ? Time-series data analysis is becoming of increasingly high importance , thanks to the decreasing cost of hardware and the increasing on-line processing capability . The objective of this tutorial is to provide a concise and intuitive overview of the most important tools that can help us find meaningful patterns in large-scale time-series data . Specifically we review the state of the art in three related fields : ( 1 ) similarity search , pattern discovery and summarization , ( 2 ) non-linear modeling and forecasting , and ( 3 ) the extension of time-series mining and tensor analysis . We also introduce case studies that illustrate their practical use for social media and Web-based services .
2K_dev_961	T cells must receive signals through the T cell receptor ( TCR ) and the costimulatory receptor CD28 to become fully activated . Critical to this process is the reorganization of plasma membrane actin at the immunological synapse , the interface between a T cell and an antigen-presenting cell . Roybal et al . imaged actin and fluorescently tagged actin regulatory proteins in T cells activated through the TCR in the absence or presence of CD28 signaling . Computational image processing to normalize differences in cell shape enabled tracking of the fluorescent proteins . The regulatory proteins WAVE2 and cofilin were efficiently recruited to the immunological synapse only when both TCR and CD28 signaled . Constitutive activation of either protein in TCR-stimulated T cells enabled normal actin reorganization even when CD28 signaling was blocked . This combination of imaging and computational analysis could be applied to other systems to determine the spatiotemporal dynamics of signaling molecules .
2K_dev_962	A grand challenge for state estimation in newly built smart grid lies in how to deal with the increasing uncertainties . To solve the problem , we propose a data-driven state estimation approach based on recent targeted investment on sensors , data storage , and computing devices . An architecture is proposed to use power system physics and pattern to systematically clean historical data and conduct supervised learning , where historical similar measurements and their states are used to learn the relationship between the current measurement and the state . In order to deal with nonlinearity , kernel trick is used to produce linear mapping in a carefully selected higher dimensional space . To speed up the data-driven approach for online services , we analyze power system data set and discover its clustering property due to the periodic pattern of power systems . This leads to significant dimension reduction and the idea of preorganizing data points in a tree structure for inquiry , leading to 1000 times speedup . Numerical results show that the proposed data-driven approach works well in a smart grid setting with increasing uncertainties and it produces an online state estimate excelling current industrial approach .
2K_dev_963	Multi-person tracking plays a critical role in the analysis of surveillance video . However , most existing work focus on shorter-term ( e.g . minute-long or hour-long ) video sequences . Therefore , we propose a multi-person tracking algorithm for very long-term ( e.g . month-long ) multi-camera surveillance scenarios . Long-term tracking is challenging because 1 ) the apparel/appearance of the same person will vary greatly over multiple days and 2 ) a person will leave and re-enter the scene numerous times . To tackle these challenges , we leverage face recognition information , which is robust to apparel change , to automatically reinitialize our tracker over multiple days of recordings . Unfortunately , recognized faces are unavailable oftentimes . Therefore , our tracker propagates identity information to frames without recognized faces by uncovering the appearance and spatial manifold formed by person detections . We tested our algorithm on a 23-day 15-camera data set ( 4,935 hours total ) , and we were able to localize a person 53.2 % of the time with 69.8 % precision . We further performed video summarization experiments based on our tracking output . Results on 116.25 hours of video showed that we were able to generate a reasonable visual diary ( i.e . a summary of what a person did ) for different people , thus potentially opening the door to automatic summarization of the vast amount of surveillance video generated every day .
2K_dev_964	Cyber-physical systems ( CPS ) are heterogeneous , be- cause they tightly couple computation , communication , and control along with physical dynamics , which are traditionally considered separately . Without a comprehensive modeling formalism , model- based development of CPS involves using a multitude of models in a variety of formalisms that capture various aspects of the system design , such as software design , networking design , physical mod- els , and protocol design . Without a rigorous unifying framework , system integration and integration of the analysis results for vari- ous models remains ad hoc . In this paper , we propose a multi-view architecture framework that treats models as views of the under- lying system structure and uses structural and semantic mappings to ensure consistency and enable system-level verification in a hierarchical and compositional manner . Throughout the paper , the theoretical concepts are illustrated using two examples : a quad- rotor and an automotive intersection collision avoidance system . Index TermsControl design , control engineering , formal veri- fication , software architecture .
2K_dev_965	In this paper , we introduce InstructableCrowd , a system that allows end-users to instruct the crowd to create trigger-action ( `` if , then '' ) rules based on their needs . We create a framework which enables users to converse with the crowd using their phone and describe a problem which they might have . We create an interface for a crowd worker to both chat with the user and compose a rule with an `` IF '' part connected to the user 's phone sensors ( e.g . incoming emails , GPS location , meeting calendar , weather information etc . ) , and a `` THEN '' part connected to user 's phone effectors ( e.g . sending an email , creating an alarm , posting a tweet , etc. ) . The system then sends the rules created by the crowd to the user 's phone in order to help the user solve his problem .
2K_dev_966	The maximum Nash welfare ( MNW ) solution -- - which selects an allocation that maximizes the product of utilities -- - is known to provide outstanding fairness guarantees when allocating divisible goods . And while it seems to lose its luster when applied to indivisible goods , we show that , in fact , the MNW solution is unexpectedly , strikingly fair even in that setting . In particular , we prove that it selects allocations that are envy free up to one good -- - a compelling notion that is quite elusive when coupled with economic efficiency . We also establish that the MNW solution provides a good approximation to another popular ( yet possibly infeasible ) fairness property , the maximin share guarantee , in theory and -- - even more so -- - in practice . While finding the MNW solution is computationally hard , we develop a nontrivial implementation , and demonstrate that it scales well on real data . These results lead us to believe that MNW is the ultimate solution for allocating indivisible goods , and underlie its deployment on a popular fair division website .
2K_dev_967	Our research is delivered as Portable Document Format ( PDF ) documents , and very few include basic metadata to make them accessible to people with disabilities . As a result , many people are either unable to read them efficiently or at all . Over the past few years , we have tried everything from writing guidelines and giving accessibility feedback , to enforcing accessibility standards and volunteering to make PDFs accessible ourselves . The problem with making PDFs accessible is in part due to the lack of good tools , but the complexity of the PDF format makes improving tools difficult . Making accessible research papers is as much about our choices as a community : our choice of publication format , and our choice to make accessibility a voluntary task for authors . In this paper , we overview the context in which PDFs became our publication format , the difficulty in making PDF documents accessible given current tools , what we have tried to make our PDFs more accessible , and potential options for doing better in the future .
2K_dev_968	The safety of mobile robots in dynamic environments is predicated on making sure that they do not collide with obstacles . In support of such safety arguments , we analyze and formally verify a series of increasingly powerful safety properties of controllers fo r avoiding both stationary and moving obstacles : ( i ) static safety , which ensures that no collisions can happen with stationary obstacles , ( ii ) passive safety , which ensures that no collisions can happen with stationary or moving obstacles while the robot moves , ( iii ) the stronger passive friendly safety in which the robot further maintains sufficient maneuvering distance for obstacles to avoid collision as well , and ( iv ) passive orientation safety , which allows for imperfect sensor coverage of the robot , i. e. , the robot is aw are that not everything in its environment will be visible . We complement these provably correct safety properties with liveness properties : we prove that provably safe motion is flexible enough to let the r obot still navigate waypoints and pass intersections . We use hybrid system models and theorem proving techniques that describe and formally verify the robots discrete control decisions along with it s continuous , physical motion . Moreover , we formally prove that safety can still be guaranteed despite s ensor uncertainty and actuator perturbation , and when control choices for more aggressive maneuvers are introduced . Our verification results are generic in the sense that they are not limited to the particul ar choices of one specific control algorithm but identify conditions that make them simultaneously apply to a broad class of control algorithms .
2K_dev_969	It is difficult to accomplish meaningful goals with limited time and attentional resources . However , recent research has shown that concrete plans with actionable steps allow people to complete tasks better and faster . With advances in techniques that can decompose larger tasks into smaller units , we envision that a transformation from larger tasks to smaller microtasks will impact when and how people perform complex information work , enabling efficient and easy completion of tasks that currently seem challenging . In this workshop , we bring together researchers in task decomposition , completion , and sourcing . We will pursue a broad understanding of the challenges in creating , allocating , and scheduling microtasks , as well as how accomplishing these microtasks can contribute towards productivity . The goal is to discuss how intersections of research across these areas can pave the path for future research in this space .
2K_dev_970	Given a bipartite graph of users and the products that they review , or followers and followees , how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral , etc . ) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph . Fraudsters can evade these methods using camouflage , by adding reviews or follows with honest targets so that they look `` normal '' . Even worse , some fraudsters use hijacked accounts from honest users , and then the camouflage is indeed organic . Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts . We propose FRAUDAR , an algorithm that ( a ) is camouflage-resistant , ( b ) provides upper bounds on the effectiveness of fraudsters , and ( c ) is effective in real-world data . Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud . Additionally , in real-world experiments with a Twitter follower-followee graph of 1.47 billion edges , FRAUDAR successfully detected a subgraph of more than 4000 detected accounts , of which a majority had tweets showing that they used follower-buying services .
2K_dev_971	The Curriculum Task Force ( CTF ) of ISCBs Education Committee seeks to define curricular guidelines for those who educate or train bioinformatics professionals at all career stages . A recent report of the CTF [ 1 ] presented a draft set of bioinformatics core competencies , derived from the results of surveys of ( 1 ) core facility directors , ( 2 ) career opportunities , and ( 3 ) existing curricula . Since the publication of its 2014 report , the CTF has focused on the application of the guidelines in varied contexts to identify areas where refinement is needed . As a first step , the task force held an open meeting at the ISMB conference in July 2014 . The ideas discussed at the meeting spawned four working groups ( WGs ) , which focus on ( i ) defining core competencies for specific types and levels of bioinformatics training , ( ii ) mapping the curriculum guidelines and competencies to existing materials in order to identify the need for development of new materials , and ( iii ) identifying where revision of the guidelines may be valuable . The CTF is engaging the ISCB community through open WG meetings at ISCBs official conferences . Thus far , the WGs have convened at the ISCB Great Lakes Bioinformatics Conference ( Purdue University , May 2015 ) and at the ISMB/ECCB Conference ( Dublin , Ireland , July 2015 ) . Additionally , the CTF held a workshop at the Annual General Meeting of the Global Organization of Bioinformatics Learning , Education and Training ( Cape Town , South Africa , November 2015 ) . Specifically , the draft competencies have been employed in a wide range of activities and contexts ( see Table 1 and [ 211 ] ) , including the development of new curricula , the analysis of existing curricula , and the creation of new roles involving bioinformatics . These activities have resulted in the identification of several areas where refinement would be useful : Table 1 Summary of the activities of the ISCB Curriculum Task Force . Identify different levels or phases of competency . It would be helpful to define different phases of competency development , or different levels of competency appropriate for distinct roles . Define competency profiles for disciplines that dont fit into our current silos . Bioengineering provides an illustrative example of a discipline that requires core competency in bioinformatics but does not fit into our current categories . There are almost certainly others . It would be helpful if we could provide some guidance on how to produce hybrid competency profiles , perhaps borrowing some competencies from the TFs core set and others from different disciplines . The LifeTrain initiative ( www.lifetrain.eu ) [ 2 , 3 ] is collecting competency profiles for a range of disciplines of relevance to the biomedical sciences and may provide a useful resource kit for this . Broaden the scope of the competency profiles in response to cutting-edge and emerging research . Current areas requiring improvement include incorporating competencies that capture a fundamental understanding of the biological principles central to analyzing biomolecular data , and broadening the user WG to include applications beyond medicine . Provide guidance on the evidence required to assess whether someone has acquired each competency . For undergraduate , Masters and PhD programs , learning outcomes for each competency , perhaps with examples of appropriate means of assessment , would be valuable . For established professionals who need to assimilate competencies into their working lives , a different approach may be required ( such as keeping a portfolio to capture evidence of competency ) ; the CTF should seek guidance from relevant professional bodies , especially in regulated professions such as healthcare . Provide indicative course content or examples of programs that map to the competency requirements . We do not wish to prescribe what course providers should teach or how they should teach it ; however , if a course provider is designing a course to meet a specific competency requirement , it may be helpful to find examples of other programs that do this successfully . One way of achieving this is by mapping existing training content to the TFs competencies . Another way might be to provide an indication , perhaps based on several courses , of the course content that would meet the competency requirements . This would give course providers the freedom to build their own course syllabi without having to reinvent the wheel . Initiatives to collect examples of Creative Commons ( or otherwise reusable ) course materials will provide an extremely valuable bank of training materials that could be mapped to the core competencies .
2K_dev_972	It is my great pleasure to welcome you to the 12th International Symposium on Information Processing in Sensor Networks ( IPSN 2013 ) . The symposium continues its tradition of excellence with a vibrant program this year that combines the Information Processing ( IP ) and the Sensor Platforms ( SPOTS ) tracks . The goal is to bring together researchers and practitioners from industry and academia to address the gamut of challenges from theoretical foundations of information processing in sensor networks to new hardware architecture and platform prototypes . I sincerely thank the program chairs Kay Romer and Raj Rajkumar for putting together this exciting program . We owe the quality of this year 's symposium to their dedicated efforts . Being part of CPSWeek , a landmark multidisciplinary event in the area of cyber-physical systems , IPSN is colocated with four related conferences ( namely , HSCC , ICCPS , HiCONS , and RTAS ) , and a significant number of workshops and tutorials . I sincerely thank the organizers of CPSWeek for working together with IPSN and the other conferences and workshops on ironing out all the challenging coordination details and making the coordinated event a success . CPSWeek also features three exciting keynotes : `` Sensemaking for Mobile Health '' by Deborah Estrin , `` Aerial Robot Swarms '' by Vijay Kumar , and `` Challenges in Modeling Cyber-physical Systems '' by Manfred Broy . In addition to keynote and paper presentations , IPSN features a poster and demonstration session . Special thanks goes to Luca Mottola , the Demo Chair , and Tian He , the Poster Chair , for organizing it . A Ph.D. forum is also held to offer Ph.D. students a chance to present their dissertation research and to get advice on thesis directions . Thanks to Polly Huang for putting that forum together .
2K_dev_973	Abstract : We study the emergence of global behavior in large scale networks . The underlying motivating application is epidemics like computer virus spreading , for example , in wide campus local networks . We consider multiple classes of viruses , each type bearing their own statistical characterization -- exogenous contamination , contagious propagation , and healing . The network state ( distribution of nodes infected by each class in the network ) is a jump Markov process , not necessarily reversible , making it a challenge to obtain its invariant distribution . By suitable renormalization , in the limit of a large network ( number of nodes ) , we describe the macroscopic or emergent behavior of the network by the solution of a set of deterministic nonlinear differential equations . These nonlinear differential equations are obtained by mean field analysis of the microscopic random dynamics . We study the qualitative behavior of the nonlinear differential equations describing the mean field dynamics .
2K_dev_974	The ubiquitous deployment of mobile and sensor technologies has led to both the capacity to observe human behavior in physical ( offline ) settings as well as to record it . This provides researchers with a new lens to study and better understand the individual decision processes that were previously unobserved . In this paper , we study decision making behavior of 11,196 taxi drivers in a large Asian city using a rich data set consisting of 10.6 million fine-grained GPS trip records . These records include detailed taxi GPS trajectories , taxi occupancy data ( i.e. , whether a taxi was occupied with a passenger or was vacant ) and taxi drivers daily incomes . This capacity to use data where occupancy of the taxi is known is a distinctive feature of our data set and sets this work apart from prior work which has attempted to study driver behavior . The specific decision we focus on pertains to actions drivers take to find new passengers after they have dropped off their current passengers . In particular , we study the role of information derivable from the GPS trace data ( e.g. , where passengers are dropped off , where passengers are picked up , longitudinal taxicab travel history with fine-grained time stamps ) observable by or made available to drivers in enabling them to learn the distribution of demand for their services over space and time . We conduct our study using a heterogeneous Bayesian learning model . We find strong heterogeneity in individual learning behavior and driving decisions , which is significantly associated with individual economic outcomes . Drivers with higher incomes benefit significantly from their ability to learn from not only demand information directly observable in the local market , but also aggregate information on demand flows across markets . Interestingly , our policy simulations indicate information that is noisy at the individual level becomes valuable after being aggregated across various spatial and temporal dimensions . Moreover , the value of information does not increase monotonically with the scale and frequency of information sharing . Finally , our study has important welfare implications in that efficient information sharing leads to an income increase among all drivers , instead of a redistribution of income between different types of drivers . Our work allows us not only to explain driver decision making behavior using these detailed behavioral traces , but also to prescribe information sharing strategy for the firm in order to improve the overall market efficiency .
2K_dev_975	Unfortunately , the original version of this article [ 1 ] contained an error . Figures2 , ,44 and and55 were incorrect and the captions for Figs.4 and and55 were incorrect . Below are the correct figures and captions : Fig . 2 Histogram of k-mer relative abundances . Both 20- and 25-mer relative abundance densities appear log-laplacian . These data included 20- and 25-mers found in all tumor cells . a Histogram of 20-mer relative abundances in log10 scale . b Histogram of 25-mer ... Fig . 4 20-mer bootstrap consensus neighbor-joining tree built from T10 primary breast tumor cells ( prefix C ) , T16 primary ( prefix P ) and metastatic data ( prefix M ) . Distinct groupings of cells are labeled as clusters Fig . 5 20-mer bootstrap consensus neighbor-joining tree built from T16 primary ( prefix P ) and metastatic data ( prefix M ) . Distinct groupings of cells are labeled as clusters
2K_dev_976	We present AD3 , a new algorithm for approximate maximum a posteriori ( MAP ) inference on factor graphs , based on the alternating directions method of multipliers . Like other dual decomposition algorithms , AD3 has a modular architecture , where local subproblems are solved independently , and their solutions are gathered to compute a global update . The key characteristic of AD3 is that each local subproblem has a quadratic regularizer , leading to faster convergence , both theoretically and in practice . We provide closed-form solutions for these AD3 subproblems for binary pairwise factors and factors imposing first-order logic constraints . For arbitrary factors ( large or combinatorial ) , we introduce an active set method which requires only an oracle for computing a local MAP configuration , making AD3 applicable to a wide range of problems . Experiments on synthetic and real-world problems show that AD3 compares favorably with the state-of-the-art .
2K_dev_977	We describe a system called Olive that freezes and precisely reproduces the environment necessary to execute software long after its creation . It uses virtual machine ( VM ) technology to encapsulate legacy software , complete with all its software dependencies . This legacy world can be completely closed-source : there is no requirement for availability of source code , nor a requirement for recompilation or relinking . The entire VM is streamed over the Internet from a web server , much as video is streamed today .
2K_dev_978	What is the growth pattern of social networks , like Facebook and WeChat ? Does it truly exhibit exponential early growth , as predicted by textbook models like the Bass model , SI , or the Branching Process ? How about the count of links , over time , for which there are few published models ? We examine the growth of several real networks , including one of the world 's largest online social network , `` WeChat '' , with 300 million nodes and 4.75 billion links by 2013 ; and we observe power law growth for both nodes and links , a fact that completely breaks the sigmoid models ( like SI , and Bass ) . In its place , we propose NETTIDE , along with differential equations for the growth of the count of nodes , as well as links . Our model accurately fits the growth patterns of real graphs ; it is general , encompassing as special cases all the known , traditional models ( including Bass , SI , log-logistic growth ) ; while still remaining parsimonious , requiring only a handful of parameters . Moreover , our NETTIDE for link growth is the first one of its kind , accurately fitting real data , and naturally leading to the densification phenomenon . We validate our model with four real , time-evolving social networks , where NETTIDE gives good fitting accuracy , and , more importantly , applied on the WeChat data , our NETTIDE forecasted more than 730 days into the future , with 3 % error .
2K_dev_979	Presented at the NCRN Meeting Spring 2016 in Washington DC on May 9-10 , 2016 ; see http : //www.ncrn.info/event/ncrn-spring-2016-meeting
2K_dev_980	Abstract Genome-wide association studies have revealed individual genetic variants associated with phenotypic traits such as disease risk and gene expressions . However , detecting pairwise interaction effects of genetic variants on traits still remains a challenge due to a large number of combinations of variants ( 1011 SNP pairs in the human genome ) , and relatively small sample sizes ( typically < 104 ) . Despite recent breakthroughs in detecting interaction effects , there are still several open problems , including : ( 1 ) how to quickly process a large number of SNP pairs , ( 2 ) how to distinguish between true signals and SNPs/SNP pairs merely correlated with true signals , ( 3 ) how to detect nonlinear associations between SNP pairs and traits given small sample sizes , and ( 4 ) how to control false positives . In this article , we present a unified framework , called SPHINX , which addresses the aforementioned challenges . We first propose a piecewise linear model for interaction detection , because it is simple enough to ...
2K_dev_981	How do social groups , such as Facebook groups and Wechat groups , dynamically evolve over time ? How do people join the social groups , uniformly or with burst ? What is the pattern of people quitting from groups ? Is there a simple universal model to depict the come-and-go patterns of various groups ? In this paper , we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users . We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR . For example , we observe both diffusion and non-diffusion mechanism in the group joining process , and power-law decay in group quitting process , rather than exponential decay as expected in SIR model . Therefore we propose a new model comeNgo , a concise yet flexible dynamic model for group evolution . Our model has the following advantages : ( a ) unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation . ( b ) succinctness and interpretability : it contains only six parameters with clear physical meanings . ( c ) accuracy : it can capture various kinds of group evolution patterns preciously and the goodness of fit increase by 58 % over baseline . ( d ) usefulness : it can be used in multiple application scenarios such as forecasting and pattern discovery .
2K_dev_982	System management includes the selection of maintenance actions depending on the available observations : when a system is made up by components known to be similar , data collected on one is also relevant for the management of others . This is typically the case of wind farms , which are made up by similar turbines . Optimal management of wind farms is an important task due to high cost of turbines operation and maintenance : in this context , we recently proposed a method for planning and learning at system-level , called PLUS , built upon the Partially Observable Markov Decision Process ( POMDP ) framework , which treats transition and emission probabilities as random variables , and is therefore suitable for including model uncertainty . PLUS models the components as independent or identical . In this paper , we extend that formulation , allowing for a weaker similarity among components . The proposed approach , called Multiple Uncertain POMDP ( MU-POMDP ) , models the components as POMDPs , and assumes the corresponding parameters as dependent random variables . Through this framework , we can calibrate specific degradation and emission models for each component while , at the same time , process observations at system-level . We compare the performance of the proposed MU-POMDP with PLUS , and discuss its potential and computational complexity .
2K_dev_983	Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services . Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type , and the tensor-based summaries look for high-order dense blocks by clustering the values ( including timestamps ) in each dimension . However , the human behaviors are multicontextual and dynamic : ( 1 ) each behavior takes place within multiple contexts in a few dimensions , which requires the representation to enable non-value and set-values for each dimension ; ( 2 ) many behavior collections , such as tweets or papers , evolve over time . In this paper , we represent the behavioral data as a two-level matrix ( temporal-behaviors by dimensional-values ) and propose a novel representation for behavioral summary called Tartan that includes a set of dimensions , the values in each dimension , a list of consecutive time slices and the behaviors in each slice . We further develop a propagation method CatchTartan to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way : it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner . CatchTartan outperforms the baselines on both the accuracy and speed . We apply CatchTartan to four Twitter datasets up to 10 million tweets and the DBLP data , providing comprehensive summaries for the events , human life and scientific development .
2K_dev_984	In this paper , we address the distributed filtering and prediction of time-varying random fields represented by linear time-invariant ( LTI ) dynamical systems . The field is observed by a sparsely connected network of agents/sensors collaborating among themselves . We develop a Kalman filter type consensus + innovations distributed linear estimator of the dynamic field termed as Consensus+Innovations Kalman Filter . We analyze the convergence properties of this distributed estimator . We prove that the mean-squared error of the estimator asymptotically converges if the degree of instability of the field dynamics is within a prespecified threshold defined as tracking capacity of the estimator . The tracking capacity is a function of the local observation models and the agent communication network . We design the optimal consensus and innovation gain matrices yielding distributed estimates with minimized mean-squared error . Through numerical evaluations , we show that the distributed estimator with optimal gains converges faster and with approximately 3dB better mean-squared error performance than previous distributed estimators .
2K_dev_985	In this report , we describe CMU-SMUs participation in the Video Hyperlinking task of TRECVID 2015 . We treat video hyperlinking as ad-hoc retrieval scenario and use a variety of retrieval methods . Our experiments mainly focus on the study of different features on the performance of video hyperlinking , including subtitle , metadata , audio and visual features , as well as the consideration of surrounding context . Different combination strategies are used to combine those features . Besides , we also attempt to categorize the queries and use different search strategies for different categories . Experiments results show that ( 1 ) the context does not generally improve results , ( 2 ) the search performance mainly rely on textual features , and the combination of audio and visual feature can not provide improvements ; ( 3 ) due to the lack of training examples , machine learning techniques can not provide contributions .
2K_dev_986	The physical constraints of smartwatches limit the range and complexity of tasks that can be completed . Despite interface improvements on smartwatches , the promise of enabling productive work remains largely unrealized . This paper presents WearWrite , a system that enables users to write documents from their smartwatches by leveraging a crowd to help translate their ideas into text . WearWrite users dictate tasks , respond to questions , and receive notifications of major edits on their watch . Using a dynamic task queue , the crowd receives tasks issued by the watch user and generic tasks from the system . In a week-long study with seven smartwatch users supported by approximately 29 crowd workers each , we validate that it is possible to manage the crowd writing process from a watch . Watch users captured new ideas as they came to mind and managed a crowd during spare moments while going about their daily routine . WearWrite represents a new approach to getting work done from wearables using the crowd .
2K_dev_987	We present a new algorithmic approach to the group fused lasso , a convex model that approximates a multi-dimensional signal via an approximately piecewise-constant signal . This model has found many applications in multiple change point detection , signal compression , and total variation denoising , though existing algorithms typically using first-order or alternating minimization schemes . In this paper we instead develop a specialized projected Newton method , combined with a primal active set approach , which we show to be substantially faster that existing methods . Furthermore , we present two applications that use this algorithm as a fast subroutine for a more complex outer loop : segmenting linear regression models for time series data , and color image denoising . We show that on these problems the proposed method performs very well , solving the problems faster than state-of-the-art methods and to higher accuracy .
2K_dev_988	Crowdsourced clustering approaches present a promising way to harness deep semantic knowledge for clustering complex information . However , existing approaches have difficulties supporting the global context needed for workers to generate meaningful categories , and are costly because all items require human judgments . We introduce Alloy , a hybrid approach that combines the richness of human judgments with the power of machine algorithms . Alloy supports greater global context through a new `` sample and search '' crowd pattern which changes the crowd 's task from classifying a fixed subset of items to actively sampling and querying the entire dataset . It also improves efficiency through a two phase process in which crowds provide examples to help a machine cluster the head of the distribution , then classify low-confidence examples in the tail . To accomplish this , Alloy introduces a modular `` cast and gather '' approach which leverages a machine learning backbone to stitch together different types of judgment tasks .
2K_dev_989	SkinTrack is a wearable system that enables continuous touch tracking on the skin . It consists of a ring , which emits a continuous high frequency AC signal , and a sensing wristband with multiple electrodes . Due to the phase delay inherent in a high-frequency AC signal propagating through the body , a phase difference can be observed between pairs of electrodes . SkinTrack measures these phase differences to compute a 2D finger touch coordinate . Our approach can segment touch events at 99 % accuracy , and resolve the 2D location of touches with a mean error of 7.6mm . As our approach is compact , non-invasive , low-cost and low-powered , we envision the technology being integrated into future smartwatches , supporting rich touch interactions beyond the confines of the small touchscreen .
2K_dev_990	Strong Nash equilibrium ( SNE ) is an appealing solution concept when rational agents can form coalitions . A strategy profile is an SNE if no coalition of agents can benefit by deviating . We present the first general-purpose algorithms for SNE finding in games with more than two agents . An SNE must simultaneously be a Nash equilibrium ( NE ) and the optimal solution of multiple non-convex optimization problems . This makes even the derivation of necessary and sufficient mathematical equilibrium constraints difficult . We show that forcing an SNE to be resilient only to pure-strategy deviations by coalitions , unlike for NEs , is only a necessary condition here . Second , we show that the application of Karush-Kuhn-Tucker conditions leads to another set of necessary conditions that are not sufficient . Third , we show that forcing the Pareto efficiency of an SNE for each coalition with respect to coalition correlated strategies is sufficient but not necessary . We then develop a tree search algorithm for SNE finding . At each node , it calls an oracle to suggest a candidate SNE and then verifies the candidate . We show that our new necessary conditions can be leveraged to make the oracle more powerful . Experiments validate the overall approach and show that the new conditions significantly reduce search tree size compared to using NE conditions alone .
2K_dev_991	How can we predict Smith 's main hobby if we know the main hobby of Smith 's friends ? Can we measure the confidence in our predic- tion if we are given the main hobby of only a few of Smith 's friends ? In this paper , we focus on how to estimate the confidence on the node classi- fication problem . Providing a confidence level for the classification prob- lem is important because most nodes in real world networks tend to have few neighbors , and thus , a small amount of evidence . Our contributions are three-fold : ( a ) novel algorithm ; we propose a semi-supervised learning algorithm that converges fast , and provides the confidence estimate ( b ) theoretical analysis ; we show the solid theoretical foundation of our algo- rithm and the connections to label propagation and Bayesian inference ( c ) empirical analysis ; we perform extensive experiments on three dif- ferent real networks . Specifically , the experimental results demonstrate that our algorithm outperforms other algorithms on graphs with less smoothness and low label density .
2K_dev_992	Given their large energy footprints and the availability of building energy management systems , airports are uniquely positioned to take advantage of demand response ( DR ) programs . Although a baselinethe estimation of what the load would have been without load reductionis essential to assess the performance of DR strategies , there has been very little published research on developing baselines for airports . Therefore , the research described in this paper aims to develop baseline models specially intended for airport facilities . Specifically , the authors propose piece-wise linear regression models for predicting electricity demand using time-of-week , temperature , and flight schedule information . For the given period of April and May , test results reveals that a model , which has trained over specific seasonal data with only time-of-week and temperature as inputs , has the best prediction performance . The number of passengers of departure flight schedules is shown to have a positive relationship to the load , but does not improve the model accuracy significantly . However , since this study is done for the spring season , when heating , ventilating , and air conditioning ( HVAC ) systems run the least , the results may not represent other seasons with high cooling or heating demand . Therefore , further studies should be carried out to conclude the potential of flight schedules in improving accuracies of energy prediction baselines .
2K_dev_993	The fair division of indivisible goods has long been an important topic in economics and , more recently , computer science . We investigate the existence of envyfree allocations of indivisible goods , that is , allocations where each player values her own allocated set of goods at least as highly as any other player 's allocated set of goods . Under additive valuations , we show that even when the number of goods is larger than the number of agents by a linear fraction , envy-free allocations are unlikely to exist.We then show that when the number of goods is larger by a logarithmic factor , such allocations exist with high probability . We support these results experimentally and show that the asymptotic behavior of the theory holds even when the number of goods and agents is quite small . We demonstrate that there is a sharp phase transition from nonexistence to existence of envy-free allocations , and that on average the computational problem is hardest at that transition .
2K_dev_994	A method and apparatus that resolve near touch ambiguities in a touch screen includes detecting a touch screen touch event and detecting a vibro-acoustic event . These events generate signals received respectively by two different sensors and/or processes . When the two events occur within a pre-defined window of time , they may be considered to be part of the same touch event and may signify a true touch .
2K_dev_995	Cyber-physical systems ( CPS ) combine cyber aspects such as communication and computer control with physical aspects such as movement in space , which arise frequently in many safety-critical application domains , including aviation , automotive , railway , and robotics . But how can we ensure that these systems are guaranteed to meet their design goals , e.g. , that an aircraft will not crash into another one ? This tutorial focuses on the most elementary CPS model : hybrid systems , which are dynamical systems with interacting discrete transitions and continuous evolutions along differential equations . It describes a compositional programming language for hybrid systems and shows how to specify and verify correctness properties of hybrid systems in differential dynamic logic . Extensions of this logic that support CPS models with more general dynamics will be surveyed briefly . In addition to providing a strong theoretical foundation for CPS , differential dynamic logics have also been instrumental in verifying many applications , including the Airborne Collision Avoidance System ACAS X , the European Train Control System ETCS , several automotive systems , mobile robot navigation with the dynamic window algorithm , and a surgical robotic system for skull-base surgery . The approach is implemented in the theorem prover KeYmaera X .
2K_dev_996	Non-Technical Loss ( NTL ) represents a major challenge when providing reliable electrical service in developing countries , where it often accounts for 11-15 % of total generation capacity [ 1 ] . NTL is caused by a variety of factors such as theft , unmetered homes , and inability to pay , which at volume can lead to system instability , grid failure , and major financial losses for providers . In this paper , we investigate error sources and techniques for separating NTL from total losses in microgrids . We adopt and compare two classes of approaches for detecting NTL : ( 1 ) model- driven and ( 2 ) data- driven . The model-driven class considers the primary sources of state uncertainty including line losses , meter consumption , meter calibration error , packet loss , and sample synchronization error . In the data-driven class , we use two approaches that learn grid state based on training data . The first approach uses a regression technique on an NTL-free period of grid operation to capture the relationship between state error and total consumption . The second approach uses an SVM trained on synthetic NTL data . Both classes of approaches can provide a confidence interval based on the amount of detected NTL . We experimentally evaluate and compare the approaches on wireless meter data collected from a 525-home microgrid deployed in Les Anglais , Haiti . We see that both are quite effective , but that the data-driven class is significantly easier to implement . In both cases , we are able to experimentally evaluate to what degree we can reliably separate NTL from total losses .
2K_dev_997	Kidney exchanges are organized markets where patients swap willing but incompatible donors . In the last decade , kidney exchanges grew from small and regional to large and national -- -and soon , international . This growth results in more lives saved , but exacerbates the empirical hardness of the $ \mathcal { NP } $ -complete problem of optimally matching patients to donors . State-of-the-art matching engines use integer programming techniques to clear fielded kidney exchanges , but these methods must be tailored to specific models and objective functions , and may fail to scale to larger exchanges . In this paper , we observe that if the kidney exchange compatibility graph can be encoded by a constant number of patient and donor attributes , the clearing problem is solvable in polynomial time . We give necessary and sufficient conditions for losslessly shrinking the representation of an arbitrary compatibility graph . Then , using real compatibility graphs from the UNOS nationwide kidney exchange , we show how many attributes are needed to encode real compatibility graphs . The experiments show that , indeed , small numbers of attributes suffice .
2K_dev_998	Friendsourcing consists of broadcasting questions and help requests to friends on social networking sites . Despite its potential value , friendsourcing requests often fall on deaf ears . One way to improve response rates and motivate friends to undertake more effortful tasks may be to offer extrinsic rewards , such as money or a gift , for responding to friendsourcing requests . However , past research suggests that these extrinsic rewards can have unintended consequences , including undermining intrinsic motivations and undercutting the relationship between people . To explore the effects of extrinsic reward on friends ' response rate and perceived relationship , we conducted an experiment on a new friendsourcing platform - Mobilyzr . Results indicate that large extrinsic rewards increase friends ' response rates without reducing the relationship strength between friends . Additionally , the extrinsic rewards allow requesters to explain away the failure of friendsourcing requests and thus preserve their perceptions of relationship ties with friends .
2K_dev_999	Abstract In studying the strength and specificity of interaction between members of two protein families , key questions center on which pairs of possible partners actually interact , how well they interact , and why they interact while others do not . The advent of large-scale experimental studies of interactions between members of a target family and a diverse set of possible interaction partners offers the opportunity to address these questions . We develop here a method , DgSpi ( data-driven graphical models of specificity in protein : protein interactions ) , for learning and using graphical models that explicitly represent the amino acid basis for interaction specificity ( why ) and extend earlier classification-oriented approaches ( which ) to predict the G of binding ( how well ) . We demonstrate the effectiveness of our approach in analyzing and predicting interactions between a set of 82 PDZ recognition modules against a panel of 217 possible peptide partners , based on data from MacBeath and colleagues . Our pred ...
2K_dev_1000	Crowdsourcing offers a powerful new paradigm for online work . However , real world tasks are often interdependent , requiring a big picture view of the difference pieces involved . Existing crowdsourcing approaches that support such tasks -- ranging from Wikipedia to flash teams -- are bottlenecked by relying on a small number of individuals to maintain the big picture . In this paper , we explore the idea that a computational system can scaffold an emerging interdependent , big picture view entirely through the small contributions of individuals , each of whom sees only a part of the whole . To investigate the viability , strengths , and weaknesses of this approach we instantiate the idea in a prototype system for accomplishing distributed information synthesis and evaluate its output across a variety of topics . We also contribute a set of design patterns that may be informative for other systems aimed at supporting big picture thinking in small pieces .
2K_dev_1001	We investigate synergy , or lack thereof , between agents in co-operative games , building on the popular notion of Shapley value . We think of a pair of agents as synergistic ( resp. , antagonistic ) if the Shapley value of one agent when the other agent participates in a joint effort is higher ( resp . lower ) than when the other agent does not participate . Our main theoretical result is that any graph specifying synergistic and antagonistic pairs can arise even from a restricted class of cooperative games . We also study the computational complexity of determining whether a given pair of agents is synergistic . Finally , we use the concepts developed in the paper to uncover the structure of synergies in two real-world organizations , the European Union and the International Monetary Fund .
2K_dev_1002	How do users behave if they can tag each other in social networks ? In this paper , we answer this question by studying the interactive tagging network constructed by Twitter lists . Twitter lists can be regarded as the tagging process ; a user ( i.e. , tagger ) creates a list with a name ( i.e. , tag ) and adds other users ( i.e. , tagged users ) into the list . This tagging network is by nature different from the resource tagging networks ( e.g. , Flickr and Delicious ) because users on this network can tag each other . We address the following research questions : ( RQ1 ) What is the common patterns and the difference between the interactive tagging network and the resource tagging networks ? ( RQ2 ) Do users tag each other on the interactive tagging network ? And if so , to what extent ? ( RQ3 ) What is the difference between the two types of relationships on Twitter : who-tags-whom and who-follows-whom ? By quantitatively studying million-scale networks , we found the pervasive patterns across the different tagging networks , and the interactive patterns within the interactive tagging network . This study sheds light on the underlying characteristics of the interactive tagging network , which is relevant to the social scientists and the system designers of the tagging systems .
2K_dev_1003	Social media is an increasingly important part of modern life . We investigate the use of and usability of Twitter by blind users , via a combination of surveys of blind Twitter users , large-scale analysis of tweets from and Twitter profiles of blind and sighted users , and analysis of tweets containing embedded imagery . While Twitter has traditionally been thought of as the most accessible social media platform for blind users , Twitter 's increasing integration of image content and users ' diverse uses for images have presented emergent accessibility challenges . Our findings illuminate the importance of the ability to use social media for people who are blind , while also highlighting the many challenges such media currently present this user base , including difficulty in creating profiles , in awareness of available features and settings , in controlling revelations of one 's disability status , and in dealing with the increasing pervasiveness of image-based content . We propose changes that Twitter and other social platforms should make to promote fuller access to users with visual impairments .
2K_dev_1004	Utilising both key mathematical tools and state-of-the-art research results , this text explores the principles underpinning large-scale information processing over networks and examines the crucial interaction between big data and its associated communication , social and biological networks . Written by experts in the diverse fields of machine learning , optimisation , statistics , signal processing , networking , communications , sociology and biology , this book employs two complementary approaches : first analysing how the underlying network constrains the upper-layer of collaborative big data processing , and second , examining how big data processing may boost performance in various networks . Unifying the broad scope of the book is the rigorous mathematical treatment of the subjects , which is enriched by in-depth discussion of future directions and numerous open-ended problems that conclude each chapter . Readers will be able to master the fundamental principles for dealing with big data over large systems , making it essential reading for graduate students , scientific researchers and industry practitioners alike .
2K_dev_1005	We focus on detecting complex events in unconstrained Internet videos . While most existing works rely on the abundance of labeled training data , we consider a more difficult zero-shot setting where no training data is supplied . We first pre-train a number of concept classifiers using data from other sources . Then we evaluate the semantic correlation of each concept w.r.t . the event of interest . After further refinement to take prediction inaccuracy and discriminative power into account , we apply the discovered concept classifiers on all test videos and obtain multiple score vectors . These distinct score vectors are converted into pairwise comparison matrices and the nuclear norm rank aggregation framework is adopted to seek consensus . To address the challenging optimization formulation , we propose an efficient , highly scalable algorithm that is an order of magnitude faster than existing alternatives . Experiments on recent TRECVID datasets verify the superiority of the proposed approach .
2K_dev_1006	We introduce disciplined convex stochastic programming ( DCSP ) , a modeling framework that can significantly lower the barrier for modelers to specify and solve convex stochastic optimization problems , by allowing modelers to naturally express a wide variety of convex stochastic programs in a manner that reflects their underlying mathematical representation . DCSP allows modelers to express expectations of arbitrary expressions , partial optimizations , and chance constraints across a wide variety of convex optimization problem families ( e.g. , linear , quadratic , second order cone , and semidefinite programs ) . We illustrate DCSP 's expressivity through a number of sample implementations of problems drawn from the operations research , finance , and machine learning literatures .
2K_dev_1007	Nudging behaviors through user interface design is a practice that is well-studied in HCI research . Corporations often use this knowledge to modify online interfaces to influence user information disclosure . In this paper , we experimentally test the impact of a norm-shaping design patterns on information divulging behavior . We show that ( 1 ) a set of images , biased toward more revealing figures , change subjects ' personal views of appropriate information to share ; ( 2 ) that shifts in perceptions significantly increases the probability that a subject divulges personal information ; and ( 3 ) that these shift also increases the probability that the subject advises others to do so . Our main contribution is empirically identifying a key mechanism by which norm-shaping designs can change beliefs and subsequent disclosure behaviors .
2K_dev_1008	A class of models for describing sets of time series generated by interacting agents using directed , weighted graphs is introduced . A computationally tractable algorithm for estimating the graph adjacency matrix of this model from observed time series data is presented . The performance guarantees of this algorithm for prediction are outlined under several assumptions on the properties of the dynamics of the system of agents and on the true values of the parameters . These guarantees are tested empirically through simulation studies using several random graph models .
2K_dev_1009	Finding densely connected subgraphs , also called communities , in networks are of interest for many applications . In previous work , we showed an optimization method for efficiently finding subgraphs denser than the overall network [ 1 ] . This result is derived from our studies of network processes , dynamical processes that model interactions between individual agents in networks ( i.e. , spread of infection or cascading failures ) . In this paper , we prove that these subgraphs are also unique in the sense that there are no other subgraphs in the network isomorphic to these subgraphs .
2K_dev_1010	In this paper we provide faster algorithms for solving the geometric median problem : given n points in d compute a point that minimizes the sum of Euclidean distances to the points . This is one of the oldest non-trivial problems in computational geometry yet despite a long history of research the previous fastest running times for computing a ( 1+ ) -approximate geometric median were O ( d n 4/3 8/3 ) by Chin et . al , O ( d exp 4 log 1 ) by Badoiu et . al , O ( nd + poly ( d , 1 ) ) by Feldman and Langberg , and the polynomial running time of O ( ( nd ) O ( 1 ) log1/ ) by Parrilo and Sturmfels and Xue and Ye . In this paper we show how to compute such an approximate geometric median in time O ( nd log 3 n / ) and O ( d 2 ) . While our O ( d 2 ) is a fairly straightforward application of stochastic subgradient descent , our O ( nd log 3 n / ) time algorithm is a novel long step interior point method . We start with a simple O ( ( nd ) O ( 1 ) log1/ ) time interior point method and show how to improve it , ultimately building an algorithm that is quite non-standard from the perspective of interior point literature . Our result is one of few cases of outperforming standard interior point theory . Furthermore , it is the only case we know of where interior point methods yield a nearly linear time algorithm for a canonical optimization problem that traditionally requires superlinear time .
2K_dev_1011	Computer security problems often occur when there are disconnects between users understanding of their role in computer security and what is expected of them . To help users make good security decisions more easily , we need insights into the challenges they face in their daily computer usage . We built and deployed the Security Behavior Observatory ( SBO ) to collect data on user behavior and machine configurations from participants home computers . Combining SBO data with user interviews , this paper presents a qualitative study comparing users attitudes , behaviors , and understanding of computer security to the actual states of their computers . Qualitative inductive thematic analysis of the interviews produced engagement as the overarching theme , whereby participants with greater engagement in computer security and maintenance did not necessarily have more secure computer states . Thus , user engagement alone may not be predictive of computer security . We identify several other themes that inform future directions for better design and research into security interventions . Our findings emphasize the need for better understanding of how users computers get infected , so that we can more effectively design user-centered mitigations .
2K_dev_1012	Methods and apparatus of embodiments of the present invention include a classification system configured to treat edge contact of a touch screen as a separate class of touch events such that any touches occurring near the edge of the touch screen are to be processed by a classifier that is configured to process edge contacts as compared to a classifier that is configured to process other contacts that may occur in the approximate middle of the touch screen which may be wholly digitized . An apparatus may employ two separate and distinct classifiers , including a full touch classifier and an edge touch classifier . The touch screen may be configured to have two different sensing regions to determine which of the two classifiers is appropriate for a touch event .
2K_dev_1013	The environment of a living cell is vastly different from that of an in vitro reaction system , an issue that presents great challenges to the use of in vitro models , or computer simulations based on them , for understanding biochemistry in vivo . Virus capsids make an excellent model system for such questions because they typically have few distinct components , making them amenable to in vitro and modeling studies , yet their assembly can involve complex networks of possible reactions that can not be resolved in detail by any current experimental technology . We previously fit kinetic simulation parameters to bulk in vitro assembly data to yield a close match between simulated and real data , and then used the simulations to study features of assembly that can not be monitored experimentally . The present work seeks to project how assembly in these simulations fit to in vitro data would be altered by computationally adding features of the cellular environment to the system , specifically the presence of nucleic acid about which many capsids assemble . The major challenge of such work is computational : simulating fine-scale assembly pathways on the scale and in the parameter domains of real viruses is far too computationally costly to allow for explicit models of nucleic acid interaction . We bypass that limitation by applying analytical models of nucleic acid effects to adjust kinetic rate parameters learned from in vitro data to see how these adjustments , singly or in combination , might affect fine-scale assembly progress . The resulting simulations exhibit surprising behavioral complexity , with distinct effects often acting synergistically to drive efficient assembly and alter pathways relative to the in vitro model . The work demonstrates how computer simulations can help us understand how assembly might differ between the in vitro and in vivo environments and what features of the cellular environment account for these differences .
2K_dev_1014	We present a novel extension of normal form games that we call biased games . In these games , a player 's utility is influenced by the distance between his mixed strategy and a given base strategy . We argue that biased games capture important aspects of the interaction between software agents . Our main result is that biased games satisfying certain mild conditions always admit an equilibrium . We also tackle the computation of equilibria in biased games .
2K_dev_1015	A kidney exchange is an organized barter market where patients in need of a kidney swap willing but incompatible donors . Determining an optimal set of exchanges is theoretically and empirically hard . Traditionally , exchanges took place in cycles , with each participating patient-donor pair both giving and receiving a kidney . The recent introduction of chains , where a donor without a paired patient triggers a sequence of donations without requiring a kidney in return , increased the efficacy of fielded kidney exchanges -- -while also dramatically raising the empirical computational hardness of clearing the market in practice . While chains can be quite long , unbounded-length chains are not desirable : planned donations can fail before transplant for a variety of reasons , and the failure of a single donation causes the rest of that chain to fail , so parallel shorter chains are better in practice . In this paper , we address the tractable clearing of kidney exchanges with short cycles and chains that are long but bounded . This corresponds to the practice at most modern fielded kidney exchanges . We introduce three new integer programming formulations , two of which are compact . Furthermore , one of these models has a linear programming relaxation that is exactly as tight as the previous tightest formulation ( which was not compact ) for instances in which each donor has a paired patient . On real data from the UNOS nationwide exchange in the United States and the NLDKSS nationwide exchange in the United Kingdom , as well as on generated realistic large-scale data , we show that our new models are competitive with all existing solvers -- -in many cases outperforming all other solvers by orders of magnitude . Finally , we note that our position-indexed chain-edge formulation can be modified in a straightforward way to take post-match edge failure into account , under the restriction that edges have equal probabilities of failure . Post-match edge failure is a primary source of inefficiency in presently-fielded kidney exchanges . We show how to implement such failure-aware matching in our model , and also extend the state-of-the-art general branch-and-price-based non-compact formulation for the failure-aware problem to run its pricing problem in polynomial time .
2K_dev_1016	Kidney exchange is a barter market where patients trade willing but medically incompatible donors . These trades occur via cycles , where each patient-donor pair both gives and receives a kidney , and via chains , which begin with an altruistic donor who does not require a kidney in return . For logistical reasons , the maximum length of a cycle is typically limited to a small constant , while chains can be much longer . Given a compatibility graph of patient-donor pairs , altruists , and feasible potential transplants between them , finding even a maximum-cardinality set of vertex-disjoint cycles and chains is NP-hard . There has been much work on developing provably optimal solvers that are efficient in practice . One of the leading techniques has been branch and price , where column generation is used to incrementally bring cycles and chains into the optimization model on an as-needed basis . In particular , only positive-price columns need to be brought into the model . We prove that finding a positive-price chain is NP-complete . This shows incorrectness of two leading branch-and-price solvers that suggested polynomial-time chain pricing algorithms .
2K_dev_1017	An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy . In a system with redundant requests , each job that arrives to the system is copied and dispatched to multiple servers . As soon as the first copy completes service , the job is considered complete , and all remaining copies are deleted . A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists . We propose a theoretical model of redundancy , the Redundancy-d system , in which each job sends redundant copies to d servers chosen uniformly at random . We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers . We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity .
2K_dev_1018	Complex networks have been shown to exhibit universal properties , with one of the most consistent patterns being the scale-free degree distribution , but are there regularities obeyed by the r-hop neighborhood in real networks ? We answer this question by identifying another power-law pattern that describes the relationship between the fractions of node pairs C ( r ) within r hops and the hop count r. This scale-free distribution is pervasive and describes a large variety of networks , ranging from social and urban to technological and biological networks . In particular , inspired by the definition of the fractal correlation dimension D2 on a point-set , we consider the hop-count r to be the underlying distance metric between two vertices of the network , and we examine the scaling of C ( r ) with r. We find that this relationship follows a power-law in real networks within the range 2 r d , where d is the effective diameter of the network , that is , the 90-th percentile distance . We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. We provide theoretical justification for this pattern under successful existing network models , while we analyze a large set of real and synthetic network datasets and we show the pervasiveness of the power-hop .
2K_dev_1019	Modernsmartphoneplatformshavemillionsofapps , manyofwhich request permissions to access private data and resources , like user accounts or location . While these smartphone platforms provide varying degrees of control over these permissions , the sheer number of decisions that users are expected to manage has been shown to be unrealistically high . Prior research has shown that users are often unaware of , if not uncomfortable with , many of their permission settings . Prior work also suggests that it is theoretically possible to predict many of the privacy settings a user would want by asking the user a small number of questions . However , this approach has neither been operationalized nor evaluated with actual users before . We report on a field study ( n=72 ) in which we implemented and evaluated a Personalized Privacy Assistant ( PPA ) with participants using their own Android devices . The results of our study are encouraging . We find that 78.7 % of the recommendations made by the PPA were adopted by users . Following initial recommendations on permission settings , participants were motivated to further review and modify their settings with daily privacy nudges . Despite showing substantial engagement with these nudges , participants only changed 5.1 % of the settings previously adopted based on the PPAs recommendations . The PPA and its recommendations were perceived as useful and usable . We discuss the implications of our results for mobile permission management and the design of personalized privacy assistant solutions .
2K_dev_1020	The rise of Internet-scale networks , such as web graphs and social media with hundreds of millions to billions of nodes , presents new scientific opportunities , such as overlapping community detection to discover the structure of the Internet , or to analyze trends in online social behavior . However , many existing probabilistic network models are difficult or impossible to deploy at these massive scales . We propose a scalable approach for modeling and inferring latent spaces in Internet-scale networks , with an eye towards overlapping community detection as a key application . By applying a succinct representation of networks as a bag of triangular motifs , developing a parsimonious statistical model , deriving an efficient stochastic variational inference algorithm , and implementing it as a distributed cluster program via the Petuum parameter server system , we demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours . Compared to other state-of-the-art probabilistic network approaches , our method is several orders of magnitude faster , with competitive or improved accuracy at overlapping community detection .
2K_dev_1021	This work addresses the main challenges in real-world application of guided-waves for damage detection of pipelines , namely their complex nature and sensitivity to environmental and operational conditions ( EOCs ) . Different propagation characteristics of the wave modes , their distinctive sensitivities to different types and ranges of EOCs , and to different damage scenarios , make the interpretation of diffuse-field guided-wave signals a challenging task . This paper proposes an unsupervised feature-extraction method for online damage detection of pipelines under varying EOCs . The objective is to simplify diffuse-field guided-wave signals to a sparse subset of the arrivals that contains the majority of the energy carried by the signal . We show that such a subset is less affected by EOCs compared to the complete time-traces of the signals . Moreover , it is shown that the effects of damage on the energy of this subset suppress those of EOCs . A set of signals from the undamaged state of a pipe are used as reference records . The reference dataset is used to extract the aforementioned sparse representation . During the monitoring stage , the sparse subset , representing the undamaged pipe , will not accurately reconstruct the energy of a signal from a damaged pipe . In other words , such a sparse representation of guided-waves is sensitive to occurrence of damage . Therefore , the energy estimation errors are used as damage-sensitive features for damage detection purposes . A diverse set of experimental analyses are conducted to verify the hypotheses of the proposed feature-extraction approach , and to validate the detection performance of the damage-sensitive features . The empirical validation of the proposed method includes ( 1 ) detecting a structural abnormality in an aluminum pipe , under varying temperature at different ranges , ( 2 ) detecting multiple small damages of different types , at different locations , in a steel pipe , under varying temperature , ( 3 ) detecting a structural abnormality in an operating hot-water piping system , under multiple varying EOCs , such as temperature , water flow rate , and inner pressure ; and ( 4 ) detecting a structural abnormality as the ratio of the damaged pipe 's signals in the reference dataset increases .
2K_dev_1022	While Bayesian methods are praised for their ability to incorporate useful prior knowledge , in practice , priors that allow for computationally convenient or tractable inference are more commonly used . In this paper , we investigate the following question : for a given model , is it possible to use any convenient prior to infer a false posterior , and afterwards , given some true prior of interest , quickly transform this result into the true posterior ? We present a procedure to carry out this task : given an inferred false posterior and true prior , our algorithm generates samples from the true posterior . This transformation procedure , which we call `` prior swapping '' works for arbitrary priors . Notably , its cost is independent of data size . It therefore allows us , in some cases , to apply significantly less-costly inference procedures to more-sophisticated models than previously possible . It also lets us quickly perform any additional inferences , such as with updated priors or for many different hyperparameter settings , without touching the data . We prove that our method can generate asymptotically exact samples , and demonstrate it empirically on a number of models and priors .
2K_dev_1023	We propose a family of models to study the evolution of ties in a network of interacting agents by reinforcement and penalization of their connections according to certain local laws of interaction . The family of stochastic dynamical systems , on the edges of a graph , exhibits \emph { good } convergence properties , in particular , we prove a strong-stability result : a subset of binary matrices or graphs -- characterized by certain compatibility properties -- is a global almost sure attractor of the family of stochastic dynamical systems . To illustrate finer properties of the corresponding strong attractor , we present some simulation results that capture , e.g. , the conspicuous phenomenon of emergence and downfall of leaders in social networks .
2K_dev_1024	A method of classifying touch screen events uses known non-random patterns of touch events over short periods of time to increase the accuracy of analyzing such events . The method takes advantage of the fact that after one touch event , certain actions are more likely to follow than others . Thus if a touch event is classified as a knock , and then within 500 ms a new event in a similar location occurs , but the classification confidence is low ( e.g. , 60 % nail , 40 % knuckle ) , the classifier may add weight to the knuckle classification since this touch sequence is far more likely . Knowledge about the probabilities of follow-on touch events can be used to bias subsequent classification , adding weight to particular events .
2K_dev_1025	An age-old problem in the design of server farms is the choice of the task assignment policy . This is the algorithm that determines how to assign incoming jobs to servers . Popular policies include Round-Robin assignment , Join-the-Shortest-Queue , Join-Queue-with-Least-Work , and so on . While much research has studied assignment policies , little has taken into account server-side variability -- the fact that the server we choose might be temporarily and unpredictably slow . We show that when server-side variability dominates runtime , replication of jobs can be very beneficial . We introduce the Replication-d algorithm that replicates each arrival to d servers chosen at random , where the job is considered `` done '' as soon as the first replica completes . We provide an exact closed-form analysis of Replication-d. We next introduce a much more general model , one which takes both the inherent job size distribution and the server-side variability into account . This is a departure from traditional queueing models which only allow for one `` size '' distribution . We propose and analyze a new task assignment policy , Replicate-Idle-Queue ( RIQ ) , which is designed to perform well given these dual sources of variability .
2K_dev_1026	Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled a myriad of new applications in many different domains from personal entertainment to process and infrastructure online monitoring in large industrial sites , among other . Our work focuses on how one can use several small UAVs collaboratively to provide extended reach to an online video monitoring system . We demonstrate how a TDMA overlay using 802.11 radios on low-cost commercial-off-the-shelf ( COTS ) UAVs can be used to enable high channel utilization in multi-hop networks , by avoiding mutual interference . This paper presents an extensive network characterisation and modelling of the quality of the UAV-to-UAV link , in terms of packet delivery ratio as a function of distance , packet size and orientation . We show that this platform is non-omnidirectional in the flight plane and that UAV-to-UAV communication ceases around 75m . Then , we solve the mathematical problem of finding the optimal link length and number of hops that maximize the end-to-end throughput , as we extend the network . We validate our mathematical model with extensive experimental campaigns transmitting payloads up to 200m ( over 802.11g @ 54MBps ) .
2K_dev_1027	More than 10 % of the population has dyslexia , and most are diagnosed only after they fail in school . This work seeks to change this through scalable early detection via machine learning models that predict reading and writing difficulties by watching how people interact with a linguistic web-based game : Dytective . The design of Dytective is based on ( i ) the empirical linguistic analysis of the errors that people with dyslexia make , ( ii ) principles of language acquisition , and ( iii ) specific linguistic skills related to dyslexia . Experiments with 243 children and adults ( 95 with diagnosed dyslexia ) revealed differences in how people with dyslexia read and write . We trained a machine learning model that was able to predict dyslexia with 83 % accuracy in a held-out test set with 100 participants . Currently , we are working with schools to put our approach into practice at scale to reduce school failure as a primary way dyslexia is diagnosed .
2K_dev_1028	Motivation : Most methods for reconstructing response networks from high throughput data generate static models which can not distinguish between early and late response stages . Results : We present TimePath , a new method that integrates time series and static datasets to reconstruct dynamic models of host response to stimulus . TimePath uses an Integer Programming formulation to select a subset of pathways that , together , explain the observed dynamic responses . Applying TimePath to study human response to HIV-1 led to accurate reconstruction of several known regulatory and signaling pathways and to novel mechanistic insights . We experimentally validated several of TimePaths predictions highlighting the usefulness of temporal models . Availability and Implementation : Data , Supplementary text and the TimePath software are available from http : //sb.cs.cmu.edu/timepath Contact : ude.umc.sc @ jbviz Supplementary information : Supplementary data are available at Bioinformatics online .
2K_dev_1029	We propose an explicitly discriminative and 'simple ' approach to generate invariance to nuisance transformations modeled as unitary . In practice , the approach works well to handle non-unitary transformations as well . Our theoretical results extend the reach of a recent theory of invariance to discriminative and kernelized features based on unitary kernels . As a special case , a single common framework can be used to generate subject-specific pose-invariant features for face recognition and vice-versa for pose estimation . We show that our main proposed method ( DIKF ) can perform well under very challenging large-scale semisynthetic face matching and pose estimation protocols with unaligned faces using no landmarking whatsoever . We additionally benchmark on CMU MPIE and outperform previous work in almost all cases on off-angle face matching while we are on par with the previous state-of-the-art on the LFW unsupervised and image-restricted protocols , without any low-level image descriptors other than raw-pixels .
2K_dev_1030	Large graphs are prevalent in many applications and enable a variety of information dissemination processes , e.g. , meme , virus , and influence propagation . How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way ( e.g. , stop a virus propagation , facilitate the propagation of a piece of good idea , etc ) ? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models . In this paper , we study the problem of how to optimally place a set of edges ( e.g. , edge deletion and edge addition ) to optimize the leading eigenvalue of the underlying graph , so that we can guide the dissemination process in a desired way . We propose effective , scalable algorithms for edge deletion and edge addition , respectively . In addition , we reveal the intrinsic relationship between edge deletion and node deletion problems . Experimental results validate the effectiveness and efficiency of the proposed algorithms .
2K_dev_1031	Robust face detection in the wild is one of the ultimate components to support various facial related problems , i.e . unconstrained face recognition , facial periocular recognition , facial landmarking and pose estimation , facial expression recognition , 3D facial model construction , etc . Although the face detection problem has been intensely studied for decades with various commercial applications , it still meets problems in some real-world scenarios due to numerous challenges , e.g . heavy facial occlusions , extremely low resolutions , strong illumination , exceptionally pose variations , image or video compression artifacts , etc . In this paper , we present a face detection approach named Contextual Multi-Scale Region-based Convolution Neural Network ( CMS-RCNN ) to robustly solve the problems mentioned above . Similar to the region-based CNNs , our proposed network consists of the region proposal component and the region-of-interest ( RoI ) detection component . However , far apart of that network , there are two main contributions in our proposed network that play a significant role to achieve the state-of-the-art performance in face detection . Firstly , the multi-scale information is grouped both in region proposal and RoI detection to deal with tiny face regions . Secondly , our proposed network allows explicit body contextual reasoning in the network inspired from the intuition of human vision system . The proposed approach is benchmarked on two recent challenging face detection databases , i.e . the WIDER FACE Dataset which contains high degree of variability , as well as the Face Detection Dataset and Benchmark ( FDDB ) . The experimental results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin , and consistently achieves competitive results on FDDB against the recent state-of-the-art face detection methods .
2K_dev_1032	Recently fair division theory has emerged as a promising approach for allocation of multiple computational resources among agents . While in reality agents are not all present in the system simultaneously , previous work has studied static settings where all relevant information is known upfront . Our goal is to better understand the dynamic setting . On the conceptual level , we develop a dynamic model of fair division , and propose desirable axiomatic properties for dynamic resource allocation mechanisms . On the technical level , we construct two novel mechanisms that provably satisfy some of these properties , and analyze their performance using real data . We believe that our work informs the design of superior multiagent systems , and at the same time expands the scope of fair division theory by initiating the study of dynamic and fair resource allocation mechanisms .
2K_dev_1033	Systems and methods are provided that determine when an initial stroke and a subsequent stroke track may be part of a common user input action . A method may include receiving a signal from which an initial stroke track representing an initial movement of a user controlled indicator against a touch sensitive surface and sensing a subsequent stroke track representing subsequent movement of the user controlled indicator against the touch sensitive surface can be determined . The method further includes determining that the initial stroke track and the subsequent stroke track comprise portions of common user input action when the initial stroke track is followed by the subsequent stroke track within a predetermined period of time and a trajectory of the initial stroke track is consistent with a trajectory of the subsequent stroke track .
2K_dev_1034	We revisit the classic problem of estimating the population mean of an unknown single-dimensional distribution from samples , taking a game-theoretic viewpoint . In our setting , samples are supplied by strategic agents , who wish to pull the estimate as close as possible to their own value . In this setting , the sample mean gives rise to manipulation opportunities , whereas the sample median does not . Our key question is whether the sample median is the best ( in terms of mean squared error ) truthful estimator of the population mean . We show that when the underlying distribution is symmetric , there are truthful estimators that dominate the median . Our main result is a characterization of worst-case optimal truthful estimators , which provably outperform the median , for possibly asymmetric distributions with bounded support .
2K_dev_1035	This paper develops an approach for efficiently solving general convex optimization problems specified as disciplined convex programs ( DCP ) , a common general-purpose modeling framework . Specifically we develop an algorithm based upon fast epigraph projections , projections onto the epigraph of a convex function , an approach closely linked to proximal operator methods . We show that by using these operators , we can solve any disciplined convex program without transforming the problem to a standard cone form , as is done by current DCP libraries . We then develop a large library of efficient epigraph projection operators , mirroring and extending work on fast proximal algorithms , for many common convex functions . Finally , we evaluate the performance of the algorithm , and show it often achieves order of magnitude speedups over existing general-purpose optimization solvers .
2K_dev_1036	Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines . Yet there has been relatively little work in modeling the behaviors and temporal properties of websites , as most research focuses on detecting whether a website distributes malware . In this paper we ask : How does web-based malware spread ? We conduct an extensive study and follow a website-centric and user-centric point of view . We collect data from four online databases , including Symantec 's WINE Project , for a total of more than 600K malicious URLs and over 500K users . First , we find that legitimate but compromised websites constitute 33.1 % of the malicious websites in our dataset . In order to conduct this study , we develop a classifier to distinguish between compromised vs. malicious websites with an accuracy of 95.3 % , which could be of interest to studies on website profiling . Second , we find that malicious URLs can be surprisingly long-lived , with 10 % of malicious sites staying active for three months or more . Third , we observe that a significant number of URLs exhibit the same temporal pattern that suggests a flush-crowd behavior , inflicting most of their damage during the first few days of appearance . Finally , the distribution of the visits to malicious sites per user is skewed , with 1.4 % of users visiting more than 10 malicious sites in 8 months . Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models .
2K_dev_1037	The design of revenue-maximizing combinatorial auctions , i.e . multi item auctions over bundles of goods , is one of the most fundamental problems in computational economics , unsolved even for two bidders and two items for sale . In the traditional economic models , it is assumed that the bidders ' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution . Despite this strong and oftentimes unrealistic assumption , it is remarkable that the revenue-maximizing combinatorial auction remains unknown . In recent years , automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions . The most scalable automated mechanism design algorithms take as input samples from the bidders ' valuation distribution and then search for a high-revenue auction in a rich auction class . In this work , we provide the first sample complexity analysis for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design . In particular , we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying , unknown distribution over bidder valuations , for each of the auction classes in the hierarchy . In addition to helping set automated mechanism design on firm foundations , our results also push the boundaries of learning theory . In particular , the hypothesis functions used in our contexts are defined through multi stage combinatorial optimization procedures , rather than simple decision boundaries , as are common in machine learning .
2K_dev_1038	Online content have become an important medium to disseminate information and express opinions . With their proliferation , users are faced with the problem of missing the big picture in a sea of irrelevant and/or diverse content . In this paper , we addresses the problem of information organization of online document collections , and provide algorithms that create a structured representation of the otherwise unstructured content . We leverage the expressiveness of latent probabilistic models ( e.g. , topic models ) and non-parametric Bayes techniques ( e.g. , Dirichlet processes ) , and give online and distributed inference algorithms that scale to terabyte datasets and adapt the inferred representation with the arrival of new documents . This paper is an extended abstract of the 2012 ACM SIGKDD best doctoral dissertation award of Ahmed [ 2011 ] .
2K_dev_1039	Motivation : It remains a challenge to detect associations between genotypes and phenotypes because of insufficient sample sizes and complex underlying mechanisms involved in associations . Fortunately , it is becoming more feasible to obtain gene expression data in addition to genotypes and phenotypes , giving us new opportunities to detect true genotypephenotype associations while unveiling their association mechanisms . Results : In this article , we propose a novel method , NETAM , that accurately detects associations between SNPs and phenotypes , as well as gene traits involved in such associations . We take a network-driven approach : NETAM first constructs an association network , where nodes represent SNPs , gene traits or phenotypes , and edges represent the strength of association between two nodes . NETAM assigns a score to each path from an SNP to a phenotype , and then identifies significant paths based on the scores . In our simulation study , we show that NETAM finds significantly more phenotype-associated SNPs than traditional genotypephenotype association analysis under false positive control , taking advantage of gene expression data . Furthermore , we applied NETAM on late-onset Alzheimers disease data and identified 477 significant path associations , among which we analyzed paths related to beta-amyloid , estrogen , and nicotine pathways . We also provide hypothetical biological pathways to explain our findings . Availability and implementation : Software is available at http : //www.sailing.cs.cmu.edu/ . Contact : ude.umc.sc @ gnixpe
2K_dev_1040	We introduce the Multiple Quantile Graphical Model ( MQGM ) , which extends the neighborhood selection approach of Meinshausen and Buhlmann for learning sparse graphical models . The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others . Our approach models a set of conditional quantiles of one variable as a sparse function of all others , and hence offers a much richer , more expressive class of conditional distribution estimates . We establish that , under suitable regularity conditions , the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows , even outside of the usual homoskedastic Gaussian data model . We develop an efficient algorithm for fitting the MQGM using the alternating direction method of multipliers . We also describe a strategy for sampling from the joint distribution that underlies the MQGM estimate . Lastly , we present detailed experiments that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data .
2K_dev_1041	'Alice ' is submitting one web search per five minutes , for three hours in a row - is it normal ? How to detect abnormal search behaviors , among Alice and other users ? Is there any distinct pattern in Alice 's ( or other users ' ) search behavior ? We studied what is probably the largest , publicly available , query log that contains more than 30 million queries from 0.6 million users . In this paper , we present a novel , user-and group-level framework , M3A : Model , MetaModel and Anomaly detection . For each user , we discover and explain a surprising , bi-modal pattern of the inter-arrival time ( IAT ) of landed queries ( queries with user click-through ) . Specifically , the model Camel-Log is proposed to describe such an IAT distribution ; we then notice the correlations among its parameters at the group level . Thus , we further propose the metamodel Meta-Click , to capture and explain the two-dimensional , heavy-tail distribution of the parameters . Combining Camel-Log and Meta-Click , the proposed M3A has the following strong points : ( 1 ) the accurate modeling of marginal IAT distribution , ( 2 ) quantitative interpretations , and ( 3 ) anomaly detection .
2K_dev_1042	The GraphBLAS standard ( GraphBlas.org ) is being developed to bring the potential of matrix-based graph algorithms to the broadest possible audience . Mathematically , the GraphBLAS defines a core set of matrix-based graph operations that can be used to implement a wide class of graph algorithms in a wide range of programming environments . This paper provides an introduction to the mathematics of the GraphBLAS . Graphs represent connections between vertices with edges . Matrices can represent a wide range of graphs using adjacency matrices or incidence matrices . Adjacency matrices are often easier to analyze while incidence matrices are often better for representing data . Fortunately , the two are easily connected by matrix multiplication . A key feature of matrix mathematics is that a very small number of matrix operations can be used to manipulate a very wide range of graphs . This composability of a small number of operations is the foundation of the GraphBLAS . A standard such as the GraphBLAS can only be effective if it has low performance overhead . Performance measurements of prototype GraphBLAS implementations indicate that the overhead is low .
2K_dev_1043	The large number of user-generated videos uploaded on to the Internet everyday has led to many commercial video search engines , which mainly rely on text metadata for search . However , metadata is often lacking for user-generated videos , thus these videos are unsearchable by current search engines . Therefore , content-based video retrieval ( CBVR ) tackles this metadata-scarcity problem by directly analyzing the visual and audio streams of each video . CBVR encompasses multiple research topics , including low-level feature design , feature fusion , semantic detector training and video search/reranking . We present novel strategies in these topics to enhance CBVR in both accuracy and speed under different query inputs , including pure textual queries and query by video examples . Our proposed strategies have been incorporated into our submission for the TRECVID 2014 Multimedia Event Detection evaluation , where our system outperformed other submissions in both text queries and video example queries , thus demonstrating the effectiveness of our proposed approaches .
2K_dev_1044	Recent computer systems research has proposed using redundant requests to reduce latency . The idea is to run a request on multiple servers and wait for the first completion ( discarding all remaining copies of the request ) . However , there is no exact analysis of systems with redundancy . This paper presents the first exact analysis of systems with redundancy . We allow for any number of classes of redundant requests , any number of classes of non-redundant requests , any degree of redundancy , and any number of heterogeneous servers . In all cases we derive the limiting distribution of the state of the system . In small ( two or three server ) systems , we derive simple forms for the distribution of response time of both the redundant classes and non-redundant classes , and we quantify the `` gain '' to redundant classes and `` pain '' to non-redundant classes caused by redundancy . We find some surprising results . First , the response time of a fully redundant class follows a simple exponential distribution and that of the non-redundant class follows a generalized hyperexponential . Second , fully redundant classes are `` immune '' to any pain caused by other classes becoming redundant . We also compare redundancy with other approaches for reducing latency , such as optimal probabilistic splitting of a class among servers ( Opt-Split ) and join-the-shortest-queue ( JSQ ) routing of a class . We find that , in many cases , redundancy outperforms JSQ and Opt-Split with respect to overall response time , making it an attractive solution .
2K_dev_1045	Work in human-computer interaction has generally assumed either a single user or a group of users working together in a shared virtual space . Recent crowd-powered systems use a different model in which a dynamic group of individuals ( the crowd ) collectively form a single actor that responds to real-time performance tasks , e.g. , controlling an on-screen character , driving a robot , or operating an existing desktop interface . In this paper , we introduce the idea of the crowd actor as a way to model coordination strategies and resulting collective performance , and discuss how the crowd actor is influenced not only by the domain on which it is asked to operate but also by the personality endowed to it by algorithms used to combine the inputs of constituent participants . Nowhere is the focus on the individual performer more finely resolved than in the study of the human psychomotor system , a mainstay topic in psychology that , largely owing to Fitts law , also has a legacy in HCI . Therefore , we explored our notion of a crowd actor by modeling the crowd as a individual motor system performing pointing tasks . We combined the input of 200 participants in a controlled offline experiment to demonstrate the inherent trade-offs between speed and errors based on personality , the number of constituent individuals , and the mechanism used to distribute work across the group . Finally , 10 workers participated in a synchronous experiment to explore how the crowd actor responds in a real online setting . This work contributes to the beginning of a predictive science for the general crowd actor model .
2K_dev_1046	We propose an identity-aware multi-object tracker based on the solution path algorithm . Our tracker not only produces identity-coherent trajectories based on cues such as face recognition , but also has the ability to pinpoint potential tracking errors . The tracker is formulated as a quadratic optimization problem with l0 norm constraints , which we propose to solve with the solution path algorithm . The algorithm successively solves the same optimization problem but under different lp norm constraints , where p gradually decreases from 1 to 0 . Inspired by the success of the solution path algorithm in various machine learning tasks , this strategy is expected to converge to a better local minimum than directly minimizing the hardly solvable l0 norm or the roughly approximated l1 norm constraints . Furthermore , the acquired solution path complies with the `` decision making process '' of the tracker , which provides more insight to locating potential tracking errors . Experiments show that not only is our proposed tracker effective , but also the solution path enables automatic pinpointing of potential tracking failures , which can be readily utilized in an active learning framework to improve identity-aware multi-object tracking .
2K_dev_1047	In this work , we have undertaken the task of occlusion and low-resolution robust facial gender classification . Inspired by the trainable attention model via deep architecture , and the fact that the periocular region is proven to be the most salient region for gender classification purposes , we are able to design a progressive convolutional neural network training paradigm to enforce the attention shift during the learning process . The hope is to enable the network to attend to particular high-profile regions ( e.g . the periocular region ) without the need to change the network architecture itself . The network benefits from this attention shift and becomes more robust towards occlusions and low-resolution degradations . With the progressively trained CNN models , we have achieved better gender classification results on the large-scale PCSO mugshot database with 400K images under occlusion and low-resolution settings , compared to the one undergone traditional training . In addition , our progressively trained network is sufficiently generalized so that it can be robust to occlusions of arbitrary types and at arbitrary locations , as well as low resolution .
2K_dev_1048	How can we correlate the neural activity in the human brain as it responds to typed words , with properties of these terms like 'edible ' , 'fits in hand ' ? In short , we want to find latent variables , that jointly explain both the brain activity , as well as the behavioral responses . This is one of many settings of the Coupled Matrix-Tensor Factorization CMTF problem .
2K_dev_1049	Cytometry Part A Early View ( Online Version of Record published before inclusion in an issue )
2K_dev_1050	Tensors or multiway arrays are functions of three or more indices $ ( i , j , k , \ldots ) $ similar to matrices ( two-way arrays ) , which are functions of two indices $ ( r , c ) $ for ( row , column ) . Tensors have a rich history , stretching over almost a century , and touching upon numerous disciplines ; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing , statistics , data mining , and machine learning . This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors . As such , it focuses on fundamentals and motivation ( using various application examples ) , aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software . Some background in applied optimization is useful but not strictly required . The material covered includes tensor rank and rank decomposition ; basic tensor factorization models and their relationships and properties ( including fairly good coverage of identifiability ) ; broad coverage of algorithms ranging from alternating optimization to stochastic gradient ; statistical performance analysis ; and applications ranging from source separation to collaborative filtering , mixture and topic modeling , classification , and multilinear subspace learning .
2K_dev_1051	Advances in fluorescence in situ hybridization ( FISH ) make it feasible to detect multiple copy-number changes in hundreds of cells of solid tumors . Studies using FISH , sequencing , and other technologies have revealed substantial intra-tumor heterogeneity . The evolution of subclones in tumors may be modeled by phylogenies . Tumors often harbor aneuploid or polyploid cell populations . Using a FISH probe to estimate changes in ploidy can guide the creation of trees that model changes in ploidy and individual gene copy-number variations . We present FISHtrees 3.0 , which implements a ploidy-based tree building method based on mixed integer linear programming ( MILP ) . The ploidy-based modeling in FISHtrees includes a new formulation of the problem of merging trees for changes of a single gene into trees modeling changes in multiple genes and the ploidy . When multiple samples are collected from each patient , varying over time or tumor regions , it is useful to evaluate similarities in tumor progression among the samples . Therefore , we further implemented in FISHtrees 3.0 a new method to build consensus graphs for multiple samples . We validate FISHtrees 3.0 on a simulated data and on FISH data from paired cases of cervical primary and metastatic tumors and on paired breast ductal carcinoma in situ ( DCIS ) and invasive ductal carcinoma ( IDC ) . Tests on simulated data show improved accuracy of the ploidy-based approach relative to prior ploidyless methods . Tests on real data further demonstrate novel insights these methods offer into tumor progression processes . Trees for DCIS samples are significantly less complex than trees for paired IDC samples . Consensus graphs show substantial divergence among most paired samples from both sets . Low consensus between DCIS and IDC trees may help explain the difficulty in finding biomarkers that predict which DCIS cases are at most risk to progress to IDC . The FISHtrees software is available at ftp : //ftp.ncbi.nih.gov/pub/FISHtrees .
2K_dev_1052	In learning latent variable models ( LVMs ) , it is important to effectively capture infrequent patterns and shrink model size without sacrificing modeling power . Various studies have been done to `` diversify '' a LVM , which aim to learn a diverse set of latent components in LVMs . Most existing studies fall into a frequentist-style regularization framework , where the components are learned via point estimation . In this paper , we investigate how to `` diversify '' LVMs in the paradigm of Bayesian learning , which has advantages complementary to point estimation , such as alleviating overfitting via model averaging and quantifying uncertainty . We propose two approaches that have complementary advantages . One is to define diversity-promoting mutual angular priors which assign larger density to components with larger mutual angles based on Bayesian network and von Mises-Fisher distribution and use these priors to affect the posterior via Bayes rule . We develop two efficient approximate posterior inference algorithms based on variational inference and Markov chain Monte Carlo sampling . The other approach is to impose diversity-promoting regularization directly over the post-data distribution of components . These two methods are applied to the Bayesian mixture of experts model to encourage the `` experts '' to be diverse and experimental results demonstrate the effectiveness and efficiency of our methods .
2K_dev_1053	We study the problem of automatically building hypernym taxonomies from textual and visual data . Previous works in taxonomy induction generally ignore the increasingly prominent visual data , which encode important perceptual semantics . Instead , we propose a probabilistic model for taxonomy induction by jointly leveraging text and images . To avoid hand-crafted feature engineering , we design end-to-end features based on distributed representations of images and words . The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images . We evaluate our model and features on the WordNet hierarchies , where our system outperforms previous approaches by a large gap .
2K_dev_1054	A system for classifying touch events includes a touch screen configured to display an interactive element , one or more vibro-acoustic sensors coupled to the touch screen , a touch event detector configured to monitor the one or more vibro-acoustic sensors and to save vibro-acoustic signals sensed by the one or more vibro acoustic sensors , wherein the touch event detector is further configured to detect touch events in which the interactive element is touched by a first or a second finger part of a user , and wherein the touch events result in generating the vibro-acoustic signals , and a vibro-acoustic classifier configured to classify the vibro-acoustic signals .
2K_dev_1055	Besides appearance information , the video contains temporal evolution , which represents an important and useful source of information about its content . Many video representation approaches are based on the motion information within the video . The common approach to extract the motion information is to compute the optical flow from the vertical and the horizontal temporal evolution of two consecutive frames . However , the computation of optical flow is very demanding in terms of computational cost , in many cases being the most significant processing step within the overall pipeline of the target video analysis application . In this work we propose a very efficient approach to capture the motion information within the video . Our method is based on a simple temporal and spatial derivation , which captures the changes between two consecutive frames . The proposed descriptor , Histograms of Motion Gradients ( HMG ) , is validated on the UCF50 human action recognition dataset . Our HMG pipeline with several additional speed-ups is able to achieve real-time video processing and outperforms several well-known descriptors including descriptors based on the costly optical flow .
2K_dev_1056	The applications of laser scanning technology are rapidly expanding in the civil engineering domain . LiDAR technology is now commonly used in the surveying and monitoring of large infrastructures . In particular , tunnels have become key transport infrastructures , subjected to maintenance processes that allow quality checks for tunnel modifications or tunnel clearance and profile checks . The research described in this paper targets developing an approach to semi-automatically retrieve the tunnel vertical clearance based on ground based mobile LiDAR data . The steps of this approach include extraction of cross sections orthogonal to the vehicle trajectory and road markings based on radiometric information , and conversion of cross section to a two-dimensional profile to estimate the vertical clearance . The validation of the developed approach is done using real-life case study , a road tunnel in southern Galicia , Spain . An accuracy of 100 % in detection of cross sections is achieved . Only one of the cross sections shows a relative error in vertical clearance measurement higher than 1 % . The results demonstrated the effectiveness of the developed approach for computing vertical clearances and demonstrating that tunnel management activities can definitely benefit from using mobile LiDAR by minimizing survey time and increasing productivity in dangerous environments .
2K_dev_1057	Complex event detection on unconstrained Internet videos has seen much progress in recent years . However , state-of-the-art performance degrades dramatically when the number of positive training exemplars falls short . Since label acquisition is costly , laborious , and time-consuming , there is a real need to consider the much more challenging semantic event search problem , where no example video is given . In this paper , we present a state-of-the-art event search system without any example videos . Relying on the key observation that events ( e.g . dog show ) are usually compositions of multiple mid-level concepts ( e.g . `` dog , '' `` theater , '' and `` dog jumping '' ) , we first train a skip-gram model to measure the relevance of each concept with the event of interest . The relevant concept classifiers then cast votes on the test videos but their reliability , due to lack of labeled training videos , has been largely unaddressed . We propose to combine the concept classifiers based on a principled estimate of their accuracy on the unlabeled test videos . A novel warping technique is proposed to improve the performance and an efficient highly-scalable algorithm is provided to quickly solve the resulting optimization . We conduct extensive experiments on the latest TRECVID MEDTest 2014 , MEDTest 2013 and CCV datasets , and achieve state-of-the-art performances .
2K_dev_1058	Kidney exchange , where candidates with organ failure trade incompatible but willing donors , is a lifesaving alternative to the deceased donor waitlist , which has inadequate supply to meet demand . While fielded kidney exchanges see huge benefit from altruistic kidney donors ( who give an organ without a paired needy candidate ) , a significantly higher medical risk to the donor deters similar altruism with livers . In this paper , we begin by proposing the idea of liver exchange , and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level . We then explore crossorgan donation where kidneys and livers can be bartered for each other . We show theoretically that this multiorgan exchange provides linearly more transplants than running separate kidney and liver exchanges ; this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool . We support this result experimentally on demographically accurate multi-organ exchanges . We conclude with thoughts regarding the fielding of a nationwide liver or joint liverkidney exchange from a legal and computational point of view .
2K_dev_1059	Given a directed graph of millions of nodes , how can we automatically spot anomalous , suspicious nodes judging only from their connectivity patterns ? Suspicious graph patterns show up in many applications , from Twitter users who buy fake followers , manipulating the social network , to botnet members performing distributed denial of service attacks , disturbing the network traffic graph . We propose a fast and effective method , C atch S ync , which exploits two of the tell-tale signs left in graphs by fraudsters : ( a ) synchronized behavior : suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together ( such as follow the same user ) ; and ( b ) rare behavior : their connectivity patterns are very different from the majority . We introduce novel measures to quantify both concepts ( synchronicity and normality ) and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots . Thanks to careful design , C atch S ync has the following desirable properties : ( a ) it is scalable to large datasets , being linear in the graph size ; ( b ) it is parameter free ; and ( c ) it is side-information-oblivious : it can operate using only the topology , without needing labeled data , nor timing information , and the like. , while still capable of using side information if available . We applied C atch S ync on three large , real datasets , 1-billion-edge Twitter social graph , 3-billion-edge , and 12-billion-edge Tencent Weibo social graphs , and several synthetic ones ; C atch S ync consistently outperforms existing competitors , both in detection accuracy by 36p on Twitter and 20p on Tencent Weibo , as well as in speed .
2K_dev_1060	Action recognition ( AR ) is one of the most important tasks in video analysis and computer vision . Recently a large number of related methods have been proposed . While most of these methods are investigated on AR datasets collected from the visible spectrum , the AR problem under infrared scenarios still has not attracted much attention . There is even few public infrared datasets available for supporting the fundamental evaluation requirements of this research . To this issue , this work aims to emphasize the importance of the infrared AR problem in applications and arouse researchers ' attention on this task . Specifically , we construct a new Infrared Action Recognition ( InfAR ) dataset captured at different times , including in summer and winter , and explore how discriminable actions in our InfAR dataset are with the state-of-the-art pipelines based on low-level features and deep convolutional neural network ( CNN ) , respectively . Our results reveal : ( 1 ) In all , dense trajectory feature can achieve the best performance while the appearance features , e.g. , HOG , have relatively poorer performance ; ( 2 ) the encoding method of vector of locally aggregated descriptors is evidently better than that of the widely-used Fisher Vector ; ( 3 ) the late fusion facilitates a better performance than early fusion ; ( 4 ) action videos captured in winter is more discriminable than in summer ; ( 5 ) compared to appearance information , the motion information is more essential for infrared action recognition and utilizing this information through deep CNN can improve greatly the performance . The best performance achieved on our dataset is 76.66 % ( Average Precision ) , leaving a reasonable space for further exploring the insights underlying such type of infrared AR problem and accordingly designing proper techniques to further promote the performance on this specifically constructed InfAR dataset .
2K_dev_1061	Suppose you are a teacher , and have to convey a set of object-property pairs 'lions eat meat ' . A good teacher will convey a lot of information , with little effort on the student side . What is the best and most intuitive way to convey this information to the student , without the student being overwhelmed ? A related , harder problem is : how can we assign a numerical score to each lesson plan i.e. , way of conveying information ? Here , we give a formal definition of this problem of forming learning units and we provide a metric for comparing different approaches based on information theory . We also design an algorithm , groupNteach , for this problem . Our proposed groupNteach is scalable near-linear in the dataset size ; it is effective , achieving excellent results on real data , both with respect to our proposed metric , but also with respect to encoding length ; and it is intuitive , conforming to well-known educational principles . Experiments on real and synthetic datasets demonstrate the effectiveness of groupNteach .
2K_dev_1062	It is common that users are interested in finding video segments , which contain further information about the video contents in a segment of interest . To facilitate users to find and browse related video contents , video hyperlinking aims at constructing links among video segments with relevant information in a large video collection . In this study , we explore the effectiveness of various video features on the performance of video hyperlinking , including subtitle , metadata , content features ( i.e. , audio and visual ) , surrounding context , as well as the combinations of those features . Besides , we also test different search strategies over different types of queries , which are categorized according to their video contents . Comprehensive experimental studies have been conducted on the dataset of TRECVID 2015 video hyperlinking task . Results show that ( 1 ) text features play a crucial role in search performance , and the combination of audio and visual features can not provide improvements ; ( 2 ) the consideration of contexts can not obtain better results ; and ( 3 ) due to the lack of training examples , machine learning techniques can not improve the performance .
2K_dev_1063	Clustering is the task of grouping a set of objects so that objects in the same cluster are more similar to each other than to those in other clusters . The crucial step in most clustering algorithms is to find an appropriate similarity metric , which is both challenging and problem-dependent . Supervised clustering approaches , which can exploit labeled clustered training data that share a common metric with the test set , have thus been proposed . Unfortunately , current metric learning approaches for supervised clustering do not scale to large or even medium-sized datasets . In this paper , we propose a new structured Mahalanobis Distance Metric Learning method for supervised clustering . We formulate our problem as an instance of large margin structured prediction and prove that it can be solved very efficiently in closed-form . The complexity of our method is ( in most cases ) linear in the size of the training dataset . We further reveal a striking similarity between our approach and multivariate linear regression . Experiments on both synthetic and real datasets confirm several orders of magnitude speedup while still achieving state-of-the-art performance .
2K_dev_1064	This book presents how multimedia data analysis , information retrieval and indexing are central for comprehensive , personalized , adaptive quality care and the prolongation of independent living at home . With sophisticated technologies in monitoring , diagnosis , and treatment , multimodal data plays an increasingly central role in healthcare . Experts in computer vision , image processing , medical imaging , biomedical engineering , medical informatics , physical education and motor control , visual learning , nursing and human sciences , information retrieval , content based image retrieval , eHealth , information fusion , multimedia communications and human computer interaction come together to provide athorough overview of multimedia analysis in medicine and daily life .
2K_dev_1065	A camera and method identify moving objects of interest in a field of view of the camera . The method includes : capturing two or more images successively over a time period , each image being associated with different times during the time period ; obtaining binary image from each successive pair of images , the binary image comprising a binary value at each pixel indicating whether a change in pixel values of at least a predetermined magnitude has occurred at that pixel between the time associated with the First image of the success pair of images and time associated with the second image of the successive pair of images ; deriving one or more motion boxes each encapsulating one or more nearby pixels in binary image ; processing motion boxes of each binary image to obtain refined motion boxes ; and classifying refined motion boxes each into categories representative of one moving object of interest .
2K_dev_1066	Matrix sketching is aimed at finding close approximations of a matrix by factors of much smaller dimensions , which has important applications in optimization and machine learning . Given a matrix A of size m by n , state-of-the-art randomized algorithms take O ( m * n ) time and space to obtain its low-rank decomposition . Although quite useful , the need to store or manipulate the entire matrix makes it a computational bottleneck for truly large and dense inputs . Can we sketch an m-by-n matrix in O ( m + n ) cost by accessing only a small fraction of its rows and columns , without knowing anything about the remaining data ? In this paper , we propose the cascaded bilateral sampling ( CABS ) framework to solve this problem . We start from demonstrating how the approximation quality of bilateral matrix sketching depends on the encoding powers of sampling . In particular , the sampled rows and columns should correspond to the code-vectors in the ground truth decompositions . Motivated by this analysis , we propose to first generate a pilot-sketch using simple random sampling , and then pursue more advanced , `` follow-up '' sampling on the pilot-sketch factors seeking maximal encoding powers . In this cascading process , the rise of approximation quality is shown to be lower-bounded by the improvement of encoding powers in the follow-up sampling step , thus theoretically guarantees the algorithmic boosting property . Computationally , our framework only takes linear time and space , and at the same time its performance rivals the quality of state-of-the-art algorithms consuming a quadratic amount of resources . Empirical evaluations on benchmark data fully demonstrate the potential of our methods in large scale matrix sketching and related areas .
2K_dev_1067	This paper studies attackers with control objectives against cyber-physical systems ( CPS ) . The system is equipped with its own controller and attack detector , and the goal of the attacker is to move the system to a target state while altering the system 's actuator input and sensor output to avoid detection . We formulate a cost function that reflects the attacker 's goals , and , using dynamic programming , we show that the optimal attack strategy reduces to a linear feedback of the attacker 's state estimate . By changing the parameters of the cost function , we show how an attacker can design optimal attacks to balance the control objective and the detection avoidance objective . Finally , we provide a numerical illustration based on a remotely-controlled helicopter under attack .
2K_dev_1068	At least 10 % of the global population has dyslexia . In the United States and Spain , dyslexia is associated with a large percentage of school drop out . Current methods to detect risk of dyslexia are language specific , expensive , or do not scale well because they require a professional or extensive equipment . A central challenge to detecting dyslexia is handling its differing manifestations across languages . To address this , we designed a browser-based game , Dytective , to detect risk of dyslexia across the English and Spanish languages . Dytective consists of linguistic tasks informed by analysis of common errors made by persons with dyslexia . To evaluate Dytective , we conducted a user study with 60 English and Spanish speaking children between 7 and 12 years old . We found children with and without dyslexia differed significantly in their performance on the game . Our results suggest that Dytective is able to differentiate school age children with and without dyslexia in both English and Spanish speakers .
2K_dev_1069	The rapidly growing field of computational social choice , at the intersection of computer science and economics , deals with the computational aspects of collective decision making . This handbook , written by thirty-six prominent members of the computational social choice community , covers the field comprehensively . Chapters devoted to each of the field 's major themes offer detailed introductions . Topics include voting theory ( such as the computational complexity of winner determination and manipulation in elections ) , fair allocation ( such as algorithms for dividing divisible and indivisible goods ) , coalition formation ( such as matching and hedonic games ) , and many more . Graduate students , researchers , and professionals in computer science , economics , mathematics , political science , and philosophy will benefit from this accessible and self-contained book .
2K_dev_1070	Acute hypotensive episodes ( AHEs ) are serious clinical events in intensive care units ( ICUs ) , and require immediate treatment to prevent patient injury . Reducing the risks associated with an AHE requires effective and efficient mining of data generated from multiple physiological time series . We propose HeartCast , a model that extracts essential features from such data to effectively predict AHE . HeartCast combines a non-linear support vector machine with best-feature extraction via analysis of the baseline threshold , quartile parameters , and window size of the physiological signals . Our approach has the following benefits : ( a ) it extracts the most relevant features ; ( b ) it provides the best results for identification of an AHE event ; ( c ) it is fast and scales with linear complexity over the length of the window ; and ( d ) it can manage missing values and noise/outliers by using a best-feature extraction method . We performed experiments on data continuously captured from physiological time series of ICU patients ( roughly 3 GB of processed data ) . HeartCast was found to outperform other state-of-the-art methods found in the literature with a 13.7 % improvement in classification accuracy .
2K_dev_1071	Summary An important experimental design question for high-throughput time series studies is the number of replicates required for accurate reconstruction of the profiles . Due to budget and sample availability constraints , more replicates imply fewer time points and vice versa . We analyze the performance of dense and replicate sampling by developing a theoretical framework that focuses on a restricted yet expressive set of possible curves over a wide range of noise levels and by analyzing real expression data . For both the theoretical analysis and experimental data , we observe that , under reasonable noise levels , autocorrelations in the time series data allow dense sampling to better determine the correct levels of non-sampled points when compared to replicate sampling . A Java implementation of our framework can be used to determine the best replicate strategy given the expected noise . These results provide theoretical support to the large number of high-throughput time series experiments that do not use replicates .
2K_dev_1072	In this paper , we present an advanced deep learning based approach to automatically determine whether a driver is using a cell-phone as well as detect if his/her hands are on the steering wheel ( i.e . counting the number of hands on the wheel ) . To robustly detect small objects such as hands , we propose Multiple Scale Faster-RCNN ( MSFRCNN ) approach that uses a standard Region Proposal Network ( RPN ) generation and incorporates feature maps from shallower convolution feature maps , i.e . conv3 and conv4 , for ROI pooling . In our driver distraction detection framework , we first make use of the proposed MS-FRCNN to detect individual objects , namely , a hand , a cell-phone , and a steering wheel . Then , the geometric information is extracted to determine if a cell-phone is being used or how many hands are on the wheel . The proposed approach is demonstrated and evaluated on the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge database and the challenging Strategic Highway Research Program ( SHRP-2 ) face view videos that was acquired to monitor drivers under naturalistic driving conditions . The experimental results show that our method archives better performance than Faster R-CNN on both hands on wheel detection and cell-phone usage detection while remaining at similar testing cost . Compare to the state-of-the-art cell-phone usage detection , our approach obtains higher accuracy , is less time consuming and is independent to landmarking . The groundtruth database will be publicly available .
2K_dev_1073	What is a fair way to assign rooms to several housemates , and divide the rent between them ? This is not just a theoretical question : many people have used the Spliddit website to obtain envy-free solutions to rent division instances . But envy freeness , in and of itself , is insufficient to guarantee outcomes that people view as intuitive and acceptable . We therefore focus on solutions that optimize a criterion of social justice , subject to the envy freeness constraint , in order to pinpoint the `` fairest '' solutions . We develop a general algorithmic framework that enables the computation of such solutions in polynomial time . We then study the relations between natural optimization objectives , and identify the maximin solution , which maximizes the minimum utility subject to envy freeness , as the most attractive . We demonstrate , in theory and using experiments on real data from Spliddit , that the maximin solution gives rise to significant gains in terms of our optimization objectives . Finally , a user study with Spliddit users as subjects demonstrates that people find the maximin solution to be significantly fairer than arbitrary envy-free solutions ; this user study is unprecedented in that it asks people about their real-world rent division instances . Based on these results , the maximin solution has been deployed on Spliddit since April 2015 .
2K_dev_1074	Weakly supervised methods have recently become one of the most popular machine learning methods since they are able to be used on large-scale datasets without the critical requirement of richly annotated data . In this paper , we present a novel , self-taught , discriminative facial feature analysis approach in the weakly supervised framework . Our method can find regions which are discriminative across classes yet consistent within a class and can solve many face related problems . The proposed method first trains a deep face model with high discriminative capability to extract facial features . The hypercolumn features are then used to give pixel level representation for better classification performance along with discriminative region detection . In addition , calibration approaches are proposed to enable the system to deal with multi-class and mixed-class problems . The system is also able to detect multiple discriminative regions from one image . Our uniform method is able to achieve competitive results in various face analysis applications , such as occlusion detection , face recognition , gender classification , twins verification and facial attractiveness analysis .
2K_dev_1075	We study a component-based approach to simplify the challenges of verifying large-scale hybrid systems . Component-based modeling can be used to split large models into partial models to reduce modeling complexity . Yet , verification results also need to transfer from components to composites . In this paper , we propose a component-based hybrid system verification approach that combines the advantages of component-based modeling e.g. , reduced model complexity with the advantages of formal verification e.g. , guaranteed contract compliance . Our strategy is to decompose the system into components , verify their local safety individually and compose them to form an overall system that provably satisfies a global contract , without proving the whole system . We introduce the necessary formalism to define the structure and behavior of components and a technique how to compose components such that safety properties provably emerge from component safety .
2K_dev_1076	Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community . A considerable amount of videos on the web are associated with rich but noisy contextual information , such as the title , which provides weak annotations or labels about the video content . To leverage the big noisy web labels , this paper proposes a novel method called WEbly-Labeled Learning ( WELL ) , which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human . WELL introduces a number of novel multi-modal approaches to incorporate meaningful prior knowledge called curriculum from the noisy web videos . To investigate this problem , we empirically study the curriculum constructed from the multi-modal features of the videos collected from YouTube and Flickr . The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks , including the largest multimedia dataset and the largest manually-labeled video set . The comprehensive experimental results demonstrate that WELL outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data . In addition , the results also verify that WELL is robust to the level of noisiness in the video data . Notably , WELL trained on sufficient noisy web labels is able to achieve a comparable accuracy to supervised learning methods trained on the clean manually-labeled data .
2K_dev_1077	Brain-computer interfaces ( BCIs ) have the potential to restore motor abilities to paralyzed individuals . These systems act by reading motor intent signals directly from the brain and using them to control , for example , the movement of a cursor on a computer screen or the motion of a robotic limb . To construct a BCI , a mapping must be specified that dictates how neural activity will actuate the device . How should these mappings be constructed to maximize user performance ? Most approaches have focused on this problem from an estimation standpoint , i.e. , mappings are designed to implement the best estimate of motor intent possible , under various sets of assumptions about how the recorded neural signals represent motor intent . Here we forward an alternate approach to the BCI design problem , using ideas from optimal control theory . We first argue that the brain can be considered as an optimal controller . We then introduce a mathematical definition of BCI usability , and formulate the BCI design problem as a constrained optimization problem that maximizes this usability .
2K_dev_1078	Teaching chess to students with learning disabilities has been shown to benefit their school performance in unrelated domains . At the same time , chess involves skills that are highly correlated with dyslexia , such as visuospatial and calculation abilities . In this paper , we created a online chess game designed for people with dyslexia and seek to understand whether people with dyslexia learn and play chess online in ways that differ from other students and whether such differences may be leveraged to improve classroom performance . To test how people with dyslexia learn to play chess we carried out a within-subject experiment with 62 participants , 31 of them with diagnosed dyslexia . Participants used an instrumented web-based chess learning platform that we developed to ( i ) complete lessons on how to play chess and about chess theory , ( ii ) work through exercises designed to test and reaffirm their skills , and ( iii ) play chess against a computer opponent . We could not find significant differences on four dependent measures out of the twelve measures we collected . Therefore , dyslexia might have an impact on how people learn and play chess using a computer , suggesting that chess may be useful as a fun way to help people with dyslexia improve their abilities .
2K_dev_1079	Cyber-physical systems CPS combine cyber aspects such as communication and computer control with physical aspects such as movement in space , which arise frequently in many safety-critical application domains , including aviation , automotive , railway , and robotics . But how can we ensure that these systems are guaranteed to meet their design goals , e.g. , that an aircraft will not crash into another one ? This paper highlights some of the most fascinating aspects of cyber-physical systems and their dynamical systems models , such as hybrid systems that combine discrete transitions and continuous evolution along differential equations . Because of the impact that they can have on the real world , CPSs deserve proof as safety evidence . Multi-dynamical systems understand complex systems as a combination of multiple elementary dynamical aspects , which makes them natural mathematical models for CPS , since they tame their complexity by compositionality . The family of differential dynamic logics achieves this compositionality by providing compositional logics , programming languages , and reasoning principles for CPS . Differential dynamic logics , as implemented in the theorem prover KeYmaerai ? X , have been instrumental in verifying many applications , including the Airborne Collision Avoidance System ACAS X , the European Train Control System ETCS , automotive systems , mobile robot navigation , and a surgical robot system for skull-base surgery . This combination of strong theoretical foundations with practical theorem proving challenges and relevant applications makes Logic for CPS an ideal area for compelling and rewarding research .
2K_dev_1080	This paper studies attackers with specific objectives against a cyber physical system . The attacker performs an integrity attack in order to move the system to a target state while evading detection over a finite time window . We formulate and solve an optimal control problem that captures the attacker 's objective - the solution gives the optimal sequence of attacks . We provide a sufficient condition for the existence of an optimal attack sequence . Finally , we demonstrate our proposed attack strategy in a numerical example .
2K_dev_1081	We study the problem of variable selection in convex nonparametric regression . Under the assumption that the true regression function is convex and sparse , we develop a screening procedure to select a subset of variables that contains the relevant variables . Our approach is a two-stage quadratic programming method that estimates a sum of one-dimensional convex functions , followed by one-dimensional concave regression fits on the residuals . In contrast to previous methods for sparse additive models , the optimization is finite dimensional and requires no tuning parameters for smoothness . Under appropriate assumptions , we prove that the procedure is faithful in the population setting , yielding no false negatives . We give a finite sample statistical analysis , and introduce algorithms for efficiently carrying out the required quadratic programs . The approach leads to computational and statistical advantages over fitting a full model , and provides an effective , practical approach to variable screening in convex regression .
2K_dev_1082	Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in imperfect-information games . However , imperfect-recall abstractions are poorly understood , and only weak algorithm-specific guarantees on solution quality are known . We develop the first general , algorithm-agnostic , solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions , when implemented in the original ( perfect-recall ) game . Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions for which any such results have been obtained . Further , our analysis is tighter in two ways , each of which can lead to an exponential reduction in the solution quality error bound . We then show that for extensive-form games that satisfy certain properties , the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem , where the increase in our bound is the distance function . This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds . We proceed to show a divide in the class of abstraction problems . If payoffs are at the same scale at all information sets considered for abstraction , the input forms a metric space , and this immediately yields a $ 2 $ -approximation algorithm for abstraction . Conversely , if this condition is not satisfied , we show that the input does not form a metric space . Finally , we provide computational experiments to evaluate the practical usefulness of the abstraction techniques . They show that running counterfactual regret minimization on such abstractions leads to good strategies in the original games .
2K_dev_1083	With the availability of high resolution digital technology , there has been increased interest in developing statistical and image processing techniques that can enhance the existing capabilities of analyzing works of art for authenticity . This work explores the merits of using advanced correlation filters in supplementing art experts efforts in identifying forgeries among disputed paintings . We show that by training the optimal trade-off synthetic discriminant function ( OTSDF ) filter on each section of a coarsely parceled image of an original painting , we are not only able to distinguish between a low-quality digitized representation of a painting and its forgery , but also specifically indicate where the differences occur and where the replica is particularly faithful to the original . This method is also valuable in determining whether an original painting has undergone any modifications , given that a representation of the initial version is available .
2K_dev_1084	Abstract SummaryWith the rapid advances in technologies of microarray and massively parallel sequencing , data of multiple omics sources from a large patient cohort are now frequently seen in many consortium studies . Effective multi-level omics data integration has brought new statistical challenges . One important biological objective of such integrative analysis is to cluster patients in order to identify clinically relevant disease subtypes , which will form basis for tailored treatment and personalized medicine . Several methods have been proposed in the literature for this purpose , including the popular iCluster method used in many cancer applications . When clustering high-dimensional omics data , effective feature selection is critical for better clustering accuracy and biological interpretation . It is also common that a portion of `` scattered samples '' has patterns distinct from all major clusters and should not be assigned into any cluster as they may represent a rare disease subcategory or be in transition between disease subtypes . In this paper , we firstly propose to improve feature selection of the iCluster factor model by an overlapping sparse group lasso penalty on the omics features using prior knowledge of inter-omics regulatory flows . We then perform regularization over samples to allow clustering with scattered samples and generate tight clusters . The proposed group structured tight iCluster method will be evaluated by two real breast cancer examples and simulations to demonstrate its improved clustering accuracy , biological interpretation , and ability to generate coherent tight clusters .
2K_dev_1085	Computational offloading services at the edge of the Internet for mobile devices are becoming a reality . Using a wide range of mobile applications , we explore how such infrastructure improves latency and energy consumption relative to the cloud . We present experimental results from WiFi and 4G LTE networks that confirm substantial wins from edge computing for highly interactive mobile applications .
2K_dev_1086	Demand response is seeing increased popularity worldwide and industrial loads are actively taking part in this trend . As a host of energy-intensive industrial processes , steel plants have both the motivation and potential to provide demand response . However , the scheduling of steel plants is very complex and the involved computations are intense . In this paper , we focus on these difficulties and propose methods such as adding cuts and implementing an application-specific branch and bound algorithm to make the computations more tractable .
2K_dev_1087	Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society , ranging from online personalization to insurance and credit decisions to predictive policing . But their decision-making processes are often opaque -- it is difficult to explain why a certain decision was made . We develop a formal foundation to improve the transparency of such decision-making systems . Specifically , we introduce a family of Quantitative Input Influence ( QII ) measures that capture the degree of influence of inputs on outputs of systems . These measures provide a foundation for the design of transparency reports that accompany system decisions ( e.g. , explaining a specific credit decision ) and for testing tools useful for internal and external oversight ( e.g. , to detect algorithmic discrimination ) . Distinctively , our causal QII measures carefully account for correlated inputs while measuring influence . They support a general class of transparency queries and can , in particular , explain decisions about individuals ( e.g. , a loan decision ) and groups ( e.g. , disparate impact based on gender ) . Finally , since single inputs may not always have high influence , the QII measures also quantify the joint influence of a set of inputs ( e.g. , age and income ) on outcomes ( e.g . loan decisions ) and the marginal influence of individual inputs within such a set ( e.g. , income ) . Since a single input may be part of multiple influential sets , the average marginal influence of the input is computed using principled aggregation measures , such as the Shapley value , previously applied to measure influence in voting . Further , since transparency reports could compromise privacy , we explore the transparency-privacy tradeoff and prove that a number of useful transparency reports can be made differentially private with very little addition of noise . Our empirical validation with standard machine learning algorithms demonstrates that QII measures are a useful transparency mechanism when black box access to the learning system is available . In particular , they provide better explanations than standard associative measures for a host of scenarios that we consider . Further , we show that in the situations we consider , QII is efficiently approximable and can be made differentially private while preserving accuracy .
2K_dev_1088	Cities are increasingly equipped with low-resolution cameras . Video from some of these cameras is publicly accessible in real time . In this project , the authors addressed the problem of building a traffic model for parts of the roads visible from publicly accessible cameras . In particular , the end goal is to build a model capable of detecting different types of vehicles in images in various weather conditions and times of the day except night . Models learn different appearance of vehicles as seen from different viewpoints . A major difficulty with any type of analysis like this is the need for large amounts of training data . In our case , it is easy to collect unlabeled data from publicly available low-resolution low-framerate cameras in Pittsburgh or NYC .
2K_dev_1089	Multimedia event detection has been receiving increasing attention in recent years . Besides recognizing an event , the discovery of evidences ( which is refered to as `` recounting '' ) is also crucial for user to better understand the searching result . Due to the difficulty of evidence annotation , only limited supervision of event labels are available for training a recounting model . To deal with the problem , we propose a weakly supervised evidence discovery method based on self-paced learning framework , which follows a learning process from easy `` evidences '' to gradually more complex ones , and simultaneously exploit more and more positive evidence samples from numerous weakly annotated video segments . Moreover , to evaluate our method quantitatively , we also propose two metrics , \textit { PctOverlap } and \textit { F1-score } , for measuring the performance of evidence localization specifically . The experiments are conducted on a subset of TRECVID MED dataset and demonstrate the promising results obtained by our method .
2K_dev_1090	Given a large-scale and high-order tensor , how can we find dense blocks in it ? Can we find them in near-linear time but with a quality guarantee ? Extensive previous work has shown that dense blocks in tensors as well as graphs indicate anomalous or fraudulent behavior e.g. , lockstep behavior in social networks . However , available methods for detecting such dense blocks are not satisfactory in terms of speed , accuracy , or flexibility . In this work , we propose M-Zoom , a flexible framework for finding dense blocks in tensors , which works with a broad class of density measures . M-Zoom has the following properties : 1 Scalable : M-Zoom scales linearly with all aspects of tensors and is upito 114 $ $ \times $ $ faster than state-of-the-art methods with similar accuracy . 2 Provably accurate : M-Zoom provides a guarantee on the lowest density of the blocks it finds . 3 Flexible : M-Zoom supports multi-block detection and size bounds as well as diverse density measures . 4 Effective : M-Zoom successfully detected edit wars and bot activities in Wikipedia , and spotted network attacks from a TCP dump with near-perfect accuracy AUCi=i0.98 . The data and software related to this paper are available at http : //www.cs.cmu.edu/~kijungs/codes/mzoom/ .
2K_dev_1091	We propose local binary convolution ( LBC ) , an efficient alternative to convolutional layers in standard convolutional neural networks ( CNN ) . The design principles of LBC are motivated by local binary patterns ( LBP ) . The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process , a non-linear activation function and a set of learnable linear weights . The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer . The LBC layer affords significant parameter savings , 9x to 169x in the number of learnable parameters compared to a standard convolutional layer . Furthermore , due to lower model complexity and sparse and binary nature of the weights also results in up to 9x to 169x savings in model size compared to a standard convolutional layer . We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer . Empirically , CNNs with LBC layers , called local binary convolutional neural networks ( LBCNN ) , reach state-of-the-art performance on a range of visual datasets ( MNIST , SVHN , CIFAR-10 , and a subset of ImageNet ) while enjoying significant computational savings .
2K_dev_1092	The NSF workshop on Security and Formal Methods , held 19 -- 20 November 2015 , brought together developers of formal methods , researchers exploring how to apply formal methods to various kinds of systems , and people familiar with the security problem space .
2K_dev_1093	Machine comprehension tests the systems ability to understand a piece of text through a reading comprehension task . For this task , we propose an approach using the Abstract Meaning Representation ( AMR ) formalism . We construct meaning representation graphs for the given text and for each question-answer pair by merging the AMRs of comprising sentences using cross-sentential phenomena such as coreference and rhetorical structures . Then , we reduce machine comprehension to a graph containment problem . We posit that there is a latent mapping of the question-answer meaning representation graph onto the text meaning representation graph that explains the answer . We present a unified max-margin framework that learns to find this mapping ( given a corpus of texts and question-answer pairs ) , and uses what it learns to answer questions on novel texts . We show that this approach leads to state of the art results on the task .
2K_dev_1094	Rapid advances in biology demand new tools for more active research dissemination and engaged teaching . This paper presents Synteny Explorer , an interactive visualization application designed to let college students explore genome evolution of mammalian species . The tool visualizes synteny blocks : segments of homologous DNA shared between various extant species that can be traced back or reconstructed in extinct , ancestral species . We take a karyogram-based approach to create an interactive synteny visualization , leading to a more appealing and engaging design for undergraduate-level genome evolution education . For validation , we conduct three user studies : two focused studies on color and animation design choices and a larger study that performs overall system usability testing while comparing our karyogram-based designs with two more common genome mapping representations in an educational context . While existing views communicate the same information , study participants found the interactive , karyogram-based views much easier and likable to use . We additionally discuss feedback from biology and genomics faculty , who judge Synteny Explorer 's fitness for use in classrooms .
2K_dev_1095	Online learning is used in a wide range of real applications , e.g. , predicting ad click-through rates ( CTR ) and personalized recommendations . Based on the analysis of users ' behaviors in Video-On-Demand ( VoD ) recommender systems , we discover that the most recent users ' actions can better reflect users ' current intentions and preferences . Under this observation , we thereby propose a novel time-decaying online learning algorithm derived from the state-of-the-art FTRL-proximal algorithm , called Time-Decaying Adaptive Prediction ( TDAP ) algorithm . To scale Big Data , we further parallelize our algorithm following the data parallel scheme under both BSP and SSP consistency model . We experimentally evaluate our TDAP algorithm on real IPTV VoD datasets using two state-of-the-art distributed computing platforms , TDAP achieves good accuracy : it improves at least 5.6 % in terms of prediction accuracy , compared to FTRL-proximal algorithm ; and TDAP scales well : it runs 4 times faster when the number of machines increases from 2 to 10 .
2K_dev_1096	An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer . We propose several candidate authentication protocols for a setting in which the human user can only receive assistance from a semi-trusted computer -- - a computer that stores information and performs computations correctly but does not provide confidentiality . Our schemes use a semi-trusted computer to store and display public challenges $ C_i\in [ n ] ^k $ . The human user memorizes a random secret mapping $ \sigma : [ n ] \rightarrow\mathbb { Z } _d $ and authenticates by computing responses $ f ( \sigma ( C_i ) ) $ to a sequence of public challenges where $ f : \mathbb { Z } _d^k\rightarrow\mathbb { Z } _d $ is a function that is easy for the human to evaluate . We prove that any statistical adversary needs to sample $ m=\tilde { \Omega } ( n^ { s ( f ) } ) $ challenge-response pairs to recover $ \sigma $ , for a security parameter $ s ( f ) $ that depends on two key properties of $ f $ . To obtain our results , we apply the general hypercontractivity theorem to lower bound the statistical dimension of the distribution over challenge-response pairs induced by $ f $ and $ \sigma $ . Our lower bounds apply to arbitrary functions $ f $ ( not just to functions that are easy for a human to evaluate ) , and generalize recent results of Feldman et al . As an application , we propose a family of human computable password functions $ f_ { k_1 , k_2 } $ in which the user needs to perform $ 2k_1+2k_2+1 $ primitive operations ( e.g. , adding two digits or remembering $ \sigma ( i ) $ ) , and we show that $ s ( f ) 0 \min\ { k_1+1 , ( k_2+1 ) /2\ } $ . For these schemes , we prove that forging passwords is equivalent to recovering the secret mapping . Thus , our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts .
2K_dev_1097	Vast quantities of videos are now being captured at astonishing rates , but the majority of these are not labelled . To cope with such data , we consider the task of content-based activity recognition in videos without any manually labelled examples , also known as zero-shot video recognition . To achieve this , videos are represented in terms of detected visual concepts , which are then scored as relevant or irrelevant according to their similarity with a given textual query . In this paper , we propose a more robust approach for scoring concepts in order to alleviate many of the brittleness and low precision problems of previous work . Not only do we jointly consider semantic relatedness , visual reliability , and discriminative power . To handle noise and non-linearities in the ranking scores of the selected concepts , we propose a novel pairwise order matrix approach for score aggregation . Extensive experiments on the large-scale TRECVID Multimedia Event Detection data show the superiority of our approach .
2K_dev_1098	Counterfactual Regret Minimization ( CFR ) is the most popular iterative algorithm for solving zero-sum imperfect-information games . Regret-Based Pruning ( RBP ) is an improvement that allows poorly-performing actions to be temporarily pruned , thus speeding up CFR . We introduce Total RBP , a new form of RBP that reduces the space requirements of CFR as actions are pruned . We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium . This leads to provably faster convergence and lower space requirements . Experiments show that Total RBP results in an order of magnitude reduction in space , and the reduction factor increases with game size .
2K_dev_1099	Many applications collect a large number of time series , for example , the financial data of companies quoted in a stock exchange , the health care data of all patients that visit the emergency room of a hospital , or the temperature sequences continuously measured by weather stations across the US . These data are often referred to as un structured . The first task in its analytics is to derive a low dimensional representation , a graph or discrete manifold , that describes well the inter relations among the time series and their intra relations across time . This paper presents a computationally tractable algorithm for estimating this graph that structures the data . The resulting graph is directed and weighted , possibly capturing causal relations , not just reciprocal correlations as in many existing approaches in the literature . A convergence analysis is carried out . The algorithm is demonstrated on random graph datasets and real network time series datasets , and its performance is compared to that of related methods . The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested .
2K_dev_1100	A method for ensuring that verification results about models apply to cyber-physical systems ( CPS ) implementations is presented . The invention provides correctness guarantees for CPS executions at runtime . Offline verification of CPS models are combined with runtime validation of system executions for compliance with the model . The invention ensures that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model , assuming the system dynamics deviation is bounded . If , at some point , the observed behavior no longer complies with the model , such that offline verification results no longer apply , provably safe fallback actions are initiated . The invention includes a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic .
2K_dev_1101	Pooling plays an important role in generating a discriminative video representation . In this paper , we propose a new semantic pooling approach for challenging event analysis tasks ( e.g. , event detection , recognition , and recounting ) in long untrimmed Internet videos , especially when only a few shots/segments are relevant to the event of interest while many other shots are irrelevant or even misleading . The commonly adopted pooling strategies aggregate the shots indifferently in one way or another , resulting in a great loss of information . Instead , in this work we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest . We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event analysis . Next , we propose a new isotonic regularizer that is able to exploit the constructed semantic ordering information . The resulting nearly-isotonic support vector machine classifier exhibits higher discriminative power in event analysis tasks . Computationally , we develop an efficient implementation using the proximal gradient algorithm , and we prove new and closed-form proximal steps . We conduct extensive experiments on three real-world video datasets and achieve promising improvements .
2K_dev_1102	A system for classifying touch events of different interaction layers includes a touch screen configured to display an interactive element , one or more vibro-acoustic sensors coupled to the touch screen , a touch event detector configured to monitor the one or more vibro-acoustic sensors and to save vibro-acoustic signals sensed by the one or more vibro acoustic sensors , wherein the touch event detector is further configured to detect touch events in which the interactive element is touched by a first or a second finger part of a user , and wherein the touch events result in generating the vibro-acoustic signals , and a vibro-acoustic classifier is configured to classify the vibro-acoustic signals and activate corresponding functions in the different layers dependent upon which finger part is used .
2K_dev_1103	Recently , there has been a surge of interest in using spectral methods for estimating latent variable models . However , it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family . In this paper , we study the estimation of an $ m $ -state hidden Markov model ( HMM ) with only smoothness assumptions , such as H\ '' olderian conditions , on the emission densities . By leveraging some recent advances in continuous linear algebra and numerical analysis , we develop a computationally efficient spectral algorithm for learning nonparametric HMMs . Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as \emph { continuous matrices } . We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices . We implement our method using Chebyshev polynomial approximations . Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient .
2K_dev_1104	Meeting tail latency Service Level Objectives ( SLOs ) in shared cloud networks is both important and challenging . One primary challenge is determining limits on the multi-tenancy such that SLOs are met . Doing so involves estimating latency , which is difficult , especially when tenants exhibit bursty behavior as is common in production environments . Nevertheless , recent papers in the past two years ( Silo , QJump , and PriorityMeister ) show techniques for calculating latency based on a branch of mathematical modeling called Deterministic Network Calculus ( DNC ) . The DNC theory is designed for adversarial worst-case conditions , which is sometimes necessary , but is often overly conservative . Typical tenants do not require strict worst-case guarantees , but are only looking for SLOs at lower percentiles ( e.g. , 99th , 99.9th ) . This paper describes SNC-Meister , a new admission control system for tail latency SLOs . SNC-Meister improves upon the state-of-the-art DNC-based systems by using a new theory , Stochastic Network Calculus ( SNC ) , which is designed for tail latency percentiles . Focusing on tail latency percentiles , rather than the adversarial worst-case DNC latency , allows SNC-Meister to pack together many more tenants : in experiments with production traces , SNC-Meister supports 75 % more tenants than the state-of-the-art .
2K_dev_1105	This paper presents a robust , fully automatic and semi self-training system to detect and segment facial beard/moustache simultaneously in challenging facial images . Based on the observation that some certain facial areas , e.g . cheeks , do not typically contain any facial hair whereas the others , e.g . brows , often contain facial hair , a self-trained model is first built using a testing image itself . To overcome the limitation of that facial hairs in brows regions and beard/moustache regions are different in length , density , color , etc. , a pre-trained model is also constructed using training data . The pre-trained model is only pursued when the self-trained model produces low confident classification results . In the proposed system , we employ the superpixel together a combination of two classifiers , i.e . Random Ferns ( rFerns ) and Support Vector Machines ( SVM ) to obtain good classification performance as well as improve time efficiency . A feature vector , consisting of Histogram of Gabor ( HoG ) and Histogram of Oriented Gradient of Gabor ( HOGG ) at different directions and frequencies , is generated from both the bounding box of the superpixel and the super pixel foreground . The segmentation result is then refined by our proposed aggregately searching strategy in order to deal with inaccurate landmarking points . Experimental results have demonstrated the robustness and effectiveness of the proposed system . It is evaluated in images drawn from three entire databases i.e . the Multiple Biometric Grand Challenge ( MBGC ) still face database , the NIST color Facial Recognition Technology FERET database and a large subset from Pinellas County database . Detect and segment beard/moustache simultaneouslyUse advantages of both pre-trained model and self-trained modelWork on superpixelPropose an aggregate searching strategy to overcome the limits of landmarkerPropose a new feature that is able to emphasize high frequency information of facial hair
2K_dev_1106	We present a complete algorithm for finding an epsilon-Nash equilibrium , for arbitrarily small epsilon , in games with more than two players . The best prior complete algorithm has significantly worse complexity and has , to our knowledge , never been implemented . The main components of our tree-search-based method are a node-selection strategy , an exclusion oracle , and a subdivision scheme . The node-selection strategy determines the next region to be explored -- -based on the region 's size and an estimate of whether the region contains an equilibrium . The exclusion oracle provides a provably correct sufficient condition for there not to exist an equilibrium in the region . The subdivision scheme determines how the region is split if it can not be excluded . Unlike well-known incomplete methods , our method does not need to proceed locally , which avoids it getting stuck in a local minimum that may be far from any actual equilibrium . The run time grows rapidly with the game size , and this suggests a hybrid scheme where one of the relatively fast prior incomplete algorithms is run , and if it fails to find an equilibrium , then our method is used .
2K_dev_1107	Multimodal sentiment analysis is drawing an increasing amount of attention these days . It enables mining of opinions in video reviews and surveys which are now available aplenty on online platforms like YouTube . However , the limited number of high-quality multimodal sentiment data samples may introduce the problem of the sentiment being dependent on the individual specific features in the dataset . This results in a lack of generalizability of the trained models for classification on larger online platforms . In this paper , we first examine the data and verify the existence of this dependence problem . Then we propose a Select-Additive Learning ( SAL ) procedure that improves the generalizability of trained discriminative neural networks . SAL is a two-phase learning method . In Selection phase , it selects the confounding learned representation . In Addition phase , it forces the classifier to discard confounded representations by adding Gaussian noise . In our experiments , we show how SAL improves the generalizability of state-of-the-art models . We increase prediction accuracy significantly in all three modalities ( text , audio , video ) , as well as in their fusion . We show how SAL , even when trained on one dataset , achieves good accuracy across test datasets .
2K_dev_1108	We revisit the problem of designing optimal , individually rational matching mechanisms ( in a general sense , allowing for cycles in directed graphs ) , where each player -- -who is associated with a subset of vertices -- -matches as many of his own vertices when he opts into the matching mechanism as when he opts out . We offer a new perspective on this problem by considering an arbitrary graph , but assuming that vertices are associated with players at random . Our main result asserts that , under certain conditions , any fixed optimal matching is likely to be individually rational up to lower-order terms . We also show that a simple and practical mechanism is ( fully ) individually rational , and likely to be optimal up to lower-order terms . We discuss the implications of our results for market design in general , and kidney exchange in particular .
2K_dev_1109	A descending clock auction ( DCA ) is for buying items from multiple sellers . The literature has focused on the case where each bidder has two options : to accept or reject the offered price . However , in many settings -- such as the FCC 's imminent incentive auction -- each bidder may be able to sell one from a set of options . We present a multi-option DCA ( MDCA ) framework where at each round , the auctioneer offers each bidder different prices for different options , and a bidder may find multiple options still acceptable . Setting prices during a MDCA is trickier than in a DCA . We develop a Markov chain model for the dynamics of each bidder 's state ( which options are still acceptable ) . We leverage it to optimize the trajectory of price offers to different bidders for different options . This is unlike most auctions which only compute the next price vector . Computing the trajectory enables better planning . We reoptimize the trajectory after each round . Each optimization minimizes total payment while ensuring feasibility in a stochastic sense . We also introduce percentile-based approaches to decrementing prices . Experiments with real FCC incentive auction interference constraint data show that the optimization-based approach dramatically outperforms the percentile-based approach -- because it takes feasibility into account in pricing . Both pricing techniques scale to the large .
2K_dev_1110	FlexRR provides a scalable , efficient solution to the straggler problem for iterative machine learning ( ML ) . The frequent ( e.g. , per iteration ) barriers used in traditional BSP-based distributed ML implementations cause every transient slowdown of any worker thread to delay all others . FlexRR combines a more flexible synchronization model with dynamic peer-to-peer re-assignment of work among workers to address straggler threads . Experiments with real straggler behavior observed on Amazon EC2 and Microsoft Azure , as well as injected straggler behavior stress tests , confirm the significance of the problem and the effectiveness of FlexRR 's solution . Using FlexRR , we consistently observe near-ideal run-times ( relative to no performance jitter ) across all real and injected straggler behaviors tested .
2K_dev_1111	A method and apparatus for determining pitch and yaw of an elongated interface object as it interacts with a touchscreen surface . A touch image is received , and this touch image has at least a first area that corresponds to an area of the touchscreen that has an elongated interface object positioned at least proximate to it . The elongated interface object has a pitch and a yaw with respect to the touchscreen surface . A first transformation is performed to obtain a first transformation image of the touch image , and a second transformation is performed to obtain a second transformation image of the touch image . The first transformation differs from the second transformation . The yaw is determined for the elongated interface object based on both the first and second transformation images . The pitch is determined based on at least one of the first and second transformation images .
2K_dev_1112	Which team is the best in the league ? How does my team fare with respect to the rest of the league ? These are questions that every sports fan is interested in knowing the answers to . In other cases , such as in college sports , knowing the answer to these questions is crucial for shaping the picture of spe- cific contests . In professional sports , sports networks provide power rankings regularly - typically every week or month de- pending on the season length of the league - based on their experts opinion . In this work we propose an alternative , ob- jective and network-based way of ranking sports teams . In brief , our method is based on analyzing a directed network formed between the teams of the corresponding leagues that captures their win-lose relationships . Using data from the National Football League and the National Basketball As- sociation , we show that even simple network theory metrics ( e.g. , Page Rank ) can provide a ranking that has the same ac- curacy in predicting winners of upcoming match-ups as more complicated systems ( e.g. , Cortana ) . We further explore the impact of the network structure on the prediction accuracy and we show that the cycles in the network are significantly correlated with the performance . We finally propose an ad- vanced ranking technique based on tensor decomposition .
2K_dev_1113	Advancements in mobile technologies mean that consumers can engage the digital world wherever they are and whenever they want . This intersection between the digital and the physical has important implications for consumer decision-making . We propose that mobile ecosystems vary in their capabilities and pervasivity ( i.e. , the degree to which a mobile ecosystem is accessible everywhere and at all times ) . Further , we propose that accounting for distinguishing aspects of mobile ecosystems , the context in which mobile ecosystems are used , and interactions between mobile ecosystems and mobile contexts are critical in advancing theoretical and substantive understanding of the role of mobile technologies in the marketplace . This perspective helps identify : 1 ) the types of data that empirical researchers may seek to gather and 2 ) the ways in which this data may be analyzed . Based on this perspective , we identify important research questions as well as opportunities and challenges for modeling mobile consumer decision-making .
2K_dev_1114	Describing videos with natural language is one of the ultimate goals of video understanding . Video records multi-modal information including image , motion , aural , speech and so on . MSR Video to Language Challenge provides a good chance to study multi-modality fusion in caption task . In this paper , we propose the multi-modal fusion encoder and integrate it with text sequence decoder into an end-to-end video caption framework . Features from visual , aural , speech and meta modalities are fused together to represent the video contents . Long Short-Term Memory Recurrent Neural Networks ( LSTM-RNNs ) are then used as the decoder to generate natural language sentences . Experimental results show the effectiveness of multi-modal fusion encoder trained in the end-to-end framework , which achieved top performance in both common metrics evaluation and human evaluation .
2K_dev_1115	Our position is that a key component of securing cyber-physical systems ( CPS ) is to develop a theory of accountability that encompasses both control and computing systems . We envision that a unified theory of accountability in CPS can be built on a foundation of causal information flow analysis . This theory will support design and analysis of mechanisms at various stages of the accountability regime : attack detection , responsibility-assignment ( e.g. , attack identification or localization ) , and corrective measures ( e.g. , via resilient control ) As an initial step in this direction , we summarize our results on attack detection in control systems . We use the Kullback-Liebler ( KL ) divergence as a causal information flow measure . We then recover , using information flow analyses , a set of existing results in the literature that were previously proved using different techniques . These results cover passive detection , stealthy attack characterization , and active detection . This research direction is related to recent work on accountability in computational systems [ 1 ] , [ 2 ] , [ 3 ] , [ 4 ] . We envision that by casting accountability theories in computing and control systems in terms of causal information flow , we can provide a common foundation to develop a theory for CPS that compose elements from both domains .
2K_dev_1116	Electric vehicles ( EVs ) , specifically Battery EVs ( BEVs ) , can offer significant energy and emission savings over internal combustion engine based vehicles . Norway has a long history of research and government incentives for BEVs . The BEV market and ample data sets in Norway allow us to fully examine consumers ' BEV choices influenced by car specifications , prices and government incentives ( public bus lanes access , toll waivers and charging stations ) . To capture the choices of heterogeneous personal consumers and business buyers , we use Random-Coefficient Discrete Choice Model ( referred to BLP model ) . Our study is instantiated on the entire BEV sales data in Norway from 2011 to 2013 , as well as demographics information at municipality level . The results suggest significant positive effects of BEV technology improvement , toll waivers and charging station density on BEV sales for both personal consumers and business buyers , except that bus lanes access may have a negative impact for personal consumers , possibly due to consumers ' concern regarding bus lane congestion . The effects on business buyers are generally less pronounced than on personal consumers . In addition , we find significant heterogeneity in consumer preferences over BEV price and car specifications . In particular , a 9,500 NOK increases in consumer income can lead to approximately 10 % decrease in price sensitivity on average . In other words , individual consumers with higher income would be less price-sensitive than those with lower income . Significant heterogeneity in incentive policy impacts on different brands are also found , especially for Renault , Ford , Nissan ( all three being a good compromise of prices and ranges ) and Tesla ( with an exceptionally long range )
2K_dev_1117	We study efficiency and budget balance for designing mechanisms in general quasi-linear domains . Green and Laffonti ? [ 13 ] proved that one can not generically achieve both . We consider strategyproof budget-balanced mechanisms that are approximately efficient . For deterministic mechanisms , we show that a strategyproof and budget-balanced mechanism must have a sink agent whose valuation function is ignored in selecting an alternative , and she is compensated with the payments made by the other agents . We assume the valuations of the agents come from a bounded open interval . This result strengthens Green and Laffont 's impossibility result by showing that even in a restricted domain of valuations , there does not exist a mechanism that is strategyproof , budget balanced , and takes every agent 's valuation into consideration -- a corollary of which is that it can not be efficient . Using this result , we find a tight lower bound on the inefficiencies of strategyproof , budget-balanced mechanisms in this domain . The bound shows that the inefficiency asymptotically disappears when the number of agents is large -- a result close in spirit to Green and Laffonti ? [ 13 , Theorem 9.4 ] . However , our results provide worst-case bounds and the best possible rate of convergence . Next , we consider minimizing any convex combination of inefficiency and budget imbalance . We show that if the valuations are unrestricted , no deterministic mechanism can do asymptotically better than minimizing inefficiency alone . Finally , we investigate randomized mechanisms and provide improved lower bounds on expected inefficiency . We give a tight lower bound for an interesting class of strategyproof , budget-balanced , randomized mechanisms . We also use an optimization-based approach -- in the spirit of automated mechanism design -- to provide a lower bound on the minimum achievable inefficiency of any randomized mechanism . Experiments with real data from two applications show that the inefficiency for a simple randomized mechanism is 5 -- -100 times smaller than the worst case . This relative difference increases with the number ofi ? agents .
2K_dev_1118	Tensors and tensor decompositions are very powerful and versatile tools that can model a wide variety of heterogeneous , multiaspect data . As a result , tensor decompositions , which extract useful latent information out of multiaspect data tensors , have witnessed increasing popularity and adoption by the data mining community . In this survey , we present some of the most widely used tensor decompositions , providing the key insights behind them , and summarizing them from a practitioners point of view . We then provide an overview of a very broad spectrum of applications where tensors have been instrumental in achieving state-of-the-art performance , ranging from social network analysis to brain data analysis , and from web mining to healthcare . Subsequently , we present recent algorithmic advances in scaling tensor decompositions up to todays big data , outlining the existing systems and summarizing the key ideas behind them . Finally , we conclude with a list of challenges and open problems that outline exciting future research directions .
2K_dev_1119	The Next-Generation Airborne Collision Avoidance System ( ACAS X ) is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft . It is currently being developed by the Federal Aviation Administration ( FAA ) . In this paper , we determine the geometric configurations under which the advice given by ACAS X is safe under a precise set of assumptions and formally verify these configurations using hybrid systems theorem proving techniques . We consider subsequent advisories and show how to adapt our formal verification to take them into account . We examine the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version , demonstrating how formal hybrid systems proving approaches are helping to ensure the safety of ACAS X . Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety .
2K_dev_1120	Low engagement rates and high attrition rates have been formidable challenges for mobile apps and their long-term success , especially for those whose revenues come mainly from in-app purchases . To date , still little is known about how companies can comprehensively identify user engagement stages so as to improve business revenues . This paper proposes a structural econometric framework for modeling of consumer latent engagement stages that accounts for both the time-varying nature of engagement and consumer forward-looking consumption behavior . The present study analyzed a fine-grained mobile tapstream dataset on mobile users ' continuous content consumption behavior in a popular mobile reading app . Our policy simulation enabled us to tailor , based on the model-detected engagement stages , an optimal pricing strategy to each consumer . Interestingly , we found that such an engagement-specific pricing strategy leads , simultaneously , to lower average prices for consumers and higher overall business revenues for the app . To further evaluate the effectiveness of our method , we conducted a randomized field experiment on a mobile app platform . Our experimental results provide more causal evidence that a personalized promotion strategy targeting user engagement stages can both decrease costs to app users and enhance overall business performance . Our structural-model- and field-experimentation-based findings are nontrivial and suggest , with respect to the crucial role of modeling user engagement , potential overall welfare improvements in the mobile app market .
2K_dev_1121	An increasingly prevalent technique for improving response time in queueing systems is the use of redundancy . In a system with redundant requests , each job that arrives to the system is copied and dispatched to multiple servers . As soon as the first copy completes service , the job is considered complete , and all remaining copies are deleted . A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists . We propose a theoretical model of redundancy , the Redundancy- d system , in which each job sends redundant copies to d servers chosen uniformly at random . We derive the first exact expressions for mean response time in Redundancy-d systems with any finite number of servers . We also find asymptotically exact expressions for the distribution of response time as the number of servers approaches infinity .
2K_dev_1122	HIV-1 CA capsid protein possesses intrinsic conformational flexibility , which is essential for its assembly into conical capsids and interactions with host factors . CA is dynamic in the assembled capsid , and residues in functionally important regions of the protein undergo motions spanning many decades of time scales . Chemical shift anisotropy ( CSA ) tensors , recorded in magic-angle-spinning NMR experiments , provide direct residue-specific probes of motions on nano- to microsecond time scales . We combined NMR , MD , and density-functional-theory calculations , to gain quantitative understanding of internal backbone dynamics in CA assemblies , and we found that the dynamically averaged 15N CSA tensors calculated by this joined protocol are in remarkable agreement with experiment . Thus , quantitative atomic-level understanding of the relationships between CSA tensors , local backbone structure , and motions in CA assemblies is achieved , demonstrating the power of integrating NMR experimental data and theory for cha ...
2K_dev_1123	Purpose This paper aims to examine how reversibility in disclosing personal information that is , having ( vs not having ) to option to later revise or retract personal information can impact consumers willingness to divulge personal information . Design/methodology/approach Three studies examined how informing consumers they may ( reversible condition ) or may not ( irreversible condition ) revise their personal information in the future affected their propensity to disclose personal information , compared to a control condition . Findings Study 1 ( which included three experiments with different time intervals between initial and revised disclosure ) showed that consumers disclose less in both the reversible and irreversible conditions , compared to the control condition . Studies 2 and 3 showed that this is because consumers treat reversibility as a cue to the sensitivity of the information they are asked to divulge , and that leads them to disclose less when reversibility or irreversibility is made explicitly salient beforehand . Practical implications As many marketers are interested in hoarding consumers personal information , privacy advocates call for methods that would ensure careful and well-informed disclosure . Offering reversibility to a decision to disclose personal information , or merely pointing out the irreversibility of that decision , can make consumers reevaluate the sensitivity of the situation , leading to more careful disclosures . Originality/value Although previous research on reversibility in consumer behavior focused on product return policies and showed that reversibility increases purchases , none have studied how reversibility affects self-disclosure and how it can decrease it .
2K_dev_1124	We study the performance of linear solvers for graph Laplacians based on the combinatorial cycle adjustment methodology proposed by [ Kelner-Orecchia-Sidford-Zhu STOC-13 ] . The approach finds a dual flow solution to this linear system through a sequence of flow adjustments along cycles . We study both data structure oriented and recursive methods for handling these adjustments . The primary difficulty faced by this approach , updating and querying long cycles , motivated us to study an important special case : instances where all cycles are formed by fundamental cycles on a length $ n $ path . Our methods demonstrate significant speedups over previous implementations , and are competitive with standard numerical routines .
2K_dev_1125	This paper studies an attacker against a cyber-physical system ( CPS ) whose goal is to move the state of a CPS to a target state while ensuring that his or her probability of being detected does not exceed a given bound . The attacker 's probability of being detected is related to the nonnegative bias induced by his or her attack on the CPS ' detection statistic . We formulate a linear quadratic cost function that captures the attacker 's control goal and establish constraints on the induced bias that reflect the attacker 's detection-avoidance objectives . When the attacker is constrained to be detected at the false-alarm rate of the detector , we show that the optimal attack strategy reduces to a linear feedback of the attacker 's state estimate . In the case that the attacker 's bias is upper bounded by a positive constant , we provide two algorithms -- an optimal algorithm and a sub-optimal , less computationally intensive algorithm -- to find suitable attack sequences . Finally , we illustrate our attack strategies in numerical examples based on a remotely-controlled helicopter under attack .
2K_dev_1126	Person re-identification ( re-ID ) has become increasingly popular in the community due to its application and research significance . It aims at spotting a person of interest in other cameras . In the early days , hand-crafted algorithms and small-scale evaluation were predominantly reported . Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes . Considering different tasks , we classify most current re-ID methods into two classes , i.e. , image-based and video-based ; in both tasks , hand-crafted and deep learning systems will be reviewed . Moreover , two new re-ID tasks which are much closer to real-world applications are described and discussed , i.e. , end-to-end re-ID and fast re-ID in very large galleries . This paper : 1 ) introduces the history of person re-ID and its relationship with image classification and instance retrieval ; 2 ) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image- and video-based re-ID ; 3 ) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries ; and 4 ) finally briefs some important yet under-developed issues .
2K_dev_1127	Real-time virtualization techniques have been investigated with the primary goal of consolidating multiple real-time systems onto a single hardware platform while ensuring timing predictability . However , a shared last-level cache ( LLC ) on recent multi-core platforms can easily hamper timing predictability due to the resulting temporal interference among consolidated workloads . Since such interference caused by the LLC is highly variable and may have not even existed in legacy systems to be consolidated , it poses a significant challenge for real-time virtualization . In this paper , we propose a real-time cache management framework for multi-core virtualization . Our framework introduces two hypervisor-level techniques , vLLC and vColoring , that enable the cache allocation of individual tasks running in a virtual machine ( VM ) , which is not achievable by the current state of the art . Our framework also provides a cache management scheme that determines cache allocation to tasks , designs VMs in a cache-aware manner , and minimizes the aggregated utilization of VMs to be consolidated . As a proof of concept , we implemented vLLC and vColoring in the KVM hypervisor running on x86 and ARM multi-core platforms . Experimental results with three different guest OSs , namely Linux/RK , vanilla Linux and MS Windows Embedded , show that our techniques can effectively control the cache allocation of tasks in VMs . Our cache management scheme yields a significant utilization benefit compared to other approaches .
2K_dev_1128	When navigating indoors , blind people are often unaware of key visual information , such as posters , signs , and exit doors . Our VizMap system uses computer vision and crowdsourcing to collect this information and make it available non-visually . VizMap starts with videos taken by on-site sighted volunteers and uses these to create a 3D spatial model . These video frames are semantically labeled by remote crowd workers with key visual information . These semantic labels are located within and embedded into the reconstructed 3D model , forming a query-able spatial representation of the environment . VizMap can then localize the user with a photo from their smartphone , and enable them to explore the visual elements that are nearby . We explore a range of example applications enabled by our reconstructed spatial representation . With VizMap , we move towards integrating the strengths of the end user , on-site crowd , online crowd , and computer vision to solve a long-standing challenge in indoor blind exploration .
2K_dev_1129	Electrical Impedance Tomography ( EIT ) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch . This prior work demonstrated great promise for non-invasive , high accuracy recognition of gestures for interactive control . We introduce a new system that offers improved sampling speed and resolution . In turn , this enables superior interior reconstruction and gesture recognition . More importantly , we use our new system as a vehicle for experimentation ' we compare two EIT sensing methods and three different electrode resolutions . Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input .
2K_dev_1130	We present a novel data classifier that is based on the regularization of graph signals . Our approach is based on the theory of discrete signal processing on graphs where the graph represents similarities between data and we interpret labels for the dataset elements as a signal indexed by the nodes of the graph . We postulate that true labels form a low-frequency graph signal and the classifier finds the smoothest graph signal that satisfies constraints given by known data labels . Our experiments demonstrate that our approach achieves high accuracy in multiclass classification and outperforms other classification approaches .
2K_dev_1131	Existing smartwatches rely on touchscreens for display and input , which inevitably leads to finger occlusion and confines interactivity to a small area . In this work , we introduce AuraSense , which enables rich , around-device , smartwatch interactions using electric field sensing as an adapted device . To explore how this sensing approach could enhance smartwatch interactions , we considered different antenna configurations and how they could enable useful interaction modalities . We identified four configurations that can support six well-known modalities of particular interest and utility , including gestures above or in close proximity to watches , and touchscreen-like finger tracking on the skin . We quantify the feasibility of these input modalities , suggesting that AuraSense can be low latency and robust across users and environments .
2K_dev_1132	We propose a geodesic distance on a Grassmannian manifold that can be used to quantify the shape progression patterns of the bilateral hippocampi , amygdalas , and lateral ventricles in healthy control ( HC ) , mild cognitive impairment ( MCI ) , and Alzheimer 's disease ( AD ) . Longitudinal magnetic resonance imaging ( MRI ) scans of 754 subjects ( 3092 scans in total ) were used in this study . Longitudinally , the geodesic distance was found to be proportional to the elapsed time separating the two scans in question . Cross-sectionally , utilizing a linear mixed-effects statistical model , we found that each structures annualized rate of change in the geodesic distance followed the order of AD > MCI > HC , with statistical significance being reached in every case . In addition , for each of the six structures of interest , within the same time interval ( e.g. , from baseline to the 6th month ) , we observed significant correlations between the geodesic distance and the cognitive deterioration as quantified by the ADAS-cog increase and the MMSE decrease . Furthermore , as the disease progresses over time , this linkage between the inter-shape geodesic distance and the cognitive decline becomes considerably stronger and more significant .
2K_dev_1133	The world is full of physical interfaces that are inaccessible to blind people , from microwaves and information kiosks to thermostats and checkout terminals . Blind people can not independently use such devices without at least first learning their layout , and usually only after labeling them with sighted assistance . We introduce VizLens - an accessible mobile application and supporting backend that can robustly and interactively help blind people use nearly any interface they encounter . VizLens users capture a photo of an inaccessible interface and send it to multiple crowd workers , who work in parallel to quickly label and describe elements of the interface to make subsequent computer vision easier . The VizLens application helps users recapture the interface in the field of the camera , and uses computer vision to interactively describe the part of the interface beneath their finger ( updating 8 times per second ) . We show that VizLens provides accurate and usable real-time feedback in a study with 10 blind participants , and our crowdsourcing labeling workflow was fast ( 8 minutes ) , accurate ( 99.7 % ) , and cheap ( $ 1.15 ) . We then explore extensions of VizLens that allow it to ( i ) adapt to state changes in dynamic interfaces , ( ii ) combine crowd labeling with OCR technology to handle dynamic displays , and ( iii ) benefit from head-mounted cameras . VizLens robustly solves a long-standing challenge in accessibility by deeply integrating crowdsourcing and computer vision , and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone .
2K_dev_1134	We propose a nonlinear dynamic model for an invasive electroencephalogram analysis that learns the optimal parameters of the neural population model via the LevenbergMarquardt algorithm . We introduce the crucial windows where the estimated parameters present patterns before seizure onset . The optimal parameters minimizes the error between the observed signal and the generated signal by the model . The proposed approach effectively discriminates between healthy signals and epileptic seizure signals . We evaluate the proposed method using an electroencephalogram dataset with normal and epileptic seizure sequences . The empirical results show that the patterns of parameters as a seizure approach and the method is efficient in analyzing nonlinear epilepsy electroencephalogram data . The accuracy of estimating the optimal parameters is improved by using the nonlinear dynamic model .
2K_dev_1135	Smartwatches and wearables are unique in that they reside on the body , presenting great potential for always-available input and interaction . Their position on the wrist makes them ideal for capturing bio-acoustic signals . We developed a custom smartwatch kernel that boosts the sampling rate of a smartwatch 's existing accelerometer to 4 kHz . Using this new source of high-fidelity data , we uncovered a wide range of applications . For example , we can use bio-acoustic data to classify hand gestures such as flicks , claps , scratches , and taps , which combine with on-device motion tracking to create a wide range of expressive input modalities . Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects , enabling passive object recognition that can augment everyday experiences with context-aware functionality . Finally , we can generate structured vibrations using a transducer , and show that data can be transmitted through the human body . Overall , our contributions unlock user interface techniques that previously relied on special-purpose and/or cumbersome instrumentation , making such interactions considerably more feasible for inclusion in future consumer devices .
2K_dev_1136	Given `` who-trusts/distrusts-whom '' information , how can we propagate the trust and distrust ? With the appearance of fraudsters in social network sites , the importance of trust prediction has increased . Most such methods use only explicit and implicit trust information ( e.g. , if Smith likes several of Johnson 's reviews , then Smith implicitly trusts Johnson ) , but they do not consider distrust . In this paper , we propose PIN -TRUST , a novel method to handle all three types of interaction information : explicit trust , implicit trust , and explicit distrust . The novelties of our method are the following : ( a ) it is carefully designed , to take into account positive , implicit , and negative information , ( b ) it is scalable ( i.e. , linear on the input size ) , ( c ) most importantly , it is effective and accurate . Our extensive experiments with a real dataset , Epinions.com data , of 100K nodes and 1M edges , confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy , achieving up to 50.4 percentage relative improvement .
2K_dev_1137	Many applications in speech , robotics , finance , and biology deal with sequential data , where ordering matters and recurrent structures are common . However , this structure can not be easily captured by standard kernel functions . To model such structure , we propose expressive closed-form kernel functions for Gaussian processes . The resulting model , GP-LSTM , fully encapsulates the inductive biases of long short-term memory ( LSTM ) recurrent networks , while retaining the non-parametric probabilistic advantages of Gaussian processes . We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic procedure and exploit the structure of these kernels for fast and scalable training and prediction . We demonstrate state-of-the-art performance on several benchmarks , and thoroughly investigate a consequential autonomous driving application , where the predictive uncertainties provided by GP-LSTM are uniquely valuable .
2K_dev_1138	Advancements in information technology often task users with complex and consequential privacy and security decisions . A growing body of research has investigated individuals choices in the presence of privacy and information security tradeoffs , the decision-making hurdles affecting those choices , and ways to mitigate such hurdles . This article provides a multi-disciplinary assessment of the literature pertaining to privacy and security decision making . It focuses on research on assisting individuals privacy and security choices with soft paternalistic interventions that nudge users toward more beneficial choices . The article discusses potential benefits of those interventions , highlights their shortcomings , and identifies key ethical , design , and research challenges .
2K_dev_1139	As mobile computing and cloud computing converge , the sensing and interaction capabilities of mobile devices can be seamlessly fused with compute-intensive and data-intensive processing in the cloud . Cloudlets are important architectural components in this convergence , representing the middle tier of a mobile device cloudlet cloud hierarchy . We show how cloudlets enable a new genre of applications called cognitive assistance applications that augment human perception and cognition . We describe a plug-and-play architecture for cognitive assistance , and a proof of concept using Google Glass .
2K_dev_1140	In the human genome , distal enhancers are involved in regulating target genes through proximal promoters by forming enhancer-promoter interactions . However , although recently developed high-throughput experimental approaches have allowed us to recognize potential enhancer-promoter interactions genome-wide , it is still largely unknown whether there are sequence-level instructions encoded in our genome that help govern such interactions . Here we report a new computational method ( named `` SPEID '' ) using deep learning models to predict enhancer-promoter interactions based on sequence-based features only , when the locations of putative enhancers and promoters in a particular cell type are given . Our results across six different cell types demonstrate that SPEID is effective in predicting enhancer-promoter interactions as compared to state-of-the-art methods that use non-sequence features from functional genomic signals . This work shows for the first time that sequence-based features alone can reliably predict enhancer-promoter interactions genome-wide , which provides important insights into the sequence determinants for long-range gene regulation .
2K_dev_1141	Sensorized commercial buildings are a rich target for building a new class of applications that improve operational and energy efficiency of building operations that take into account human activities . Such applications , however , rarely experience widespread adoption due to the lack of a common descriptive schema that would enable porting these applications and systems to different buildings . Our demo presents Brick [ 4 ] , a uniform schema for representing metadata in buildings . Our schema defines a concrete ontology for sensors , subsystems and relationships among them , which enables portable applications . Using a web application , we will demonstrate real buildings that have been mapped to the Brick schema , and show application queries that extracts relevant metadata from these buildings . The attendees would be able to create example buildings and write their own queries .
2K_dev_1142	In this paper , we present a platform designed for low-power real-time sensing of the number of occupants in indoor spaces . The system transmits a wide-band ultrasonic signal into a room and then processes the superposition of the reflections recorded by a microphone . The system has two modes of operation , one for presence detection and one for estimating the number of occupants in a region . The presence detection uses the difference between multiple transmissions in succession with a set of general classifiers that make a binary decision about if the room contains occupants . We then use a semi-supervised learning approach based on Weighted Principal Component Analysis ( WPCA ) that requires minimal training data to estimate the number of occupants . We also present the design of an energy harvesting embedded platform and demonstrate that our algorithm can continuously execute using energy harvested from indoor solar panels . The platform has a dual Bluetooth Low-Energy and 802.15.4 interface to communicate with a gateway or nearby mobile phone that runs an interface that aids in collecting training data . We evaluate our algorithm on a wide-variety of indoor spaces as well as benchmark the hardware in terms of sampling rate given an energy budget . On more than three weeks of data , we see that we can detect motions with an average of 85 % recall rate and perform occupancy counting with an average error of 10 % in terms of maximum occupancy .
2K_dev_1143	Commercial buildings have long since been a primary target for applications from a number of areas : from cyber-physical systems to building energy use to improved human interactions in built environments . While technological advances have been made in these areas , such solutions rarely experience widespread adoption due to the lack of a common descriptive schema which would reduce the now-prohibitive cost of porting these applications and systems to different buildings . Recent attempts have sought to address this issue through data standards and metadata schemes , but fail to capture the set of relationships and entities required by real applications . Building upon these works , this paper describes Brick , a uniform schema for representing metadata in buildings . Our schema defines a concrete ontology for sensors , subsystems and relationships among them , which enables portable applications . We demonstrate the completeness and effectiveness of Brick by using it to represent the entire vendor-specific sensor metadata of six diverse buildings across different campuses , comprising 17,700 data points , and running eight complex unmodified applications on these buildings .
2K_dev_1144	Several generations of inexpensive depth cameras have opened the possibility for new kinds of interaction on everyday surfaces . A number of research systems have demonstrated that depth cameras , combined with projectors for output , can turn nearly any reasonably flat surface into a touch-sensitive display . However , even with the latest generation of depth cameras , it has been difficult to obtain sufficient sensing fidelity across a table-sized surface to get much beyond a proof-of-concept demonstration . In this paper we present DIRECT , a novel touch-tracking algorithm that merges depth and infrared imagery captured by a commodity sensor . This yields significantly better touch tracking than from depth data alone , as well as any prior system . Further extending prior work , DIRECT supports arbitrary user orientation and requires no prior calibration or background capture . We describe the implementation of our system and quantify its accuracy through a comparison study of previously published , depth-based touch-tracking algorithms . Results show that our technique boosts touch detection accuracy by 15 % and reduces positional error by 55 % compared to the next best-performing technique .
2K_dev_1145	In this demonstration we present Pulsar , a speed-of-light propagation-aware time synchronization platform . Pulsar uses ultra-wideband ( UWB ) radios for time transfer with each node backed by a chip scale atomic clock ( CSAC ) that in combination are able to provide accuracy on the order of 10 's of nanoseconds for indoor applications . The demonstration will show two Pulsar boards generating a tightly synchronized PPS output that is able to adjust for the distance between the nodes . Even without communication , the devices maintain synchronization over multiple seconds due to a stable CSAC clocking the system .
2K_dev_1146	Cohesion and structural equivalence are two competing network models to explain diffusion of innovation . The dispute of which model plays a more influential role has not been resolved . This paper attempts to reconcile this problem in a large network setting adoption of caller ringback tone ( CRBT ) in a cellular telephone conversation network . Since this societal scale network is very large , we use a novel technique to extract multiple densely connected and self-contained subpopulations from the network . We found subpopulation size in such million-node network only falls in two levels , 200 and 500 , in the extraction step . Using a new auto-probit model with network terms , we then compare the competing influences of cohesion and structural equivalence on each of the subpopulation extracted . Finally we use meta-analysis to summarize the estimated parameters from all subpopulations . The results show CRBT adoption is affected by both cohesion and structural equivalence . The size and direction of network influence both change with the size of group . Structural equivalence has a negative effect on adoption when group size is at about 200 , and has a positive effect when group size is at about 500 . The effect of cohesion , on the other hand , is consistent .
2K_dev_1147	To respond to environmental changes , such as drought , plants must regulate numerous cellular processes . Working in the model plant Arabidopsis , Song et al . profiled the binding of 21 transcription factors to chromatin and mapped the complex gene regulatory networks involved in the response to the plant hormone abscisic acid . The work provides a framework for understanding and modulating plant responses to stress . Science , this issue p. [ 598 ] [ 1 ] [ 1 ] : http : //www.sciencemag.org/content/354/6312/aag1550.full
2K_dev_1148	In this paper we introduce BOLT , a novel approach for the problem of energy disaggregation that performs online binary matrix factorization on a sequence of high frequency current cycles collected in a building to infer additive subcomponents of the current signal . The system learns these constituent current waveforms in an unsupervised fashion and , in a subsequent step , seeks to find combinations of these subcomponents that constitute appliances . By doing so , points in time when appliances are active and , to some degree , their power consumption can be estimated by BOLT . Our system treats energy disaggregation as a binary matrix factorization problem and uses a neural network , with binary activations in the one but last layer and a linear output layer , to solve it . The algorithmic performance of the proposed method is evaluated on a publicly available dataset . Furthermore , we show that , once the model is trained , the algorithm can perform inference in real-time on inexpensive off-the-shelf and general purpose hardware which allows leveraging high-frequency information without having to explicitly transmit and store large amounts of data to a centralized repository .
2K_dev_1149	Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures . We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification , multi-task learning , additive covariance structures , and stochastic gradient training . Specifically , we apply additive base kernels to subsets of output features from deep neural architectures , and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective . Within this framework , we derive an efficient form of stochastic variational inference which leverages local kernel interpolation , inducing points , and structure exploiting algebra . We show improved performance over stand alone deep networks , SVMs , and state of the art scalable Gaussian processes on several classification benchmarks , including an airline delay dataset containing 6 million training points , CIFAR , and ImageNet .
2K_dev_1150	In graph signal processing , the graph adjacency matrix or the graph Laplacian commonly define the shift operator . The spectral decomposition of the shift operator plays an important role in that the eigenvalues represent frequencies and the eigenvectors provide a spectral basis . This is useful , for example , in the design of filters . However , the graph or network may be uncertain due to stochastic influences in construction and maintenance , and , under such conditions , the eigenvalues of the shift matrix become random variables . This paper examines the spectral distribution of the eigenvalues of random networks formed by including each link of a D-dimensional lattice supergraph independently with identical probability , a percolation model . Using the stochastic canonical equation methods developed by Girko for symmetric matrices with independent upper triangular entries , a deterministic distribution is found that asymptotically approximates the empirical spectral distribution of the scaled adjacency matrix for a model with arbitrary parameters . The main results characterize the form of the solution to an important system of equations that leads to this deterministic distribution function and significantly reduce the number of equations that must be solved to find the solution for a given set of model parameters . Simulations comparing the expected empirical spectral distributions and the computed deterministic distributions are provided for sample parameters .
2K_dev_1151	In this paper , we address the problem of range-based beacon placement given a floor plan to support indoor localization systems . Existing approaches for trilateration require three or more beacons to determine a unique position solution . We show that with prior knowledge of the map and a model of beacon coverage , it is possible to uniquely localize with only two beacons . This not only reduces installation cost by requiring fewer nodes , but can also improve robustness . One of the main challenges with respect to beacon placement algorithms is defining a metric for estimating performance . We propose augmenting the commonly used Geometric Dilution of Precision ( GDOP ) metric to account for indoor spaces . We then use this enhanced GDOP metric as part of a toolchain to compare various beacon placement algorithms in terms of coverage and expected accuracy . When applied to a set of real floor plans , our approach is able to reduce the number of beacons between 22 % and 60 % ( 33 % on an average ) as compared to standard trilateration .
2K_dev_1152	With the potential to enhance the power system 's operational flexibility in a cost-effective way , demand response is gaining increased attention worldwide . Industrial loads such as cement crushing plants consume large amounts of electric energy and therefore are prime candidates for the provision of significant amounts of demand response . They have the capability to turn on/off an arbitrary number of their crushers thereby adjusting their electric power consumption . However , the change in power consumption by cement crushing plants and also other industrial loads are often not granular enough to provide valuable ancillary services such as regulation and load following . In this paper , we propose a coordination method based on model predictive control to overcome the granularity restriction with the help of an energy storage .
2K_dev_1153	Background Reconstructing ancestral gene orders in the presence of duplications is important for a better understanding of genome evolution . Current methods for ancestral reconstruction are limited by either computational constraints or the availability of reliable gene trees , and often ignore duplications altogether . Recently , methods that consider duplications in ancestral reconstructions have been developed , but the quality of reconstruction , counted as the number of contiguous ancestral regions found , decreases rapidly with the number of duplicated genes , complicating the application of such approaches to mammalian genomes . However , such high fragmentation is not encountered when reconstructing mammalian genomes at the synteny-block level , although the relative positions of genes in such reconstruction can not be recovered .
2K_dev_1154	We present CapCam , a novel technique that enables smartphones ( and similar devices ) to establish quick , ad-hoc connections with a host touchscreen device , simply by pressing a device to the screen 's surface . Pairing data , used to bootstrap a conventional wireless connection , is transmitted optically to the phone 's rear camera . This approach utilizes the near-ubiquitous rear camera on smart devices , making it applicable to a wide range of devices , both new and old . CapCam also tracks phones ' physical positions on the host capacitive touchscreen without any instrumentation , enabling a wide range of targeted interactions . We quantify the communication performance of our pairing approach and demonstrate data transmission rates up to four times faster than prior camera-based techniques . To demonstrate the unique capability and utility of our system , we built a series of example applications , highlighting different interaction techniques CapCam enables .
2K_dev_1155	This notebook paper describes our solution from UTSCMU team in the THUMOS 2015 action recognition challenge . Our system contains two major components , video representation generated by VLAD encoding from ConvNet features and multi-skip improved Dense Trajectories . In addition , we explore optical flow ConvNet and acoustic features such as MFCC and ASR in our system . We demonstrate that our complete system can achieve state-of-the-art performance in large-scale action recognition tasks .
2K_dev_1156	Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications . These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset . However , the high variance inherent in these noisy gradients degrades performance and leads to slower mixing . In this paper , we present techniques for reducing variance in stochastic gradient Langevin dynamics , yielding novel stochastic Monte Carlo methods that improve performance by reducing the variance in the stochastic gradient . We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics . This is complemented by impressive empirical results obtained on a variety of real world datasets , and on four different machine learning tasks ( regression , classification , independent component analysis and mixture modeling ) . These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods .
2K_dev_1157	We address the problem of estimating core numbers of nodes by reading edges of a large graph stored in external memory . The core number of a node is the highest k-core in which the node participates . Core numbers are useful in many graph mining tasks , especially ones that involve finding communities of nodes , influential spreaders and dense subgraphs . Large graphs often do not fit on the memory of a single machine . Existing external memory solutions do not give bounds on the required space . In practice , existing solutions also do not scale with the size of the graph . We propose NimbleCore , an iterative external-memory algorithm , which estimates core numbers of nodes using O ( n log d max ) space , where n is the number of nodes and d max is the maximum node-degree in the graph . We also show that NimbleCore requires O ( n ) space for graphs with power-law degree distributions . Experiments on forty-eight large graphs from various domains demonstrate that NimbleCore gives space savings up to 60X , while accurately estimating core numbers with average relative error less than 2.3 % .
2K_dev_1158	This paper studies recursive nonlinear least squares parameter estimation in inference networks with observations distributed across multiple agents and sensed sequentially over time . Conforming to a given inter-agent communication or interaction topology , distributed recursive estimators of the consensus + innovations type are presented in which at every observation sampling epoch the network agents exchange a single round of messages with their communication neighbors and recursively update their local parameter estimates by simultaneously processing the received neighborhood data and the new information ( innovation ) embedded in the observation sample . Under rather weak conditions on the connectivity of the inter-agent communication and a global observability criterion , it is shown that the proposed algorithms lead to consistent parameter estimates at each agent . Furthermore , under standard smoothness assumptions on the sensing nonlinearities , the distributed estimators are shown to yield order-optimal convergence rates , i.e. , as far as the order of pathwise convergence is concerned , the local agent estimates are as good as the optimal centralized nonlinear least squares estimator having access to the entire network observation data at all times .
2K_dev_1159	How can we design a product or movie that will attract , for example , the interest of Pennsylvania adolescents or liberal newspaper critics ? What should be the genre of that movie and who should be in the cast ? In this work , we seek to identify how we can design new movies with features tailored to a specific user population . We formulate the movie design as an optimization problem over the inference of user-feature scores and selection of the features that maximize the number of attracted users . Our approach , PNP , is based on a heterogeneous , tripartite graph of users , movies , and features ( e.g . actors , directors , genres ) , where users rate movies and features contribute to movies . We learn the preferences by leveraging user similarities defined through different types of relations , and show that our method outperforms state-of-the-art approaches , including matrix factorization and other heterogeneous graph-based analysis . We evaluate PNP on publicly available real-world data and show that it is highly scalable and effectively provides movie designs oriented towards different groups of users , including men , women , and adolescents .
2K_dev_1160	The World Wide Web ( WWW ) has become a rapidly growing platform consisting of numerous sources which provide supporting or contradictory information about claims ( e.g. , `` Chicken meat is healthy '' ) . In order to decide whether a claim is true or false , one needs to analyze content of different sources of information on the Web , measure credibility of information sources , and aggregate all these information . This is a tedious process and the Web search engines address only part of the overall problem , viz. , producing only a list of relevant sources . In this paper , we present ClaimEval , a novel and integrated approach which given a set of claims to validate , extracts a set of pro and con arguments from the Web information sources , and jointly estimates credibility of sources and correctness of claims . ClaimEval uses Probabilistic Soft Logic ( PSL ) , resulting in a flexible and principled framework which makes it easy to state and incorporate different forms of prior-knowledge . Through extensive experiments on real-world datasets , we demonstrate ClaimEval 's capability in determining validity of a set of claims , resulting in improved accuracy compared to state-of-the-art baselines .
2K_dev_1161	Human-Computer Music Performance for popular music - where musical structure is important , but where musicians often decide on the spur of the moment exactly what the musical form will be - presents many challenges to make computer systems that are flexible and adaptable to human musicians . One particular challenge is that humans easily follow scores and chord charts , adapt these to new performance plans , and understand media locations in musical terms ( beats and measures ) , while computer music systems often use rigid and even numerical representations that are difficult to work with . We present new formalisms and representations , and a corresponding implementation , where musical material in various media is synchronized , where musicians can quickly alter the performance order by specifying ( re- ) arrangements of the material , and where interfaces are supported in a natural way by music notation .
2K_dev_1162	In networks such as the smart grid , communication networks , and social networks , local measurements/observations are scattered over a wide geographical area . Centralized inference algorithm are based on gathering all the observations at a central processing unit . However , with data explosion and ever-increasing network sizes , centralized inference suffers from large communication overhead , heavy computation burden at the center , and susceptibility to central node failure . This paper considers inference over networks using factor graphs and a distributed inference algorithm based on Gaussian belief propagation . The distributed inference involves only local computation of the information matrix and of the mean vector and message passing between neighbors . We discover and show analytically that the message information matrix converges exponentially fast to a unique positive definite limit matrix for arbitrary positive semidefinite initialization . We provide the necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator . An easily verifiable sufficient convergence condition on the topology of a factor graph is further provided .
2K_dev_1163	For numerous scientific applications Sparse Matrix-Vector multiplication ( SpMV ) is one of the most important kernels . Unfortunately , due to its very low ratio of computation to memory access SpMV is inherently a memory bound problem . On the other hand , the main memory bandwidth of commercial off-the-shelf ( COTS ) architectures is insufficient for available computation resources on these platforms , well known as the memory wall problem . As a result , COTS architectures are unsuitable for SpMV . Furthermore , SpMV requires random access into a memory space which is far too big for cache . Hence , it becomes difficult to utilize the main memory bandwidth which is already scarce .
2K_dev_1164	High performance dense linear algebra ( DLA ) libraries often rely on a general matrix multiply ( Gemm ) kernel that is implemented using assembly or with vector intrinsics . In particular , the real-valued Gemm kernels provide the overwhelming fraction of performance for the complex-valued Gemm kernels , along with the entire level-3 BLAS and many of the real and complex LAPACK routines . Thus , achieving high performance for the Gemm kernel translates into a high performance linear algebra stack above this kernel . However , it is a monumental task for a domain expert to manually implement the kernel for every library-supported architecture . This leads to the belief that the craft of a Gemm kernel is more dark art than science . It is this premise that drives the popularity of autotuning with code generation in the domain of DLA . This paper , instead , focuses on an analytical approach to code generation of the Gemm kernel for different architecture , in order to shed light on the details or voo-doo required for implementing a high performance Gemm kernel . We distill the implementation of the kernel into an even smaller kernel , an outer-product , and analytically determine how available SIMD instructions can be used to compute the outer-product efficiently . We codify this approach into a system to automatically generate a high performance SIMD implementation of the Gemm kernel . Experimental results demonstrate that our approach yields generated kernels with performance that is competitive with kernels implemented manually or using empirical search .
2K_dev_1165	Many real-world graphs , such as those that arise from the web , biology and transportation , appear random and without a structure that can be exploited for performance on modern computer architectures . However , these graphs have a scale-free graph topology that can be leveraged for locality . Existing sparse data formats are not designed to take advantage of this structure . They focus primarily on reducing storage requirements and improving the cost of certain matrix operations for these large data sets . Therefore , we propose a data structure for storing real-world scale-free graphs in a sparse and hierarchical fashion . By maintaining the structure of the graph , we preserve locality in the graph and in the cache . For synthetic scale-free graph data we outperform the state of the art for graphs with up to 10 7 non-zero edges .
2K_dev_1166	We present a novel subset scan method to detect if a probabilistic binary classifier has statistically significant bias -- over or under predicting the risk -- for some subgroup , and identify the characteristics of this subgroup . This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias and poor classifier fit , not just in one or two dimensions of features of a priori interest , but in the space of all possible feature subgroups . We use subset scan and parametric bootstrap methods to efficiently address the difficulty of assessing the exponentially many possible subgroups . We also suggest several useful extensions of this method for increasing interpretability of predictive models and prediction performance .
2K_dev_1167	Understanding how brain functions has been an intriguing topic for years . With the recent progress on collecting massive data and developing advanced technology , people have become interested in addressing the challenge of decoding brain wave data into meaningful mind states , with many machine learning models and algorithms being revisited and developed , especially the ones that handle time series data because of the nature of brain waves . However , many of these time series models , like HMM with hidden state in discrete space or State Space Model with hidden state in continuous space , only work with one source of data and can not handle different sources of information simultaneously . In this paper , we propose an extension of State Space Model to work with different sources of information together with its learning and inference algorithms . We apply this model to decode the mind state of students during lectures based on their brain waves and reach a significant better results compared to traditional methods .
2K_dev_1168	Abstract This paper presents a Grammar-aware Driver Parsing ( GDP ) algorithm , with deep features , to provide a novel driver behavior situational awareness system ( DB-SAW ) . A deep model is first trained to extract highly discriminative features of the driver . Then , a grammatical structure on the deep features is defined to be used as prior knowledge for a semi-supervised proposal candidate generation . The Region with Convolutional Neural Networks ( R-CNN ) method is ultimately utilized to precisely segment parts of the driver . The proposed method not only aims to automatically find parts of the driver in challenging drivers in the wild databases , i.e . the standardized Strategic Highway Research Program ( SHRP-2 ) and the challenging Vision for Intelligent Vehicles and Application ( VIVA ) , but is also able to investigate seat belt usage and the position of the driver 's hands ( on a phone vs on a steering wheel ) . We conduct experiments on various applications and compare our GDP method against other state-of-the-art detection and segmentation approaches , i.e . SDS [ 1 ] , CRF-RNN [ 2 ] , DJTL [ 3 ] , and R-CNN [ 4 ] on SHRP-2 and VIVA databases .
2K_dev_1169	Mobile botnets have proliferated with the popularization of mobile and portable devices , being a simple and powerful method to launch Distributed Denial of Service ( DDoS ) attacks . This letter presents a stochastic adaptive model for mobile botnets dynamics and their self-organized and self-adaptive behavior to generate DDoS attacks . The bots collaborations combine reinforcement and fading rules based upon the level of servers activity and map to a time-varying weighted directed graph . This model can explain the natural emergence of two distinct time-scales when bots massively attack a server .
2K_dev_1170	Recent computer systems research has proposed using redundant requests to reduce latency . The idea is to replicate a request so that it joins the queue at multiple servers . The request is considered complete as soon as any one copy of the request completes . Redundancy is beneficial because it allows us to overcome server-side variability the fact that the server we choose might be temporarily slow due to factors such as background load , network interrupts , and garbage collection . When there is significant server-side variability , replicating requests can greatly reduce response times . In the past few years , queueing theorists have begun to study redundancy , first via approximations , and , more recently , via exact analysis . Unfortunately , for analytical tractability , most existing theoretical analysis has assumed an Independent Runtimes ( IR ) model , wherein the replicas of a job each experience independent runtimes ( service times ) at different servers . The IR model is unrealistic and has led to theoretical results which can be at odds with computer systems implementation results . This paper introduces a much more realistic model of redundancy . Our model allows us to decouple the inherent job size ( X ) from the server-side slowdown ( S ) , where we track both S and X for each job . Analysis within the S & X model is , of course , much more difficult . Nevertheless , we design a policy , Redundant-to-Idle-Queue ( RIQ ) which is both analytically tractable within the S & X model and has provably excellent performance .
2K_dev_1171	The kernel trick becomes a burden for some machine learning tasks such as dictionary learning , where a huge amount of training samples are needed , making the kernel matrix gigantic and infeasible to store or process . In this work , we propose to alleviate this problem and achieve Gaussian RBF kernel expansion explicitly for dictionary learning using Fastfood transform , which is an approximation of full kernel expansion . We have shown , in the context of missing data recovery through joint dictionary learning i.e . periocular-based full face hallucination , that the approximated kernel expansion using Fastfood transform for joint dictionary learning yields much better results than its image space counterparts . Also , explicit kernel expansion through Fastfood allows us to de-kernelize the reconstructed image in the feature space back to the image space , enabling applications that require reconstructive dictionaries such as cross-domain reconstruction , image super-resolution , missing data recovery , etc .
2K_dev_1172	Policy approaches for addressing emerging consumer privacy concerns increasingly rely on providing consumers with more information and control over the usage of their personal data . In three experiments , we evaluate the efficacy of such mechanisms in the face of subtle but common variation in the presentation of privacy choices to consumers . We find that consumers decision frames and thus , their propensity to select privacy protective alternatives can be subtly but powerfully influenced by commonplace heterogeneity in the presentation of privacy choices . Our results suggest that choice mechanisms alone may not reliably serve policy maker goals of protecting consumers privacy in the face of emerging data practices by firms . 1
2K_dev_1173	Counterfactual Regret Minimization ( CFR ) is a popular iterative algorithm for approximating Nash equilibria in imperfect-information multi-step two-player zero-sum games . We introduce the first general , principled method for warm starting CFR . Our approach requires only a strategy for each player , and accomplishes the warm start at the cost of a single traversal of the game tree . The method provably warm starts CFR to as many iterations as it would have taken to reach a strategy profile of the same quality as the input strategies , and does not alter the convergence bounds of the algorithms . Unlike prior approaches to warm starting , ours can be applied in all cases . Our method is agnostic to the origins of the input strategies . For example , they can be based on human domain knowledge , the observed strategy of a strong agent , the solution of a coarser abstraction , or the output of some algorithm that converges rapidly at first but slowly as it gets closer to an equilibrium . Experiments demonstrate that one can improve overall convergence in a game by first running CFR on a smaller , coarser abstraction of the game and then using the strategy in the abstract game to warm start CFR in the full game .
2K_dev_1174	Long-standing policy approaches to privacy protection are centered on consumer notice and control and assume that privacy decision making is a deliberative process of comparison between costs and benefits from information disclosure . An emerging body of work , however , documents the powerful effects of factors unrelated to objective trade-offs in privacy settings . In this paper , we investigate how focusing on the process by which individuals make privacy choices can help explain the impact of rational and behavioral factors on privacy decision making . In an online experiment , we borrow from query-theory literature and measure individuals ' considerations ( that is , queries ) across manipulations of rational and behavioral factors . We find that effects of rational and behavioral factors are associated with differences in the order and valence of queries considered in privacy settings . Our results confirm that understanding how differences in privacy choice emerge can help harmonize disparate perspectives on privacy decision making .
2K_dev_1175	Touch sensitive devices , methods and computer readable recording mediums are provided that allow for improved classification of objects against a touch sensitive surface of a touch sensitive device based upon analysis of subdivisions of data representing contact with the touch sensitive surface during a period of time .
2K_dev_1176	Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled a myriad of new applications many of which provide aerial vision-based sensing . In intrusion detection or target tracking applications , it is important to reach a given area of interest in the shortest time , and create an online data streaming connection to a monitoring station for immediate delivery of content to the operator . However , if the area of interest ( AOI ) is not contained in the field of view of a single UAV , it is necessary to move the sensor-UAV to sweep the region in order to provide the most fresh information as possible . In order to improve the collection time of the AOI , we can cooperatively use multiple UAVs creating an array of moving cameras , that always remain connected without breaks in communication . In this work , we validate our approach with a simulation that captures physical models and the application layer of each UAV , as well as the wireless network . We propose an optimal solution to sweep the AOI as well as a decentralised formation control algorithm that maintains UAVs equally separated along the optimal path that covers the whole AOI , even with external disturbances such as wind gusts . We show a seven-fold increase in the refresh rate of the AOI coverage when comparing to a solution without the optimal sweeping and decentralised formation control .
2K_dev_1177	Skill ladders may help crowd workers to `` skill up '' as they work . But what other technical innovations will lead to better opportunities for crowd work ?
2K_dev_1178	With ever-advancing genomic technologies , it has become increasingly clear that cell-to-cell genomic variability is a ubiquitous feature of multicellular systems with importance to numerous phenomena in health and disease . While technologies for single-cell genomics are rapidly improving , though , they are still impractical for the scales needed to characterize genomic heterogeneity of complex mixtures across large patient populations , leaving the field highly dependent on computational inference to fill in the gaps in what it is practical to measure experimentally . Genomic deconvolution and phylogenetic methods have become subfields in themselves for making sense of still-limited genomic data in terms of coherent models of genomic heterogeneity . There is probably no system for which this phenomenon has been more intensively studied than cancers , where cell-to-cell genetic heterogeneity is now appreciated as key to tumor initiation , progression , and response to treatment . This talk will explore computational challenges in reconstructing models of genomic heterogeneity and the evolutionary processes by which it develops , as well as strategies for meeting those challenges , with particular focus on intra-tumor heterogeneity . It will in the process explore computational strategies for various sources of genomic data ( bulk , single-cell , and combinations ) and examine the tradeoffs between them . It will conclude with consideration of some emerging directions and open problems in studies of heterogeneity in multicellular systems , in cancers and beyond .
2K_dev_1179	Robust face detection is one of the most important preprocessing steps to support facial expression analysis , facial landmarking , face recognition , pose estimation , building of 3D facial models , etc . Although this topic has been intensely studied for decades , it is still challenging due to numerous variants of face images in real-world scenarios . In this paper , we present a novel approach named Multiple Scale Faster Region-based Convolutional Neural Network ( MS-FRCNN ) to robustly detect human facial regions from images collected under various challenging conditions , e.g . large occlusions , extremely low resolutions , facial expressions , strong illumination variations , etc . The proposed approach is benchmarked on two challenging face detection databases , i.e . the Wider Face database and the Face Detection Dataset and Benchmark ( FDDB ) , and compared against recent other face detection methods , e.g . Two-stage CNN , Multi-scale Cascade CNN , Faceness , Aggregate Chanel Features , HeadHunter , Multi-view Face Detection , Cascade CNN , etc . The experimental results show that our proposed approach consistently achieves highly competitive results with the state-of-the-art performance against other recent face detection methods .
2K_dev_1180	This report documents the program and the outcomes of Dagstuhl Seminar 16232 `` Fair Division '' . The seminar was composed of technical sessions with regular talks , and discussion sessions distributed over the full week .
2K_dev_1181	The HCOMP conference was created as a venue for exchanging ideas and developments on principles , experiments , and implementations of systems that rely on programmatic access to human intellect to perform some aspect of computation , or where human perception , knowledge , reasoning , or coordinated physical activities contributes to the operation of larger systems and applications . Human computation promises to play an important role in research on principles of artificial intelligence as well as in the engineering of systems that can take advantage of the changing complementarities of human and machine intellect .
2K_dev_1182	Hybrid systems combine discrete dynamics with continuous dynamics along differential equations . They arise frequently in many safety-critical application domains , including aviation , automotive , railway , and robotics . But how can we ensure that these systems are guaranteed to meet their design goals , e.g. , that an aircraft will not crash into another one ? This talk describes how hybrid systems can be proved using differential dynamic logic . Differential dynamic logic ( dL ) provides compositional logics , programming languages , and reasoning principles for hybrid systems . As implemented in the theorem prover KeYmaera X , dL has been instrumental in verifying many applications , including the Airborne Collision Avoidance System ACAS X , the European Train Control System ETCS , automotive systems , mobile robot navigation , and a surgical robot system for skull-base surgery .
2K_dev_1183	5aural speech by converting it into visual text with less than a five second delay . Keeping the delay short 6 allows end-users to follow and participate in conversations . This article focuses on the fundamental prob7 lem that makes real-time captioning difficult : sequential keyboard typing is much slower than speaking . We 8 first surveyed the audio characteristics of 240 one-hour-long captioned lectures on YouTube , such as speed 9 and duration of speaking bursts . We then analyzed how these characteristics impact caption generation and 10 readability , considering specifically our human-powered collaborative captioning approach . We note that 11 most of these characteristics are also present in more general domains . For our caption comparison evalu12 ation , we transcribed a classroom lecture in real-time using all three captioning approaches . We recruited 13 48 participants ( 24 DHH ) to watch these classroom transcripts in an eye-tracking laboratory . We presented 14 these captions in a randomized , balanced order . We show that both hearing and DHH participants preferred 15 and followed collaborative captions better than those generated by automatic speech recognition ( ASR ) or 16 professionals due to the more consistent flow of the resulting captions . These results show the potential to 17 reliably capture speech even during sudden bursts of speed , as well as for generating enhanced captions , 18 unlike other human-powered captioning approaches . 19
2K_dev_1184	Kidney exchange is a type of barter market where patients exchange willing but incompatible donors . These exchanges are conducted via cycleswhere each incompatible patient-donor pair in the cycle both gives and receives a kidneyand chains , which are started by an altruist donor who does not need a kidney in return . Finding the best combination of cycles and chains is hard . The leading algorithms for this optimization problem use either branch and pricea combination of branch and bound and column generationor constraint generation . We show a correctness error in the leading prior branch-and-price-based approach [ Glorie et al . 2014 ] . We develop a provably correct fix to it , which also necessarily changes the algorithm 's complexity , as well as other improvements to the search algorithm . Next , we compare our solver to the leading constraint-generation-based solver and to the best prior correct branch-and-price-based solver . We focus on the setting where chains have a length cap . A cap is desirable in practice since if even one edge in the chain fails , the rest of the chain fails : the cap precludes very long chains that are extremely unlikely to execute and instead causes the solution to have more parallel chains and cycles that are more likely to succeed . We work with the UNOS nationwide kidney exchange , which uses a chain cap . Algorithms from our group autonomously make the transplant plans for that exchange . On that real data and demographically-accurate generated data , our new solver scales significantly better than the prior leading approaches .
2K_dev_1185	We consider the problem of computing a maximal independent set ( MIS ) in an extremely harsh broadcast model that relies only on carrier sensing . The model consists of an anonymous broadcast network in which nodes have no knowledge about the topology of the network or even an upper bound on its size . Furthermore , it is assumed that an adversary chooses at which time slot each node wakes up . At each time slot a node can either beep , that is , emit a signal , or be silent . At a particular time slot , beeping nodes receive no feedback , while silent nodes can only differentiate between none of its neighbors beeping , or at least one of its neighbors beeping . We start by proving a lower bound that shows that in this model , it is not possible to locally converge to an MIS in sub-polynomial time . We then study four different relaxations of the model which allow us to circumvent the lower bound and find an MIS in polylogarithmic time . First , we show that if a polynomial upper bound on the network size is known , it is possible to find an MIS in \ ( \mathcal O ( \log ^3 n ) \ ) time . Second , if we assume sleeping nodes are awoken by neighboring beeps , then we can also find an MIS in \ ( \mathcal O ( \log ^3 n ) \ ) time . Third , if in addition to this wakeup assumption we allow sender-side collision detection , that is , beeping nodes can distinguish whether at least one neighboring node is beeping concurrently or not , we can find an MIS in \ ( \mathcal O ( \log ^2 n ) \ ) time . Finally , if instead we endow nodes with synchronous clocks , it is also possible to find an MIS in \ ( \mathcal O ( \log ^2 n ) \ ) time .
2K_dev_1186	Industry investment and research interest in edge computing , in which computing and storage nodes are placed at the Internet 's edge in close proximity to mobile devices or sensors , have grown dramatically in recent years . This emerging technology promises to deliver highly responsive cloud services for mobile computing , scalability and privacy-policy enforcement for the Internet of Things , and the ability to mask transient cloud outages . The web extra at www.youtube.com/playlist ? list=PLmrZVvFtthdP3fwHPy_4d61oDvQY_RBgS includes a five-video playlist demonstrating proof-of-concept implementations for three tasks : assembling 2D Lego models , freehand sketching , and playing Ping-Pong .
2K_dev_1187	This paper studies attackers with control objectives and explicit detection constraints against cyber-physical systems . The cyber-physical system is equipped with a Kalman filter and an attack detector that uses the innovations process of the Kalman filter . The attacker performs an integrity attack on the actuators and sensors of the system with the aim of moving the system to a target state under the constraint that the probability of him or her being detected is equal to the false alarm probability of the attack detector . We formulate and solve a constrained optimization problem that gives the optimal sequence of attacks and demonstrate our attack strategy in a numerical example .
2K_dev_1188	Having a shared and accurate sense of time is critical to distributed Cyber-Physical Systems ( CPS ) and the Internet of Things ( IoT ) . Thanks to decades of research in clock technologies and synchronization protocols , it is now possible to measure and synchronize time across distributed systems with unprecedented accuracy . However , applications have not benefited to the same extent due to limitations of the system services that help manage time , and hardware-OS and OS-application interfaces through which timing information flows to the application . Due to the importance of time awareness in a broad range of emerging applications , running on commodity platforms and operating systems , it is imperative to rethink how time is handled across the system stack . We advocate the adoption of a holistic notion of Quality of Time ( QoT ) that captures metrics such as resolution , accuracy , and stability . Building on this notion we propose an architecture in which the local perception of time is a controllable operating system primitive with observable uncertainty , and where time synchronization balances applications ' timing demands with system resources such as energy and bandwidth . Our architecture features an expressive application programming interface that is centered around the abstraction of a timeline a virtual temporal coordinate frame that is defined by an application to provide its components with a shared sense of time , with a desired accuracy and resolution . The timeline abstraction enables developers to easily write applications whose activities are choreographed across time and space . Leveraging open source hardware and software components , we have implemented an initial Linux realization of the proposed timeline-driven QoT stack on a standard embedded computing platform . Results from its evaluation are also presented .
2K_dev_1189	Topic models represent latent topics as probability distributions over words which can be hard to interpret due to the lack of grounded semantics . In this paper , we propose a structured topic representation based on an entity taxonomy from a knowledge base . A probabilistic model is developed to infer both hidden topics and entities from text corpora . Each topic is equipped with a random walk over the entity hierarchy to extract semantically grounded and coherent themes . Accurate entity modeling is achieved by leveraging rich textual features from the knowledge base . Experiments show significant superiority of our approach in topic perplexity and key entity identification , indicating potentials of the grounded modeling for semantic extraction and language understanding applications .
2K_dev_1190	Formative assessments allow learners to quickly identify knowledge gaps . In traditional educational settings , expert instructors can create assessments , but in informal learning environment , it is difficult for novice learners to self assess because they do n't know what they do n't know . This paper introduces Questimator , an automated system that generates multiple-choice assessment questions for any topic contained within Wikipedia . Given a topic , Questimator traverses the Wikipedia graph to find and rank related topics , and uses article text to form questions , answers and distractor options . In a study with 833 participants from Mechanical Turk , we found that participants ' scores on Questimator-generated quizzes correlated well with their scores on existing online quizzes on topics ranging from philosophy to economics . Also Questimator generates questions with comparable discriminatory power as existing online quizzes . Our results suggest Questimator may be useful for assessing learning in topics for which there is not an existing quiz .
2K_dev_1191	Concurrent C0 is an imperative programming language in the C family with session-typed message-passing concurrency . The previously proposed semantics implements asynchronous ( non-blocking ) output ; we extend it here with non-blocking input . A key idea is to postpone message reception as much as possible by interpreting receive commands as a request for a message . We implemented our ideas as a translation from a blocking intermediate language to a non-blocking language . Finally , we evaluated our techniques with several benchmark programs and show the results obtained . While the abstract measure of span always decreases ( or remains unchanged ) , only a few of the examples reap a practical benefit .
2K_dev_1192	We describe Concurrent C0 , a type-safe C-like language with contracts and session-typed communication over channels . Concurrent C0 supports an operation called forwarding which allows channels to be combined in a well-defined way . The language 's type system enables elegant expression of session types and message-passing concurrent programs . We provide a Go-based implementation with language based optimizations that outperforms traditional message passing techniques .
2K_dev_1193	Matrix-parametrized models ( MPMs ) are widely used in machine learning ( ML ) applications . In large-scale ML problems , the parameter matrix of a MPM can grow at an unexpected rate , resulting in high communication and parameter synchronization costs . To address this issue , we offer two contributions : first , we develop a computation model for a large family of MPMs , which share the following property : the parameter update computed on each data sample is a rank-1 matrix , i.e . the outer product of two `` sufficient factors '' ( SFs ) . Second , we implement a decentralized , peer-to-peer system , Sufficient Factor Broadcasting ( SFB ) , which broadcasts the SFs among worker machines , and reconstructs the update matrices locally at each worker . SFB takes advantage of small rank-1 matrix updates and efficient partial broadcasting strategies to dramatically improve communication efficiency . We propose a graph optimization based partial broadcasting scheme , which minimizes the delay of information dissemination under the constraint that each machine only communicates with a subset rather than all of machines . Furthermore , we provide theoretical analysis to show that SFB guarantees convergence of algorithms ( under full broadcasting ) without requiring a centralized synchronization mechanism . Experiments corroborate SFB 's efficiency on four MPMs .
2K_dev_1194	We explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational Bayes ( SGVB ) with global model parameters . Regular SGVB estimators rely on sampling of parameters once per minibatch of data , and have variance that is constant w.r.t . the minibatch size . The efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch . Such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size , generally leading to much faster convergence.We find an important connection with regularization by dropout : the original Gaussian dropout objective corresponds to SGVB with local noise , a scale-invariant prior and proportionally fixed posterior variance . Our method allows inference of more flexibly parameterized posteriors ; specifically , we propose \emph { variational dropout } , a generalization of Gaussian dropout , but with a more flexibly parameterized posterior , often leading to better generalization . The method is demonstrated through several experiments .
2K_dev_1195	How should one aggregate ordinal preferences expressed by voters into a measurably superior social choice ? A well-established approach -- which we refer to as implicit utilitarian voting -- assumes that voters have latent utility functions that induce the reported rankings , and seeks voting rules that approximately maximize utilitarian social welfare . We extend this approach to the design of rules that select a subset of alternatives . We derive analytical bounds on the performance of optimal ( deterministic as well as randomized ) rules in terms of two measures , distortion and regret . Empirical results show that regret-based rules are more compelling than distortion-based rules , leading us to focus on developing a scalable implementation for the optimal ( deterministic ) regret-based rule . Our methods underlie the design and implementation of an upcoming social choice website .
2K_dev_1196	This paper considers the definition of the graph Fourier transform ( GFT ) and of the spectral decomposition of graph signals . Current literature does not address the lack of unicity of the GFT . The GFT is the mapping from the signal set into its representation by a direct sum of irreducible shift invariant subspaces : 1 ) this decomposition may not be unique ; and 2 ) there is freedom in the choice of basis for each component subspace . These issues are particularly relevant when the graph shift has repeated eigenvalues as is the case in many real-world applications ; by ignoring them , there is no way of knowing if different researchers are using the same definition of the GFT and whether their results are comparable or not . The paper presents how to resolve the above degrees of freedom . We develop a quasi -coordinate free definition of the GFT and graph spectral decomposition of graph signals that we implement through oblique spectral projectors . We present properties of the GFT and of the spectral projectors and discuss a generalized Parseval 's inequality . An illustrative example for a large real-world urban traffic dataset is provided .
2K_dev_1197	Optical music recognition ( OMR ) is the task of recognizing images of musical scores . In this paper , improved algorithms for the first steps of optical music recognition were developed , which facilitated bulk annotation of scanned scores for use in an interactive score display system . Creating an initial annotation by OMR and verifying by hand substantially reduced the manual effort required to process scanned scores to be used in a live performance setting .
2K_dev_1198	Computer music systems can interact with humans at different levels , including scores , phrases , notes , beats , and gestures . However , most current systems lack basic musicianship skills . As a consequence , the results of human-computer interaction are often far less musical than the interaction between human musicians . In this paper , we explore the possibility of learning some basic music performance skills from rehearsal data . In particular , we consider the piano duet scenario where two musicians expressively interact with each other . Our work extends previous automatic accompaniment systems . We have built an artificial pianist that can automatically improve its ability to sense and coordinate with a human pianist , learning from rehearsal experience . We describe different machine learning algorithms to learn musicianship for duet interaction , explore the properties of the learned models , such as dominant features , limits of validity , and minimal training size , and claim that a more human-like interaction is achieved .
2K_dev_1199	Processes such as disease propagation and information diffusion often spread over some latent network structure which must be learned from observation . Given a set of unlabeled training examples representing occurrences of an event type of interest ( e.g. , a disease outbreak ) , our goal is to learn a graph structure that can be used to accurately detect future events of that type . Motivated by new theoretical results on the consistency of constrained and unconstrained subset scans , we propose a novel framework for learning graph structure from unlabeled data by comparing the most anomalous subsets detected with and without the graph constraints . Our framework uses the mean normalized log-likelihood ratio score to measure the quality of a graph structure , and efficiently searches for the highest-scoring graph structure . Using simulated disease outbreaks injected into real-world Emergency Department data from Allegheny County , we show that our method learns a structure similar to the true underlying graph , but enables faster and more accurate detection .
2K_dev_1200	We propose an inexact method for the graph Fourier transform of a graph signal , as defined by the signal decomposition over the Jordan subspaces of the graph adjacency matrix . This method projects the signal over the generalized eigenspaces of the adjacency matrix , which accelerates the transform computation over large , sparse , and directed adjacency matrices . The trade-off between execution time and fidelity to the original graph structure is discussed . In addition , properties such as a generalized Parseval 's identity and total variation ordering of the generalized eigenspaces are discussed . The method is applied to 2010-2013 NYC taxi trip data to identify traffic hotspots on the Manhattan grid . Our results show that identical highly expressed geolocations can be identified with the inexact method and the method based on eigenvector projections , while reducing computation time by a factor of 26,000 and reducing energy dispersal among the spectral components corresponding to the multiple zero eigenvalue .
2K_dev_1201	Design of filters for graph signal processing benefits from knowledge of the spectral decomposition of matrices that encode graphs , such as the adjacency matrix and the Laplacian matrix , used to define the shift operator . For shift matrices with real eigenvalues , which arise for symmetric graphs , the empirical spectral distribution captures the eigenvalue locations . Under realistic circumstances , stochastic influences often affect the network structure and , consequently , the shift matrix empirical spectral distribution . Nevertheless , deterministic functions may often be found to approximate the asymptotic behavior of empirical spectral distributions of random matrices . This paper uses stochastic canonical equation methods developed by Girko to derive such deterministic equivalent distributions for the empirical spectral distributions of random graphs formed by structured , non-uniform percolation of a D-dimensional lattice supergraph . Included simulations demonstrate the results for sample parameters .
2K_dev_1202	Ensuring the safety of fully autonomous vehicles requires a multi-disciplinary approach across all the levels of functional hierarchy , from hardware fault tolerance , to resilient machine learning , to cooperating with humans driving conventional vehicles , to validating systems for operation in highly unstructured environments , to appropriate regulatory approaches . Significant open technical challenges include validating inductive learning in the face of novel environmental inputs and achieving the very high levels of dependability required for full-scale fleet deployment . However , the biggest challenge may be in creating an end-to-end design and deployment process that integrates the safety concerns of a myriad of technical specialties into a unified approach .
2K_dev_1203	Robust principal component analysis PCA is one of the most important dimension-reduction techniques for handling high-dimensional data with outliers . However , most of the existing robust PCA presupposes that the mean of the data is zero and incorrectly utilizes the average of data as the optimal mean of robust PCA . In fact , this assumption holds only for the squared -norm-based traditional PCA . In this letter , we equivalently reformulate the objective of conventional PCA and learn the optimal projection directions by maximizing the sum of projected difference between each pair of instances based on -norm . The proposed method is robust to outliers and also invariant to rotation . More important , the reformulated objective not only automatically avoids the calculation of optimal mean and makes the assumption of centered data unnecessary , but also theoretically connects to the minimization of reconstruction error . To solve the proposed nonsmooth problem , we exploit an efficient optimization algorithm to soften the contributions from outliers by reweighting each data point iteratively . We theoretically analyze the convergence and computational complexity of the proposed algorithm . Extensive experimental results on several benchmark data sets illustrate the effectiveness and superiority of the proposed method .
2K_dev_1204	Biological adaptation is a powerful mechanism that makes many disorders hard to combat . In this paper we study steering such adaptation through sequential planning . We propose a general approach where we leverage Monte Carlo tree search to compute a treatment plan , and the biological entity is modeled by a black-box simulator that the planner calls during planning . We show that the framework can be used to steer a biological entity modeled via a complex signaling pathway network that has numerous feedback loops that operate at different rates and have hard-to-understand aggregate behavior . We apply the framework to steering the adaptation of a patient 's immune system . In particular , we apply it to a leading T cell simulator ( available in the biological modeling package BioNetGen ) . We run experiments with two alternate goals : developing regulatory T cells or developing effector T cells . The former is important for preventing autoimmune diseases while the latter is associated with better survival rates in cancer patients . We are especially interested in the effect of sequential plans , an approach that has not been explored extensively in the biological literature . We show that for the development of regulatory cells , sequential plans yield significantly higher utility than the best static therapy . In contrast , for developing effector cells , we find that ( at least for the given simulator , objective function , action possibilities , and measurement possibilities ) single-step plans suffice for optimal treatment .
2K_dev_1205	As publishers gather more information about their users , they can use that information to enable advertisers to create increasingly targeted campaigns . This enables better usage of advertising inventory . However , it also dramatically increases the complexity that the publisher faces when optimizing campaign admission decisions and inventory allocation to campaigns . We develop an optimal anytime algorithm for abstracting fine-grained audience segments into coarser abstract segments that are not too numerous for use in such optimization . Compared to the segment abstraction algorithm by Walsh et al . [ 2010 ] for the same problem , it yields two orders of magnitude improvement in run time and significant improvement in abstraction quality . These benefits hold both for guaranteed and non-guaranteed campaigns . The performance stems from three improvements : 1 ) a quadratic-time ( as opposed to doubly exponential or heuristic ) algorithm for finding an optimal split of an abstract segment , 2 ) a better scoring function for evaluating splits , and 3 ) splitting time lossily like any other targeting attribute ( instead of losslessly segmenting time first ) .
2K_dev_1206	Learning detectors that can recognize concepts , such as people actions , objects , etc. , in video content is an interesting but challenging problem . In this paper , we study the problem of automatically learning detectors from the big video data on the web without any additional manual annotations . The contextual information available on the web provides noisy labels to the video content . To leverage the noisy web labels , we propose a novel method called WEbly-Labeled Learning ( WELL ) . It is established on two theories called curriculum learning and self-paced learning and exhibits useful properties that can be theoretically verified . We provide compelling insights on the latent non-convex robust loss that is being minimized on the noisy data . In addition , we propose two novel techniques that not only enable WELL to be applied to big data but also lead to more accurate results . The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks , including the largest multimedia dataset and the largest manually-labeled video set . Experimental results show that WELL significantly outperforms the state-of-the-art methods . To the best of our knowledge , WELL achieves by far the best reported performance on these two webly-labeled big video datasets .
2K_dev_1207	State-of-the-art applications of Stackelberg security games -- including wildlife protection -- offer a wealth of data , which can be used to learn the behavior of the adversary . But existing approaches either make strong assumptions about the structure of the data , or gather new data through online algorithms that are likely to play severely suboptimal strategies . We develop a new approach to learning the parameters of the behavioral model of a bounded rational attacker ( thereby pinpointing a near optimal strategy ) , by observing how the attacker responds to only three defender strategies . We also validate our approach using experiments on real and synthetic data .
2K_dev_1208	Robust principal component analysis ( PCA ) is one of the most important dimension reduction techniques to handle high-dimensional data with outliers . However , the existing robust PCA presupposes that the mean of the data is zero and incorrectly utilizes the Euclidean distance based optimal mean for robust PCA with l1-norm . Some studies consider this issue and integrate the estimation of the optimal mean into the dimension reduction objective , which leads to expensive computation . In this paper , we equivalently reformulate the maximization of variances for robust PCA , such that the optimal projection directions are learned by maximizing the sum of the projected difference between each pair of instances , rather than the difference between each instance and the mean of the data . Based on this reformulation , we propose a novel robust PCA to automatically avoid the calculation of the optimal mean based on l1-norm distance . This strategy also makes the assumption of centered data unnecessary . Additionally , we intuitively extend the proposed robust PCA to its 2D version for image recognition . Efficient non-greedy algorithms are exploited to solve the proposed robust PCA and 2D robust PCA with fast convergence and low computational complexity . Some experimental results on benchmark data sets demonstrate the effectiveness and superiority of the proposed approaches on image reconstruction and recognition .
2K_dev_1209	Imperfect-information games , where players have private information , pose a unique challenge in artificial intelligence . In recent years , Heads-Up No-Limit Texas Hold'em poker , a popular version of poker , has emerged as the primary benchmark for evaluating game-solving algorithms for imperfect-information games . We demonstrate a winning agent from the 2016 Annual Computer Poker Competition , Baby Tartanian8 .
2K_dev_1210	In this paper , we give a comprehensive review of a num- ber of practical problems associated with the use of static priority scheduling . We first present a new approach to stabilize the rate -monotonic algorithm in the presence of Abstract In this paper , we give a comprehensive review of a num ber of practical problems associated with the use of static priority scheduling . We first present a new approach to stabilize the rate-monotonic algorithm in the presence of transient processor overloads . We also present a new class of algorithms to handle aperiodic tasks which im prove the response times to aperiodic tasks while guaranteeing the deadlines of periodic tasks . We then study the problem of integrated processor and data I/O scheduling . Finally we review the problem of scheduling of messages over a bus with insufficient priority levels but with multiple buffers .
2K_dev_1211	We investigate the problem of representing an entire video using CNN features for human action recognition . End-to-end learning of CNN/RNNs is currently not possible for whole videos due to GPU memory limitations and so a common practice is to use sampled frames as inputs along with the video labels as supervision . However , the global video labels might not be suitable for all of the temporally local samples as the videos often contain content besides the action of interest . We therefore propose to instead treat the deep networks trained on local inputs as local feature extractors . The local features are then aggregated to form global features which are used to assign video-level labels through a second classification stage . We investigate a number of design choices for this local feature approach . Experimental results on the HMDB51 and UCF101 datasets show that a simple maximum pooling on the sparsely sampled local features leads to significant performance improvement .
2K_dev_1212	Hybrid systems verification is quite important for developing correct controllers for physical systems , but is also challenging . Verification engineers , thus , need to be empowered with ways of guiding hybrid systems verification while receiving as much help from automation as possible . Due to undecidability , verification tools need sufficient means for intervening during the verification and need to allow verification engineers to provide system design insights . This paper presents the design ideas behind the user interface for the hybrid systems theorem prover KeYmaera X . We discuss how they make it easier to prove hybrid systems as well as help learn how to conduct proofs in the first place . Unsurprisingly , the most difficult user interface challenges come from the desire to integrate automation and human guidance . We also share thoughts how the success of such a user interface design could be evaluated and anecdotal observations about it .
2K_dev_1213	In human-robot teams , humans often start with an inaccurate model of the robot capabilities . As they interact with the robot , they infer the robot 's capabilities and partially adapt to the robot , i.e. , they might change their actions based on the observed outcomes and the robot 's actions , without replicating the robot 's policy . We present a game-theoretic model of human partial adaptation to the robot , where the human responds to the robot 's actions by maximizing a reward function that changes stochastically over time , capturing the evolution of their expectations of the robot 's capabilities . The robot can then use this model to decide optimally between taking actions that reveal its capabilities to the human and taking the best action given the information that the human currently has . We prove that under certain observability assumptions , the optimal policy can be computed efficiently . We demonstrate through a human subject experiment that the proposed model significantly improves human-robot team performance , compared to policies that assume complete adaptation of the human to the robot .
2K_dev_1214	Biological systems are increasingly being studied by high throughput profiling of molecular data over time . Determining the set of time points to sample in studies that profile several different types of molecular data is still challenging . Here we present the Time Point Selection ( TPS ) method that solves this combinatorial problem in a principled and practical way . TPS utilizes expression data from a small set of genes sampled at a high rate . As we show by applying TPS to study mouse lung development , the points selected by TPS can be used to reconstruct an accurate representation for the expression values of the non selected points . Further , even though the selection is only based on gene expression , these points are also appropriate for representing a much larger set of protein , miRNA and DNA methylation changes over time . TPS can thus serve as a key design strategy for high throughput time series experiments . Supporting Website : www.sb.cs.cmu.edu/TPS
2K_dev_1215	Identifying a masked suspect is one of the toughest challenges in biometrics that exist . This is an important problem faced in many law-enforcement applications on almost a daily basis . In such situations , investigators often only have access to the periocular region of a suspect 's face and , unfortunately , conventional commercial matchers are unable to process these images in such a way that the suspect can be identified . Herein , a practical method to hallucinate a full frontal face given only a periocular region of a face is presented . This approach reconstructs the entire frontal face based on an image of an individual 's periocular region . By using an approach based on a modified sparsifying dictionary learning algorithm , faces can be effectively reconstructed more accurately than with conventional methods . Further , various methods presented herein are open set , and thus can reconstruct faces even if the algorithms are not specifically trained using those faces .
2K_dev_1216	Complex event detection has been progressively researched in recent years for the broad interest of video indexing and retrieval . To fulfill the purpose of event detection , one needs to train a classifier using both positive and negative examples . Current classifier training treats the negative videos as equally negative . However , we notice that many negative videos resemble the positive videos in different degrees . Intuitively , we may capture more informative cues from the negative videos if we assign them fine-grained labels , thus benefiting the classifier learning . Aiming for this , we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event . Based on these decisive attributes , we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation . The resulting fine-grained labels may be not optimal to capture the discriminative cues from the negative videos . Hence , we propose to jointly optimize the fine-grained labels with the classifier learning , which brings mutual reciprocality . Meanwhile , the labels of positive examples are supposed to remain unchanged . We thus additionally introduce a constraint for this purpose . On the other hand , the state-of-the-art deep convolutional neural network features are leveraged in our approach for event detection to further boost the performance . Extensive experiments on the challenging TRECVID MED 2014 dataset have validated the efficacy of our proposed approach .
2K_dev_1217	Generalized canonical correlation analysis ( GCCA ) aims at extracting common structure from multiple 'views ' , i.e. , high-dimensional matrices representing the same objects in different feature domains an extension of classical two-view CCA . Existing ( G ) CCA algorithms have serious scalability issues , since they involve square root factorization of the correlation matrices of the views . The memory and computational complexity associated with this step grow as a quadratic and cubic function of the problem dimension ( the number of samples / features ) , respectively . To circumvent such difficulties , we propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements , respectively . Consequently , the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100,000 while the current approaches can only handle thousands of features / samples . Our second contribution is a distributed algorithm for GCCA , which computes the canonical components of different views in parallel and thus can further reduce the runtime significantly ( by 30 % in experiments ) if multiple cores are available . Judiciously designed synthetic and real-data experiments using a multilingual dataset are employed to showcase the effectiveness of the proposed algorithms .
2K_dev_1218	Display appropriation provides a means by which mobile users can cyber-forage local display hardware to provide them with access to a high-quality output device . However , displays are of little use without applications to drive them and yet the nature of application support has been largely ignored in the field with the prevailing assumption being that applications will be cloud-based and Web-centric . In this demonstration we show a system that presents an alternative vision in which users are able to cyber-forage for both display and compute resources in their local area enabling them to execute high-performance applications that would not be possible using purely Web-centric technologies . The demonstration leverages a cohesive suite of existing systems , i.e . cloudlets , Internet Suspend/Resume ( ISR ) , Yarely and Tacita , to deliver this vision .
2K_dev_1219	We seek to extract and explore statistics that characterize New York City traffic flows based on 700 million taxi trips in the 20102013 New York City taxi data . This paper presents a two-part solution for intensive computation : space and time design considerations for estimating taxi trajectories with Dijkstra 's algorithm , and job parallelization and scheduling with HTCondor . Our contribution is to present a solution that reduces execution time from 3,000 days to less than a day with detailed analysis of the necessary design decisions .
2K_dev_1220	In social voting Web sites , how do the user actions up-votes , down-votes and comments evolve over time ? Are there relationships between votes and comments ? What is normal and what is suspicious ? These are the questions we focus on . We analyzed over 20,000 submissions corresponding to more than 100 million user interactions from three social voting Web sites : Reddit , Imgur and Digg . Our first contribution is two discoveries : ( i ) the number of comments grows as a power-law on the number of votes and ( ii ) the time between a submission creation and a user 's reaction obeys a log-logistic distribution . Based on these patterns , we propose VnC ( Vote-and-Comment ) , a parsimonious but accurate and scalable model that models the coevolution of user activities . In our experiments on real data , VnC outperformed state-of-the-art baselines on accuracy . Additionally , we illustrate VnC usefulness for forecasting and outlier detection .
2K_dev_1221	Given a heterogeneous network , with nodes of different types - e.g. , products , users and sellers from an online recommendation site like Amazon - and labels for a few nodes ( 'honest ' , 'suspicious ' , etc ) , can we find a closed formula for Belief Propagation ( BP ) , exact or approximate ? Can we say whether it will converge ? BP , traditionally an inference algorithm for graphical models , exploits so-called `` network effects '' to perform graph classification tasks when labels for a subset of nodes are provided ; and it has been successful in numerous settings like fraudulent entity detection in online retailers and classification in social networks . However , it does not have a closed-form nor does it provide convergence guarantees in general . We propose ZooBP , a method to perform fast BP on undirected heterogeneous graphs with provable convergence guarantees . ZooBP has the following advantages : ( 1 ) Generality : It works on heterogeneous graphs with multiple types of nodes and edges ; ( 2 ) Closed-form solution : ZooBP gives a closed-form solution as well as convergence guarantees ; ( 3 ) Scalability : ZooBP is linear on the graph size and is up to 600 faster than BP , running on graphs with 3.3 million edges in a few seconds . ( 4 ) Effectiveness : Applied on real data ( a Flipkart e-commerce network with users , products and sellers ) , ZooBP identifies fraudulent users with a near-perfect precision of 92.3 % over the top 300 results .
2K_dev_1222	Many theories have emerged which investigate how in- variance is generated in hierarchical networks through sim- ple schemes such as max and mean pooling . The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations . We con- jecture that hierarchically building selective invariance ( i.e . carefully choosing the range of the transformation to be in- variant to at each layer of a hierarchical network ) is im- portant for pattern recognition . We utilize a novel pooling layer called adaptive pooling to find linear pooling weights within networks . These networks with the learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks . In- terestingly , adaptive pooling can converge to mean pooling ( when initialized with random pooling weights ) , find more general linear pooling schemes or even decide not to pool at all . We illustrate the general notion of selective invari- ance through object categorization experiments on large- scale datasets such as SVHN and ILSVRC 2012 .
2K_dev_1223	How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we use them for algorithm design and applications ? A k-core is the maximal subgraph where all vertices have degree at least k. This concept has been applied to such diverse areas as hierarchical structure analysis , graph visualization , and graph clustering . Here , we explore pervasive patterns that are related to k-cores and emerging in graphs from several diverse domains . Our discoveries are as follows : ( 1 ) Mirror Pattern : coreness of vertices ( i.e. , maximum k such that each vertex belongs to the k-core ) is strongly correlated to their degree . ( 2 ) Core-Triangle Pattern : degeneracy of a graph ( i.e. , maximum k such that the k-core exists in the graph ) obeys a 3-to-1 power law with respect to the count of triangles . ( 3 ) Structured Core Pattern : degeneracy-cores are not cliques but have non-trivial structures such as core-periphery and communities . Our algorithmic contributions show the usefulness of these patterns . ( 1 ) Core-A , which measures the deviation from Mirror Pattern , successfully finds anomalies in real-world graphs complementing densest-subgraph based anomaly detection methods . ( 2 ) Core-D , a single-pass streaming algorithm based on Core-Triangle Pattern , accurately estimates the degeneracy of billion-scale graphs up to 7 faster than a recent multipass algorithm . ( 3 ) Core-S , inspired by Structured Core Pattern , identifies influential spreaders up to 17 faster than top competitors with comparable accuracy .
2K_dev_1224	Methods and apparatuses are provided for determining a pitch and yaw of an elongated interface object relative to a proximity sensitive surface . In one aspect , a proximity image is received having proximity image data from which it can be determined which areas of the proximity sensitive surface sensed the elongated interface object during a period of time . A proximity blob is identified in the proximity image and the proximity image is transformed using a plurality of different transformations to obtain a plurality of differently transformed proximity images . A plurality of features is determined for the identified blob in the transformed proximity images and the pitch of the elongated interface object relative to the proximity sensitive surface is determined based upon the determined features and a multi dimensional heuristic regression model of the proximity sensitive surface ; and a yaw is determined based upon the pitch .
2K_dev_1225	Given a set of attributed subgraphs known to be from different classes , how can we discover their differences ? There are many cases where collections of subgraphs may be contrasted against each other . For example , they may be as- signed ground truth labels ( spam/not-spam ) , or it may be desired to directly compare the biological networks of different species or compound networks of different chemicals . In this work we introduce the problem of characterizing the differences between attributed subgraphs that belong to different classes . We define this characterization problem as one of partitioning the attributes into as many groups as the number of classes , while maximizing the total attributed quality score of all the given subgraphs . We show that our attribute-to-class assignment problem is NP-hard and an optimal ( 1 -- 1/e ) -approximation algorithm exists . We also propose two different faster heuristics that are linear-time in the number of attributes and subgraphs . Unlike previous work where only attributes were taken into account for characterization , here we exploit both attributes and social ties ( i.e . graph structure ) . Through extensive experiments , we compare our proposed algorithms , show findings that agree with human intuition on datasets from Amazon co-purchases , Congressional bill sponsorships and DBLP co-authorships . We also show that our approach of characterizing subgraphs is better suited for sense-making than discriminating classification approaches .
2K_dev_1226	The ubiquity of mobile devices and cloud services has led to an unprecedented growth of online personal photo and video collections . Due to the scarcity of personal media search log data , research to date has mainly focused on searching images and videos on the web . However , in order to manage the exploding amount of personal photos and videos , we raise a fundamental question : what are the differences and similarities when users search their own photos versus the photos on the web ? To the best of our knowledge , this paper is the first to study personal media search using large-scale real-world search logs . We analyze different types of search sessions mined from Flickr search logs and discover a number of interesting characteristics of personal media search in terms of information needs and click behaviors . The insightful observations will not only be instrumental in guiding future personal media search methods , but also benefit related tasks such as personal photo browsing and recommendation . Our findings suggest there is a significant gap between personal queries and automatically detected concepts , which is responsible for the low accuracy of many personal media search queries . To bridge the gap , we propose the deep query understanding model to learn a mapping from the personal queries to the concepts in the clicked photos . Experimental results verify the efficacy of the proposed method in improving personal media search , where the proposed method consistently outperforms baseline methods .
2K_dev_1227	Ultrasonic Lamb waves are a widely used research tool for nondestructive structural health monitoring . They travel long distances with little attenuation , enabling the interrogation of large areas . To analyze Lamb wave propagation data , it is often important to know precisely how they propagate . Yet , since wave propagation is influenced by many factors , including material properties , temperature , and other varying conditions , acquiring this knowledge is a significant challenge . In prior work , this information has been recovered by reconstructing Lamb wave dispersion curves with sparse wavenumber analysis . While effective , sparse wavenumber analysis requires a large number of sensors and is sensitive to noise in the data . In this paper , it extended and significantly improved by constraining the reconstructed dispersion curves to be continuous across frequencies . To enforce this constraint , it is included explicitly in a sparse optimization formulation , and by including in the reconstruction an edge detecti ...
2K_dev_1228	AbstractThe Sixth International BrainComputer Interface ( BCI ) Meeting was held 30 May3 June 2016 at the Asilomar Conference Grounds , Pacific Grove , California , USA . The conference included 28 workshops covering topics in BCI and brainmachine interface research . Topics included BCI for specific populations or applications , advancing BCI research through use of specific signals or technological advances , and translational and commercial issues to bring both implanted and non-invasive BCIs to market . BCI research is growing and expanding in the breadth of its applications , the depth of knowledge it can produce , and the practical benefit it can provide both for those with physical impairments and the general public . Here we provide summaries of each workshop , illustrating the breadth and depth of BCI research and highlighting important issues and calls for action to support future research and development .
2K_dev_1229	With the ubiquitous development of mobile technologies , many cities today have installed mobile-enabled bike sharing systems - both publicly and privately owned - in an effort to nudge dwellers towards a more sustainable mode of transportation . However , there is little evidence - apart from anecdote stories - for the success of these systems . In this work we are focusing on analyzing the impact of a shared bike system on the parking demand . The latter can be thought of as a lower bound for the car trips generated towards a specific area and has implications towards potential substitution effects between driving and biking . In particular , we use data from Healthy Ride , the newly installed shared bike system in the city of Pittsburgh , combined with data we obtained from the Pittsburgh Parking Authority , and using the difference-in-differences framework we quantify the impact of the bike stations on the parking demand around them . Our findings provide evidence that even when controlling for the lost parking space ( used to build the parking stations ) the parking demand in the nearby areas was reduced by approximately 2 % . This can have significant implications that shared bike systems can shift transportation modes , which consequently can have rippling effects for the economy and environment . In particular , our follow-up analyses indicate that the new bike share system could lead to a monthly reduction of 0.82 metric tones CO2 emissions per square mile , or approximately 4,381 metric tones of CO2 in the metro area of Pittsburgh !
2K_dev_1230	Multi-aspect data appear frequently in many web-related applications . For example , product reviews are quadruplets of ( user , product , keyword , timestamp ) . How can we analyze such web-scale multi-aspect data ? Can we analyze them on an off-the-shelf workstation with limited amount of memory ? Tucker decomposition has been widely used for discovering patterns in relationships among entities in multi-aspect data , naturally expressed as high-order tensors . However , existing algorithms for Tucker decomposition have limited scalability , and especially , fail to decompose high-order tensors since they explicitly materialize intermediate data , whose size rapidly grows as the order increases ( 4 ) . We call this problem M-Bottleneck ( `` Materialization Bottleneck '' ) . To avoid M-Bottleneck , we propose S-HOT , a scalable high-order tucker decomposition method that employs the on-the-fly computation to minimize the materialized intermediate data . Moreover , S-HOT is designed for handling disk-resident tensors , too large to fit in memory , without loading them all in memory at once . We provide theoretical analysis on the amount of memory space and the number of scans of data required by S-HOT . In our experiments , S-HOT showed better scalability not only with the order but also with the dimensionality and the rank than baseline methods . In particular , S-HOT decomposed tensors 1000 larger than baseline methods in terms dimensionality . S- HOT also successfully analyzed real-world tensors that are both large-scale and high-order on an off-the-shelf workstation with limited amount of memory , while baseline methods failed . The source code of S-HOT is publicly available at http : //dm.postech.ac.kr/shot to encourage reproducibility .
2K_dev_1231	How can we detect fraudulent lockstep behavior in large-scale multi-aspect data ( i.e. , tensors ) ? Can we detect it when data are too large to fit in memory or even on a disk ? Past studies have shown that dense blocks in real-world tensors ( e.g. , social media , Wikipedia , TCP dumps , etc . ) signal anomalous or fraudulent behavior such as retweet boosting , bot activities , and network attacks . Thus , various approaches , including tensor decomposition and search , have been used for rapid and accurate dense-block detection in tensors . However , all such methods have low accuracy , or assume that tensors are small enough to fit in main memory , which is not true in many real-world applications such as social media and web . To overcome these limitations , we propose D-Cube , a disk-based dense-block detection method , which also can be run in a distributed manner across multiple machines . Compared with state-of-the-art methods , D-Cube is ( 1 ) Memory Efficient : requires up to 1,600 times less memory and handles 1,000 times larger data ( 2.6TB ) , ( 2 ) Fast : up to 5 times faster due to its near-linear scalability with all aspects of data , ( 3 ) Provably Accurate : gives a guarantee on the densities of the blocks it finds , and ( 4 ) Effective : successfully spotted network attacks from TCP dumps and synchronized behavior in rating data with the highest accuracy .
2K_dev_1232	We study the unsupervised learning of CNNs for optical flow estimation using proxy ground truth data . Supervised CNNs , due to their immense learning capacity , have shown superior performance on a range of computer vision problems including optical flow prediction . They however require the ground truth flow which is usually not accessible except on limited synthetic data . Without the guidance of ground truth optical flow , unsupervised CNNs often perform worse as they are naturally ill-conditioned . We therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used to guide the CNN learning . The models are further refined in an unsupervised fashion using an image reconstruction loss . Our guided learning approach is competitive with or superior to state-of-the-art approaches on three standard benchmark datasets yet is completely unsupervised and can run in real time .
2K_dev_1233	a b s t r a c t We consider the M/G/k/staggered-setup , where idle servers are turned off to save cost , necessitating a setup time for turning a server back on ; however , at most one server may be in setup mode at any time . We show that , for exponentially distributed setup times , the response time of an M/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/k and the setup time , where the approximation is nearly exact . This generalizes a prior decomposition result for an M/M/k/staggeredsetup .
2K_dev_1234	This paper studies the resilient distributed estimation of an unknown vector parameter belonging to a compact set . A group of agents makes linear measurements of the unknown parameter . The agent measurements are locally unobservable , and the agents exchange information over a communication network in order to compute an estimate . A subset of the agents is adversarial and exchanges false information in order to prevent the remaining , normally-behaving agents from correctly estimating the parameter . We present and analyze a Flag Raising Distributed Estimation ( FRDE ) algorithm that allows the normally-behaving agents to perform parameter estimation and adversary detection . The FRDE algorithm is a consensus+innovations type estimator in which agents combine estimates of neighboring agents ( consensus ) with local sensing information ( innovations ) . Under the FRDE algorithm , global observability for connected normally-behaving agents is a necessary and sufficient condition to either correctly estimate the parameter or correctly detect the presence of an adversary . If FRDE detects an adversary , we show how existing methods for attack identification in cyber-physical systems can be used to identify the adversarial agents . Finally , we provide numerical examples of the performance of the FRDE algorithm .
2K_dev_1235	Unmanned aerial vehicles ( UAVs ) recently enabled a myriad of new applications spanning domains from personal entertainment to surveillance and monitoring . In this paper , we focus on using several small UAVs collaboratively to provide extended reach to an online video monitoring system for inspection of industrial installations . We make use of 802.11 radios on low-cost commercial-off-the-shelf UAVs , set up a time-division multiple access overlay protocol to avoid mutual interference , and enable high channel utilization in multihop networks . In particular , we provide a model for the quality of the UAV-to-UAV link , in terms of packet delivery ratio as a function of distance , packet size , and orientation , based on an extensive measurement campaign . We show that this platform is not omnidirectional in the horizontal plane and that UAV-to-UAV communication ceases around 75m . Concerning the operation in a multihop mode to allow extending the network , the paper derives the optimal number of hops that maximize the end-to-end throughput , as well as the corresponding hop lengths . We validate our mathematical model with extensive experimental measurements transmitting payloads up to 200m ( over 802.11g at 54MBps ) .
2K_dev_1236	Summary Successful application of two-photon imaging withgenetic tools in awake macaque monkeys will enable fundamental advances in our understanding of higher cognitive function at the level of molecular and neuronal circuits . Here we report techniques for long-term two-photon imaging in awake macaque monkeys . Using genetically encoded indicators including GCaMP5 and GCaMP6s delivered by AAV2/1 into the visual cortex , we demonstrate that high-quality two-photon imaging of large neuronal populations can be achieved and maintained in awake monkeys for months . Simultaneous intracellular recording and two-photon calcium imaging confirm that fluorescence activity is linearly proportional to neuronal spiking activity across a wide range of firing rates ( 10Hz to 150Hz ) . By providing two-photon imaging access to cortical neuronal populations at single-cell or single dendritic spine resolution in awake monkeys , the techniques reported can help bridge the use of modern genetic and molecular tools and the study of higher cognitive function .
2K_dev_1237	How do people interact with their Facebook wall ? At a high level , this question captures the essence of our work . While most prior efforts focus on Twitter , the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study . In this work , we model Facebook user behavior : we analyze the wall activities of users focusing on identifying common patterns and surprising phenomena . We conduct an extensive study of roughly 7k users over 3 years during 4-month intervals each year . We propose PowerWall , a lesser known heavy-tailed distribution to fit our data . Our key results can be summarized in the following points . First , we find that many wall activities , including number of posts , number of likes , number of posts of type photo , can be described by the PowerWall distribution . What is more surprising is that most of these distributions have similar slope , with a value close to 1 ! Second , we show how our patterns and metrics can help us spot surprising behaviors and anomalies . For example , we find a user posting every two days , exactly the same count of posts ; another user posting at midnight , with no other activity before or after . Our work provides a solid step toward a systematic and quantitative wall-centric profiling of Facebook user activity .
2K_dev_1238	To infer the histories of population admixture , one important challenge with methods based on the admixture linkage disequilibrium ( ALD ) is to get rid of the effect of source LD ( SLD ) which is directly inherited from source populations . In previous methods , only the decay curve of weighted LD between pairs of sites whose genetic distance were larger than a certain starting distance was fitted by single or multiple exponential functions , for the inference of recent single- or multiple-wave of admixture . However , the effect of SLD has not been well defined and no tool has been developed to estimate the effect of SLD on weighted LD decay . In this study , we defined the SLD in the formularized weighted LD statistic under the two-way admixture model , and proposed polynomial spectrum ( p-spectrum ) to study the weighted SLD and weighted LD . We also found reference populations could be used to reduce the SLD in weighted LD statistic . We further developed a method , iMAAPs , to infer Multiple-wave Admixture by fitting ALD using Polynomial spectrum . We evaluated the performance of iMAAPs under various admixture models in simulated data and applied iMAAPs into analysis of genome-wide single nucleotide polymorphism data from the Human Genome Diversity Project ( HGDP ) and the HapMap Project . We showed that iMAAPs is a considerable improvement over other current methods and further facilitates the inference of the histories of complex population admixtures .
2K_dev_1239	We formulate a set of time-varying stochastic networked dynamical systems to model the evolution of tie strength among interacting agents . The dynamics of the strength of connections abide by local laws of reinforcement and penalization due to interactions among the agents . The proposed stochastic dynamical systems exhibit a strong-attractor as a certain subset of the set of binary matrices . Moreover , the family of models adapts well to capture the phenomenon of emergence and downfall of leaders in social networks as it will be illustrated via numerical simulations .
2K_dev_1240	Abstract Rapid advances in high-throughput sequencing and a growing realization of the importance of evolutionary theory to cancer genomics have led to a proliferation of phylogenetic studies of tumour progression . These studies have yielded not only new insights but also a plethora of experimental approaches , sometimes reaching conflicting or poorly supported conclusions . Here , we consider this body of work in light of the key computational principles underpinning phylogenetic inference , with the goal of providing practical guidance on the design and analysis of scientifically rigorous tumour phylogeny studies . We survey the range of methods and tools available to the researcher , their key applications , and the various unsolved problems , closing with a perspective on the prospects and broader implications of this field .
2K_dev_1241	Sparse iterative methods , in particular first-order methods , are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games . The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on . We investigate the acceleration of first-order methods for solving extensive-form games through better design of the dilated entropy function -- -a class of distance-generating functions related to the domains associated with the extensive-form games . By introducing a new weighting scheme for the dilated entropy function , we develop the first distance-generating function for the strategy spaces of sequential games that only a logarithmic dependence on the branching factor of the player . This result improves the convergence rate of several first-order methods by a factor of ( b dd ) , where b is the branching factor of the player , and d is the depth of the game tree . Thus far , counterfactual regret minimization methods have been faster in practice , and more popular , than first-order methods despite their theoretically inferior convergence rates . Using our new weighting scheme and practical tuning we show that , for the first time , the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm , CFRP , in practice .
2K_dev_1242	tuning any thresholds . We then conduct an extensive empirical study to validate the proposed methods on both simulated and real data including the analysis of a large volume of spatio-temporal Manhattan urban data . The analysis validates the effectiveness of the approach and suggests that graph signal processing tools may aid in urban planning and traffic forecasting . Motivated by the need to extract meaning from large amounts of complex structured data , we consider three critical problems on graphs : localization , decomposition , and dictionary learning of piecewise-constant signals . These graph-based problems are related to many real-world applications , such as localizing stimulus in brain connectivity networks , and mining traffic events in city street networks , where the key issue is to find the supports of localized activated patterns . Counterparts of these problems in classical signal/image processing , such as impulse detection and foreground detection , have been studied over the past few decades . We use piecewise-constant graph signals to model localized patterns , where each piece indicates a localized pattern that exhibits homogeneous internal behavior and the number of pieces indicates the number of localized patterns . For such signals , we show that decomposition and dictionary learning are natural extensions of localization , the goal of which is not only to efficiently approximate graph signals , but also to accurately find supports of localized patterns . For each of the three problems , i.e. , localization , decomposition , and dictionary learning , we propose a specific graph signal model , an optimization problem , and a computationally efficient solver . The proposed solvers directly find the supports of arbitrary localized activated patterns without
2K_dev_1243	Video semantic recognition usually suffers from the curse of dimensionality and the absence of enough high-quality labeled instances , thus semisupervised feature selection gains increasing attentions for its efficiency and comprehensibility . Most of the previous methods assume that videos with close distance ( neighbors ) have similar labels and characterize the intrinsic local structure through a predetermined graph of both labeled and unlabeled data . However , besides the parameter tuning problem underlying the construction of the graph , the affinity measurement in the original feature space usually suffers from the curse of dimensionality . Additionally , the predetermined graph separates itself from the procedure of feature selection , which might lead to downgraded performance for video semantic recognition . In this paper , we exploit a novel semisupervised feature selection method from a new perspective . The primary assumption underlying our model is that the instances with similar labels should have a larger probability of being neighbors . Instead of using a predetermined similarity graph , we incorporate the exploration of the local structure into the procedure of joint feature selection so as to learn the optimal graph simultaneously . Moreover , an adaptive loss function is exploited to measure the label fitness , which significantly enhances model 's robustness to videos with a small or substantial loss . We propose an efficient alternating optimization algorithm to solve the proposed challenging problem , together with analyses on its convergence and computational complexity in theory . Finally , extensive experimental results on benchmark datasets illustrate the effectiveness and superiority of the proposed approach on video semantic recognition related tasks .
2K_dev_1244	Motivation : Cellular Electron CryoTomography ( CECT ) enables 3D visualization of cellular organization at near-native state and in sub-molecular resolution , making it a powerful tool for analyzing structures of macromolecular complexes and their spatial organizations inside single cells . However , high degree of structural complexity together with practical imaging limitations make the systematic de novo discovery of structures within cells challenging . It would likely require averaging and classifying millions of subtomograms potentially containing hundreds of highly heterogeneous structural classes . Although it is no longer difficult to acquire CECT data containing such amount of subtomograms due to advances in data acquisition automation , existing computational approaches have very limited scalability or discrimination ability , making them incapable of processing such amount of data . Results : To complement existing approaches , in this paper we propose a new approach for subdividing subtomograms into smaller but relatively homogeneous subsets . The structures in these subsets can then be separately recovered using existing computation intensive methods . Our approach is based on supervised structural feature extraction using deep learning , in combination with unsupervised clustering and reference-free classification . Our experiments show that , compared to existing unsupervised rotation invariant feature and pose-normalization based approaches , our new approach achieves significant improvements in both discrimination ability and scalability . More importantly , our new approach is able to discover new structural classes and recover structures that do not exist in training data .
2K_dev_1246	A method for locating a mobile device is disclosed . Initially , a set of modulated ultrasound signals and a set of radio signals are separately broadcast from a group of transmitters . The ultrasound signals include at least one symbol configured for pulse compression . After the receipt of a demodulated ultrasound signal from a mobile device , wherein the demodulated ultrasound signal is derived from the modulated ultrasound signals , transmitter identifier and timing information are extracted from the demodulated ultrasound signal . Timing information include , for example , the arrival time of the demodulated ultrasound signal in relation to the start time of its transmission . After the locations of the transmitters have been ascertained from the transmitter identifier information , the location of the mobile device can be determined based on the timing information and the locations of the transmitters .
2K_dev_1247	Researchers and educators have designed curricula and resources for introductory programming environments such as Scratch , App Inventor , and Kodu to foster computational thinking in K-12 . This paper is an empirical study of the effectiveness and usefulness of tiles and flashcards developed for Microsoft Kodu Game Lab to support students in learning how to program and develop games . In particular , we investigated the impact of physical manipulatives on 3rd -- 5th grade students ' ability to understand , recognize , construct , and use game programming design patterns . We found that the students who used physical manipulatives performed well in rule construction , whereas the students who engaged more with the rule editor of the programming environment had better mental simulation of the rules and understanding of the concepts .
2K_dev_1248	The main result presented in this paper ( whose proof can be found in [ 1 ] ) is that the fraction of agents ( Y N k ( t ) ) at state k X : 0 { 1 , , K } associated with an interacting particle system over an appropriate dynamical communication network converges weakly to the solution of a differential equation . The vector macroprocess ( Y N ( t ) ) 0 ( Y N 1 ( t ) , , Y N k ( t ) ) is not Markov since its evolution depends not only on its current state , but on finer real-time microscopic high-dimensional information of the system namely , the state of the N nodes X N ( t ) X N . Our result essentially states that under an appropriate dynamics of the underlying network of contacts , the macroprocess ( Y N ( t ) ) becomes asymptotically ( in N ) Markov .
2K_dev_1249	Abstract The heterogeneity-gap between different modalities brings a significant challenge to multimedia information retrieval . Some studies formalize the cross-modal retrieval tasks as a ranking problem and learn a shared multi-modal embedding space to measure the cross-modality similarity . However , previous methods often establish the shared embedding space based on linear mapping functions which might not be sophisticated enough to reveal more complicated inter-modal correspondences . Additionally , current studies assume that the rankings are of equal importance , and thus all rankings are used simultaneously , or a small number of rankings are selected randomly to train the embedding space at each iteration . Such strategies , however , always suffer from outliers as well as reduced generalization capability due to their lack of insightful understanding of procedure of human cognition . In this paper , we involve the self-paced learning theory with diversity into the cross-modal learning to rank and learn an optimal multi-modal embedding space based on non-linear mapping functions . This strategy enhances the models robustness to outliers and achieves better generalization via training the model gradually from easy rankings by diverse queries to more complex ones . An efficient alternative algorithm is exploited to solve the proposed challenging problem with fast convergence in practice . Extensive experimental results on several benchmark datasets indicate that the proposed method achieves significant improvements over the state-of-the-arts in this literature .
2K_dev_1250	We study the estimation of the latent variable Gaussian graphical model ( LVGGM ) , where the precision matrix is the superposition of a sparse matrix and a low-rank matrix . In order to speed up the estimation of the sparse plus low-rank components , we propose a sparsity constrained maximum likelihood estimator based on matrix factorization , and an efficient alternating gradient descent algorithm with hard thresholding to solve it . Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM . In addition , we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision . Experiments on both synthetic and genomic data demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory .
2K_dev_1251	Despite progress in visual perception tasks such as image classification and detection , computers still struggle to understand the interdependency of objects in the scene as a whole , e.g. , relations between objects or their attributes . Existing methods often ignore global context cues capturing the interactions among different object instances , and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships . To capture such global interdependency , we propose a deep Variation-structured Reinforcement Learning ( VRL ) framework to sequentially discover object relationships and attributes in the whole image . First , a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories , predicates , and attributes . Next , we use a variation-structured traversal over the action graph to construct a small , adaptive action set for each step based on the current state and historical actions . In particular , an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish . We then make sequential predictions using a deep RL framework , incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector . Our experiments on the Visual Relationship Detection ( VRD ) dataset and the large-scale Visual Genome dataset validate the superiority of VRL , which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types . We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes .
2K_dev_1252	Cyberphysical systems ( CPSs ) , ranging from critical infrastructures such as power plants , to modern ( semi ) autonomous vehicles , are systems that use software to control physical processes . CPSs are made up of many different computational components . Each component runs its own piece of software that implements its control algorithms , based on its model of the environment . Every component then interacts with other components through the signals and values it sends out . Collectively , these components , and the code they run , drive the complex behaviors modern society has come to expect and rely on . Due to these intricate interactions between components , managing the hundreds to millions of lines of software to ensure that the system , as a whole , performs as desired can often be unwieldy .
2K_dev_1253	This paper presents an end-to-end framework for automatically detecting and segmenting blood cells including normal red blood cells ( RBCs ) , connected RBCs , abnormal RBCs ( i.e . tear drop , burr cell , helmet , etc . ) and white blood cells ( WBCs ) . Our proposed system contains several components to solve different problems regarding RBCs and WBCs . We first design a novel blood cell color representation which is able to emphasize the RBCs and WBCs in separate channels . Template matching technique is then employed to individually detect RBCs and WBCs in our proposed representation . In order to automatically segment the RBCs and nuclei from WBCs , we develop an adaptive level set-based segmentation method which makes use of both local and global information . The detected and segmented RBCs , however , can be a single RBC , a connected RBC or an abnormal RBC . Therefore , we first separate and reconstruct RBCs from the connected RBCs by our suggested modified template matching . Shape matching by inner distance is later used to classify the abnormal RBCs from the normal RBCs . Our proposed method has been tested and evaluated on different images from ALL-IDB , 10 WebPath , 24 UPMC , 23 Flicker datasets , and the one used by Mohamed et al . 14 The precision and recall of RBCs detection are 98.43 % and 94.99 % respectively , whereas those of WBCs detection are 99.12 % and 99.12 % . The F-measure of our proposed WBCs segmentation gets up to 95.8 % .
2K_dev_1254	As smartphones and tablets have been widely adopted and mobile banking apps have come into ubiquitous use , mobile devices have increasingly become new tools that customers use for banking , payments , budgeting , and shopping . This paper examines the impact of the mobile channel on customer service demand across banking digital channels , and investigates how the use of the mobile channel influences customer financial decision-making . Our analysis is validated based on a novel large-scale dataset that contains 43 million individual transactions from 190,000 customers during April to June 2013 from a financial institution in the United States . Our findings suggest that : ( 1 ) the use of the mobile channel increases customer demand for digital services ; ( 2 ) lower ATM density and higher branch channel density in the customers vicinity is associated with higher digital service demand ; ( 3 ) the mobile phone channel serves as a complement to the PC channel , the tablet channel substitutes for the PC channel , and the mobile phone channel and the tablet channel complement one another ; ( 4 ) customers acquire more information for financial decision-making following the use of the mobile channel , and mobile phone and tablet users are less likely to incur overdraft and credit card penalty fees . Net benefit of the mobile channel to the bank is $ 0.07 USD per month per ( average ) customer . This study has implications for banks managers related to the design and management of service delivery channels , and for financial regulators related to the inclusiveness of financial system .
2K_dev_1255	Intelligent conversational assistants , such as Apple 's Siri , Microsoft 's Cortana , and Amazon 's Echo , have quickly become a part of our digital life . However , these assistants have major limitations , which prevents users from conversing with them as they would with human dialog partners . This limits our ability to observe how users really want to interact with the underlying system . To address this problem , we developed a crowd-powered conversational assistant , Chorus , and deployed it to see how users and workers would interact together when mediated by the system . Chorus sophisticatedly converses with end users over time by recruiting workers on demand , which in turn decide what might be the best response for each user sentence . Up to the first month of our deployment , 59 users have held conversations with Chorus during 320 conversational sessions . In this paper , we present an account of Chorus ' deployment , with a focus on four challenges : ( i ) identifying when conversations are over , ( ii ) malicious users and workers , ( iii ) on-demand recruiting , and ( iv ) settings in which consensus is not enough . Our observations could assist the deployment of crowd-powered conversation systems and crowd-powered systems in general .
2K_dev_1256	Voting systems typically treat all voters equally . We argue that perhaps they should not : Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones . To develop a formal framework for desirable weighting schemes , we draw on no-regret learning . Specifically , given a voting rule , we wish to design a weighting scheme such that applying the voting rule , with voters weighted by the scheme , leads to choices that are almost as good as those endorsed by the best voter in hindsight . We derive possibility and impossibility results for the existence of such weighting schemes , depending on whether the voting rule and the weighting scheme are deterministic or randomized , as well as on the social choice axioms satisfied by the voting rule .
2K_dev_1257	Understanding traffic density from large-scale web camera ( webcam ) videos is a challenging problem because such videos have low spatial and temporal resolution , high occlusion and large perspective . To deeply understand traffic density , we explore both deep learning based and optimization based methods . To avoid individual vehicle detection and tracking , both methods map the image into vehicle density map , one based on rank constrained regression and the other one based on fully convolution networks ( FCN ) . The regression based method learns different weights for different blocks in the image to increase freedom degrees of weights and embed perspective information . The FCN based method jointly estimates vehicle density map and vehicle count with a residual learning framework to perform end-to-end dense prediction , allowing arbitrary image resolution , and adapting to different vehicle scales and perspectives . We analyze and compare both methods , and get insights from optimization based method to improve deep model . Since existing datasets do not cover all the challenges in our work , we collected and labelled a large-scale traffic video dataset , containing 60 million frames from 212 webcams . Both methods are extensively evaluated and compared on different counting tasks and datasets . FCN based method significantly reduces the mean absolute error from 10.99 to 5.31 on the public dataset TRANCOS compared with the state-of-the-art baseline .
2K_dev_1258	In this paper , we present reasoning techniques for a component-based modeling and verification approach for hybrid systems comprising discrete dynamics as well as continuous dynamics , in which the components have local responsibilities . Our approach supports component contracts i.e. , input assumptions and output guarantees of interfaces that are more general than previous component-based hybrid systems verification techniques in the following ways : We introduce change contracts , which characterize how current values exchanged between components along ports relate to previous values . We also introduce delay contracts , which describe the change relative to the time that has passed since the last value was exchanged . Together , these contracts can take into account what has changed between two components in a given amount of time since the last exchange of information . Most crucially , we prove that the safety of compatible components implies safety of the composite . The proof steps of the theorem are also implemented as a tactic in KeYmaerai ? X , allowing automatic generation of a KeYmaerai ? X proof for the composite system from proofs of the concrete components .
2K_dev_1259	Processes such as disease propagation and information diffusion often spread over some latent network structure that must be learned from observation . Given a set of unlabeled training examples representing occurrences of an event type of interest ( such as a disease outbreak ) , the authors aim to learn a graph structure that can be used to accurately detect future events of that type . They propose a novel framework for learning graph structure from unlabeled data by comparing the most anomalous subsets detected with and without the graph constraints . Their framework uses the mean normalized log-likelihood ratio score to measure the quality of a graph structure , and it efficiently searches for the highest-scoring graph structure . Using simulated disease outbreaks injected into real-world Emergency Department data from Allegheny County , the authors show that their method learns a structure similar to the true underlying graph , but enables faster and more accurate detection .
2K_dev_1260	We introduce the first goal-driven training for visual question answering and dialog agents . Specifically , we pose a cooperative 'image guessing ' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images . We use deep reinforcement learning ( RL ) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward . We demonstrate two experimental results . First , as a 'sanity check ' demonstration of pure RL ( from scratch ) , we show results on a synthetic world , where the agents communicate in ungrounded vocabulary , i.e. , symbols with no pre-specified meanings ( X , Y , Z ) . We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes ( shape/color/style ) . Thus , we demonstrate the emergence of grounded language and communication among 'visual ' dialog agents with no human supervision . Second , we conduct large-scale real-image experiments on the VisDial dataset , where we pretrain with supervised dialog data and show that the RL 'fine-tuned ' agents significantly outperform SL agents . Interestingly , the RL Qbot learns to ask questions that Abot is good at , ultimately resulting in more informative dialog and a better team .
2K_dev_1261	Chest X-ray ( CXR ) is one of the most commonly prescribed medical imaging procedures , often with over 2-10x more scans than other imaging modalities such as MRI , CT scan , and PET scans . These voluminous CXR scans place significant workloads on radiologists and medical practitioners . Organ segmentation is a crucial step to obtain effective computer-aided detection on CXR . In this work , we propose Structure Correcting Adversarial Network ( SCAN ) to segment lung fields and the heart in CXR images . SCAN incorporates a critic network to impose on the convolutional segmentation network the structural regularities emerging from human physiology . During training , the critic network learns to discriminate between the ground truth organ annotations from the masks synthesized by the segmentation network . Through this adversarial process the critic network learns the higher order structures and guides the segmentation model to achieve realistic segmentation outcomes . Extensive experiments show that our method produces highly accurate and natural segmentation . Using only very limited training data available , our model reaches human-level performance without relying on any existing trained model or dataset . Our method also generalizes well to CXR images from a different patient population and disease profiles , surpassing the current state-of-the-art .
2K_dev_1262	The disclosure describes a sensor system that provides end users with intelligent sensing capabilities , and embodies both crowd sourcing and machine learning together . Further , a sporadic crowd assessment is used to ensure continued sensor accuracy when the system is relying on machine learning analysis . This sensor approach requires minimal and non-permanent sensor installation by utilizing any device with a camera as a sensor host , and provides human- centered and actionable sensor output .
2K_dev_1263	Modeling the long-term facial aging process is extremely challenging due to the presence of large and non-linear variations during the face development stages . In order to efficiently address the problem , this work first decomposes the aging process into multiple short-term stages . Then , a novel generative probabilistic model , named Temporal Non-Volume Preserving ( TNVP ) transformation , is presented to model the facial aging process at each stage . Unlike Generative Adversarial Networks ( GANs ) , which requires an empirical balance threshold , and Restricted Boltzmann Machines ( RBM ) , an intractable model , our proposed TNVP approach guarantees a tractable density function , exact inference and evaluation for embedding the feature transformations between faces in consecutive stages . Our model shows its advantages not only in capturing the non-linear age related variance in each stage but also producing a smooth synthesis in age progression across faces . Our approach can model any face in the wild provided with only four basic landmark points . Moreover , the structure can be transformed into a deep convolutional network while keeping the advantages of probabilistic models with tractable log-likelihood density estimation . Our method is evaluated in both terms of synthesizing age-progressed faces and cross-age face verification and consistently shows the state-of-the-art results in various face aging databases , i.e . FG-NET , MORPH , AginG Faces in the Wild ( AGFW ) , and Cross-Age Celebrity Dataset ( CACD ) . A large-scale face verification on Megaface challenge 1 is also performed to further show the advantages of our proposed approach .
2K_dev_1264	Analyzing videos of human actions involves understanding the temporal relationships among video frames . CNNs are the current state-of-the-art methods for action recognition in videos . However , the CNN architectures currently being used have difficulty in capturing these relationships . State-of-the-art action recognition approaches rely on traditional local optical flow estimation methods to pre-compute the motion information for CNNs . Such a two-stage approach is computationally expensive , storage demanding , and not end-to-end trainable . In this paper , we present a novel CNN architecture that implicitly captures motion information . Our method is 10x faster than a two-stage approach , does not need to cache flow information , and is end-to-end trainable . Experimental results on UCF101 and HMDB51 show that it achieves competitive accuracy with the two-stage approaches .
2K_dev_1265	Redundancy is an important strategy for reducing response time in multi-server distributed queueing systems . This strategy has been used in a variety of settings , but only recently have researchers begun analytical studies . The idea behind redundancy is that customers can greatly reduce response time by waiting in multiple queues at the same time , thereby experiencing the minimum time across queues . Redundancy has been shown to produce significant response time improvements in applications ranging from organ transplant waitlists to Googles BigTable service . However , despite the growing body of theoretical and empirical work on the benefits of redundancy , there is little work addressing the questions of how many copies one needs to make to achieve a response time benefit , and the magnitude of the potential gains . In this paper we propose a theoretical model and dispatching policy to evaluate these questions . Our system consists of k servers , each with its own queue . We introduce the Redundancy-d policy , u ...
2K_dev_1266	We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network ( DCNN ) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience . The re-training process effectively performs distance metric learning under the object persistency constraints , to modify the view-manifold of object representations . It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects , resulting in the untangling of the view-manifolds between individual objects within the same category and across categories . This untangling enables the model to discriminate and recognize objects within the same category , independent of viewpoints . We found that this ability is not limited to the trained objects , but transfers to novel objects in both trained and untrained categories , as well as to a variety of completely novel artificial synthetic objects . This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract , likely at the levels of parts , and independent of the specific objects or categories experienced during training . Interestingly , the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet , suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks .
2K_dev_1267	Given a collection of seasonal time-series , how can we find regular ( cyclic ) patterns and outliers ( i.e . rare events ) ? These two types of patterns are hidden and mixed in the time-varying activities . How can we robustly separate regular patterns and outliers , without requiring any prior information ? We present CycloneM , a unifying model to capture both cyclic patterns and outliers , and CycloneFact , a novel algorithm which solves the above problem . We also present an automatic mining framework AutoCyclone , based on CycloneM and CycloneFact . Our method has the following properties ; ( a ) effective : it captures important cyclic features such as trend and seasonality , and distinguishes regular patterns and rare events clearly ; ( b ) robust and accurate : it detects the above features and patterns accurately against outliers ; ( c ) fast : CycloneFact takes linear time in the data size and typically converges in a few iterations ; ( d ) parameter free : our modeling framework frees the user from having to provide parameter values . Extensive experiments on 4 real datasets demonstrate the benefits of the proposed model and algorithm , in that the model can capture latent cyclic patterns , trends and rare events , and the algorithm outperforms the existing state-of-the-art approaches . CycloneFact was up to 5 times more accurate and 20 times faster than top competitors .
2K_dev_1268	The recently developed variational autoencoders ( VAEs ) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods . However , most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution , thereby restricting its applications to relatively simple phenomena . In this work , we propose hierarchical nonparametric variational autoencoders , which combines tree-structured Bayesian nonparametric priors with VAEs , to enable infinite flexibility of the latent representation space . Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference . The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus , and infers accurate representations of data instances . We apply our model in video representation learning . Our method is able to discover highly interpretable activity hierarchies , and obtain improved clustering accuracy and generalization capacity based on the learned rich representations .
2K_dev_1269	Airports are poised to take advantage of demand response ( DR ) opportunities because of their large energy footprint and continuous operations . To develop an energy baseline model ( i.e. , the estimate of the expected load without curtailment ) , airports need special attention because of their continually changing operations and occupant levels , which result from varying flight schedules . However , an accurate baseline is also important for determining fair incentives and assessing DR strategies . This study , therefore , aimed to develop airport-specific energy baseline models by incorporating flight departure and arrival information . Therefore , the paper first analyzes relationships between airport power demand and potential predictors , such as time of day , time of week , outside temperature , and number of passengers on departing and arriving flights at a case study airport . Second , it develops piecewise linear regression models with combinations of variables and compares the models prediction performance . The ...
2K_dev_1270	Many problems in image processing and computer vision ( e.g . colorization , style transfer ) can be posed as 'manipulating ' an input image into a corresponding output image given a user-specified guiding signal . A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals ( even signals unseen during training ) , such as diverse paintings and arbitrary descriptive attributes . However , existing methods are either inefficient to simultaneously process multiple signals ( let alone generalize to unseen signals ) , or unable to handle signals from other modalities . In this paper , we make the first attempt to address the zero-shot image manipulation task . We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal ( even unseen ones ) . To this end , we propose the Zero-shot Manipulation Net ( ZM-Net ) , a fully-differentiable architecture that jointly optimizes an image-transformation network ( TNet ) and a parameter network ( PNet ) . The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself . Extensive experiments show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals ( e.g . style images and attributes ) in real-time ( tens of milliseconds per image ) even for unseen signals . Moreover , a large-scale style dataset with over 20,000 style images is also constructed to promote further research .
2K_dev_1271	Reading , tracing , and explaining the behavior of code are strongly correlated with the ability to write code effectively . To investigate program understanding in young children , we introduced two groups of third graders to Microsoft 's Kodu Game Lab ; the second group was also given four semantic `` Laws of Kodu '' to better scaffold their reasoning and discourage some common misconceptions . Explicitly teaching semantics proved helpful with one type of misconception but not with others . During each session , students were asked to predict the behavior of short Kodu programs . We found different styles of student reasoning ( analytical and analogical ) that may correspond to distinct neo-Piagetian stages of development as described by Teague and Lister ( 2014 ) . Kodu reasoning problems appear to be a promising tool for assessing computational thinking in young programmers .
2K_dev_1272	We analyze how alternative consumer data handling regimes affect the welfare of consumers , advertising firms , and an intermediary Ad exchange in the context of targeted advertising . We find that the collection and use of consumer data for targeting purposes affect consumer welfare through three distinct , and possibly countervailing , effects : match improvement , offer discrimination , and supply expansion . Furthermore , we find that the economic interests of the three agents can be misaligned , depending on the degree of heterogeneity in consumer preferences . Finally , we find that a strategic intermediary may choose to share with advertising firms only a subset of consumer data , maximizing its profits at their cost . In situations where the intermediary has an incentive to reveal the information that maximizes its payoff , overlooking the other agents interests , regulation of data collection and sharing may increase consumers welfare .
2K_dev_1273	When tasked to find fraudulent social network users , what is a practitioner to do ? Traditional classification can lead to poor generalization and high misclassification given few and possibly biased labels . We tackle this problem by analyzing fraudulent behavioral patterns , featurizing users to yield strong discriminative performance , and building algorithms to handle new and multimodal fraud types . First , we set up honeypots , or `` dummy '' social network accounts on which we solicit fake followers ( after careful IRB approval ) . We report the signs of such behaviors , including oddities in local network connectivity , account attributes , and similarities and differences across fraud providers . We discover several types of fraud behaviors , with the possibility of even more . We discuss how to leverage these insights in practice , build strongly performing entropy-based features , and propose OEC ( Open-ended Classification ) , an approach for `` future-proofing '' existing algorithms to account for the complexities of link fraud . Our contributions are ( a ) observations : we analyze our honeypot fraudster ecosystem and give insights regarding various fraud behaviors , ( b ) features : we engineer features which give exceptionally strong ( > 0.95 precision/recall ) discriminative power on ground-truth data , and ( c ) algorithm : we motivate and discuss OEC , which reduces misclassification rate by > 18 % over baselines and routes practitioner attention to samples at high-risk of misclassification .
2K_dev_1274	Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled countless new applications in the domain of aerial sensing . In scenarios such as intrusion detection , target tracking and facility monitoring it is important to reach a given area of interest ( AOI ) , and create an online data streaming connection to a monitoring ground station ( GS ) for immediate delivery of content to the operator . In previous work , we showed that a multi-hop line network can increase the range of the mission by finding the optimal number of relay UAVs , and their optimal placement . In this demo , we show that CSMA ( typical 802.11 's medium access protocol ) behaves poorly in this type of networks due to mutual interference , and that TDMA is a better alternative . We will also discuss how changing slot width online can overcome typical and less known TDMA in-efficiencies , and therefore reach maximum end-to-end throughput and low delay .
2K_dev_1275	Most cameras are equipped with an auto-contrast feature that enables them to take high quality pictures in a wide range of lighting conditions . Auto-contrast works by increasing the sensitivity of the camera to light in dimly lit surroundings , but reducing it in bright conditions to ensure that images do not become saturated . Our visual system is equipped with a similar feature . Neurons in the visual system increase or decrease their sensitivity to light as appropriate to enable us to see in both dimly lit rooms and dazzling sunshine . This process , which is known as dynamic range adaptation , also occurs in neurons that are sensitive to sound or touch . Rasmussen et al . therefore wondered whether the same might hold true for neurons that encode non-sensory stimuli such as the direction of movement . Would these neurons change their sensitivity to direction if presented with a wide range of possible directions instead of a narrow range ? If so , this would suggest that dynamic range adaptation occurs throughout the nervous system . To find out , Rasmussen et al . trained two rhesus macaque monkeys to use their brain activity to move a cursor on a virtual reality screen in either 2D or 3D . Studying this brain activity showed that neurons became less sensitive to the cursors direction of movement when the task switched from 2D to 3D . This makes sense because in a 3D task , which also features depth , the neurons have a greater range of possible movement directions to encode . Conversely , the neurons became more sensitive to the direction of movement when the task switched from 3D to 2D . Under these circumstances the neurons can use activity that was previously dedicated to encoding depth to instead represent the 2D space in finer detail . These results presented by Rasmussen et al . raise several additional questions . Are the mechanisms that support dynamic range adaptation the same in sensory and motor neurons ? If these neurons also encode other aspects of movement , such as speed , would these also be included in the same range as direction or is the adaptation process segregated by specific parameter categories ? And how do these changes in sensitivity affect the movements that animals produce ?
2K_dev_1276	With ever growing data volume and model size , an error-tolerant , communication efficient , yet versatile distributed algorithm has become vital for the success of many large-scale machine learning applications . In this work we propose m-PAPG , an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol . The worker machines communicate asynchronously with a controlled staleness bound $ s $ and operate at different frequencies . We characterize various convergence properties of m-PAPG : 1 ) Under a general non-smooth and non-convex setting , we prove that every limit point of the sequence generated by m-PAPG is a critical point of the objective function ; 2 ) Under an error bound condition , we prove that the function value decays linearly for every $ s $ steps ; 3 ) Under the Kurdyka- $ { \L } $ ojasiewicz inequality , we prove that the sequences generated by m-PAPG converge to the same critical point , provided that a proximal Lipschitz condition is satisfied .
2K_dev_1277	Self-driving vehicle technologies are progressing rapidly and are expected to play a significant role in the future of transportation . One of the main challenges for self-driving vehicles on public roads is the safe cooperation and collaboration among multiple vehicles using sensor-based perception and inter-vehicle communications . When self-driving vehicles try to occupy the same spatial area simultaneously , they might collide with one another , might become deadlocked , or might slam on the brakes making it uncomfortable or unsafe for passengers in a self-driving vehicle . In this paper , we study how a self-driving vehicle can safely navigate merge points , where two lanes with different priorities meet . We present a safe protocol for merge points named Autonomous Vehicle Protocol for Merge Points , where self-driving vehicles use both vehicular communications and their own perception systems for cooperating with other self-driving and/or human-driven vehicles . Our simulation results show that our traffic protocol has higher traffic throughput , compared to simple traffic protocols , while ensuring safety .
2K_dev_1278	The rapid growth of the Internet , particularly the explosion of social media , has led to unprecedented increases in the volume of network data worldwide . Already , the Yahoo Web Graph collected in 2002 contains in excess of one billion URLs , the Facebook social network recently exceeded one billion users , and numerous other social networks or online communities easily claim memberships in the millions of users . One fundamental task towards understanding the structural and functional properties of large-scale networks is to detect its community structure , where each community consists of a group of ( relatively ) densely interconnected nodes . Recently , there has been growing interest in overlapping community detection due to the evidence of significant community overlaps found in large-scale real networks with ground-truth communities ( Yang and Leskovec 2012 ) . Not surprisingly , for example , it is generally accepted that actors in a social network tend to belong to multiple social groups ( such as family , colleagues , and friends ) , depending on whom they are interacting with . The discovered communities can be explored and utilized in a number of important applications such as identifying fraudulent actions in telecommunication networks ( Pinheiro 2012 ) , studying dynamics of viral marketing ( Leskovec et al . 2007 ) , and identifying target groups in consumer networks ( Hill et al . 2006 ) . However , only a few algorithms have been successfully applied to large networks in excess of hundreds of millions of nodes and to the best of our knowledge , none of them are based on a statistical framework .
2K_dev_1279	Gaussian belief propagation ( BP ) has been widely used for distributed estimation in large-scale networks such as the smart grid , communication networks , and social networks , where local meansurements/observations are scattered over a wide geographical area . However , the convergence of Gaussian BP is still an open issue . In this paper , we consider the convergence of Gaussian BP , focusing in particular on the convergence of the information matrix . We show analytically that the exchanged message information matrix converges for arbitrary positive semidefinite initial value , and its distance to the unique positive definite limit matrix decreases exponentially fast .
2K_dev_1280	Intelligent personalization systems are becoming increasingly reliant on contextually-relevant devices and services , such as those available within modern IoT deployments . An IoT context may emerge -- -or become pervasive -- -when the intelligent system generates knowledge from dialogue-based interactions with the end-user ; the context is strengthened even further by incorporating state representations about the environment ( e.g. , generated from wireless sensor data ) into the knowledge graph . This is crucial for pervasive applications like digital assistance in IoT , where context-aware systems need to adapt quickly : activities like leaving work home-bound , driving to the grocery store , arriving at home , and walking the dog , for example , can occur in a relatively short period of time -- - during which an intelligent assistant must be able to support user requests in a consistent and coherent manner . Given that computational ontologies can serve as semantic models for heterogeneous data , they are becoming increasingly viable for reasoning across different IoT contexts . This involves : ( a ) federation and dynamic pruning of multiple modular ontologies , ideally , to comprehensively capture only the knowledge that will facilitate execution of a multi-context task ; ( b ) fast consistency-checking and ontology-based inferences , aided by rules-based execution environments that can evaluate/transform ambient wireless sensor network ( WSN ) data , in real-time ; and ( c ) run-time execution of ontology-based control procedures , through rule-engine actuation commands sent across the WSN . Only by realizing these functionalities may intelligent systems be capable of reasoning over device properties , system states , and user activities , while appropriately delegating commands to other intelligent agents or other relevant IoT services . In this poster , we illustrate how a multi-context knowledge base can be structured on the basis of modular ontologies and integrated with a distributed rules-based inference engine in multiple smart-building environments , in order to enable scalable contextual reasoning for intelligent assistance . Preliminary results are also discussed . This work is conducted through the partnership of Bosch Research Pittsburgh and Carnegie Mellon University ( CMU ) , and is in partial satisfaction of CMU 's Bosch Energy Research Network ( BERN ) grant , awarded for developments in intelligent building solutions . The approach we describe is also partially based on the Ubiquitous Personal Assistant ( UPA ) project , Bosch Research 's largest research initiative worldwide .
2K_dev_1281	Achieving an optimized control schema for building automation systems relies on the accurate collection of environmental modalities and accurate modeling of how the environment reacts to changes . Recent advances in indoor localization coupled with the influx of smart devices have created a unique and efficient method of collecting these modalities in real-time . This allows for quicker adaptation of the control schema by increasing both temporal and spatial information resulting in a reduction of operating costs . Visualizing these environmental modalities with fine grained positional accuracy allows for a better understanding of inter-dependency between modalities and also allows for a comparison between the real-time data across several modalities and pre-defined models .
2K_dev_1282	Developers of autonomous systems face distinct challenges in conforming to established methods of validating safety . It is well known that testing alone is insufficient to assure safety , because testing long enough to establish ultra-dependability is generally impractical . Thatfis why software safety standards emphasize high quality development processes . Testing then validates process execution rather than directly validating dependability . Two significant challenges arise in applying traditional safety processes to autonomous vehicles . First , simply gathering a complete set of system requirements is difficult because of the sheer number of combinations of possible scenarios and faults . Second , autonomy systems commonly use machine learning ( ML ) in a way that makes the requirements and design of the system opaque . After training , usually we know what an ML component will do for an input it has seen , but generally not what it will do for at least some other inputs until we try them . Both of these issues make it difficult to trace requirements and designs to testing as is required for executing a safety validation process . In other words , we are building systems that can not be validated due to incomplete or even unknown requirements and designs . Adaptation makes the problem even worse by making the system that must be validated a moving target . In the general case , it is impractical to validate all the possible adaptation states of an autonomy system using traditional safety design processes . An approach that can help with the requirements , design , and adaptation problems is basing a safety argument not on correctness of the autonomy functionality itself , but rather on conformance to a set of safety envelopes . Each safety envelope describes a boundary within the operational state space of the autonomy system . A system operating within a `` safe '' envelope knows that it is safe and can operate with full autonomy . A system operating within an `` unsafe '' envelope knows that it is unsafe , and must invoke a failsafe action . Multiple partial specifications can be used as an envelope set , with the intersection of safe envelopes permitting full autonomy , and the union of unsafe envelopes provoking validated , and potentially complex , failsafe responses . Envelope mechanisms can be implemented using traditional software engineering techniques , reducing the problems with requirements , design , and adaptation that would otherwise impede safety validation . Rather than attempting to prove that autonomy will always work correctly ( which is still a valuable goal to improve availability ) , the envelope approach measures the behavior of one or more autonomous components to determine if the result is safe . While this is not necessarily an easy thing to do , there is reason to believe that checking autonomy behaviors for safety is easier than implementing perfect , optimized autonomy actions . This envelope approach might be used to detect faults during development and to trigger failsafes in fleet vehicles . Inevitably there will be tension between simplicity of the envelope definitions and permissiveness , with more permissive envelope definitions likely being more complex . Operating in the gap areas between `` safe '' and `` unsafe '' requires human supervision , because the autonomy system can not be sure it is safe . One way to look at the progression from partial to full autonomy is that , over time , systems can increase permissiveness by defining and growing `` safe '' envelopes , shrinking `` unsafe '' envelopes , and eliminating any gap areas .
2K_dev_1283	Traditional generative adversarial networks ( GAN ) and many of its variants are trained by minimizing the KL or JS-divergence loss that measures how close the generated data distribution is from the true data distribution . A recent advance called the WGAN based on Wasserstein distance can improve on the KL and JS-divergence based GANs , and alleviate the gradient vanishing , instability , and mode collapse issues that are common in the GAN training . In this work , we aim at improving on the WGAN by first generalizing its discriminator loss to a margin-based one , which leads to a better discriminator , and in turn a better generator , and then carrying out a progressive training paradigm involving multiple GANs to contribute to the maximum margin ranking loss so that the GAN at later stages will improve upon early stages . We call this method Gang of GANs ( GoGAN ) . We have shown theoretically that the proposed GoGAN can reduce the gap between the true data distribution and the generated data distribution by at least half in an optimally trained WGAN . We have also proposed a new way of measuring GAN quality which is based on image completion tasks . We have evaluated our method on four visual datasets : CelebA , LSUN Bedroom , CIFAR-10 , and 50K-SSFF , and have seen both visual and quantitative improvement over baseline WGAN .
2K_dev_1284	The problems of hand detection have been widely addressed in many areas , e.g . human computer interaction environment , driver behaviors monitoring , etc . However , the detection accuracy in recent hand detection systems are still far away from the demands in practice due to a number of challenges , e.g . hand variations , highly occlusions , low-resolution and strong lighting conditions . This paper presents the Multiple Scale Faster Region-based Convolutional Neural Network ( MS-FRCNN ) to handle the problems of hand detection in given digital images collected under challenging conditions . Our proposed method introduces a multiple scale deep feature extraction approach in order to handle the challenging factors to provide a robust hand detection algorithm . The method is evaluated on the challenging hand database , i.e . the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge , and compared against various recent hand detection methods . Our proposed method achieves the state-of-the-art results with 20 % of the detection accuracy higher than the second best one in the VIVA challenge .
2K_dev_1285	While only recently developed , the ability to profile expression data in single cells ( scRNA-Seq ) has already led to several important studies and findings . However , this technology has also raised several new computational challenges including questions related to handling the noisy and sometimes incomplete data , how to identify unique group of cells in such experiments and how to determine the state or function of specific cells based on their expression profile . To address these issues we develop and test a method based on neural networks ( NN ) for the analysis and retrieval of single cell RNA-Seq data . We tested various NN architectures , some biologically motivated , and used these to obtain a reduced dimension representation of the single cell expression data . We show that the NN method improves upon prior methods in both , the ability to correctly group cells in experiments not used in the training and the ability to correctly infer cell type or state by querying a database of tens of thousands of single cell profiles . Such database queries ( which can be performed using our web server ) will enable researchers to better characterize cells when analyzing heterogeneous scRNA-Seq samples .
2K_dev_1286	High Assurance SPIRAL ( HA-SPIRAL ) is a tool that synthesizes a faithful and high performance implementation from the mathematical specification of a given controller or monitor . At the heart of HA-SPIRAL is a mathematical identity rewrite engine based on a computer algebra system . The rewrite engine refines the mathematical expression provided by a control engineer , through mathematical identities , into an equivalent mathematical expression that can be implemented in code . In this paper , we discuss the use of HA-SPIRAL in generating provably-correct and high-performance implementations for different controllers and monitors for autonomous land and air vehicles .
2K_dev_1287	The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated the faster propagation of news and rumors . How quickly does a piece of news spread over these media ? How does its popularity diminish over time ? Does the rising and falling pattern follow a simple universal law ? In this article , we propose S pike M , a concise yet flexible analytical model of the rise and fall patterns of information diffusion . Our model has the following advantages . First , unification power : it explains earlier empirical observations and generalizes theoretical models including the SI and SIR models . We provide the threshold of the take-off versus die-out conditions for S pike M and discuss the generality of our model by applying it to an arbitrary graph topology . Second , practicality : it matches the observed behavior of diverse sets of real data . Third , parsimony : it requires only a handful of parameters . Fourth , usefulness : it makes it possible to perform analytic tasks such as forecasting , spotting anomalies , and interpretation by reverse engineering the system parameters of interest ( quality of news , number of interested bloggers , etc. ) . We also introduce an efficient and effective algorithm for the real-time monitoring of information diffusion , namely S pike S tream , which identifies multiple diffusion patterns in a large collection of online event streams . Extensive experiments on real datasets demonstrate that S pike M accurately and succinctly describes all patterns of the rise and fall spikes in social networks .
2K_dev_1288	In face recognition tasks , the changing pose of the face can cause enough information to be lost to cause the recognition to fail so being able to determine the pose of the face beforehand can allow for some better recognition performance . Many methods used for pose estimation tasks rely on finding some underlying structure of the data given to create a classifier . We propose an alternative method in which the training data itself is the underlying structure of a classifier . This is accomplished through the use of matrix decomposition equations . However , instead of decomposing a matrix , one is created by carefully selecting the terms in the decomposition equation such that the resulting matrix has the desired properties for classification . We show two recomposition methods using the Spectral Decomposition and Singular Value Decomposition equations . We show this method can perform pose estimation with a high accuracy of 85.21 % and an accuracy of 98.42 % when allowing a 15 tolerance on the pose estimate on the CUbiC FacePix dataset . We also show results on both yaw and pitch estimation on the Pointing'04 dataset with our methods achieving 77.01 % accuracy on yaw estimation .
2K_dev_1289	We study the design of pricing mechanisms and auctions when the mechanism designer does not know the distribution of buyers ' values . Instead the mechanism designer receives a set of samples from this distribution and his goal is to use the sample to design a pricing mechanism or auction with high expected profit . We provide generalization guarantees which bound the difference between average profit on the sample and expected profit over the distribution . These bounds are directly proportional to the intrinsic complexity of the mechanism class the designer is optimizing over . We present a single , overarching theorem that uses empirical Rademacher complexity to measure the intrinsic complexity of a variety of widely-studied single- and multi-item auction classes , including affine maximizer auctions , mixed-bundling auctions , and second-price item auctions . Despite the extensive applicability of our main theorem , we match and improve over the best-known generalization guarantees for many auction classes . This all-encompassing theorem also applies to multi- and single-item pricing mechanisms in both multi- and single-unit settings , such as linear and non-linear pricing mechanisms . Finally , our central theorem allows us to easily derive generalization guarantees for every class in several finely grained hierarchies of auction and pricing mechanism classes . We demonstrate how to determine the precise level in a hierarchy with the optimal tradeoff between profit and generalization using structural profit maximization . The mechanism classes we study are significantly different from well-understood function classes typically found in machine learning , so bounding their complexity requires a sharp understanding of the interplay between mechanism parameters and buyer valuations .
2K_dev_1290	Rapid improvements in the precision of mobile technologies make it possible for advertisers to go beyond using the real-time static location and contextual information about consumers . In this study , we propose a novel trajectory-based targeting strategy for mobile recommendation that leverages full information on consumers physical movement trajectories using granular behavioral information from different mobility dimensions . To analyze the effectiveness of this new strategy , we design a large-scale randomized field experiment in a large shopping mall that involved 83,370 unique user responses for a 14-day period in June 2014 . We find that trajectory-based mobile targeting can lead to higher redemption probability , faster redemption behavior , and higher transaction amount from customers compared to other baselines . It also facilitates higher revenues for the focal store as well as the overall shopping mall . Moreover , the effect of trajectory-based targeting comes not only from improvements in the efficiency of customers current shopping process , but also from its ability to nudge customers towards changing their future shopping patterns and generate additional revenues . Finally , we find significant heterogeneity in the impact of trajectory-based targeting . It is especially effective in influencing high-income consumers . Interestingly , it becomes less effective in boosting the revenues of the shopping mall during the weekends and for those shoppers who like to explore across products categories . Our finding suggests that highly targeted mobile promotions can have the inadvertent impact of reducing impulse purchase behavior by customers who are in an exploratory shopping stage . On a broader note , our work can be viewed as a first step towards studying the large-scale , fine-grained digital trace of individual physical behavior , and how it can be used to predict and market to individual anticipated future behavior .
2K_dev_1291	Common appliances have shifted toward flat interface panels , making them inaccessible to blind people . Although blind people can label appliances with Braille stickers , doing so generally requires sighted assistance to identify the original functions and apply the labels . We introduce Facade - a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel . Facade users capture a photo of the appliance with a readily available fiducial marker ( a dollar bill ) for recovering size information . This image is sent to multiple crowd workers , who work in parallel to quickly label and describe elements of the interface . Facade then generates a 3D model for a layer of tactile and pressable buttons that fits over the original controls . Finally , a home 3D printer or commercial service fabricates the layer , which is then aligned and attached to the interface by the blind person . We demonstrate the viability of Facade in a study with 11 blind participants .
2K_dev_1292	Crowdsourcing and human computation are useful in a number of real-world applications . Crowds generate large data sets useful for natural language processing and computer vision ; they work together to formulate intelligent responses far beyond what we can automate ; and they power intelligent interactive systems currently impossible with automated approaches alone . In this course , affendees will learn how to work with the crowd to enable research and practical applications . They will gain experience from the worker 's perspective , receive an introduction to writing programs that work with existing sources of crowds , ( e.g. , Amazon Mechanical Turk ) , apply usability principles for designing crowd tasks that elicit high-quality responses , use statistical methods to improve the quality of the work received , build systems that interface with crowd labor in real time , and conduct experiments to improve understanding of the differences between different sources of crowd work . The course will provide hands-on activities on each of these topics , and provide pointers to material that can provide more in depth information .
2K_dev_1293	Is it possible to monitor the entire traffic in Manhattan at a few intersections ? This paper proposes a series of sampling , recovery and representation techniques based on graph signal processing to handle complex , nonsmooth graph signals . We validate our proposed techniques on Manhattan 's taxi pickups during the years of 2014 and 2015 . We are able to approximately recover the taxi-pick activities in Manhattan by sampling at only 5 selected intersections . The same techniques can be applied to monitor other types of traffic data .
2K_dev_1294	Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues , which motivates the use of annotated implicit connectives to improve the recognition . We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives , and thus encouraged to extract similarly salient features for accurate classification . We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator . Our method effectively transfers discriminability of connectives to the implicit features , and achieves state-of-the-art performance on the PDTB benchmark .
2K_dev_1295	Hearing-impaired people and non-native speakers rely on captions for access to video content , yet most videos remain uncaptioned or have machine-generated captions with high error rates . In this paper , we present the design , implementation and evaluation of BandCaption , a system that combines automatic speech recognition with input from crowd workers to provide a cost-efficient captioning solution for accessible online videos . We consider four stakeholder groups as our source of crowd workers : ( i ) individuals with hearing impairments , ( ii ) second-language speakers with low proficiency , ( iii ) second-language speakers with high proficiency , and ( iv ) native speakers . Each group has different abilities and incentives , which our workflow leverages . Our findings show that BandCaption enables crowd workers who have different needs and strengths to accomplish micro-tasks and make complementary contributions . Based on our results , we outline opportunities for future research and provide design suggestions to deliver cost-efficient captioning solutions .
2K_dev_1296	The promise of smart environments and the Internet of Things ( IoT ) relies on robust sensing of diverse environmental facets . Traditional approaches rely on direct and distributed sensing , most often by measuring one particular aspect of an environment with a special purpose sensor . This approach can be costly to deploy , hard to maintain , and aesthetically and socially obtrusive . In this work , we explore the notion of general purpose sensing , wherein a single enhanced sensor can indirectly monitor a large context , without direct instrumentation of objects . Further , through what we call Synthetic Sensors , we can virtualize raw sensor data into actionable feeds , whilst simultaneously mitigating immediate privacy issues . A series of structured , formative studies informed the development of our new sensor hardware and accompanying information architecture . We deployed our system across many months and environments , the results of which show the versatility , accuracy and potential utility of our approach .
2K_dev_1297	Objective We present the multidimensional tensor scan ( MDTS ) , a new method for identifying emerging patterns in multidimensional spatio-temporal data , and demonstrate the utility of this approach for discovering emerging geographic , demographic , and behavioral trends in fatal drug overdoses . Introduction Drug overdoses are an increasingly serious problem in the United States and worldwide . The CDC estimates that 47,055 drug overdose deaths occurred in the United States in 2014 , 61 % of which involved opioids ( including heroin , pain relievers such as oxycodone , and synthetics ) . 1 Overdose deaths involving opioids increased 3-fold from 2000 to 2014 . 1 These statistics motivate public health to identify emerging trends in overdoses , including geographic , demographic , and behavioral patterns ( e.g. , which combinations of drugs are involved ) . Early detection can inform prevention and response efforts , as well as quantifying the effects of drug legislation and other policy changes . The fast subset scan 2 detects significant spatial patterns of disease by efficiently maximizing a log-likelihood ratio statistic over subsets of data points , and has recently been extended to multidimensional data ( MD-Scan ) . 3 While MD-Scan is a potentially useful tool for drug overdose surveillance , the high dimensionality and sparsity of the data requires a new approach to estimate and represent baselines ( expected counts ) , maintaining both accuracy and efficient computation when searching over subsets . Methods The multidimensional tensor scan ( MDTS ) is a new approach to subset scanning in multidimensional data . In addition to detecting the spatial area ( subset of locations ) and time window affected by an emerging outbreak , MDTS can also identify the affected subset of values for each observed attribute . For example , given the drug overdose surveillance data described below , MDTS can identify the affected genders , races , age ranges , and which drugs were involved . MDTS finds subsets of the attribute space with higher than expected case counts , first using a novel tensor decomposition approach to estimate the expected counts . MDTS then iteratively applies a conditional optimization step , optimizing over all subsets of values for each attribute conditional on the current subsets of values for all other attributes 3 , and using the linear-time subset scanning property 2 to make each conditional optimization step computationally efficient . The resulting approach has high power to detect and characterize emerging trends which may only affect a subset of the monitored population ( e.g. , specific ages , genders , neighborhoods , or users of particular combinations of drugs ) . Results We used MDTS to analyze publicly available data from the Allegheny County , PA medical examiners office and to detect emerging overdose patterns and trends . The dataset consists of ~2000 fatal accidental drug overdoses between 2008 and 2015 . For each overdose victim , we have date , location ( zip code ) , age decile , gender , race , and the presence/absence of 27 commonly abused drugs in their system . The highest-scoring clusters discovered by MDTS were shared with Allegheny Countys Dept . of Human Services and their feedback obtained . One set of potentially relevant findings from our analysis involved fentanyl , a dangerous and potent opioid which has been a serious problem in western PA . In addition to identifying two well- known , large clusters of overdoses14 deaths in January 2014 and 26 deaths in March-April 2015MDTS was able to provide additional information about each cluster . For example , the first cluster was likely due to fentanyl-laced heroin , while the second was more likely due to fentanyl disguised as heroin ( only 11 victims had heroin in their system ) . Moreover , the second cluster was initially confined to the Pittsburgh suburb of McKeesport and a typical demographic ( white males ages 20-49 ) , before spreading across the county . Our analysis demonstrated that prospective surveillance using MDTS would have identified the cluster as early as March 29th , enabling targeted prevention efforts . MDTS also discovered a previously unidentified , highly localized cluster of fentanyl-related overdoses affecting an unusual and underserved demographic ( elderly black males near downtown Pittsburgh ) . This cluster occurred in January- February 2015 , and may have been related to the larger cluster of fentanyl-related overdoses that occurred two months later . Finally , we identified multiple overdose clusters involving combinations of methadone and Xanax between 2008 and 2012 , and observed dramatic reductions in these clusters corresponding to the passage of the Methadone Death and Incident Review Act ( October 2012 ) , which increased state oversight of methadone clinics and prescribing physicians . Conclusions Retrospective analysis of Allegheny County overdose data suggests high potential utility for a prospective overdose surveillance system , which would enable public health users to identify emerging patterns of overdoses in their early stages and facilitate targeted and effective health interventions . The MDTS approach can also be used for other multidimensional public health surveillance tasks , such as STI surveillance , where the patterns or outbreaks of interest may have demographic , geographic , and behavioral components .
2K_dev_1298	Homes , offices and many other environments will be increasingly saturated with connected , computational appliances , forming the `` Internet of Things '' ( IoT ) . At present , most of these devices rely on mechanical inputs , webpages , or smartphone apps for control . However , as IoT devices proliferate , these existing interaction methods will become increasingly cumbersome . Will future smart-home owners have to scroll though pages of apps to select and dim their lights ? We propose an approach where users simply tap a smartphone to an appliance to discover and rapidly utilize contextual functionality . To achieve this , our prototype smartphone recognizes physical contact with uninstrumented appliances , and summons appliance-specific interfaces . Our user study suggests high accuracy 98.8 % recognition accuracy among 17 appliances . Finally , to underscore the immediate feasibility and utility of our system , we built twelve example applications , including six fully functional end-to-end demonstrations .
2K_dev_1299	Small , local groups who share protected resources ( e.g. , families , work teams , student organizations ) have unmet authentication needs . For these groups , existing authentication strategies either create unnecessary social divisions ( e.g. , biometrics ) , do not identify individuals ( e.g. , shared passwords ) , do not equitably distribute security responsibility ( e.g. , individual passwords ) , or make it difficult to share or revoke access ( e.g. , physical keys ) . To explore an alternative , we designed Thumprint : inclusive group authentication with a shared secret knock . All group members share one secret knock , but individual expressions of the secret are discernible . We evaluated the usability and security of our concept through two user studies with 30 participants . Our results suggest that ( 1 ) individuals who enter the same shared thumprint are distinguishable from one another , ( 2 ) that people can enter thumprints consistently over time , and ( 3 ) that thumprints are resilient to casual adversaries .
2K_dev_1300	Infrastructure monitoring applications currently lack a cost-effective and reliable solution for supporting the last communication hop for low-power devices . The use of cellular infrastructure requires contracts and complex radios that are often too power hungry and cost prohibitive for sensing applications that require just a few bits of data each day . New low-power , sub-GHz , long-range radios are an ideal technology to help fill this communication void by providing access points that are able to cover multiple kilometers of urban space with thousands of end-point devices . These new Low-Power Wide-Area Networking ( LPWAN ) platforms provide a cost-effective and highly deployable option that could piggyback off of existing public and private wireless networks ( WiFi , Cellular , etc ) . In this paper , we present OpenChirp , a prototype end-to-end LPWAN architecture built using LoRa Wide-Area Network ( LoRaWAN ) with the goal of simplifying the design and deployment of Internet-of-Things ( IoT ) devices across wide areas like campuses and cities . We present a software architecture that exposes an application layer allowing users to register devices , describe transducer properties , transfer data and retrieve historical values . We define a service model on top of LoRaWAN that acts as a session layer to provide basic encoding and syntax to raw data streams . At the device-level , we introduce and benchmark an open-source hardware platform that uses Bluetooth Low-Energy ( BLE ) to help provision LoRa clients that can be extended with custom transducers . We evaluate the system in terms of end-node energy consumption , radio penetration into buildings as well as coverage provided by a network currently deployed at Carnegie Mellon University .
2K_dev_1301	We relate the time-limiting behavior of a network epidemics process to the spectral radius of the underlying network . The process we study is the scaled SIS network process , a continuous-time Markov process on a static network . Our analysis differs from previous work in that the scaled SIS process accounts for the possibility that a healthy individual has a nonzero probability of becoming infected even when all of its neighbors are healthy . For example , the source of infection may be outside a human only contact network for diseases with animal to human transmissions such as Ebola . We show that the sufficient condition for infection to become extinct not only depends on the ratio of infection and healing rates but also on N , the size of the network , whereas previous models , assuming no exogenous infection , showed dependency only on the infection and healing rates .
2K_dev_1302	The proliferation of mobile and sensor technologies has contributed to the rise of location-based mobile targeting . Beyond the location , time and spatial context of individuals , the social context wherein they are embedded can reveal rich information about their behavior . Such real-time social dynamics can help mobile advertisers to more fully understand consumer contextual preferences and , thereby , provide better digital experiences . In this study , we automatically detected the real-time social contexts of customers based on their detailed GPS trajectories using state-of-the-art machine-learning methods . To evaluate the effectiveness of mobile targeting under different social contexts , we designed a randomized field experiment for a large shopping mall in Asia based on 52,500 unique user responses for 252 stores over the course of a 21-day period in April 2015 . Our analyses indicated significant heterogeneity in consumer behavior under different real-time social contexts . We found , for example , that a customer in a group with others is on average 1.97 times more responsive to mobile promotions than is a solo shopper , and that this impact increases with increased group size ( from dyad to triad ) . Interestingly , we also found that couples seemed to have an attention deficit with respect to mobile promotions and were the least responsive compared with the other social groups . Meanwhile , high-income customers and male customers were more likely to respond to mobile promotions when shopping alone than when shopping with social groups . Our analyses also revealed significant heterogeneity in the interaction effect between mobile promotion design and real-time social contexts . Overall , our study demonstrates the potential of inferring individuals social contexts in real time from their movement trajectories as well as the value of leveraging such real-time social dynamics for improved mobile-targeting effectiveness .
2K_dev_1303	We apply graph signal processing to the study of taxi movement in New York City based on 20102013 New York City taxi data . Such analysis requires a signal extraction method that involves computing shortest paths between the start and end locations for each of the 700 million trip records . We perform spectral analysis on these graph signals , for which it is necessary to address the challenge of finding the eigendecomposition of the 6K-node directed Manhattan road network . We show that PSNR=29.90 dB is recovered for graph signals reconstructed from 70 % of the graph frequency components . We illustrate that graph frequency components reveal taxi behaviors that are not obvious from the raw signal .
2K_dev_1304	Current touch input technologies are best suited for small and flat applications , such as smartphones , tablets and kiosks . In general , they are too expensive to scale to large surfaces , such as walls and furniture , and can not provide input on objects having irregular and complex geometries , such as tools and toys . We introduce Electrick , a low-cost and versatile sensing technique that enables touch input on a wide variety of objects and surfaces , whether small or large , flat or irregular . This is achieved by using electric field tomography in concert with an electrically conductive material , which can be easily and cheaply added to objects and surfaces . We show that our technique is compatible with commonplace manufacturing methods , such as spray/brush coating , vacuum forming , and casting/molding enabling a wide range of possible uses and outputs . Our technique can also bring touch interactivity to rapidly fabricated objects , including those that are laser cut or 3D printed . Through a series of studies and illustrative example uses , we show that Electrick can enable new interactive opportunities on a diverse set of objects and surfaces that were previously static .
2K_dev_1305	Blind people often need to identify objects around them , from packages of food to items of clothing . Automatic object recognition continues to provide limited assistance in such tasks because models tend to be trained on images taken by sighted people with different background clutter , scale , viewpoints , occlusion , and image quality than in photos taken by blind users . We explore personal object recognizers , where visually impaired people train a mobile application with a few snapshots of objects of interest and provide custom labels . We adopt transfer learning with a deep learning system for user-defined multi-label k-instance classification . Experiments with blind participants demonstrate the feasibility of our approach , which reaches accuracies over 90 % for some participants . We analyze user data and feedback to explore effects of sample size , photo-quality variance , and object shape ; and contrast models trained on photos by blind participants to those by sighted participants and generic recognizers .
2K_dev_1306	Current tools for screening dyslexia use linguistic elements , since most dyslexia manifestations are related to difficulties in reading and writing . These tools can only be used with children that have already acquired some reading skills and ; sometimes , this detection comes too late to apply proper remediation . In this paper , we propose a method and present DysMusic , a prototype which aims to predict risk of having dyslexia before acquiring reading skills . The prototype was designed with the help of five children and five parents who tested the game using the think aloud protocol and being observed while playing . The advantages of DysMusic are that the approach is language independent and could be used with younger children , i.e. , pre-readers .
2K_dev_1307	While a number of schemes exist for mixed-criticality scheduling in a single processor setting , no solution exists to cover the industry need for end-to-end scheduling across multiple processors in a pipeline . In this paper , we present an end-to-end zero-slack rate-monotonic scheme ( ZSRM ) based on real-time pipelines , called the ZSRM pipeline scheduler , that addresses this need . Under ZSRM , each task is associated with a parameter called zero-slack instant , and whenever a higher-criticality job has not finished at its zero-slack instant relative to its arrival time , all jobs of lower criticality are suspended to meet the deadline of the higher-criticality job . We develop a new schedulability test and algorithm for computing the zero-slack instants of tasks scheduled across a pipeline .
2K_dev_1308	It is our great pleasure to welcome you to the 2013 ACM MM Workshop on Multimedia Indexing and Information Retrieval for Healthcare -- MIIRH'13 . This is the first workshop on Multimedia Information Indexing and Retrieval for Healthcare and is intended to establish a platform for the continued discussion of key research issues in multimedia for healthcare , remote monitoring and treatment . Multimodal monitoring for health can take place at home , but should be discreet , unobtrusive and personalized . Theoretical , i.e . research-oriented and practical , application-specific issues related to the extraction of lifestyle , behavior and health information from multimodal data will be examined . The setup of smart homes has become of great interest lately , as the latter need to ensure accurate , reliable , discreet and cost-efficient measurements , involving , among others , privacy protection and appropriate sensor placement . Marketing of multimodal remote monitoring options also needs to be examined carefully , as independent living at home , smart homes and remote care are forecast to gain importance in years to come . Multimedia Indexing and Retrieval research is now being oriented towards this application domain of primarily importance for the society . Workshop papers and presentations will focus on the analysis of multimodal data to obtain information pertinent to healthcare problems and also examine remote monitoring solutions . Personalization , unobtrusiveness , accuracy of analysis results , data storage , retrieval , transmission , marketability and privacy concerns are among the many topics that will be discussed . The workshop will provide a forum for researchers from all over the world to share information on their latest investigations on multimedia information retrieval and indexing with healthcare applications . MIIRH is held as a one day workshop with presentations from 9:00 am to 5:00 pm with breaks and a 1-hour lunch period . The call for papers attracted many submissions from Asia , Europe , Australia and the Americas . The program committee accepted 10 papers that cover a variety of topics , including : recognition of daily living activities , analysis of therapy exercises , fall detection , context aware recommendation , experience sharing , and medical image retrieval . In addition to paper presentations , posters and demos , MIIRH will feature an invited keynote talk by Prof. Linda Shapiro on `` Image Analysis for Biomedical and Healthcare Applications '' as well as an invited talk by Prof. Jean-Francois Dartigues on `` Dementia and Dependency : a Major Challenge for the 21st century '' . We hope that these proceedings will serve as a valuable reference for healthcarerelated multimedia indexing and retrieval researchers and developers .
2K_dev_1309	Information cascades are ubiquitous in both physical society and online social media , taking on large variations in structures , dynamics and semantics . Although there has been much progress on understanding the dynamics and semantics of information cascades , little is known about their structural patterns . In this paper , we explore a large-scale dataset including 432 million information cascades with explicit records of spreading traces . We find that the structural complexity of information cascades is far beyond the previous conjectures . We first propose seven-dimensional metrics , which reflect size and spreading orientation aspects , to quantify the structural characteristics of millions of information cascades . Further , we analyze the correlations of these metrics , finding some brand new structure patterns of information cascades , potentially providing insights into intrinsic mechanisms governing information spreading in nature and new models to forecast as well as to impose good control over information cascades in real applications .
2K_dev_1310	This paper continues the program initiated in [ 5 ] , towards a derivation system for security protocols . The general idea is that complex protocols can be formally derived , starting from basic security components , using a sequence of refinements and transformations , just like logical proofs are derived starting from axioms , using proof rules and transformations . The claim is that in practice , many protocols are already derived in such a way , but informally . Capturing this practice in a suitable formalism turns out to be a considerable task . The present paper proposes rules for composing security protocols from given security components . In general , security protocols are , of course , not compositional : information revealed by one may interfere with the security of the other . However , annotating protocol steps by pre- and post-conditions , allows secure sequential composition . Establishing that protocol components satisfy each other 's invariants allows more general forms of composition , ensuring that the individually secure sub-protocols will not interact insecurely in the composite protocol . The applicability of the method is demonstrated on modular derivations of two standard protocols , together with their simple security properties .
2K_dev_1311	An object recognition device that senses electrical signals conducted by the body of a human user ( e.g. , as a result of direct contact with or close proximity to a device emitting or conducting electromagnetic noise ) , compares the sensed electrical signals to a plurality of signatures of electrical signals produced by a corresponding plurality of types of electrical and electromechanical devices to determine the type of electrical or electromechanical device that generated the electrical signals sensed by the sensor , and communicates information to the human user related to or triggered by the electrical or electromechanical device .
2K_dev_1312	Feature extraction and encoding represent two of the most crucial steps in an action recognition system . For building a powerful action recognition pipeline it is important that both steps are efficient and in the same time provide reliable performance . This work proposes a new approach for feature extraction and encoding that allows us to obtain real-time frame rate processing for an action recognition system . The motion information represents an important source of information within the video . The common approach to extract the motion information is to compute the optical flow . However , the estimation of optical flow is very demanding in terms of computational cost , in many cases being the most significant processing step within the overall pipeline of the target video analysis application . In this work we propose an efficient approach to capture the motion information within the video . Our proposed descriptor , Histograms of Motion Gradients ( HMG ) , is based on a simple temporal and spatial derivation , which captures the changes between two consecutive frames . For the encoding step a widely adopted method is the Vector of Locally Aggregated Descriptors ( VLAD ) , which is an efficient encoding method , however , it considers only the difference between local descriptors and their centroids . In this work we propose Shape Difference VLAD ( SD-VLAD ) , an encoding method which brings complementary information by using the shape information within the encoding process . We validated our proposed pipeline for action recognition on three challenging datasets UCF50 , UCF101 and HMDB51 , and we propose also a real-time framework for action recognition .
2K_dev_1313	As online fraudsters invest more resources , including purchasing large pools of fake user accounts and dedicated IPs , fraudulent attacks become less obvious and their detection becomes increasingly challenging . Existing approaches such as average degree maximization suffer from the bias of including more nodes than necessary , resulting in lower accuracy and increased need for manual verification . Hence , we propose HoloScope , which uses information from graph topology and temporal spikes to more accurately detect groups of fraudulent users . In terms of graph topology , we introduce `` contrast suspiciousness , '' a dynamic weighting approach , which allows us to more accurately detect fraudulent blocks , particularly low-density blocks . In terms of temporal spikes , HoloScope takes into account the sudden bursts and drops of fraudsters ' attacking patterns . In addition , we provide theoretical bounds for how much this increases the time cost needed for fraudsters to conduct adversarial attacks . Additionally , from the perspective of ratings , HoloScope incorporates the deviation of rating scores in order to catch fraudsters more accurately . Moreover , HoloScope has a concise framework and sub-quadratic time complexity , making the algorithm reproducible and scalable . Extensive experiments showed that HoloScope achieved significant accuracy improvements on synthetic and real data , compared with state-of-the-art fraud detection methods .
2K_dev_1314	In comparison-shopping services ( CSS ) , there exist frauds who perform excessive clicks on a target item in order to boost the popularity of it . In this paper , we introduce the problem of detecting frauds in CSS and propose three anomaly scores designed based on click behaviors of users in CSS .
2K_dev_1315	As one of the featured initiatives in smart grids , demand response is enabling active participation of electricity consumers in the supply/demand balancing process , thereby enhancing the power systems operational flexibility in a costeffective way . Industrial load plays an important role in demand response because of its intense power consumption , already existing advanced monitoring and control infrastructure , and its strong economic incentive due to the high energy costs . As typical industrial loads , cement plants are able to quickly adjust their power consumption rate by switching on/off the crushers . However , in the cement plant as well as other industrial loads , switching on/off the loading units only achieves discrete power changes , which restricts the load from offering valuable ancillary services such as regulation and load following , as continuous power changes are required for these services . In this paper , we overcome this restriction of poor granularity by proposing methods that enable these loads to provide regulation or load following with the support of an on-site energy storage system .
2K_dev_1316	Large graph datasets have caused renewed interest for graph partitioning . However , existing well-studied graph partitioners often assume that vertices of the graph are always active during the computation , which may lead to time-varying skewness for traversal-style graph workloads , like Breadth First Search , since they only explore part of the graph in each superstep . Additionally , existing solutions do not consider what vertices each partition will have , as a result , high-degree vertices may be concentrated into a few partitions , causing imbalance . Towards this , we introduce the idea of skew-resistant graph partitioning , where the objective is to create an initial partitioning that will `` hold well '' over time without suffering from skewness . Skewresistant graph partitioning tries to mitigate skewness by taking the characteristics of both the target workload and the graph structure into consideration .
2K_dev_1317	This paper presents an approach to formalizing and enforcing a class of use privacy properties in data-driven systems . In contrast to prior work , we focus on use restrictions on proxies ( i.e . strong predictors ) of protected information types . Our definition relates proxy use to intermediate computations that occur in a program , and identify two essential properties that characterize this behavior : 1 ) its result is strongly associated with the protected information type in question , and 2 ) it is likely to causally affect the final output of the program . For a specific instantiation of this definition , we present a program analysis technique that detects instances of proxy use in a model , and provides a witness that identifies which parts of the corresponding program exhibit the behavior . Recognizing that not all instances of proxy use of a protected information type are inappropriate , we make use of a normative judgment oracle that makes this inappropriateness determination for a given witness . Our repair algorithm uses the witness of an inappropriate proxy use to transform the model into one that provably does not exhibit proxy use , while avoiding changes that unduly affect classification accuracy . Using a corpus of social datasets , our evaluation shows that these algorithms are able to detect proxy use instances that would be difficult to find using existing techniques , and subsequently remove them while maintaining acceptable classification performance .
2K_dev_1318	Science of Security ( SoS ) emphasizes the advancement of research methods as well as the development of new research results . This dual focus is intended to improve both the confidence we gain from scientific results and also the capacity and efficiency through which we address increasingly challenging technical problems . The HotSoS conferences have focused on work related to one or more of the five Hard Problems identified by the Science of Security community : Scalability and composability in the construction of secure systems Policy-governed collaboration in handling data across different domains of authority for security and privacy protection Predictive security metrics to guide choice-making in security engineering and response Resilient architectures that can deliver service despite compromised components Human behavior , modeling users , operators , and adversaries to support improved design and analysis A second and equally major focus of the conferences is on the advancement of scientific methods , including data gathering and analysis , experimental methods , and mathematical models for modeling and reasoning . This includes the exploration of interactions among these methods to enhance validity .
2K_dev_1319	Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community . A considerable amount of videos on the web is associated with rich but noisy contextual information , such as the title and other multi-modal information , which provides weak annotations or labels about the video content . To tackle the problem of large-scale noisy learning , We propose a novel method called Multi-modal WEbly-Labeled Learning ( WELL-MM ) , which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human . WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos . We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images . The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data . In addition , the results also verify that WELL-MM is robust to the level of noisiness in the video data . Notably , WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data .
2K_dev_1320	Summary Cryo-electron tomography ( cryo-ET ) captures the 3Delectron density distribution of macromolecular complexes in close to native state . With the rapid advance of cryo-ET acquisition technologies , it is possible to generate large numbers ( > 100,000 ) of subtomograms , each containing a macromolecular complex . Often , these subtomograms represent a heterogeneous sample due to variations in the structure and composition of a complex insitu form or because particles are a mixture of different complexes . In this case subtomograms must be classified . However , classification of large numbers of subtomograms is a time-intensive task and often a limiting bottleneck . This paper introduces an open source software platform , TomoMiner , for large-scale subtomogram classification , template matching , subtomogram averaging , and alignment . Its scalable and robust parallel processing allows efficient classification of tens to hundreds of thousands of subtomograms . In addition , TomoMiner provides a pre-configured TomoMinerCloud computing service permitting users without sufficient computing resources instant access to TomoMiners high-performance features .
2K_dev_1321	The Kinect sensing devices have been widely used in current Human-Computer Interaction entertainment . A fundamental issue involved is to detect users motions accurately and quickly . In this paper , we tackle it by proposing a linear algorithm , which is augmented by feature interaction . The linear property guarantees its speed whereas feature interaction captures the higher order effect from the data to enhance its accuracy . The Schatten-p norm is leveraged to integrate the main linear effect and the higher order nonlinear effect by mining the correlation between them . The resulted classification model is a desirable combination of speed and accuracy . We propose a novel solution to solve our objective function . Experiments are performed on three public Kinect-based entertainment data sets related to fitness and gaming . The results show that our method has its advantage for motion detection in a real-time Kinect entertaining environment .
2K_dev_1322	In this paper , we study automatic geo-localization of online event videos . Different from general image localization task through matching , the appearance of an environment during significant events varies greatly from its daily appearance , since there are usually crowds , decorations or even destruction when a major event happens . This introduces a major challenge : matching the event environment to the daily environment , e.g . as recorded by Google Street View . We observe that some regions in the image , as part of the environment , still preserve the daily appearance even though the whole image ( environment ) looks quite different . Based on this observation , we formulate the problem as joint saliency estimation and matching at the image region level , as opposed to the key point or whole-image level . As image-level labels of daily environment are easily generated with GPS information , we treat region based saliency estimation and matching as a weakly labeled learning problem over the training data . Our solution is to iteratively optimize saliency and the region-matching model . For saliency optimization , we derive a closed form solution , which has an intuitive explanation . For region matching model optimization , we use self-paced learning to learn from the pseudo labels generated by ( sub-optimal ) saliency values . We conduct extensive experiments on two challenging public datasets : Boston Marathon 2013 and Tokyo Time Machine . Experimental results show that our solution significantly improves over matching on whole images and the automatically learned saliency is a strong predictor of distinctive building areas .
2K_dev_1323	Utility maximization under a budget constraint is a classical problem in economics and management science . It is commonly assumed that the utility is a `` nice '' known analytic function , for example , continuous and concave . In many domains , such as marketing , increased availability of computational resources and data has enabled the development of sophisticated simulations to evaluate the impact of allocating a fixed budget among alternatives ( e.g. , marketing channels ) on outcomes , such as demand . While simulations enable high resolution evaluation of alternative budget allocation strategies , they significantly complicate the associated budget optimization problem . In particular , simulation runs are time consuming , significantly limiting the space of options that can be explored . An important second challenge is the common presence of budget complementarities , where non-negligible budget increments are required for an appreciable marginal impact from a channel . This introduces a combinatorial structure on the decision space . We propose to address these challenges by first converting the problem into a multi-choice knapsack optimization problem with unknown weights . We show that if weights ( corresponding to marginal impact thresholds for each channel ) are well approximated , we can achieve a solution within a factor of 2 of optimal , and this bound is tight . We then develop several parsimonious query algorithms for achieving this approximation in an online fashion . Experimental evaluation demonstrates the effectiveness of our approach .
2K_dev_1324	Deep generative models have achieved impressive success in recent years . Generative Adversarial Networks ( GANs ) and Variational Autoencoders ( VAEs ) , as powerful frameworks for deep generative model learning , have largely been considered as two distinct paradigms and received extensive independent study respectively . This paper establishes formal connections between deep generative modeling approaches through a new formulation of GANs and VAEs . We show that GANs and VAEs are essentially minimizing KL divergences with opposite directions and reversed latent/visible treatments , extending the two learning phases of classic wake-sleep algorithm , respectively . The unified view provides a powerful tool to analyze a diverse set of existing model variants , and enables to exchange ideas across research lines in a principled way . For example , we transfer the importance weighting method in VAE literatures for improved GAN learning , and enhance VAEs with an adversarial mechanism . Quantitative experiments show generality and effectiveness of the imported extensions .
2K_dev_1325	In this paper , we propose a coordinate descent approach to low-rank structured semidefinite programming . The approach , which we call the Mixing method , is extremely simple to implement , has no free parameters , and typically attains an order of magnitude or better improvement in optimization performance over the current state of the art . We show that for certain problems , the method is strictly decreasing and guaranteed to converge to a critical point . We then apply the algorithm to three separate domains : solving the maximum cut semidefinite relaxation , solving a ( novel ) maximum satisfiability relaxation , and solving the GloVe word embedding optimization problem . In all settings , we demonstrate improvement over the existing state of the art along various dimensions . In total , this work substantially expands the scope and scale of problems that can be solved using semidefinite programming methods .
2K_dev_1326	Mathematical and statistical models have played important roles in neuroscience , especially by describing the electrical activity of neurons recorded individually , or collectively across large networks . As the field moves forward rapidly , new challenges are emerging . For maximal effectiveness , those working to advance computational neuroscience will need to appreciate and exploit the complementary strengths of mechanistic theory and the statistical paradigm . Expected final online publication date for the Annual Review of Statistics and Its Application Volume 5 is March 7 , 2018 . Please see http : //www.annualreviews.org/catalog/pubdates.aspx for revised estimates .
2K_dev_1327	In this paper , we address the challenge of recovering a time sequence of counts from aggregated historical data . For example , given a mixture of the monthly and weekly sums , how can we find the daily counts of people infected with flu ? In general , what is the best way to recover historical counts from aggregated , possibly overlapping historical reports , in the presence of missing values ? Equally importantly , how much should we trust this reconstruction ? We propose H-FUSE , a novel method that solves above problems by allowing injection of domain knowl- edge in a principled way , and turning the task into a well- defined optimization problem . H-FUSE has the following desirable properties : ( a ) Effectiveness , recovering histori- cal data from aggregated reports with high accuracy ; ( b ) Self-awareness , providing an assessment of when the re- covery is not reliable ; ( c ) Scalability , computationally lin- ear on the size of the input data . Experiments on the real data ( epidemiology counts from the Tycho project [ 13 ] ) demonstrates that H-FUSE reconstructs the original data 30 81 % better than the least squares method .
2K_dev_1328	Occupancy estimation is an important primitive for a wide range of applications including building energy efficiency , safety , and security . In this paper , we explore the potential of using depth sensors to detect , estimate , identify , and track occupants in buildings . While depth sensors have been widely used for human detection and gesture recognition , computer vision algorithms are typically run on a powerful computer like XBOX or Intel R CoreTM i7 processor . In this work , we develop a prototype system called FORK using off-the-shelf components that performs the entire depth data processing on a cheaper and low power ARM processor in real-time . As ARM processors are extremely weak in running computer vision algorithms , FORK is designed to detect humans and track them in a very efficient way by leveraging a novel lightweight model based approach instead of traditional approaches based on histogram of oriented gradients ( HOG ) features . Unlike other camera based approaches , FORK is much less privacy invasive ( even if the sensor is compromised ) . Based on a complete implementation , real-world deployment , and extensive evaluation at realistic scenarios , we observe that FORK achieves over 99 % accuracy in real-time ( 4-9 FPS ) in occupancy estimation .
2K_dev_1329	We present OpenFace , our new open-source face recognition system that approaches state-of-the-art accuracy . Integrating OpenFace with inter-frame tracking , we build RTFace , a mechanism for denaturing video streams that selectively blurs faces according to specified policies at full frame rates . This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions . Finally , we present a scalable , privacy-aware architecture for large camera networks using RTFace .
2K_dev_1330	As video cameras proliferate , the ability to scalably capture and search their data becomes important . Scalability is improved by performing video analytics on cloudlets at the edge of the Internet , and only shipping extracted index information and meta-data to the cloud . In this setting , we describe interactive data exploration ( IDE ) , which refers to human-in-the-loop content-based retrospective search using predicates that may not have been part of any prior indexing . We also describe a new technique called just-in-time indexing ( JITI ) that improves response times in IDE .
2K_dev_1331	Due to the advent of active safety features and automated driving capabilities , the complexity of embedded computing systems within automobiles continues to increase . Such advanced driver assistance systems ( ADAS ) are inherently safetycritical and must tolerate failures in any subsystem . However , fault-tolerance in safety-critical systems has been traditionally supported by hardware replication , which is prohibitively expensive in terms of cost , weight , and size for the automotive market . Recent work has studied the use of software-based faulttolerance techniques that utilize task-level hot and cold standbys to tolerate fail-stop processor and task failures . The benefit of using standbys is maximal when a task and any of its standbys obey the placement constraint of not being co-located on the same processor . We propose a new heuristic based on a `` tiered '' placement constraint , and show that our heuristic produces a better task assignment that saves at least one processor up to 40 % of the time relative to the best known heuristic to date . We then introduce a task allocation algorithm that , for the first time to our knowledge , leverages the run-time attributes of cold standbys . Our empirical study finds that our heuristic uses no more than one additional processor in most cases relative to an optimal allocation that we construct for evaluation purposes using a creative technique . We have designed and implemented our software fault-tolerance framework in AUTOSAR , an automotive industry standard . We use this implementation to provide an experimental evaluation of our task-level fault-tolerance features . Finally , we present an analysis of the worst-case behavior of our task recovery features .
2K_dev_1332	Audience Participation Games challenge traditional assumptions about gameplay by blurring the line between audience and player , allowing audience members to impact gameplay in a meaningful way . Their recent rise in popularity has created new opportunities for game research and development . To better understand this design space , we developed several versions of two prototype games as design probes . We livestreamed them to an online audience in order to develop a framework for audience motivations and participation styles , to explore ways in which mechanics can affect audience members ' sense of agency , and to identify promising design spaces . Our results show the breadth of opportunities and challenges that designers face in creating engaging Audience Participation Games .
2K_dev_1333	Ein Verfahren zum Verwalten einer Task-Ausfuhrung in einem Mehrkernprozessor umfasst , dass ein Spinlock eingesetzt wird , um eine dynamisch erzwingbare Beschrankung eines gegenseitigen Ausschlusses zu bewirken , und ein Multiprozessorprioritatsobergrenzenprotokoll eingesetzt wird , um zu bewirken , dass die dynamisch erzwingbare Beschrankung eines gegenseitigen Ausschlusses mehrere Tasks synchronisiert , die in dem ersten und zweiten Verarbeitungskern des Mehrkernprozessors ausgefuhrt werden . A method for managing task execution in a multi-core processor includes that a spinlock is used to effect a dynamic enforceable restriction of a mutual exclusion , and a multiprocessor priority ceilings protocol is used to cause the dynamic enforceable restriction of a mutual exclusion multiple tasks synchronized , are performed in the first and second processing core of the multi-core processor .
2K_dev_1334	In this paper , we introduce Pulsar , a wireless time transfer platform that can achieve clock synchronization to better than five nanosecond between indoor or GPS-denied devices . Nanosecond-level clock synchronization is a missing capability for many real-time applications like next-generation wireless systems that leverage spatial multiplexing to improve channel capacity and provide services like time-of-flight localization . With finegrained synchronization , both clock stability and propagation delays introduce significant sources of error . Pulsar leverages a stable clock source derived from a Chip-Scale Atomic Clock ( CSAC ) along with an Ultra-WideBand ( UWB ) radio able to perform sub-nanosecond packet timestamping to estimate and correct for clock offsets . We design and evaluate a proof-ofconcept network-wide synchronization protocol for Pulsar that selects low-jitter links to both estimate the location of nodes and reduce cumulative synchronization error across multiple hops . The Pulsar platform and protocol together provide a phase synchronized one pulse per second ( 1PPS ) signal and 10 MHz reference clock that can be easily integrated with typical enduser applications like software-defined radios and communication systems . We experimentally evaluate the Pulsar platform in terms of clock synchronization accuracy , Allan deviation between pairwise clocks and ranging accuracy to show a clock synchronization of better than five nanoseconds per hop with an average of 2.12 ns and a standard deviation of 0.84 ns . The platform is able to identify and avoid clock error in cases where there is heavy multi-path or non-Line-of-Sight signals .
2K_dev_1335	Despite their growing prominence , optimization in generative adversarial networks ( GANs ) is still a poorly-understood topic . In this paper , we analyze the `` gradient descent '' form of GAN optimization ( i.e. , the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters ) . We show that even though GAN optimization does not correspond to a convex-concave game , even for simple parameterizations , under proper conditions , equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation . On the other hand , we show that the recently-proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium . Motivated by this stability analysis , we propose an additional regularization term for gradient descent GAN updates , which is able to guarantee local stability for both the WGAN and for the traditional GAN , and also shows practical promise in speeding up convergence and addressing mode collapse .
2K_dev_1336	Consider a stream of retweet events - how can we spot fraudulent lock-step behavior in such multi-aspect data ( i.e. , tensors ) evolving over time ? Can we detect it in real time , with an accuracy guarantee ? Past studies have shown that dense subtensors tend to indicate anomalous or even fraudulent behavior in many tensor data , including social media , Wikipedia , and TCP dumps . Thus , several algorithms have been proposed for detecting dense subtensors rapidly and accurately . However , existing algorithms assume that tensors are static , while many real-world tensors , including those mentioned above , evolve over time . We propose DENSESTREAM , an incremental algorithm that maintains and updates a dense subtensor in a tensor stream ( i.e. , a sequence of changes in a tensor ) , and DENSESALERT , an incremental algorithm spotting the sudden appearances of dense subtensors . Our algorithms are : ( 1 ) Fast and `` any time '' : updates by our algorithms are up to a million times faster than the fastest batch algorithms , ( 2 ) Provably accurate : our algorithms guarantee a lower bound on the density of the subtensor they maintain , and ( 3 ) Effective : our DENSESALERT successfully spots anomalies in real-world tensors , especially those overlooked by existing algorithms .
2K_dev_1337	Abstract : The multidisciplinary goal was to develop an integrated conceptualization of the mid-level encoding of 3D object structure from multiple surface cues . Psychophysical studies showed that depth continuity is a prerequisite for facilitation of Gabor target detection in the context of flanking Gabors , and that , similarly , surface continuity in purely disparity-defined slanted surfaces was strongly enhanced in distributed patch detections as a function of stimulus duration in this dual discrimination task . The unprecedented dips of performance reduction in the component psychometric functions was captured in a computational model based on a novel Leaky Drift Diffusion Theory that we developed for the underlying neural signals , which can serve as an analytic basis for the time course of all neural decision processes . The time course of depth surface perception was studied in a coordinated trio of psychophysical , neurophysiological and functional imaging studies , showing that the perceptual processing of disparity and integration of 3D surface information across depth cues has time courses of several seconds , attesting to complexity of the neural processing hardware . Three complementary computational modeling projects from three collaborating laboratories showed how surface reconstruction could be accomplished across the typically sparse depth information available , and integrated among sparse , incommensurate cue modalities .
2K_dev_1338	Deep learning models can take weeks to train on a single GPU-equipped machine , necessitating scaling out DL training to a GPU-cluster . However , current distributed DL implementations can scale poorly due to substantial parameter synchronization over the network , because the high throughput of GPUs allows more data batches to be processed per unit time than CPUs , leading to more frequent network synchronization . We present Poseidon , an efficient communication architecture for distributed DL on GPUs . Poseidon exploits the layered model structures in DL programs to overlap communication and computation , reducing bursty network communication . Moreover , Poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer , according to layer properties and the number of machines . We show that Poseidon is applicable to different DL frameworks by plugging Poseidon into Caffe and TensorFlow . We show that Poseidon enables Caffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines , even with limited bandwidth ( 10GbE ) and the challenging VGG19-22K network for image classification . Moreover , Poseidon-enabled TensorFlow achieves 31.5x speed-up with 32 single-GPU machines on Inception-V3 , a 50 % improvement over the open-source TensorFlow ( 20x speed-up ) .
2K_dev_1339	Recent advances in Unmanned Aerial Vehicles ( UAVs ) have enabled countless new applications in the domain of aerial sensing . In scenarios such as intrusion detection , target tracking and facility monitoring it is important to reach a given area of interest ( AOI ) , and create an online data streaming connection to a monitoring ground station ( GS ) for immediate delivery of content to the operator . In previous work , we showed that a multi-hop line network can increase the range of the mission by finding the optimal number of relay UAVs , and their optimal placement . In this demo , we show that CSMA ( typical 802.11 's medium access protocol ) behaves poorly in this type of networks due to mutual interference , and that TDMA is a better alternative . We will also discuss how changing slot width online can overcome typical and less known TDMA inefficiencies , and therefore reach maximum end-to-end throughput and low delay .
2K_dev_1340	Gaussian belief propagation ( BP ) has been widely used for distributed inference in large-scale networks such as the smart grid , sensor networks , and social networks , where local measurements/observations are scattered over a wide geographical area . One particular case is when two neighboring agents share a common observation . For example , to estimate voltage in the direct current ( DC ) power flow model , the current measurement over a power line is proportional to the voltage difference between two neighboring buses . When applying the Gaussian BP algorithm to this type of problem , the convergence condition remains an open issue . In this paper , we analyze the convergence properties of Gaussian BP for this pairwise linear Gaussian model . We show analytically that the updating information matrix converges at a geometric rate to a unique positive definite matrix with arbitrary positive semidefinite initial value and further provide the necessary and sufficient convergence condition for the belief mean vector to the optimal estimate .
2K_dev_1341	Developing a remote exploit is not easy . It requires a comprehensive understanding of a vulnerability and delicate techniques to bypass defense mechanisms . As a result , attackers may prefer to reuse an existing exploit and make necessary changes over developing a new exploit from scratch . One such adaptation is the replacement of the original shellcode ( i.e. , the attacker-injected code that is executed as the final step of the exploit ) in the original exploit with a replacement shellcode , resulting in a modified exploit that carries out the actions desired by the attacker as opposed to the original exploit author . We call this a shellcode transplant . Current automated shellcode placement methods are insufficient because they over-constrain the replacement shellcode , and so can not be used to achieve shellcode transplant . For example , these systems consider the shellcode as an integrated memory chunk and require that the execution path of the modified exploit must be same as the original one . To resolve these issues , we present ShellSwap , a system that uses symbolic tracing , with a combination of shellcode layout remediation and path kneading to achieve shellcode transplant . We evaluated the ShellSwap system on a combination of 20 exploits and 5 pieces of shellcode that are independently developed and different from the original exploit . Among the 100 test cases , our system successfully generated 88 % of the exploits .
2K_dev_1342	The availability of large idea repositories ( e.g. , the U.S. patent database ) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems . However , finding useful analogies in these large , messy , real-world repositories remains a persistent challenge for either human or automated methods . Previous approaches include costly hand-created databases that have high relational structure ( e.g. , predicate calculus representations ) but are very sparse . Simpler machine-learning/information-retrieval similarity metrics can scale to large , natural-language datasets , but struggle to account for structural similarity , which is central to analogy . In this paper we explore the viability and value of learning simpler structural representations , specifically , `` problem schemas '' , which specify the purpose of a product and the mechanisms by which it achieves that purpose . Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions . We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods . In an ideation experiment , analogies retrieved by our models significantly increased people 's likelihood of generating creative ideas compared to analogies retrieved by traditional methods . Our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations .
2K_dev_1343	Abstract Whole-genome assemblies of 19 placental mammals and two outgroup species were used to reconstruct the order and orientation of syntenic fragments in chromosomes of the eutherian ancestor and six other descendant ancestors leading to human . For ancestral chromosome reconstructions , we developed an algorithm ( DESCHRAMBLER ) that probabilistically determines the adjacencies of syntenic fragments using chromosome-scale and fragmented genome assemblies . The reconstructed chromosomes of the eutherian , boreoeutherian , and euarchontoglires ancestor each included > 80 % of the entire length of the human genome , whereas reconstructed chromosomes of the most recent common ancestor of simians , catarrhini , great apes , and humans and chimpanzees included > 90 % of human genome sequence . These high-coverage reconstructions permitted reliable identification of chromosomal rearrangements over 105 My of eutherian evolution . Orangutan was found to have eight chromosomes that were completely conserved in homologous sequence order and orientation with the eutherian ancestor , the largest number for any species . Ruminant artiodactyls had the highest frequency of intrachromosomal rearrangements , and interchromosomal rearrangements dominated in murid rodents . A total of 162 chromosomal breakpoints in evolution of the eutherian ancestral genome to the human genome were identified ; however , the rate of rearrangements was significantly lower ( 0.80/My ) during the first 60 My of eutherian evolution , then increased to greater than 2.0/My along the five primate lineages studied . Our results significantly expand knowledge of eutherian genome evolution and will facilitate greater understanding of the role of chromosome rearrangements in adaptation , speciation , and the etiology of inherited and spontaneously occurring diseases .
2K_dev_1344	With the explosion in the availability of user-generated videos documenting any conflicts and human rights abuses around the world , analysts and researchers increasingly find themselves overwhelmed with massive amounts of video data to acquire and analyze useful information . In this paper , we develop a temporal localization framework for intense audio events in videos which addresses the problem . The proposed method utilizes Localized Self-Paced Reranking ( LSPaR ) to refine the localization results . LSPaR utilizes samples from easy to noisier ones so that it can overcome the noisiness of the initial retrieval results from user-generated videos . We show our framework 's efficacy on localizing intense audio event like gunshot , and further experiments also indicate that our methods can be generalized to localizing other audio events in noisy videos .
2K_dev_1345	We revisit the classic problem of designing voting rules that aggregate objective opinions , in a setting where voters have noisy estimates of a true ranking of the alternatives . Previous work has replaced structural assumptions on the noise with a worst-case approach that aims to choose an outcome that minimizes the maximum error with respect to any feasible true ranking . This approach underlies algorithms that have recently been deployed on the social choice website RoboVote.org . We take a less conservative viewpoint by minimizing the average error with respect to the set of feasible ground truth rankings . We derive ( mostly sharp ) analytical bounds on the expected error and establish the practical benefits of our approach through experiments .
2K_dev_1346	Low-income families pay substantial portions of their total expenditure on household energy bills , making them vulnerable to rising energy costs . Habitat for Humanity houses are built for low-income families and made affordable with volunteer work and construction material donations . Hence , the trade-off between the homesO initial construction costs and their life-time energy costs must be evaluated carefully . This paper targets to support better-informed decisions that balance the affordability of certain construction materials with their potential for energy efficiency . In collaboration with Habitat for Humanity of Westchester , we created an energy simulation model of an existing low-income house and calculated the homeOs annual energy usage with different design alternatives for windows and walls . The resulting estimated annual energy savings are then evaluated alongside their initial investment costs , which were retrieved from RS Means standard construction cost data and quotations from industry . The results show that it is possible to reduce the energy cost of these houses without significantly increasing the construction costs through exploration of different wall and window options . While specific enclosure suggestions apply to this case-study , the utilized approach on exploring different options to identify opportunities to save energy can be used to understand impact on the lives of low-income families .
2K_dev_1347	We are interested in the problem of dividing a cake -- a heterogeneous divisible good -- among n players , in a way that is e- equitable : every pair of players must have the same value for their own allocated pieces , up to a difference of at most e. It is known that such allocations can be computed using O ( n ln ( 1/e ) ) operations in the standard Robertson-Webb Model . We establish a lower bound of ( ln ( 1/e ) /lnln ( 1/e ) ) on the complexity of this problem , which is almost tight for a constant number of players . Importantly , our result implies that allocations that are exactly equitable can not be computed .
2K_dev_1348	In the era of social media , a large number of user-generated videos are uploaded to the Internet every day , capturing events all over the world . Reconstructing the event truth based on information mined from these videos has been an emerging challenging task . Temporal alignment of videos in the wild which capture different moments at different positions with different perspectives is the critical step . In this paper , we propose a hierarchical approach to synchronize videos . Our system utilizes clustered audio-signatures to align video pairs . Global alignment for all videos is then achieved via forming alignable video groups with self-paced learning . Experiments on the Boston Marathon dataset show that the proposed method achieves excellent precision and robustness .
2K_dev_1349	Mainstream crowdwork platforms treat microtasks as indivisible units ; however , in this article , we propose that there is value in re-examining this assumption . We argue that crowdwork platforms can improve their value proposition for all stakeholders by supporting subcontracting within microtasks . After describing the value proposition of subcontracting , we then define three models for microtask subcontracting : real-time assistance , task management , and task improvement , and reflect on potential use cases and implementation considerations associated with each . Finally , we describe the outcome of two tasks on Mechanical Turk meant to simulate aspects of subcontracting . We reflect on the implications of these findings for the design of future crowd work platforms that effectively harness the potential of subcontracting workflows .
2K_dev_1350	Achieving an optimized control schema for building automation systems relies on the accurate collection of environmental modalities and accurate modeling of how the environment reacts to changes . Recent advances in indoor localization coupled with the influx of smart devices have created a unique and efficient method of collecting these modalities in real-time . This allows for quicker adaptation of the control schema by increasing both temporal and spatial information resulting in a reduction of operating costs . Visualizing these environmental modalities with fine grained positional accuracy allows for a better understanding of inter-dependency between modalities and also allows for a comparison between the real-time data across several modalities and pre-defined models .
2K_dev_1351	The world is full of physical interfaces that are inaccessible to blind people , from microwaves and information kiosks to thermostats and checkout terminals . Blind people can not independently use such devices without at least first learning their layout , and usually only after labeling them with sighted assistance . To address this problem , we introduce VizLens -- -a robust and interactive screen reader for interfaces in the real world . VizLens users take a picture of an interface they would like to use , it is interpreted quickly and robustly by multiple crowd workers in parallel , and then computer vision is able to give interactive feedback and guidance to users to help them use the interface in real time . Built on top of VizLens , we developed automatically generating tactile overlays to physical interfaces to provide blind people with a permanent static solution . We introduce Facade -- -a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel .
2K_dev_1352	Crowd work is an increasingly prevalent and important kind of work . Because of its flexible nature , crowd work may offer benefits for people with disabilities . Unfortunately , people with disabilities currently lack access to much of this work because the tasks that are posted are often inaccessible . In this paper , we first characterize the accessibility of the tasks posted to a popular crowd marketplace , Amazon Mechanical Turk , by performing manual and automatic checks on 120 tasks from several common types . We then outline research directions that could have positive impact on this problem . Given ongoing and upcoming changes to the world economy and technological progress , we believe it is important to find a way to make sure people with disabilities are able to equally participate in this kind of work .
2K_dev_1353	Providing navigation assistance to people with visual impairments often requires augmenting the environment with after-market technology . However , installing navigation infrastructure in large environments requires a critical mass of trained personnel . Recruiting , training and managing participants for such a task is difficult . LuzDeploy is a computational method to recruit , instruct and orchestrate volunteers to perform physical crowdsourcing tasks . We use LuzDeploy to orchestrate volunteers to install physical infrastructure for the navigation assistance of people with visual impairments . Our system provides on-the-go enrollment so that volunteers can participate to the collective action whenever they have time , coming and leaving as needed . Providing automated instructions also allows to avoid instructing participants directly , so experts do not need to be available on-site .
2K_dev_1354	What can humans compute in their heads ? We are thinking of a variety of Crypto Protocols , games like Sudoku , Crossword Puzzles , Speed Chess , and so on . The intent of this paper is to apply the ideas and methods of theoretical computer science to better understand what humans can compute in their heads . For example , can a person compute a function in their head so that an eavesdropper with a powerful computer -- - who sees the responses to random input -- - still can not infer responses to new inputs ? To address such questions , we propose a rigorous model of human computation and associated measures of complexity . We apply the model and measures first and foremost to the problem of ( 1 ) humanly computable password generation , and then consider related problems of ( 2 ) humanly computable `` one-way functions '' and ( 3 ) humanly computable `` pseudorandom generators '' . The theory of Human Computability developed here plays by different rules than standard computability , and this takes some getting used to . For reasons to be made clear , the polynomial versus exponential time divide of modern computability theory is irrelevant to human computation . In human computability , the step-counts for both humans and computers must be more concrete . Specifically , we restrict the adversary to at most 10^24 ( Avogadro number of ) steps . An alternate view of this work is that it deals with the analysis of algorithms and counting steps for the case that inputs are small as opposed to the usual case of inputs large-in-the-limit .
2K_dev_1355	How do the k-core structures of real-world graphs look like ? What are the common patterns and the anomalies ? How can we exploit them for applications ? A k-core is the maximal subgraph in which all vertices have degree at least k. This concept has been applied to such diverse areas as hierarchical structure analysis , graph visualization , and graph clustering . Here , we explore pervasive patterns related to k-cores and emerging in graphs from diverse domains . Our discoveries are : ( 1 ) Mirror Pattern : coreness ( i.e. , maximum k such that each vertex belongs to the k-core ) is strongly correlated with degree . ( 2 ) Core-Triangle Pattern : degeneracy ( i.e. , maximum k such that the k-core exists ) obeys a 3-to-1 power-law with respect to the count of triangles . ( 3 ) Structured Core Pattern : degeneracycores are not cliques but have non-trivial structures such as coreperiphery and communities . Our algorithmic contributions show the usefulness of these patterns . ( 1 ) Core-A , which measures the deviation from Mirror Pattern , successfully spots anomalies in real-world graphs , ( 2 ) Core-D , a single-pass streaming algorithm based on Core-Triangle Pattern , accurately estimates degeneracy up to 12\ ( \times \ ) faster than its competitor . ( 3 ) Core-S , inspired by Structured Core Pattern , identifies influential spreaders up to 17\ ( \times \ ) faster than its competitors with comparable accuracy .
2K_dev_1356	Systems for providing mixed physical-virtual interaction on desktop surfaces have been proposed for decades , though no such systems have achieved widespread use . One major factor contributing to this lack of acceptance may be that these systems are not designed for the variety and complexity of actual work surfaces , which are often in flux and cluttered with physical objects . In this paper , we use an elicitation study and interviews to synthesize a list of ten interactive behaviors that desk-bound , digital interfaces should implement to support responsive cohabitation with physical objects . As a proof of concept , we implemented these interactive behaviors in a working augmented desk system , demonstrating their imminent feasibility .
2K_dev_1357	Given a bipartite graph of users and the products that they review , or followers and followees , how can we detect fake reviews or follows ? Existing fraud detection methods ( spectral , etc . ) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph . Fraudsters can evade these methods using camouflage , by adding reviews or follows with honest targets so that they look normal . Even worse , some fraudsters use hijacked accounts from honest users , and then the camouflage is indeed organic . Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts . We propose FRAUDAR , an algorithm that ( a ) is camouflage resistant , ( b ) provides upper bounds on the effectiveness of fraudsters , and ( c ) is effective in real-world data . Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud . Additionally , in real-world experiments with a Twitter follower -- followee graph of 1.47 billion edges , FRAUDAR successfully detected a subgraph of more than 4 , 000 detected accounts , of which a majority had tweets showing that they used follower-buying services .
2K_dev_1358	Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling . In this paper , we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors . Our method enables efficient inference in the low-dimensional embedding space , reducing previous cubic or quadratic time complexity to linear w.r.t the topic size . We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence . Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results , without sacrificing modeling quality by providing competitive or superior performance in document classification and retrieval .
2K_dev_1359	A system for automatically managing the delivery of digital media assets allocates media assets to video programs so that audience members will receive the digital media assets when they view the video programs . An example is the automated allocation of sponsored videos to television programs . The system includes data stores , a digital media server , and a campaign manager system . The campaign manager system receives consumption data corresponding to a first campaign , along with an allocation value and other parameters , and uses that information to automatically allocate digital media assets to additional digital programming files and their corresponding video programs for a second campaign .
2K_dev_1360	Intelligent personalization systems are becoming increasingly reliant on contextually-relevant devices and services , such as those available within modern IoT deployments . An IoT context may emerge -- -or become pervasive -- -when the intelligent system generates knowledge from dialogue-based interactions with the end-user ; the context is strengthened even further by incorporating state representations about the environment ( e.g. , generated from wireless sensor data ) into the knowledge graph . This is crucial for pervasive applications like digital assistance in IoT , where context-aware systems need to adapt quickly : activities like leaving work home-bound , driving to the grocery store , arriving at home , and walking the dog , for example , can occur in a relatively short period of time -- -during which an intelligent assistant must be able to support user requests in a consistent and coherent manner . Given that computational ontologies can serve as semantic models for heterogeneous data , they are becoming increasingly viable for reasoning across different IoT contexts . This involves : ( a ) federation and dynamic pruning of multiple modular ontologies , ideally , to comprehensively capture only the knowledge that will facilitate execution of a multi-context task ; ( b ) fast consistency-checking and ontology-based inferences , aided by rules-based execution environments that can evaluate/transform ambient wireless sensor network ( WSN ) data , in real-time ; and ( c ) run-time execution of ontology-based control procedures , through rule-engine actuation commands sent across the WSN . Only by realizing these functionalities may intelligent systems be capable of reasoning over device properties , system states , and user activities , while appropriately delegating commands to other intelligent agents or other relevant IoT services . In this poster , we illustrate how a multi-context knowledge base can be structured on the basis of modular ontologies and integrated with a distributed rules-based inference engine in multiple smart-building environments , in order to enable scalable contextual reasoning for intelligent assistance . Preliminary results are also discussed . This work is conducted through the partnership of Bosch Research Pittsburgh and Carnegie Mellon University ( CMU ) , and is in partial satisfaction of CMU 's Bosch Energy Research Network ( BERN ) grant , awarded for developments in intelligent building solutions . The approach we describe is also partially based on the Ubiquitous Personal Assistant ( UPA ) project , Bosch Research 's largest research initiative worldwide .
2K_dev_1361	Audio transcription is an important task for making content accessible to people who are deaf or hard of hearing . Much of the transcription work is increasingly done by crowd workers , people online who pick up the work as it becomes available often in small bits at a time . Whereas work typically provides a ladder for skill development -- a series of opportunities to acquire new skills that lead to advancement -- crowd transcription work generally does not . To demonstrate how crowd work might create a skill ladder , we created Scopist , a JavaScript application for learning an efficient text-entry method known as stenotype while doing audio transcription tasks . Scopist facilitates on-the-job learning to prepare crowd workers for remote , real-time captioning by supporting both touch-typing and chording . Real-time captioning is a difficult skill to master but is important for making live events accessible . We conducted 3 crowd studies of Scopist focusing on Scopist 's performance and support for learning . We show that Scopist can distinguish touch-typing from stenotyping with 94 % accuracy . Our research demonstrates a new way for workers on crowd platforms to align their work and skill development with the accessibility domain while they work .
2K_dev_1362	Situational awareness involves the timely acquisition of knowledge about real-world events , distillation of those events into higher-level conceptual constructs , and their synthesis into a coherent context-sensitive view . We explore how convergent trends in video sensing , crowd sourcing and edge computing can be harnessed to create a shared real-time information system for situational awareness in vehicular systems that span driverless and drivered vehicles .
2K_dev_1363	We have entered the era of big data . Massive datasets , surpassing terabytes and petabytes in size are now commonplace . They arise in numerous settings in science , government , and enterprises , and technology exists by which we can collect and store such massive amounts of information . Yet , making sense of these data remains a fundamental challenge . We lack the means to exploratively analyze databases of this scale . Currently , few technologies allow us to freely `` wander '' around the data , and make discoveries by following our intuition , or serendipity . While standard data mining aims at finding highly interesting results , it is typically computationally demanding and time consuming , thus may not be well-suited for interactive exploration of large datasets . Interactive data mining techniques that aptly integrate human intuition , by means of visualization and intuitive human-computer interaction techniques , and machine computation support have been shown to help people gain significant insights into a wide range of problems . However , as datasets are being generated in larger volumes , higher velocity , and greater variety , creating effective interactive data mining techniques becomes a much harder task .
2K_dev_1364	Social media has become a popular and important tool for human communication . However , due to this popularity , spam and the distribution of malicious content by computer-controlled users , known as bots , has become a widespread problem . At the same time , when users use social media , they generate valuable data that can be used to understand the patterns of human communication . In this article , we focus on the following important question : Can we identify and use patterns of human communication to decide whether a human or a bot controls a user ? The first contribution of this article is showing that the distribution of inter-arrival times ( IATs ) between postings is characterized by following four patterns : ( i ) heavy-tails , ( ii ) periodic-spikes , ( iii ) correlation between consecutive values , and ( iv ) bimodallity . As our second contribution , we propose a mathematical model named Act-M ( Activity Model ) . We show that Act-M can accurately fit the distribution of IATs from social media users . Finally , we use Act-M to develop a method that detects if users are bots based only on the timing of their postings . We validate Act-M using data from over 55 million postings from four social media services : Reddit , Twitter , Stack-Overflow , and Hacker-News . Our experiments show that Act-M provides a more accurate fit to the data than existing models for human dynamics . Additionally , when detecting bots , Act-M provided a precision higher than 93 % and 77 % with a sensitivity of 70 % for the Twitter and Reddit datasets , respectively .
2K_dev_1365	In this work , we introduce Video Question Answering in the temporal domain to infer the past , describe the present and predict the future . We present an encoderdecoder approach using Recurrent Neural Networks to learn the temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions . We explore approaches for finer understanding of video content using the question form of fill-in-the-blank , and collect our Video Context QA dataset consisting of 109,895 video clips with a total duration of more than 1000 h from existing TACoS , MPII-MD and MEDTest 14 datasets . In addition , 390,744 corresponding questions are generated from annotations . Extensive experiments demonstrate that our approach significantly outperforms the compared baselines .
2K_dev_1366	Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain . This paper aims at generating plausible natural language sentences , whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics . We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures . With differentiable approximation to discrete text samples , explicit constraints on independent attribute controls , and efficient collaborative learning of generator and discriminators , our model learns highly interpretable representations from even only word annotations , and produces realistic sentences with desired attributes . Quantitative evaluation validates the accuracy of sentence and attribute generation .
2K_dev_1367	Nuclear organization has an important role in determining genome function ; however , it is not clear how spatiotemporal organization of the genome relates to functionality . To elucidate this relationship , a high-throughput method for tracking any locus of interest is desirable . Here , we report an efficient and scalable method named SHACKTeR ( Short Homology and CRISPR/Cas9-mediated Knock-in of a TetO Repeat ) for live cell imaging of specific chromosomal regions . Compared to alternatives , our method does not require a nearby repetitive sequence and it requires only two modifications to the genome : CRISPR/Cas9-mediated knock-in of an optimized TetO repeat and its visualization by TetR-EGFP expression . Our simplified knock-in protocol , utilizing short homology arms integrated by PCR , was successful at labeling 9 different loci in HCT116 cells with up to 20 % efficiency . These loci included both nuclear speckle-associated , euchromatin regions and nuclear lamina-associated , heterochromatin regions . We anticipate the general applicability and scalability of our method will enhance causative analyses between gene function and compartmentalization in a high-throughput manner .
2K_dev_1368	Robust hand detection and classification is one of the most crucial pre-processing steps to support human computer interaction , driver behavior monitoring , virtual reality , etc . This problem , however , is very challenging due to numerous variations of hand images in real-world scenarios . This work presents a novel approach named Multiple Scale Region-based Fully Convolutional Networks ( MSRFCN ) to robustly detect and classify human hand regions under various challenging conditions , e.g . occlusions , illumination , low-resolutions . In this approach , the whole image is passed through the proposed fully convolutional network to compute score maps . Those score maps with their position-sensitive properties can help to efficiently address a dilemma between translation-invariance in classification and detection . The method is evaluated on the challenging hand databases , i.e . the Vision for Intelligent Vehicles and Applications ( VIVA ) Challenge , Oxford hand dataset and compared against various recent hand detection methods . The experimental results show that our proposed MS-FRCN approach consistently achieves the state-of-the-art hand detection results , i.e . Average Precision ( AP ) / Average Recall ( AR ) of 95.1 % / 94.5 % at level 1 and 86.0 % / 83.4 % at level 2 , on the VIVA challenge . In addition , the proposed method achieves the state-of-the-art results for left/right hand and driver/passenger classification tasks on the VIVA database with a significant improvement on AP/AR of ~7 % and ~13 % for both classification tasks , respectively . The hand detection performance of MS-RFCN reaches to 75.1 % of AP and 77.8 % of AR on Oxford database .
2K_dev_1369	Abstract Server-side variability the idea that the same job can take longer to run on one server than another due to server-dependent factors isan increasingly important concern in many queueing systems . One strategy for overcoming server-side variability to achieve low response time is redundancy , under which jobs create copies of themselves and send these copies to multiple different servers , waiting for only one copy to complete service . Most of the existing theoretical work on redundancy has focused on developing bounds , approximations , and exact analysis to study the response time gains offered by redundancy . However , response time is not the only important metric in redundancy systems : in addition to providing low overall response time , the system should also be fair in the sense that no job class should have a worse mean response time in the system with redundancy than it did in the system before redundancy is allowed . In this paper we use scheduling to address the simultaneous goals of ( 1 ) achieving low response time and ( 2 ) maintaining fairness across job classes . We develop new exact analysis for per-class response time under First-Come First-Served ( FCFS ) scheduling for a general type of system structure ; our analysis shows that FCFS can be unfair in that it can hurt non-redundant jobs . We then introduce the Least Redundant First ( LRF ) scheduling policy , which we prove is optimal with respect to overall system response time , but which can be unfair in that it can hurt the jobs that become redundant . Finally , we introduce the Primaries First ( PF ) scheduling policy , which is provably fair and also achieves excellent overall mean response time .
2K_dev_1370	It is our great pleasure to welcome you to the 2017 ACM International Conference on Multimedia Retrieval ( ICMR 2017 ) . This year 's conference continues a 17-year tradition of being the premier forum for presentation of research results and experience reports on core topics in multimedia retrieval , as well as the broader set of topics that must be addressed to ensure that multimedia retrieval technologies are of practical use in real-world use cases . Special emphasis was placed on topics related to large-scale indexing , user interaction , exploiting diverse and multimodal data , and domain-specific challenges . This annual international conference , combines the long-standing experience of the former ACM CIVR ( International Conference on Image and Video Retrieval ) and ACM MIR ( International Conference on Multimedia Information Retrieval ) series , and was set up in 2011 to highlight the state of the art in multimedia ( e.g. , text , image , video , audio , sensor data , and 3D ) retrieval . This year 's edition has attracted submissions from all over the world : Asia , Canada , Australia , Africa , Europe , and the United States . In total , it received 178 valid submissions , which were reviewed by the program committee . For the classic research paper tracks , out of 95 full paper submissions , 33 were accepted -- 20 for full oral presentation and 13 for spotlight oral presentation ( acceptance rate of 34 % ; full oral paper acceptance rate of 21 % ) -- and , out of 35 short paper submissions , 13 were accepted ( acceptance rate of 37 % ) . Next to full and short paper tracks , the conference includes a diversity of other tracks : open-source software ( acceptance rate of 80 % ) , technical demonstrations ( acceptance rate of 85 % ) , brave new ideas papers ( acceptance rate of 75 % ) , doctoral symposium papers ( acceptance rate of 80 % ) , and special session papers ( acceptance rate of 66 % ) . In addition to the tracks of the main conference , two workshops are offered : `` Wearable MultiMedia '' and `` Multimedia Forensics and Security '' . The conference program is complemented by a full day of tutorials on `` Video Indexing , Search , Detection , and Description '' , keynote talks given by world-class researchers ( `` Searching for a thing '' , Arnold W.M . Smeulders & Ran Tao , and `` Making cultural visits with a smart mate '' , Alberto Del Bimbo ) , industry keynote talks given by leading companies ( `` With 5G Approaching , How will Audio/Video Technology that Serves 800 Million QQ Users Bring Forth New Ideas '' , Xiaozheng Huang , Tencent , China , `` Information Retrieval from Multi-Sensor Data for Enriching Location Services at HERE Technologies '' , Matei Stroil , HERE , USA , and `` Intelligently Connecting People with Information '' , Changhu Wang , Toutiao AI Lab , China ) and a panel session on `` Indicators of Innovation and Signs of Success : Recognizing which multimedia information retrieval research is most valuable to industry '' .
2K_dev_1371	Generative Adversarial Networks ( GANs ) have recently achieved significant improvement on paired/unpaired image-to-image translation , such as photo $ \rightarrow $ sketch and artist painting style transfer . However , existing models can only be capable of transferring the low-level information ( e.g . color or texture changes ) , but fail to edit high-level semantic meanings ( e.g. , geometric structure or content ) of objects . On the other hand , while some researches can synthesize compelling real-world images given a class label or caption , they can not condition on arbitrary shapes or structures , which largely limits their application scenarios and interpretive capability of model results . In this work , we focus on a more challenging semantic manipulation task , which aims to modify the semantic meaning of an object while preserving its own characteristics ( e.g . viewpoints and shapes ) , such as cow $ \rightarrow $ sheep , motor $ \rightarrow $ bicycle , cat $ \rightarrow $ dog . To tackle such large semantic changes , we introduce a contrasting GAN ( contrast-GAN ) with a novel adversarial contrasting objective . Instead of directly making the synthesized samples close to target data as previous GANs did , our adversarial contrasting objective optimizes over the distance comparisons between samples , that is , enforcing the manipulated data be semantically closer to the real data with target category than the input data . Equipped with the new contrasting objective , a novel mask-conditional contrast-GAN architecture is proposed to enable disentangle image background with object semantic changes . Experiments on several semantic manipulation tasks on ImageNet and MSCOCO dataset show considerable performance gain by our contrast-GAN over other conditional GANs . Quantitative results further demonstrate the superiority of our model on generating manipulated results with high visual fidelity and reasonable object semantics .
2K_dev_1372	The influence of motor cortex on muscles during different behaviors is incompletely understood . In this issue of Neuron , Miri etal . ( 2017 ) show that the population activity patterns produced by motor cortex during different behaviors determine the selective routing of signals along different pathways between motor cortex and muscles .
2K_dev_1373	Future frame prediction in videos is a promising avenue for unsupervised video representation learning . Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video . However , existing methods focus on directly hallucinating pixel values , resulting in blurry predictions . In this paper , we develop a dual motion Generative Adversarial Net ( GAN ) architecture , which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism . The primal future-frame prediction and dual future-flow prediction form a closed loop , generating informative feedback signals to each other for better video prediction . To make both synthesized future frames and flows indistinguishable from reality , a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames , while the future-frame prediction in turn leads to realistic optical flows . Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder , which is based on variational autoencoders . Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows . Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning .
2K_dev_1374	How do social groups , such as Facebook groups and Wechat groups , dynamically evolve over time ? How do people join the social groups , uniformly or with burst ? What is the pattern of people quitting from groups ? Is there a simple universal model to depict the come-and-go patterns of various groups ? In this article , we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users . We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR . For example , we observe both diffusion and non-diffusion mechanism in the group joining process , and power-law decay in group quitting process , rather than exponential decay as expected in SIR model . Therefore , we propose a new model come N go , a concise yet flexible dynamic model for group evolution . Our model has the following advantages : ( a ) Unification power : it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation . ( b ) Succinctness and interpretability : it contains only six parameters with clear physical meanings . ( c ) Accuracy : it can capture various kinds of group evolution patterns preciously , and the goodness of fit increases by 58 % over baseline . ( d ) Usefulness : it can be used in multiple application scenarios , such as forecasting and pattern discovery . Furthermore , our model can provide insights about different evolution patterns of social groups , and we also find that group structure and its evolution has notable relations with temporal patterns of group evolution .
2K_dev_1375	Computer vision based technologies have seen widespread adoption over the recent years . This use is not limited to the rapid adoption of facial recognition technology but extends to facial expression recognition , scene recognition and more . These developments raise privacy concerns and call for novel solutions to ensure adequate user awareness , and ideally , control over the resulting collection and use of potentially sensitive data . While cameras have become ubiquitous , most of the time users are not even aware of their presence . In this paper we introduce a novel distributed privacy infrastructure for the Internet-of-Things and discuss in particular how it can help enhance user 's awareness of and control over the collection and use of video data about them . The infrastructure , which has undergone early deployment and evaluation on two campuses , supports the automated discovery of IoT resources and the selective notification of users . This includes the presence of computer vision applications that collect data about users . In particular , we describe an implementation of functionality that helps users discover nearby cameras and choose whether or not they want their faces to be denatured in the video streams .
2K_dev_1376	Information cascades are ubiquitous in both physical society and online social media , taking on large variations in structures , dynamics and semantics . Although the dynamics and semantics of information cascades have been studied , the structural patterns and their correlations with dynamics and semantics are largely unknown . Here we explore a large-scale dataset including $ 432 $ million information cascades with explicit records of spreading traces , spreading behaviors , information content as well as user profiles . We find that the structural complexity of information cascades is far beyond the previous conjectures . We first propose a ten-dimensional metric to quantify the structural characteristics of information cascades , reflecting cascade size , silhouette , direction and activity aspects . We find that bimodal law governs majority of the metrics , information flows in cascades have four directions , and the self-loop number and average activity of cascades follows power law . We then analyze the high-order structural patterns of information cascades . Finally , we evaluate to what extent the structural features of information cascades can explain its dynamic patterns and semantics , and finally uncover some notable implications of structural patterns in information cascades . Our discoveries also provide a foundation for the microscopic mechanisms for information spreading , potentially leading to implications for cascade prediction and outlier detection .
2K_dev_1377	Extreme Classification comprises multi-class or multi-label prediction where there is a large number of classes , and is increasingly relevant to many real-world applications such as text and image tagging . In this setting , standard classification methods , with complexity linear in the number of classes , become intractable , while enforcing structural constraints among classes ( such as low-rank or tree-structure ) to reduce complexity often sacrifices accuracy for efficiency . The recent PD-Sparse method addresses this via an algorithm that is sub-linear in the number of variables , by exploiting primal-dual sparsity inherent in a specific loss function , namely the max-margin loss . In this work , we extend PD-Sparse to be efficiently parallelized in large-scale distributed settings . By introducing separable loss functions , we can scale out the training , with network communication and space efficiency comparable to those in one-versus-all approaches while maintaining an overall complexity sub-linear in the number of classes . On several large-scale benchmarks our proposed method achieves accuracy competitive to the state-of-the-art while reducing the training time from days to tens of minutes compared with existing parallel or sparse methods on a cluster of 100 cores .
2K_dev_1378	Time-limited promotions that exploit consumers ' sense of urgency to boost sales account for billions of dollars in consumer spending each year . However , it is challenging to discover the right timing and duration of a promotion to increase its chances of being redeemed . In this work , we consider the problem of delivering time-limited discount coupons , where we partner with a large national bank functioning as a commission-based third-party coupon provider . Specifically , we use large-scale anonymized transaction records to model consumer spending and forecast future purchases , based on which we generate data-driven , personalized coupons . Our proposed model RUSH ! ( 1 ) predicts { both the time and category } of the next event ; ( 2 ) captures correlations between purchases in different categories ( such as shopping triggering dining purchases ) ; ( 3 ) incorporates temporal dynamics of purchase behavior ( such as increased spending on weekends ) ; ( 4 ) is composed of additive factors that are easily interpretable ; and finally ( 5 ) scales linearly to millions of transactions . We design a cost-benefit framework that facilitates systematic evaluation in terms of our application , and show that RUSH ! provides higher expected value than various baselines that do not jointly model time and category information .
2K_dev_1379	How do people make friends dynamically in social networks ? What are the temporal patterns for an individual increasing its social connectivity ? What are the basic mechanisms governing the formation of these temporal patterns ? No matter cyber or physical social systems , their structure and dynamics are mainly driven by the connectivity dynamics of each individual . However , due to the lack of empirical data , little is known about the empirical dynamic patterns of social connectivity at microscopic level , let alone the regularities or models governing these microscopic dynamics . We examine the detailed growth process of `` WeChat '' , the largest online social network in China , with 300 million users and 4.75 billion links spanning two years . We uncover a wide range of long-term power law growth and short-term bursty growth for the social connectivity of different users . We propose three key ingredients , namely average-effect , multiscale-effect and correlation-effect , which govern the observed growth patterns at microscopic level . As a result , we propose the long short memory process incorporating these ingredients , demonstrating that it successfully reproduces the complex growth patterns observed in the empirical data . By analyzing modeling parameters , we discover statistical regularities underlying the empirical growth dynamics . Our model and discoveries provide a foundation for the microscopic mechanisms of network growth dynamics , potentially leading to implications for prediction , clustering and outlier detection on human dynamics .
2K_dev_1380	With the pervasiveness of mobile technology and location-based computing , new forms of smart urban transportation , such as Uber & Lyft , have become increasingly popular . These new forms of urban infrastructure can influence individuals ' movement frictions and patterns , in turn influencing local consumption patterns and the economic performance of local businesses . To gain insights about future impact of urban transportation changes , in this paper , we utilize a novel dataset and econometric analysis methods to present a quasi-experimental examination of how the emerging growth of peer-to-peer car sharing services may have affected local consumer mobility and consumption patterns .
2K_dev_1381	We consider the M/G/k/staggeredM/G/k/staggered-setup , where idle servers are turned off to save cost , necessitating a setup time for turning a server back on ; however , at most one server may be in setup mode at any time . We show that , for exponentially distributed setup times , the response time of an M/G/k/staggeredM/G/k/staggered-setup approximately decomposes into the sum of the response time for an M/G/kM/G/k and the setup time , where the approximation is nearly exact . This generalizes a prior decomposition result for an M/M/k/staggeredM/M/k/staggered-setup .
2K_dev_1382	Quickly converting speech to text allows deaf and hard of hearing people to interactively follow along with live speech . Doing so reliably requires a combination of perception , understanding , and speed that neither humans nor machines possess alone . In this article , we discuss how our Scribe system combines human labor and machine intelligence in real time to reliably convert speech to text with less than 4s latency . To achieve this speed while maintaining high accuracy , Scribe integrates automated assistance in two ways . First , its user interface directs workers to different portions of the audio stream , slows down the portion they are asked to type , and adaptively determines segment length based on typing speed . Second , it automatically merges the partial input of multiple workers into a single transcript using a custom version of multiple-sequence alignment . Scribe illustrates the broad potential for deeply interleaving human labor and machine intelligence to provide intelligent interactive services that neither can currently achieve alone .
2K_dev_1383	The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents , and therefore , makes the video captioning task even more challenging . In this paper , we propose an unified caption framework , M & M TGM , which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics . Compared to pre-defined topics , the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better . We formulate the topic-aware caption generation as a multi-task learning problem , in which we add a parallel task , topic prediction , in addition to the caption task . For the topic prediction task , we use the mined topics as the teacher to train a student topic prediction model , which learns to predict the latent topics from multimodal contents of videos . The topic prediction provides intermediate supervision to the learning process . As for the caption task , we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics . The entire learning procedure is end-to-end and it optimizes both tasks simultaneously . The results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the effectiveness of our proposed model . M & M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets , but also achieves better generalization ability .
2K_dev_1384	Data association , which could be categorized into offline approaches and the online counterparts , is a crucial part of a multi-object tracker in the tracking-by-detection framework . On the one hand , classical offline data association methods exploit all the video data and have high computation cost , which makes them unscalable to long-term offline video data . On the other hand , online approaches have much lower computation cost , but they suffer from ID-switches and tracklet drifting problem when directly applied to offline data as they are only aware of past observations . In this paper , we propose a mixed style tracker , which is not only as efficient as the online tracker but also aware of future observations in offline setting . We start from a Markov Decision Process ( MDP ) online tracker and design a parallelized apprenticeship learning algorithm to learn both the reward function and transition policy in MDP . By proposing a rewind to track strategy to generate backward tracklets , future detections in offline data are efficiently utilized to obtain a more stable similarity measurement for association . Experiment results show that our approach achieves the state-of-the-art performance on challenging datasets .
2K_dev_1385	As an important branch of multimedia content analysis , Surveillance Event Detection ( SED ) is still a quite challenging task due to high abstraction and complexity such as occlusions , cluttered backgrounds and viewpoint changes etc . To address the problem , we propose a unified SED detection framework which divides events into two categories , i.e. , short-term events and long-duration events . The former can be represented as a kind of snapshots of static key-poses and embodies an inner-dependencies , while the latter contains complex interactions between pedestrians , and shows obvious inter-dependencies and temporal context . For short-term event , a novel cascade Convolutional Neural Network ( CNN ) -HsNet is first constructed to detect the pedestrian , and then the corresponding events are classified . For long-duration event , Dense Trajectory ( DT ) and Improved Dense Trajectory ( IDT ) are first applied to explore the temporal features of the events respectively , and subsequently , Fisher Vector ( FV ) coding is adopted to encode raw features and linear SVM classifiers are learned to predict . Finally , a heuristic fusion scheme is used to obtain the results . In addition , a new large-scale pedestrian dataset , named SED-PD , is built for evaluation . Comprehensive experiments on TRECVID SEDtest datasets demonstrate the effectiveness of proposed framework .
2K_dev_1386	Genome-wide association studies have discovered a large number of genetic variants associated with complex diseases such as Alzheimers disease . However , the genetic background of such diseases is largely unknown due to the complex mechanisms underlying genetic effects on traits , as well as a small sample size ( e.g. , 1000 ) and a large number of genetic variants ( e.g. , 1 million ) . Fortunately , datasets that contain genotypes , transcripts , and phenotypes are becoming more readily available , creating new opportunities for detecting disease-associated genetic variants . In this paper , we present a novel approach called Backward Three-way Association Mapping ( BTAM ) for detecting three-way associations among genotypes , transcripts , and phenotypes . Assuming that genotypes affect transcript levels , which in turn affect phenotypes , we first find transcripts associated with the phenotypes , and then find genotypes associated with the chosen transcripts . The backward ordering of association mappings allows us to avoid a large number of association testings between all genotypes and all transcripts , making it possible to identify three-way associations with a small computational cost . In our simulation study , we demonstrate that BTAM significantly improves the statistical power over forward three-way association mapping that finds genotypes associated with both transcripts and phenotypes and genotype-phenotype association mapping . Furthermore , we apply BTAM on an Alzheimers disease dataset and report top 10 genotype-transcript-phenotype associations .
2K_dev_1387	The Next-Generation Airborne Collision Avoidance System ACASi ? X is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft . It is currently being developed by the Federal Aviation Administration FAA . In this paper we determine the geometric configurations under which the advice given by ACAS X is safe under a precise set of assumptions and formally verify these configurations using hybrid systems theorem proving techniques . We conduct an initial examination of the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version , demonstrating how formal , hybrid approaches are helping ensure the safety of ACAS X . Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety .
2K_dev_1388	Recently , to solve large-scale lasso and group lasso problems , screening rules have been developed , the goal of which is to reduce the problem size by efficiently discarding zero coefficients using simple rules independently of the others . However , screening for overlapping group lasso remains an open challenge because the overlaps between groups make it infeasible to test each group independently . In this paper , we develop screening rules for overlapping group lasso . To address the challenge arising from groups with overlaps , we take into account overlapping groups only if they are inclusive of the group being tested , and then we derive screening rules , adopting the dual polytope projection approach . This strategy allows us to screen each group independently of each other . In our experiments , we demonstrate the efficiency of our screening rules on various datasets .
2K_dev_1389	We consider the mechanism design problem for agents with single-peaked preferences over multi-dimensional domains when multiple alternatives can be chosen . Facility location and committee selection are classic embodiments of this problem . We propose a class of percentile mechanisms , a form of generalized median mechanisms , that are strategy-proof , and derive worst-case approximation ratios for social cost and maximum load for L1 and L2 cost models . More importantly , we propose a sample-based framework for optimizing the choice of percentiles relative to any prior distribution over preferences , while maintaining strategy-proofness . Our empirical investigations , using social cost and maximum load as objectives , demonstrate the viability of this approach and the value of such optimized mechanisms vis-a-vis mechanisms derived through worst-case analysis .
2K_dev_1390	Controlling a registered-user session of a registered user on a device using first and second authentication processes and a handoff from the first process to the second process . In one embodiment , the first authentication process is a stronger process performed at the outset of a session , and the second authentication process is a weaker process iteratively performed during the session . The stronger authentication process may require cooperation from the user , while the weaker authentication process is preferably one that requires little or no user cooperation . In other embodiments , a strong authentication process may be iteratively performed during the session .
2K_dev_1391	Social choice theory provides insights into a variety of collective decision making settings , but nowadays some of its tenets are challenged by internet environments , which call for dynamic decision making under constantly changing preferences . In this paper we model the problem via Markov decision processes ( MDP ) , where the states of the MDP coincide with preference profiles and a ( deterministic , stationary ) policy corresponds to a social choice function . We can therefore employ the axioms studied in the social choice literature as guidelines in the design of socially desirable policies . We present tractable algorithms that compute optimal policies under different prominent social choice constraints . Our machinery relies on techniques for exploiting symmetries and isomorphisms between MDPs .
2K_dev_1392	While the analysis of unlabeled networks has been studied extensively in the past , finding patterns in different kinds of labeled graphs is still an open challenge . Given a large edge-labeled network , e.g. , a time-evolving network , how can we find interesting patterns ? We propose Com $ $ ^2 $ $ 2 , a novel , fast and incremental tensor analysis approach which can discover communities appearing over subsets of the labels . The method is ( a ) scalable , being linear on the input size , ( b ) general , ( c ) needs no user-defined parameters and ( d ) effective , returning results that agree with intuition . We apply our method to real datasets , including a phone call network , a computer-traffic network and a flight information network . The phone call network consists of 4 million mobile users , with 51 million edges ( phone calls ) , over 14 days , while the flights dataset consists of 7733 airports and 5995 airline companies flying 67,663 different routes . We show that Com $ $ ^2 $ $ 2 spots intuitive patterns regarding edge labels that carry temporal or other discrete information . Our findings include large `` star '' -like patterns , near-bipartite cores , as well as tiny groups ( five users ) , calling each other hundreds of times within a few days . We also show that we are able to automatically identify competing airline companies .
2K_dev_1393	The Pitman-Yor process provides an elegant way to cluster data that exhibit power law behavior , where the number of clusters is unknown or unbounded . Unfortunately , inference in Pitman-Yor process-based models is typically slow and does not scale well with dataset size . In this paper we present new auxiliary-variable representations for the Pitman-Yor process and a special case of the hierarchical Pitman-Yor process that allows us to develop parallel inference algorithms that distribute inference both on the data space and the model space . We show that our method scales well with increasing data while avoiding any degradation in estimate quality .
2K_dev_1394	In this paper , we investigate information validation tasks that are initiated as queries from either automated agents or humans . We introduce OpenEval , a new online information validation technique , which uses information on the web to automatically evaluate the truth of queries that are stated as multiargument predicate instances ( e.g. , DrugHasSideEffect ( Aspirin , GI Bleeding ) ) ) . OpenEval gets a small number of instances of a predicate as seed positive examples and automatically learns how to evaluate the truth of a new predicate instance by querying the web and processing the retrieved unstructured web pages . We show that OpenEval is able to respond to the queries within a limited amount of time while also achieving high F1 score . In addition , we show that the accuracy of responses provided by OpenEval is increased as more time is given for evaluation . We have extensively tested our model and shown empirical results that illustrate the effectiveness of our approach compared to related techniques .
2K_dev_1395	How can we correlate the neural activity in the human brain as it responds to typed words , with properties of these terms ( like edible , fits in hand ) ? In short , we want to find latent variables , that jointly explain both the brain activity , as well as the behavioral responses . This is one of many settings of the Coupled Matrix-Tensor Factorization ( CMTF ) problem . Can we accelerate any CMTF solver , so that it runs within a few minutes instead of tens of hours to a day , while maintaining good accuracy ? We introduce TURBO-SMT , a meta-method capable of doing exactly that : it boosts the performance of any CMTF algorithm , by up to 200 , along with an up to 65 fold increase in sparsity , with comparable accuracy to the baseline . We apply TURBO-SMT to BRAINQ , a dataset consisting of a ( nouns , brain voxels , human subjects ) tensor and a ( nouns , properties ) matrix , with coupling along the nouns dimension . TURBO-SMT is able to find meaningful latent variables , as well as to predict brain activity with competitive accuracy .
2K_dev_1396	Given electroencephalogram time series data from patients with epilepsy , can we find patterns and regularities ? The typical approach thus far is to use tensors or dynamical systems . Here , we present EEG-MINE , a nonlinear , chaos-based `` gray box model '' , that blends domain knowledge with data observations . When applied to numerous , real EEG sequences , EEG-MINE ( a ) can successfully reconstruct the signals with high accuracy ; ( b ) can spot surprising patterns within seizure EEG signals ; and ( c ) may provide early warning of epileptic seizures .
2K_dev_1397	An electronic device includes a touch-sensitive surface , for example a touch pad or touch screen . The user interacts with the touch-sensitive surface , producing touch interactions . The resulting actions taken depend at least in part on the touch type . For example , the same touch interactions performed by three different touch types of a finger pad , a finger nail and a knuckle , may result in the execution of different actions .
2K_dev_1398	Live music performance with computers has motivated many research projects in science , engineering , and the arts . In spite of decades of work , it is surprising that there is not more technology for , and a better understanding of the computer as music performer . We review the development of techniques for live music performance and outline our efforts to establish a new direction , Human-Computer Music Performance ( HCMP ) , as a framework for a variety of coordinated studies . Our work in this area spans performance analysis , synchronization techniques , and interactive performance systems . Our goal is to enable musicians to ncorporate computers into performances easily and effectively through a better understanding of requirements , new techniques , and practical , performance-worthy implementations . We conclude with directions for future work .
2K_dev_1399	Matching function binaries -- the process of identifying similar functions among binary executables -- is a challenge that underlies many security applications such as malware analysis and patch-based exploit generation . Recent work tries to establish semantic similarity based on static analysis methods . Unfortunately , these methods do not perform well if the compared binaries are produced by different compiler toolchains or optimization levels . In this work , we propose blanket execution , a novel dynamic equivalence testing primitive that achieves complete coverage by overriding the intended program logic . Blanket execution collects the side effects of functions during execution under a controlled randomized environment . Two functions are deemed similar , if their corresponding side effects , as observed under the same environment , are similar too . We implement our blanket execution technique in a system called BLEX . We evaluate BLEX rigorously against the state of the art binary comparison tool BinDiff . When comparing optimized and un-optimized executables from the popular GNU coreutils package , BLEX outperforms BinDiff by up to 3.5 times in correctly identifying similar functions . BLEX also outperforms BinDiff if the binaries have been compiled by different compilers . Using the functionality in BLEX , we have also built a binary search engine that identifies similar functions across optimization boundaries . Averaged over all indexed functions , our search engine ranks the correct matches among the top ten results 77 % of the time .
2K_dev_1400	Complex systems are designed using the model-based design paradigm in which mathematical models of systems are created and checked against specifications . Cyber-physical systems ( CPS ) are complex systems in which the physical environment is sensed and controlled by computational or cyber elements possibly distributed over communication networks . Various aspects of CPS design such as physical dynamics , software , control , and communication networking must interoperate correctly for correct functioning of the systems . Modeling formalisms , analysis techniques and tools for designing these different aspects have evolved ind ependently , and remain dissimilar and disparate . There is no unifying formalism in which one can model all these aspects equally well . Therefore , model-based design of CPS must make use of a collection of models in several different formalisms and use respective analysis methods and tools together to ensure correct system design . To enable doing this in a formal manner , this thesis develops a framework for multi-model verification of cyber-physical systems based on behavioral semantics . Heterogeneity arising from the different interacting aspects of CPS design must be addressed in order to enable system-level verification . In current practice , there is no principled approach that deals with this modeling heterogeneity within a formal framework . We develop behavioral semantics to address heterogeneity in a general yet formal manner . Our framework makes no assumptions about the specifics of any particular formalism , therefore it readily supports various formalisms , techniques and tools . Models can be analyzed independently in isolation , supporting separation of concerns . Mappings across heterogeneous semantic domains enable associations between analysis results . Interdependencies across different models and specifications can be formally represented as constraints over parameters and verification can be carried out in a semantically consistent manner . Composition of analysis results is supported both hierarchically across different levels of abstraction and structurally into interacting component models at a given level of abstraction . The theoretical concepts developed in the thesis are illustrated using a case study on the hierarchical heterogeneous verification of an automotive intersection collision avoidance system .
2K_dev_1401	As NSF and the academic computer science community strive to improve the quality of high school computing instruction , there are proposals afoot that could drastically reshape this landscape . Why wait for high school , or even middle school , when we could be teaching programming in primary school ? How should we do that ? Highly scaffolded environments such as Scratch and Alice offer young children some assistance with syntax , but I would argue that they are not optimal for very young novice programmers because they use sequential control flow and a graphics programming model . In contrast , Microsoft 's Kodu Game Lab uses a parallel conditional rule formalism ( as do AgentSheets and AgentCubes ) , plus a robot programming model with a physics engine . Together these allow young children to produce complex , engaging behaviors with just a few lines of code . But the carefully crafted constraints that make Kodu so effective for beginners come with a price . Migration to a more conventional framework such as Scratch is an attractive next step .
2K_dev_1402	Randomly mutating well-formed program inputs or simply fuzzing , is a highly effective and widely used strategy to find bugs in software . Other than showing fuzzers find bugs , there has been little systematic effort in understanding the science of how to fuzz properly . In this paper , we focus on how to mathematically formulate and reason about one critical aspect in fuzzing : how best to pick seed files to maximize the total number of bugs found during a fuzz campaign . We design and evaluate six different algorithms using over 650 CPU days on Amazon Elastic Compute Cloud ( EC2 ) to provide ground truth data . Overall , we find 240 bugs in 8 applications and show that the choice of algorithm can greatly increase the number of bugs found . We also show that current seed selection strategies as found in Peach may fare no better than picking seeds at random . We make our data set and code publicly available .
2K_dev_1403	A reliable and accurate biometric identification system must be able to distinguish individuals even in situations where their biometric signatures are very similar . However , the strong similarity in the facial appearance of twins has complicated facial feature based recognition and has even compromised commercial face recognition systems . This paper addresses the above problem and proposes two novel methods to distinguish identical twins using ( 1 ) facial aging features and ( 2 ) asymmetry decomposition features . Facial aging features are extracted using Gabor filters from regions of the face that typically exhibit wrinkles and laugh lines , while Facial asymmetry decomposition based features are obtained by projecting the difference between the two left sides ( consisting of the left half of the face and its mirror ) and two right sides ( consisting of the right half of the face and its mirror ) of a face onto a subspace . Feature vectors obtained using these methods were used for classification . Experiments conducted on images of five types of twins from the University of Notre Dame ND-Twins database indicate that both our proposed approaches achieve high identification rates and are hence quite promising at distinguishing twins . HighlightsThe proposal of two novel approaches to distinguishing identical twins.Facial aging and intrinsic facial symmetry features are sued.A thorough evaluation on a challenging database.The summarizing of existing techniques and the results obtained by them .
2K_dev_1404	Function identification is a fundamental challenge in reverse engineering and binary program analysis . For instance , binary rewriting and control flow integrity rely on accurate function detection and identification in binaries . Although many binary program analyses assume functions can be identified a priori , identifying functions in stripped binaries remains a challenge . In this paper , we propose BYTEWEIGHT , a new automatic function identification algorithm . Our approach automatically learns key features for recognizing functions and can therefore easily be adapted to different platforms , new compilers , and new optimizations . We evaluated our tool against three well-known tools that feature function identification : IDA , BAP , and Dyninst . Our data set consists of 2,200 binaries created with three different compilers , with four different optimization levels , and across two different operating systems . In our experiments with 2,200 binaries , we found that BYTE-WEIGHT missed 44,621 functions in comparison with the 266,672 functions missed by the industry-leading tool IDA . Furthermore , while IDA misidentified 459,247 functions , BYTEWEIGHT misidentified only 43,992 functions .
2K_dev_1405	We propose a new framework for single-channel source separation that lies between the fully supervised and unsupervised setting . Instead of supervision , we provide input features for each source signal and use convex methods to estimate the correlations between these features and the unobserved signal decomposition . Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision ; our motivating application is energy disaggregation of hourly smart meter data ( the separation of whole-home power signals into different energy uses ) . Here contextual supervision allows us to provide itemized energy usage for thousands homes , a task previously impossible due to the need for specialized data collection hardware . On smaller datasets which include labels , we demonstrate that contextual supervision improves significantly over a reasonable baseline and existing unsupervised methods for source separation . Finally , we analyze the case of l2 loss theoretically and show that recovery of the signal components depends only on cross-correlation between features for different signals , not on correlations between features for the same signal .
2K_dev_1406	Computing equilibria of games is a central task in computer science . A large number of results are known for Nash equilibrium ( NE ) . However , these can be adopted only when coalitions are not an issue . When instead agents can form coalitions , NE is inadequate and an appropriate solution concept is strong Nash equilibrium ( SNE ) . Few computational results are known about SNE . In this paper , we first study the problem of verifying whether a strategy profile is an SNE , showing that the problem is in P. We then design a spatial branch -- and -- bound algorithm to find an SNE , and we experimentally evaluate the algorithm .
2K_dev_1407	This paper explores how to find , track , and learn models of arbitrary objects in a video without a predefined method for object detection . We present a model that localizes objects via unsupervised tracking while learning a representation of each object , avoiding the need for pre-built detectors . Our model uses a dependent Dirichlet process mixture to capture the uncertainty in the number and appearance of objects and requires only spatial and color video data that can be efficiently extracted via frame differencing . We give two inference algorithms for use in both online and offline settings , and use them to perform accurate detection-free tracking on multiple real videos . We demonstrate our method in difficult detection scenarios involving occlusions and appearance shifts , on videos containing a large number of objects , and on a recent human-tracking benchmark where we show performance comparable to state of the art detector-based methods .
2K_dev_1408	Given the re-broadcasts ( i.e . retweets ) of posts in Twitter , how can we spot fake from genuine user reactions ? What will be the tell-tale sign the connectivity of retweeters , their relative timing , or something else ? High retweet activity indicates influential users , and can be monetized . Hence , there are strong incentives for fraudulent users to artificially boost their retweets ' volume . Here , we explore the identifi- cation of fraudulent and genuine retweet threads . Our main contribu- tions are : ( a ) the discovery of patterns that fraudulent activity seems to follow ( the `` triangles `` a nd `` homogeneity '' patterns , the formation of micro-clusters in appropriate feature spaces ) ; and ( b ) `` RTGen '' , a realistic generator that mimics the behaviors of both honest and fraud- ulent users . We present experiments on a dataset of more than 6 million retweets crawled from Twitter .
2K_dev_1409	Safety critical systems often have shutdown mechanisms to bring the system to a safe state in the event of a malfunction . We ex- amine the use of ride-through , a technique to reduce the frequency of safety shutdowns by allowing small transient violations of safety rules . An illustrative example of enforcing a speed limit for an autonomous vehicle shows that using a rate-limited ride-through bound permits a tighter safety limit on speed than a xed threshold without creating false alarm shutdowns . Adding state machines to select speci c safety bounds based on vehicle state accommodates expected control system transients . Testing these principles on an autonomous utility vehicle resulted in im- proved detection of speed limit violations and shorter shutdown stopping distances without needing to increase the false alarm shutdown rate .
2K_dev_1410	Background Probabilistic models have gained widespread acceptance in the systems biology community as a useful way to represent complex biological systems . Such models are developed using existing knowledge of the structure and dynamics of the system , experimental observations , and inferences drawn from statistical analysis of empirical data . A key bottleneck in building such models is that some system variables can not be measured experimentally . These variables are incorporated into the model as numerical parameters . Determining values of these parameters that justify existing experiments and provide reliable predictions when model simulations are performed is a key research problem . Domain experts usually estimate the values of these parameters by fitting the model to experimental data . Model fitting is usually expressed as an optimization problem that requires minimizing a cost-function which measures some notion of distance between the model and the data . This optimization problem is often solved by combining local and global search methods that tend to perform well for the specific application domain . When some prior information about parameters is available , methods such as Bayesian inference are commonly used for parameter learning . Choosing the appropriate parameter search technique requires detailed domain knowledge and insight into the underlying system .
2K_dev_1411	Many vision tasks require a multi-class classifier to discriminate multiple categories , on the order of hundreds or thousands . In this paper , we propose sparse output coding , a principled way for large-scale multi-class classification , by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem . Specifically , sparse output coding is composed of two steps : efficient coding matrix learning with scalability to thousands of classes , and probabilistic decoding . Empirical results on object recognition and scene classification demonstrate the effectiveness of our proposed approach .
2K_dev_1412	The lessons of HCI can therefore be brought to bear on different aspects of collective intelligence . On the one hand , the people in the collective ( the crowd ) will only contribute if there are proper incentives and if the interface guides them in usable and meaningful ways . On the other , those interested in leveraging the collective need usable ways of coordinating , making sense of , and extracting value from the collective work that is being done , often on their behalf . Ultimately , collective intelligence involves the co-design of technical infrastructure and human-human interaction : a socio-technical system . In crowdsourcing , we might differentiate between two broad classes of users : requesters and crowd members . The requesters are the individuals or group for whom work is done or who takes the responsibility to aggregate the work done by the collective . The crowd member ( or crowd worker ) is one of many people to contribute . While we often use the word worker , crowd workers do not have need to be ( and often arent ) contributing as part of what we might consider standard work . They may work for pay or not , work for small periods of time or contribute for days to a project they care about , and they may work in such a way as each individuals contribution may be difficult to discern from the collective final output . HCI has a long history of studying not only the interaction between individuals with technology , but also the interaction of groups with or mediated by technology . For example , computer-supported cooperative work ( CSCW ) investigates how to allow groups to accomplish tasks together using a shared or distributed computer interfaces , either at the same time or asynchronously . Current crowdsourcing research alters some of the standard assumptions about the size , composition , and stability of these groups , but the fundamental approaches remain the same . For instance , workers drawn from the crowd may be less reliable than groups of employees working on a shared task , and group membership in the crowd may change more quickly . There are three main vectors of study for HCI and collective intelligence . The first is directed crowdsourcing , where a single individual attempts to recruit and guide a large set of people to help accomplish a goal . The second is collaborative crowdsourcing , where a group gathers based on shared interest and self-determine their organization and work . The third vector is passive crowdsourcing , where the crowd or collective may never meet or coordinate , but it is still possible to mine their collective behavior patterns for information . We cover each vector in turn . We conclude with a list of challenges for researches in HCI related to crowdsourcing and collective intelligence .
2K_dev_1413	Given a multimillion-node social network , how can we sum- marize connectivity pattern from the data , and how can we find unex- pected user behavior ? In this paper we study a complete graph from a large who-follows-whom network and spot lockstep behavior that large groups of followers connect to the same groups of followees . Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep . We discover that ( a ) the lockstep behavior on the graph shapes dense `` block '' in its adjacency matrix and creates `` ray '' in spectral subspaces , and ( b ) partially overlapping of the behavior shapes `` staircase '' in the matrix and creates `` pearl '' in the subspaces . The second contribution is that we provide a fast algorithm , using the discovery as a guide for practi- tioners , to detect users who offer the lockstep behavior . We demonstrate that our approach is effective on both synthetic and real data .
2K_dev_1414	Out of the many potential factors that determine which links form in a document citation network , two in particular are of high importance : first , a document may be cited based on its subject matterthis can be modeled by analyzing document content ; second , a document may be cited based on which other documents have previously cited itthis can be modeled by analyzing citation structure . Both factors are important for users to make informed decisions and choose appropriate citations as the network grows . In this paper , we present a novel model that integrates the merits of content and citation analyses into a single probabilistic framework . We demonstrate our model on three real-world citation networks . Compared with existing baselines , our model can be used to effectively explore a citation network and provide meaningful explanations for links while still maintaining competitive citation prediction performance .
2K_dev_1415	The computational characterization of game-theoretic solution concepts is a prominent topic in computer science . The central solution concept is Nash equilibrium ( NE ) . However , it fails to capture the possibility that agents can form coalitions . Strong Nash equilibrium ( SNE ) refines NE to this setting . It is known that finding an SNE is NP-complete when the number of agents is constant . This hardness is solely due to the existence of mixed-strategy SNEs , given that the problem of enumerating all pure-strategy SNEs is trivially in P. Our central result is that , in order for an n-agent game to have at least one non-pure-strategy SNE , the agents ' payoffs restricted to the agents ' supports must lie on an ( n - 1 ) -dimensional space . Small perturbations make the payoffs fall outside such a space and thus , unlike NE , finding an SNE is in smoothed polynomial time .
2K_dev_1416	Automatically recognizing a large number of action categories from videos is of significant importance for video understanding . Most existing works focused on the design of more discriminative feature representation , and have achieved promising results when the positive samples are enough . However , very limited efforts were spent on recognizing a novel action without any positive exemplars , which is often the case in the real settings due to the large amount of action classes and the users ' queries dramatic variations . To address this issue , we propose to perform action recognition when no positive exemplars of that class are provided , which is often known as the zero-shot learning . Different from other zero-shot learning approaches , which exploit attributes as the intermediate layer for the knowledge transfer , our main contribution is SIR , which directly leverages the semantic inter-class relationships between the known and unknown actions followed by label transfer learning . The inter-class semantic relationships are automatically measured by continuous word vectors , which learned by the skip-gram model using the large-scale text corpus . Extensive experiments on the UCF101 dataset validate the superiority of our method over fully-supervised approaches using few positive exemplars .
2K_dev_1417	Creating safe Transportation Cyber-Physical Systems ( CPSs ) presents new challenges as autonomous operation is attempted in unconstrained operational environments . The extremely high safety level required of such systems ( perhaps one critical failure per billion operating hours ) means that validation approaches will need to consider not only normal operation , but also operation with system faults and in exceptional environments . Additional challenges will need to be overcome in the areas of rigorously defining safety requirements , trusting the safety of multi-vendor distributed system components , tolerating environmental uncertainty , providing a realistic role for human oversight , and ensuring sufficiently rigorous validation of autonomy technology .
2K_dev_1418	With the goal of improving the quality of life for people suffering from various motor control disorders , brain-machine interfaces provide direct neural control of prosthetic devices by translating neural signals into control signals . These systems act by reading motor intent signals directly from the brain and using them to control , for example , the movement of a cursor on a computer screen . Over the past two decades , much attention has been devoted to the decoding problem : how should recorded neural activity be translated into the movement of the cursor ? Most approaches have focused on this problem from an estimation standpoint , i.e. , decoders are designed to return the best estimate of motor intent possible , under various sets of assumptions about how the recorded neural signals represent motor intent . Here we recast the decoder design problem from a physical control system perspective , and investigate how various classes of decoders lead to different types of physical systems for the subject to control . This framework leads to new interpretations of why certain types of decoders have been shown to perform better than others . These results have implications for understanding how motor neurons are recruited to perform various tasks , and may lend insight into the brain 's ability to conceptualize artificial systems .
2K_dev_1419	We propose a novel method to predict accurately trust relationships of a target user even if he/she does not have much interaction information . The proposed method considers positive , implicit , and negative information of all users in a network based on belief propagation to predict trust relationships of a target user .
2K_dev_1420	Matrix factorization ( MF ) has been attracting much attention due to its wide applications . However , since MF models are generally non-convex , most of the existing methods are easily stuck into bad local minima , especially in the presence of outliers and missing data . To alleviate this deficiency , in this study we present a new MF learning methodology by gradually including matrix elements into MF training from easy to complex . This corresponds to a recently proposed learning fashion called self-paced learning ( SPL ) , which has been demonstrated to be beneficial in avoiding bad local minima . We also generalize the conventional binary ( hard ) weighting scheme for SPL to a more effective real-valued ( soft ) weighting manner . The effectiveness of the proposed self-paced MF method is substantiated by a series of experiments on synthetic , structure from motion and background subtraction data .
2K_dev_1421	Given the retweeting activity for the posts of several Twitter users , how can we distinguish organic activity from spammy retweets by paid followers to boost a post 's appearance of popularity ? More gen- erally , given groups of observations , can we spot strange groups ? Our main intuition is that organic behavior has more variability , while fraud- ulent behavior , like retweets by botnet members , is more synchronized . We refer to the detection of such synchronized observations as the Syn- chonization Fraud problem , and we study a specific instance of it , Retweet Fraud Detection , manifested in Twitter . Here , we propose : ( A ) ND-Sync , an efficient method for detecting group fraud , and ( B ) a set of carefully designed features for characterizing retweet threads . ND-Sync is effec- tive in spotting retweet fraudsters , robust to different types of abnormal activity , and adaptable as it can easily incorporate additional features . Our method achieves a 97 % accuracy on a real dataset of 12 million retweets crawled from Twitter .
2K_dev_1422	Age-estimation of a face of an individual is represented in image data . In one embodiment , age-estimation techniques involves combining a Contourlet Appearance Model ( CAM ) for facial-age feature extraction and Support Vector Regression ( SVR ) for learning aging rules in order to improve the accuracy of age-estimation over the current techniques . In a particular example , characteristics of input facial images are converted to feature vectors by CAM , then these feature vectors are analyzed by an aging-mechanism-based classifier to estimate whether the images represent faces of younger or older people prior to age-estimation , the aging-mechanism-based classifier being generated in one embodiment by running Support Vector Machines ( SVM ) on training images . In an exemplary binary youth/adult classifier , faces classified as adults are passed to an adult age-estimation function and the others are passed to a youth age-estimation function .
2K_dev_1423	Refactoring of code is a common device in software engineering . As cyber-physical systems CPS become ever more complex , similar engineering practices become more common in CPS development . Proper safe developments of CPS designs are accompanied by a proof of correctness . Since the inherent complexities of CPS practically mandate iterative development , frequent changes of models are standard practice , but require reverification of the resulting models after every change . To overcome this issue , we develop proof-aware refactorings for CPS . That is , we study model transformations on CPS and show how they correspond to relations on correctness proofs . As the main technical device , we show how the impact of model transformations on correctness can be characterized by different notions of refinement in differential dynamic logic . Furthermore , we demonstrate the application of refinements on a series of safety-preserving and liveness-preserving refactorings . For some of these we can give strong results by proving on a meta-level that they are correct . Where this is impossible , we construct proof obligations for showing that the refactoring respects the refinement relation .
2K_dev_1424	Most work building on the Stackelberg security games model assumes that the attacker can perfectly observe the defender 's randomized assignment of resources to targets . This assumption has been challenged by recent papers , which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance . We analytically demonstrate that in zero-sum security games , lazy defenders , who simply keep optimizing against perfectly informed attackers , are almost optimal against diligent attackers , who go to the effort of gathering a reasonable number of observations . This result implies that , in some realistic situations , limited surveillance may not need to be explicitly addressed .
2K_dev_1425	Motivated by the success of CNNs in object recognition on images , researchers are striving to develop CNN equivalents for learning video features . However , learning video features globally has proven to be quite a challenge due to the difficulty of getting enough labels , processing large-scale video data , and representing motion information . Therefore , we propose to leverage effective techniques from both data-driven and data-independent approaches to improve action recognition system . Our contribution is three-fold . First , we explicitly show that local handcrafted features and CNNs share the same convolution-pooling network structure . Second , we propose to use independent subspace analysis ( ISA ) to learn descriptors for state-of-the-art handcrafted features . Third , we enhance ISA with two new improvements , which make our learned descriptors significantly outperform the handcrafted ones . Experimental results on standard action recognition benchmarks show competitive performance .
2K_dev_1426	Exemplary systems , methods and computer-accessible mediums can be provided which can receive information related to a consumer ( s ) , and determine the search behavior of the consumer ( s ) based on the information and using a consumer search model that is based on heterogeneous preferences and a search cost model of a second consumer ( s ) .
2K_dev_1427	Methods and software for managing electric power networks in a distributed manner . A supply and demand balancing scheme is used across a network of agents to facilitate agreement on the optimal incremental price for energy provision in an electric power network subject to the constraint that the total generation in the electric power network matches the total network demand . A multi-step optimization approach is provided that incorporates inter-temporal constraints , allowing for optimal integration of flexible power loads and power storage entities and taking into account power generation ramp rate constraints at individual generation entities . The approach is extended to cope with line flow constraints imposed by the physical system topology and transmission line limits/capacities .
2K_dev_1428	A computer-implemented method includes , in one aspect , receiving a request for a spatial analysis of building behavior of an entity within a building facility ; retrieving building maintenance information about the entity within the building facility ; accessing a building information model for the building facility ; identifying a portion of the building information model that pertains to the entity ; and based on the retrieved building maintenance information and the identified portion of the building information model , generating the spatial analysis of the building behavior for the entity within the building facility .
2K_dev_1429	Designers of human computation systms often face the need to aggregate noisy information provided by multiple people . While voting is often used for this purpose , the choice of voting method is typically not principled . We conduct extensive experiments on Amazon Mechanical Turk to better understand how different voting rules perform in practice . Our empirical conclusions show that noisy human voting can differ from what popular theoretical models would predict . Our short-term goal is to motivate the design of better human computation systems ; our long-term goal is to spark an interaction between researchers in ( computational ) social choice and human computation .
2K_dev_1430	The Poisson distribution has been widely studied and used for modeling univariate count-valued data . Multivariate generalizations of the Poisson distribution that permit dependencies , however , have been far less popular . Yet , real-world high-dimensional count-valued data found in word counts , genomics , and crime statistics , for example , exhibit rich dependencies , and motivate the need for multivariate distributions that can appropriately model this data . We review multivariate distributions derived from the univariate Poisson , categorizing these models into three main classes : 1 ) where the marginal distributions are Poisson , 2 ) where the joint distribution is a mixture of independent multivariate Poisson distributions , and 3 ) where the node-conditional distributions are derived from the Poisson . We discuss the development of multiple instances of these classes and compare the models in terms of interpretability and theory . Then , we empirically compare multiple models from each class on three real-world datasets that have varying data characteristics from different domains , namely traffic accident data , biological next generation sequencing data , and text data . These empirical experiments develop intuition about the comparative advantages and disadvantages of each class of multivariate distribution that was derived from the Poisson . Finally , we suggest new research directions as explored in the subsequent discussion section .
2K_dev_1431	A large number of distal enhancers and proximal promoters form enhancer-promoter interactions to regulate target genes in the human genome . Although recent high-throughput genome-wide mapping approaches have allowed us to more comprehensively recognize potential enhancer-promoter interactions , it is still largely unknown whether sequence-based features alone are sufficient to predict such interactions .
2K_dev_1432	We analyze naturally occurring datasets from student use of educational technologies to explore a long-standing question of the scope of transfer of learning . We contrast a faculty theory of broad transfer with a component theory of more constrained transfer . To test these theories , we develop statistical models of them . These models use latent variables to represent mental functions that are changed while learning to cause a reduction in error rates for new tasks . Strong versions of these models provide a common explanation for the variance in task difficulty and transfer . Weak versions decouple difficulty and transfer explanations by describing task difficulty with parameters for each unique task . We evaluate these models in terms of both their prediction accuracy on held-out data and their power in explaining task difficulty and learning transfer . In comparisons across eight datasets , we find that the component models provide both better predictions and better explanations than the faculty models . Weak model variations tend to improve generalization across students , but hurt generalization across items and make a sacrifice to explanatory power . More generally , the approach could be used to identify malleable components of cognitive functions , such as spatial reasoning or executive functions .
2K_dev_1433	Current methods for depression assessment depend almost entirely on clinical interview or self-report ratings . Such measures lack systematic and efficient ways of incorporating behavioral observations that are strong indicators of psychological disorder . We compared a clinical interview of depression severity with automatic measurement in 48 participants undergoing treatment for depression . Interviews were obtained at 7-week intervals on up to four occasions . Following standard cut-offs , participants at each session were classified as remitted , intermediate , or depressed . Logistic regression classifiers using leave-one-out validation were compared for facial movement dynamics , head movement dynamics , and vocal prosody individually and in combination . Accuracy ( remitted versus depressed ) for facial movement dynamics was higher than that for head movement dynamics ; and each was substantially higher than that for vocal prosody . Accuracy for all three modalities together reached 88.93 % , exceeding that for any single modality or pair of modalities . These findings suggest that automatic detection of depression from behavioral indicators is feasible and that multimodal measures afford most powerful detection .
2K_dev_1434	Results on two-particle angular correlations for charged particles produced in pp collisions at a center-of-mass energy of 13TeV are presented . The data were taken with the CMS detector at the LHC and correspond to an integrated luminosity of about 270__nb^ { -1 } . The correlations are studied over a broad range of pseudorapidity ( |_| < 2.4 ) and over the full azimuth ( _ ) as a function of charged particle multiplicity and transverse momentum ( p_ { T } ) . In high-multiplicity events , a long-range ( |__| > 2 ) , near-side ( __0 ) structure emerges in the two-particle __-__ correlation functions . The magnitude of the correlation exhibits a pronounced maximum in the range 1 < p_ { T } < 2.0__GeV/c and an approximately linear increase with the charged particle multiplicity , with an overall correlation strength similar to that found in earlier pp data at sqrt [ s ] 0 . The present measurement extends the study of near-side long-range correlations up to charged particle multiplicities N_ { ch } _180 , a region so far unexplored in pp collisions . The observed long-range correlations are compared to those seen in pp , pPb , and PbPb collisions at lower collision energies .
2K_dev_1435	An all-soft-matter composite consisting of liquid metal microdroplets embedded in a soft elastomer matrix is presented by C. Majidi and co-workers on page 3726 . This composite exhibits a high dielectric constant while maintaining exceptional elasticity and compliance . The image shows the composite 's microstructure captured by 3D X-ray imaging using a nano-computed tomographic scanner .
2K_dev_1436	An all-soft-matter composite consisting of liquid metal microdroplets embedded in a soft elastomer matrix is presented by C. Majidi and co-workers on page 3726 . This composite exhibits a high dielectric constant while maintaining exceptional elasticity and compliance . The image shows the composite 's microstructure captured by 3D X-ray imaging using a nano-computed tomographic scanner .
2K_dev_1437	In preclinical testing , ventricular wall injection of hydrogels has been shown to be effective in modulating ventricular remodeling and preserving cardiac function . For some approaches , early-stage clinical trials are under way . The hydrogel delivery method varies , with minimally invasive approaches being preferred . Endocardial injections carry a risk of hydrogel regurgitation into the circulation , and precise injection patterning is a challenge . An epicardial approach with a thermally gelling hydrogel through the subxiphoid pathway overcomes these disadvantages .
2K_dev_1438	We inductively tested if a coherent field of inquiry in human conflict research emerged in an analysis of published research involving `` conflict '' in the Web of Science ( WoS ) over a 66-year period ( 1945-2011 ) . We created a citation network that linked the 62504 WoS records and their cited literature . We performed a critical path analysis ( CPA ) , a specialized social network analysis on this citation network ( ~1.5 million works ) , to highlight the main contributions in conflict research and to test if research on conflict has in fact evolved to represent a coherent field of inquiry . Out of this vast dataset , 49 academic works were highlighted by the CPA suggesting a coherent field of inquiry ; which means that researchers in the field acknowledge seminal contributions and share a common knowledge base . Other conflict concepts that were also analyzed-such as interpersonal conflict or conflict among pharmaceuticals , for example , did not form their own CP . A single path formed , meaning that there was a cohesive set of ideas that built upon previous research . This is in contrast to a main path analysis of conflict from 1957-1971 where ideas did n't persist in that multiple paths existed and died or emerged reflecting lack of scientific coherence ( Carley , Hummon , and Harty , 1993 ) . The critical path consisted of a number of key features : 1 ) Concepts that built throughout include the notion that resource availability drives conflict , which emerged in the 1960s-1990s and continued on until 2011 . More recent intrastate studies that focused on inequalities emerged from interstate studies on the democracy of peace earlier on the path . 2 ) Recent research on the path focused on forecasting conflict , which depends on well-developed metrics and theories to model . 3 ) We used keyword analysis to independently show how the CP was topically linked ( i.e. , through democracy , modeling , resources , and geography ) . Publically available conflict datasets developed early on helped shape the operationalization of conflict . In fact , 94 % of the works on the CP that analyzed data either relied on publically available datasets , or they generated a dataset and made it public . These datasets appear to be important in the development of conflict research , allowing for cross-case comparisons , and comparisons to previous works .
2K_dev_1439	Cellular Electron CryoTomography ( CECT ) enables 3D visualization of cellular organization at near-native state and in sub-molecular resolution , making it a powerful tool for analyzing structures of macromolecular complexes and their spatial organizations inside single cells . However , high degree of structural complexity together with practical imaging limitations makes the systematic de novo discovery of structures within cells challenging . It would likely require averaging and classifying millions of subtomograms potentially containing hundreds of highly heterogeneous structural classes . Although it is no longer difficult to acquire CECT data containing such amount of subtomograms due to advances in data acquisition automation , existing computational approaches have very limited scalability or discrimination ability , making them incapable of processing such amount of data .
2K_dev_1440	The core premise of evidence-based medicine is that clinical decisions are informed by the peer-reviewed literature . To extract meaningful conclusions from this literature , one must first understand the various forms of biases inherent within the process of peer review . We performed an exhaustive search that identified articles exploring the question of whether survival benefit was associated with maximal high-grade glioma ( HGG ) resection and analysed this literature for patterns of publication . We found that the distribution of these 108 articles among the 26 journals to be non-random ( p < 0.01 ) , with 75 of the 108 published articles ( 69 % ) appearing in 6 of the 26 journals ( 25 % ) . Moreover , certain journals were likely to publish a large number of articles from the same medical academic genealogy ( authors with shared training history and/or mentor ) . We term the tendency of certain types of articles to be published in select journals 'journal bias ' and discuss the implication of this form of bias as it pertains to evidence-based medicine .
2K_dev_1441	Fluorescence microscopy is one of the most important tools in cell biology research because it provides spatial and temporal information to investigate regulatory systems inside cells . This technique can generate data in the form of signal intensities at thousands of positions resolved inside individual live cells . However , given extensive cell-to-cell variation , these data can not be readily assembled into three- or four-dimensional maps of protein concentration that can be compared across different cells and conditions . We have developed a method to enable comparison of imaging data from many cells and applied it to investigate actin dynamics in T cell activation . Antigen recognition in T cells by the T cell receptor ( TCR ) is amplified by engagement of the costimulatory receptor CD28 . We imaged actin and eight core actin regulators to generate over a thousand movies of T cells under conditions in which CD28 was either engaged or blocked in the context of a strong TCR signal . Our computational analysis showed that the primary effect of costimulation blockade was to decrease recruitment of the activator of actin nucleation WAVE2 ( Wiskott-Aldrich syndrome protein family verprolin-homologous protein 2 ) and the actin-severing protein cofilin to F-actin . Reconstitution of WAVE2 and cofilin activity restored the defect in actin signaling dynamics caused by costimulation blockade . Thus , we have developed and validated an approach to quantify protein distributions in time and space for the analysis of complex regulatory systems .
2K_dev_1442	One goal of human genetics is to understand the genetic basis of disease , a challenge for diseases of complex inheritance because risk alleles are few relative to the vast set of benign variants . Risk variants are often sought by association studies in which allele frequencies in case subjects are contrasted with those from population-based samples used as control subjects . In an ideal world we would know population-level allele frequencies , releasing researchers to focus on case subjects . We argue this ideal is possible , at least theoretically , and we outline a path to achieving it in reality . If such a resource were to exist , it would yield ample savings and would facilitate the effective use of data repositories by removing administrative and technical barriers . We call this concept the Universal Control Repository Network ( UNICORN ) , a means to perform association analyses without necessitating direct access to individual-level control data . Our approach to UNICORN uses existing genetic resources and various statistical tools to analyze these data , including hierarchical clustering with spectral analysis of ancestry ; and empirical Bayesian analysis along with Gaussian spatial processes to estimate ancestry-specific allele frequencies . We demonstrate our approach using tens of thousands of control subjects from studies of Crohn disease , showing how it controls 0 positives , provides power similar to that achieved when all control data are directly accessible , and enhances power when control data are limiting or even imperfectly matched ancestrally . These results highlight how UNICORN can enable reliable , powerful , and convenient genetic association analyses without access to the individual-level data .
2K_dev_1443	SimSensei is a Virtual Human ( VH ) interviewing platform that uses off-the-shelf sensors ( i.e. , webcams , Microsoft Kinect and a microphone ) to capture and interpret real-time audiovisual behavioral signals from users interacting with the VH system . The system was specifically designed for clinical interviewing and health care support by providing a face-to-face interaction between a user and a VH that can automatically react to the inferred state of the user through analysis of behavioral signals gleaned from the user 's facial expressions , body gestures and vocal parameters . Akin to how non-verbal behavioral signals have an impact on human-to-human interaction and communication , SimSensei aims to capture and infer user state from signals generated from user non-verbal communication to improve engagement between a VH and a user and to quantify user state from the data captured across a 20 minute interview . Results from of sample of service members ( SMs ) who were interviewed before and after a deployment to Afghanistan indicate that SMs reveal more PTSD symptoms to the VH than they report on the Post Deployment Health Assessment . Pre/Post deployment facial expression analysis indicated more sad expressions and few happy expressions at post deployment .
2K_dev_1444	Multifunctional polymer-based composites have been widely used in various research and industrial applications , such as flexible and stretchable electronics and sensors and sensor-integrated smart structures . This study investigates the influence of particle coalescence on the mechanical and electrical properties of spherical nickel powder ( SNP ) /polydimethylsiloxane ( PDMS ) composites in which SNP was aligned using an external magnetic field . With the increase of the volume fraction of the SNP , the aligned SNP/PDMS composites exhibited a higher tensile strength and a lower ultimate strain . In addition , the composites with aligned SNP showed a lower percolation threshold and a higher electrical conductivity compared with those with randomly dispersed SNP . However , when the concentration of the SNP reached a certain level ( 40 vol . % ) , the anisotropy of the effective material property became less noticeable than that of the lower concentration ( 20 vol . % ) composites due to the change of the microstructure of the particles caused by the coalescence of the particles at a high concentration . This work may provide rational methods for the fabrication of aligned composites .
2K_dev_1445	An all-soft-matter composite with exceptional electro-elasto properties is demonstrated by embedding liquid-metal inclusions in an elastomer matrix . This material exhibits a unique combination of high dielectric constant , low stiffness , and large strain limit ( ca . 600 % strain ) . The elasticity , electrostatics , and electromechanical coupling of the composite are investigated , and strong agreement with predictions from effective medium theory is found .
2K_dev_1447	Recent studies have shown that brain-machine interfaces ( BMIs ) offer great potential for restoring upper limb function . However , grasping objects is a complicated task and the signals extracted from the brain may not always be capable of driving these movements reliably . Vision-guided robotic assistance is one possible way to improve BMI performance . We describe a method of shared control where the user controls a prosthetic arm using a BMI and receives assistance with positioning the hand when it approaches an object .
2K_dev_1448	Complex networks have been shown to exhibit universal properties , with one of the most consistent patterns being the scale-free degree distribution , but are there regularities obeyed by the r-hop neighborhood in real networks ? We answer this question by identifying another power-law pattern that describes the relationship between the fractions of node pairs C ( r ) within r hops and the hop count r. This scale-free distribution is pervasive and describes a large variety of networks , ranging from social and urban to technological and biological networks . In particular , inspired by the definition of the fractal correlation dimension D2 on a point-set , we consider the hop-count r to be the underlying distance metric between two vertices of the network , and we examine the scaling of C ( r ) with r. We find that this relationship follows a power-law in real networks within the range 2 r d , where d is the effective diameter of the network , that is , the 90-th percentile distance . We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. We provide theoretical justification for this pattern under successful existing network models , while we analyze a large set of real and synthetic network datasets and we show the pervasiveness of the power-hop .
2K_dev_1449	Eumelanins are extended heterogeneous biopolymers composed of molecular subunits with ambiguous macromolecular topology . Here , an electrochemical fingerprinting technique is described , which suggests that natural eumelanin pigments contain indole-based tetramers that are arranged into porphyrin-like domains . Spectroscopy and density functional theory calculations suggest that sodium ions undergo occupancy-dependent stepwise insertion into the core of porphyrin-like tetramers in natural eumelanins at discrete potentials .
2K_dev_1450	High throughput screening determines the effects of many conditions on a given biological target . Currently , to estimate the effects of those conditions on other targets requires either strong modeling assumptions ( e.g . similarities among targets ) or separate screens . Ideally , data-driven experimentation could be used to learn accurate models for many conditions and targets without doing all possible experiments . We have previously described an active machine learning algorithm that can iteratively choose small sets of experiments to learn models of multiple effects . We now show that , with no prior knowledge and with liquid handling robotics and automated microscopy under its control , this learner accurately learned the effects of 48 chemical compounds on the subcellular localization of 48 proteins while performing only 29 % of all possible experiments . The results represent the first practical demonstration of the utility of active learning-driven biological experimentation in which the set of possible phenotypes is unknown in advance .
2K_dev_1451	Despite the enormous medical impact of cancers and intensive study of their biology , detailed characterization of tumor growth and development remains elusive . This difficulty occurs in large part because of enormous heterogeneity in the molecular mechanisms of cancer progression , both tumor-to-tumor and cell-to-cell in single tumors . Advances in genomic technologies , especially at the single-cell level , are improving the situation , but these approaches are held back by limitations of the biotechnologies for gathering genomic data from heterogeneous cell populations and the computational methods for making sense of those data . One popular way to gain the advantages of whole-genome methods without the cost of single-cell genomics has been the use of computational deconvolution ( unmixing ) methods to reconstruct clonal heterogeneity from bulk genomic data . These methods , too , are limited by the difficulty of inferring genomic profiles of rare or subtly varying clonal subpopulations from bulk data , a problem that can be computationally reduced to that of reconstructing the geometry of point clouds of tumor samples in a genome space . Here , we present a new method to improve that reconstruction by better identifying subspaces corresponding to tumors produced from mixtures of distinct combinations of clonal subpopulations . We develop a nonparametric clustering method based on medoidshift clustering for identifying subgroups of tumors expected to correspond to distinct trajectories of evolutionary progression . We show on synthetic and real tumor copy-number data that this new method substantially improves our ability to resolve discrete tumor subgroups , a key step in the process of accurately deconvolving tumor genomic data and inferring clonal heterogeneity from bulk data .
2K_dev_1452	Recent studies have used cluster analysis to identify phenotypic clusters of asthma with differences in clinical traits , as well as differences in response to therapy with anti-inflammatory medications . However , the correspondence between different phenotypic clusters and differences in the underlying molecular mechanisms of asthma pathogenesis remains unclear .
2K_dev_1453	In eukaryotic cells , mitochondria form a dynamic interconnected network to respond to changing needs at different subcellular locations . A fundamental yet unanswered question regarding this network is whether , and if so how , local fusion and fission of individual mitochondria affect their global distribution . To address this question , we developed high-resolution computational image analysis techniques to examine the relations between mitochondrial fusion/fission and spatial distribution within the axon of Drosophila larval neurons . We found that stationary and moving mitochondria underwent fusion and fission regularly but followed different spatial distribution patterns and exhibited different morphology . Disruption of inner membrane fusion by knockdown of dOpa1 , Drosophila Optic Atrophy 1 , not only increased the spatial density of stationary and moving mitochondria but also changed their spatial distributions and morphology differentially . Knockdown of dOpa1 also impaired axonal transport of mitochondria . But the changed spatial distributions of mitochondria resulted primarily from disruption of inner membrane fusion because knockdown of Milton , a mitochondrial kinesin-1 adapter , caused similar transport velocity impairment but different spatial distributions . Together , our data reveals that stationary mitochondria within the axon interconnect with moving mitochondria through fusion and fission and that local inner membrane fusion between individual mitochondria mediates their global distribution .
2K_dev_1454	Bayesian theory has provided a compelling conceptualization for perceptual inference in the brain . Central to Bayesian inference is the notion of statistical priors . To understand the neural mechanisms of Bayesian inference , we need to understand the neural representation of statistical regularities in the natural environment . In this paper , we investigated empirically how statistical regularities in natural 3D scenes are represented in the functional connectivity of disparity-tuned neurons in the primary visual cortex of primates . We applied a Boltzmann machine model to learn from 3D natural scenes , and found that the units in the model exhibited cooperative and competitive interactions , forming a `` disparity association field '' , analogous to the contour association field . The cooperative and competitive interactions in the disparity association field are consistent with constraints of computational models for stereo matching . In addition , we simulated neurophysiological experiments on the model , and found the results to be consistent with neurophysiological data in terms of the functional connectivity measurements between disparity-tuned neurons in the macaque primary visual cortex . These findings demonstrate that there is a relationship between the functional connectivity observed in the visual cortex and the statistics of natural scenes . They also suggest that the Boltzmann machine can be a viable model for conceptualizing computations in the visual cortex and , as such , can be used to predict neural circuits in the visual cortex from natural scene statistics .
2K_dev_1456	We investigated the dynamics of head movement in mothers and infants during an age-appropriate , well-validated emotion induction , the Still Face paradigm . In this paradigm , mothers and infants play normally for 2 minutes ( Play ) followed by 2 minutes in which the mothers remain unresponsive ( Still Face ) , and then two minutes in which they resume normal behavior ( Reunion ) . Participants were 42 ethnically diverse 4-month-old infants and their mothers . Mother and infant angular displacement and angular velocity were measured using the CSIRO head tracker . In male but not female infants , angular displacement increased from Play to Still-Face and decreased from Still Face to Reunion . Infant angular velocity was higher during Still-Face than Reunion with no differences between male and female infants . Windowed cross-correlation suggested changes in how infant and mother head movements are associated , revealing dramatic changes in direction of association . Coordination between mother and infant head movement velocity was greater during Play compared with Reunion . Together , these findings suggest that angular displacement , angular velocity and their coordination between mothers and infants are strongly related to age-appropriate emotion challenge . Attention to head movement can deepen our understanding of emotion communication .
2K_dev_1457	Trauma often cooccurs with cardiac arrest and hemorrhagic shock . Skin and muscle injuries often lead to significant inflammation in the affected tissue . The primary mechanism by which inflammation is initiated , sustained , and terminated is cytokine-mediated immune signaling , but this signaling can be altered by cardiac arrest . The complexity and context sensitivity of immune signaling in general has stymied a clear understanding of these signaling dynamics .
2K_dev_1458	Characterizing the spatial distribution of proteins directly from microscopy images is a difficult problem with numerous applications in cell biology ( e.g . identifying motor-related proteins ) and clinical research ( e.g . identification of cancer biomarkers ) . Here we describe the design of a system that provides automated analysis of punctate protein patterns in microscope images , including quantification of their relationships to microtubules . We constructed the system using confocal immunofluorescence microscopy images from the Human Protein Atlas project for 11 punctate proteins in three cultured cell lines . These proteins have previously been characterized as being primarily located in punctate structures , but their images had all been annotated by visual examination as being simply `` vesicular '' . We were able to show that these patterns could be distinguished from each other with high accuracy , and we were able to assign to one of these subclasses hundreds of proteins whose subcellular localization had not previously been well defined . In addition to providing these novel annotations , we built a generative approach to modeling of punctate distributions that captures the essential characteristics of the distinct patterns . Such models are expected to be valuable for representing and summarizing each pattern and for constructing systems biology simulations of cell behaviors .
2K_dev_1460	The objective of this study was to compare to each other the methods currently recommended by the American Academy of Sleep Medicine ( AASM ) to measure snoring : an acoustic sensor , a piezoelectric sensor and a nasal pressure transducer ( cannula ) . Ten subjects reporting habitual snoring were included in the study , performed at Landspitali-University Hospital , Iceland . Snoring was assessed by listening to the air medium microphone located on a patient 's chest , compared to listening to two overhead air medium microphones ( stereo ) and manual scoring of a piezoelectric sensor and nasal cannula vibrations . The chest audio picked up the highest number of snore events of the different snore sensors . The sensitivity and positive predictive value of scoring snore events from the different sensors was compared to the chest audio : overhead audio ( 0.78 , 0.98 ) , cannula ( 0.55 , 0.67 ) and piezoelectric sensor ( 0.78 , 0.92 ) , respectively . The chest audio was capable of detecting snore events with lower volume and higher fundamental frequency than the other sensors . The 200 Hz sampling rate of the cannula and piezoelectric sensor was one of their limitations for detecting snore events . The different snore sensors do not measure snore events in the same manner . This lack of consistency will affect future research on the clinical significance of snoring . Standardization of objective snore measurements is therefore needed . Based on this paper , snore measurements should be audio-based and the use of the cannula as a snore sensor be discontinued , but the piezoelectric sensor could possibly be modified for improvement .
2K_dev_1461	Quantitative shape analysis is required by a wide range of biological studies across diverse scales , ranging from molecules to cells and organisms . In particular , high-throughput and systems-level studies of biological structures and functions have started to produce large volumes of complex high-dimensional shape data . Analysis and understanding of high-dimensional biological shape data require dimension-reduction techniques .
2K_dev_1462	Games for health ( G4H ) aim to improve health outcomes and encourage behavior change . While existing theoretical frameworks describe features of both games and health interventions , there has been limited systematic investigation into how disciplinary and interdisciplinary stakeholders understand design features in G4H . We recruited 18 experts from the fields of game design , behavioral health , and games for health , and prompted them with 16 sample games . Applying methods including open card sorting and triading , we elicited themes and features ( e.g. , real-world interaction , game mechanics ) around G4H . We found evidence of conceptual differences suggesting that a G4H perspective is not simply the sum of game and health perspectives . At the same time , we found evidence of convergence in stakeholder views , including areas where game experts provided insights about health and vice versa . We discuss how this work can be applied to provide conceptual tools , improve the G4H design process , and guide approaches to encoding G4H-related data for large-scale empirical analysis .
2K_dev_1463	Both the occurrence and intensity of facial expressions are critical to what the face reveals . While much progress has been made towards the automatic detection of facial expression occurrence , controversy exists about how to estimate expression intensity . The most straight-forward approach is to train multiclass or regression models using intensity ground truth . However , collecting intensity ground truth is even more time consuming and expensive than collecting binary ground truth . As a shortcut , some researchers have proposed using the decision values of binary-trained maximum margin classifiers as a proxy for expression intensity . We provide empirical evidence that this heuristic is flawed in practice as well as in theory . Unfortunately , there are no shortcuts when it comes to estimating smile intensity : researchers must take the time to collect and train on intensity ground truth . However , if they do so , high reliability with expert human coders can be achieved . Intensity-trained multiclass and regression models outperformed binary-trained classifier decision values on smile intensity estimation across multiple databases and methods for feature extraction and dimensionality reduction . Multiclass models even outperformed binary-trained classifiers on smile occurrence detection .
2K_dev_1464	Latent HIV-1 reservoirs are identified as one of the major challenges to achieve HIV-1 cure . Currently available strategies are associated with wide variability in outcomes both in patients and CD4 ( + ) T cell models . This underlines the critical need to develop innovative strategies to predict and recognize ways that could result in better reactivation and eventual elimination of latent HIV-1 reservoirs .
2K_dev_1465	Computer-assisted diagnosis of dermoscopic images of skin lesions has the potential to improve melanoma early detection .
2K_dev_1466	Laser photocoagulation is a mainstay or adjuvant treatment for a variety of common retinal diseases . Automated laser photocoagulation during intraocular surgery has not yet been established . The authors introduce an automated laser photocoagulation system for intraocular surgery , based on a novel handheld instrument . The goals of the system are to enhance accuracy and efficiency and improve safety .
2K_dev_1467	As sampling-based motion planners become faster , they can be re-executed more frequently by a robot during task execution to react to uncertainty in robot motion , obstacle motion , sensing noise , and uncertainty in the robot 's kinematic model . We investigate and analyze high-frequency replanning ( HFR ) , where , during each period , fast sampling-based motion planners are executed in parallel as the robot simultaneously executes the first action of the best motion plan from the previous period . We consider discrete-time systems with stochastic nonlinear ( but linearizable ) dynamics and observation models with noise drawn from zero mean Gaussian distributions . The objective is to maximize the probability of success ( i.e. , avoid collision with obstacles and reach the goal ) or to minimize path length subject to a lower bound on the probability of success . We show that , as parallel computation power increases , HFR offers asymptotic optimality for these objectives during each period for goal-oriented problems . We then demonstrate the effectiveness of HFR for holonomic and nonholonomic robots including car-like vehicles and steerable medical needles .
2K_dev_1468	Nationally sponsored cancer-care quality-improvement efforts have been deployed in community health centers to increase breast , cervical , and colorectal cancer-screening rates among vulnerable populations . Despite several immediate and short-term gains , screening rates remain below national benchmark objectives . Overall improvement has been both difficult to sustain over time in some organizational settings and/or challenging to diffuse to other settings as repeatable best practices . Reasons for this include facility-level changes , which typically occur in dynamic organizational environments that are complex , adaptive , and unpredictable . This study seeks to understand the factors that shape community health center facility-level cancer-screening performance over time . This study applies a computational-modeling approach , combining principles of health-services research , health informatics , network theory , and systems science .
2K_dev_1469	Management decisions underpinning availability of ecosystem services and the organisms that provide them in agroecosystems , such as pollinators and pollination services , have emerged as a foremost consideration for both conservation and crop production goals . There is growing evidence that innovative management practices can support diverse pollinators and increase crop pollination . However , there is also considerable debate regarding factors that support adoption of these innovative practices . This study investigated pollination management practices and related knowledge systems in a major crop producing region of southwest Michigan in the United States , where 367 growers were surveyed to evaluate adoption of three innovative practices that are at various stages of adoption . The goals of this quantitative , social survey were to investigate grower experience with concerns and benefits associated with each practice , as well as the influence of grower networks , which are comprised of contacts that reflect potential pathways for social and technical learning . The results demonstrated that 17 % of growers adopted combinations of bees ( e.g . honey bees , Apis mellifera , with other species ) , representing an innovation in use by early adopters ; 49 % of growers adopted flowering cover crops , an innovation in use by the early majority 55 % of growers retained permanent habitat for pollinators , an innovation in use by the late majority . Not all growers adopted innovative practices . We found that growers ' personal experience with potential benefits and concerns related to the management practices had significant positive and negative relationships , respectively , with adoption of all three innovations . The influence of these communication links likely has different levels of importance , depending on the stage of the adoption that a practice is experiencing in the agricultural community . Social learning was positively associated with adopting the use of combinations of bees , highlighting the potentially critical roles of peer-to-peer networks and social learning in supporting early stages of adoption of innovations . Engaging with grower networks and understanding grower experience with benefits and concerns associated with innovative practices is needed to inform outreach , extension , and policy efforts designed to stimulate management innovations in agroecosystems .
2K_dev_1470	Tumorigenesis is an evolutionary process by which tumor cells acquire mutations through successive diversification and differentiation . There is much interest in reconstructing this process of evolution due to its relevance to identifying drivers of mutation and predicting future prognosis and drug response . Efforts are challenged by high tumor heterogeneity , though , both within and among patients . In prior work , we showed that this heterogeneity could be turned into an advantage by computationally reconstructing models of cell populations mixed to different degrees in distinct tumors . Such mixed membership model approaches , however , are still limited in their ability to dissect more than a few well-conserved cell populations across a tumor data set .
2K_dev_1471	Robust , efficient , and low-cost networks are advantageous in both biological and engineered systems . During neural network development in the brain , synapses are massively over-produced and then pruned-back over time . This strategy is not commonly used when designing engineered networks , since adding connections that will soon be removed is considered wasteful . Here , we show that for large distributed routing networks , network function is markedly enhanced by hyper-connectivity followed by aggressive pruning and that the global rate of pruning , a developmental parameter not previously studied by experimentalists , plays a critical role in optimizing network structure . We first used high-throughput image analysis techniques to quantify the rate of pruning in the mammalian neocortex across a broad developmental time window and found that the rate is decreasing over time . Based on these results , we analyzed a model of computational routing networks and show using both theoretical analysis and simulations that decreasing rates lead to more robust and efficient networks compared to other rates . We also present an application of this strategy to improve the distributed design of airline networks . Thus , inspiration from neural network formation suggests effective ways to design distributed networks across several domains .
2K_dev_1472	There are likely marked differences in endotracheal intubation ( ETI ) techniques between novice and experienced providers . We performed a proof of concept study to determine if portable motion technology could identify the motion components of ETI between novice and experienced providers .
2K_dev_1473	On average , two thousand residents in the United States experience a stroke every day . These circumstances account for $ 28 billion direct costs annually and given the latest predictions , these costs will more than triple by 2030 . In our research , we propose a portfolio of serious games for home-based stroke rehabilitation . The objective of the game approach is to enrich the training experience and establish a higher level of compliance to prescribed exercises , while maintaining a supportive training environment as found in common therapy sessions . Our system provides a collection of mini games based on rehabilitation exercises used in conventional physical therapy , monitors the patient 's performance while exercising and provides clinicians with an interface to personalize the training . The clinician can set the current state of rehabilitation and change the playable games over time to drive diversification . While the system still has to be evaluated , an early stage case study with one patient offered positive indications towards this concept .
2K_dev_1474	Analogous to genomic sequence alignment , biological network alignment identifies conserved regions between networks of different species . Then , function can be transferred from well- to poorly-annotated species between aligned network regions . Network alignment typically encompasses two algorithmic components : node cost function ( NCF ) , which measures similarities between nodes in different networks , and alignment strategy ( AS ) , which uses these similarities to rapidly identify high-scoring alignments . Different methods use both different NCFs and different ASs . Thus , it is unclear whether the superiority of a method comes from its NCF , its AS , or both . We already showed on state-of-the-art methods , MI-GRAAL and IsoRankN , that combining NCF of one method and AS of another method can give a new superior method . Here , we evaluate MI-GRAAL against a newer approach , GHOST , by mixing-and-matching the methods ' NCFs and ASs to potentially further improve alignment quality . While doing so , we approach important questions that have not been asked systematically thus far . First , we ask how much of the NCF information should come from protein sequence data compared to network topology data . Existing methods determine this parameter more-less arbitrarily , which could affect alignment quality . Second , when topological information is used in NCF , we ask how large the size of the neighborhoods of the compared nodes should be . Existing methods assume that the larger the neighborhood size , the better .
2K_dev_1476	In studying the strength and specificity of interaction between members of two protein families , key questions center on which pairs of possible partners actually interact , how well they interact , and why they interact while others do not . The advent of large-scale experimental studies of interactions between members of a target family and a diverse set of possible interaction partners offers the opportunity to address these questions . We develop here a method , DgSpi ( data-driven graphical models of specificity in protein : protein interactions ) , for learning and using graphical models that explicitly represent the amino acid basis for interaction specificity ( why ) and extend earlier classification-oriented approaches ( which ) to predict the _G of binding ( how well ) . We demonstrate the effectiveness of our approach in analyzing and predicting interactions between a set of 82 PDZ recognition modules against a panel of 217 possible peptide partners , based on data from MacBeath and colleagues . Our predicted _G values are highly predictive of the experimentally measured ones , reaching correlation coefficients of 0.69 in 10-fold cross-validation and 0.63 in leave-one-PDZ-out cross-validation . Furthermore , the model serves as a compact representation of amino acid constraints underlying the interactions , enabling protein-level _G predictions to be naturally understood in terms of residue-level constraints . Finally , the model DgSpi readily enables the design of new interacting partners , and we demonstrate that designed ligands are novel and diverse .
2K_dev_1477	Peeling procedures in retinal surgery require micron-scale manipulation and control of sub-tactile forces .
2K_dev_1478	It is often assumed that central pattern generators , which generate rhythmic patterns without rhythmic inputs , play a key role in the spinal control of human locomotion . We propose a neural control model in which the spinal control generates muscle stimulations mainly through integrated reflex pathways with no central pattern generator . Using a physics-based neuromuscular human model , we show that this control network is sufficient to compose steady and transitional 3-D locomotion behaviours , including walking and running , acceleration and deceleration , slope and stair negotiation , turning , and deliberate obstacle avoidance . The results suggest feedback integration to be functionally more important than central pattern generation in human locomotion across behaviours . In addition , the proposed control architecture may serve as a guide in the search for the neurophysiological origin and circuitry of spinal control in humans .
2K_dev_1479	This paper presents a technique for automated intraocular laser surgery using a handheld micromanipulator known as Micron . The novel handheld manipulator enables the automated scanning of a laser probe within a cylinder of 4 mm long and 4 mm in diameter . For the automation , the surface of the retina is reconstructed using a stereomicroscope , and then preplanned targets are placed on the surface . The laser probe is precisely located on the target via visual servoing of the aiming beam , while maintaining a specific distance above the surface . In addition , the system is capable of tracking the surface of the eye in order to compensate for any eye movement introduced during the operation . We compared the performance of the automated scanning using various control thresholds , in order to find the most effective threshold in terms of accuracy and speed . Given the selected threshold , we conducted the handheld operation above a fixed target surface . The average error and execution time are reduced by 63.6 % and 28.5 % , respectively , compared to the unaided trials . Finally , the automated laser photocoagulation was demonstrated also in an eye phantom , including compensation for the eye movement .
2K_dev_1480	Many organisms move using traveling waves of body undulation , and most work has focused on single-plane undulations in fluids . Less attention has been paid to multiplane undulations , which are particularly important in terrestrial environments where vertical undulations can regulate substrate contact . A seemingly complex mode of snake locomotion , sidewinding , can be described by the superposition of two waves : horizontal and vertical body waves with a phase difference of 90 . We demonstrate that the high maneuverability displayed by sidewinder rattlesnakes ( Crotalus cerastes ) emerges from the animal 's ability to independently modulate these waves . Sidewinder rattlesnakes used two distinct turning methods , which we term differential turning ( 26 change in orientation per wave cycle ) and reversal turning ( 89 ) . Observations of the snakes suggested that during differential turning the animals imposed an amplitude modulation in the horizontal wave whereas in reversal turning they shifted the phase of the vertical wave by 180 . We tested these mechanisms using a multimodule snake robot as a physical model , successfully generating differential and reversal turning with performance comparable to that of the organisms . Further manipulations of the two-wave system revealed a third turning mode , frequency turning , not observed in biological snakes , which produced large ( 127 ) in-place turns . The two-wave system thus functions as a template ( a targeted motor pattern ) that enables complex behaviors in a high-degree-of-freedom system to emerge from relatively simple modulations to a basic pattern . Our study reveals the utility of templates in understanding the control of biological movement as well as in developing control schemes for limbless robots .
2K_dev_1481	The assessment of jaundice in outpatient neonates is problematic . Visual assessment is inaccurate , and more exact methodologies are cumbersome and/or expensive . Our goal in this study was to assess the accuracy of a technology based on the analysis of digital images of newborns obtained using a smartphone application called BiliCam .
2K_dev_1482	Gene therapies have emerged as a promising treatment for congestive heart failure , yet they lack a method for minimally invasive , uniform delivery . To address this need we developed Cerberus , a minimally invasive parallel wire robot for cardiac interventions . Prior work on controlling the movement of Cerberus required accurate knowledge of device geometry . In order to determine the geometry of the device in vivo , this paper presents work on developing an auto-calibration procedure to measure the geometry of the robot using force sensors to move injector . The presented auto-calibration routine is able to identify the shape of the device to within 0.5 mm and 0.9 .
2K_dev_1483	Recent studies implicate chromatin modifiers in autism spectrum disorder ( ASD ) through the identification of recurrent de novo loss of function mutations in affected individuals . ASD risk genes are co-expressed in human midfetal cortex , suggesting that ASD risk genes converge in specific regulatory networks during neurodevelopment . To elucidate such networks , we identify genes targeted by CHD8 , a chromodomain helicase strongly associated with ASD , in human midfetal brain , human neural stem cells ( hNSCs ) and embryonic mouse cortex . CHD8 targets are strongly enriched for other ASD risk genes in both human and mouse neurodevelopment , and converge in ASD-associated co-expression networks in human midfetal cortex . CHD8 knockdown in hNSCs results in dysregulation of ASD risk genes directly targeted by CHD8 . Integration of CHD8-binding data into ASD risk models improves detection of risk genes . These results suggest loss of CHD8 contributes to ASD by perturbing an ancient gene regulatory network during human brain development .
2K_dev_1484	The form that an animal takes during development is directed by gene regulatory networks ( GRNs ) . Developmental GRNs interpret maternally deposited molecules and externally supplied signals to direct cell-fate decisions , which ultimately leads to the arrangements of organs and tissues in the organism . Genetically encoded modifications to these networks have generated the wide range of metazoan diversity that exists today . Most studies of GRN evolution focus on changes to cis-regulatory DNA , and it was historically theorized that changes to the transcription factors that bind to these cis-regulatory modules ( CRMs ) contribute to this process only rarely . A growing body of evidence suggests that changes to the coding regions of transcription factors play a much larger role in the evolution of developmental gene regulatory networks than originally imagined . Just as cis-regulatory changes make use of modular binding site composition and tissue-specific modules to avoid pleiotropy , transcription factor coding regions also predominantly evolve in ways that limit the context of functional effects . Here , we review the recent works that have led to this unexpected change in the field of Evolution and Development ( Evo-Devo ) and consider the implications these studies have had on our understanding of the evolution of developmental processes .
2K_dev_1485	The overall purpose of this study was to learn how community-dwelling older adults would interact with our prototype multi-user telehealth kiosk and their views about its usability . Seven subjects participated in laboratory-based usability sessions to evaluate the physical design , appearance , functionality and perceived ease of use of a multi-user telehealth kiosk prototype . During usability testing participants recommended 18 new features ( 29 % of comments ) , identified 15 software errors ( 23 % of comments ) and 29 user interface errors ( 47 % of comments ) .
2K_dev_1486	The quantitative relationship between presynaptic calcium influx and transmitter release critically depends on the spatial coupling of presynaptic calcium channels to synaptic vesicles . When there is a close association between calcium channels and synaptic vesicles , the flux through a single open calcium channel may be sufficient to trigger transmitter release . With increasing spatial distance , however , a larger number of open calcium channels might be required to contribute sufficient calcium ions to trigger vesicle fusion . Here we used a combination of pharmacological calcium channel block , high-resolution calcium imaging , postsynaptic recording , and 3D Monte Carlo reaction-diffusion simulations in the adult frog neuromuscular junction , to show that release of individual synaptic vesicles is predominately triggered by calcium ions entering the nerve terminal through the nearest open calcium channel . Furthermore , calcium ion flux through this channel has a low probability of triggering synaptic vesicle fusion ( _6 % ) , even when multiple channels open in a single active zone . These mechanisms work to control the rare triggering of vesicle fusion in the frog neuromuscular junction from each of the tens of thousands of individual release sites at this large model synapse .
2K_dev_1487	Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data . For example , they are often used to explore complex systems where connections between entities are not well understood , such as in functional brain networks or genetic networks . Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable , such as a binary neural activation state or a continuous mRNA abundance measurement , even though in many real world problems , nodes can represent multivariate variables with much richer meanings , such as whole images , text documents , or multi-view feature vectors . In this paper , we propose a new principled framework for estimating the structure of undirected graphical models from such multivariate ( or multi-attribute ) nodal data . The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes . Under a Gaussian model , this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection ( Dempster , 1972 ) . We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective . Extensive simulation studies demonstrate the effectiveness of the method under various conditions . We provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles , and uncovering brain connectivity graph from positron emission tomography data . Finally , we provide sufficient conditions under which the 1 graphical structure can be recovered correctly .
2K_dev_1488	Calculating the edit-distance ( i.e . minimum number of insertions , deletions and substitutions ) between short DNA sequences is the primary task performed by seed-and-extend based mappers , which compare billions of sequences . In practice , only sequence pairs with a small edit-distance provide useful scientific data . However , the majority of sequence pairs analyzed by seed-and-extend based mappers differ by significantly more errors than what is typically allowed . Such error-abundant sequence pairs needlessly waste resources and severely hinder the performance of read mappers . Therefore , it is crucial to develop a fast and accurate filter that can rapidly and efficiently detect error-abundant string pairs and remove them from consideration before more computationally expensive methods are used .
2K_dev_1489	Individuals who exhibit large-magnitude blood pressure ( BP ) reactions to acute psychological stressors are at risk for hypertension and premature death by cardiovascular disease . This study tested whether a multivariate pattern of stressor-evoked brain activity could reliably predict individual differences in BP reactivity , providing novel evidence for a candidate neurophysiological source of stress-related cardiovascular risk .
2K_dev_1490	In the sparse linear regression setting , we consider testing the significance of the predictor variable that enters the current lasso model , in the sequence of models visited along the lasso solution path . We propose a simple test statistic based on lasso fitted values , called the covariance test statistic , and show that when the 1 model is linear , this statistic has an Exp ( 1 ) asymptotic distribution under the null hypothesis ( the null being that all truly active variables are contained in the current lasso model ) . Our proof of this result for the special case of the first predictor to enter the model ( i.e. , testing for a single significant predictor variable against the global null ) requires only weak assumptions on the predictor matrix X . On the other hand , our proof for a general step in the lasso path places further technical assumptions on X and the generative model , but still allows for the important high-dimensional case p > n , and does not necessarily require that the current lasso model achieves perfect recovery of the truly active variables . Of course , for testing the significance of an additional variable between two nested linear models , one typically uses the chi-squared test , comparing the drop in residual sum of squares ( RSS ) to a [ Formula : see text ] distribution . But when this additional variable is not fixed , and has been chosen adaptively or greedily , this test is no longer appropriate : adaptivity makes the drop in RSS stochastically much larger than [ Formula : see text ] under the null hypothesis . Our analysis explicitly accounts for adaptivity , as it must , since the lasso builds an adaptive sequence of linear models as the tuning parameter _ decreases . In this analysis , shrinkage plays a key role : though additional variables are chosen adaptively , the coefficients of lasso active variables are shrunken due to the [ Formula : see text ] penalty . Therefore , the test statistic ( which is based on lasso fitted values ) is in a sense balanced by these two opposing properties-adaptivity and shrinkage-and its null distribution is tractable and asymptotically Exp ( 1 ) .
2K_dev_1491	Recently there has been substantial interest in spectral methods for learning dynamical systems . These methods are popular since they often offer a good tradeoff between computational and statistical efficiency . Unfortunately , they can be difficult to use and extend in practice : e.g. , they can make it difficult to incorporate prior information such as sparsity or structure . To address this problem , we present a new view of dynamical system learning : we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems , thereby allowing users to incorporate prior knowledge via standard techniques such as L1 regularization . Many existing spectral methods are special cases of this new framework , using linear regression as the supervised learner . We demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does ; the correctness of these instances follows directly from our general analysis .
2K_dev_1492	While studies show that autism is highly heritable , the nature of the genetic basis of this disorder remains illusive . Based on the idea that highly correlated genes are functionally interrelated and more likely to affect risk , we develop a novel statistical tool to find more potentially autism risk genes by combining the genetic association scores with gene co-expression in specific brain regions and periods of development . The gene dependence network is estimated using a novel partial neighborhood selection ( PNS ) algorithm , where node specific properties are incorporated into network estimation for improved statistical and computational efficiency . Then we adopt a hidden Markov random field ( HMRF ) model to combine the estimated network and the genetic association scores in a systematic manner . The proposed modeling framework can be naturally extended to incorporate additional structural information concerning the dependence between genes . Using currently available genetic association data from whole exome sequencing studies and brain gene expression levels , the proposed algorithm successfully identified 333 genes that plausibly affect autism risk .
2K_dev_1493	Studying markets for illegal drugs is important , but difficult . Data usually come from a selected subset of consumers , such as arrestees , treatment clients , or household survey respondents . There are rarely opportunities to study how such groups may differ from other market participants or how much of total consumption they represent .
2K_dev_1494	Reconstructing regulatory and signaling response networks is one of the major goals of systems biology . While several successful methods have been suggested for this task , some integrating large and diverse datasets , these methods have so far been applied to reconstruct a single response network at a time , even when studying and modeling related conditions . To improve network reconstruction we developed MT-SDREM , a multi-task learning method which jointly models networks for several related conditions . In MT-SDREM , parameters are jointly constrained across the networks while still allowing for condition-specific pathways and regulation . We formulate the multi-task learning problem and discuss methods for optimizing the joint target function . We applied MT-SDREM to reconstruct dynamic human response networks for three flu strains : H1N1 , H5N1 and H3N2 . Our multi-task learning method was able to identify known and novel factors and genes , improving upon prior methods that model each condition independently . The MT-SDREM networks were also better at identifying proteins whose removal affects viral load indicating that joint learning can still lead to accurate , condition-specific , networks . Supporting website with MT-SDREM implementation : http : //sb.cs.cmu.edu/mtsdrem .
2K_dev_1495	Methods to assess individual facial actions have potential to shed light on important behavioral phenomena ranging from emotion and social interaction to psychological disorders and health . However , manual coding of such actions is labor intensive and requires extensive training . To date , establishing reliable automated coding of unscripted facial actions has been a daunting challenge impeding development of psychological theories and applications requiring facial expression assessment . It is therefore essential that automated coding systems be developed with enough precision and robustness to ease the burden of manual coding in challenging data involving variation in participant gender , ethnicity , head pose , speech , and occlusion . We report a major advance in automated coding of spontaneous facial actions during an unscripted social interaction involving three strangers . For each participant ( n 0 80 , 47 % women , 15 % Nonwhite ) , 25 facial action units ( AUs ) were manually coded from video using the Facial Action Coding System . Twelve AUs occurred more than 3 % of the time and were processed using automated FACS coding . Automated coding showed very strong reliability for the proportion of time that each AU occurred ( mean intraclass correlation 0 0.89 ) , and the more stringent criterion of frame-by-frame reliability was moderate to strong ( mean Matthew 's correlation 0 0.61 ) . With few exceptions , differences in AU detection related to gender , ethnicity , pose , and average pixel intensity were small . Fewer than 6 % of frames could be coded manually but not automatically . These findings suggest automated FACS coding has progressed sufficiently to be applied to observational research in emotion and related areas of study .
2K_dev_1496	Current methods for reconstructing dynamic regulatory networks are focused on modeling a single response network using model organisms or cell lines . Unlike these models or cell lines , humans differ in their background expression profiles due to age , genetics and life factors . In addition , there are often differences in start and end times for time series human data and in the rate of progress based on the specific individual . Thus , new methods are required to integrate time series data from multiple individuals when modeling and constructing disease response networks .
2K_dev_1497	Autism is a psychiatric/neurological condition in which alterations in social interaction ( among other symptoms ) are diagnosed by behavioral psychiatric methods . The main goal of this study was to determine how the neural representations and meanings of social concepts ( such as to insult ) are altered in autism . A second goal was to determine whether these alterations can serve as neurocognitive markers of autism . The approach is based on previous advances in fMRI analysis methods that permit ( a ) the identification of a concept , such as the thought of a physical object , from its fMRI pattern , and ( b ) the ability to assess the semantic content of a concept from its fMRI pattern . These factor analysis and machine learning methods were applied to the fMRI activation patterns of 17 adults with high-functioning autism and matched controls , scanned while thinking about 16 social interactions . One prominent neural representation factor that emerged ( manifested mainly in posterior midline regions ) was related to self-representation , but this factor was present only for the control participants , and was near-absent in the autism group . Moreover , machine learning algorithms classified individuals as autistic or control with 97 % accuracy from their fMRI neurocognitive markers . The findings suggest that psychiatric alterations of thought can begin to be biologically understood by assessing the form and content of the altered thought 's underlying brain activation patterns .
2K_dev_1498	Given a simple noun such as apple , and a question such as `` Is it edible ? , '' what processes take place in the human brain ? More specifically , given the stimulus , what are the interactions between ( groups of ) neurons ( also known as functional connectivity ) and how can we automatically infer those interactions , given measurements of the brain activity ? Furthermore , how does this connectivity differ across different human subjects ? In this work , we show that this problem , even though originating from the field of neuroscience , can benefit from big data techniques ; we present a simple , novel good-enough brain model , or GeBM in short , and a novel algorithm Sparse-SysId , which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity . Moreover , GeBM is able to simulate basic psychological phenomena such as habituation and priming ( whose definition we provide in the main text ) . We evaluate GeBM by using real brain data . GeBM produces brain activity patterns that are strikingly similar to the real ones , where the inferred functional connectivity is able to provide neuroscientific insights toward a better understanding of the way that neurons interact with each other , as well as detect regularities and outliers in multisubject brain activity measurements .
2K_dev_1499	Story understanding involves many perceptual and cognitive subprocesses , from perceiving individual words , to parsing sentences , to understanding the relationships among the story characters . We present an integrated computational model of reading that incorporates these and additional subprocesses , simultaneously discovering their fMRI signatures . Our model predicts the fMRI activity associated with reading arbitrary text passages , well enough to distinguish which of two story segments is being read with 74 % accuracy . This approach is the first to simultaneously track diverse reading subprocesses during complex story processing and predict the detailed neural representation of diverse story features , ranging from visual word properties to the mention of different story characters and different actions they perform . We construct brain representation maps that replicate many results from a wide range of classical studies that focus each on one aspect of language processing and offer new insights on which type of information is processed by different areas involved in language processing . Additionally , this approach is promising for studying individual differences : it can be used to create single subject maps that may potentially be used to measure reading comprehension and diagnose reading disorders .
2K_dev_1500	This paper presents the design and actuation of a six-degree-of-freedom ( 6-DOF ) manipulator for a handheld instrument , known as `` Micron , '' which performs active tremor compensation during microsurgery . The design incorporates a Gough-Stewart platform based on piezoelectric linear motor , with a specified minimum workspace of a cylinder 4 mm long and 4 mm in diameter at the end-effector . Given the stall force of the motors and the loading typically encountered in vitreoretinal microsurgery , the dimensions of the manipulator are optimized to tolerate a transverse load of 0.2 N on a remote center of motion near the midpoint of the tool shaft . The optimization yields a base diameter of 23 mm and a height of 37 mm . The fully handheld instrument includes a custom-built optical tracking system for control feedback , and an ergonomic housing to serve as a handle . The manipulation performance was investigated in both clamped and handheld conditions . In positioning experiments with varying side loads , the manipulator tolerates side load up to 0.25 N while tracking a sinusoidal target trajectory with less than 20 _m error . Physiological hand tremor is reduced by about 90 % in a pointing task , and error less than 25 _m is achieved in handheld circle-tracing .
2K_dev_1501	In studying the strength and specificity of interaction between members of two protein families , key questions center on which pairs of possible partners actually interact , how well they interact , and why they interact while others do not . The advent of large-scale experimental studies of interactions between members of a target family and a diverse set of possible interaction partners offers the opportunity to address these questions . We develop here a method , DgSpi ( Data-driven Graphical models of Specificity in Protein : protein Interactions ) , for learning and using graphical models that explicitly represent the amino acid basis for interaction specificity ( why ) and extend earlier classification-oriented approaches ( which ) to predict the _G of binding ( how well ) . We demonstrate the effectiveness of our approach in analyzing and predicting interactions between a set of 82 PDZ recognition modules , against a panel of 217 possible peptide partners , based on data from MacBeath and colleagues . Our predicted _G values are highly predictive of the experimentally measured ones , reaching correlation coefficients of 0.69 in 10-fold cross-validation and 0.63 in leave-one-PDZ-out cross-validation . Furthermore , the model serves as a compact representation of amino acid constraints underlying the interactions , enabling protein-level _G predictions to be naturally understood in terms of residue-level constraints . Finally , as a generative model , DgSpi readily enables the design of new interacting partners , and we demonstrate that designed ligands are novel and diverse .
2K_dev_1502	Computer-mediated communication is driving fundamental changes in the nature of written language . We investigate these changes by statistical analysis of a dataset comprising 107 million Twitter messages ( authored by 2.7 million unique user accounts ) . Using a latent vector autoregressive model to aggregate across thousands of words , we identify high-level patterns in diffusion of linguistic change over the United States . Our model is robust to unpredictable changes in Twitter 's sampling rate , and provides a probabilistic characterization of the relationship of macro-scale linguistic influence to a set of demographic and geographic predictors . The results of this analysis offer support for prior arguments that focus on geographical proximity and population size . However , demographic similarity - especially with regard to race - plays an even more central role , as cities with similar racial demographics are far more likely to share linguistic influence . Rather than moving towards a single unified `` netspeak '' dialect , language evolution in computer-mediated communication reproduces existing fault lines in spoken American English .
2K_dev_1503	Many statistical methods gain robustness and flexibility by sacrificing convenient computational structures . In this paper , we illustrate this fundamental tradeoff by studying a semi-parametric graph estimation problem in high dimensions . We explain how novel computational techniques help to solve this type of problem . In particular , we propose a nonparanormal neighborhood pursuit algorithm to estimate high dimensional semiparametric graphical models with theoretical guarantees . Moreover , we provide an alternative view to analyze the tradeoff between computational efficiency and statistical error under a smoothing optimization framework . Though this paper focuses on the problem of graph estimation , the proposed methodology is widely applicable to other problems with similar structures . We also report thorough experimental results on text , stock , and genomic datasets .
2K_dev_1504	While only recently developed , the ability to profile expression data in single cells ( scRNA-Seq ) has already led to several important studies and findings . However , this technology has also raised several new computational challenges . These include questions about the best methods for clustering scRNA-Seq data , how to identify unique group of cells in such experiments , and how to determine the state or function of specific cells based on their expression profile . To address these issues we develop and test a method based on neural networks ( NN ) for the analysis and retrieval of single cell RNA-Seq data . We tested various NN architectures , some of which incorporate prior biological knowledge , and used these to obtain a reduced dimension representation of the single cell expression data . We show that the NN method improves upon prior methods in both , the ability to correctly group cells in experiments not used in the training and the ability to correctly infer cell type or state by querying a database of tens of thousands of single cell profiles . Such database queries ( which can be performed using our web server ) will enable researchers to better characterize cells when analyzing heterogeneous scRNA-Seq samples .
2K_dev_1505	Limbless organisms such as snakes can navigate nearly all terrain . In particular , desert-dwelling sidewinder rattlesnakes ( Crotalus cerastes ) operate effectively on inclined granular media ( such as sand dunes ) that induce failure in field-tested limbless robots through slipping and pitching . Our laboratory experiments reveal that as granular incline angle increases , sidewinder rattlesnakes increase the length of their body in contact with the sand . Implementing this strategy in a physical robot model of the snake enables the device to ascend sandy slopes close to the angle of maximum slope stability . Plate drag experiments demonstrate that granular yield stresses decrease with increasing incline angle . Together , these three approaches demonstrate how sidewinding with contact-length control mitigates failure on granular media .
2K_dev_1506	Whole-exome sequencing ( WES ) studies have demonstrated the contribution of de novo loss-of-function single-nucleotide variants ( SNVs ) to autism spectrum disorder ( ASD ) . However , challenges in the reliable detection of de novo insertions and deletions ( indels ) have limited inclusion of these variants in prior analyses . By applying a robust indel detection method to WES data from 787 ASD families ( 2963 individuals ) , we demonstrate that de novo frameshift indels contribute to ASD risk ( OR 0 1.6 ; 95 % CI 0 1.0-2.7 ; p 0 0.03 ) , are more common in female probands ( p 0 0.02 ) , are enriched among genes encoding FMRP targets ( p 0 6 _ 10 ( -9 ) ) , and arise predominantly on the paternal chromosome ( p < 0.001 ) . On the basis of mutation rates in probands versus unaffected siblings , we conclude that de novo frameshift indels contribute to risk in approximately 3 % of individuals with ASD . Finally , by observing clustering of mutations in unrelated probands , we uncover two ASD-associated genes : KMT2E ( MLL5 ) , a chromatin regulator , and RIMS1 , a regulator of synaptic vesicle release .
2K_dev_1507	Fluctuations in the growth rate of a bacterial culture during unbalanced growth are generally considered undesirable in quantitative studies of bacterial physiology . Under well-controlled experimental conditions , however , these fluctuations are not random but instead reflect the interplay between intra-cellular networks underlying bacterial growth and the growth environment . Therefore , these fluctuations could be considered quantitative phenotypes of the bacteria under a specific growth condition . Here , we present a method to identify `` phenotypic signatures '' by time-frequency analysis of unbalanced growth curves measured with high temporal resolution . The signatures are then applied to differentiate amongst different bacterial strains or the same strain under different growth conditions , and to identify the essential architecture of the gene network underlying the observed growth dynamics . Our method has implications for both basic understanding of bacterial physiology and for the classification of bacterial strains .
2K_dev_1508	Spontaneously arising ( de novo ) mutations have an important role in medical genetics . For diseases with extensive locus heterogeneity , such as autism spectrum disorders ( ASDs ) , the signal from de novo mutations is distributed across many genes , making it difficult to distinguish disease-relevant mutations from background variation . Here we provide a statistical framework for the analysis of excesses in de novo mutation per gene and gene set by calibrating a model of de novo mutation . We applied this framework to de novo mutations collected from 1078 ASD family trios , and , whereas we affirmed a significant role for loss-of-function mutations , we found no excess of de novo loss-of-function mutations in cases with IQ above 100 , suggesting that the role of de novo mutations in ASDs might reside in fundamental neurodevelopmental processes . We also used our model to identify _1,000 genes that are significantly lacking in functional coding variation in non-ASD samples and are enriched for de novo loss-of-function mutations identified in ASD cases .
2K_dev_1509	Influence maximization in social networks has been widely studied motivated by applications like spread of ideas or innovations in a network and viral marketing of products . Current studies focus almost exclusively on unsigned social networks containing only positive relationships ( e.g . friend or trust ) between users . Influence maximization in signed social networks containing both positive relationships and negative relationships ( e.g . foe or distrust ) between users is still a challenging problem that has not been studied . Thus , in this paper , we propose the polarity-related influence maximization ( PRIM ) problem which aims to find the seed node set with maximum positive influence or maximum negative influence in signed social networks . To address the PRIM problem , we first extend the standard Independent Cascade ( IC ) model to the signed social networks and propose a Polarity-related Independent Cascade ( named IC-P ) diffusion model . We prove that the influence function of the PRIM problem under the IC-P model is monotonic and submodular Thus , a greedy algorithm can be used to achieve an approximation ratio of 1-1/e for solving the PRIM problem in signed social networks . Experimental results on two signed social network datasets , Epinions and Slashdot , validate that our approximation algorithm for solving the PRIM problem outperforms state-of-the-art methods .
2K_dev_1510	The HMT3522 progression series of human breast cells have been used to discover how tissue architecture , microenvironment and signaling molecules affect breast cell growth and behaviors . However , much remains to be elucidated about malignant and phenotypic reversion behaviors of the HMT3522-T4-2 cells of this series . We employed a `` pan-cell-state '' strategy , and analyzed jointly microarray profiles obtained from different state-specific cell populations from this progression and reversion model of the breast cells using a tree-lineage multi-network inference algorithm , Treegl . We found that different breast cell states contain distinct gene networks . The network specific to non-malignant HMT3522-S1 cells is dominated by genes involved in normal processes , whereas the T4-2-specific network is enriched with cancer-related genes . The networks specific to various conditions of the reverted T4-2 cells are enriched with pathways suggestive of compensatory effects , consistent with clinical data showing patient resistance to anticancer drugs . We validated the findings using an external dataset , and showed that aberrant expression values of certain hubs in the identified networks are associated with poor clinical outcomes . Thus , analysis of various reversion conditions ( including non-reverted ) of HMT3522 cells using Treegl can be a good model system to study drug effects on breast cancer .
2K_dev_1511	Fast and accurate mapping and localization of the retinal vasculature is critical to increasing the effectiveness and clinical utility of robot-assisted intraocular microsurgery such as laser photocoagulation and retinal vessel cannulation .
2K_dev_1512	A key component of genetic architecture is the allelic spectrum influencing trait variability . For autism spectrum disorder ( herein termed autism ) , the nature of the allelic spectrum is uncertain . Individual risk-associated genes have been identified from rare variation , especially de novo mutations . From this evidence , one might conclude that rare variation dominates the allelic spectrum in autism , yet recent studies show that common variation , individually of small effect , has substantial impact en masse . At issue is how much of an impact relative to rare variation this common variation has . Using a unique epidemiological sample from Sweden , new methods that distinguish total narrow-sense heritability from that due to common variation and synthesis of results from other studies , we reach several conclusions about autism 's genetic architecture : its narrow-sense heritability is _52.4 % , with most due to common variation , and rare de novo mutations contribute substantially to individual liability , yet their contribution to variance in liability , 2.6 % , is modest compared to that for heritable variation .
2K_dev_1513	In effort to improve thermal control in minimally invasive cryosurgery , the concept of a miniature , wireless , implantable sensing unit has been developed recently . The sensing unit integrates a wireless power delivery mechanism , wireless communication means , and a sensing core-the subject matter of the current study . The current study presents a CMOS ultra-miniature PTAT temperature sensing core and focuses on design principles , fabrication of a proof-of-concept , and characterization in a cryogenic environment . For this purpose , a 100 _m _ 400 _m sensing core prototype has been fabricated using a 130 nm CMOS process . The senor has shown to operate between -180C and room temperature , to consume power of less than 1 _W , and to have an uncertainty range of 1.4C and non-linearity of 1.1 % . Results of this study suggest that the sensing core is ready to be integrated in the sensing unit , where system integration is the subject matter of a parallel effort .
2K_dev_1514	Stochastic models are increasingly used to study the behaviour of biochemical systems . While the structure of such models is often readily available from first principles , unknown quantitative features of the model are incorporated into the model as parameters . Algorithmic discovery of parameter values from experimentally observed facts remains a challenge for the computational systems biology community . We present a new parameter discovery algorithm that uses simulated annealing , sequential hypothesis testing , and statistical model checking to learn the parameters in a stochastic model . We apply our technique to a model of glucose and insulin metabolism used for in-silico validation of artificial pancreata and demonstrate its effectiveness by developing parallel CUDA-based implementation for parameter synthesis in this model .
2K_dev_1515	Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism . The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks , around which transcriptional regulation is focused owing to a disease or a genetic perturbation . Using a z-score-based scoring scheme , RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network . The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes . In this article , we propose a mutual information-based multivariate reporter algorithm ( MIRA ) with the goal of eliminating the following problems in detecting reporter metabolites : ( i ) conventional statistical methods suffer from small sample sizes , ( ii ) as z-score ranges from minus to plus infinity , calculating average scores can lead to canceling out opposite effects and ( iii ) analyzing genes one by one , then aggregating results can lead to information loss . MIRA is a multivariate and combinatorial algorithm that calculates the aggregate transcriptional response around a metabolite using mutual information . We show that MIRA 's results are biologically sound , empirically significant and more reliable than RA .
2K_dev_1516	Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism . The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks , around which transcriptional regulation is focused owing to a disease or a genetic perturbation . Using a z-score-based scoring scheme , RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network . The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes . In this article , we propose a mutual information-based multivariate reporter algorithm ( MIRA ) with the goal of eliminating the following problems in detecting reporter metabolites : ( i ) conventional statistical methods suffer from small sample sizes , ( ii ) as z-score ranges from minus to plus infinity , calculating average scores can lead to canceling out opposite effects and ( iii ) analyzing genes one by one , then aggregating results can lead to information loss . MIRA is a multivariate and combinatorial algorithm that calculates the aggregate transcriptional response around a metabolite using mutual information . We show that MIRA 's results are biologically sound , empirically significant and more reliable than RA .
2K_dev_1517	The vast bacteriophage population harbors an immense reservoir of genetic information . Almost 2000 phage genomes have been sequenced from phages infecting hosts in the phylum Actinobacteria , and analysis of these genomes reveals substantial diversity , pervasive mosaicism , and novel mechanisms for phage replication and lysogeny . Here , we describe the isolation and genomic characterization of 46 phages from environmental samples at various geographic locations in the U.S. infecting a single Arthrobacter sp . strain . These phages include representatives of all three virion morphologies , and Jasmine is the first sequenced podovirus of an actinobacterial host . The phages also span considerable sequence diversity , and can be grouped into 10 clusters according to their nucleotide diversity , and two singletons each with no close relatives . However , the clusters/singletons appear to be genomically well separated from each other , and relatively few genes are shared between clusters . Genome size varies from among the smallest of siphoviral phages ( 15319 bp ) to over 70 kbp , and G+C contents range from 45-68 % , compared to 63.4 % for the host genome . Although temperate phages are common among other actinobacterial hosts , these Arthrobacter phages are primarily lytic , and only the singleton Galaxy is likely temperate .
2K_dev_1518	With the continuous improvement in genotyping and molecular phenotyping technology and the decreasing typing cost , it is expected that in a few years , more and more clinical studies of complex diseases will recruit thousands of individuals for pan-omic genetic association analyses . Hence , there is a great need for algorithms and software tools that could scale up to the whole omic level , integrate different omic data , leverage rich structure information , and be easily accessible to non-technical users . We present GenAMap , an interactive analytics software platform that 1 ) automates the execution of principled machine learning methods that detect genome- and phenome-wide associations among genotypes , gene expression data , and clinical or other macroscopic traits , and 2 ) provides new visualization tools specifically designed to aid in the exploration of association mapping results . Algorithmically , GenAMap is based on a new paradigm for GWAS and PheWAS analysis , termed structured association mapping , which leverages various structures in the omic data . We demonstrate the function of GenAMap via a case study of the Brem and Kruglyak yeast dataset , and then apply it on a comprehensive eQTL analysis of the NIH heterogeneous stock mice dataset and report some interesting findings . GenAMap is available from http : //sailing.cs.cmu.edu/genamap .
2K_dev_1519	As we move towards the miniaturization of devices to perform tasks at the nano and microscale , it has become increasingly important to develop new methods for actuation , sensing , and control . Over the past decade , bio-hybrid methods have been investigated as a promising new approach to overcome the challenges of scaling down robotic and other functional devices . These methods integrate biological cells with artificial components and therefore , can take advantage of the intrinsic actuation and sensing functionalities of biological cells . Here , the recent advancements in bio-hybrid actuation are reviewed , and the challenges associated with the design , fabrication , and control of bio-hybrid microsystems are discussed . As a case study , focus is put on the development of bacteria-driven microswimmers , which has been investigated as a targeted drug delivery carrier . Finally , a future outlook for the development of these systems is provided . The continued integration of biological and artificial components is envisioned to enable the performance of tasks at a smaller and smaller scale in the future , leading to the parallel and distributed operation of functional systems at the microscale .
2K_dev_1520	Drug discovery and development has been aided by high throughput screening methods that detect compound effects on a single target . However , when using focused initial screening , undesirable secondary effects are often detected late in the development process after significant investment has been made . An alternative approach would be to screen against undesired effects early in the process , but the number of possible secondary targets makes this prohibitively expensive .
2K_dev_1521	Vitreoretinal microsurgery requires precise hand-eye coordination to manipulate delicate structures within the eye on the order of tens of microns . To achieve these tasks , surgeons use tools of diameter 0.9 mm or less to access the eye 's interior structures . The level of force required during these manipulations is often below the human tactile threshold , requiring the surgeon to rely on subtle visual cues or to apply larger forces above the tactile threshold for feedback . However , both of these methods can lead to tissue damage . Excursions can be made into tissues which are not felt by the surgeon , while larger forces have a higher chance of damaging tissue within the eye . To prevent damage to the retina and other anatomy , we present the implementation of hybrid position/force control operating in the sub-tactile force range for a handheld robotic system . This approach resulted in a 42 % reduction in the mean force and 52 % reduction in maximum force during peeling tasks .
2K_dev_1522	This Perspective presents recent advances in macromolecular engineering enabled by ATRP . They include the fundamental mechanistic and synthetic features of ATRP with emphasis on various catalytic/initiation systems that use parts-per-million concentrations of Cu catalysts and can be run in environmentally friendly media , e.g. , water . The roles of the major components of ATRP -- monomers , initiators , catalysts , and various additives -- are explained , and their reactivity and structure are correlated . The effects of media and external stimuli on polymerization rates and control are presented . Some examples of precisely controlled elements of macromolecular architecture , such as chain uniformity , composition , topology , and functionality , are discussed . Syntheses of polymers with complex architecture , various hybrids , and bioconjugates are illustrated . Examples of current and forthcoming applications of ATRP are covered . Future challenges and perspectives for macromolecular engineering by ATRP are discussed .
2K_dev_1523	Overview . The Centers for Disease Control and Prevention ( CDC ) health-related quality of life ( HRQoL ) indicators are widely used in the general population to determine the burden of disease , identify health needs , and direct public health policy . These indicators also allow the burden of illness to be compared across different diseases . Although Lyme disease has recently been acknowledged as a major health threat in the USA with more than 300000 new cases per year , no comprehensive assessment of the health burden of this tickborne disease is available . This study assesses the HRQoL of patients with chronic Lyme disease ( CLD ) and compares the severity of CLD to other chronic conditions . Methods . Of 5357 subjects who responded to an online survey , 3090 were selected for the study . Respondents were characterized as having CLD if they were clinically diagnosed with Lyme disease and had persisting symptoms lasting more than 6 months following antibiotic treatment . HRQoL of CLD patients was assessed using the CDC 9-item metric . The HRQoL analysis for CLD was compared to published analyses for the general population and other chronic illnesses using standard statistical methods . Results . Compared to the general population and patients with other chronic diseases reviewed here , patients with CLD reported significantly lower health quality status , more bad mental and physical health days , a significant symptom disease burden , and greater activity limitations . They also reported impairment in their ability to work , increased utilization of healthcare services , and greater out of pocket medical costs . Conclusions . CLD patients have significantly impaired HRQoL and greater healthcare utilization compared to the general population and patients with other chronic diseases . The heavy burden of illness associated with CLD highlights the need for earlier diagnosis and innovative treatment approaches that may reduce the burden of illness and concomitant costs posed by this illness .
2K_dev_1524	Performing micromanipulation and delicate operations in submillimeter workspaces is difficult because of destabilizing tremor and imprecise targeting . Accurate micromanipulation is especially important for microsurgical procedures , such as vitreoretinal surgery , to maximize successful outcomes and minimize collateral damage . Robotic aid combined with filtering techniques that suppress tremor frequency bands increases performance ; however , if knowledge of the operator 's goals is available , virtual fixtures have been shown to further improve performance . In this paper , we derive a virtual fixture framework for active handheld micromanipulators that is based on high-bandwidth position measurements rather than forces applied to a robot handle . For applicability in surgical environments , the fixtures are generated in real-time from microscope video during the procedure . Additionally , we develop motion scaling behavior around virtual fixtures as a simple and direct extension to the proposed framework . We demonstrate that virtual fixtures significantly outperform tremor cancellation algorithms on a set of synthetic tracing tasks ( p < 0.05 ) . In more medically relevant experiments of vein tracing and membrane peeling in eye phantoms , virtual fixtures can significantly reduce both positioning error and forces applied to tissue ( p < 0.05 ) .
2K_dev_1525	Population stratification is an important task in genetic analyses . It provides information about the ancestry of individuals and can be an important confounder in genome-wide association studies . Public genotyping projects have made a large number of datasets available for study . However , practical constraints dictate that of a geographical/ethnic population , only a small number of individuals are genotyped . The resulting data are a sample from the entire population . If the distribution of sample sizes is not representative of the populations being sampled , the accuracy of population stratification analyses of the data could be affected . We attempt to understand the effect of biased sampling on the accuracy of population structure analysis and individual ancestry recovery . We examined two commonly used methods for analyses of such datasets , ADMIXTURE and EIGENSOFT , and found that the accuracy of recovery of population structure is affected to a large extent by the sample used for analysis and how representative it is of the underlying populations . Using simulated data and real genotype data from cattle , we show that sample selection bias can affect the results of population structure analyses . We develop a mathematical framework for sample selection bias in models for population structure and also proposed a correction for sample selection bias using auxiliary information about the sample . We demonstrate that such a correction is effective in practice using simulated and real data .
2K_dev_1526	Can we use graph mining algorithms to find patterns in tumor molecular mechanisms ? Can we model disease progression with multiple time-specific graph comparison algorithms ? In this paper , we will focus on this area . Our main contributions are 1 ) we proposed the Temporal-Omics ( Temp-O ) workflow to model tumor progression in non-small cell lung cancer ( NSCLC ) using graph comparisons between multiple stage-specific graphs , and 2 ) we showed that temporal structures are meaningful in the tumor progression of NSCLC . Other identified temporal structures that were not highlighted in this paper may also be used to gain insights to possible novel mechanisms . Importantly , the Temp-O workflow is generic ; while we applied it on NSCLC , it can be applied in other cancers and diseases . We used gene expression data from tumor samples across disease stages to model lung cancer progression , creating stage-specific tumor graphs . Validating our findings in independent datasets showed that differences in temporal network structures capture diverse mechanisms in NSCLC . Furthermore , results showed that structures are consistent and potentially biologically important as we observed that genes with similar protein names were captured in the same cliques for all cliques in all datasets . Importantly , the identified temporal structures are meaningful in the tumor progression of NSCLC as they agree with the molecular mechanism in the tumor progression or carcinogenesis of NSCLC . In particular , the identified major histocompatibility complex of class II temporal structures capture mechanisms concerning carcinogenesis ; the proteasome temporal structures capture mechanisms that are in early or late stages of lung cancer ; the ribosomal cliques capture the role of ribosome biosynthesis in cancer development and sustainment . Further , on a large independent dataset we validated that temporal network structures identified proteins that are prognostic for overall survival in NSCLC adenocarcinoma .
2K_dev_1527	The spindle is a highly complex and dynamic molecular machine that is assembled during cell division for accurate segregation of replicated chromosomes . Successful completion of cell division relies on the right spindle proteins to be at the right place at the right time to serve their functions . Quantitative characterization and analysis of spatiotemporal behaviors of spindle proteins are therefore essential to understanding related cell division mechanisms . The main goal of this chapter is to introduce basic concepts and methods for computational tracking and analysis of spindle protein spatiotemporal dynamics that is visualized and recorded in fluorescence microscopy images . An emphasis is placed on providing practical and useful information on related software tools . Examples are used to demonstrate applications of related computational methods and software tools .
2K_dev_1528	Matched field processing is a model-based framework for localizing targets in complex propagation environments . In underwater acoustics , it has been extensively studied for improving localization performance in multimodal and multipath media . For guided wave structural health monitoring problems , matched field processing has not been widely applied but is an attractive option for damage localization due to equally complex propagation environments . Although effective , matched field processing is often challenging to implement because it requires accurate models of the propagation environment , and the optimization methods used to generate these models are often unreliable and computationally expensive . To address these obstacles , this paper introduces data-driven matched field processing , a framework to build models of multimodal propagation environments directly from measured data , and then use these models for localization . This paper presents the data-driven framework , analyzes its behavior under unmodeled multipath interference , and demonstrates its localization performance by distinguishing two nearby scatterers from experimental measurements of an aluminum plate . Compared with delay-based models that are commonly used in structural health monitoring , the data-driven matched field processing framework is shown to successfully localize two nearby scatterers with significantly smaller localization errors and finer resolutions .
2K_dev_1529	Previous studies have identified asthma phenotypes based on small numbers of clinical , physiologic , or inflammatory characteristics . However , no studies have used a wide range of variables using machine learning approaches .
2K_dev_1530	Recent technological advances coupled with large sample sets have uncovered many factors underlying the genetic basis of traits and the predisposition to complex disease , but much is left to discover . A common thread to most genetic investigations is familial relationships . Close relatives can be identified from family records , and more distant relatives can be inferred from large panels of genetic markers . Unfortunately these empirical estimates can be noisy , especially regarding distant relatives . We propose a new method for denoising genetically-inferred relationship matrices by exploiting the underlying structure due to hierarchical groupings of correlated individuals . The approach , which we call Treelet Covariance Smoothing , employs a multiscale decomposition of covariance matrices to improve estimates of pairwise relationships . On both simulated and real data , we show that smoothing leads to better estimates of the relatedness amongst distantly related individuals . We illustrate our method with a large genome-wide association study and estimate the `` heritability '' of body mass index quite accurately . Traditionally heritability , defined as the fraction of the total trait variance attributable to additive genetic effects , is estimated from samples of closely related individuals using random effects models . We show that by using smoothed relationship matrices we can estimate heritability using population-based samples . Finally , while our methods have been developed for refining genetic relationship matrices and improving estimates of heritability , they have much broader potential application in statistics . Most notably , for error-in-variables random effects models and settings that require regularization of matrices with block or hierarchical structure .
2K_dev_1531	Good feature design is important to achieve effective image classification . This paper presents a novel feature design with two main contributions . First , prior to computing the feature descriptors , we propose to transform the images with learning-based filters to obtain more representative feature descriptors . Second , we propose to transform the computed descriptors with another set of learning-based filters to further improve the classification accuracy . In this way , while generic feature descriptors are used , data-adaptive information is integrated into the feature extraction process based on the optimization objective to enhance the discriminative power of feature descriptors . The feature design is applicable to different application domains , and is evaluated on both lung tissue classification in high-resolution computed tomography ( HRCT ) images and apoptosis detection in time-lapse phase contrast microscopy image sequences . Both experiments show promising performance improvements over the state-of-the-art .
2K_dev_1532	One of the central concerns of Evolutionary Developmental biology is to understand how the specification of cell types can change during evolution . In the last decade , developmental biology has progressed toward a systems level understanding of cell specification processes . In particular , the focus has been on determining the regulatory interactions of the repertoire of genes that make up gene regulatory networks ( GRNs ) . Echinoderms provide an extraordinary model system for determining how GRNs evolve . This review highlights the comparative GRN analyses arising from the echinoderm system . This work shows that certain types of GRN subcircuits or motifs , i.e. , those involving positive feedback , tend to be conserved and may provide a constraint on development . This conservation may be due to a required arrangement of transcription factor binding sites in cis regulatory modules . The review will also discuss ways in which novelty may arise , in particular through the co-option of regulatory genes and subcircuits . The development of the sea urchin larval skeleton , a novel feature that arose in echinoderms , has provided a model for study of co-option mechanisms . Finally , the types of GRNs that can permit the great diversity in the patterns of ciliary bands and their associated neurons found among these taxa are discussed . The availability of genomic resources is rapidly expanding for echinoderms , including genome sequences not only for multiple species of sea urchins but also a species of sea star , sea cucumber , and brittle star . This will enable echinoderms to become a particularly powerful system for understanding how developmental GRNs evolve .
2K_dev_1533	This paper presents a fast and efficient computational approach to higher order spectral graph matching . Exploiting the redundancy in a tensor representing the affinity between feature points , we approximate the affinity tensor with the linear combination of Kronecker products between bases and index tensors . The bases and index tensors are highly compressed representations of the approximated affinity tensor , requiring much smaller memory than in previous methods , which store the full affinity tensor . We compute the principal eigenvector of the approximated affinity tensor using the small bases and index tensors without explicitly storing the approximated tensor . To compensate for the loss of matching accuracy by the approximation , we also adopt and incorporate a marginalization scheme that maps a higher order tensor to matrix as well as a one-to-one mapping constraint into the eigenvector computation process . The experimental results show that the proposed method is faster and requires smaller memory than the existing methods with little or no loss of accuracy .
2K_dev_1534	Atomistic simulations of the conformational dynamics of proteins can be performed using either Molecular Dynamics or Monte Carlo procedures . The ensembles of three-dimensional structures produced during simulation can be analyzed in a number of ways to elucidate the thermodynamic and kinetic properties of the system . The goal of this chapter is to review both traditional and emerging methods for learning generative models from atomistic simulation data . Here , the term 'generative ' refers to a model of the joint probability distribution over the behaviors of the constituent atoms . In the context of molecular modeling , generative models reveal the correlation structure between the atoms , and may be used to predict how the system will respond to structural perturbations . We begin by discussing traditional methods , which produce multivariate Gaussian models . We then discuss GAMELAN ( GRAPHICAL MODELS OF ENERGY LANDSCAPES ) , which produces generative models of complex , non-Gaussian conformational dynamics ( e.g. , allostery , binding , folding , etc . ) from long timescale simulation data .
2K_dev_1535	We describe the design and control of a wearable robotic device powered by pneumatic artificial muscle actuators for use in ankle-foot rehabilitation . The design is inspired by the biological musculoskeletal system of the human foot and lower leg , mimicking the morphology and the functionality of the biological muscle-tendon-ligament structure . A key feature of the device is its soft structure that provides active assistance without restricting natural degrees of freedom at the ankle joint . Four pneumatic artificial muscles assist dorsiflexion and plantarflexion as well as inversion and eversion . The prototype is also equipped with various embedded sensors for gait pattern analysis . For the subject tested , the prototype is capable of generating an ankle range of motion of 27 ( 14 dorsiflexion and 13 plantarflexion ) . The controllability of the system is experimentally demonstrated using a linear time-invariant ( LTI ) controller . The controller is found using an identified LTI model of the system , resulting from the interaction of the soft orthotic device with a human leg , and model-based classical control design techniques . The suitability of the proposed control strategy is demonstrated with several angle-reference following experiments .
2K_dev_1536	Virus capsid assembly has been widely studied as a biophysical system , both for its biological and medical significance and as an important model for complex self-assembly processes . No current technology can monitor assembly in detail and what information we have on assembly kinetics comes exclusively from in vitro studies . There are many differences between the intracellular environment and that of an in vitro assembly assay , however , that might be expected to alter assembly pathways . Here , we explore one specific feature characteristic of the intracellular environment and known to have large effects on macromolecular assembly processes : molecular crowding . We combine prior particle simulation methods for estimating crowding effects with coarse-grained stochastic models of capsid assembly , using the crowding models to adjust kinetics of capsid simulations to examine possible effects of crowding on assembly pathways . Simulations suggest a striking difference depending on whether or not a system uses nucleation-limited assembly , with crowding tending to promote off-pathway growth in a nonnucleation-limited model but often enhancing assembly efficiency at high crowding levels even while impeding it at lower crowding levels in a nucleation-limited model . These models may help us understand how complicated assembly systems may have evolved to function with high efficiency and fidelity in the densely crowded environment of the cell .
2K_dev_1537	Computational cancer phylogenetics seeks to enumerate the temporal sequences of aberrations in tumor evolution , thereby delineating the evolution of possible tumor progression pathways , molecular subtypes , and mechanisms of action . We previously developed a pipeline for constructing phylogenies describing evolution between major recurring cell types computationally inferred from whole-genome tumor profiles . The accuracy and detail of the phylogenies , however , depend on the identification of accurate , high-resolution molecular markers of progression , i.e. , reproducible regions of aberration that robustly differentiate different subtypes and stages of progression . Here , we present a novel hidden Markov model ( HMM ) scheme for the problem of inferring such phylogenetically significant markers through joint segmentation and calling of multisample tumor data . Our method classifies sets of genome-wide DNA copy number measurements into a partitioning of samples into normal ( diploid ) or amplified at each probe . It differs from other similar HMM methods in its design specifically for the needs of tumor phylogenetics , by seeking to identify robust markers of progression conserved across a set of copy number profiles . We show an analysis of our method in comparison to other methods on both synthetic and real tumor data , which confirms its effectiveness for tumor phylogeny inference and suggests avenues for future advances .
2K_dev_1538	In many behavioral domains , such as facial expression and gesture , sparse structure is prevalent . This sparsity would be well suited for event detection but for one problem . Features typically are confounded by alignment error in space and time . As a consequence , high-dimensional representations such as SIFT and Gabor features have been favored despite their much greater computational cost and potential loss of information . We propose a Kernel Structured Sparsity ( KSS ) method that can handle both the temporal alignment problem and the structured sparse reconstruction within a common framework , and it can rely on simple features . We characterize spatio-temporal events as time-series of motion patterns and by utilizing time-series kernels we apply standard structured-sparse coding techniques to tackle this important problem . We evaluated the KSS method using both gesture and facial expression datasets that include spontaneous behavior and differ in degree of difficulty and type of ground truth coding . KSS outperformed both sparse and non-sparse methods that utilize complex image features and their temporal extensions . In the case of early facial event classification KSS had 10 % higher accuracy as measured by F1 score over kernel SVM methods .
2K_dev_1539	We present a novel device mounted on the fingertip for acquiring and transmitting visual information through haptic channels . In contrast to previous systems in which the user interrogates an intermediate representation of visual information , such as a tactile display representing a camera generated image , our device uses a fingertip-mounted camera and haptic stimulator to allow the user to feel visual features directly from the environment . Visual features ranging from simple intensity or oriented edges to more complex information identified automatically about objects in the environment may be translated in this manner into haptic stimulation of the finger . Experiments using an initial prototype to trace a continuous straight edge have quantified the user 's ability to discriminate the angle of the edge , a potentially useful feature for higher levels analysis of the visual scene .
2K_dev_1540	How can we correlate the neural activity in the human brain as it responds to typed words , with properties of these terms ( like 'edible ' , 'fits in hand ' ) ? In short , we want to find latent variables , that jointly explain both the brain activity , as well as the behavioral responses . This is one of many settings of the Coupled Matrix-Tensor Factorization ( CMTF ) problem . Can we accelerate any CMTF solver , so that it runs within a few minutes instead of tens of hours to a day , while maintaining good accuracy ? We introduce TURBO-SMT , a meta-method capable of doing exactly that : it boosts the performance of any CMTF algorithm , by up to 200_ , along with an up to 65 fold increase in sparsity , with comparable accuracy to the baseline . We apply TURBO-SMT to BRAINQ , a dataset consisting of a ( nouns , brain voxels , human subjects ) tensor and a ( nouns , properties ) matrix , with coupling along the nouns dimension . TURBO-SMT is able to find meaningful latent variables , as well as to predict brain activity with competitive accuracy .
2K_dev_1541	Whole-genome assemblies of 19 placental mammals and two outgroup species were used to reconstruct the order and orientation of syntenic fragments in chromosomes of the eutherian ancestor and six other descendant ancestors leading to human . For ancestral chromosome reconstructions , we developed an algorithm ( DESCHRAMBLER ) that probabilistically determines the adjacencies of syntenic fragments using chromosome-scale and fragmented genome assemblies . The reconstructed chromosomes of the eutherian , boreoeutherian , and euarchontoglires ancestor each included > 80 % of the entire length of the human genome , whereas reconstructed chromosomes of the most recent common ancestor of simians , catarrhini , great apes , and humans and chimpanzees included > 90 % of human genome sequence . These high-coverage reconstructions permitted reliable identification of chromosomal rearrangements over _105 My of eutherian evolution . Orangutan was found to have eight chromosomes that were completely conserved in homologous sequence order and orientation with the eutherian ancestor , the largest number for any species . Ruminant artiodactyls had the highest frequency of intrachromosomal rearrangements , and interchromosomal rearrangements dominated in murid rodents . A total of 162 chromosomal breakpoints in evolution of the eutherian ancestral genome to the human genome were identified ; however , the rate of rearrangements was significantly lower ( 0.80/My ) during the first _60 My of eutherian evolution , then increased to greater than 2.0/My along the five primate lineages studied . Our results significantly expand knowledge of eutherian genome evolution and will facilitate greater understanding of the role of chromosome rearrangements in adaptation , speciation , and the etiology of inherited and spontaneously occurring diseases .
2K_dev_1542	Chemical transformations of silver nanoparticles ( Ag NPs ) and zinc oxide nanoparticles ( ZnO NPs ) during wastewater treatment and sludge treatment must be characterized to accurately assess the risks that these nanomaterials pose from land application of biosolids . Here , X-ray absorption spectroscopy ( XAS ) and supporting characterization methods are used to determine the chemical speciation of Ag and Zn in sludge from a pilot wastewater treatment plant ( WWTP ) that had received PVP coated 50 nm Ag NPs and 30 nm ZnO NPs , dissolved metal ions , or no added metal . The effects of composting and lime and heat treatment on metal speciation in the resulting biosolids were also examined . All added Ag was converted to Ag2S , regardless of the form of Ag added ( NP vs ionic ) . Zn was transformed to three Zn-containing species , ZnS , Zn3 ( PO4 ) 2 , and Zn associated Fe oxy/hydroxides , also regardless of the form of Zn added . Zn speciation was the same in the unamended control sludge . Ag2S persisted in all sludge treatments . Zn3 ( PO4 ) 2 persisted in sludge and biosolids , but the ratio of ZnS and Zn associated with Fe oxy/hydroxide depended on the redox state and water content of the biosolids . Limited differences in Zn and Ag speciation among NP-dosed , ion-dosed , and control biosolids indicate that these nanoparticles are transformed to similar chemical forms as bulk metals already entering the WWTP .
2K_dev_1543	Studies of gene regulation often utilize genome-wide predictions of transcription factor ( TF ) binding sites . Most existing prediction methods are based on sequence information alone , ignoring biological contexts such as developmental stages and tissue types . Experimental methods to study in vivo binding , including ChIP-chip and ChIP-seq , can only study one transcription factor in a single cell type and under a specific condition in each experiment , and therefore can not scale to determine the full set of regulatory interactions in mammalian transcriptional regulatory networks .
2K_dev_1544	The inner environment of the cell is highly dynamic and heterogeneous yet exquisitely organized . Successful completion of cellular processes within this environment depends on the right molecules or molecular complexes to function at the right place at the right time . Understanding spatiotemporal behaviors of cellular processes is therefore essential to understanding their molecular mechanisms at the systems level . These behaviors are usually visualized and recorded using imaging techniques . However , to infer from them systems-level molecular mechanisms , computational analysis and understanding of recorded image data is crucial , not only for acquiring quantitative behavior measurements but also for comprehending complex interactions among the molecules or molecular complexes involved . The technology of computational analysis and understanding of biological images is often referred to simply as bioimage informatics . This article introduces fundamentals of bioimage informatics for understanding spatiotemporal dynamics of cellular processes and reviews recent advances on this topic . Basic bioimage informatics concepts and techniques for characterizing spatiotemporal cell dynamics are introduced first . Studies on specific cellular processes such as cell migration and signal transduction are then used as examples to analyze and summarize recent advances , with the focus on transforming quantitative measurements of spatiotemporal cellular behaviors into knowledge of underlying molecular mechanisms . Despite the advances made , substantial technological challenges remain , especially in representation of spatiotemporal cellular behaviors and inference of systems-level molecular mechanisms . These challenges are briefly discussed . Overall , understanding spatiotemporal cell dynamics will provide critical insights into how specific cellular processes as well as the entire inner cellular environment are dynamically organized and regulated .
2K_dev_1545	With the introduction of next-generation sequencing ( NGS ) technologies , we are facing an exponential increase in the amount of genomic sequence data . The success of all medical and genetic applications of next-generation sequencing critically depends on the existence of computational techniques that can process and analyze the enormous amount of sequence data quickly and accurately . Unfortunately , the current read mapping algorithms have difficulties in coping with the massive amounts of data generated by NGS.We propose a new algorithm , FastHASH , which drastically improves the performance of the seed-and-extend type hash table based read mapping algorithms , while maintaining the high sensitivity and comprehensiveness of such methods . FastHASH is a generic algorithm compatible with all seed-and-extend class read mapping algorithms . It introduces two main techniques , namely Adjacency Filtering , and Cheap K-mer Selection.We implemented FastHASH and merged it into the codebase of the popular read mapping program , mrFAST . Depending on the edit distance cutoffs , we observed up to 19-fold speedup while still maintaining 100 % sensitivity and high comprehensiveness .
2K_dev_1546	In this paper , we propose to use a weakly supervised machine learning framework for automatic detection of Parkinson 's Disease motor symptoms in daily living environments . Our primary goal is to develop a monitoring system capable of being used outside of controlled laboratory settings . Such a system would enable us to track medication cycles at home and provide valuable clinical feedback . Most of the relevant prior works involve supervised learning frameworks ( e.g. , Support Vector Machines ) . However , in-home monitoring provides only coarse ground truth information about symptom occurrences , making it very hard to adapt and train supervised learning classifiers for symptom detection . We address this challenge by formulating symptom detection under incomplete ground truth information as a multiple instance learning ( MIL ) problem . MIL is a weakly supervised learning framework that does not require exact instances of symptom occurrences for training ; rather , it learns from approximate time intervals within which a symptom might or might not have occurred on a given day . Once trained , the MIL detector was able to spot symptom-prone time windows on other days and approximately localize the symptom instances . We monitored two Parkinson 's disease ( PD ) patients , each for four days with a set of five triaxial accelerometers and utilized a MIL algorithm based on axis parallel rectangle ( APR ) fitting in the feature space . We were able to detect subject specific symptoms ( e.g . dyskinesia ) that conformed with a daily log maintained by the patients .
2K_dev_1547	In this paper , we present a magnetically actuated multimodal drug release mechanism using a tetherless soft capsule endoscope for the treatment of gastric disease . Because the designed capsule has a drug chamber between both magnetic heads , if it is compressed by the external magnetic field , the capsule could release a drug in a specific position locally . The capsule is designed to release a drug in two modes according to the situation . In the first mode , a small amount of drug is continuously released by a series of pulse type magnetic field ( 0.01-0.03 T ) . The experimental results show that the drug release can be controlled by the frequency of the external magnetic pulse . In the second mode , about 800 mm ( 3 ) of drug is released by the external magnetic field of 0.07 T , which induces a stronger magnetic attraction than the critical force for capsule 's collapsing . As a result , a polymeric coating is formed around the capsule . The coated area is dependent on the drug viscosity . This paper presents simulations and various experiments to evaluate the magnetically actuated multimodal drug release capability . The proposed soft capsules could be used as minimally invasive tetherless medical devices with therapeutic capability for the next generation capsule endoscopy .
2K_dev_1548	Recognizing facial action units ( AUs ) is important for situation analysis and automated video annotation . Previous work has emphasized face tracking and registration and the choice of features classifiers . Relatively neglected is the effect of imbalanced data for action unit detection . While the machine learning community has become aware of the problem of skewed data for training classifiers , little attention has been paid to how skew may bias performance metrics . To address this question , we conducted experiments using both simulated classifiers and three major databases that differ in size , type of FACS coding , and degree of skew . We evaluated influence of skew on both threshold metrics ( Accuracy , F-score , Cohen 's kappa , and Krippendorf 's alpha ) and rank metrics ( area under the receiver operating characteristic ( ROC ) curve and precision-recall curve ) . With exception of area under the ROC curve , all were attenuated by skewed distributions , in many cases , dramatically so . While ROC was unaffected by skew , precision-recall curves suggest that ROC may mask poor performance . Our findings suggest that skew is a critical factor in evaluating performance metrics . To avoid or minimize skew-biased estimates of performance , we recommend reporting skew-normalized scores along with the obtained ones .
2K_dev_1549	We propose a parameter server system for distributed ML , which follows a Stale Synchronous Parallel ( SSP ) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms , while still providing correctness guarantees . The parameter server provides an easy-to-use shared interface for read/write access to an ML model 's values ( parameters and variables ) , and the SSP model allows distributed workers to read older , stale versions of these values from a local cache , instead of waiting to get them from a central storage . This significantly increases the proportion of time workers spend computing , as opposed to waiting . Furthermore , the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values . We provide a proof of correctness under SSP , as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems , compared to fully-synchronous and asynchronous schemes .
2K_dev_1550	We propose a scalable approach for making inference about latent spaces of large networks . With a succinct representation of networks as a bag of triangular motifs , a parsimonious statistical model , and an efficient stochastic variational inference algorithm , we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours , a setting that is out of reach for many existing methods . When compared to the state-of-the-art probabilistic approaches , our method is several orders of magnitude faster , with competitive or improved accuracy for latent space recovery and link prediction .
2K_dev_1551	This paper introduces a new approach to prediction by bringing together two different nonparametric ideas : distribution free inference and nonparametric smoothing . Specifically , we consider the problem of constructing nonparametric tolerance/prediction sets . We start from the general conformal prediction approach and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution . The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cut-off values . Under standard smoothness conditions , we get an asymptotic efficiency result that is near optimal for a wide range of function classes . But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size . The performance of our method is investigated through simulation studies and illustrated in a real data example .
2K_dev_1552	Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D . Manual needle insertion allows the clinician to control the insertion speed , ensuring patient safety . This paper presents a control law for automatic 3D steering of manually inserted flexible needles , enabling path-following control . A look-ahead proportional controller for position and orientation is presented . The look-ahead distance is a linear function of insertion speed . Simulations in a 3D brain-like environment demonstrate the performance of the proposed controller . Experimental results also show the feasibility of this technique in 2D and 3D environments .
2K_dev_1554	Investigated the relationship between change over time in severity of depression symptoms and facial expression . Depressed participants were followed over the course of treatment and video recorded during a series of clinical interviews . Facial expressions were analyzed from the video using both manual and automatic systems . Automatic and manual coding were highly consistent for FACS action units , and showed similar effects for change over time in depression severity . For both systems , when symptom severity was high , participants made more facial expressions associated with contempt , smiled less , and those smiles that occurred were more likely to be accompanied by facial actions associated with contempt . These results are consistent with the `` social risk hypothesis '' of depression . According to this hypothesis , when symptoms are severe , depressed participants withdraw from other people in order to protect themselves from anticipated rejection , scorn , and social exclusion . As their symptoms fade , participants send more signals indicating a willingness to affiliate . The finding that automatic facial expression analysis was both consistent with manual coding and produced the same pattern of depression effects suggests that automatic facial expression analysis may be ready for use in behavioral and clinical science .
2K_dev_1555	Biological processes are often dynamic , thus researchers must monitor their activity at multiple time points . The most abundant source of information regarding such dynamic activity is time-series gene expression data . These data are used to identify the complete set of activated genes in a biological process , to infer their rates of change , their order and their causal effects and to model dynamic systems in the cell . In this Review we discuss the basic patterns that have been observed in time-series experiments , how these patterns are combined to form expression programs , and the computational analysis , visualization and integration of these data to infer models of dynamic biological systems .
2K_dev_1556	G protein coupled receptors ( GPCRs ) are seven helical transmembrane proteins that function as signal transducers . They bind ligands in their extracellular and transmembrane regions and activate cognate G proteins at their intracellular surface at the other side of the membrane . The relay of allosteric communication between the ligand binding site and the distant G protein binding site is poorly understood . In this study , GREMLIN 1 , a recently developed method that identifies networks of co-evolving residues from multiple sequence alignments , was used to identify those that may be involved in communicating the activation signal across the membrane . The GREMLIN-predicted long-range interactions between amino acids were analyzed with respect to the seven GPCR structures that have been crystallized at the time this study was undertaken .
2K_dev_1557	We describe an R package named huge which provides easy-to-use functions for estimating high dimensional undirected graphs from data . This package implements recent results in the literature , including Friedman et al . ( 2007 ) , Liu et al . ( 2009 , 2012 ) and Liu et al . ( 2010 ) . Compared with the existing graph estimation package glasso , the huge package provides extra features : ( 1 ) instead of using Fortan , it is written in C , which makes the code more portable and easier to modify ; ( 2 ) besides fitting Gaussian graphical models , it also provides functions for fitting high dimensional semiparametric Gaussian copula models ; ( 3 ) more functions like data-dependent model selection , data generation and graph visualization ; ( 4 ) a minor convergence problem of the graphical lasso algorithm is corrected ; ( 5 ) the package allows the user to apply both lossless and lossy screening rules to scale up large-scale problems , making a tradeoff between computational and statistical efficiency .
2K_dev_1558	We consider the problem of sparse variable selection in nonparametric additive models , with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly . Previous works either study the group sparsity in the parametric setting ( e.g. , group lasso ) , or address the problem in the nonparametric setting without exploiting the structural information ( e.g. , sparse additive models ) . In this paper , we present a new method , called group sparse additive models ( GroupSpAM ) , which can handle group sparsity in additive models . We generalize the _1/_2 norm to Hilbert spaces as the sparsity-inducing penalty in GroupSpAM . Moreover , we derive a novel thresholding condition for identifying the functional sparsity at the group level , and propose an efficient block coordinate descent algorithm for constructing the estimate . We demonstrate by simulation that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models , and also conduct a comparative experiment on a real breast cancer dataset .
2K_dev_1559	This paper describes an inexpensive pico-projector-based augmented reality ( AR ) display for a surgical microscope . The system is designed for use with Micron , an active handheld surgical tool that cancels hand tremor of surgeons to improve microsurgical accuracy . Using the AR display , virtual cues can be injected into the microscope view to track the movement of the tip of Micron , show the desired position , and indicate the position error . Cues can be used to maintain high performance by helping the surgeon to avoid drifting out of the workspace of the instrument . Also , boundary information such as the view range of the cameras that record surgical procedures can be displayed to tell surgeons the operation area . Furthermore , numerical , textual , or graphical information can be displayed , showing such things as tool tip depth in the work space and on/off status of the canceling function of Micron .
2K_dev_1560	We study the problem of estimating a temporally varying coefficient and varying structure ( VCVS ) graphical model underlying data collected over a period of time , such as social states of interacting individuals or microarray expression profiles of gene networks , as opposed to i.i.d . data from an invariant model widely considered in current literature of structural estimation . In particular , we consider the scenario in which the model evolves in a piece-wise constant fashion . We propose a procedure that estimates the structure of a graphical model by minimizing the temporally smoothed L1 penalized regression , which allows jointly estimating the partition boundaries of the VCVS model and the coefficient of the sparse precision matrix on each block of the partition . A highly scalable proximal gradient method is proposed to solve the resultant convex optimization problem ; and the conditions for sparsistent estimation and the convergence rate of both the partition boundaries and the network structure are established for the first time for such estimators .
2K_dev_1561	Semantic grounding is the process of relating meaning to symbols ( e.g. , words ) . It is the foundation for creating a representational symbolic system such as language . Semantic grounding for verb meaning is hypothesized to be achieved through two mechanisms : sensorimotor mapping , i.e. , directly encoding the sensorimotor experiences the verb describes , and verb-category mapping , i.e. , encoding the abstract category a verb belongs to . These two mechanisms were investigated by examining neuronal-level spike ( i.e . neuronal action potential ) activities from the motor , somatosensory and parietal areas in two human participants . Motor and a portion of somatosensory neurons were found to be involved in primarily sensorimotor mapping , while parietal and some somatosensory neurons were found to be involved in both sensorimotor and verb-category mapping . The time course of the spike activities and the selective tuning pattern of these neurons indicate that they belong to a large neural network used for semantic processing . This study is the first step towards understanding how words are processed by neurons .
2K_dev_1562	Present treatments for ventricular tachycardia have significant drawbacks . To ameliorate these drawbacks , it may be advantageous to employ an epicardial robotic walker that performs mapping and ablation with precise control of needle insertion depth . This paper examines the feasibility of such a system .
2K_dev_1563	Cryo-electron tomography ( cryo-ET ) captures the 3Delectron density distribution of macromolecular complexes in close to native state . With the rapid advance of cryo-ET acquisition technologies , it is possible to generate large numbers ( > 100000 ) of subtomograms , each containing a macromolecular complex . Often , these subtomograms represent a heterogeneous sample due to variations in the structure and composition of a complex insitu form or because particles are a mixture of different complexes . In this case subtomograms must be classified . However , classification of large numbers of subtomograms is a time-intensive task and often a limiting bottleneck . This paper introduces an open source software platform , TomoMiner , for large-scale subtomogram classification , template matching , subtomogram averaging , and alignment . Its scalable and robust parallel processing allows efficient classification of tens to hundreds of thousands of subtomograms . In addition , TomoMiner provides a pre-configured TomoMinerCloud computing service permitting users without sufficient computing resources instant access to TomoMiners high-performance features .
2K_dev_1564	The gene regulation mechanisms necessary for the development of complex multicellular animals have been found in sponges .
2K_dev_1565	We present a computer-aided programming approach to concurrency . The approach allows programmers to program assuming a friendly , non-preemptive scheduler , and our synthesis procedure inserts synchronization to ensure that the final program works even with a preemptive scheduler . The correctness specification is implicit , inferred from the non-preemptive behavior . Let us consider sequences of calls that the program makes to an external interface . The specification requires that any such sequence produced under a preemptive scheduler should be included in the set of sequences produced under a non-preemptive scheduler . We guarantee that our synthesis does not introduce deadlocks and that the synchronization inserted is optimal w.r.t . a given objective function . The solution is based on a finitary abstraction , an algorithm for bounded language inclusion moduloan independence relation , and generation of a set of global constraints over synchronization placements . Each model of the global constraints set corresponds to a correctness-ensuring synchronization placement . The placement that is optimal w.r.t . the given objective function is chosen as the synchronization solution . We apply the approach to device-driver programming , where the driver threads call the software interface of the device and the API provided by the operating system . Our experiments demonstrate that our synthesis method is precise and efficient . The implicit specification helped us find one concurrency bug previously missed when model-checking using an explicit , user-provided specification . We implemented objective functions for coarse-grained and fine-grained locking and observed that different synchronization placements are produced for our experiments , favoring a minimal number of synchronization operations or maximum concurrency , respectively .
2K_dev_1566	Macroautophagy is regarded as a nonspecific bulk degradation process of cytoplasmic material within the lysosome . However , the process has mainly been studied by nonspecific bulk degradation assays using radiolabeling . In the present study we monitor protein turnover and degradation by global , unbiased approaches relying on quantitative mass spectrometry-based proteomics . Macroautophagy is induced by rapamycin treatment , and by amino acid and glucose starvation in differentially , metabolically labeled cells . Protein dynamics are linked to image-based models of autophagosome turnover . Depending on the inducing stimulus , protein as well as organelle turnover differ . Amino acid starvation-induced macroautophagy leads to selective degradation of proteins important for protein translation . Thus , protein dynamics reflect cellular conditions in the respective treatment indicating stimulus-specific pathways in stress-induced macroautophagy .
2K_dev_1567	The etiology of autism spectrum disorder ( ASD ) is complex , involving both genetic and environmental contributions to individual and population-level liability . Early researchers hypothesized that ASD arises from polygenic inheritance , but later results , such as the identification of mutations in certain genes that are responsible for syndromes associated with ASD , led others to propose that de novo mutations of major effect would account for most cases . This yin and yang of monogenic causes and polygenic inheritance continues to this day . The development of genome-wide genotyping and sequencing techniques has resulted in remarkable advances in our understanding of the genetic architecture of risk for ASD . The combined research findings provide solid evidence that ASD is a complex polygenic disorder . Rare de novo and inherited variations act within the context of a common-variant genetic load , and this load accounts for the largest portion of ASD liability .
2K_dev_1568	The etiology of autism spectrum disorder ( ASD ) is complex , involving both genetic and environmental contributions to individual and population-level liability . Early researchers hypothesized that ASD arises from polygenic inheritance , but later results , such as the identification of mutations in certain genes that are responsible for syndromes associated with ASD , led others to propose that de novo mutations of major effect would account for most cases . This yin and yang of monogenic causes and polygenic inheritance continues to this day . The development of genome-wide genotyping and sequencing techniques has resulted in remarkable advances in our understanding of the genetic architecture of risk for ASD . The combined research findings provide solid evidence that ASD is a complex polygenic disorder . Rare de novo and inherited variations act within the context of a common-variant genetic load , and this load accounts for the largest portion of ASD liability .
2K_dev_1569	In this paper we describe work towards retinal vessel cannulation using an actively stabilized handheld robot , guided by monocular vision . We employ a previously developed monocular camera based surface reconstruction method using automated laser beam scanning over the retina . We use the reconstructed plane to find a coordinate transform between the 2D image plane coordinate system and the global 3D frame . Within a hemispherical region around the target , we use motion scaling for higher precision . The contribution of this work is the homography matrix estimation using monocular vision and application of the previously developed laser surface reconstruction to Micron guided vein cannulation . Experiments are conducted in a wet eye phantom to show the higher accuracy of the surface reconstruction as compared to standard stereo reconstruction . Further , experiments to show the increased surgical accuracy due to motion scaling are also carried out .
2K_dev_1570	Neuromechanical simulations have been used to study the spinal control of human locomotion which involves complex mechanical dynamics . So far , most neuromechanical simulation studies have focused on demonstrating the capability of a proposed control model in generating normal walking . As many of these models with competing control hypotheses can generate human-like normal walking behaviors , a more in-depth evaluation is required . Here , we conduct the more in-depth evaluation on a spinal-reflex-based control model using five representative gait disturbances , ranging from electrical stimulation to mechanical perturbation at individual leg joints and at the whole body . The immediate changes in muscle activations of the model are compared to those of humans across different gait phases and disturbance magnitudes . Remarkably similar response trends for the majority of investigated muscles and experimental conditions reinforce the plausibility of the reflex circuits of the model . However , the model 's responses lack in amplitude for two experiments with whole body disturbances suggesting that in these cases the proposed reflex circuits need to be amplified by additional control structures such as location-specific cutaneous reflexes . A model that captures these selective amplifications would be able to explain both steady and reactive spinal control of human locomotion . Neuromechanical simulations that investigate hypothesized control models are complementary to gait experiments in better understanding the control of human locomotion .
2K_dev_1571	To localize genetic variation affecting risk for psychotic disorders in the population of Palau , we genotyped DNA samples from 203 Palauan individuals diagnosed with psychotic disorders , broadly defined , and 125 control subjects using a genome-wide single nucleotide polymorphism array . Palau has unique features advantageous for this study : due to its population history , Palauans are substantially interrelated ; affected individuals often , but not always , cluster in families ; and we have essentially complete ascertainment of affected individuals . To localize risk variants to genomic regions , we evaluated long-shared haplotypes , 10 Mb , identifying clusters of affected individuals who share such haplotypes . This extensive sharing , typically identical by descent , was significantly greater in cases than population controls , even after controlling for relatedness . Several regions of the genome exhibited substantial excess of shared haplotypes for affected individuals , including 3p21 , 3p12 , 4q28 , and 5q23-q31 . Two of these regions , 4q28 and 5q23-q31 , showed significant linkage by traditional LOD score analysis and could harbor variants of more sizeable risk for psychosis or a multiplicity of risk variants . The pattern of haplotype sharing in 4q28 highlights PCDH10 , encoding a cadherin-related neuronal receptor , as possibly involved in risk .
2K_dev_1572	We investigate the problem of learning an evolution equation directly from some given data . This work develops a learning algorithm to identify the terms in the underlying partial differential equations and to approximate the coefficients of the terms only using data . The algorithm uses sparse optimization in order to perform feature selection and parameter estimation . The features are data driven in the sense that they are constructed using nonlinear algebraic equations on the spatial derivatives of the data . Several numerical experiments show the proposed method 's robustness to data noise and size , its ability to capture the 1 features of the data , and its capability of performing additional analytics . Examples include shock equations , pattern formation , fluid flow and turbulence , and oscillatory convection .
2K_dev_1573	Three-dimensional live cell imaging of the interaction of T cells with antigen-presenting cells ( APCs ) visualizes the subcellular distributions of signaling intermediates during T cell activation at thousands of resolved positions within a cell . These information-rich maps of local protein concentrations are a valuable resource in understanding T cell signaling . Here , we describe a protocol for the efficient acquisition of such imaging data and their computational processing to create four-dimensional maps of local concentrations . This protocol allows quantitative analysis of T cell signaling as it occurs inside live cells with resolution in time and space across thousands of cells .
2K_dev_1574	Quantitative image analysis procedures are necessary for the automated discovery of effects of drug treatment in large collections of fluorescent micrographs . When compared to their mammalian counterparts , the effects of drug conditions on protein localization in plant species are poorly understood and underexplored . To investigate this relationship , we generated a large collection of images of single plant cells after various drug treatments . For this , protoplasts were isolated from six transgenic lines of A. thaliana expressing fluorescently tagged proteins . Eight drugs at three concentrations were applied to protoplast cultures followed by automated image acquisition . For image analysis , we developed a cell segmentation protocol for detecting drug effects using a Hough transform-based region of interest detector and a novel cross-channel texture feature descriptor . In order to determine treatment effects , we summarized differences between treated and untreated experiments with an L1 Cramr-von Mises statistic . The distribution of these statistics across all pairs of treated and untreated replicates was compared to the variation within control replicates to determine the statistical significance of observed effects . Using this pipeline , we report the dose dependent drug effects in the first high-content Arabidopsis thaliana drug screen of its kind . These results can function as a baseline for comparison to other protein organization modeling approaches in plant cells . 2017 International Society for Advancement of Cytometry .
2K_dev_1575	Event discovery aims to discover a temporal segment of interest , such as human behavior , actions or activities . Most approaches to event discovery within or between time series use supervised learning . This becomes problematic when some relevant event labels are unknown , are difficult to detect , or not all possible combinations of events have been anticipated . To overcome these problems , this paper explores Common Event Discovery ( CED ) , a new problem that aims to discover common events of variable-length segments in an unsupervised manner . A potential solution to CED is searching over all possible pairs of segments , which would incur a prohibitive quartic cost . In this paper , we propose an efficient branch-and-bound ( B & B ) framework that avoids exhaustive search while guaranteeing a globally optimal solution . To this end , we derive novel bounding functions for various commonality measures and provide extensions to multiple commonality discovery and accelerated search . The B & B framework takes as input any multidimensional signal that can be quantified into histograms . A generalization of the framework can be readily applied to discover events at the same or different times ( synchrony and event commonality , respectively ) . We consider extensions to video search and supervised event detection . The effectiveness of the B & B framework is evaluated in motion capture of deliberate behavior and in video of spontaneous facial behavior in diverse interpersonal contexts : interviews , small groups of young adults , and parent-infant face-to-face interaction .
2K_dev_1577	Rapid advances in high-throughput sequencing and a growing realization of the importance of evolutionary theory to cancer genomics have led to a proliferation of phylogenetic studies of tumour progression . These studies have yielded not only new insights but also a plethora of experimental approaches , sometimes reaching conflicting or poorly supported conclusions . Here , we consider this body of work in light of the key computational principles underpinning phylogenetic inference , with the goal of providing practical guidance on the design and analysis of scientifically rigorous tumour phylogeny studies . We survey the range of methods and tools available to the researcher , their key applications , and the various unsolved problems , closing with a perspective on the prospects and broader implications of this field .
2K_dev_1578	Reconstructing ancestral gene orders in the presence of duplications is important for a better understanding of genome evolution . Current methods for ancestral reconstruction are limited by either computational constraints or the availability of reliable gene trees , and often ignore duplications altogether . Recently , methods that consider duplications in ancestral reconstructions have been developed , but the quality of reconstruction , counted as the number of contiguous ancestral regions found , decreases rapidly with the number of duplicated genes , complicating the application of such approaches to mammalian genomes . However , such high fragmentation is not encountered when reconstructing mammalian genomes at the synteny-block level , although the relative positions of genes in such reconstruction can not be recovered .
2K_dev_1580	The clinical course of idiopathic pulmonary fibrosis ( IPF ) is unpredictable . Clinical prediction tools are not accurate enough to predict disease outcomes .
2K_dev_1581	State estimation is the most critical capability for MAV ( Micro-Aerial Vehicle ) localization , autonomous obstacle avoidance , robust flight control and 3D environmental mapping . There are three main challenges for MAV state estimation : ( 1 ) it can deal with aggressive 6 DOF ( Degree Of Freedom ) motion ; ( 2 ) it should be robust to intermittent GPS ( Global Positioning System ) ( even GPS-denied ) situations ; ( 3 ) it should work well both for low- and high-altitude flight . In this paper , we present a state estimation technique by fusing long-range stereo visual odometry , GPS , barometric and IMU ( Inertial Measurement Unit ) measurements . The new estimation system has two main parts , a stochastic cloning EKF ( Extended Kalman Filter ) estimator that loosely fuses both absolute state measurements ( GPS , barometer ) and the relative state measurements ( IMU , visual odometry ) , and is derived and discussed in detail . A long-range stereo visual odometry is proposed for high-altitude MAV odometry calculation by using both multi-view stereo triangulation and a multi-view stereo inverse depth filter . The odometry takes the EKF information ( IMU integral ) for robust camera pose tracking and image feature matching , and the stereo odometry output serves as the relative measurements for the update of the state estimation . Experimental results on a benchmark dataset and our real flight dataset show the effectiveness of the proposed state estimation system , especially for the aggressive , intermittent GPS and high-altitude MAV flight .
2K_dev_1582	Autonomous robots often rely on models of their sensing and actions for intelligent decision making . However , when operating in unconstrained environments , the complexity of the world makes it infeasible to create models that are accurate in every situation . This article addresses the problem of using potentially large and high-dimensional sets of robot execution data to detect situations in which a robot model is inaccurate-that is , detecting context-dependent model inaccuracies in a high-dimensional context space . To find inaccuracies tractably , the robot conducts an informed search through low-dimensional projections of execution data to find parametric Regions of Inaccurate Modeling ( RIMs ) . Empirical evidence from two robot domains shows that this approach significantly enhances the detection power of existing RIM-detection algorithms in high-dimensional spaces .
2K_dev_1583	Severe asthma ( SA ) is a heterogeneous disease with multiple molecular mechanisms . Gene expression studies of bronchial epithelial cells in individuals with asthma have provided biological insight and underscored possible mechanistic differences between individuals .
2K_dev_1584	Quantifying differences or similarities in connectomes has been a challenge due to the immense complexity of global brain networks . Here we introduce a noninvasive method that uses diffusion MRI to characterize whole-brain white matter architecture as a single local connectome fingerprint that allows for a direct comparison between structural connectomes . In four independently acquired data sets with repeated scans ( total N 0 213 ) , we show that the local connectome fingerprint is highly specific to an individual , allowing for an accurate self-versus-others classification that achieved 100 % accuracy across 17398 identification tests . The estimated classification error was approximately one thousand times smaller than fingerprints derived from diffusivity-based measures or region-to-region connectivity patterns for repeat scans acquired within 3 months . The local connectome fingerprint also revealed neuroplasticity within an individual reflected as a decreasing trend in self-similarity across time , whereas this change was not observed in the diffusivity measures . Moreover , the local connectome fingerprint can be used as a phenotypic marker , revealing 12.51 % similarity between monozygotic twins , 5.14 % between dizygotic twins , and 4.51 % between none-twin siblings , relative to differences between unrelated subjects . This novel approach opens a new door for probing the influence of pathological , genetic , social , or environmental factors on the unique configuration of the human connectome .
2K_dev_1585	Clinical decision support tools ( DSTs ) are computational systems that aid healthcare decision-making . While effective in labs , almost all these systems failed when they moved into clinical practice . Healthcare researchers speculated it is most likely due to a lack of user-centered HCI considerations in the design of these systems . This paper describes a field study investigating how clinicians make a heart pump implant decision with a focus on how to best integrate an intelligent DST into their work process . Our findings reveal a lack of perceived need for and trust of machine intelligence , as well as many barriers to computer use at the point of clinical decision-making . These findings suggest an alternative perspective to the traditional use models , in which clinicians engage with DSTs at the point of making a decision . We identify situations across patients ' healthcare trajectories when decision supports would help , and we discuss new forms it might take in these situations .
2K_dev_1586	Death by suicide demonstrates profound personal suffering and societal failure . While basic sciences provide the opportunity to understand biological markers related to suicide , computer science provides opportunities to understand suicide thought markers . In this novel prospective , multimodal , multicenter , mixed demographic study , we used machine learning to measure and fuse two classes of suicidal thought markers : verbal and nonverbal . Machine learning algorithms were used with the subjects ' words and vocal characteristics to classify 379 subjects recruited from two academic medical centers and a rural community hospital into one of three groups : suicidal , mentally ill but not suicidal , or controls . By combining linguistic and acoustic characteristics , subjects could be classified into one of the three groups with up to 85 % accuracy . The results provide insight into how advanced technology can be used for suicide assessment and prevention .
2K_dev_1587	How neural stem cells generate the correct number and type of differentiated neurons in appropriate places remains an important question . Although nervous systems are diverse across phyla , in many taxa the larva forms an anterior concentration of serotonergic neurons , or apical organ . The sea star embryo initially has a pan-neurogenic ectoderm , but the genetic mechanism that directs a subset of these cells to generate serotonergic neurons in a particular location is unresolved . We show that neurogenesis in sea star larvae begins with soxc-expressing multipotent progenitors . These give rise to restricted progenitors that express lhx2/9 soxc- and lhx2/9-expressing cells can undergo both asymmetric divisions , allowing for progression towards a particular neural fate , and symmetric proliferative divisions . We show that nested concentric domains of gene expression along the anterior-posterior ( AP ) axis , which are observed in a great diversity of metazoans , control neurogenesis in the sea star larva by promoting particular division modes and progression towards becoming a neuron . This work explains how spatial patterning in the ectoderm controls progression of neurogenesis in addition to providing spatial cues for neuron location . Modification to the sizes of these AP territories provides a simple mechanism to explain the diversity of neuron number among apical organs .
2K_dev_1588	A fundamental problem in comparative genomics is to compute the distance between two genomes in terms of its higher level organization ( given by genes or syntenic blocks ) . For two genomes without duplicate genes , we can easily define ( and almost always efficiently compute ) a variety of distance measures , but the problem is NP-hard under most models when genomes contain duplicate genes . To tackle duplicate genes , three formulations ( exemplar , maximum matching , and any matching ) have been proposed , all of which aim to build a matching between homologous genes so as to minimize some distance measure . Of the many distance measures , the breakpoint distance ( the number of nonconserved adjacencies ) was the first one to be studied and remains of significant interest because of its simplicity and model-free property . The three breakpoint distance problems corresponding to the three formulations have been widely studied . Although we provided last year a solution for the exemplar problem that runs very fast on full genomes , computing optimal solutions for the other two problems has remained challenging . In this article , we describe very fast , exact algorithms for these two problems . Our algorithms rely on a compact integer-linear program that we further simplify by developing an algorithm to remove variables , based on new results on the structure of adjacencies and matchings . Through extensive experiments using both simulations and biological data sets , we show that our algorithms run very fast ( in seconds ) on mammalian genomes and scale well beyond . We also apply these algorithms ( as well as the classic orthology tool MSOAR ) to create orthology assignment , then compare their quality in terms of both accuracy and coverage . We find that our algorithm for the `` any matching '' formulation significantly outperforms other methods in terms of accuracy while achieving nearly maximum coverage .
2K_dev_1589	Over 100 genetic loci harbor schizophrenia-associated variants , yet how these variants confer liability is uncertain . The CommonMind Consortium sequenced RNA from dorsolateral prefrontal cortex of people with schizophrenia ( N 0 258 ) and control subjects ( N 0 279 ) , creating a resource of gene expression and its genetic regulation . Using this resource , _20 % of schizophrenia loci have variants that could contribute to altered gene expression and liability . In five loci , only a single gene was involved : FURIN , TSNARE1 , CNTN4 , CLCN3 or SNAP91 . Altering expression of FURIN , TSNARE1 or CNTN4 changed neurodevelopment in zebrafish ; knockdown of FURIN in human neural progenitor cells yielded abnormal migration . Of 693 genes showing significant case-versus-control differential expression , their fold changes were 1.33 , and an independent cohort yielded similar results . Gene co-expression implicates a network relevant for schizophrenia . Our findings show that schizophrenia is polygenic and highlight the utility of this resource for mechanistic interpretations of genetic liability for brain diseases .
2K_dev_1590	Genome-wide association studies have discovered a large number of genetic variants associated with complex diseases such as Alzheimer 's disease . However , the genetic background of such diseases is largely unknown due to the complex mechanisms underlying genetic effects on traits , as well as a small sample size ( e.g. , 1000 ) and a large number of genetic variants ( e.g. , 1 million ) . Fortunately , datasets that contain genotypes , transcripts , and phenotypes are becoming more readily available , creating new opportunities for detecting disease-associated genetic variants . In this paper , we present a novel approach called `` Backward Three-way Association Mapping '' ( BTAM ) for detecting three-way associations among genotypes , transcripts , and phenotypes . Assuming that genotypes affect transcript levels , which in turn affect phenotypes , we first find transcripts associated with the phenotypes , and then find genotypes associated with the chosen transcripts . The backward ordering of association mappings allows us to avoid a large number of association testings between all genotypes and all transcripts , making it possible to identify three-way associations with a small computational cost . In our simulation study , we demonstrate that BTAM significantly improves the statistical power over `` forward '' three-way association mapping that finds genotypes associated with both transcripts and phenotypes and genotype-phenotype association mapping . Furthermore , we apply BTAM on an Alzheimer 's disease dataset and report top 10 genotype-transcript-phenotype associations .
2K_dev_1591	Stable chronic functionality of intracortical probes is of utmost importance toward realizing clinical application of brain-machine interfaces . Sustained immune response from the brain tissue to the neural probes is one of the major challenges that hinder stable chronic functionality . There is a growing body of evidence in the literature that highly compliant neural probes with sub-cellular dimensions may significantly reduce the foreign-body response , thereby enhancing long term stability of intracortical recordings . Since the prevailing commercial probes are considerably larger than neurons and of high stiffness , new approaches are needed for developing miniature probes with high compliance . In this paper , we present design , fabrication , and in vitro evaluation of ultra-miniature ( 2.7_m x 10_m cross section ) , ultra-compliant ( 1.4___10 ( -2 ) _N/_m in the axial direction , and 2.6___10 ( -5 ) _N/_m and 1.8___10 ( -6 ) _N/_m in the lateral directions ) neural probes and associated probe-encasing biodissolvable delivery needles toward addressing the aforementioned challenges . The high compliance of the probes is obtained by micron-scale cross-section and meandered shape of the parylene-C insulated platinum wiring . Finite-element analysis is performed to compare the strains within the tissue during micromotion when using the ultra-compliant meandered probes with that when using stiff silicon probes . The standard batch microfabrication techniques are used for creating the probes . A dissolvable delivery needle that encases the probe facilitates failure-free insertion and precise placement of the ultra-compliant probes . Upon completion of implantation , the needle gradually dissolves , leaving behind the ultra-compliant neural probe . A spin-casting based micromolding approach is used for the fabrication of the needle . To demonstrate the versatility of the process , needles from different biodissolvable materials , as well as two-dimensional needle arrays with different geometries and dimensions , are fabricated . Further , needles incorporating anti-inflammatory drugs are created to show the co-delivery potential of the needles . An automated insertion device is developed for repeatable and precise implantation of needle-encased probes into brain tissue . Insertion of the needles without mechanical failure , and their subsequent dissolution are demonstrated . It is concluded that ultra-miniature , ultra-compliant probes and associated biodissolvable delivery needles can be successfully fabricated , and the use of the ultra-compliant meandered probes results in drastic reduction in strains imposed in the tissue as compared to stiff probes , thereby showing promise toward chronic applications .
2K_dev_1592	Automated machine-reading biocuration systems typically use sentence-by-sentence information extraction to construct meaning representations for use by curators . This does not directly reflect the typical discourse structure used by scientists to construct an argument from the experimental data available within a article , and is therefore less likely to correspond to representations typically used in biomedical informatics systems ( let alone to the mental models that scientists have ) . In this study , we develop Natural Language Processing methods to locate , extract , and classify the individual passages of text from articles ' Results sections that refer to experimental data . In our domain of interest ( molecular biology studies of cancer signal transduction pathways ) , individual articles may contain as many as 30 small-scale individual experiments describing a variety of findings , upon which authors base their overall research conclusions . Our system automatically classifies discourse segments in these texts into seven categories ( fact , hypothesis , problem , goal , method , result , implication ) with an F-score of 0.68 . These segments describe the essential building blocks of scientific discourse to ( i ) provide context for each experiment , ( ii ) report experimental details and ( iii ) explain the data 's meaning in context . We evaluate our system on text passages from articles that were curated in molecular biology databases ( the Pathway Logic Datum repository , the Molecular Interaction MINT and INTACT databases ) linking individual experiments in articles to the type of assay used ( coprecipitation , phosphorylation , translocation etc. ) . We use supervised machine learning techniques on text passages containing unambiguous references to experiments to obtain baseline F1 scores of 0.59 for MINT , 0.71 for INTACT and 0.63 for Pathway Logic . Although preliminary , these results support the notion that targeting information extraction methods to experimental results could provide accurate , automated methods for biocuration . We also suggest the need for finer-grained curation of experimental methods used when constructing molecular biology databases .
2K_dev_1593	We use a combination of coarse-grained molecular dynamics simulations and theoretical modeling to examine three-junctions in mixed lipid bilayer membranes . These junctions are localized defect lines in which three bilayers merge in such a way that each bilayer shares one monolayer with one of the other two bilayers . The resulting local morphology is non-lamellar , resembling the threefold symmetric defect lines in inverse hexagonal phases , but it regularly occurs during membrane fission and fusion events . We realize a system of junctions by setting up a honeycomb lattice , which in its primitive cell contains two hexagons and four three-line junctions , permitting us to study their stability as well as their line tension . We specifically consider the effects of lipid composition and intrinsic curvature in binary mixtures , which contain a fraction of negatively curved lipids in a curvature-neutral background phase . Three-junction stability results from a competition between the junction and an open edge , which arises if one of the three bilayers detaches from the other two . We show that the stable phase is the one with the lower defect line tension . The strong and opposite monolayer curvatures present in junctions and edges enhance the mole fraction of negatively curved lipids in junctions and deplete it in edges . This lipid sorting affects the two line tensions and in turn the relative stability of the two phases . It also leads to a subtle entropic barrier for the transition between junction and edge that is absent in uniform membranes .
2K_dev_1594	Cancer genomes exhibit a large number of different alterations that affect many genes in a diverse manner . An improved understanding of the generative mechanisms behind the mutation rules and their influence on gene community behavior is of great importance for the study of cancer .
2K_dev_1595	The paradigm of evidence-based medicine dictates that clinical practice should reflect the shifting landscape of the peer-reviewed literature . Here , we examined the extent to which this premise is fulfilled as it pertains to the surgical resection of high-grade gliomas ( HGGs ) .
2K_dev_1596	Reconstructing the full-length expressed transcripts ( a.k.a . the transcript assembly problem ) from the short sequencing reads produced by RNA-seq protocol plays a central role in identifying novel genes and transcripts as well as in studying gene expressions and gene functions . A crucial step in transcript assembly is to accurately determine the splicing junctions and boundaries of the expressed transcripts from the reads alignment . In contrast to the splicing junctions that can be efficiently detected from spliced reads , the problem of identifying boundaries remains open and challenging , due to the fact that the signal related to boundaries is noisy and weak .
2K_dev_1597	Aneuploidy and structural variations ( SVs ) generate cancer genomes containing a mixture of rearranged genomic segments with extensive somatic copy number alterations . However , existing methods can identify either SVs or allele-specific copy number alterations but not both simultaneously , which provides a limited view of cancer genome structure . Here , we introduce Weaver , an algorithm for the quantification and analysis of allele-specific copy numbers of SVs . Weaver uses a Markov random field to estimate joint probabilities of allele-specific copy numbers of SVs and their inter-connectivity based on paired-end whole-genome sequencing data . Weaver also predicts the timing of SVs relative to chromosome amplifications . We demonstrate the accuracy of Weaver using simulations and findingsfrom whole-genome optical mapping . We apply Weaver to generate allele-specific copy numbers ofSVs for MCF-7 and HeLa cell lines and identify recurrent SV patterns in 44 TCGA ovarian cancer whole-genome sequencing datasets . Our approach provides a more complete assessment of the complex genomic architectures inherent to many cancer genomes .
2K_dev_1598	Wearable activity trackers have become a viable business opportunity . Nevertheless , research has raised concerns over their potentially detrimental effects on wellbeing . For example , a recent study found that while counting steps with a pedometer increased steps taken throughout the day , at the same time it decreased the enjoyment people derived from walking . This poses a serious threat to the incorporation of healthy routines into everyday life . Most studies aim at proving the effectiveness of activity trackers . In contrast , a wellbeing-oriented perspective calls for a deeper understanding of how trackers create and mediate meaningful experiences in everyday life .
2K_dev_1599	Advances in fluorescence in situ hybridization ( FISH ) make it feasible to detect multiple copy-number changes in hundreds of cells of solid tumors . Studies using FISH , sequencing , and other technologies have revealed substantial intra-tumor heterogeneity . The evolution of subclones in tumors may be modeled by phylogenies . Tumors often harbor aneuploid or polyploid cell populations . Using a FISH probe to estimate changes in ploidy can guide the creation of trees that model changes in ploidy and individual gene copy-number variations . We present FISHtrees 3 , which implements a ploidy-based tree building method based on mixed integer linear programming ( MILP ) . The ploidy-based modeling in FISHtrees includes a new formulation of the problem of merging trees for changes of a single gene into trees modeling changes in multiple genes and the ploidy . When multiple samples are collected from each patient , varying over time or tumor regions , it is useful to evaluate similarities in tumor progression among the samples . Therefore , we further implemented in FISHtrees 3 a new method to build consensus graphs for multiple samples . We validate FISHtrees 3 on a simulated data and on FISH data from paired cases of cervical primary and metastatic tumors and on paired breast ductal carcinoma in situ ( DCIS ) and invasive ductal carcinoma ( IDC ) . Tests on simulated data show improved accuracy of the ploidy-based approach relative to prior ploidyless methods . Tests on real data further demonstrate novel insights these methods offer into tumor progression processes . Trees for DCIS samples are significantly less complex than trees for paired IDC samples . Consensus graphs show substantial divergence among most paired samples from both sets . Low consensus between DCIS and IDC trees may help explain the difficulty in finding biomarkers that predict which DCIS cases are at most risk to progress to IDC . The FISHtrees software is available at ftp : //ftp.ncbi.nih.gov/pub/FISHtrees .
2K_dev_1600	The recent proliferation of cryptomarkets and the associated emergence of a sub-field of research on the anonymous web have outpaced the development of an ethical consensus regarding research methods and dissemination amongst scholars working in this unique online space . The peculiar characteristics of cryptomarket research , which often involves encryption , illegal activity , large-scale data collection , and geographical separation from research participants , challenge conventional ethical frameworks . A further complicating factor for reaching ethical consensus is the confluence of scholars drawn from a variety of academic disciplines , each with their own particular norms , practices and perspectives . This paper is intended to stimulate awareness and debate , and to prompt further reflection amongst scholars studying these fascinating online phenomena . The paper explores tensions and addresses some of the more prominent and pressing ethical questions , including public vs. private online spaces , anonymity , data sharing and ownership , risks and threats to research subjects and researchers . Also discussed is how best to balance the potential harms of cryptomarket research against benefits to the public .
2K_dev_1601	Efforts to model how signaling and regulatory networks work in cells have largely either not considered spatial organization or have used compartmental models with minimal spatial resolution . Fluorescence microscopy provides the ability to monitor the spatiotemporal distribution of many molecules during signaling events , but as of yet no methods have been described for large scale image analysis to learn a complex protein regulatory network . Here we present and evaluate methods for identifying how changes in concentration in one cell region influence concentration of other proteins in other regions .
2K_dev_1602	Within the last 20 years , there has been an increasing interest in the computer vision community in automated facial image analysis algorithms . This has been driven by applications in animation , market research , autonomous-driving , surveillance , and facial editing among others . To date , there exist several commercial packages for specific facial image analysis tasks such as facial expression recognition , facial attribute analysis or face tracking . However , free and easy-to-use software that incorporates all these functionalities is unavailable . This paper presents IntraFace ( IF ) , a publicly-available software package for automated facial feature tracking , head pose estimation , facial attribute recognition , and facial expression analysis from video . In addition , IFincludes a newly develop technique for unsupervised synchrony detection to discover correlated facial behavior between two or more persons , a relatively unexplored problem in facial image analysis . In tests , IF achieved state-of-the-art results for emotion expression and action unit detection in three databases , FERA , CK+ and RU-FACS ; measured audience reaction to a talk given by one of the authors ; and discovered synchrony for smiling in videos of parent-infant interaction . IF is free of charge for academic use at http : //www.humansensing.cs.cmu.edu/intraface/ .
2K_dev_1603	The generativity and complexity of human thought stem in large part from the ability to represent relations among concepts and form propositions . The current study reveals how a given object such as rabbit is neurally encoded differently and identifiably depending on whether it is an agent ( `` the rabbit punches the monkey '' ) or a patient ( `` the monkey punches the rabbit '' ) . Machine-learning classifiers were trained on functional magnetic resonance imaging ( fMRI ) data evoked by a set of short videos that conveyed agent-verb-patient propositions . When tested on a held-out video , the classifiers were able to reliably identify the thematic role of an object from its associated fMRI activation pattern . Moreover , when trained on one subset of the study participants , classifiers reliably identified the thematic roles in the data of a left-out participant ( mean accuracy_=_.66 ) , indicating that the neural representations of thematic roles were common across individuals .
2K_dev_1604	Despite the widespread popularity of genome-wide association studies ( GWAS ) for genetic mapping of complex traits , most existing GWAS methodologies are still limited to the use of static phenotypes measured at a single time point . In this work , we propose a new method for association mapping that considers dynamic phenotypes measured at a sequence of time points . Our approach relies on the use of Time-Varying Group Sparse Additive Models ( TV-GroupSpAM ) for high-dimensional , functional regression .
2K_dev_1605	To enable real-time , person-independent 3D registration from 2D video , we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60 degrees . From a single 2D image of a person 's face , a dense 3D shape is registered in real time for each frame . The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression . The algorithm first estimates the location of a dense set of markers and their visibility , then reconstructs face shapes by fitting a part-based 3D model . Because no assumptions are required about illumination or surface properties , the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video . The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction . Experimental findings strongly support the validity of real-time , 3D registration and reconstruction from 2D video . The software is available online at http : //zface.org .
2K_dev_1606	The environment of a living cell is vastly different from that of an in vitro reaction system , an issue that presents great challenges to the use of in vitro models , or computer simulations based on them , for understanding biochemistry in vivo . Virus capsids make an excellent model system for such questions because they typically have few distinct components , making them amenable to in vitro and modeling studies , yet their assembly can involve complex networks of possible reactions that can not be resolved in detail by any current experimental technology . We previously fit kinetic simulation parameters to bulk in vitro assembly data to yield a close match between simulated and real data , and then used the simulations to study features of assembly that can not be monitored experimentally . The present work seeks to project how assembly in these simulations fit to in vitro data would be altered by computationally adding features of the cellular environment to the system , specifically the presence of nucleic acid about which many capsids assemble . The major challenge of such work is computational : simulating fine-scale assembly pathways on the scale and in the parameter domains of real viruses is far too computationally costly to allow for explicit models of nucleic acid interaction . We bypass that limitation by applying analytical models of nucleic acid effects to adjust kinetic rate parameters learned from in vitro data to see how these adjustments , singly or in combination , might affect fine-scale assembly progress . The resulting simulations exhibit surprising behavioral complexity , with distinct effects often acting synergistically to drive efficient assembly and alter pathways relative to the in vitro model . The work demonstrates how computer simulations can help us understand how assembly might differ between the in vitro and in vivo environments and what features of the cellular environment account for these differences .
2K_dev_1607	Establishing quantitative bounds on the execution cost of programs is essential in many areas of computer science such as complexity analysis , compiler optimizations , security and privacy . Techniques based on program analysis , type systems and abstract interpretation are well-studied , but methods for analyzing how the execution costs of two programs compare to each other have not received attention . Naively combining the worst and best case execution costs of the two programs does not work well in many cases because such analysis forgets the similarities between the programs or the inputs . In this work , we propose a relational cost analysis technique that is capable of establishing precise bounds on the difference in the execution cost of two programs by making use of relational properties of programs and inputs . We develop Rel Cost , a refinement type and effect system for a higher-order functional language with recursion and subtyping . The key novelty of our technique is the combination of relational refinements with two modes of typing-relational typing for reasoning about similar computations/inputs and unary typing for reasoning about unrelated computations/inputs . This combination allows us to analyze the execution cost difference of two programs more precisely than a naive non-relational approach . We prove our type system sound using a semantic model based on step-indexed unary and binary logical relations accounting for non-relational and relational reasoning principles with their respective costs . We demonstrate the precision and generality of our technique through examples .
2K_dev_1608	Our human-robot collaboration research aims to improve the fluency and efficiency of interactions between humans and robots when executing a set of tasks in a shared workspace . During human-robot collaboration , a robot and a user must often complete a disjoint set of tasks that use an overlapping set of objects , without using the same object simultaneously . A key challenge is deciding what task the robot should perform next in order to facilitate fluent and efficient collaboration . Most prior work does so by first predicting the human 's intended goal , and then selecting actions given that goal . However , it is often difficult , and sometimes impossible , to infer the human 's exact goal in real time , and this serial predict-then-act method is not adaptive to changes in human goals . In this paper , we present a system for inferring a probability distribution over human goals , and producing assistance actions given that distribution in real time . The aim is to minimize the disruption caused by the nature of human-robot shared workspace . We extend recent work utilizing Partially Observable Markov Decision Processes ( POMDPs ) for shared autonomy in order to provide assistance without knowing the exact goal . We evaluate our system in a study with 28 participants , and show that our POMDP model outperforms state of the art predict-then-act models by producing fewer human-robot collisions and less human idling time .
2K_dev_1609	We provide a novel incremental data association method to complement our previous work on acoustic structure from motion ( ASFM ) , which recovers 3D scene structure from multiple 2D sonar images , while at the same time localizing the sonar . Given point features extracted from multiple overlapping sonar images , our algorithm automatically finds the correspondences between the features . Our data association method uses information about the geometric correlations of the entire set of landmarks to reject spurious measurements or 0 positives that might otherwise have been accepted . For each new sonar measurement , the algorithm uses a gating procedure to narrow the landmark match search space . Using the pruned surviving candidate correspondences , we identify the correct hypothesis based on a posterior compatibility cost , penalizing for null matches to avoid all measurements being declared new landmarks . Unlike other methods , ASFM does not require any planar scene assumptions and uses constraints from more than two images to increase accuracy in both mapping and localization . We evaluate our algorithm in simulation and demonstrate successful data association results on real sonar images .
2K_dev_1610	Despite significant progress in GPS-denied autonomous flight , long-distance traversals ( > 100 km ) in the absence of GPS remain elusive . This paper demonstrates a method capable of accurately estimating the aircraft state over a 218 km flight with a final position error of 27 m , 0.012\ % of the distance traveled . Our technique efficiently captures the full state dynamics of the air vehicle with semi-intermittent global corrections using LIDAR measurements matched against an a priori Digital Elevation Model ( DEM ) . Using an error-state Kalman filter with IMU bias estimation , we are able to maintain a high-certainty state estimate , reducing the computation time to search over a global elevation map . A sub region of the DEM is scanned with the latest LIDAR projection providing a correlation map of landscape symmetry . The optimal position is extracted from the correlation map to produce a position correction that is applied to the state estimate in the filter . This method provides a GPS-denied state estimate for long range drift-free navigation . We demonstrate this method on two flight data sets from a full-sized helicopter , showing significantly longer flight distances over the current state of the art .
2K_dev_1611	As robots aspire for long-term autonomous operations in complex dynamic environments , the ability to reliably take mission-critical decisions in ambiguous situations becomes critical . This motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision . We call this self-evaluating capability as introspection . In this paper , we take a small step in this direction and propose a generic framework for introspective behavior in perception systems . Our goal is to learn a model to reliably predict failures in a given system , with respect to a task , directly from input sensor data . We present this in the context of vision-based autonomous MAV flight in outdoor natural environments , and show that it effectively handles uncertain situations .
2K_dev_1612	The design of legged robots is often inspired by animals evolved to excel at different tasks . However , while mimicking morphological features seen in nature can be very powerful , robots may need to perform motor tasks that their living counterparts do not . In the absence of designs that can be mimicked , an alternative is to resort to mathematical models that allow the relationship between a robot 's form and function to be explored . In this paper , we propose such a model to co-design the motion and leg configurations of a robot such that a measure of performance is optimized . The framework begins by planning trajectories for a simplified model consisting of the center of mass and feet . The framework then optimizes the length of each leg link while solving for associated full-body motions . Our model was successfully used to find optimized designs for legged robots performing tasks that include jumping , walking , and climbing up a step . Although our results are preliminary and our analysis makes a number of simplifying assumptions , our findings indicate that the cost function , the sum of squared joint torques over the duration of a task , varies substantially as the design parameters change .
2K_dev_1613	We relax parametric inference to a non parametric representation towards more general solutions on factor graphs . We use the Bayes tree factorization to maximally exploit structure in the joint posterior thereby minimizing computation . We use kernel density estimation to represent a wider class of constraint beliefs , which naturally encapsulates multi-hypothesis and non-Gaussian inference . A variety of new uncertainty models can now be directly applied in the factor graph , and have the solver recover a potentially multi modal posterior . For example , data association for loop closure proposals can be incorporated at inference time without further modifications to the factor graph . Our implementation of the presented algorithm is written entirely in the Julia language , exploiting high performance parallel computing . We show a larger scale use case with the well known Victoria park mapping and localization data set inferring over uncertain loop closures .
2K_dev_1614	This article presents a resource analysis system for OCaml programs . The system automatically derives worst-case resource bounds for higher-order polymorphic programs with user-defined inductive types . The technique is parametric in the resource and can derive bounds for time , memory allocations and energy usage . The derived bounds are multivariate resource polynomials which are functions of different size parameters that depend on the standard OCaml types . Bound inference is fully automatic and reduced to a linear optimization problem that is passed to an off-the-shelf LP solver . Technically , the analysis system is based on a novel multivariate automatic amortized resource analysis ( AARA ) . It builds on existing work on linear AARA for higher-order programs with user-defined inductive types and on multivariate AARA for first-order programs with built-in lists and binary trees . This is the first amortized analysis , that automatically derives polynomial bounds for higher-order functions and polynomial bounds that depend on user-defined inductive types . Moreover , the analysis handles a limited form of side effects and even outperforms the linear bound inference of previous systems . At the same time , it preserves the expressivity and efficiency of existing AARA techniques . The practicality of the analysis system is demonstrated with an implementation and integration with Inria 's OCaml compiler . The implementation is used to automatically derive resource bounds for 411 functions and 6018 lines of code derived from OCaml libraries , the CompCert compiler , and implementations of textbook algorithms . In a case study , the system infers bounds on the number of queries that are sent by OCaml programs to DynamoDB , a commercial NoSQL cloud database service .
2K_dev_1615	We present POMP ( Pareto Optimal Motion Planner ) , an anytime algorithm for geometric path planning on roadmaps . For robots with several degrees of freedom , collision checks are computationally expensive and often dominate planning time . Our goal is to minimize the number of collision checks for obtaining the first feasible path and successively shorter feasible paths . We assume that the roadmaps we search over are embedded in a continuous ambient space , where nearby points tend to share the same collision state . This enables us to formulate a probabilistic model that computes the probability of unevaluated configurations being collision-free . We update the model over time as more checks are performed . This model lets us define a weighting function for roadmap edges that is related to the probability of the edge being in collision . Our approach is to trade off between these two weights , gradually prioritizing edge length over collision likelihood . We also show that this tradeoff is approximately equivalent to minimizing the expected path length , with a penalty of being in collision . Our experiments demonstrate that POMP performs comparably with RRTConnect and LazyPRM for the first feasible path , and BIT { * } for anytime performance , both in terms of collision checks and total planning time .
2K_dev_1616	We propose a submap-based technique for mapping of underwater structures with complex geometries . Our approach relies on the use of probabilistic volumetric techniques to create submaps from multibeam sonar scans , as these offer increased outlier robustness . Special attention is paid to the problem of denoising/enhancing sonar data . Pairwise submap alignment constraints are used in a factor graph framework to correct for navigation drift and improve map accuracy . We provide experimental results obtained from the inspection of the running gear and bulbous bow of a 600-foot , Wright-class supply ship .
2K_dev_1617	Inertial reorientation of airborne articulated bodies has been an active area of research in the robotics community , as this behavior can help guide dynamic robots to a safe landing with minimal damage . The main objective of this work is emulating the aggressive and large angle correction maneuvers , like somersaults , that are performed by human divers . To this end , a planar three link robot , called DiverBot , is proposed . By considering a gravity-free scenario , a local connection is obtained between joint angles and the body orientation , resulting in a reduction in the system dynamics . An optimal control policy applied on this reduced configuration space yielded diving maneuvers that are dynamically feasible . Numerical results show that the DiverBot can execute one somersault without drift and multiple somersaults with minimal drift .
2K_dev_1618	We present a long-range visual signal detection system that is suitable for an unmanned aerial vehicle to find an optical signal released at a desired landing site for the purposes of cargo delivery or rescue situations where radio signals or other communication systems are not available or the wind conditions at the landing site need to be signaled . The challenge here is to have a signal and detection system that works from long range ( > 1000m ) amongst ground clutter during various seasonal conditions on passive imagery . We use a smoke-grenade as a ground signal , which has the advantageous properties of being easy to carry by ground crews because of its light weight and small size , but when released has a long visual signaling range . We employ a camera system on the UAV with a visual texture feature extraction approach in a machine learning framework to classify image patches as `signal ' or `background ' . We study conventional approaches and develop a visual feature descriptor that can better differentiate the appearance of the visual signal under varying conditions and , when used to train a random-forest classifier , outperforms commonly used feature descriptors . The system was rigorously and quantitatively evaluated on data collected from a camera mounted on a helicopter and flown towards a plume of signal smoke over a variety of seasons , ground conditions , weather conditions , and environments . Our system was capable of detecting the smoke cloud with both precision and recall rates greater than 0.95 from ranges between 1000m and 1500m . Further , we develop a method to estimate wind orientation and approximate wind strength by assessing the shape of the smoke signal . We present a preliminary evaluation of the wind estimation in conditions with different wind intensities and orientations relative to the approach direction .
2K_dev_1619	In a hierarchical motion planning system for urban autonomous driving , it is a common practice to separate tactical reasoning from the lower-level trajectory planning . This separation makes it difficult to achieve robust maneuver-based tactical reasoning , which is intrinsically linked to trajectory planning . We therefore propose a planning method that automatically discovers tactical maneuver patterns , and fuses pattern reasoning and sampling-based trajectory planning . The results demonstrate enhanced planning feasibility , coherency and scalability .
2K_dev_1620	We consider the problem of generating dynamically feasible and safe plans for teams of aerial robots ( quadrotors ) while holding a fixed relative formation as well as transitioning between a sequence of formations . We extend the existing assignment and planning approaches for quadrotor teams to find minimal-time trajectories to enable team transition between non-rest initial and ending states while ensuring dynamic feasibility with respect to predefined kinematic , dynamic , and collision constraints . This work also presents a method for safe splitting and merging of robot formations according to input specification . The proposed methodology is capable of generating dynamically feasible and safe plans for teams of quadrotors in real time . We validate the performance of the proposed approach through various trials and scenarios conducted in simulation .
2K_dev_1621	In many multi-robot applications such as target search , environmental monitoring and reconnaissance , the multi-robot system operates semi-autonomously , but under the supervision of a remote human who monitors task progress . In these applications , each robot collects a large amount of task-specific data that must be sent to the human periodically to keep the human aware of task progress . It is often the case that the human-robot communication links are extremely bandwidth constrained and/or have significantly higher latency than inter-robot communication links , so it is impossible for all robots to send their task-specific data together . Thus , only a subset of robots , which we call the knowledge leaders , can send their data at a time . In this paper , we study the knowledge leader selection problem , where the goal is to select a subset of robots with a given cardinality that transmits the most informative task-specific data for the human . We prove that the knowledge leader selection is a submodular function maximization problem under explicit conditions and present a novel distributed submodular optimization algorithm that has the same approximation guarantees as the centralized greedy algorithm . The effectiveness of our approach is demonstrated using numerical simulations .
2K_dev_1622	Formal constructive type theory has proved to be an effective language for mechanized proof . By avoiding non-constructive principles , such as the law of the excluded middle , type theory admits sharper proofs and broader interpretations of results . From a computer science perspective , interest in type theory arises from its applications to programming languages . Standard constructive type theories used in mechanization admit computational interpretations based on meta-mathematical normalization theorems . These proofs are notoriously brittle ; any change to the theory potentially invalidates its computational meaning . As a case in point , Voevodsky 's univalence axiom raises questions about the computational meaning of proofs . We consider the question : Can higher-dimensional type theory be construed as a programming language ? We answer this question affirmatively by providing a direct , deterministic operational interpretation for a representative higher-dimensional dependent type theory with higher inductive types and an instance of univalence . Rather than being a formal type theory defined by rules , it is instead a computational type theory in the sense of Martin-Lof 's meaning explanations and of the NuPRL semantics . The definition of the type theory starts with programs ; types are specifications of program behavior . The main result is a canonicity theorem stating that closed programs of boolean type evaluate to 1 or 0 .
2K_dev_1623	The common operation of popular web and mobile information systems involves the collection and retention of personal information and sensitive information about their users . This information needs to remain private and each system should show a privacy policy that describes in-depth how the users ' information is managed and disclosed . However , the lack of a clear understanding and of a precise mechanism to enforce the statements described in the policy can constraint the development and adoption of these requirements . RSLingo4Privacy is a multi-language approach that intends to improve the specification and analysis of such policies , and which includes several processes with respective tools , namely : ( P1 ) automatic classification and extraction of statements and text snippets from original policies into equivalent and logically consistent specifications ( based on a privacy-aware specific language ) ; ( P2 ) visualization and authoring these statements in a consistent and rigorous way based on that privacy-aware specific language ; ( P3 ) automatic analysis and validation of the quality of these specifications ; and finally ( P4 ) policies ( re ) publishing . This paper presents and discusses the first two processes ( P1 and P2 ) . Despite having been evaluated against the policies of the most popular systems , for the sake of briefness , we just consider the Facebook policy for supporting the presentation and discussion of current results of the proposed approach .
2K_dev_1624	Planning in CPSs requires temporal reasoning to handle the dynamics of the environment , including human behavior , as well as temporal constraints on system goals and durations of actions that systems and human actors may take . The discrete abstraction of time in a state space planning should have a time sampling parameter value that satisfies some relation to achieve a certain precision . In particular , the sampling period should be small enough to allow the dynamics of the problem domain to be modeled with sufficient precision . Meanwhile , in many cases , events in the far future ( relative to the sampling period ) may be relevant to the decision making earlier in the planning timeline ; therefore , a longer planning look-ahead horizon can yield a closer-to optimal plan . Unfortunately , planning with a uniform fine-grained discrete abstraction of time and a long look-ahead horizon is typically computationally infeasible . In this paper , we propose a multiscale temporal planning approach formulated as MDP planning to preserve the required time fidelity of the problem domain and at the same time approximate a globally optimal plan . We illustrate our approach in a middleware used to monitor large sensor networks .
2K_dev_1625	Modern frameworks are required to be extendable as well as secure . However , these two qualities are often at odds . In this poster we describe an approach that uses a combination of static analysis and run-time management , based on software architecture models , that can improve security while maintaining framework extendability . We implement a prototype of the approach for the Android platform . Static analysis identifies the architecture and communication patterns among the collection of apps on an Android device and which communications might be vulnerable to attack . Run-time mechanisms monitor these potentially vulnerable communication patterns , and adapt the system to either deny them , request explicit approval from the user , or allow then .
2K_dev_1626	The Android platform is designed to support mutually un-trusted third-party apps , which run as isolated processes but may interact via platform-controlled mechanisms , called Intents . Interactions among third-party apps are intended and can contribute to a rich user experience , for example , the ability to share pictures from one app with another . The Android platform presents an interesting point in a design space of module systems that is biased toward isolation , extensibility , and untrusted contributions . The Intent mechanism essentially provides message channels among modules , in which the set of message types is extensible . However , the module system has design limitations including the lack of consistent mechanisms to document message types , very limited checking that a message conforms to its specifications , the inability to explicitly declare dependencies on other modules , and the lack of checks for backward compatibility as message types evolve over time . In order to understand the degree to which these design limitations result in real issues , we studied a broad corpus of apps and cross-validated our results against app documentation and Android support forums . Our findings suggest that design limitations do indeed cause development problems . Based on our results , we outline further research questions and propose possible mitigation strategies .
2K_dev_1627	Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed . A major complaint is that programmers often write minimal and low quality handlers . We used the Boa tool to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions . We found that programmers handle exceptions locally in catch blocks much of the time , rather than propagating by throwing an Exception . Programmers make heavy use of actions like Log , Print , Return , or Throw in catch blocks , and also frequently copy code between handlers . We found bad practices like empty catch blocks or catching Exception are indeed widespread . We discuss evidence that programmers may misjudge risk when catching Exception , and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception . Some of these issues might be addressed by future tools which autocomplete more complete handlers .
2K_dev_1628	The goal of this paper is to develop a form of functional arrays ( sequences ) that are as efficient as imperative arrays , can be used in parallel , and have well defined cost-semantics . The key idea is to consider sequences with functional value semantics but nonfunctional cost semantics . Because the value semantics is functional , `` updating { '' } a sequence returns a new sequence . We allow operations on `` older { '' } sequences ( called interior sequences ) to be more expensive than operations on the `` most recent { '' } sequences ( called leaf sequences ) . We embed sequences in a language supporting fork-join parallelism . Due to the parallelism , operations can be interleaved non-deterministically , and , in conjunction with the different cost for interior and leaf sequences , this can lead to non-deterministic costs for a program . Consequently the costs of programs can be difficult to analyze . The main result is the derivation of a deterministic cost dynamics which makes analyzing the costs easier . The theorems are not specific to sequences and can be applied to other data types with different costs for operating on interior and leaf versions . We present a wait-free concurrent implementation of sequences that requires constant work for accessing and updating leaf sequences , and logarithmic work for accessing and linear work for updating interior sequences . We sketch a proof of correctness for the sequence implementation . The key advantages of the present approach compared to current approaches is that our implementation requires no changes to existing programming languages , supports nested parallelism , and has well defined cost semantics . At the same time , it allows for functional implementations of algorithms such as depth-first search with the same asymptotic complexity as imperative implementations .
2K_dev_1629	Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation . We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility , then reconstructs face shape by fitting a part-based 3D model . Next , the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation . The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze . To evaluate the algorithm in comparison with alternative approaches , three publicly-available databases were used , Boston University Head Tracking , Multi-View Gaze and CAVE Gaze datasets . Precision for head pose and gaze averaged 4 degrees or less for pitch , yaw , and roll . The algorithm outperformed alternative methods in both datasets .
2K_dev_1630	Massive , Open , Online Courses ( MOOCs ) have been promoted as a means to revolutionize access to education . In this paper , we describe experiences with MOOCs for Liberian students who are connected to the iLab technology hub . We describe their motivations for participating as well as the challenges they encountered . We also describe the importance of the face-to-face learning environment provided by the iLab as a source of community support .
2K_dev_1631	Massive online classes can benefit from peer interactions such as discussion , critique , or tutoring . However , to scaffold productive peer interactions , systems must be able to detect student behavior in interactions at scale , which is challenging when interactions occur over rich media like video . This paper introduces an imprecise yet simple browser-based conversational turn detector for video conversations . Turns are detected without accessing video or audio data . We show how this turn detector can find dominance in video-based conversations . In a case study with 1027 students using Talkabout , a video-based discussion system for online classes , we show how detected conversational turn behavior correlates with participants ' subjective experience in discussions and their final course grade .
2K_dev_1632	Although xMOOCs are not designed to directly engage students via social media platforms , some students in these courses join MOOC-associated Facebook groups . This study explores the prevalence of Facebook groups associated with courses from MITx and HarvardX , the geographic distribution of students in such groups as compared to the courses at large , and the extent to which such groups are location and/or language homophilous . Results suggests that a non-trivial number of MOOC students engage in Facebook groups , that learners from a number of non-U.S. locations are disproportionately likely to participate in such groups , and that the groups display both location and language homophily . These findings have implications for how MOOCs and social media platforms can support learners from non-English speaking contexts .
2K_dev_1633	Massive Open Online Courses ( MOOCs ) provide an effective learning platform with various high-quality educational materials accessible to learners from all over the world . However , current MOOCs lack personalized learning guidance and intelligent assessment for individuals . Though a few recent attempts have been made to trace students ' knowledge states by adapting the popular Bayesian Knowledge Tracing ( BKT ) model , they have largely ignored the rich structures and correlations among knowledge components ( KCs ) within a course . This paper proposes to model both the hierarchical and the temporal properties of the knowledge states in order to improve the modeling accuracy . Based on the content organization characteristics on the Coursera MOOC platform , we provide a well-defined KC model , and develop Multi-Grained-BKT and Historical-BKT to capture the above features effectively . Experiments on a Coursera course dataset show our approach significantly improves over previous vanilla BKT models on predicting students ' quiz performance .
2K_dev_1634	We present online algorithms for covering and packing problems with ( non-linear ) convex objectives . The convex covering problem is defined as : min ( x is an element of R+ ) ( n ) f ( x ) s.t . Ax > 0 1 , where f : R-+ ( n ) - > R+ is a monotone convex function , and A is an m x n matrix with non-negative entries . In the online version , a new row of the constraint matrix , representing a new covering constraint , is revealed in each step and the algorithm is required to maintain a feasible and monotonically non-decreasing assignment x over time . We also consider a convex packing problem defined as : max ( y is an element of R+ ) ( m ) Sigma ( m ) ( j-1 ) y ( j ) - g ( A ( T ) y ) , where g : R-+ ( n ) - > R+ is a monotone convex function . In the online version , each variable y ( j ) arrives online and the algorithm must decide the value of y ( j ) on its arrival . This represents the Fenchel dual of the convex covering program , when g is the convex conjugate of f. We use a primal-dual approach to give online algorithms for these generic problems , and use them to simplify , unify , and improve upon previous results for several applications .
2K_dev_1635	We consider the robust curve fitting problem , for both algebraic and Fourier ( trigonometric ) polynomials , in the presence of outliers . In particular , we study the model of Arora and Khot ( STOC 2002 ) , who were motivated by applications in computer vision . In their model , the input data consists of ordered pairs ( x ( i ) , y ( i ) ) is an element of { [ } -1 , 1 ] x { [ } -1 , 1 ] , i 0 1 , 2 , ... , N , and there is an unknown degree-d polynomial p such that for all but rho fraction of the i , we have vertical bar p ( x ( i ) ) - y ( i ) vertical bar < 0 delta . Unlike Arora-Khot , we also study the trigonometric setting , where the input is from T x { [ } -1 , 11 , where T is the unit circle . In both scenarios , the i corresponding to errors are chosen randomly , and for such i the errors in the y ( i ) can be arbitrary . The goal is to output a degree-d polynomial q such that parallel to p - q parallel to ( infinity ) is small ( for example , O ( delta ) ) . Arora and Khot could achieve a polynomial-time algorithm only for rho 0 0 . Daltrophe et al . observed that a simple median-based algorithm can correct errors if the desired accuracy delta is large enough . ( Larger delta makes the output guarantee easier to achieve , which seems to typically outweigh the weaker input promise . ) We dramatically expand the range of parameters for which recovery of q is possible in polynomial time . Specifically , we show that there are polynomial-time algorithms in both settings that recover q up to l ( infinity ) error O ( delta ( 0.99 ) ) provided 1 ) rho < 0 c ( 1 ) / log d and delta > 0 1/ ( log d ) ( c ) , or 2 ) rho < 0 c ( 1 ) / log d/log ( 2 ) d , and delta > 0 1/d ( c ) . Here c is any constant and c ( 1 ) is a small enough constant depending on c. The number of points that suffices is N 0 ( O ) over tilde ( d ) in the trigonometric setting for random x ( i ) , or arbitrary x ( i ) , that are roughly equally spaced , or in the algebraic setting when the x ( i ) are chosen according to the Chebyshev distribution , and N 0 ( O ) over tilde ( d ( 2 ) ) in the algebraic setting with random ( or roughly equally spaced ) x ( i ) .
2K_dev_1636	, determine whether a ( T ) X and b ( T ) Y are uncorrelated for every a is an element of R-p , b is an element of R-q or not . We give minimax lower bound for this problem ( when p + q , n - > infinity , ( p + q ) /n < 0 kappa < infinity , without sparsity assumptions ) . In summary , our results imply that n must be at least as large as root pq/parallel to Sigma ( XY ) parallel to ( 2 ) ( F ) for any procedure ( test ) to have non-trivial power , where Sigma ( XY ) is the cross- covariance matrix of X , Y . We also provide evidence that the lower bound is tight , by connections to two-sample testing and regression when q 0 1 . Linear independence testing is a fundamental information-theoretic and statistical problem that can be posed as follows : given n points \ { ( X ( i ) ; Y-i ) \ } ( n ) ( i=1 ) from a p + q dimensional multivariate distribution where X-i is an element of R-p and Y-i is an element of R-q
2K_dev_1637	Given `` who-trusts/distrusts-whom { '' } information , how can we propagate the trust and distrust ? With the appearance of fraudsters in social network sites , the importance of trust prediction has increased . Most such methods use only explicit and implicit trust information ( e.g. , if Smith likes several of Johnson 's reviews , then Smith implicitly trusts Johnson ) , but they do not consider distrust . In this paper , we propose PIN-TRUST , a novel method to handle all three types of interaction information : explicit trust , implicit trust , and explicit distrust . The novelties of our method are the following : ( a ) it is carefully designed , to take into account positive , implicit , and negative information , ( b ) it is scalable ( i.e. , linear on the input size ) , ( c ) most importantly , it is effective and accurate . Our extensive experiments with a real dataset , Epinions . corn data , of 100K nodes and 1M edges , confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy , achieving up to 50.4 percentage relative improvement .
2K_dev_1638	Recent work in dense monocular 3D reconstruction relies on dense pixel correspondences and assumes brightness constancy and saliency , and thus are fundamentally unable to reconstruct low-textured or non-lambertian objects such as glass or metal . Occlusion boundaries differ from texture in that each unique view generates a unique set of occlusions . By detecting and solving for the depths of occlusion boundaries , we show how dense reconstructions of challenging objects can be integrated with existing monocular reconstruction algorithms by compensating with an increasing number of unique views . In a sequence containing the Stanford Bunny ( 8.5 cm ) the points of our reconstructed edge cloud have an RMS error of 2.3 mm .
2K_dev_1639	Change introduces conflict into software ecosystems : breaking changes may ripple through the ecosystem and trigger rework for users of a package , but often developers can invest additional effort or accept opportunity costs to alleviate or delay downstream costs . We performed a multiple case study of three software ecosystems with different tooling and philosophies toward change , Eclipse , R/CRAN , and Node . js/npm , to understand how developers make decisions about change and change-related costs and what practices , tooling , and policies are used . We found that all three ecosystems differ substantially in their practices and expectations toward change and that those differences can be explained largely by different community values in each ecosystem . Our results illustrate that there is a large design space in how to build an ecosystem , its policies and its supporting infrastructure ; and there is value in making community values and accepted tradeoffs explicit and transparent in order to resolve conflicts and negotiate change-related costs .
2K_dev_1640	We explore online reinforcement learning techniques to find good policies to control the orientation of a mobile robot during social group conversations . In this scenario , we assume that the correct behavior for the robot should convey attentiveness to the focus of attention of the conversation . Thus , the robot should turn towards the speaker . Our results from tests in a simulated environment show that a new state representation that we designed for this problem can be used to find good policies for the robot . These policies can generalize across interactions with different numbers of people and can handle various levels of sensing noise .
2K_dev_1641	We describe a method for low-cost awareness of characteristics of dense , moving crowds such as group formation , personal space approximation , and occlusion compensation for use in navigating through crowds . It incorporates social expectations and is inspired by human perceptual processes . The approach uses a single Kinect to cluster all moving objects into groups , applies a 2D polygon projection in obscured regions , and a group personal space modeled using asymmetric Gaussians in order to inhibit certain socially inappropriate robot paths . This approach trades off detection of individual people for higher coverage and lower cost , while preserving high speed processing . A real-world evaluation of this approach showed good performance in comparison to an existing people detection approach . The projected polygon step captures significantly more people in the scene ( 77\ % vs. 80\ % ) and supports group clustering in dense , complex scenarios . Examples are provided for group splitting and merging , dense crowds with obstructions , and cases where other approaches typically encounter difficulty .
2K_dev_1642	Trajectory planning methods for on-road autonomous driving are commonly formulated to optimize a Single Objective calculated by accumulating Multiple Weighted Feature terms ( SOMWF ) . Such formulation typically suffers from the lack of planning tunability . Two main causes are the lack of physical intuition and relative feature prioritization due to the complexity of SOMWF , especially when the number of features is big . This paper addresses this issue by proposing a framework with multiple tunable phases of planning , along with two novel techniques : Optimization-free trajectory smoothing/nudging . Sampling-based trajectory search with cascaded ranking .
2K_dev_1643	Proactive latency-aware adaptation is an approach for self-adaptive systems that improves over reactive adaptation by considering both the current and anticipated adaptation needs of the system , and taking into account the latency of adaptation tactics so that they can be started with the necessary lead time . Making an adaptation decision with these characteristics requires solving an optimization problem to select the adaptation path that maximizes an objective function over a finite look-ahead horizon . Since this is a problem of selecting adaptation actions in the context of the probabilistic behavior of the environment , Markov decision processes ( MDP ) are a suitable approach . However , given all the possible interactions between the different and possibly concurrent adaptation tactics , the system , and the environment , constructing the MDP is a complex task . Probabilistic model checking can be used to deal with this problem since it takes as input a formal specification of the stochastic system , which is internally translated into an MDP , and solved . One drawback of this solution is that the MDP has to be constructed every time an adaptation decision has to be made to incorporate the latest predictions of the environment behavior . In this paper we present an approach that eliminates that run-time overhead by constructing most of the MDP offline , also using formal specification . At run time , the adaptation decision is made by solving the MDP through stochastic dynamic programming , weaving in the stochastic environment model as the solution is computed . Our experimental results show that this approach reduces the adaptation decision time by an order of magnitude compared to the probabilistic model checking approach , while producing the same results .
2K_dev_1644	This paper assesses the potential cost-saving incentives for content distribution networks to shift traffic load among geographically distributed data centers in response to hourly variation in electricity prices . Such incentives are likely aligned with benefits to utilities and grid operators , which might take the form of peak-shaving or ancillary services . However , private cost savings are not strictly aligned with public benefits related to the avoidance of health and environmental damages from power plant emissions , so we compare private cost minimization with a strategy that minimizes these externalities . We find that feasible strategies exist to simultaneously realize public and private benefits and that load shifting can result in substantial cost savings and avoided damages in some circumstances . Concerns over increased latency and bandwidth costs can be mitigated with modifications to the model . However , the level of realized savings is dependent upon the specifics of a particular network operator and electricity rate schedule .
2K_dev_1645	Personal informatics systems are becoming increasing prevalent as their price , form , and ease of use improves . Though these systems offer great potential value to users , many systems are hampered by issues that limit their ability to foster engagement , and people often abandon use of these systems without garnering meaningful outcomes . While continued use of these systems is not necessary for all people , there is an opportunity to better support people working towards achievement-based goals . In this paper , we draw from the literature and our own prior Work to identify a number of problems that hinder engagement with achievement-based personal informatics systems-problems related to inadequate support for goal setting , misalignment of user and system goals , and the burden of system maintenance . We then propose seven strategies for the design community to explore for mitigating these problems and discuss how these strategies could be used to foster engagement with PI systems .
2K_dev_1646	Machine learning improves mobile user experience . Interestingly , envisioning apps with adaptive interfaces that reduce navigation and selection effort is not standard UX practice . When implementing an adaptive UI for our mobile transit app , we encountered a number of problems . Our original design did not log necessary information nor did it induce users to provide good labels . On reflection , we realized UX designers should identify and refine UI adaptions when sketching wireframes . To advance on this insight , we reviewed the interfaces of popular apps and extracted six design patterns where UI adaptation can improve in-app navigation . Next , we designed an exemplar set of wireframes , illustrating how UX designers might annotate their interaction flows to communicate planned adaptation and note the information ( logs and labels ) needed to make the desired inferences .
2K_dev_1647	With the aim of better scaffolding discussion to improve learning in a MOOC context , this work investigates what kinds of discussion behaviors contribute to learning . We explored whether engaging in higher-order thinking behaviors results in more learning than paying general or focused attention to course materials . In order to evaluate whether to attribute the effect to engagement in the associated behaviors versus persistent characteristics of the students , we adopted two approaches . First , we used propensity score matching to pair students who exhibit a similar level of involvement in other course activities . Second , we explored individual variation in engagement in higher-order thinking behaviors across weeks . The results of both analyses support the attribution of the effect to the behavioral interpretation . A further analysis using LDA applied to course materials suggests that more social oriented topics triggered richer discussion than more biopsychology oriented topics .
2K_dev_1648	Our previous work has demonstrated that players who perceive a game as more challenging are likely to perceive greater learning from that game { [ } 8 ] . However , this may not be the case for all sources of challenge . In this study of a Science learning game called Quantum Spectre , we found that students ' progress through the first zone of the game seemed to encounter a `` roadblock { '' } during gameplay , dropping out when they can not ( or do not want to ) progress further . Previously we had identified two primary types of errors in the learning game , Quantum Spectre : Science Errors related to the game 's core educational content ; and Puzzle Errors related to rules of the game but not to science knowledge . Using this prior analysis , alongside Survival Analysis techniques for analyzing time-series data and drop-out rates , we explored players ' gameplay patterns to help us understand player dropout in Quantum Spectre . These results demonstrate that modeling player behavior can be useful for both assessing learning and for designing complex problem solving content for learning environments .
2K_dev_1649	Compressed sensing is a simple and efficient technique that has a number of applications in signal processing and machine learning . In machine learning it provides answers to questions such as : `` under what conditions is the sparse representation of data efficient ? { '' } ; `` when is learning a large margin classifier directly on the compressed domain possible ? { '' } ; and `` why does a large margin classifier learn more effectively if the data is sparse ? { '' } . This work tackles the problem of feature representation from the context of sparsity and affine rank minimization by leveraging compressed sensing from the learning perspective in order to provide answers to the aforementioned questions . We show , for a full-rank signal , the high dimensional sparse representation of data is efficient because from the classifiers viewpoint such a representation is in fact a low dimensional problem . We provide practical bounds on the linear classifier to investigate the relationship between the SVM classifier in the high dimensional and compressed domains and show for the high dimensional sparse signals , when the bounds are tight , directly learning in the compressed domain is possible .
2K_dev_1650	An important research problem in learning analytics is to expedite the cycle of data leading to the analysis of student progress and the improvement of student support . For this goal in the context of social learning , we propose a pipeline that includes data infrastructure , learning analytics , and intervention , along with computational models for individual components . Next , we describe an example of applying this pipeline to real data in a case study , whose goal is to investigate the positive effects that goal-setting students have on their peers , which suggests ways in which we might foster these social benefits through intervention .
2K_dev_1651	Various trends are reshaping content delivery on the Internet : the explosive growth of traffic due to video , users ' increasing expectations for higher quality of experience ( QoE ) , and the proliferation of server capacity from a variety of sources ( e.g. , cloud computing services , content provider-owned datacenters , CDNs , and ISP-owned CDNs ) . In order to meet the scale and quality demands imposed by users , content providers have started to spread demand across multiple CDNs using a broker . Brokers break many traditional CDN assumptions ( e.g. , unexpected traffic skew and significant variance in demand over short timescales ) . Through an analysis of data from a leading broker and a leading CDN , we show the potential challenges and opportunities that brokers impart on content delivery . We take the first steps towards improvement through a redesigned broker-CDN interface .
2K_dev_1652	We address the problem of estimating core numbers of nodes by reading edges of a large graph stored in external memory . The core number of a node is the highest k-core in which the node participates . Core numbers are useful in many graph mining tasks , especially ones that involve finding communities of nodes , influential spreaders and dense subgraphs . Large graphs often do not fit on the memory of a single machine . Existing external memory solutions do not give bounds on the required space . In practice , existing solutions also do not scale with the size of the graph . We propose Nimble Core , an iterative external-memory algorithm , which estimates core numbers of nodes using O ( n log d ( max ) ) space , where n is the number of nodes and d ( max ) is the maximum node-degree in the graph . We also show that Nimble Core requires O ( n ) space for graphs with power-law degree distributions . Experiments on forty-eight large graphs from various domains demonstrate that Nimble Core gives space savings up to 60X , while accurately estimating core numbers with average relative error less than 2.3\ % .
2K_dev_1653	Making effective problem selection decisions is a challenging Self-Regulated Learning skill . Students need to learn effective problem-selection strategies but also develop the motivation to use them . A mastery-approach orientation is generally associated with positive problem selection behaviors such as willingness to work on new materials . We conducted a classroom experiment with 200 6th - 8th graders to investigate the effectiveness of shared control over problem selection with mastery-oriented features ( i.e. , features that aim at fostering a mastery-approach orientation that simulates effective problem-selection behaviors ) on students ' domain-level learning outcomes , problem-selection skills , enjoyment , future learning and future problem selection . The results show that shared control over problem selection accompanied by mastery-oriented features leads to significantly better learning outcomes , as compared to fully system-controlled problem selection , as well as better declarative knowledge of a key problem-selection strategy . Nevertheless , there was no effect on future problem selection and future learning . Our experiment contributes to prior literature by demonstrating that with tutor features to foster a mastery-approach orientation , shared control over problem selection can lead to significantly better learning outcomes than full system control .
2K_dev_1654	In this paper we study the effect of adaptive scaffolding to learning by teaching . We hypothesize that learning by teaching is facilitated if ( 1 ) students receive adaptive scaffolding on how to teach and how to prepare for teaching ( the metacognitive hypothesis ) , ( 2 ) students receive adaptive scaffolding on how to solve problems ( the cognitive hypothesis ) , or ( 3 ) both ( the hybrid hypothesis ) . We conducted a classroom study to test these hypotheses in the context of learning to solve equations by teaching a synthetic peer , SimStudent . The results show that the metacognitive scaffolding facilitated tutor learning ( regardless of the presence of the cognitive scaffolding ) , whereas cognitive scaffolding had virtually no effect . The same pattern was confirmed by two additional datasets collected from two previous school studies we conducted .
2K_dev_1655	We hypothesize that when cognitive tutors are integrated into online courseware , the online courseware can provide a new type of adaptive instructions , such as impasse-driven adaptive remediation and need-based assessments . As a proof of concept , we have developed an adaptive online course on the Open Learning Initiative ( OLI ) platform by integrating four new instances of cognitive tutors into an existing OLI course . Cognitive tutors were created with an innovative cognitive tutor authoring system called WATSON . To evaluate the effectiveness of the adaptive online course , a quasi-experiment was conducted in a gateway course at Carnegie Mellon University . The results show that the proposed adaptive online course technology is robust enough to be used in actual classroom with mixed effect for learning .
2K_dev_1656	Ambiguity arises in requirements when a statement is unintentionally or otherwise incomplete , missing information , or when a word or phrase has more than one possible meaning . For web-based and mobile information systems , ambiguity , and vagueness in particular , undermines the ability of organizations to align their privacy policies with their data practices , which can confuse or mislead users thus leading to an increase in privacy risk . In this paper , we introduce a theory of vagueness for privacy policy statements based on a taxonomy of vague terms derived from an empirical content analysis of 15 privacy policies . The taxonomy was evaluated in a paired comparison experiment and results were analyzed using the Bradley-Terry model to yield a rank order of vague terms in both isolation and composition . The theory predicts how vague modifiers to information actions and information types can be composed to increase or decrease overall vagueness . We further provide empirical evidence based on factorial vignette surveys to show how increases in vagueness will decrease users ' acceptance of privacy risk and thus decrease users ' willingness to share personal information .
2K_dev_1657	confidentiality of training data induced by releasing machine-learning models , and has recently received increasing attention . Motivated by existing MI attacks and other previous attacks that turn out to be MI `` in disguise , { '' } this paper initiates a formal study of MI attacks by presenting a game-based methodology . Our methodology uncovers a number of subtle issues , and devising a rigorous game-based definition , analogous to those in cryptography , is an interesting avenue for future work . We describe methodologies for two types of attacks . The first is for black-box attacks , which consider an adversary who infers sensitive values with only oracle access to a model . The second methodology targets the white-box scenario where an adversary has some additional knowledge about the structure of a model . For the restricted class of Boolean models and black-box attacks , we characterize model invertibility using the concept of influence from Boolean analysis in the noiseless case , and connect model invertibility with stable influence in the noisy case . Interestingly , we also discovered an intriguing phenomenon , which we call `` invertibility interference , { '' } where a highly invertible model quickly becomes highly non-invertible by adding little noise . For the white-box case , we consider a common phenomenon in machine-learning models where the model is a sequential composition of several sub-models . We show , quantitatively , that even very restricted communication between layers could leak a significant amount of information . Perhaps more importantly , our study also unveils unexpected computational power of these restricted communication channels , which , to the best of our knowledge , were not previously known .
2K_dev_1658	Social networking sites ( SNSs ) offer users a platform to build and maintain social connections . Understanding when people feel comfortable sharing information about themselves on SNSs is critical to a good user experience , because self-disclosure helps maintain friendships and increase relationship closeness . This observational research develops a machine learning model to measure self-disclosure in SNSs and uses it to understand the contexts where it is higher or lower . Features include emotional valence , social distance between the poster and people mentioned in the post , the language similarity between the post and the community and post topic . To validate the model and advance our understanding about online self-disclosure , we applied it to de-identified , aggregated status updates from Facebook users . Results show that women self-disclose more than men . People with a stronger desire to manage impressions self-disclose less . Network size is negatively associated with self-disclosure , while tie strength and network density are positively associated .
2K_dev_1659	Anonymity online is important to people at times in their lives . Anonymous communication applications such as Whisper and YikYak enable people to communicate with strangers anonymously through their smartphones . We report results from semi-structured interviews with 18 users of these apps . The goal of our study was to identify why and how people use anonymous apps , their perceptions of their audience and interactions on the apps , and how these apps compare with other online social communities . We present a typology of the content people share , and their motivations for participation in anonymous apps . People share various types of content that range from deep confessions and secrets to lighthearted jokes and momentary feelings . An important driver for participation and posting is to get social validation from others , even though they are anonymous strangers . We also find that participants believe these anonymous apps allow more honesty , openness , and diversity of opinion than they can find elsewhere . Our results provide implications for how anonymity in mobile apps can encourage expressiveness and interaction among users .
2K_dev_1660	When health services involve long-term treatment over months or years , providers have the ability , not present in acute emergency care , to collaboratively reflect on clients ' changing health data and adjust interventions . In this paper , we discuss temporality as a factor in the design of health information technology . We define a temporal spectrum ranging from time-critical services that benefit from standardization to long-term services that require more flexibility . We provide empirical evidence from fieldwork that we performed in organizations providing long-term behavioral and mental health services for children . Our fieldwork in this context complements and provides contrasts to previous CSCW studies performed in time-critical hospital settings . Current literature shows a bias toward standardized records and routines in the implementation of health information technology , a policy that may not be appropriate for long-term health services . We discuss how the design of information systems should vary based on temporal factors .
2K_dev_1661	Hackathons are events where people who are not normally collocated converge for a few days to write code together . Hackathons , it seems , are everywhere . We know that long-term collocation helps advance technical work and facilitate enduring interpersonal relationships , but can similar benefits come from brief , hackathon-style collocation ? How do participants spend their time preparing , working face-to-face , and following through these brief encounters ? Do the activities participants select suggest a tradeoff between the social and technical benefits of collocation ? We present results from a multiple-case study that suggest the way that hackathon-style collocation advances technical work varies across technical domain , community structure , and expertise of participants . Building social ties , in contrast , seems relatively constant across hackathons . Results from different hackathon team formation strategies suggest a tradeoff between advancing technical work and building social ties . Our findings have implications for technology support that needs to be in place for hackathons and for understanding the role of brief interludes of collocation in loosely-coupled , geographically distributed work .
2K_dev_1662	People are more creative at solving difficult design problems when they use relevant examples from outside of the problem 's domain as inspirations . However , finding such `` outside-the-box { '' } inspirations is difficult , particularly in large idea repositories such as the web , because without guidance people select domains to search based on surface similarity to the problem 's domain . In this paper , we demonstrate an approach in which non-experts identify domains that have the potential to yield useful and non-obvious inspirations for solutions . We report an empirical study demonstrating how crowds can generate domains of expertise and that showing people an abstract representation rather than the original problem helps them identify more distant domains . Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own , or drew inspiration from domains closer to or not sharing structural correspondence with the original problem .
2K_dev_1663	Large graph datasets have caused renewed interest for graph partitioning . However , existing well-studied graph partitioners often assume that vertices of the graph are always active during the computation , which may lead to time-varying skewness for traversal-style graph workloads , like Breadth First Search , since they only explore part of the graph in each superstep . Additionally , existing solutions do not consider what vertices each partition will have ; as a result , high-degree vertices may be concentrated into a few partitions , causing imbalance . Towards this , we introduce the idea of skew-resistant graph partitioning , where the objective is to create an initial partitioning that will `` hold well { '' } over time without suffering from skewness . Skewresistant graph partitioning tries to mitigate skewness by taking the characteristics of both the target workload and the graph structure into consideration .
2K_dev_1664	Current dialogue systems typically lack a variation of audio-visual feedback tokens . Either they do not encompass feedback tokens at all , or only support a limited set of stereotypical functions . However , this does not mirror the subtleties of spontaneous conversations . If we want to be able to build an artificial listener , as a first step towards building an empathetic artificial agent , we also need to be able to synthesize more subtle audio-visual feedback tokens . In this study , we devised an array of monomodal and multimodal binary comparison perception tests and experiments to understand how different realisations of verbal and visual feedback tokens influence third-party perception of the degree of attentiveness . This allowed us to investigate i ) which features ( amplitude , frequency , duration ... ) of the visual feedback influences attentiveness perception ; ii ) whether visual or verbal backchannels are perceived to be more attentive iii ) whether the fusion of unimodal tokens with low perceived attentiveness increases the degree of perceived attentiveness compared to unimodal tokens with high perceived attentiveness taken alone ; iv ) the automatic ranking of audio-visual feedback token in terms of conveyed degree of attentiveness .
2K_dev_1665	This paper proposes a method to detect unknown words during natural reading of non-native language text by using eye-tracking features . A previous approach utilizes gaze duration and word rarity features to perform this detection . However , while this system can be used by trained users , its performance is not sufficient during natural reading by untrained users . In this paper , we 1 ) apply support vector machines ( SVM ) with novel eye movement features that were not considered in the previous work and 2 ) examine the effect of personalization . The experimental results demonstrate that learning using SVMs and proposed eye movement features improves detection performance as measured by F-measure and that personalization further improves results .
2K_dev_1666	Automatic emotion recognition plays a central role in the technologies underlying social robots , affect-sensitive human computer interaction design and affect-aware tutors . Although there has been a considerable amount of research on automatic emotion recognition in adults , emotion recognition in children has been understudied . This problem is more challenging as children tend to fidget and move around more than adults , leading to more self-occlusions and non-frontal head poses . Also , the lack of publicly available datasets for children with annotated emotion labels leads most researchers to focus on adults . In this paper , we introduce a newly collected multimodal emotion dataset of children between the ages of four and fourteen years old . The dataset contains 1102 audio-visual clips annotated for 17 different emotional states : six basic emotions , neutral , valence and nine complex emotions including curiosity , uncertainty and frustration . Our experiments compare unimodal and multimodal emotion recognition baseline models to enable future research on this topic . Finally , we present a detailed analysis of the most indicative behavioral cues for emotion recognition in children .
2K_dev_1667	Cyber-attacks aimed at breaking into networks and bringing websites down appear to have become an every-day phenomenon , but there is less clarity on where the attacks come from and who are the top targets . We used DDoS attacks data shared by Arbor Networks , from June 2013 to Mar 2016 , to understand the cyber-attacks network . We take a high-level view of attacks mostly considering aggregate country-to-country attacks , using which we summarize the major players and trends in DDoS cyber-attacks .
2K_dev_1668	Cyber-attacks are cheap , easy to conduct and often pose little risk in terms of attribution , but their impact could be lasting . The low attribution is because tracing cyber-attacks is primitive in the current network architecture . Moreover , even when attribution is known , the absence of enforcement provisions in international law makes cyber attacks tough to litigate , and hence attribution is hardly a deterrent . Rather than attributing attacks , we can re-look at cyber-attacks as societal events associated with social , political , economic and cultural ( SPEC ) motivations . Because it is possible to observe SPEC motives on the internet , social media data could be valuable in understanding cyber attacks . In this research , we use sentiment in Twitter posts to observe country-to-country perceptions , and Arbor Networks data to build ground truth of country-to-country DDoS cyber-attacks . Using this dataset , this research makes three important contributions : a ) We evaluate the impact of heightened sentiments towards a country on the trend of cyber-attacks received by the country . We find that , for some countries , the probability of attacks increases by up to 27\ % while experiencing negative sentiments from other nations . b ) Using cyber-attacks trend and sentiments trend , we build a decision tree model to find attacks that could be related to extreme sentiments . c ) To verify our model , we describe three examples in which cyber-attacks follow increased tension between nations , as perceived in social media .
2K_dev_1669	Increasing proliferation of mobile and online social networking platforms have given us unprecedented opportunity to observe and study social interactions at a fine temporal scale . A collection of all such social interactions among a group of individuals ( or agents ) observed over an interval of time is referred to as a temporally-detailed ( TD ) social network . A TD social network opens up the opportunity to explore TD questions on the underlying social system , e.g. , `` How is the betweenness centrality of an individual changing with time ? { '' } To this end , related work has proposed temporal extensions of centrality metrics ( e.g. , betweenness and closeness ) . However , scalable computation of these metrics for long time-intervals is challenging . This is due to the non-stationary ranking of shortest paths ( the underlying structure of betweenness and closeness ) between a pair of nodes which violates the assumptions of classical dynamic programming based techniques . To this end , we propose a novel computational paradigm called epoch-point based techniques for addressing the non-stationarity challenge of TD social networks . Using the concept of epoch-points , we develop a novel algorithm for computing shortest path based centrality metric such as betweenness on a TD social network . We prove the correctness and completeness of our algorithm . Our experimental analysis shows that the proposed algorithm out performs the alternatives by a wide margin .
2K_dev_1670	Software architects inhabit a complex , rapidly evolving technological landscape . An ever growing collection of competing architecturally significant technologies , ranging from distributed databases to middleware and cloud platforms , makes rigorously comparing alternatives and selecting appropriate solutions a daunting engineering task . To address this problem , we envisage an ecosystem of curated , automatically updated knowledge bases that enable straightforward and streamlined technical comparisons of related products . These knowledge bases would emulate engineering handbooks that are commonly found in other engineering disciplines . As a first step towards this vision , we have built a curated knowledge base for comparing distributed databases based on a semantically defined feature taxonomy . We report in this paper on the initial results of using supervised machine learning to assist with knowledge base curation . Our results show immense promise in recommending Web pages that are highly relevant to curators . We also describe the major obstacles , both practical and scientific , that our work has uncovered . These must be overcome by future research in order to make our vision of curated knowledge bases a reality .
2K_dev_1671	Large-scale deep learning requires huge computational resources to train a multi-layer neural network . Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections . While the computation involved can be done more efficiently on GPUs than on more traditional CPU cores , training such networks on a single GPU is too slow and training on distributed GPUs can be inefficient , due to data movement overheads , GPU stalls , and limited GPU memory . This paper describes a new parameter server , called GeePS , that supports scalable deep learning across GPUs distributed among multiple machines , overcoming these obstacles . We show that GeePS enables a state-of-the-art single-node GPU implementation to scale well , such as to 13 times the number of training images processed per second on 16 machines ( relative to the original optimized single-node code ) . Moreover , GeePS achieves a higher training throughput with just four GPU machines than that a state-of-the-art CPU-only system achieves with 108 machines .
2K_dev_1672	Machine learning ( ML ) algorithms are commonly applied to big data , using distributed systems that partition the data across machines and allow each machine to read and update all ML model parameters - a strategy known as data parallelism . An alternative and complimentary strategy , model parallelism , partitions the model parameters for non-shared parallel access and updates , and may periodically repartition the parameters to facilitate communication . Model parallelism is motivated by two challenges that data-parallelism does not usually address : ( 1 ) parameters may be dependent , thus naive concurrent updates can introduce errors that slow convergence or even cause algorithm failure ; ( 2 ) model parameters converge at different rates , thus a small subset of parameters can bottleneck ML algorithm completion . We propose scheduled model parallelism ( SchMP ) , a programming approach that improves ML algorithm convergence speed by efficiently scheduling parameter updates , taking into account parameter dependencies and uneven convergence . To support SchMP at scale , we develop a distributed framework STRADS which optimizes the throughput of SchMP programs , and benchmark four common ML applications written as SchMP programs : LDA topic modeling , matrix factorization , sparse least-squares ( Lasso ) regression and sparse logistic regression . By improving ML progress per iteration through SchMP programming whilst improving iteration throughput through STRADS we show that SchMP programs running on STRADS outperform non-model-parallel ML implementations : for example , SchMP LDA and SchMP Lasso respectively achieve 10x and 5x faster convergence than recent , well-established baselines .
2K_dev_1673	Today 's cellular core , which connects the radio access network to the Internet , relies on fixed hardware appliances placed at a few dedicated locations and uses relatively static routing policies . As such , today 's core design has key limitations-it induces inefficient provisioning tradeoffs and is poorly equipped to handle overload , failure scenarios , and diverse application requirements . To address these limitations , ongoing efforts envision `` clean slate { '' } solutions that depart from cellular standards and routing protocols ; e.g. , via programmable switches at base stations and per-flow SDN-like orchestration . The driving question of this work is to ask if a clean-slate redesign is necessary and if not , how can we design a flexible cellular core that is minimally disruptive . We propose KLEIN , a design that stays within the confines of current cellular standards and addresses the above limitations by combining network functions virtualization with smart resource management . We address key challenges w.r.t . scalability and responsiveness in realizing KLEIN via backwards-compatible orchestration mechanisms . Our evaluations through data-driven simulations and real prototype experiments using OpenAirInterface show that KLEIN can scale to billions of devices and is close to optimal for wide variety of traffic and deployment parameters .
2K_dev_1674	Mutual adaptation is critical for effective team collaboration . This paper presents a formalism for human-robot mutual adaptation in collaborative tasks . We propose the bounded-memory adaptation model ( BAM ) , which captures human adaptive behaviors based on a bounded memory assumption . We integrate BAM into a partially observable stochastic model , which enables robot adaptation to the human . When the human is adaptive , the robot will guide the human towards a new , optimal collaborative strategy unknown to the human in advance . When the human is not willing to change their strategy , the robot adapts to the human in order to retain human trust . Human subject experiments indicate that the proposed formalism can significantly improve the effectiveness of human-robot teams , while human subject ratings on the robot performance and trust are comparable to those achieved by cross training , a state-ofthe-art human-robot team training practice .
2K_dev_1675	In three studies we found further evidence for a previously discovered Human-Robot ( HR ) asymmetry in moral judgments : that people blame robots more for inaction than action in a moral dilemma but blame humans more for action than inaction in the identical dilemma ( where inaction allows four persons to die and action sacrifices one to save the four ) . Importantly , we found that people 's representation of the `` robot { '' } making these moral decisions appears to be one of a mechanical robot . For when we manipulated the pictorial display of a verbally described robot , people showed the HR asymmetry only when making judgments about a mechanical-looking robot , not a humanoid robot . This is the first demonstration that robot appearance affects people 's moral judgments about robots .
2K_dev_1676	When interacting with robots deployed in the open world , people may often attempt to engage with them in a playful manner or test their competencies . Such engagements are often associated with language and behaviors that fall outside of designed task capabilities and can lead to interaction failures . Detecting when users are driven by play and curiosity can help a robot to understand why some interactions are breaking down , respond more appropriately by conveying its capabilities to its users , and enhance perceptions of its situational awareness and social intelligence . We have been studying the intentions of everyday users in their engagement with a long-lived robot system that provides directions within an office building . We report on a pilot field-study exploring the use of direct queries to elicit the sincerity of user requests , in terms of their actual need for directions . We discuss early results from this initial study and frame research directions and design implications for robots deployed in the wild .
2K_dev_1677	2D alignment of face images works well provided images are frontal or nearly so and pitch and yaw remain modest . In spontaneous facial behavior , these constraints often are violated by moderate to large head rotation . 3D alignment from 2D video has been proposed as a solution . A number of approaches have been explored , but comparisons among them have been hampered by the lack of common test data . To enable comparisons among alternative methods , The 3D Face Alignment in the Wild ( 3DFAW ) Challenge , presented for the first time , created an annotated corpus of over 23000 multi-view images from four sources together with 3D annotation , made training and validation sets available to investigators , and invited them to test their algorithms on an independent test-set . Eight teams accepted the challenge and submitted test results . We report results for four that provided necessary technical descriptions of their methods . The leading approach achieved prediction consistency error of 3.48\ % . Corresponding result for the lowest ranked approach was 5.9\ % . The results suggest that 3D alignment from 2D video is feasible on a wide range of face orientations . Differences among methods are considered and suggest directions for further research .
2K_dev_1678	The Visual Object Tracking challenge VOT2016 aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance . Results of 70 trackers are presented , with a large number of trackers being published at major computer vision conferences and journals in the recent years . The number of tested state-of-the-art trackers makes the VOT 2016 the largest and most challenging benchmark on short-term tracking to date . For each participating tracker , a short description is provided in the Appendix . The VOT2016 goes beyond its predecessors by ( i ) introducing a new semi-automatic ground truth bounding box annotation methodology and ( ii ) extending the evaluation system with the no-reset experiment .
2K_dev_1679	Much robotics research has focused on intent-expressive ( legible ) motion . However , algorithms that can autonomously generate legible motion have implicitly made the strong assumption of an omniscient observer , with access to the robot 's configuration as it changes across time . In reality , human observers have a particular viewpoint , which biases the way they perceive the motion . In this work , we free robots from this assumption and introduce the notion of an observer with a specific point of view into legibility optimization . In doing so , we account for two factors : ( 1 ) depth uncertainty induced by a particular viewpoint , and ( 2 ) occlusions along the motion , during which ( part of ) the robot is hidden behind some object . We propose viewpoint and occlusion models that enable autonomous generation of viewpoint-based legible motions , and show through large-scale user studies that the produced motions are significantly more legible compared to those generated assuming an omniscient observer .
2K_dev_1680	Nonverbal behaviors play an important role in communication for both humans and social robots . However , adding contextually appropriate animations by hand is time consuming and does not scale well . Previous researchers have developed automated systems for inserting animations based on utterance text , yet these systems lack human understanding of social context and are still being improved . This work proposes a middle ground where untrained human workers label semantic information , which is input to an automatic system to produce appropriate gestures . To test this approach , untrained workers from Mechanical Turk labeled semantic information , specifically emotion and emphasis , for each utterance , which was used to automatically add animations . Videos of a robot performing the animated dialogue were rated by a second set of participants . Results showed untrained workers are capable of providing reasonable labeling of semantic information and that emotional expressions derived from the labels were rated more highly than control videos . More study is needed to determine the effects of emphasis labels .
2K_dev_1681	Morphable face models are a powerful tool , but have previously failed to model the eye accurately due to complexities in its material and motion . We present a new multi-part model of the eye that includes a morphable model of the facial eye region , as well as an anatomy-based eyeball model . It is the first morphable model that accurately captures eye region shape , since it was built from high-quality head scans . It is also the first to allow independent eyeball movement , since we treat it as a separate part . To showcase our model we present a new method for illumination-and head-pose invariant gaze estimation from a single RGB image . We fit our model to an image through analysis-by-synthesis , solving for eye region shape , texture , eyeball pose , and illumination simultaneously . The fitted eyeball pose parameters are then used to estimate gaze direction . Through evaluation on two standard datasets we show that our method generalizes to both webcam and high-quality camera images , and outperforms a state-of-the-art CNN method achieving a gaze estimation accuracy of 9.44 degrees in a challenging user-independent scenario .
2K_dev_1682	Computer vision has a great potential to help our daily lives by searching for lost keys , watering flowers or reminding us to take a pill . To succeed with such tasks , computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes . While most of such scenes are not particularly exciting , they typically do not appear on YouTube , in movies or TV broadcasts . So how do we collect sufficiently many diverse but boring samples representing our lives ? We propose a novel Hollywood in Homes approach to collect such data . Instead of shooting videos in the lab , we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation . Following this procedure we collect a new dataset , Charades , with hundreds of people recording videos in their own homes , acting out casual everyday activities . The dataset is composed of 9848 annotated videos with an average length of 30 s , showing activities of 267 people from three continents . Each video is annotated by multiple free-text descriptions , action labels , action intervals and classes of interacted objects . In total , Charades provides 27847 video descriptions , 66500 temporally localized intervals for 157 action classes and 41104 labels for 46 object classes . Using this rich data , we evaluate and provide baseline results for several tasks including action recognition and automatic description generation . We believe that the realism , diversity , and casual nature of this dataset will present unique challenges and new opportunities for computer vision community .
2K_dev_1684	What is the right supervisory signal to train visual representations ? Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets . However , in case of biological agents , visual representation learning does not require millions of semantic labels . We argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations ( images and videos downloaded from web ) . For example , babies push objects , poke them , put them in their mouth and throw them to learn representations . Towards this goal , we build one of the first systems on a Baxter platform that pushes , pokes , grasps and observes objects in a tabletop environment . It uses four different types of physical interactions to collect more than 130K datapoints , with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations . We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation . Quantitatively , we evaluate our learned ConvNet on image classification tasks and show improvements compared to learning without external data . Finally , on the task of instance retrieval , our network outperforms the ImageNet network on recall @ 1 by 3\ % .
2K_dev_1685	Discrete energy minimization is widely-used in computer vision and machine learning for problems such as MAP inference in graphical models . The problem , in general , is notoriously intractable , and finding the global optimal solution is known to be NP-hard . However , is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time ? We show in this paper that the answer is no . Specifically , we show that general energy minimization , even in the 2-label pairwise case , and planar energy minimization with three or more labels are exp-APX-complete . This finding rules out the existence of any approximation algorithm with a sub-exponential approximation ratio in the input size for these two problems , including constant factor approximations . Moreover , we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes - PO , APX , and exp-APX , corresponding to problems that are solvable , approximable , and inapproximable in polynomial time . Problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones . This paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms .
2K_dev_1686	What happens if one pushes a cup sitting on a table toward the edge of the table ? How about pushing a desk against a wall ? In this paper , we study the problem of understanding the movements of objects as a result of applying external forces to them . For a given force vector applied to a specific location in an image , our goal is to predict long-term sequential movements caused by that force . Doing so entails reasoning about scene geometry , objects , their attributes , and the physical rules that govern the movements of objects . We design a deep neural network model that learns long-term sequential dependencies of object movements while taking into account the geometry and appearance of the scene by combining Convolutional and Recurrent Neural Networks . Training our model requires a large-scale dataset of object movements caused by external forces . To build a dataset of forces in scenes , we reconstructed all images in SUN RGB-D dataset in a physics simulator to estimate the physical movements of objects caused by external forces applied to them . Our Forces in Scenes ( ForScene ) dataset contains 65000 object movements in 3D which represent a variety of external forces applied to different types of objects . Our experimental evaluations show that the challenging task of predicting long-term movements of objects as their reaction to external forces is possible from a single image . The code and dataset are available at : http : //allenai.org/plato/forces .
2K_dev_1687	New IT functions have greatly increased the amount of in-car information delivered to drivers . Although valuable , that information can distract drivers when delivered during vehicle operation . By inferring driver state from sensor data , prior research has shown that it can accurately identify opportune moments to deliver information . Now that we know when to best deliver information , it raises the question : what information should we deliver at those interruptible moments ? To answer this question , we conducted a series of surveys and interviews and compiled a list of representative in-car information items and context factors that affect the importance of these items . By combining and exploring those context factors , we identified driving situations when each of the in-car information items is highly valuable , and verified these situations through a large online survey of drivers . Lastly , we examined what technology is available for detecting these driving situations , and which situations require further advanced technologies for detection . Results from our study offer important insights for understanding the diversity of drivers ' experiences about the value of in-car information and the ability to determine situations in which this information is valuable to drivers . With these results , researchers can then build information delivery systems that can deliver information to drivers both when they are interruptible and when they find the information valuable .
2K_dev_1688	In a given scene , humans can easily predict a set of immediate future events that might happen . However , pixel-level anticipation in computer vision is difficult because machine learning struggles with the ambiguity in predicting the future . In this paper , we focus on predicting the dense trajectory of pixels in a scene-what will move in the scene , where it will travel , and how it will deform over the course of one second . We propose a conditional variational autoencoder as a solution to this problem . In this framework , direct inference from the image shapes the distribution of possible trajectories while latent variables encode information that is not available in the image . We show that our method predicts events in a variety of scenes and can produce multiple different predictions for an ambiguous future . We also find that our method learns a representation that is applicable to semantic vision tasks .
2K_dev_1689	There is a growing interest in behavior based biometrics . Although biometric data has considerable variations for an individual and may be faked , yet the combination of such `weak experts ' can be rather strong . A remotely detectable component is gaze direction estimation and thus , eye movement patterns . Here , we present a novel personalization method for gaze estimation systems , which does not require a precise calibration setup , can be non-obtrusive , is fast and easy to use . We show that it improves the precision of gaze direction estimation algorithms considerably . The method is convenient ; we exploit 3D face model reconstruction for the enrichment of a small number of collected data artificially .
2K_dev_1690	Software architecture modeling is important for analyzing system quality attributes , particularly security . However , such analyses often assume that the architecture is completely known in advance . In many modern domains , especially those that use plugin-based frameworks , it is not possible to have such a complete model because the software system continuously changes . The Android mobile operating system is one such framework , where users can install and uninstall apps at run time . We need ways to model and analyze such architectures that strike a balance between supporting the dynamism of the underlying platforms and enabling analysis , particularly throughout a system 's lifetime . In this paper , we describe a formal architecture style that captures the modifiable architectures of Android systems , and that supports security analysis as a system evolves . We illustrate the use of the style with two security analyses : a predicate-based approach defined over architectural structure that can detect some common security vulnerabilities , and inter-app permission leakage determined by model checking . We also show how the evolving architecture of an Android device can be obtained by analysis of the apps on a device , and provide some performance evaluation that indicates that the architecture can be amenable for use throughout the system 's lifetime .
2K_dev_1691	Language usage behavior of users evolves over time , as they interact on social media such as Twitter . We study the evolution of language usage behavior of individuals , across topics , on microblogs . We propose Man-O-Meter , a framework to model such evolution . We model the evolution using a combination of three dimensions : ( a ) time , ( b ) content ( topics ) and ( c ) influence flow over social relationships . We assert the goodness of our approach , by predicting ranks of experts , with respect to their influence in their respective expertise category , using the change in language used in time . We apply our framework on 2 , 273 influential microbloggers on Twitter , across 62 categories , spanning over 10 domains . Our work is applicable in predicting activity and influence , interest evolution , job change and community change expected to happen to a user , in future .
2K_dev_1692	We present a probabilistic reachability analysis of a ( nonlinear ODE ) model of a neural circuit in Caeorhabditis elegans ( C. elegans ) , the common roundworm . In particular , we consider Tap Withdrawal ( TW ) , a reflexive behavior exhibited by a C. elegans worm in response to vibrating the surface on which it is moving . The neural circuit underlying this response is the subject of this investigation . Specially , we perform boundedtime reachability analysis on the TW circuit model of Wicks et al . ( 1996 ) to estimate the probability of various TW responses . The Wicks et al . model has a number of parameters , and we demonstrate that the various TW responses and their probability of occurrence in a population of worms can be viewed as a problem of parameter uncertainty . Our approach to this problem rests on encoding each TW response as a hybrid automaton with parametric uncertainty . We then perform probabilistic reachability analysis on these automata using a technique that combines a delta-decision procedure with statistical tests . The results we obtain are a significant extension of those of Wicks et al . ( 1996 ) , who equip their model with fixed parameter values that reproduce a single TW response . In contrast , our technique allow us to more thoroughly explore the models parameter space using statistical sampling theory , identifying in the process the distribution of TW responses . Wicks et al . conducted a number of ablation experiments on a population of worms in which one or more of the neurons in the TW circuit are surgically ablated ( removed ) . We show that our technique can be used to correctly estimate TW responseprobabilities for four of these ablation groups . We also use our technique to predict TW response behavior for two ablation groups not previously considered by Wicks et al .
2K_dev_1693	This paper studies sound proof rules for checking positive invariance of algebraic and semi-algebraic sets , that is , sets satisfying polynomial equalities and those satisfying finite boolean combinations of polynomial equalities and inequalities , under the flow of polynomial ordinary differential equations . Problems of this nature arise in formal verification of continuous and hybrid dynamical systems , where there is an increasing need for methods to expedite formal proofs . We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of benchmarks . The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting . ( C ) 2015 Elsevier Ltd. All rights reserved .
2K_dev_1694	Suppose you are a teacher , and have to convey a set of object-property pairs ( 'lions eat meat ' ) . A good teacher will convey a lot of information , with little effort on the student side . What is the best and most intuitive way to convey this information to the student , without the student being overwhelmed ? A related , harder problem is : how can we assign a numerical score to each lesson plan ( i.e. , way of conveying information ) ? Here , we give a formal definition of this problem of forming learning units and we provide a metric for comparing different approaches based on information theory . We also design an algorithm , GROUPNTEACH , for this problem . Our proposed GROUPNTEACH is scalable ( near-linear in the dataset size ) ; it is effective , achieving excellent results on real data , both with respect to our proposed metric , but also with respect to encoding length ; and it is intuitive , conforming to wellknown educational principles . Experiments on real and synthetic datasets demonstrate the effectiveness of GROUPNTEACH .
2K_dev_1695	Recent research has improved our understanding of how to create strong , memorable text passwords . However , this research has generally been in the context of desktops and laptops , while users are increasingly creating and entering passwords on mobile devices . In this paper we study whether recent password guidance carries over to the mobile setting . We compare the strength and usability of passwords created and used on mobile devices with those created and used on desktops and laptops , while varying password policy requirements and input methods . We find that creating passwords on mobile devices takes significantly longer and is more error prone and frustrating . Passwords created on mobile devices are also weaker , but only against attackers who can make more than 10 ( 13 ) guesses . We find that the effects of password policies differ between the desktop and mobile environments , and suggest ways to ease password entry for mobile users .
2K_dev_1696	Friendsourcing consists of broadcasting questions and help requests to friends on social networking sites . Despite its potential value , friendsourcing requests often fall on deaf ears . One way to improve response rates and motivate friends to undertake more effortful tasks may be to offer extrinsic rewards , such as money or a gift , for responding to friendsourcing requests . However , past research suggests that these extrinsic rewards can have unintended consequences , including undermining intrinsic motivations and undercutting the relationship between people . To explore the effects of extrinsic reward on friends ' response rate and perceived relationship , we conducted an experiment on a new friendsourcing platform - Mobilyzr . Results indicate that large extrinsic rewards increase friends ' response rates without reducing the relationship strength between friends . Additionally , the extrinsic rewards allow requesters to explain away the failure of friendsourcing requests and thus preserve their perceptions of relationship ties with friends .
2K_dev_1697	People accumulate huge assortments of a possessions , but it is not yet clear how systems and system designers can help people make meaning from these large archives . Early research in HCl has suggested that people generally appear to value their virtual things less than their material things , but theory on material possessions does not entirely explain this difference . To investigate if changes to the form and behavior of virtual things may surface valued elements of a virtual archive , we designed a technology probe that selected snippets from old emails and mailed them as physical postcards to participating households . The probe uncovered features of emails that trigger meaningful reflection , and how contextual information can help people engage in reminiscence . Our study revealed insights about how materializing virtual possessions influences factors shaping how people draw on , understand , and value those possessions . We conclude with implication and strategies for aimed at supporting people in having more meaningful interactions and experiences with their virtual possessions .
2K_dev_1698	Timebanking is a growing type of peer-to-peer service exchange , but is hampered by the effort of finding good transaction partners . We seek to reduce this effort by using a Matching Algorithm for Service Transactions ( MAST ) . MAST matches transaction partners in terms of similarity of interests and complementarity of abilities and needs . We present an experiment involving data and participants from a real timebanking network , that evaluates the acceptability of MAST , and shows that such an algorithm can retrieve matches that are subjectively better than matches based on matching the category of people 's historical offers or requests to the category of a current transaction request .
2K_dev_1699	This paper presents a case study of three participants with upper-limb amputations working with researchers to design prosthetic devices for specific tasks : playing the cello , operating a hand-cycle , and using a table knife . Our goal was to identify requirements for a design process that can engage the assistive technology user in rapidly prototyping assistive devices that fill needs not easily met by traditional assistive technology . Our study made use of 3D printing and other playful and practical prototyping materials . We discuss materials that support on-the-spot design and iteration , dimensions along which in-person iteration is most important ( such as length and angle ) and the value of a supportive social network for users who prototype their own assistive technology . From these findings we argue for the importance of extensions in supporting modularity , community engagement , and relatable prototyping materials in the iterative design of prosthetics .
2K_dev_1700	Crowdsourcing offers a powerful new paradigm for online work . However , real world tasks are often interdependent , requiring a big picture view of the difference pieces involved . Existing crowdsourcing approaches that support such tasks - ranging fromWikipedia to flash teams-are bottlenecked by relying on a small number of individuals to maintain the big picture . In this paper , we explore the idea that a computational system can scaffold an emerging interdependent , big picture view entirely through the small contributions of individuals , each of whom sees only a part of the whole . To investigate the viability , strengths , and weaknesses of this approach we instantiate the idea in a prototype system for accomplishing distributed information synthesis and evaluate its output across a variety of topics . We also contribute a set of design patterns that may be informative for other systems aimed at supporting big picture thinking in small pieces .
2K_dev_1701	More and more data nowadays exist in hierarchical formats such as JSON due to the increasing popularity of web applications and web services . While many end-user systems support getting hierarchical data from databases without programming , they provide very little support for using hierarchical data beyond turning the data into a flat string or table . In this paper , we present a spreadsheet tool for using and exploring hierarchical datasets . We introduce novel interaction techniques and algorithms to manipulate and visualize hierarchical data in a spreadsheet using the data 's relative hierarchical relationships with the data in its adjacent columns . Our tool leverages the data 's structural information to support selecting , grouping , joining , sorting and filtering hierarchical data in spreadsheets . Our lab study showed that our tool helped spreadsheet users complete data exploration tasks nearly two times faster than using Excel and even outperform programmers in most tasks .
2K_dev_1702	The present research investigated whether digital and non-digital platforms activate differing default levels of cognitive construal . Two initial randomized experiments revealed that individuals who completed the same information processing task on a digital mobile device ( a tablet or laptop computer ) versus a non-digital platform ( a physical print-out ) exhibited a lower level of construal , one prioritizing immediate , concrete details over abstract , decontextualized interpretations . This pattern emerged both in digital platform participants ' greater preference for concrete versus abstract descriptions of behaviors as well as superior performance on detail-focused items ( and inferior performance on inference-focused items ) on a reading comprehension assessment . A pair of final studies found that the likelihood of correctly solving a problem-solving task requiring higher-level `` gist { '' } processing was : ( 1 ) higher for participants who processed the information for task on a non-digital versus digital platform and ( 2 ) heightened for digital platform participants who had first completed an activity activating an abstract mindset , compared to ( equivalent ) performance levels exhibited by participants who had either completed no prior activity or completed an activity activating a concrete mindset .
2K_dev_1703	Crowdsourced clustering approaches present a promising way to harness deep semantic knowledge for clustering complex information . However , existing approaches have difficulties supporting the global context needed for workers to generate meaningful categories , and are costly because all items require human judgments . We introduce Alloy , a hybrid approach that combines the richness of human judgments with the power of machine algorithms . Alloy supports greater global context through a new `` sample and search { '' } crowd pattern which changes the crowd 's task from classifying a fixed subset of items to actively sampling and querying the entire dataset . It also improves efficiency through a two phase process in which crowds provide examples to help a machine cluster the head of the distribution , then classify low-confidence examples in the tail . To accomplish this , Alloy introduces a modular `` cast and gather { '' } approach which leverages a machine learning backbone to stitch together different types of judgment tasks .
2K_dev_1704	Although many users create predictable passwords , the extent to which users realize these passwords are predictable is not well understood . We investigate the relationship between users ' perceptions of the strength of specific passwords and their actual strength . In this 165-participant online study , we ask participants to rate the comparative security of carefully juxtaposed pairs of passwords , as well as the security and memorability of both existing passwords and common password-creation strategies . Participants had serious misconceptions about the impact of basing passwords on common phrases and including digits and keyboard patterns in passwords . However , in most other cases , participants ' perceptions of what characteristics make a password secure were consistent with the performance of current password-cracking tools . We find large variance in participants ' understanding of how passwords may be attacked , potentially explaining why users nonetheless make predictable passwords . We conclude with design directions for helping users make better passwords .
2K_dev_1705	Stateless model checking is a powerful technique for testing concurrent programs , but suffers from exponential state space explosion when the test input parameters are too large . Several reduction techniques can mitigate this explosion , but even after pruning equivalent interleavings , the state space size is often intractable . Most prior tools are limited to pre-empting only on synchronization APIs , which reduces the space further , but can miss unsynchronized thread communication bugs . Data race detection , another concurrency testing approach , focuses on suspicious memory access pairs during a single test execution . It avoids concerns of state space size , but may report races that do not lead to observable failures , which jeopardizes a user 's willingness to use the analysis . We present QUICKSAND , a new stateless model checking framework which manages the exploration of many state spaces using different preemption points . It uses state space estimation to prioritize jobs most likely to complete in a fixed CPU budget , and it incorporates data-race analysis to add new preemption points on the fly . Preempting threads during a data race 's instructions can automatically classify the race as buggy or benign , and uncovers new bugs not reachable by prior model checkers . It also enables full verification of all possible schedules when every data race is verified as benign within the CPU budget . In our evaluation , QUICKSAND found 1.25x as many bugs and verified 4.3x as many tests compared to prior model checking approaches .
2K_dev_1706	Previous work on muscle activity sensing has leveraged specialized sensors such as electromyography and force sensitive resistors . While these sensors show great potential for detecting finger/hand gestures , they require additional hardware that adds to the cost and user discomfort . Past research has utilized sensors on commercial devices , focusing on recognizing gross hand gestures . In this work we present Serendipity , a new technique for recognizing unremarkable and fine-motor finger gestures using integrated motion sensors ( accelerometer and gyroscope ) in off-the-shelf smartwatches . Our system demonstrates the potential to distinguish 5 fine-motor gestures like pinching , tapping and rubbing fingers with an average f1-score of 87\ % . Our work is the first to explore the feasibility of using solely motion sensors on everyday wearable devices to detect fine-grained gestures . This promising technology can be deployed today on current smartwatches and has the potential to be applied to cross-device interactions , or as a tool for research in fields involving finger and hand motion .
2K_dev_1707	Clinical decision support tools ( DSTs ) are computational systems that aid healthcare decision-making . While effective in labs , almost all these systems failed when they moved into clinical practice . Healthcare researchers speculated it is most likely due to a lack of user-centered HCI considerations in the design of these systems . This paper describes a field study investigating how clinicians make a heart pump implant decision with a focus on how to best integrate an intelligent DST into their work process . Our findings reveal a lack of perceived need for and trust of machine intelligence , as well as many barriers to computer use at the point of clinical decision-making . These findings suggest an alternative perspective to the traditional use models , in which clinicians engage with DSTs at the point of making a decision . We identify situations across patients ' healthcare trajectories when decision supports would help , and we discuss new forms it might take in these situations .
2K_dev_1708	There is a significant gap in the body of research on cross-device interfaces . Research has largely focused on enabling them technically , but when and how users want to use cross-device interfaces is not well understood . This paper presents an exploratory user study with XDBrowser , a cross-device web browser we are developing to enable non-technical users to adapt existing single-device web interfaces for cross-device use while viewing them in the browser . We demonstrate that an end-user customization tool like XDBrowser is a powerful means to conduct user-driven elicitation studies useful for understanding user preferences and design requirements for cross-device interfaces . Our study with 15 participants elicited 144 desirable multi-device designs for five popular web interfaces when using two mobile devices in parallel . We describe the design space in this context , the usage scenarios targeted by users , the strategies used for designing cross-device interfaces , and seven concrete mobile multi-device design patterns that emerged . We discuss the method , compare the cross-device interfaces from our users and those defined by developers in prior work , and establish new requirements from observed user behavior . In particular , we identify the need to easily switch between different interface distributions depending on the task and to have more fine-grained control over synchronization .
2K_dev_1709	Social media is an increasingly important part of modern life . We investigate the use of and usability of Twitter by blind users , via a combination of surveys of blind Twitter users , large-scale analysis of tweets from and Twitter profiles of blind and sighted users , and analysis of tweets containing embedded imagery . While Twitter has traditionally been thought of as the most accessible social media platform for blind users , Twitter 's increasing integration of image content and users ' diverse uses for images have presented emergent accessibility challenges . Our findings illuminate the importance of the ability to use social media for people who are blind , while also highlighting the many challenges such media currently present this user base , including difficulty in creating profiles , in awareness of available features and settings , in controlling revelations of one 's disability status , and in dealing with the increasing pervasiveness of image based content . We propose changes that Twitter and other social platforms should make to promote fuller access to users with visual impairments .
2K_dev_1710	We describe techniques that allow inexpensive , ultra-thin , battery-free Radio Frequency Identification ( RFID ) tags to be turned into simple paper input devices . We use sensing and signal processing techniques that determine how a tag is being manipulated by the user via an RFID reader and show how tags may be enhanced with a simple set of conductive traces that can be printed on paper , stencil-traced , or even hand-drawn . These traces modify the behavior of contiguous tags to serve as input devices . Our techniques provide the capability to use off-the-shelf RFID tags to sense touch , cover , overlap of tags by conductive or dielectric ( insulating ) materials , and tag movement trajectories . Paper prototypes can be made functional in seconds . Due to the rapid deployability and low cost of the tags used , we can create a new class of interactive paper devices that are drawn on demand for simple tasks . These capabilities allow new interactive possibilities for pop-up books and other papercraft objects .
2K_dev_1711	We address the problem of segmenting a multi-dimensional time series into stationary blocks by improving AutoSLEX { [ } 1 ] , which has been successfully used for this purpose . AutoSLEX finds the best basis in a library of smoothed localized exponentials ( SLEX ) basis functions that are orthogonal and localized in both time and frequency . We introduce DynamicSLEX , a variant of AutoSLEX that relaxes the dyadic intervals constraint of AutoSLEX , allowing for more flexible segmentation while maintaining tractability . Then , we introduce RandSLEX , which uses random projections to scale-up SLEX-based segmentation to high dimensional inputs and to establish a notion of strength of splitting points in the segmentation . We demonstrate the utility of the proposed improvements on synthetic and real data .
2K_dev_1712	Current symbol-based dictionaries providing vocabulary support for persons with the language disorder , aphasia , are housed on smartphones or other portable devices . To employ the support on these external devices requires the user to divert their attention away from their conversation partner , to the neglect of conversation dynamics like eye contact or verbal inflection . A prior study investigated head-worn displays ( HWDs ) as an alternative form factor for supporting glanceable , unobtrusive , and always-available conversation support , but it did not directly compare the HWD to a control condition . To address this limitation , we compared vocabulary support on a HWD to equivalent support on a smartphone in terms of overall experience , perceived focus , and conversational success . Lastly , we elicited critical discussion of how each device might be better designed for conversation support . Our work contributes ( 1 ) evidence that a HWD can support more efficient communication , ( 2 ) preliminary results that a HWD can provide a better overall experience using assistive vocabulary , and ( 3 ) a characterization of the design features persons with aphasia value in portable conversation support technologies . Our findings should motivate further work on head-worn conversation support for persons with aphasia .
2K_dev_1713	When navigating indoors , blind people are often unaware of key visual information , such as posters , signs , and exit doors . Our VizMap system uses computer vision and crowdsourcing to collect this information and make it available non-visually . VizMap starts with videos taken by on-site sighted volunteers and uses these to create a 3D spatial model . These video frames are semantically labeled by remote crowd workers with key visual information . These semantic labels are located within and embedded into the reconstructed 3D model , forming a query-able spatial representation of the environment . VizMap can then localize the user with a photo from their smartphone , and enable them to explore the visual elements that are nearby . We explore a range of example applications enabled by our reconstructed spatial representation . With VizMap , we move towards integrating the strengths of the end user , on-site crowd , online crowd , and computer vision to solve a long-standing challenge in indoor blind exploration .
2K_dev_1714	Playtesting , or using play to guide game design , gives designers feedback about whether their game is meeting their goals and the player 's expectations . We report a case study of designing , deploying , and iterating on a series of playtesting workshops for novice game designers . We identify common missteps made by novice designers and address these missteps through the concept of purposefulness , understanding why you are playtesting as well as how to playtest . We ground our workshops in the development of rich player experience goals , which inform playtest design , data collection and iteration . We show that by applying methods taught in our workshops , novice game designers leveraged playtest methods and tools , employed playtesting and data collection methods appropriate for their goals , and effectively applied playtest data in iterative design . We conclude with lessons learned and next steps in our research on playtesting .
2K_dev_1715	Unease over data privacy will retard consumer acceptance of IoT deployments . The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud . This is a direct consequence of the over-centralization of today 's cloud-based IoT hub designs . We propose a solution that interposes a locally-controlled software component called a privacy mediator on every raw sensor stream . Each mediator is in the same administrative domain as the sensors whose data is being collected , and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain . This solution necessitates a logical point of presence for mediators within the administrative boundaries of each organization . Such points of presence are provided by cloudlets , which are small locally-administered data centers at the edge of the Internet that can support code mobility . The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility .
2K_dev_1716	We introduce differential refinement logic ( dRL ) , a logic with first-class support for refinement relations on hybrid systems , and a proof calculus for verifying such relations . dRL simultaneously solves several seemingly different challenges common in theorem proving for hybrid systems : 1 . When hybrid systems are complicated , it is useful to prove properties about simpler and related subsystems before tackling the system as a whole . 2 . Some models of hybrid systems can be implementation-specific . Verification can be aided by abstracting the system down to the core components necessary for safety , but only if the relations between the abstraction and the original system can be guaranteed . 3 . One approach to taming the complexities of hybrid systems is to start with a simplified version of the system and iteratively expand it . However , this approach can be costly , since every iteration has to be proved safe from scratch , unless refinement relations can be leveraged in the proof . 4 . When proofs become large , it is difficult to maintain a modular or comprehensible proof structure . By using a refinement relation to arrange proofs hierarchically according to the structure of natural subsystems , we can increase the readability and modularity of the resulting proof . dRL extends an existing specification and verification language for hybrid systems ( differential dynamic logic , dL ) by adding a refinement relation to directly compare hybrid systems . This paper gives a syntax , semantics , and proof calculus for dRL . We demonstrate its usefulness with examples where using refinement results in easier and better-structured proofs .
2K_dev_1717	Self-adaptive systems have the ability to adapt their behavior to dynamic operating conditions . In reaction to changes in the environment , these systems determine the appropriate corrective actions based in part on information about which action will have the best impact on the system . Existing models used to describe the impact of adaptations are either unable to capture the underlying uncertainty and variability of such dynamic environments , or are not compositional and described at a level of abstraction too low to scale in terms of specification effort required for non-trivial systems . In this paper , we address these shortcomings by describing an approach to the specification of impact models based on architectural system descriptions , which at the same time allows us to represent both variability and uncertainty in the outcome of adaptations , hence improving the selection of the best corrective action . The core of our approach is a language equipped with a formal semantics defined in terms of Discrete Time Markov Chains that enables us to describe both the impact of adaptation tactics , as well as the assumptions about the environment . To validate our approach , we show how employing our language can improve the accuracy of predictions used for decision-making in the Rainbow framework for architecture-based self-adaptation . ( C ) 2016 Elsevier B.V. All rights reserved .
2K_dev_1718	Everyday tools and objects often need to be customized for an unplanned use or adapted for specific user , such as adding a bigger pull to a zipper or a larger grip for a pen . The advent of low-cost 3D printing offers the possibility to rapidly construct a wide range of such adaptations . However , while 3D printers are now affordable enough for even home use , the tools needed to design custom adaptations normally require skills that are beyond users with limited 3D modeling experience . In this paper , we describe Reprise-a design tool for specifying , generating , customizing and fitting adaptations onto existing household objects . Reprise allows users to express at a high level what type of action is applied to an object . Based on this high level specification , Reprise automatically generates adaptations . Users can use simple sliders to customize the adaptations to better suit their particular needs and preferences , such as increasing the tightness for gripping , enhancing torque for rotation , or making a larger base for stability . Finally , Reprise provides a toolkit of fastening methods and support structures for fitting the adaptations onto existing objects . To validate our approach , we used Reprise to replicate 15 existing adaptation examples , each of which represents a specific category in a design space distilled from an analysis of over 3000 cases found in the literature and online communities . We believe this work would benefit makers and designers for prototyping lifehacking solutions and assistive technologies .
2K_dev_1719	Patients researching medical diagnoses , scientist exploring new fields of literature , and students learning about new domains are all faced with the challenge of capturing information they find for later use . However , saving information is challenging on mobile devices , where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use . Furthermore , beyond the challenge of simply selecting a region of bounded text on a mobile device , in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user . In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster , we introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration . We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context to support identifying and saving information in an intentionally uncertain way on mobile devices . In a two part user study we find that this approach reduced selection time and was preferred by participants over the default system text selection method .
2K_dev_1720	The world is full of physical interfaces that are inaccessible to blind people , from microwaves and information kiosks to thermostats and checkout terminals . Blind people can not independently use such devices without at least first learning their layout , and usually only after labeling them with sighted assistance . We introduce VizLens-an accessible mobile application and supporting backend that can robustly and interactively help blind people use nearly any interface they encounter . VizLens users capture a photo of an inaccessible interface and send it to multiple crowd workers , who work in parallel to quickly label and describe elements of the interface to make subsequent computer vision easier . The VizLens application helps users recapture the interface in the field of the camera , and uses computer vision to interactively describe the part of the interface beneath their finger ( updating 8 times per second ) . We show that VizLens provides accurate and usable real-time feedback in a study with 10 blind participants , and our crowdsourcing labeling workflow was fast ( 8 minutes ) , accurate ( 99.7\ % ) , and cheap ( \ $ 1.15 ) . We then explore extensions of VizLens that allow it to ( i ) adapt to state changes in dynamic interfaces , ( ii ) combine crowd labeling with OCR technology to handle dynamic displays , and ( iii ) benefit from head-mounted cameras . VizLens robustly solves a long-standing challenge in accessibility by deeply integrating crowdsourcing and computer vision , and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone .
2K_dev_1721	The recent advances in image captioning stimulate the research in generating natural language description for visual content , which can be widely applied in many applications such as assisting blind people . Video description generation is a more complex task than image caption . Most works of video description generation focus on visual information in the video . However , audio provides rich information for describing video contents as well . In this paper , we propose to generate video descriptions in natural sentences using both audio and visual cues . We use unified deep neural networks with both convolutional and recurrent structure . Experimental results on the Microsoft Research Video Description ( MSVD ) corpus prove that fusing audio information greatly improves the video description performance .
2K_dev_1722	Recent research in embedded and cyber-physical systems has developed theories and tools for integration of heterogeneous components and models . These efforts , although important , are insufficient for high-quality and error-free systems integration since inconsistencies between system elements may stem from factors not directly represented in models ( e.g. , analysis tools and expert disagreements ) . Therefore , we need to broaden our perspective on integration , and devise approaches in three novel directions of integration : modeling methods , data sets , and humans . This paper summarizes the latest advances , and discusses those directions and associated challenges in integration for cyber-physical systems .
2K_dev_1723	An important feature of functional programs is that they are parallel by default . Implementing an efficient parallel functional language , however , is a major challenge , in part because the high rate of allocation and freeing associated with functional programs requires an efficient and scalable memory manager . In this paper , we present a technique for parallel memory management for strict functional languages with nested parallelism . At the highest level of abstraction , the approach consists of a technique to organize memory as a hierarchy of heaps , and an algorithm for performing automatic memory reclamation by taking advantage of a disentanglement property of parallel functional programs . More specifically , the idea is to assign to each parallel task its own heap in memory and organize the heaps in a hierarchy/tree that mirrors the hierarchy of tasks . We present a nested-parallel calculus that specifies hierarchical heaps and prove in this calculus a disentanglement property , which prohibits a task from accessing objects allocated by another task that might execute in parallel . Leveraging the disentanglement property , we present a garbage collection technique that can operate on any subtree in the memory hierarchy concurrently as other tasks ( and/or other collections ) proceed in parallel . We prove the safety of this collector by formalizing it in the context of our parallel calculus . In addition , we describe how the proposed techniques can be implemented on modern shared-memory machines and present a prototype implementation as an extension to MLton , a high-performance compiler for the Standard ML language . Finally , we evaluate the performance of this implementation on a number of parallel benchmarks .
2K_dev_1724	Micro-clones are small pieces of redundant code , such as repeated subexpressions or statements . In this paper , we establish the considerations and value toward automated detection and removal of micro-clones at scale . We leverage the Boa software mining infrastructure to detect micro-clones in a data set containing 380125 Java repositories , and yield thousands of instances where redundant code may be safely removed . By filtering our results to target popular Java projects on GitHub , we proceed to issue 43 pull requests that patch micro-clones . In summary , 95\ % of our patches to active GitHub repositories are merged rapidly ( within 15 hours on average ) . Moreover , none of our patches were contested ; they either constituted a real flaw , or have not been considered due to repository inactivity . Our results suggest that the detection and removal of micro-clones is valued by developers , can be automated at scale , and may be fixed with rapid turnaround times .
2K_dev_1725	Imperfect-recall abstraction has emerged as the leading paradigm for practical large-scale equilibrium computation in imperfect-information games . However , imperfect-recall abstractions are poorly understood , and only weak algorithm-specific guarantees on solution quality are known . We develop the first general , algorithm-agnostic , solution quality guarantees for Nash equilibria and approximate self-trembling equilibria computed in imperfect-recall abstractions , when implemented in the original ( perfect-recall ) game . Our results are for a class of games that generalizes the only previously known class of imperfect-recall abstractions for which any such results have been obtained . Further , our analysis is tighter in two ways , each of which can lead to an exponential reduction in the solution quality error bound . We then show that for extensive-form games that satisfy certain properties , the problem of computing a bound-minimizing abstraction for a single level of the game reduces to a clustering problem , where the increase in our bound is the distance function . This reduction leads to the first imperfect-recall abstraction algorithm with solution quality bounds . We proceed to show a divide in the class of abstraction problems . If payoffs are at the same scale at all information sets considered for abstraction , the input forms a metric space , and this immediately yields a 2-approximation algorithm for abstraction . Conversely , if this condition is not satisfied , we show that the input does not form a metric space . Finally , we provide computational experiments to evaluate the practical usefulness of the abstraction techniques . They show that running counterfactual regret minimization on such abstractions leads to good strategies in the original games .
2K_dev_1726	SwitchKV is a new key-value store system design that combines high-performance cache nodes with resource-constrained backend nodes to provide load balancing in the face of unpredictable workload skew . The cache nodes absorb the hottest queries so that no individual backend node is over-burdened . Compared with previous designs , SwitchKV exploits SDN techniques and deeply optimized switch hardware to enable efficient content-based routing . Programmable network switches keep track of cached keys and route requests to the appropriate nodes at line speed , based on keys encoded in packet headers . A new hybrid caching strategy keeps cache and switch forwarding rules updated with low overhead and ensures that system load is always well-balanced under rapidly changing workloads . Our evaluation results demonstrate that SwitchKV can achieve up to 5x throughput and 3x latency improvements over traditional system designs .
2K_dev_1727	Multi-stage log-structured ( MSLS ) designs , such as LevelDB , RocksDB , HBase , and Cassandra , are a family of storage system designs that exploit the high sequential write speeds of hard disks and flash drives by using multiple append-only data structures . As a first step towards accurate and fast evaluation of MSLS , we propose new analytic primitives and MSLS design models that quickly give accurate performance estimates . Our model can almost perfectly estimate the cost of inserts in LevelDB , whereas the conventional worst-case analysis gives 1.83.5X higher estimates than the actual cost . A few minutes of offline analysis using our model can find optimized system parameters that decrease LevelDB 's insert cost by up to 9.4-26.2\ % ; our analytic primitives and model also suggest changes to RocksDB that reduce its insert cost by up to 32.0\ % , without reducing query performance or requiring extra memory .
2K_dev_1728	Motivation : Reconstructing regulatory networks from expression and interaction data is a major goal of systems biology . While much work has focused on trying to experimentally and computationally determine the set of transcription-factors ( TFs ) and microRNAs ( miRNAs ) that regulate genes in these networks , relatively little work has focused on inferring the regulation of miRNAs by TFs . Such regulation can play an important role in several biological processes including development and disease . The main challenge for predicting such interactions is the very small positive training set currently available . Another challenge is the fact that a large fraction of miRNAs are encoded within genes making it hard to determine the specific way in which they are regulated . Results : To enable genome wide predictions of TF-miRNA interactions , we extended semisupervised machine-learning approaches to integrate a large set of different types of data including sequence , expression , ChIP-seq and epigenetic data . As we show , the methods we develop achieve good performance on both a labeled test set , and when analyzing general co-expression networks . We next analyze mRNA and miRNA cancer expression data , demonstrating the advantage of using the predicted set of interactions for identifying more coherent and relevant modules , genes , and miRNAs . The complete set of predictions is available on the supporting website and can be used by any method that combines miRNAs , genes , and TFs .
2K_dev_1729	We present uberSpark ( uSpark ) , an innovative architecture for compositional verification of security properties of extensible hypervisors written in C and Assembly . uSpark comprises two key ideas : ( i ) endowing low-level system software with abstractions found in higher-level languages ( e.g. , objects , interfaces , function-call semantics for implementations of interfaces , access control on interfaces , concurrency and serialization ) , enforced using a combination of commodity hardware mechanisms and lightweight static analysis ; and ( ii ) interfacing with platform hardware by programming in Assembly using an idiomatic style ( called CASM ) that is verifiable via tools aimed at C , while retaining its performance and low-level access to hardware . After verification , the C code is compiled using a certified compiler while the CASM code is translated into its corresponding Assembly instructions . Collectively , these innovations enable compositional verification of security invariants without sacrificing performance . We validate uSpark by building and verifying security invariants of an existing open-source commodity x86 micro-hypervisor and several of its extensions , and demonstrating only minor performance overhead with low verification costs .
2K_dev_1730	Human-chosen text passwords , today 's dominant form of authentication , are vulnerable to guessing attacks . Unfortunately , existing approaches for evaluating password strength by modeling adversarial password guessing are either inaccurate or orders of magnitude too large and too slow for real-time , client-side password checking . We propose using artificial neural networks to model text passwords ' resistance to guessing attacks and explore how different architectures and training methods impact neural networks ' guessing effectiveness . We show that neural networks can often guess passwords more effectively than state-of-the-art approaches , such as probabilistic context-free grammars and Markov models . We also show that our neural networks can be highly compressed-to as little as hundreds of kilobytes-without substantially worsening guessing effectiveness . Building on these results , we implement in JavaScript the first principled client-side model of password guessing , which analyzes a password 's resistance to a guessing attack of arbitrary duration with sub-second latency . Together , our contributions enable more accurate and practical password checking than was previously possible .
2K_dev_1731	Modern RDMA hardware offers the potential for exceptional performance , but design choices including which RDMA operations to use and how to use them significantly affect observed performance . This paper lays out guidelines that can be used by system designers to navigate the RDMA design space . Our guidelines emphasize paying attention to low-level details such as individual PCIe transactions and NIC architecture . We empirically demonstrate how these guidelines can be used to improve the performance of RDMA-based systems : we design a networked sequencer that outperforms an existing design by 50x , and improve the CPU efficiency of a prior high-performance key-value store by 83\ % . We also present and evaluate several new RDMA optimizations and pitfalls , and discuss how they affect the design of RDMA systems .
2K_dev_1732	Transcription makes speech accessible to deaf and hard of hearing people . This conversion of speech to text is still done manually by humans , despite high cost , because the quality of automated speech recognition ( ASR ) is still too low in real-world settings . Manual conversion can require more than 5 times the original audio time , which also introduces significant latency . Giving transcriptionists ASR output as a starting point seems like a reasonable approach to making humans more efficient and thereby reducing this cost , but the effectiveness of this approach is clearly related to the quality of the speech recognition output . At high error rates , fixing inaccurate speech recognition output may take longer than producing the transcription from scratch , and transcriptionists may not realize when transcription output is too inaccurate to be useful . In this paper , we empirically explore how the latency of transcriptions created by participants recruited on Amazon Mechanical Turk vary based on the accuracy of speech recognition output . We present results from 2 studies which indicate that starting with the ASR output is worse unless it is sufficiently accurate ( Word Error Rate of under 30\ % ) .
2K_dev_1733	Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines . Yet there has been relatively little work in modeling the behaviors and temporal properties of websites , as most research focuses on detecting whether a website distributes malware . In this paper we ask : How does web-based malware spread ? We conduct an extensive study and follow a website-centric and user-centric point of view . We collect data from four online databases , including Symantec 's WINE Project , for a total of more than 600K malicious URLs and over 500K users . First , we find that legitimate but compromised websites constitute 33.1\ % of the malicious websites in our dataset . In order to conduct this study , we develop a classifier to distinguish between compromised vs. malicious websites with an accuracy of 95.3\ % , which could be of interest to studies on website profiling . Second , we find that malicious URLs can be surprisingly long-lived , with 10\ % of malicious sites staying active for three months or more . Third , we observe that a significant number of URLs exhibit the same temporal pattern that suggests a flush-crowd behavior , inflicting most of their damage during the first few days of appearance . Finally , the distribution of the visits to malicious sites per user is skewed , with 1.4\ % of users visiting more than 10 malicious sites in 8 months . Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models .
2K_dev_1734	Modern Internet applications are being disaggregated into a microservice-based architecture , with services being updated and deployed hundreds of times a day . The accelerated software life cycle and heterogeneity of language runtimes in a single application necessitates a new approach for testing the resiliency of these applications in production infrastructures . We present Gremlin , a framework for systematically testing the failure-handling capabilities of microservices . Gremlin is based on the observation that microservices are loosely coupled and thus rely on standard message-exchange patterns over the network . Gremlin allows the operator to easily design tests and executes them by manipulating inter-service messages at the network layer . We show how to use Gremlin to express common failure scenarios and how developers of an enterprise application were able to discover previously unknown bugs in their failure-handling code without modifying the application .
2K_dev_1735	We consider the task of multiparty computation performed over networks in the presence of random noise . Given an n-party protocol that takes R rounds assuming noiseless communication , the goal is to find a coding scheme that takes R ' rounds and computes the same function with high probability even when the communication is noisy , while maintaining a constant asymptotic rate , i.e. , while keeping inf ( n , R - > infinity ) R/R ' positive . Rajagopalan and Schulman ( STOC `94 ) were the first to consider this question , and provided a coding scheme with rate O ( 1/ log ( d + 1 ) ) , where d is the maximal degree in the network . While that scheme provides a constant rate coding for many practical situations , in the worst case , e.g. , when the network is a complete graph , the rate is O ( 1/ log n ) , which tends to 0 as n tends to infinity . We revisit this question and provide an efficient coding scheme with a constant rate for the interesting case of fully connected networks . We furthermore extend the result and show that if a ( d-regular ) network has mixing time m , then there exists an efficient coding scheme with rate O ( 1/m ( 3 ) log m ) . This implies a constant rate coding scheme for any n-party protocol over a d-regular network with a constant mixing time , and in particular for random graphs with n vertices and degrees n ( Omega ( 1 ) ) .
2K_dev_1736	Concurrency bugs that stem from schedule-dependent branches are hard to understand and debug , because their root causes imply not only different event orderings , but also changes in the control-flow between failing and non-failing executions . We present Cortex : a system that helps exposing and understanding concurrency bugs that result from schedule-dependent branches , without relying on information from failing executions . Cortex preemptively exposes failing executions by perturbing the order of events and control-flow behavior in non-failing schedules from production runs of a program . By leveraging this information from production runs , Cortex synthesizes executions to guide the search for failing schedules . Production-guided search helps cope with the large execution search space by targeting failing executions that are similar to observed non-failing executions . Evaluation on popular benchmarks shows that Cortex is able to expose failing schedules with only a few perturbations to non-failing executions , and takes a practical amount of time .
2K_dev_1737	We are on the cusp of the emergence of a new wave of nonvolatile memory technologies that are projected to become the dominant type of main memory in the near future . A key property of these new memory technologies is their asymmetric read-write costs : Writes can be an order of magnitude or more higher energy , higher latency , and lower ( per module ) bandwidth than reads . This high cost for writes motivates a rethinking of algorithm design towards `` write efficient { '' } algorithms and data structures that reduce their number of writes { [ } 1 , 2 , 3 , 4 , 5 , 6 ] . Many popular techniques for sequential , distributed , and parallel algorithms are tuned to the setting where reads and writes cost the same , and hence need to be revisited . Prior work on reducing writes to contended cache lines in shared memory algorithms can be useful here , but with the new technologies , even writes to uncontended memory is costly . Moreover , the new technologies are unlikely to replace the fastest cache memory , motivating the study of a multi-level memory hierarchy comprised of smaller symmetric level ( s ) and a larger asymmetric level . Lower bounds , too , need to be revisited in light of asymmetric costs . This talk provides background on these emerging memory technologies , highlights the progress to date on these exciting research questions , and touches on a few of the many open problems .
2K_dev_1738	We present a faster distributed broadcasting primitive for the classical radio network model . The most basic distributed radio network broadcasting primitive - called Decay - dates back to a PODC'87 result of Bar-Yehuda , Goldreich , and Itai . In any radio network with some informed source nodes , running Decay for O ( d log n + log ( 2 ) n ) rounds informs all nodes at most d hops away from a source with high probability . Since 1987 this primitive has been the most important building block for implementing many other functionalities in radio networks . The only improvements to this decades-old algorithm are slight variations due to { [ } Czumaj , Rytter ; FOCS'03 ] and { [ } Kowalski and Pelc , PODC'03 ] which achieve the same functionality in O ( d log n/d + log ( 2 ) n ) rounds . To obtain a speedup from this , d and thus also the network diameter need to be near linear , i.e. , larger than n ( 1-epsilon ) . Our new distributed primitive spreads messages for d hops in O ( dlog n log log n/log d + log ( O ( 1 ) ) n ) rounds with high probability . log . This improves over Decay for any super-polylogarithmic d 0 log ( omega ( 1 ) ) n and achieves near-optimal 0 ( d log log n ) running time for d 0 n ( epsilon ) . This also makes progress on an open question of Peleg .
2K_dev_1739	The widespread availability of high-quality motion capture data and the maturity of solutions to animate virtual characters has paved the way for the next generation of interactive virtual worlds exhibiting intricate interactions between characters and the environments they inhabit . However , current motion synthesis techniques have not been designed to scale with complex environments and contact-rich motions , requiring environment designers to manually embed motion semantics in the environment geometry in order to address online motion synthesis . This paper presents an automated approach for analyzing both motions and environments in order to represent the different ways in which an environment can afford a character to move . We extract the salient features that characterize the contact-rich motion repertoire of a character and detect valid transitions in the environment where each of these motions may be possible , along with additional semantics that inform which surfaces of the environment the character may use for support during the motion . The precomputed motion semantics can be easily integrated into standard navigation and animation pipelines in order to greatly enhance the motion capabilities of virtual characters . The computational efficiency of our approach enables two additional applications . Environment designers can interactively design new environments and get instant feedback on how characters may potentially interact , which can be used for iterative modeling and refinement . End users can dynamically edit virtual worlds and characters will automatically accommodate the changes in the environment in their movement strategies .
2K_dev_1740	Data compression can be an effective method to achieve higher system performance and energy efficiency in modern data-intensive applications by exploiting redundancy and data similarity . Prior works have studied a variety of data compression techniques to improve both capacity ( e.g. , of caches and main memory ) and bandwidth utilization ( e.g. , of the on-chip and off-chip interconnects ) . In this paper , we make a new observation about the energy-efficiency of communication when compression is applied . While compression reduces the amount of transferred data , it leads to a substantial increase in the number of bit toggles ( i.e. , communication channel switchings from 0 to 1 or from 1 to 0 ) . The increased toggle count increases the dynamic energy consumed by on-chip and off-chip buses due to more frequent charging and discharging of the wires . Our results show that the total bit toggle count can increase from 20\ % to 2.2x when compression is applied for some compression algorithms , averaged across different application suites . We characterize and demonstrate this new problem across 242 GPU applications and six different compression algorithms . To mitigate the problem , we propose two new toggle-aware compression techniques : Energy Control and Metadata Consolidation . These techniques greatly reduce the bit toggle count impact of the data compression algorithms we examine , while keeping most of their bandwidth reduction benefits .
2K_dev_1741	Factorization Machines offer good performance and useful embeddings of data . However , they are costly to scale to large amounts of data and large numbers of features . In this paper we describe DiFacto , which uses a refined Factorization Machine model with sparse memory adaptive constraints and frequency adaptive regularization . We show how to distribute DiFacto over multiple machines using the Parameter Server framework by computing distributed sub gradients on minibatches asynchronously . We analyze its convergence and demonstrate its efficiency in computational advertising datasets with billions examples and features .
2K_dev_1742	Social community detection is a growing field of interest in the area of social network applications , and many approaches have been developed , including graph partitioning , latent space model , block model and spectral clustering . Most existing work purely focuses on network structure information which is , however , often sparse , noisy and lack of interpretability . To improve the accuracy and interpretability of community discovery , we propose to infer users ' social communities by incorporating their spatiotemporal data and semantic information . Technically , we propose a unified probabilistic generative model , User-Community-Geo-Topic ( UCGT ) , to simulate the generative process of communities as a result of network proximities , spatiotemporal co-occurrences and semantic similarity . With a well-designed multi-component model structure and a parallel inference implementation to leverage the power of multicores and clusters , our UCGT model is expressive while remaining efficient and scalable to growing large-scale geo-social networking data . We deploy UCGT to two application scenarios of user behavior predictions : check-in prediction and social interaction prediction . Extensive experiments on two large-scale geo-social networking datasets show that UCGT achieves better performance than existing state-of-the-art comparison methods .
2K_dev_1743	This paper introduces and analyzes the novel task of categorical classification of cuboidal objects - e.g. , distinguishing washing machines versus filing cabinets . To do so , it makes use of recent methods for automatic alignment of cuboidal objects in images . Given such geometric alignments , the natural approach for recognition might extract pose-normalized appearance features from a canonically-aligned coordinate frame . Though such approaches are extraordinarily common , we demonstrate that they are not optimal , both theoretically and empirically . One reason is that such approaches require accurate shape alignment . However , even with ground-truth alignment , posenormalized representations may still be sub-optimal . Instead , we introduce methods based on pose-synthesis , a somewhat simple approach of augmenting training data with geometrically perturbed training samples . We demonstrate , both theoretically and empirically , that synthesis is a surprisingly simple but effective strategy that allows for state-of-the-art categorization and automatic 3D alignment . To aid our empirical analysis , we introduce a novel dataset for cuboidal object categorization .
2K_dev_1744	We present Spire , a shading language and compiler framework that facilitates rapid exploration of shader optimization choices ( such as frequency reduction and algorithmic approximation ) afforded by modern real-time graphics engines . Our design combines ideas from rate-based shader programming with new language features that expand the scope of shader execution beyond traditional GPU hardware pipelines , and enable a diverse set of shader optimizations to be described by a single mechanism : overloading shader terms at various spatio-temporal computation rates provided by the pipeline . In contrast to prior work , neither the shading language 's design , nor our compiler framework 's implementation , is specific to the capabilities of any one rendering pipeline , thus Spire establishes architectural separation between the shading system and the implementation of modern rendering engines ( allowing different rendering pipelines to utilize its services ) . We demonstrate use of Spire to author complex shaders that are portable across different rendering pipelines and to rapidly explore shader optimization decisions that span multiple compute and graphics passes and even offline asset preprocessing . We further demonstrate the utility of Spire by developing a shader level-of-detail library and shader auto-tuning system on top of its abstractions , and demonstrate rapid , automatic re-optimization of shaders for different target hardware platforms .
2K_dev_1745	This paper describes a group interview technique designed to support lightweight process assessments while promoting at the same time collaboration among assessment participants . The method was successfully used in one consulting assignment were it got previously discording participants , talking to each other and agreeing on the issues . The technique borrows from agile software development the concept of user stories to cast CMMI 's specific practices in concrete terms and the Planning Poker technique , instead of document reviews and audit like interviews , for fact finding and corroboration .
2K_dev_1746	This paper proposes the use of lexical similarity across different documents in order to improve a topic segmentation task . Given a set of topically related documents , the segmentation process is carried out using a Bayesian framework . By using similar sentences from different documents more accurate segment likelihood estimations are obtained . The proposed approach was tested in an educational domain where a set of learning materials from different media sources needed to be segmented so that students could browse through them more efficiently . Initial results show that the proposed method does afford better segmentation compared to one of the present state of the art algorithms , a Bayesian baseline approach that segments the documents individually .
2K_dev_1747	A global CP constraint is presented which improves the propagation of reservoir constraints on cumulative resources in schedules with optional tasks . The global constraint is incorporated in a CP approach to solve a Single-Commodity Pickup and Delivery Problem : the Bicycle Rebalancing Problem with Time-Windows and heterogeneous fleet . This problem was recently introduced at the 2015 ACP Summer School on Constraint Programming competition . The resulting CP approach outperforms a Branch-and-Bound approach derived from two closely related problems . In addition , the CP approach presented in this paper resulted in a first place position in the competition .
2K_dev_1748	In order to achieve smooth autonomous driving in real-life urban and highway environments , a motion planner must generate trajectories that are locally smooth and responsive ( reactive ) , and at the same time , far-sighted and intelligent ( deliberative ) . Prior approaches achieved both planning qualities for full-speed-range operations at a high computational cost . Moreover , the planning formulations were mostly a trajectory search problem based on a single weighted cost , which became hard to tune and highly scenario-constrained due to overfitting . In this paper , a pipelined ( phased ) framework with tunable planning modules is proposed for general on-road motion planning to reduce the computational overhead and improve the tunability of the planner .
2K_dev_1749	The topic of kinematics of laser rangefinders has received little attention in the robotics literature , even though such sensors have been the perception sensors of choice on commercial AGVs , field robots , and aerial robots for some time . Unlike the physical mechanisms employed in manipulators and mobile robots , the basic operation for bending the optical path of the transmit and receive beams of a laser rangefinder is often a reflection about a mirror , and this operation can not be modeled naturally as a rotation . In recognition of the uniqueness of optical reflection mechanisms , this paper presents a formulation for modeling 2 axis scanning laser rangefinders based on a matrix reflection operator .
2K_dev_1750	Many applications for robotic systems require the systems to traverse diverse , unstructured environments . State estimation with Visual Odometry ( VO ) in these applications is challenging because there is no single algorithm that performs well across all environments and situations . The unique trade-offs inherent to each algorithm mean different algorithms excel in different environments . We develop a method to increase robustness in state estimation by using an ensemble of VO algorithms . The method combines the estimates by dynamically switching to the best algorithm for the current context , according to a statistical model of VO estimate errors . The model is a Random Forest regressor that is trained to predict the accuracy of each algorithm as a function of different features extracted from the sensory input . We evaluate our method in a dataset of consisting of four unique environments and eight runs , totaling over 25min of data . Our method reduces the mean translational relative pose error by 3.5\ % and the angular error by 4.3\ % compared to the single best odometry algorithm . Compared to the poorest performing odometry algorithm , our method reduces the mean translational error by 39.4\ % and the angular error by 20.1\ % .
2K_dev_1752	This paper presents a method for generating semi-algebraic invariants for systems governed by non-linear polynomial ordinary differential equations under semi-algebraic evolution constraints . Based on the notion of discrete abstraction , our method eliminates unsoundness and unnecessary coarseness found in existing approaches for computing abstractions for non-linear continuous systems and is able to construct invariants with intricate boolean structure , in contrast to invariants typically generated using template-based methods . In order to tackle the state explosion problem associated with discrete abstraction , we present invariant generation algorithms that exploit sound proof rules for safety verification , such as differential cut ( DC ) , and a new proof rule that we call differential divide-and-conquer ( DDC ) , which splits the verification problem into smaller sub-problems . The resulting invariant generation method is observed to be much more scalable and efficient than the na `` ive approach , exhibiting orders of magnitude performance improvement on many of the problems .
2K_dev_1753	Reduced frequency range in vowel production is a well documented speech characteristic of individuals with psychological and neurological disorders . Affective disorders such as depression and post-traumatic stress disorder ( PTSD ) are known to influence motor control and in particular speech production . The assessment and documentation of reduced vowel space and reduced expressivity often either rely on subjective assessments or on analysis of speech under constrained laboratory conditions ( e.g . sustained vowel production , reading tasks ) . These constraints render the analysis of such measures expensive and impractical . Within this work , we investigate an automatic unsupervised machine learning based approach to assess a speaker 's vowel space . Our experiments are based on recordings of 253 individuals . Symptoms of depression and PTSD are assessed using standard self-assessment questionnaires and their cut-off scores . The experiments show a significantly reduced vowel space in subjects that scored positively on the questionnaires . We show the measure 's statistical robustness against varying demographics of individuals and articulation rate . The reduced vowel space for subjects with symptoms of depression can be explained by the common condition of psychomotor retardation influencing articulation and motor control . These findings could potentially support treatment of affective disorders , like depression and PTSD in the future .
2K_dev_1754	Level-of-detail ( LOD ) rendering is a key optimization used by modern video game engines to achieve high-quality rendering with fast performance . These LOD systems require simplified shaders , but generating simplified shaders remains largely a manual optimization task for game developers . Prior efforts to automate this process have taken hours to generate simplified shader candidates , making them impractical for use in modern shader authoring workflows for complex scenes . We present an end-to-end system for automatically generating a LOD policy for an input shader . The system operates on shaders used in both forward and deferred rendering pipelines , requires no additional semantic information beyond input shader source code , and in only seconds to minutes generates LOD policies ( consisting of simplified shader , the desired LOD distance set , and transition generation ) with performance and quality characteristics comparable to custom hand-authored solutions . Our design contributes new shader simplification transforms such as approximate common subexpression elimination and movement of GPU logic to parameter bind-time processing on the CPU , and it uses a greedy search algorithm that employs extensive caching and up-front collection of input shader statistics to rapidly identify simplified shaders with desirable performance-quality trade-offs .
2K_dev_1755	We present a computational tool for designing ornamental curve networks-structurally-sound physical surfaces with user-controlled aesthetics . In contrast to approaches that leverage texture synthesis for creating decorative surface patterns , our method relies on user-defined spline curves as central design primitives . More specifically , we build on the physically-inspired metaphor of an embedded elastic curve that can move on a smooth surface , deform , and connect with other curves . We formalize this idea as a globally coupled energy-minimization problem , discretized with piece-wise linear curves that are optimized in the parametric space of a smooth surface . Building on this technical core , we propose a set of interactive design and editing tools that we demonstrate on manually-created layouts and semi-automated deformable packings . In order to prevent excessive compliance , we furthermore propose a structural analysis tool that uses eigenanalysis to identify potentially large deformations between geodesically-close curves and guide the user in strengthening the corresponding regions . We used our approach to create a variety of designs in simulation , validated with a set of 3D-printed physical prototypes .
2K_dev_1756	We present an interactive tool for designing physical surfaces made from flexible interlocking quadrilateral elements of a single size and shape . With the element shape fixed , the design task becomes one of finding a discrete structure-i.e. , element connectivity and binary orientations-that leads to a desired geometry . In order to address this challenging problem of combinatorial geometry , we propose a forward modeling tool that allows the user to interactively explore the space of feasible designs . Paralleling principles from conventional modeling software , our approach leverages a library of base shapes that can be instantiated , combined , and extended using two fundamental operations : merging and extrusion . In order to assist the user in building the designs , we furthermore propose a method to automatically generate assembly instructions . We demonstrate the versatility of our method by creating a diverse set of digital and physical examples that can serve as personalized lamps or decorative items .
2K_dev_1757	In this work , we present a computational framework for automatically generating kinematic models of planar mechanical linkages from raw images . The hallmark of our approach is a novel combination of supervised learning methods for detecting mechanical parts ( e.g . joints , rigid bodies ) with the optimizing power of a multiobjective evolutionary algorithm , which concurrently maximizes image consistency and mechanical feasibility . A rigorous set of experiments was conducted to systematically evaluate the performance of each phase in our framework , comparing various combinations of joint and body detection schemes and feasibility constraints . Precision-recall curves are used to assess object detection performance . For the optimization , in addition to standard accuracy measures such as top-N accuracy , we introduce a new performance metric called user effort ratio that quantifies the amount of user interaction required to correct an inaccurate optimization solution . Current state-of-the-art performance is achieved with ( i ) one ( or a cascade of ) support vector machines for joint detection , ( ii ) foreground extraction to reduce 0 positives , ( iii ) supervised body detection using normalized geodesic time , distance , and detected joint confidence , and ( iv ) feasibility constraints derived from graph theory . The proposed framework generalizes moderately well from textbook graphics to hand-drawn sketches , and user effort ratio results demonstrate the potential power of an interactive system in which simple user interactions complement computer recognition for fast kinematic modeling . ( C ) 2015 Elsevier Ltd. All rights reserved .
2K_dev_1758	Data races complicate programming language semantics , and a data race is often a bug . Existing techniques detect data races and define their semantics by detecting conflicts between synchronization-free regions ( SFRs ) . However , such techniques either modify hardware or slow programs dramatically , preventing always-on use today . This paper describes Valor , a sound , precise , software-only region conflict detection analysis that achieves high performance by eliminating the costly analysis on each read operation that prior approaches require . Valor instead logs a region 's reads and lazily detects conflicts for logged reads when the region ends . As a comparison , we have also developed FastRCD , a conflict detector that leverages the epoch optimization strategy of the FastTrack data race detector . We evaluate Valor , FastRCD , and FastTrack , showing that Valor dramatically outperforms FastRCD and FastTrack . Valor is the first region conflict detector to provide strong semantic guarantees for racy program executions with under 2X slowdown . Overall , Valor advances the state of the art in always-on support for strong behavioral guarantees for data races .
2K_dev_1759	Much robotics research explores how robots can clearly communicate 1 information . Here , we focus on the counterpart : communicating 0 information , or hiding information altogether-in one word , deception . Robot deception is useful in conveying intentionality , and in making games against the robot more engaging . We study robot deception in goal-directed motion , in which the robot is concealing its actual goal . We present an analysis of deceptive motion , starting with how humans would deceive , moving to a mathematical model that enables the robot to autonomously generate deceptive motion , and ending with a studies on the implications of deceptive motion for human-robot interactions and the effects of iterated deception .
2K_dev_1760	Communication constraints dictated by hardware often require a multi-robot system to make decisions and take actions locally . Unfortunately , local knowledge may impose limits that ultimately impede global optimality in a decentralized optimization problem . This paper enhances a recent anytime optimal assignment method based on a task-swap mechanism , redesigning the algorithm to address task allocation problems in a decentralized fashion . We propose a fully decentralized approach that allows local search processes to execute concurrently while minimizing interactions amongst the processes , needing neither global broadcast nor a multi-hop communication protocol . The formulation is analyzed in a novel way using tools from group theory and optimization duality theory to show that the convergence of local searching processes is related to a shortest path routing problem on a graph subject to the network topology . Simulation results show that this fully decentralized method converges quickly while sacrificing little optimality .
2K_dev_1761	Autonomous landing is an essential function for micro air vehicles ( MAVs ) for many scenarios . We pursue an active perception strategy that enables MAVs with limited onboard sensing and processing capabilities to concurrently assess feasible rooftop landing sites with a vision-based perception system while generating trajectories that balance continued landing site assessment and the requirement to provide visual monitoring of an interest point . The contributions of the work are twofold : ( 1 ) a perception system that employs a dense motion stereo approach that determines the 3D model of the captured scene without the need of geo-referenced images , scene geometry constraints , or external navigation aids ; and ( 2 ) an online trajectory generation approach that balances the need to concurrently explore available rooftop vantages of an interest point while ensuring confidence in the landing site suitability by considering the impact of landing site uncertainty as assessed by the perception system . Simulation and experimental evaluation of the performance of the perception and trajectory generation methodologies are analyzed independently and jointly in order to establish the efficacy and robustness of the proposed approach .
2K_dev_1762	Consider networks in harsh environments , where nodes may be lost due to failure , attack , or infection-how is the topology affected by such events ? Can we mimic and measure the effect ? We propose a new generative model of network evolution in dynamic and harsh environments . Our model can reproduce the range of topologies observed across known robust and fragile biological networks , as well as several additional transport , communication , and social networks . We also develop a new optimization measure to evaluate robustness based on preserving high connectivity following random or adversarial bursty node loss . Using this measure , we evaluate the robustness of several real-world networks and propose a new distributed algorithm to construct secure networks operating within malicious environments .
2K_dev_1763	We present a method for interactive editing of planar linkages . Given a working linkage as input , the user can make targeted edits to the shape or motion of selected parts while preserving other , e.g. , functionally-important aspects . In order to make this process intuitive and efficient , we provide a number of editing tools at different levels of abstraction . For instance , the user can directly change the structure of a linkage by displacing joints , edit the motion of selected points on the linkage , or impose limits on the size of its enclosure . Our method safeguards against degenerate configurations during these edits , thus ensuring the correct functioning of the mechanism at all times . Linkage editing poses strict requirements on performance that standard approaches fail to provide . In order to enable interactive and robust editing , we build on a symbolic kinematics approach that uses closed-form expressions instead of numerical methods to compute the motion of a linkage and its derivatives . We demonstrate our system on a diverse set of examples , illustrating the potential to adapt and personalize the structure and motion of existing linkages . To validate the feasibility of our edited designs , we fabricated two physical prototypes .
2K_dev_1764	Motivation : It remains a challenge to detect associations between genotypes and phenotypes because of insufficient sample sizes and complex underlying mechanisms involved in associations . Fortunately , it is becoming more feasible to obtain gene expression data in addition to genotypes and phenotypes , giving us new opportunities to detect 1 genotype-phenotype associations while unveiling their association mechanisms . Results : In this article , we propose a novel method , NETAM , that accurately detects associations between SNPs and phenotypes , as well as gene traits involved in such associations . We take a network-driven approach : NETAM first constructs an association network , where nodes represent SNPs , gene traits or phenotypes , and edges represent the strength of association between two nodes . NETAM assigns a score to each path from an SNP to a phenotype , and then identifies significant paths based on the scores . In our simulation study , we show that NETAM finds significantly more phenotype-associated SNPs than traditional genotype-phenotype association analysis under 0 positive control , taking advantage of gene expression data . Furthermore , we applied NETAM on late-onset Alzheimer 's disease data and identified 477 significant path associations , among which we analyzed paths related to beta-amyloid , estrogen , and nicotine pathways . We also provide hypothetical biological pathways to explain our findings .
2K_dev_1765	This paper presents a novel solution for realtime generation of stylistic human motion that automatically transforms unlabeled , heterogeneous motion data into new styles . The key idea of our approach is an online learning algorithm that automatically constructs a series of local mixtures of autoregressive models ( MAR ) to capture the complex relationships between styles of motion . We construct local MAR models on the fly by searching for the closest examples of each input pose in the database . Once the model parameters are estimated from the training data , the model adapts the current pose with simple linear transformations . In addition , we introduce an efficient local regression model to predict the timings of synthesized poses in the output style . We demonstrate the power of our approach by transferring stylistic human motion for a wide variety of actions , including walking , running , punching , kicking , jumping and transitions between those behaviors . Our method achieves superior performance in a comparison against alternative methods . We have also performed experiments to evaluate the generalization ability of our data-driven model as well as the key components of our system .
2K_dev_1766	We present Symbiosis : a concurrency debugging technique based on novel differential schedule projections ( DSPs ) . A DSP shows the small set of memory operations and data-flows responsible for a failure , as well as a reordering of those elements that avoids the failure . To build a DSP , Symbiosis first generates a full , failing , multithreaded schedule via thread path profiling and symbolic constraint solving . Symbiosis selectively reorders events in the failing schedule to produce a non-failing , alternate schedule . A DSP reports the ordering and data-flow differences between the failing and non-failing schedules . Our evaluation on buggy real-world software and benchmarks shows that , in practical time , Symbiosis generates DSPs that both isolate the small fraction of event orders and data-flows responsible for the failure , and show which event reorderings prevent failing . In our experiments , DSPs contain 81\ % fewer events and 96\ % fewer data-flows than the full failure-inducing schedules . Moreover , by allowing developers to focus on only a few events , DSPs reduce the amount of time required to find a valid fix .
2K_dev_1767	Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval . Existing methods index a video by the raw concept detection score that is dense and inconsistent , and thus can not scale to `` big data { '' } that are readily available on the Internet . This paper proposes a scalable solution . The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index . The proposed adjustment model relies on a concise optimization framework with interpretations . The proposed index leverages the text-based inverted index for video retrieval . Experimental results validate the efficacy and the efficiency of the proposed method . The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance . Specifically , the proposed method ( with reranking ) achieves the best result on the challenging TRECVID Multimedia Event Detection ( MED ) zero-example task . It only takes 0.2 second on a single CPU core to search a collection of 100 million Internet videos .
2K_dev_1768	Multimedia event detection ( MED ) and multimedia event recounting ( MER ) are fundamental tasks in managing large amounts of unconstrained web videos , and have attracted a lot of attention in recent years . Most existing systems perform MER as a post-processing step on top of the MED results . In order to leverage the mutual benefits of the two tasks , we propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events . Our premise is that a good recounting algorithm should not only explain the detection result , but should also be able to assist detection in the first place . Coupled in a joint optimization framework , recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences . To better utilize the powerful and interpretable semantic video representation , we segment each video into several shots and exploit the rich temporal structures at shot level . The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm , which , after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps , enables us to efficiently process extremely large video corpora . We test the proposed method on the large scale TRECVID MEDTest 2014 and MEDTest 2013 datasets , and obtain very promising results for both MED and MER .
2K_dev_1769	In this paper , we explore an approach to generating detectors that is radically different from the conventional way of learning a detector from a large corpus of annotated positive and negative data samples . Instead , we assume that we have evaluated `` off-line { '' } a large library of detectors against a large set of detection tasks . Given a new target task , we evaluate a subset of the models on few samples from the new task and we use the matrix of models tasks ratings to predict the performance of all the models in the library on the new task , enabling us to select a good set of detectors for the new task . This approach has three key advantages of great interest in practice : 1 ) generating a large collection of expressive models in an unsupervised manner is possible ; 2 ) a far smaller set of annotated samples is needed compared to that required for training from scratch ; and 3 ) recommending models is a very fast operation compared to the notoriously expensive training procedures of modern detectors . ( 1 ) will make the models informative across different categories ; ( 2 ) will dramatically reduce the need for manually annotating vast datasets for training detectors ; and ( 3 ) will enable rapid generation of new detectors .
2K_dev_1770	In this paper we explore the bi-directional mapping between images and their sentence-based descriptions . Critical to our approach is a recurrent neural network that attempts to dynamically build a visual representation of the scene as a caption is being generated or read . The representation automatically learns to remember long-term visual concepts . Our model is capable of both generating novel captions given an image , and reconstructing visual features given an image description . We evaluate our approach on several tasks . These include sentence generation , sentence retrieval and image retrieval . State-of-the-art results are shown for the task of generating novel image descriptions . When compared to human generated captions , our automatically generated captions are equal to or preferred by humans 21.0\ % of the time . Results are better than or comparable to state-of-the-art results on the image and sentence retrieval tasks for methods using similar visual features .
2K_dev_1771	In this paper , we focus on complex event detection in internet videos while also providing the key evidences of the detection results . Convolutional Neural Networks ( CNNs ) have achieved promising performance in image classification and action recognition tasks . However , it remains an open problem how to use CNNs for video event detection and recounting , mainly due to the complexity and diversity of video events . In this work , we propose a flexible deep CNN infrastructure , namely Deep Event Network ( DevNet ) , that simultaneously detects pre-defined events and provides key spatial-temporal evidences . Taking key frames of videos as input , we first detect the event of interest at the video level by aggregating the CNN features of the key frames . The pieces of evidences which recount the detection results , are also automatically localized , both temporally and spatially . The challenge is that we only have video level labels , while the key evidences usually take place at the frame levels . Based on the intrinsic property of CNNs , we first generate a spatial-temporal saliency map by back passing through DevNet , which then can be used to find the key frames which are most indicative to the event , as well as to localize the specific spatial position , usually an object , in the frame of the highly indicative area . Experiments on the large scale TRECVID 2014 MEDTest dataset demonstrate the promising performance of our method , both for event detection and evidence recounting .
2K_dev_1772	In this paper , we propose a novel algorithm that infers the 3D layout of building facades from a single 2D image of an urban scene . Different from existing methods that only yield coarse orientation labels or qualitative block approximations , our algorithm quantitatively reconstructs building facades in 3D space using a set of planes mutually related by 3D geometric constraints . Each plane is characterized by a continuous orientation vector and a depth distribution . An optimal solution is reached through inter-planar interactions . Due to the quantitative and plane-based nature of our geometric reasoning , our model is more expressive and informative than existing approaches . Experiments show that our method compares competitively with the state of the art on both 2D and 3D measures , while yielding a richer interpretation of the 3D scene behind the image .
2K_dev_1773	We propose an approach that utilize large collections of photo streams and blog posts , two of the most prevalent sources of data on the Web , for joint story-based summarization and exploration . Blogs consist of sequences of images and associated text : they portray events and experiences with concise sentences and representative images . We leverage Hogs to help achieve story-based semantic summarization of collections of photo streams . In the opposite direction , blog posts can be enhanced with sets of photo streams by showing interpolations between consecutive images in the blogs . We formulate the problem of joint alignment from blogs to photo streams and photo stream summarization in a unified latent ranking SVM framework . We alternate between solving the two coupled latent SVM problems , by first fixing the summarization and solving for the alignment from blog images to photo streams and vice versa . On a newly collected large-scale Disneyland dataset of 10K blogs ( 120K associated images ) and OK photo streams ( 540K images ) , we demonstrate that blog posts and photo streams are mutually beneficial for summarization , exploration , semantic knowledge transfer , and photo interpolation .
2K_dev_1774	We present a semi-supervised approach that localizes multiple unknown object instances in long videos . We start with a handful of labeled boxes and iteratively learn and label hundreds of thousands of object instances . We propose criteria for reliable object detection and tracking for constraining the semi-supervised learning process and minimizing semantic drift . Our approach does not assume exhaustive labeling of each object instance in any single frame , or any explicit annotation of negative data . Working in such a generic setting allow us to tackle multiple object instances in video , many of which are static . In contrast , existing approaches either do not consider multiple object instances per video , or rely heavily on the motion of the objects present . The experiments demonstrate the effectiveness of our approach by evaluating the automatically labeled data on a variety of metrics like quality , coverage ( recall ) , diversity , and relevance to training an object detector .
2K_dev_1775	Contact behaviors in physics simulations are important for real-time interactive applications , especially in virtual reality applications where user 's body parts are tracked and interact with the environment via contact . For these contact simulations , it is ideal to have small changes in initial condition yield predictable changes in the output . Predictable simulation is key for success in iterative learning processes as well , such as learning controllers for manipulations or locomotion tasks . Here , we present an extensive comparison of contact simulations using Bullet Physics , Dynamic Animation and Robotics Toolkit ( DART ) , MuJoCo , and Open Dynamics Engine , with a focus on predictability of behavior . We first tune each engine to match an analytical solution as closely as possible and then compare the results for a more complex simulation . We found that in the commonly available physics engines , small changes in initial condition can sometimes induce different sequences of contact events to occur and ultimately lead to a vastly different result . Our results confirmed that parameter settings do matter a great deal and suggest that there may be a trade-off between accuracy and predictability . Copyright ( C ) 2016 John Wiley \ & Sons , Ltd .
2K_dev_1776	We present a co-clustering framework that can be used to discover multiple semantic and visual senses of a given Noun Phrase ( NP ) . Unlike traditional clustering approaches which assume a one-to-one mapping between the clusters in the text-based feature space and the visual space , we adopt a one-to-many mapping between the two spaces . This is primarily because each semantic sense ( concept ) can correspond to different visual senses due to viewpoint and appearance variations . Our structure-EM style optimization not only extracts the multiple senses in both semantic and visual feature space , but also discovers the mapping between the senses . We introduce a challenging dataset ( CMU Polysemy-30 ) for this problem consisting of 30 NPs ( similar to 5600 labeled instances out of similar to 22K total instances ) . We have also conducted a large-scale experiment that performs sense disambiguation for similar to 2000 NPs .
2K_dev_1777	We envision a future time when wearable cameras ( e.g. , small cameras in glasses or pinned on a shirt collar ) are worn by the masses and record first-person point-of-view ( POV ) videos of everyday life . While these cameras can enable new assistive technologies and novel research challenges , they also raise serious privacy concerns . For example , first-person videos passively recorded by wearable cameras will necessarily include anyone who comes into the view of a camera - with or without consent . Motivated by these benefits and risks , we develop a self-search technique tailored to first-person POV videos . The key observation of our work is that the egocentric head motions of a target person ( i.e. , the self ) are observed both in the POV video of the target and observer . The motion correlation between the target person 's video and the observer 's video can then be used to uniquely identify instances of the self . We incorporate this feature into our proposed approach that computes the motion correlation over supervoxel hierarchies to localize target instances in observer videos . Our proposed approach significantly improves self-search performance over several well-known face detectors and recognizers . Furthermore , we show how our approach can enable several practical applications such as privacy filtering , automated video collection and social group discovery .
2K_dev_1778	Semantic search in video is a novel and challenging problem in information and multimedia retrieval . Existing solutions are mainly limited to text matching , in which the query words are matched against the textual metadata generated by users . This paper presents a state-of-the-art system for event search without any textual metadata or example videos . The system relies on substantial video content understanding and allows for semantic search over a large collection of videos . The novelty and practicality is demonstrated by the evaluation in NIST TRECVID 2014 , where the proposed system achieves the best performance . We share our observations and lessons in building such a state-of-the-art system , which may be instrumental in guiding the design of the future system for semantic search in video .
2K_dev_1779	Many recent works propose mechanisms demonstrating the potential advantages of managing memory at a fine ( e.g. , cache line ) granularity-e.g. , fine-grained deduplication and fine-grained memory protection . Unfortunately , existing virtual memory systems track memory at a larger granularity ( e.g. , 4 KB pages ) , inhibiting efficient implementation of such techniques . Simply reducing the page size results in an unacceptable increase in page table overhead and TLB pressure . We propose a new virtual memory framework that enables efficient implementation of a variety of fine-grained memory management techniques . In our framework , each virtual page can be mapped to a structure called a page overlay , in addition to a regular physical page . An overlay contains a subset of cache lines from the virtual page . Cache lines that are present in the overlay are accessed from there and all other cache lines are accessed from the regular physical page . Our page-overlay framework enables cache-line-granularity memory management without significantly altering the existing virtual memory framework or introducing high overheads . We show that our framework can enable simple and efficient implementations of seven memory management techniques , each of which has a wide variety of applications . We quantitatively evaluate the potential benefits of two of these techniques : overlay-on-write and sparse-data-structure computation . Our evaluations show that overlay-on-write , when applied to fork , can improve performance by 15\ % and reduce memory capacity requirements by 53\ % on average compared to traditional copy-on-write . For sparse data computation , our framework can outperform a state-of-the-art software-based sparse representation on a number of real-world sparse matrices . Our framework is general , powerful , and effective in enabling fine-grained memory management at low cost .
2K_dev_1780	Distributed in-memory key-value stores ( KVSs ) , such as memcached , have become a critical data serving layer in modern Internet-oriented datacenter infrastructure . Their performance and efficiency directly affect the QoS of web services and the efficiency of datacenters . Traditionally , these systems have had significant overheads from inefficient network processing , OS kernel involvement , and concurrency control . Two recent research thrusts have focused upon improving key-value performance . Hardware-centric research has started to explore specialized platforms including FPGA5 for KVSs ; results demonstrated an order of magnitude increase in throughput and energy efficiency over stock memcached . Software-centric research revisited the KVS application to address fundamental software bottlenecks and to exploit the full potential of modern commodity hardware ; these efforts too showed orders of magnitude improvement over stock memcached . We aim at architecting high performance and efficient KVS platforms , and start with a rigorous architectural characterization across system stacks over a collection of representative KVS implementations . Our detailed full-system characterization not only identifies the critical hardware/software ingredients for high-petformance KVS systems , but also leads to guided optimizations atop a recent design to achieve a record setting throughput of 120 million requests per second ( MRPS ) on a single commodity server . Our implementation delivers 9.2X the performance ( RPS ) and 2.8X the system energy efficiency ( RPS/watt ) of the best-published FPGA-based claims . We craft a set of design principles for future platform architectures , and via detailed simulations demonstrate the capability of achieving a billion RPS with a single server constructed following our principles .
2K_dev_1781	Recovering the motion of a non-rigid body from a set of monocular images permits the analysis of dynamic scenes in uncontrolled environments . However , the extension of factorisation algorithms for rigid structure from motion to the low-rank non-rigid case has proved challenging . This stems from the comparatively hard problem of finding a linear `` corrective transform { '' } which recovers the projection and structure matrices from an ambiguous factorisation . We elucidate that this greater difficulty is due to the need to find multiple solutions to a non-trivial problem , casting a number of previous approaches as alleviating this issue by either a ) introducing constraints on the basis , making the problems nonidentical , or b ) incorporating heuristics to encourage a diverse set of solutions , making the problems inter-dependent . While it has previously been recognised that finding a single solution to this problem is sufficient to estimate cameras , we show that it is possible to bootstrap this partial solution to find the complete transform in closed-form . However , we acknowledge that our method minimises an algebraic error and is thus inherently sensitive to deviation from the low-rank model . We compare our closed-form solution for non-rigid structure with known cameras to the closed-form solution of Dai et al . { [ } 1 ] , which we find to produce only coplanar reconstructions . We therefore make the recommendation that 3D reconstruction error always be measured relative to a trivial reconstruction such as a planar one .
2K_dev_1782	We address the task of estimating large-scale land surface conditions using overhead aerial ( macro-level ) images and street view ( micro-level ) images . These two types of images are captured from orthogonal viewpoints and have different resolutions , thus conveying very different types of information that can be used in a complementary way . Moreover , their integration is necessary to enable an accurate understanding of changes in natural phenomena over massive city-scale landscapes . The key technical challenge is devising a method to integrate these two disparate types of image data in an effective manner , to leverage the wide coverage capabilities of macro-level images and detailed resolution of micro-level images . The strategy proposed in this work uses macro-level imaging to learn the extent to which the land condition corresponds between land regions that share similar visual characteristics ( e.g. , mountains , streets , buildings , rivers ) , whereas micro-level images are used to acquire high resolution statistics of land conditions ( e.g. , the amount of debris on the ground ) . By combining macro- and micro-level information about regional correspondences and surface conditions , our proposed method is capable of generating detailed estimates of land surface conditions over an entire city . ( C ) 2016 The Authors . Published by Elsevier Inc . This is an open access article under the CC BY-NC-ND license ( http : //creativecommons.org/licenses/by-nc-nd/4.0/ ) .
2K_dev_1783	In this paper we tackle the problem of efficient video event detection . We argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation . A popular approach in this regard is to represent a sequence using a bag of words ( BOW ) representation due to its : ( i ) fixed dimensionality irrespective of the sequence length , and ( ii ) its ability to compactly model the statistics in the sequence . A drawback to the BOW representation , however , is the intrinsic destruction of the temporal ordering information . In this paper we propose a new representation that leverages the uncertainty in relative temporal alignments between pairs of sequences while not destroying temporal ordering . Our representation , like BOW , is of a fixed dimensionality making it easily integrated with a linear detection function . Extensive experiments on CK+ , 6DMG , and UvA-NEMO databases show significant performance improvements across both isolated and continuous event detection tasks .
2K_dev_1784	Occupancy count in rooms is valuable for applications such as room utilization , opportunistic meeting support , and efficient heating-cooling operations . Few buildings , however , have the means of knowing occupancy beyond simple binary presence-absence . In this paper we present the PerCCS algorithm that explores the possibility of estimating person count from CO2 sensors already integrated in everyday room airconditioning infrastructure . PerCSS uses task-driven Sparse Non-negative Matrix Factorization ( SNMF ) to learn a nonnegative low-dimensional representation of the CO2 data in the preprocessing stage . This denoised CO2 acts as the predictor variable for estimating occupancy count using Ensemble Least Square Regression . We tested the algorithm to estimate 15 minutes average occupancy count from a classroom of capacity 42 and compared its performance against existing methods from the literature . PerCSS estimates occupancy with a normalized mean squared error ( NMSE ) of 0.075 and outperformed our comparative methods in predicting occupancy count with 91 \ % and 15 \ % for exact occupancy estimation , when the room was unoccupied and occupied respectively , whereas the competing methods failed mostly .
2K_dev_1785	Understanding the purpose of why sensitive data is used could help improve privacy as well as enable new kinds of access control . In this paper , we introduce a new technique for inferring the purpose of sensitive data usage in the context of Android smartphone apps . We extract multiple kinds of features from decompiled code , focusing on app-specific features and text-based features . These features are then used to train a machine learning classifier . We have evaluated our approach in the context of two sensitive permissions , namely ACCESS FINE LOCATION and READ CONTACT LIST , and achieved an accuracy of about 85\ % and 94\ % respectively in inferring purposes . We have also found that text-based features alone are highly effective in inferring purposes .
2K_dev_1786	Some languages have very consistent mappings between graphemes and phonemes , while in other languages , this mapping is more ambiguous . Consonantal writing systems prove to be a challenge for Text to Speech Systems ( TTS ) because they do not indicate short vowels , which creates an ambiguity in pronunciation . Special letter-to-sound rules may be needed for some cases in languages that otherwise have a good correspondence between graphemes and phonemes . In the low-resource scenario , we may not have linguistic resources such as diacritizers or hand-written rules for the language . We propose a technique to automatically learn pronunciations iteratively from acoustics during ITS training and predict pronunciations from text during synthesis time . We conduct experiments on dialects of Arabic for disambiguating homographs and Hindi for discovering the schwa-deletion rules . We evaluate our systems using objective and subjective metrics of TTS and show significant improvements for dialects of Arabic . Our methods can be generalized to other languages that exhibit similar phenomena .
2K_dev_1787	Distant speech recognition ( DSR ) remains to be an open challenge , even for the state-of-the-art deep neural network ( DNN ) models . Previous work has attempted to improve DNNs under constantly distant speech . However , in real applications , the speaker-microphone distance ( SMD ) can be quite dynamic , varying even within a single utterance . This paper investigates how to alleviate the impact of dynamic SMD on DNN models . Our solution is to incorporate the frame-level SMD information into DNN training . Generation of the SMD information relies on a universal extractor that is learned on a meeting corpus . We study the utility of different architectures in instantiating the SMD extractor . On our target acoustic modeling task , two approaches are proposed to build distance-aware DNN models using the SMD information : simple concatenation and distance adaptive training ( DAT ) . Our experiments show that in the simplest case , incorporating the SMD descriptors improves word error rates of DNNs by 5.6\ % relative . Further optimizing SMD extraction and integration results in more gains .
2K_dev_1788	Spoken Term Detection ( STD ) or Keyword Search ( KWS ) techniques can locate keyword instances but do not differentiate between meanings . Spoken Word Sense Induction ( SWSI ) differentiates target instances by clustering according to context , providing a more useful result . In this paper we present a fully unsupervised SWSI approach based on distributed representations of spoken utterances . We compare this approach to several others , including the state-of-the-art Hierarchical Dirichlet Process ( HDP ) . To determine how ASR performance affects SWSI , we used three different levels of Word Error Rate ( WER ) , 40\ % , 20\ % and 0\ % ; 40\ % WER is representative of online video , 0\ % of text . We show that the distributed representation approach outperforms all other approaches , regardless of the WER . Although LDA-based approaches do well on clean data , they degrade significantly with WER . Paradoxically , lower WER does not guarantee better SWSI performance , due to the influence of common locutions .
2K_dev_1789	Ensuring language coverage in dialog systems can be a challenge , since the language in a domain may drift over time , creating a mismatch between the original training data and current input . This in turn degrades performance by increasing misunderstanding and eventually leading to task failure . Without the capability of adapting the vocabulary and the language model based on certain domains or users , recognition errors may degrade the understanding performance , and even lead to a task failure , which incurs more time and effort to recover . This paper investigates how coverage can be maintained by automatically acquiring potential out-of-vocabulary ( OOV ) words by leveraging different types of relatedness between vocabulary items and words retrieved from web-based resources . Our experiments show that both recognition and semantic parsing accuracy can thereby be improved .
2K_dev_1790	We study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems , in particular , learning the hierarchical semantic structure from the data . Given unlabelled conversations , we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots ( e.g. , both slots `` direction { '' } and `` locale { '' } convey location-related information ) for building a coherent semantic hierarchy , and then estimate the slot importance at different levels . The high-level semantic estimation involves not only within-slot but also cross slot relations . The experiments show that high-level semantic information can accurately estimate the prominence of slots , significantly improving the slot induction performance ; furthermore , a semantic decoder trained on the data with automatically extracted slots achieves about 68\ % F-measure , which is close to the one from hand-crafted grammars .
2K_dev_1791	Self-adaptive systems overcome many of the limitations of human supervision in complex software-intensive systems by endowing them with the ability to automatically adapt their structure and behavior in the presence of runtime changes . However , adaptation in some classes of systems ( e.g. , safety-critical ) can benefit by receiving information from humans ( e.g. , acting as sophisticated sensors , decision-makers ) , or by involving them as system-level effectors to execute adaptations ( e.g. , when automation is not possible , or as a fallback mechanism ) . However , human participants are influenced by factors external to the system ( e.g. , training level , fatigue ) that affect the likelihood of success when they perform a task , its duration , or even if they are willing to perform it in the first place . Without careful consideration of these factors , it is unclear how to decide when to involve humans in adaptation , and in which way . In this paper , we investigate how the explicit modeling of human participants can provide a better insight into the trade-offs of involving humans in adaptation . We contribute a formal framework to reason about human involvement in self-adaptation , focusing on the role of human participants as actors ( i.e. , effectors ) during the execution stage of adaptation . The approach consists of : ( i ) a language to express adaptation models that capture factors affecting human behavior and its interactions with the system , and ( ii ) a formalization of these adaptation models as stochastic multiplayer games ( SMGs ) that can be used to analyze human-system-environment interactions . We illustrate our approach in an adaptive industrial middleware used to monitor and manage sensor networks in renewable energy production plants .
2K_dev_1792	Future-generation self-adaptive systems will need to be able to optimize for multiple interrelated , difficult-to-measure , and evolving quality properties . To navigate this complex search space , current self-adaptive planning techniques need to be improved . In this position paper , we argue that the research community should more directly pursue the application of stochastic search techniques-search techniques , such as hill climbing or genetic algorithms , that incorporate an element of randomness-to self-adaptive systems research . These techniques are well-suited to handling multi-dimensional search spaces and complex problems , situations which arise often for self-adaptive systems . We believe that recent advances in both fields make this a particularly promising research trajectory . We demonstrate one way to apply some of these advances in a search-based planning prototype technique to illustrate both the feasibility and the potential of the proposed research . This strategy informs a number of potentially interesting research directions and problems . In the long term , this general technique could enable sophisticated plan generation techniques that improve domain specific knowledge , decrease human effort , and increase the application of self-adaptive systems .
2K_dev_1793	We present a computer-aided programming approach to concurrency . The approach allows programmers to program assuming a friendly , non-preemptive scheduler , and our synthesis procedure inserts synchronization to ensure that the final program works even with a preemptive scheduler . The correctness specification is implicit , inferred from the non-preemptive behavior . Let us consider sequences of calls that the program makes to an external interface . The specification requires that any such sequence produced under a preemptive scheduler should be included in the set of sequences produced under a non-preemptive scheduler . We guarantee that our synthesis does not introduce deadlocks and that the synchronization inserted is optimal w.r.t . a given objective function . The solution is based on a finitary abstraction , an algorithm for bounded language inclusion modulo an independence relation , and generation of a set of global constraints over synchronization placements . Each model of the global constraints set corresponds to a correctness-ensuring synchronization placement . The placement that is optimal w.r.t . the given objective function is chosen as the synchronization solution . We apply the approach to device-driver programming , where the driver threads call the software interface of the device and the API provided by the operating system . Our experiments demonstrate that our synthesis method is precise and efficient . The implicit specification helped us find one concurrency bug previously missed when model-checking using an explicit , user-provided specification . We implemented objective functions for coarse-grained and fine-grained locking and observed that different synchronization placements are produced for our experiments , favoring a minimal number of synchronization operations or maximum concurrency , respectively .
2K_dev_1794	Self-adaptive systems tend to be reactive and myopic , adapting in response to changes without anticipating what the subsequent adaptation needs will be . Adapting reactively can result in inefficiencies due to the system performing a suboptimal sequence of adaptations . Furthermore , when adaptations have latency , and take some time to produce their effect , they have to be started with sufficient lead time so that they complete by the time their effect is needed . Proactive latency-aware adaptation addresses these issues by making adaptation decisions with a look-ahead horizon and taking adaptation latency into account . In this paper we present an approach for proactive latency-aware adaptation under uncertainty that uses probabilistic model checking for adaptation decisions . The key idea is to use a formal model of the adaptive system in which the adaptation decision is left underspecified through nondeterminism , and have the model checker resolve the nondeterministic choices so that the accumulated utility over the horizon is maximized . The adaptation decision is optimal over the horizon , and takes into account the inherent uncertainty of the environment predictions needed for looking ahead . Our results show that the decision based on a look-ahead horizon , and the factoring of both tactic latency and environment uncertainty , considerably improve the effectiveness of adaptation decisions .
2K_dev_1795	Almost every complex software system today is configurable . While configurability has many benefits , it challenges performance prediction , optimization , and debugging . Often , the influences of individual configuration options on performance are unknown . Worse , configuration options may interact , giving rise to a configuration space of possibly exponential size . Addressing this challenge , we propose an approach that derives a performance-influence model for a given configurable system , describing all relevant influences of configuration options and their interactions . Our approach combines machine-learning and sampling heuristics in a novel way . It improves over standard techniques in that it ( 1 ) represents influences of options and their interactions explicitly ( which eases debugging ) , ( 2 ) smoothly integrates binary and numeric configuration options for the first time , ( 3 ) incorporates domain knowledge , if available ( which eases learning and increases accuracy ) , ( 4 ) considers complex constraints among options , and ( 5 ) systematically reduces the solution space to a tractable size . A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them .
2K_dev_1796	Column subset selection ( CSS ) is the problem of selecting a small portion of columns from a large data matrix as one form of interpretable data summarization . Leverage score sampling , which enjoys both sound theoretical guarantee and superior empirical performance , is widely recognized as the state-of-the-art algorithm for column subset selection . In this paper , we revisit iterative norm sampling , another sampling based CSS algorithm proposed even before leverage score sampling , and demonstrate its competitive performance under a wide range of experimental settings . We also compare iterative norm sampling with several of its other competitors and show its superior performance in terms of both approximation accuracy and computational efficiency . We conclude that further theoretical investigation and practical consideration should be devoted to iterative norm sampling in column subset selection .
2K_dev_1797	We present Aggregate G-Buffer Anti-Aliasing ( AGAA ) , a new technique for efficient anti-aliased deferred rendering of complex geometry using modern graphics hardware . In geometrically complex situations , where many surfaces intersect a pixel , current rendering systems shade each contributing surface at least once per pixel . As the sample density and geometric complexity increase , the shading cost becomes prohibitive for real-time rendering . Under deferred shading , so does the required framebuffer memory . AGAA uses the rasterization pipeline to generate a compact , pre-filtered geometric representation inside each pixel . We then shade this at a fixed rate , independent of geometric complexity . By decoupling shading rate from geometric sampling rate , the algorithm reduces the storage and bandwidth costs of a geometry buffer , and allows scaling to high visibility sampling rates for anti-aliasing . AGAA with 2 aggregate surfaces per-pixel generates results comparable to 8x MSAA , but requires 30\ % less memory ( 45\ % savings for 16x MSAA ) , and is up to 1.3x faster .
2K_dev_1799	Although substantial progress has been achieved in speech-to-speech translation systems over the last few years , such systems still require that the speech be written in some appropriate orthography . As speech may differ greatly from the standardized written form of a language , it can be non-trivial to collect written data when there is no standard way for it to be represented . This project addresses the problem from the other end and expects that speech alone is available in the target language , and that no ( standard or nonstandard ) orthography exists . It , therefore , treats the acoustic representation of the language as primary and uses language-independent methods to produce a phonetically-related symbolic representation that is then used in the translation system . Thus , the speech translation system is created for the target language as defined by the recording of that language rather than some body of orthographic transcripts . In this work , we are creating an application called APT ( Acoustic Patient Translator ) , which uses a novel scheme of speech recognition and translation within a targeted domain . By working with a set of predefined sentences appropriately chosen to fit a scenario , we use utterance classification as a speech recognition algorithm . The utterance classification is achieved using cross-lingual , language-independent phonetic labeling . Since we are working with a set of select phrases , the translation part is trivial . We are concentrating on communication with hospital staff , such as scheduling a doctor 's appointment , as our domain . In addition to English , we also run experiments on Tamil .
2K_dev_1800	We introduce a new method to grade non-native spoken language tests automatically . Traditional automated response grading approaches use manually engineered time-aggregated features ( such as mean length of pauses ) . We propose to incorporate general time-sequence features ( such as pitch ) which preserve more information than time-aggregated features and do not require human effort to design . We use a type of recurrent neural network to jointly optimize the learning of high level abstractions from time-sequence features with the time-aggregated features . We first automatically learn high level abstractions from time-sequence features with a Bidirectional Long Short Term Memory ( BLSTM ) and then combine the high level abstractions with time-aggregated features in a Multilayer Perceptron ( MLP ) /Linear Regression ( LR ) . We optimize the BLSTM and the MLP/LR jointly . We find such models reach the best performance in terms of correlation with human raters . We also find that when there are limited time-aggregated features available , our model that incorporates time-sequence features improves performance drastically .
2K_dev_1801	Programmers often need to revert some code to an earlier state , or restore a block of code that was deleted a while ago . However , support for this backtracking in modern programming environments is limited . Many of the backtracking tasks can be accomplished by having a selective undo feature in code editors , but this has major challenges : there can be conflicts among edit operations , and it is difficult to provide usable interfaces for selective undo . In this paper , we present AZURITE , an Eclipse plug-in that allows programmers to selectively undo fine-grained code changes made in the code editor . With AZURITE , programmers can easily perform backtracking tasks , even when the desired code is not in the undo stack or a version control system . AZURITE also provides novel user interfaces specifically designed for selective undo , which were iteratively improved through user feedback gathered from actual users in a preliminary field trial . A formal lab study showed that programmers can successfully use AZURITE , and were twice as fast as when limited to conventional features .
2K_dev_1803	Vehicular networks are inherently unstable networks with high mobility and intermittent connectivity . These networks can greatly benefit from Delay Tolerant Networking ( DTN ) solutions for opportunistic connectivity in the transmission of delaytolerant data . In this paper , we evaluate the performance of different DTN routing protocols in real world vehicular networks with different degrees of connectivity in order to understand their feasibility in real vehicular environments . Our case-study application is the upload of delay-tolerant sensing data from vehicular nodes to a server on the Internet . We deploy DTN in 3 vehicular testbeds : a heterogeneous lab testbed with controlled mobility , a low-mobility high-connectivity network of harbor trucks , and a high-mobility low-connectivity city bus network . We compare 3 routing protocols : epidemic , static and PRoPHET , for which we measure delivery ratio , average delay , path length , as well as transmission and storage overhead . We analyze the results for experimental insight , and extract lessons for DTN routing protocol design .
2K_dev_1804	Syntax extension mechanisms are powerful , but reasoning about syntax extensions can be difficult . Recent work on type-specific languages ( TSLs ) addressed reasoning about composition , hygiene and typing for extensions introducing new literal forms . We supplement TSLs with typed syntax macros ( TSMs ) , which , unlike TSLs , are explicitly invoked to give meaning to delimited segments of arbitrary syntax . To maintain a typing discipline , we describe two flavors of term-level TSMs : synthetic TSMs specify the type of term that they generate , while analytic TSMs can generate terms of arbitrary type , but can only be used in positions where the type is otherwise known . At the level of types , we describe a third flavor of TSM that generates a type of a specified kind along with its TSL and show interesting use cases where the two mechanisms operate in concert .
2K_dev_1805	Multimodal analysis has long been an integral part of studying learning . Historically multimodal analyses of learning have been extremely laborious and time intensive . However , researchers have recently been exploring ways to use multimodal computational analysis in the service of studying how people learn in complex learning environments . In an effort to advance this research agenda , we present a comparative analysis of four different data segmentation techniques . In particular , we propose affect-and pose-based data segmentation , as alternatives to human-based segmentation , and fixed-window segmentation . In a study of ten dyads working on an open-ended engineering design task , we find that affect-and pose-based segmentation are more effective , than traditional approaches , for drawing correlations between learning-relevant constructs , and multimodal behaviors . We also find that pose-based segmentation outperforms the two more traditional segmentation strategies for predicting student success on the hands-on task . In this paper we discuss the algorithms used , our results , and the implications that this work may have in non-education-related contexts .
2K_dev_1807	Given multiple perspective photographs , point correspondences form the `` joint image { '' } , effectively a replica of three-dimensional space distributed across its two-dimensional projections . This set can be characterized by multilinear equations over image coordinates , such as epipolar and trifocal constraints . We revisit in this paper the geometric and algebraic properties of the joint image , and address fundamental questions such as how many and which multilinearities are necessary and/or sufficient to determine camera geometry and/or image correspondences . The new theoretical results in this paper answer these questions in a very general setting and , in turn , are intended to serve as a `` handbook { '' } reference about multilinearities for practitioners .
2K_dev_1808	Do we really need 3D labels in order to learn how to predict 3D ? In this paper , we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label . Rather than use explicit supervision , we use the regularity of indoor scenes to learn the mapping in a completely unsupervised manner . We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D is unavailable , precluding supervised learning . Despite never seeing a 3D label , our method produces competitive results .
2K_dev_1809	We explore multi-scale convolutional neural nets ( CNNs ) for image classification . Contemporary approaches extract features from a single output layer . By extracting features from multiple layers , one can simultaneously reason about high , mid , and low-level features during classification . The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph ( DAG-CNNs ) . We use DAG-CNNs to learn a set of multi-scale features that can be effectively shared between coarse and fine-grained classification tasks . While finetuning such models helps performance , we show that even `` off-the-self { '' } multi-scale features perform quite well . We present extensive analysis and demonstrate state-of-the-art classification performance on three standard scene benchmarks ( SUN397 , MIT67 , and Scene15 ) . In terms of the heavily benchmarked MIT67 and Scene15 datasets , our results reduce the lowest previously-reported error by 23.9\ % and 9.5\ % , respectively .
2K_dev_1810	This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation . Given only a large , unlabeled image collection , we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first . We argue that doing well on this task requires the model to learn to recognize objects and their parts . We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images . For example , this representation allows us to perform unsupervised visual discovery of objects like cats , people , and even birds from the Pascal VOC 2011 detection dataset . Furthermore , we show that the learned ConvNet can be used in the R-CNN framework { [ } 19 ] and provides a significant boost over a randomly-initialized ConvNet , resulting in state-of-the-art performance among algorithms which use only Pascal provided training set annotations .
2K_dev_1811	Mobile applications frequently access sensitive personal information to meet user or business requirements . Because such information is sensitive in general , regulators increasingly require mobile-app developers to publish privacy policies that describe what information is collected . Furthermore , regulators have fined companies when these policies are inconsistent with the actual data practices of mobile apps . To help mobile-app developers check their privacy policies against their apps ' code for consistency , we propose a semi-automated framework that consists of a policy terminology-API method map that links policy phrases to API methods that produce sensitive information , and information flow analysis to detect misalignments . We present an implementation of our framework based on a privacy-policy-phrase ontology and a collection of mappings from API methods to policy phrases . Our empirical evaluation on 477 top Android apps discovered 341 potential privacy policy violations .
2K_dev_1812	We present an approach to utilize large amounts of web data for learning CNNs . Specifically inspired by curriculum learning , we present a two-step approach for CNN training . First , we use easy images to train an initial visual representation . We then use this initial CNN and adapt it to harder , more realistic images by leveraging the structure of data and categories . We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012 . We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style { [ } 19 ] detector . It achieves the best performance on VOC 2007 where no VOC training data is used . Finally , we show our approach is quite robust to noise and performs comparably even when we use image search results from March 2013 ( pre-CNN image search era ) .
2K_dev_1813	While feedforward deep convolutional neural networks ( CNNs ) have been a great success in computer vision , it is important to note that the human visual cortex generally contains more feedback than feedforward connections . In this paper , we will briefly introduce the background of feedbacks in the human visual cortex , which motivates us to develop a computational feedback mechanism in deep neural networks . In addition to the feedforward inference in traditional neural networks , a feedback loop is introduced to infer the activation status of hidden layer neurons according to the `` goal { '' } of the network , e.g. , high-level semantic labels . We analogize this mechanism as `` Look and Think Twice . { '' } The feedback networks help better visualize and understand how deep neural networks work , and capture visual attention on expected objects , even in images with cluttered background and multiple objects . Experiments on ImageNet dataset demonstrate its effectiveness in solving tasks such as image classification and object localization .
2K_dev_1814	We present an approach to capture the 3D structure and motion of a group of people engaged in a social interaction . The core challenges in capturing social interactions are : ( 1 ) occlusion is functional and frequent ; ( 2 ) subtle motion needs to be measured over a space large enough to host a social group ; and ( 3 ) human appearance and configuration variation is immense . The Panoptic Studio is a system organized around the thesis that social interactions should be measured through the perceptual integration of a large variety of view points . We present a modularized system designed around this principle , consisting of integrated structural , hardware , and software innovations . The system takes , as input , 480 synchronized video streams of multiple people engaged in social activities , and produces , as output , the labeled time-varying 3D structure of anatomical landmarks on individuals in the space . The algorithmic contributions include a hierarchical approach for generating skeletal trajectory proposals , and an optimization framework for skeletal reconstruction with trajectory reassociation .
2K_dev_1815	Varied sources of error contribute to the challenge of facial action unit detection . Previous approaches address specific and known sources . However , many sources are unknown . To address the ubiquity of error , we propose a Confident Preserving Machine ( CPM ) that follows an easy-to-hard classification strategy . During training , CPM learns two confident classifiers . A confident positive classifier separates easily identified positive samples from all else ; a confident negative classifier does same for negative samples . During testing , CPM then learns a person-specific classifier using `` virtual labels { '' } provided by confident classifiers . This step is achieved using a quasi-semi-supervised ( QSS ) approach . Hard samples are typically close to the decision boundary , and the QSS approach disambiguates them using spatio-temporal constraints . To evaluate CPM , we compared it with a baseline single-margin classifier and state-of-the-art semi-supervised learning , transfer learning , and boosting methods in three datasets of spontaneous facial behavior . With few exceptions , CPM outperformed baseline and state-of-the art methods .
2K_dev_1816	Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms . Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent , semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ . Motivated by object recognition literature and recent work on rapidly estimating linear classifiers , we treat semantic correspondence as a constrained detection problem , where an exemplar LDA classifier is learned for each pixel . LDA classifiers have two distinct benefits : ( i ) they exhibit higher average precision than similarity metrics typically used in correspondence problems , and ( ii ) unlike exemplar SVM , can output globally interpretable posterior probabilities without calibration , whilst also being significantly faster to train . We pose the correspondence problem as a graphical model , where the unary potentials are computed via convolution with the set of exemplar classifiers , and the joint potentials enforce smoothly varying correspondence assignment .
2K_dev_1817	Starting with the seminal work by Kempe et al. , a broad variety of problems , such as targeted marketing and the spread of viruses and malware , have been modeled as selecting a subset of nodes to maximize diffusion through a network . In cyber-security applications , however , a key consideration largely ignored in this literature is stealth . In particular , an attacker often has a specific target in mind , but succeeds only if the target is reached ( e.g. , by malware ) before the malicious payload is detected and corresponding countermeasures deployed . The dual side of this problem is deployment of a limited number of monitoring units , such as cyber-forensics specialists , so as to limit the likelihood of such targeted and stealthy diffusion processes reaching their intended targets . We investigate the problem of optimal monitoring of targeted stealthy diffusion processes , and show that a number of natural variants of this problem are NP-hard to approximate . On the positive side , we show that if stealthy diffusion starts from randomly selected nodes , the defender 's objective is submodular , and a fast greedy algorithm has provable approximation guarantees . In addition , we present approximation algorithms for the setting in which an attacker optimally responds to the placement of monitoring nodes by adaptively selecting the starting nodes for the diffusion process . Our experimental results show that the proposed algorithms are highly effective and scalable .
2K_dev_1818	Poor spelling is a challenge faced by people with dyslexia throughout their lives . Spellcheckers are therefore a crucial tool for people with dyslexia , but current spellcheckers do not detect real-word errors , which are a common type of errors made by people with dyslexia . Real-word errors are spelling mistakes that result in an unintended but real word , for instance , form instead of from . Nearly 20\ % of the errors that people with dyslexia make are real-word errors . In this paper , we introduce a system called Real Check that uses a probabilistic language model , a statistical dependency parser and Google n-grams to detect real-world errors . We evaluated Real Check on text written by people with dyslexia , and showed that it detects more of these errors than widely used spellcheckers . In an experiment with 34 people ( 17 with dyslexia ) , people with dyslexia corrected sentences more accurately and in less time with Real Check .
2K_dev_1819	Today , many data-driven web pages present information in a way that is difficult for blind and low vision users to navigate and to understand . EnTable addresses this challenge . It re-writes confusing and complicated template-based data sets as accessible tables . EnTable allows blind and low vision users to submit requests for pages they wish to access . The system then employs sighted informants to markup the desired page with semantic information , allowing the page to be re-written using straightforward < table > tags . Screen reader users who browse the web using the EnTable browser extension can report data sets that are confusing , and utilize data sets re-written with the < table > tag based on their own requests or on the requests of other users .
2K_dev_1820	Large scale deployment of sensors is essential to practical applications in cyber physical systems . For instance , instrumenting a commercial building for `smart energy ' management requires deployment and operation of thousands of measurement and metering sensors and actuators that direct operation of the HVAC system . Each of these sensors need to be named consistently and constantly calibrated . Doing this process manually is not only time consuming but also error prone given the scale , heterogeneity and complexity of buildings as well as lack of uniform naming schemas . To address this challenge , we propose Zodiac-a framework for automatically classifying , naming and managing sensors based on active learning from sensor metadata . In contrast to prior work , Zodiac requires minimal user input in terms of labelling examples while being more accurate . To evaluate Zodiac , we deploy it across four real buildings on our campus and label the ground truth metadata for all the sensors in these buildings manually . Using a combination of hierarchical clustering and random forest classifiers we show that Zodiac can successfully classify sensors with an average accuracy of 98\ % with 28\ % fewer training examples when compared to a regular expression based method .
2K_dev_1821	Modern cyber-physical systems interact closely with continuous physical processes like kinematic movement . Software component frameworks do not provide an explicit way to represent or reason about these processes . Meanwhile , hybrid program models have been successful in proving critical properties of discrete-continuous systems . These programs deal with diverse aspects of a cyber-physical system such as controller decisions , component communication protocols , and mechanical dynamics , requiring several programs to address the variation . However , currently these aspects are often intertwined in mostly monolithic hybrid programs , which are difficult to understand , change , and organize . These issues can be addressed by component-based engineering , making hybrid modeling more practical . This paper lays the foundation for using architectural models to provide component-based benefits to developing hybrid programs . We build formal architectural abstractions of hybrid programs and formulas , enabling analysis of hybrid programs at the component level , reusing parts of hybrid programs , and automatic transformation from views into hybrid programs and formulas . Our approach is evaluated in the context of a robotic collision avoidance case study .
2K_dev_1822	Dependencies among software projects and libraries are an indicator of the often implicit collaboration among many developers in software ecosystems . Negotiating change can be tricky : changes to one module may cause ripple effects to many other modules that depend on it , yet insisting on only backward-compatible changes may incur significant opportunity cost and stifle change . We argue that awareness mechanisms based on various notions of stability can enable developers to make decisions that are independent yet wise and provide stewardship rather than disruption to the ecosystem . In ongoing interviews with developers in two software ecosystems ( CRAN and Node.js ) , we are finding that developers in fact struggle with change , that they often use adhoc mechanisms to negotiate change , and that existing awareness mechanisms like Github notification feeds are rarely used due to information overload . We study the state of the art and current information needs and outline a vision toward a change-based awareness system .
2K_dev_1824	We introduce a set of new Compression-Aware Management Policies ( CAMP ) for on-chip caches that employ data compression . Our management policies are based
2K_dev_1825	Mobile and web applications increasingly leverage service-oriented architectures in which developers integrate third-party services into end user applications . This includes identity management , mapping and navigation , cloud storage , and advertising services , among others . While service reuse reduces development time , it introduces new privacy and security risks due to data repurposing and over-collection as data is shared among multiple parties who lack transparency into third-party data practices . To address this challenge , we propose new techniques based on Description Logic ( DL ) for modeling multiparty data flow requirements and verifying the purpose specification and collection and use limitation principles , which are prominent privacy properties found in international standards and guidelines . We evaluate our techniques in an empirical case study that examines the data practices of the Waze mobile application and three of their service providers : Facebook Login , Amazon Web Services ( a cloud storage provider ) , and Flurry.com ( a popular mobile analytics and advertising platform ) . The study results include detected conflicts and violations of the principles as well as two patterns for balancing privacy and data use flexibility in requirements specifications . Analysis of automation reasoning over the DL models show that reasoning over complex compositions of multi-party systems is feasible within exponential asymptotic timeframes proportional to the policy size , the number of expressed data , and orthogonal to the number of conflicts found .
2K_dev_1826	In software development , IDE services such as syntax highlighting , code completion , and `` jump to declaration { '' } are used to assist developers in programming tasks . In dynamic web applications , however , since the client-side code is dynamically generated from the server-side code and is embedded in the server-side program as string literals , providing IDE services for such embedded code is challenging . In this work , we introduce Varis , a tool that provides editor services on the client-side code of a PHP-based web application , while it is still embedded within server-side code . Technically , we first perform symbolic execution on a PHP program to approximate all possible variations of the generated client-side code and subsequently parse this client code into a VarDOM that compactly represents all its variations . Finally , using the VarDOM , we implement various types of IDE services for embedded client code including syntax highlighting , code completion , and `` jump to declaration { '' } . The video demonstration for Varis is available at http : //www.youtube.com/watch ? v=w1TECeRXGrg .
2K_dev_1827	To ensure quality and trustworthiness of mobile apps , Google Play store imposes various developer policies . Once an app is reported for exhibiting policy-violating behaviors , it is removed from the store to protect users . Currently , Google Play store relies on mobile users ' feedbacks to identify policy violations . Our paper takes the first step towards understanding these policy-violating apps . First , we crawl 302 Android apps , which are reported in the Reddit forum by mobile users for policy violations and are later removed from the Google Play store . Second , we perform empirical analysis , which reveals that many violating behaviors have not been studied well by industry or research communities . We discover that 53\ % of the reported apps are either copying popular apps or violating copy-rights or trademarks of brands . Moreover , 49\ % of reported apps are violating ads policies by sending push notifications , adding homescreen icon and changing browser settings . Only 8\ % show malware-like behaviors , such as downloading malicious files to users ' mobile phones . Based on our empirical analysis results , we extract 175 features for differentiating bad apps from benign apps . Our features cover use of brand names and other keywords , third-party libraries , network activities , meta data , permissions , and suspicious API calls originated from third-party libraries . We then apply 10 machine learning classifiers on the extracted features to detect reported bad apps . Our experiment result shows that the best algorithm can detect them with 86.80\ % 1 positive rate and 13.6\ % 0 positive rate . On the other hand , the same samples of policy violating apps are detected by VirusTotal with 1 positive rate of 55.63\ % and 0 positive rate of 17.48\ % .
2K_dev_1828	We propose a robust object tracking algorithm for distance keeping . Taking advantage of a context-based region of interest , we are able to maximize the performance of each sensor , and reduce the computation time since we only focus on the targets inside the region . Tracking targets in road coordinates enables finding the distance-keeping target on any curved road , while a commercial Adaptive Cruise Control ( ACC ) system works best on straight roads . We demonstrate that the overall performance of the proposed algorithm is better than that of a commercial ACC system . The distance-keeping target can either be used for lane following for a standalone ACC system or an autonomous vehicle . Our object tracking algorithm can also be extended to find the target of interest for lane changing or ramp merging for an autonomous vehicle .
2K_dev_1829	Suppose you are a teacher , and have to convey a set of object-property pairs ( 'lions eat meat ' ; or `aspirin is a blood-thinner ' ) . A good teacher will convey a lot of information , with little effort on the student side . Specifically , given a list of objects ( like animals or medical drugs ) and their associated properties , what is the best and most intuitive way to convey this information to the student , without the student being overwhelmed ? A related , harder problem is : how can we assign a numerical score to each lesson plan ( i.e . way of conveying information ) ? Here , we give a formal definition of this problem of forming learning units and we provide a metric for comparing different approaches based on information theory . We also design a multi-pronged algorithm , HYTRA , for this problem . Our proposed HYTRA is scalable ( near-linear in the dataset size ) ; it is effective , achieving excellent results on real data , both with respect to our proposed metric , but also with respect to encoding length ; and it is intuitive , conforming to well-known educational principles , such as grouping related concepts , and `` comparing { '' } and `` contrasting { '' } . Experiments on real and synthetic datasets demonstrate the effectiveness of HYTRA .
2K_dev_1830	In this paper , we investigate the issue of evaluating efficiently a large set of models on an input image in detection and classification tasks . We show that by formulating the visual task as a large matrix multiplication problem , something that is possible for a broad set of modern detectors and classifiers , we are able to dramatically reduce the rate of growth of computation as the number of models increases . The approach , based on a bilinear separation model , combines standard matrix factorization with a task-dependent term which ensures that the resulting smaller size problem maintains performance on the original task . Experiments show that we are able to maintain , or even exceed , the level of performance compared to the default approach of using all the models directly , in both detection and classification tasks . This approach is complementary to other efforts in the literature on speeding up computation through GPU implementation , fast matrix operations , or quantization , in that any of these optimizations can be incorporated .
2K_dev_1831	The rapid growth of cloud storage systems calls for fast and scalable namespace processing . While few commercial file systems offer anything better than federating individually non- scalable namespace servers , a recent academic file system , IndexFS , demonstrates scalable namespace processing based on client caching of directory entries and permissions ( directory lookup state ) with no per-client state in servers . In this paper we explore explicit replication of directory lookup state in all servers as an alternative to caching this information in all clients . Both eliminate most repeated RPCs to different servers in order to resolve hierarchical permission tests . Our realization for server replicated directory lookup state , ShardFS , employs a novel file system specific hybrid optimistic and pessimistic concurrency control favoring single object transactions over distributed transactions . Our experimentation suggests that if directory lookup state mutation is a fixed fraction of operations ( strong scaling for metadata ) , server replication does not scale as well as client caching , but if directory lookup state mutation is proportional to the number of jobs , not the number of processes per job , ( weak scaling for metadata ) , then server replication can scale more linearly than client caching and provide lower 70 percentile response times as well .
2K_dev_1832	Humans play an active role in the execution of certain kinds of programs , such as spreadsheets , workflows and interactive notebooks . Interacting closely with execution is especially useful when end-users are learning from examples while doing their work . In order to better understand the language features needed to support this kind of use , we investigated a particularly rigid and formalized category of `` program { '' } people write for each other : lab protocols . These protocols present a linear , idealized process despite the complex contingencies of the lab work they describe . However , they employ a variety of techniques for limiting or expanding the semantic interpretation of individual steps and for integrating outside protocols . We use these observations to derive implications for the design of interactive and mixed-initiative programming languages .
2K_dev_1833	Requirements analysts can model regulated data practices to identify and reason about risks of noncompliance . If terminology is inconsistent or ambiguous , however , these models and their conclusions will be unreliable . To study this problem , we investigated an approach to automatically construct an information type ontology by identifying information type hyponymy in privacy policies using Tregex patterns . Tregex is a utility to match regular expressions against constituency parse trees , which are hierarchical expressions of natural language clauses , including noun and verb phrases . We discovered the Tregex patterns by applying content analysis to 15 privacy policies from three domains ( shopping , telecommunication and social networks ) to identify all instances of information type hyponymy . From this dataset , three semantic and four syntactic categories of hyponymy emerged based on category completeness and word-order . Among these , we identified and empirically evaluated 26 Tregex patterns to automate the extraction of hyponyms from privacy policies . The patterns identify information type hypernym-hyponym pairs with an average precision of 0.83 and recall of 0.52 across our dataset of 15 policies .
2K_dev_1834	Cilk Plus and OpenMP are parallel language extensions for the C and C++ programming languages . The CPLEX Study Group of the ISO/IEC C Standards Committee is developing a proposal for a parallel programming extension to C that combines ideas from Cilk Plus and OpenMP . We conducted a preliminary comparison of Cilk Plus and OpenMP in a master 's level course on security to evaluate the design tradeoffs in the usability and security of these two approaches . The eventual goal is to inform decision-making within the committee . We found several usability problems worthy of further investigation based on student performance , including declaring and using reductions , multi-line compiler directives , and the understandability of task assignment to threads .
2K_dev_1835	The widespread presence of motion sensors on users ' personal mobile devices has spawned a growing research interest in human activity recognition ( HAR ) . However , when deployed at a large-scale , e.g. , on multiple devices , the performance of a HAR system is often significantly lower than in reported research results . This is due to variations in training and test device hardware and their operating system characteristics among others . In this paper , we systematically investigate sensor- , device-and workload-specific heterogeneities using 3 6 smart-phones and smartwatches , consisting of 1 3 different device models from four manufacturers . Furthermore , we conduct experiments with nine users and investigate popular feature representation and classification techniques in HAR research . Our results indicate that on-device sensor and sensor handling heterogeneities impair HAR performances significantly . Moreover , the impairments vary significantly across devices and depends on the type of recognition technique used . We systematically evaluate the effect of mobile sensing heterogeneities on HAR and propose a novel clustering-based mitigation technique suitable for large-scale deployment of HAR , where heterogeneity of devices and their usage scenarios are intrinsic .
2K_dev_1836	To enable real-time , person-independent 3D registration from 2D video , we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60 degrees . From a single 2D image of a person 's face , a dense 3D shape is registered in real time for each frame . The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression . The algorithm first estimates the location of a dense set of markers and their visibility , then reconstructs face shapes by fitting a part-based 3D model . Because no assumptions are required about illumination or surface properties , the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video . The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction and extension to multi-view reconstruction . Experimental findings strongly support the validity of real-time , 3D registration and reconstruction from 2D video . The software is available online at http : //zface.org .
2K_dev_1837	Cultural events are kinds of typical events closely related to history and nationality , which play an important role in cultural heritage through generations . However , automatically recognizing cultural events still remains a great challenge since it depends on understanding of complex image contents such as people , objects , and scene context . Therefore , it is intuitive to associate this task with other high-level vision problems , e.g. , object detection , recognition , and scene understanding . In this paper , we address this problem by combining both ideas of object / scene contents mining and strong image representation via CNN into a whole framework . Specifically , for object / scene contents mining , we employ selective search to extract a batch of bottom-up region proposals , which are served as key object / scene candidates in each event image ; while for representation via CNN , we investigate two state-of-the-art deep architectures , VGGNet and GoogLeNet , and adapt them to our task by performing domain-specific ( i.e. , event ) fine-tuning on both global image and hierarchical region proposals . These two models can complementarily exploit feature hierarchies spatially , which simultaneously capture the global context and local evidences within the image . In our final submission for ChaLearn LAP Challenge ICCV 2015 , nine kinds of features extracted from five different deep models were exploited and followed with two kinds of classifiers for decision level fusion . Our method achieves the best performance of mAP 0 0.854 among all the participants in the track of cultural event recognition .
2K_dev_1838	Regulatory definitions establish the scope and boundary for legal statements and provide software designers with means to assess the coverage of their designs under the law . However , the number of phrases that serve to define this boundary in a legal statement are usually large and often a simple legal statement contains or is affected by up to 10 definition-related phrases . In addition , software designers may need to design their software to operate under multiple jurisdictions , which may not use the same terminology to express conditions . Thus , it is necessary for designers to keep track of definitions in one or more regulations and to compare these definitions across jurisdictions . In this paper we report a study to develop a method to analyze and compare natural language definitions across legal texts and how to analyze the legal statements with respect to definitions . Our method helps reduce the number of comparison between definitions across multiple jurisdictions as well as allows software designers keep track of several inter-related definitions in a systematic way .
2K_dev_1839	Automated program repair ( APR ) is a challenging process of detecting bugs , localizing buggy code , generating fix candidates and validating the fixes . Effectiveness of program repair methods relies on the generated fix candidates , and the methods used to traverse the space of generated candidates to search for the best ones . Existing approaches generate fix candidates based on either syntactic searches over source code or semantic analysis of specification , e.g. , test cases . In this paper , we propose to combine both syntactic and semantic fix candidates to enhance the search space of APR , and provide a function to effectively traverse the search space . We present an automated repair method based on structured specifications , deductive verification and genetic programming . Given a function with its specification , we utilize a modular verifier to detect bugs and localize both program statements and sub-formulas in the specification that relate to those bugs . While the former are identified as buggy code , the latter are transformed as semantic fix candidates . We additionally generate syntactic fix candidates via various mutation operators . Best candidates , which receives fewer warnings via a static verification , are selected for evolution though genetic programming until we find one satisfying the specification . Another interesting feature of our proposed approach is that we efficiently ensure the soundness of repaired code through modular ( or compositional ) verification . We implemented our proposal and tested it on C programs taken from the SIR benchmark that are seeded with bugs , achieving promising results .
2K_dev_1840	In our experiment , two autonomously moving costumed robots visit 256 offices during a `reverse ' trick-or-treating task close to Halloween . Our behavioral data supports the idea that people interpret a robot 's non-verbal cues , as the robots ' costuming and baskets of candy seem to have communicated an implicit offer of candy . In fact , one third of our detection instances occurred during robot transit , i.e. , while the robots were making no verbal offer . We find that candy accessibility dominates any social influence of robot orientation and that robot speed influences both whether people will interrupt a robot in transit ( slow more interruptible ) and whether they will respond to its verbal offer ( fast more salient ) .
2K_dev_1841	The advent of multi-core systems set off a race to get concurrent programming to the masses . One of the challenging aspects of this type of system is how to deal with exceptional situations , since it is very difficult to assert the precise state of a concurrent program when an exception arises . In this paper we propose an exception-handling model for concurrent systems . Its main quality attributes are simplicity and expressiveness , allowing programmers to deal with exceptional situations in a concurrent setting in a familiar way . The proposal is centered on a new kind of exception type that defines new paths for exception propagation among concurrent threads of execution . In our model , beyond being able to control where exceptions are raised , the developer can define in which thread , and when during its execution , a particular exception will be handled . The proposed model has been implemented in Scala , and we show its application to the construction of concurrent software .
2K_dev_1843	Interface-confinement is a common mechanism that secures untrusted code by executing it inside a sandbox . The sandbox limits ( confines ) the code 's interaction with key system resources to a restricted set of interfaces . This practice is seen in web browsers , hypervisors , and other security-critical systems . Motivated by these systems , we present a program logic , called System M , for modeling and proving safety properties of systems that execute adversary-supplied code via interface-confinement . In addition to using computation types to specify effects of computations , System M includes a novel invariant type to specify the properties of interface-confined code . The interpretation of invariant type includes terms whose effects satisfy an invariant . We construct a step-indexed model built over traces and prove the soundness of System M relative to the model . System M is the first program logic that allows proofs of safety for programs that execute adversary-supplied code without forcing the adversarial code to be available for deep static analysis . System M can be used to model and verify protocols as well as system designs . We demonstrate the reasoning principles of System M by verifying the state integrity property of the design of Memoir , a previously proposed trusted computing system .
2K_dev_1844	Information flow analysis has largely focused on methods that require access to the program in question or total control over an analyzed system . We consider the case where the analyst has neither control over nor a white-box model of the analyzed system . We formalize such limited information flow analyses and study an instance of it : detecting the usage of data by websites . We reduce these problems to ones of causal inference by proving a connection between noninterference and causation . Leveraging this connection , we provide a systematic black-box methodology based on experimental science and statistical analysis . Our systematic study leads to practical advice for detecting web data usage , a previously unformalized area . We illustrate these concepts with a series of experiments collecting data on the use of information by websites .
2K_dev_1845	In today 's ubiquitous computing environment where the number of devices , applications and web services are ever increasing , human attention is the new bottleneck in computing . To minimize user cognitive load , we propose Attelia , a novel middleware that identifies breakpoints in user interaction and delivers notifications at these moments . Attelia works in real-time and uses only the mobile devices that users naturally use and wear , without any modifications to applications , and without any dedicated psycho-physiological sensors . Our evaluation proved the effectiveness of Attelia . A controlled user study showed that notifications at detected breakpoint timing resulted in 46\ % lower cognitive load compared to randomly-timed notifications . Furthermore , our `` in-the-wild { '' } user study with 30 participants for 16 days further validated Attelia 's value , with a 33\ % decrease in cognitive load compared to randomly-timed notifications .
2K_dev_1846	Applications , such as construction monitoring and planning for renovations , require the accurate recovery of existing conditions of structures . Many types of infrastructure are primarily comprised of arbitrarily-shaped thin structures ( e.g. , truss bridges , steel frame buildings under construction , and transmission towers ) , which existing automatic modeling methods are incapable of handling . To address this issue , this paper presents an approach to automatically recognize and model beams , planes , and joints from a 3D point cloud containing a complex network of thin structures , and to recover their topology . In our approach , each beam is evolved from a seed by matching and aligning the cross section images . This growing algorithm can model beams with arbitrary cross sections . By performing the algorithm on a point connectivity graph , we distinguish beams from joints and improve the algorithm 's robustness to closely spaced objects . In parallel , planes and joints are also extracted and modeled . The connectivity graph of these primitives allows for a compact , object-level understanding of the entire structure . We demonstrate the capability and robustness of our approach on both synthetic and real datasets .
2K_dev_1847	Many learning-based computer vision algorithms perform poorly when faced with examples that are dissimilar to those on which they were trained . Domain adaptation methods attempt to address this problem , but usually assume that the source domain is specified a priori . We propose a two-step approach for situations where more than one source domain is available . The first step uses a small number of labeled examples to choose the source domain most similar to the target domain , while the second step uses traditional domain adaptation methods to further adapt the chosen source domain to the target data . We demonstrate this two-step domain adaptation algorithm in the context of style-independent building component recognition , which suffers from the problem of inter-domain performance degradation . In this case , different building styles represent the domains , and the task is to reverse engineer a new building of unknown style . We evaluate several variants of the two-step method , and experiments show that the proposed approach outperforms existing single-step methods on a dataset of nine building styles . We demonstrate the generality of the approach on a large , multi-domain dataset with 22 product review categories ( i.e. , styles ) from the natural language processing field .
2K_dev_1848	Registration of Point Cloud Data ( PCD ) forms a core component of many 3D vision algorithms such as object matching and environment reconstruction . In this paper , we introduce a PCD registration algorithm that utilizes Gaussian Mixture Models ( GMM ) and a novel dual-mode parameter optimization technique which we call mixture decoupling . We show how this decoupling technique facilitates both faster and more robust registration by first optimizing over the mixture parameters ( decoupling the mixture weights , means , and covariances from the points ) before optimizing over the 6DOF registration parameters . Furthermore , we frame both the decoupling and registration process inside a unified , dual-mode Expectation Maximization ( EM ) framework , for which we derive a Maximum Likelihood Estimation ( MLE ) solution along with a parallel implementation on the GPU . We evaluate our MLE-based mixture decoupling ( MLMD ) registration method over both synthetic and real data , showing better convergence for a wider range of initial conditions and higher speeds than previous state of the art methods .
2K_dev_1849	This invited talk provides a brief exposition how hybrid systems proving works , why hybrid systems verification is an important device to ensure the safety of complex systems , and gives an idea where that technology is successful .
2K_dev_1850	Formal verification of industrial systems is very challenging , due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams . More importantly , industrial systems are rarely designed for verification , but rather for operational needs . In this paper we present an overview of our experience using hybrid systems theorem proving to formally verify ACAS X , an airborne collision avoidance system for airliners scheduled to be operational around 2020 . The methods and proof techniques presented here are an overview of the work already presented in { [ } 8 ] , while the evaluation of ACAS X has been significantly expanded and updated to the most recent version of the system , run 13 . The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team .
2K_dev_1851	Human swarm interaction ( HSI ) involves operators gathering information about a swarm 's state as it evolves , and using it to make informed decisions on how to influence the collective behavior of the swarm . In order to determine the proper input , an operator must have an accurate representation and understanding of the current swarm state , including what emergent behavior is currently happening . In this paper , we investigate how human operators perceive three types of common , emergent swarm behaviors : rendezvous , flocking , and dispersion . Particularly , we investigate how recognition of these behaviors differ from each other in the presence of background noise . Our results show that , while participants were good at recognizing all behaviors , there are indeed differences between the three , with rendezvous being easier to recognize than flocking or dispersion . Furthermore , differences in recognition are also affected by viewing time for flocking . Feedback from participants was also especially insightful for understanding how participants went about recognizing behaviors-allowing for potential avenues of research in future studies .
2K_dev_1852	We study the fundamental k-nearest neighbor ( kNN ) search problem on distributed time series . A server has constantly received various reference time series Q of length X and seeks the exact kNN over a collection of time series distributed across a set of M local sites . When X and M are large , and when the amount of query increases , simply sending each Q to all M sites incurs high communication bandwidth costs , which we would like to avoid . Prior work has presented a communication-efficient kNN algorithm for the Euclidean distance similarity measure . In this paper , we present the first communication-efficient kNN algorithm for the dynamic time warping ( DTW ) similarity measure , which is generally believed a better measure for time series . To handle the complexities of DTW , we design a new multi-resolution structure for the reference time series , and multi-resolution lower bounds that can effectively prune the search space . We present a new protocol between the server and the local sites that leverages multi-resolution pruning for communication efficiency and cascading lower bounds for computational efficiency . Empirical studies on both real-world and synthetic data sets show that our method reduces communication bandwidth by up to 92\ % .
2K_dev_1853	A multi-faceted graph defines several facets on a set of nodes . Each facet is a set of edges that represent the relationships between the nodes in a specific context . Mining multi-faceted graphs have several applications , including finding fraudster rings that launch advertising traffic fraud attacks , tracking IP addresses of botnets over time , analyzing interactions on social networks and co-authorship of scientific papers . We propose NeSim , a distributed efficient clustering algorithm that does soft clustering on individual facets . We also propose optimizations to further improve the scalability , the efficiency and the clusters quality . We employ generalpurpose graph-clustering algorithms in a novel way to discover communities across facets . Due to the qualities of NeSim , we employ it as a backbone in the distributed MuFace algorithm , which discovers multi-faceted communities . We evaluate the proposed algorithms on several real and synthetic datasets , where NeSim is shown to be superior to MCL , JP and AP , the well-established clustering algorithms . We also report the success stories of MuFace in finding advertisement click rings .
2K_dev_1854	Dataflow analysis-based dynamic parallel monitoring ( DADPM ) is a recent approach for identifying bugs in parallel software as it executes , based on the key insight of explicitly modeling a sliding window of uncertainty across parallel threads . While this makes the approach practical and scalable , it also introduces the possibility of 0 positives in the analysis . In this paper , we improve upon the DADPM framework through two observations . First , by explicitly tracking new `` uncertain { '' } states in the metadata lattice , we can distinguish potential 0 positives from 1 positives . Second , as the analysis tool runs dynamically , it can use the existence ( or absence ) of observed uncertain states to adjust the tradeoff between precision and performance on-the-fly . For example , we demonstrate how the epoch size parameter can be adjusted dynamically in response to uncertainty in order to achieve better performance and precision than when the tool is statically configured . This paper shows how to adapt a canonical dataflow analysis problem ( reaching definitions ) and a popular security monitoring tool ( TAINTCHECK ) to our new uncertainty-tracking framework , and provides new provable guarantees that reported 1 errors are now precise .
2K_dev_1855	Retinal vein cannulation is a demanding procedure proposed to treat retinal vein occlusion by direct therapeutic agent delivery methods . Challenges in identifying the moment of venous puncture , achieving cannulation and maintaining cannulation during drug delivery currently limit the feasibility of the procedure . In this study , we respond to these problems with an assistive system combining a handheld micromanipulator , Micron , with a force-sensing microneedle . The integrated system senses the instant of vein puncture based on measured forces and the position of the needle tip . The system actively holds the cannulation device securely in the vein following cannulation and during drug delivery . Preliminary testing of the system in a dry phantom , stretched vinyl membranes , demonstrates a significant improvement in the total time the needle could be maintained stably inside of the vein . This was especially evident in smaller veins and is attributed to decreased movement of the positioned cannula following venous cannulation .
2K_dev_1856	Public speaking has become an integral part of many professions and is central to career building opportunities . Yet , public speaking anxiety is often referred to as the most common fear in everyday life and can hinder one 's ability to speak in public severely . While virtual and real audiences have been successfully utilized to treat public speaking anxiety in the past , little work has been done on identifying behavioral characteristics of speakers suffering from anxiety . In this work , we focus on the characterization of behavioral indicators and the automatic assessment of public speaking anxiety . We identify several indicators for public speaking anxiety , among them are less eye contact with the audience , reduced variability in the voice , and more pauses . We automatically assess the public speaking anxiety as reported by the speakers through a self-assessment questionnaire using a speaker independent paradigm . Our approach using ensemble trees achieves a high correlation between ground truth and our estimation ( r=0.825 ) . Complementary to automatic measures of anxiety , we are also interested in speakers ' perceptual differences when interacting with a virtual audience based on their level of anxiety in order to improve and further the development of virtual audiences for the training of public speaking and the reduction of anxiety .
2K_dev_1857	Action Unit ( AU ) detection from facial images is an important classification task in affective computing . However most existing approaches use carefully engineered feature extractors along with off-the-shelf classifiers . There has also been less focus on how well classifiers generalize when tested on different datasets . In our paper , we propose a multi-label convolutional neural network approach to learn a shared representation between multiple AUs directly from the input image . Experiments on three AU datasets-CK+ , DISFA and BP4D indicate that our approach obtains competitive results on all datasets . Cross-dataset experiments also indicate that the network generalizes well to other datasets , even when under different training and testing conditions .
2K_dev_1858	Large teams of robots that operate collectively , whose behavior emerges from local interactions with neighbors , are known as swarms . While significant progress has been made improving the hardware , communication capabilities , and autonomous operation of these swarms , we still have much to learn about how human operators control and interact with them . This research is necessary if real world swarms are to be deployed in the future . The study presented here investigates different methods of displaying information about the swarm state to operators , and asks them to make predictions about the swarm 's future state . In the study , participants are shown swarms performing one of three different behaviors , and are asked to use the information available from the display to make their predictions . Results show that summarizing the swarm 's current state to just an average position and bounding ellipse allowed predictions as accurate as those made when full state information was shown . Furthermore , two leader-based methods were used , whereby the operators were shown only a small subset of the swarm . However , such display methods were inferior for prediction than either the summary center and ellipse or full information methods . With these results , and with participant feedback about the helpfulness of the four display types , we hope future studies can make more informed decision about interface design when it comes to the control of swarms .
2K_dev_1859	Planning for multirobot manipulation in dense clutter becomes particularly challenging as the motion of the manipulated object causes the connectivity of the robots ' free space to change . This paper introduces a data structure , the Feasible Transition Graph ( FTG ) , and algorithms that solve such complex motion planning problems . We define an equivalence relation over object configurations based on the robots ' free space connectivity . Within an equivalence class , the homogeneous multirobot motion planning problem is straightforward , which allows us to decouple the problems of composing feasible object motions and planning paths for individual robots . The FTG captures transitions among the equivalence classes and encodes constraints that must be satisfied for the robots to manipulate the object . From this data structure , we readily derive a complete planner to coordinate such motion . Finally , we show how to construct the FTG in some sample environments and discuss future adaptations to general environments .
2K_dev_1860	Sharing scientific data , software , and instruments is becoming increasingly common as science moves toward large-scale , distributed collaborations . Sharing these resources requires extra work to make them generally useful . Although we know much about the extra work associated with sharing data , we know little about the work associated with sharing contributions to software , even though software is of vital importance to nearly every scientific result . This paper presents a qualitative , interview-based study of the extra work that developers and end users of scientific software undertake . Our findings indicate that they conduct a rich set of extra work around community management , code maintenance , education and training , developer-user interaction , and foreseeing user needs . We identify several conditions under which they are likely to do this work , as well as design principles that can facilitate it . Our results have important implications for future empirical studies as well as funding policy .
2K_dev_1861	Eye tracking is a compelling tool for revealing people 's spatial-temporal distribution of visual attention . But quality eye tracking hardware is expensive and can only be used with one person at a time . Further , webcam eye tracking systems have significant limitations on head movement and lighting conditions that result in significant data loss and inaccuracies . To address these drawbacks , we introduce a new approach that harnesses the crowd to understand allocation of visual attention . In our approach , crowdsourcing participants use mouse clicks to self-report the positions and trajectory for the following valuable eye tracking measures : first gaze , last gaze and all gazes . We validate our crowdsourcing approach with a user study , which demonstrated good accuracy when compared to a real eye tracker . We then deployed our prototype , GazeCrowd , in a crowdsourcing setting , and showed that it accurately generated gaze heatmaps and trajectory maps . Such an approach will allow designers to evaluate and refine their visual design without requiring the use of limited/expensive eye trackers .
2K_dev_1862	In a variety of peer production settings , from Wikipedia to open source software development to crowdsourcing , individuals may encounter , edit , or review the work of unknown others . Typically this is done without much context to the person 's past behavior or performance . To understand how exposure to an unknown individual 's activity history influences attitudes and behaviors , we conducted an online experiment on Mechanical Turk varying the content , quality , and presentation of information about another Turker 's work history . Surprisingly , negative work history did not lead to negative outcomes , but in contrast , a positive work history led to positive initial impressions that persisted in the face of contrary information . This work provides insight into the impact of activity history design factors on psychological and behavioral outcomes that can be of use in other related settings .
2K_dev_1863	Telepresence means business people can make deals in other countries , doctors can give remote medical advice , and soldiers can rescue someone from thousands of miles away . When interaction is mediated , people are removed from and lack context about the person they are making decisions about . In this paper , we explore the impact of technological mediation on risk and dehumanization in decision-making . We conducted a laboratory experiment involving medical treatment decisions . The results suggest that technological mediation influences decision making , but its influence depends on an individual 's self-construal : participants who saw themselves as defined through their relationships ( interdependent self-construal ) recommended riskier and more painful treatments in video conferencing than when face-to-face . We discuss implications of our results for theory and future research .
2K_dev_1864	In this paper , we present the Group Context Framework ( GCF ) , a general-purpose toolkit that allows mobile devices to opportunistically share contextual information . GCF provides a standardized way for developers to request contextual data for their applications . The framework then intelligently groups with other devices to satisfy these requirements . Through two prototypes , we demonstrate how GCF can be used to support a broad range of collaborative and cooperative tasks . We then show how our framework 's architecture allows devices to opportunistically detect and collaborate with one another , even when running different applications . Finally , we present two real-world domains that show how GCF 's ability to form groups increases users ' access to relevant and timely information , and discuss possible incentives and safeguards to context sharing from a user standpoint .
2K_dev_1865	In many scenarios involving human interaction with a remote swarm , the human operator needs to be periodically updated with state information from the robotic swarm . A complete representation of swarm state is high dimensional and perceptually inaccessible to the human . Thus , a summary representation is often required . In addition , it is often the case that the human-swarm communication channel is extremely bandwidth constrained and may have high latency . This motivates the need for the swarm itself to compute a summary representation of its own state for transmission to the human operator . The summary representation may be generated by selecting a subset of robots , known as the information leaders , whose own states suffice to give a bounded approximation of the entire swarm , even in the presence of uncertainty . In this paper , we propose two fully distributed asynchronous algorithms for information leader selection that only rely on inter-robot local communication . In particular , by representing noisy robot states as error ellipsoids with tunable confidence level , the information leaders are selected such that the Minimum-Volume Covering Ellipsoid ( MVCE ) summarizes the noisy swarm state boundary . We provide bounded optimality analysis and proof of convergence for the algorithms . We present simulation results demonstrating the performance and effectiveness of the proposed algorithms .
2K_dev_1866	High-mobility walking robots offer unique capabilities in complex off-road environments where wheeled vehicles are not able to travel . However , these environments can also pose significant autonomous navigation challenges . Key steps in planning a safe path for the robot autonomously include estimating the height of the support ground surface - which is often occluded by vegetation - and classifying the terrain and obstacles above the ground surface . This paper describes the development and experimental evaluation of a terrain classification and ground surface height estimation system to support autonomous navigation for a high-mobility walking robot . We provide experimental evaluation on an extensive , manually-labeled dataset collected from geographically diverse sites over a 28-month period .
2K_dev_1867	We present the design , fabrication , and characterization of a fiber optically sensorized robotic hand for multi purpose manipulation tasks . The robotic hand has three fingers that enable both pinch and power grips . The main bone structure was made of a rigid plastic material and covered by soft skin . Both bone and skin contain embedded fiber optics for force and tactile sensing , respectively . Eight fiber optic strain sensors were used for rigid bone force sensing , and six fiber optic strain sensors were used for soft skin tactile sensing . For characterization , different loads were applied in two orthogonal axes at the fingertip and the sensor signals were measured from the bone structure . The skin was also characterized by applying a light load on different places for contact localization . The actuation of the hand was achieved by a tendon-driven under-actuated system . Gripping motions are implemented using an active tendon located on the volar side of each finger and connected to a motor . Opening motions of the hand were enabled by passive elastic tendons located on the dorsal side of each finger .
2K_dev_1868	We present an algorithm for generating open-loop trajectories that solve the problem of rearrangement planning under uncertainty . We frame this as a selection problem where the goal is to choose the most robust trajectory from a finite set of candidates . We generate each candidate using a kinodynamic state space planner and evaluate it using noisy rollouts . Our key insight is we can formalize the selection problem as the `` best arm { '' } variant of the multi-armed bandit problem . We use the successive rejects algorithm to efficiently allocate rollouts between candidate trajectories given a rollout budget . We show that the successive rejects algorithm identifies the best candidate using fewer rollouts than a baseline algorithm in simulation . We also show that selecting a good candidate increases the likelihood of successful execution on a real robot .
2K_dev_1869	This paper presents a novel nonlinear compliant link . It has two major properties : bi-directionality and stiffening compliance . Bi-directionality means it can be stretched and compressed , and is realized by antagonistic arrangement of an extension spring and a compression spring . Stiffening compliance means it becomes stiffer as it is stretched , and is realized by asymmetric geometry . The links are parts of Simple Hand . Because Simple Hand gives limited space for links , current iteration of links is not obviously nonlinear . However , nonlinearity should be more obvious if links are designed for larger grippers .
2K_dev_1870	A key challenge of developing robots that work closely with people is creating a user interface that allows a user to communicate complex instructions to a robot quickly and easily . We consider a walking logistics support robot , which is designed to carry heavy loads to locations that are too difficult to reach with a wheeled or tracked vehicle . In this application the robot is carrying equipment and supplies for a group of pedestrians , and the primary task for the user interface is to keep the robot traveling with the overall group in the right formation . This paper presents a marker tracking system that uses near infrared cameras , retro-reflective markers , and LIDAR to allow a particular user to designate himself as the robot 's leader , and guide the robot along a desired path . We provide an extensive quantitative evaluation to show that the proposed system is able to detect and track a leader through unconstrained and cluttered off-road environments under a wide variety of illumination and motion conditions .
2K_dev_1871	Exploration of unknown environments is an important aspect to fielding teams of robots . Without the ability to determine on their own where to go in the environment , the full potential of robotic teams is limited to the abilities of human operators to deploy them for search and rescue , mapping , or other tasks that are predicated on gaining knowledge from the environment . This is of particular importance in real-world 3-Dimensional ( 3-D ) environments where simple planar assumptions can lead to incomplete exploration , for example , real-world environments have areas underneath overhangs or inside caves . As an additional challenge , when the teams of robots have vastly different capabilities , the planning system must take those into account to efficiently utilize the available assets . In this paper , we present a combined air-ground system for conducting 3-D exploration in cluttered environments . We first describe the hardware and software components of the system . We then present our algorithm for planning 3-D goal locations for a heterogeneous team of robots to efficiently explore a previously unknown environment and demonstrate its applicability in real-world experiments .
2K_dev_1872	Navigating in a previously unknown environment and recognizing naturally occurring text in a scene are two important autonomous capabilities that are typically treated as distinct . However , these two tasks are potentially complementary , ( i ) scene and pose priors can benefit text spotting , and ( ii ) the ability to identify and associate text features can benefit navigation accuracy through loop closures . Previous approaches to autonomous text spotting typically require significant training data and are too slow for real-time implementation . In this work , we propose a novel high-level feature descriptor , the `` junction { '' } , which is particularly well-suited to text representation and is also fast to compute . We show that we are able to improve SLAM through text spotting on datasets collected with a Google Tango , illustrating how location priors enable improved loop closure with text features .
2K_dev_1873	Robot perception is generally viewed as the interpretation of data from various types of sensors such as cameras . In this paper , we study indirect perception where a robot can perceive new information by making inferences from non-visual observations of human teammates . As a proof-of-concept study , we specifically focus on a door detection problem in a stealth mission setting where a team operation must not be exposed to the visibility of the team 's opponents . We use a special type of the Noisy-OR model known as BN2O model of Bayesian inference network to represent the inter-visibility and to infer the locations of the doors , i.e. , potential locations of the opponents . Experimental results on both synthetic data and real person tracking data achieve an F-measure of over 0.9 on average , suggesting further investigation on the use of non-visual perception in human-robot team operations .
2K_dev_1874	This paper explores the design space of simple legged robots capable of leaping culminating in new behaviors for the Penn Jerboa , an underactuated , dynamically dexterous robot . Using a combination of formal reasoning and physical intuition , we analyze and test successively more capable leaping behaviors through successively more complicated body mechanics . The final version of this machine studied here bounds up a ledge 1.5 times its hip height and crosses a gap 2 times its body length , exceeding in this last regard the mark set by the far more mature RHex hexapod . Theoretical contributions include a non-existence proof of a useful class of leaps for a stripped-down initial version of the new machine , setting in motion the sequence of improvements leading to the final resulting performance . Conceptual contributions include a growing understanding of the Ground Reaction Complex as an effective abstraction for classifying and generating transitional contact behaviors in robotics .
2K_dev_1875	Robotic swarms are distributed systems whose members interact via local control laws to achieve a variety of behaviors , such as flocking . In many practical applications , human operators may need to change the current behavior of a swarm from the goal that the swarm was going towards into a new goal due to dynamic changes in mission objectives . There are two related but distinct capabilities needed to supervise a robotic swarm . The first is comprehension of the swarm 's state and the second is prediction of the effects of human inputs on the swarm 's behavior . Both of them are very challenging . Prior work in the literature has shown that inserting the human input as soon as possible to divert the swarm from its original goal towards the new goal does not always result in optimal performance ( measured by some criterion such as the total time required by the swarm to reach the second goal ) . This phenomenon has been called Neglect Benevolence , conveying the idea that in many cases it is preferable to neglect the swarm for some time before inserting human input . In this paper , we study how humans can develop an understanding of swarm dynamics so they can predict the effects of the timing of their input on the state and performance of the swarm . We developed the swarm configuration shape-changing Neglect Benevolence Task as a Human Swarm Interaction ( HSI ) reference task allowing comparison between human and optimal input timing performance in control of swarms . Our results show that humans can learn to approximate optimal timing and that displays which make consensus variables perceptually accessible can enhance performance .
2K_dev_1876	In the future , we envision domestic robots to play a large role in our everyday lives . This requires robots able to anticipate our needs and preferences and adapt their behavior . Since current robotics research takes place primarily in laboratory settings , it fails to take into account real users . In this work , we explore how organization occurs in the kitchen through a home study . Our analysis includes qualitative insights towards robot behavior during kitchen organization , an open source dataset of real life kitchens , and a proof-of-concept application of this dataset to the problem of object return .
2K_dev_1877	We develop a new paradigm for designing fully streaming , area-efficient FPGA implementations of common building blocks for vision algorithm . By focusing on avoiding redundant computation we achieve a reduction of one to two orders of magnitude reduction in design area utilization as compared to previous implementations . We demonstrate that our design works in practice by building five 325 frames per second , high resolution Harris corner detection cores onto a single FPGA .
2K_dev_1878	Experience Graphs have been shown to accelerate motion planning using parts of previous paths in an A { * } framework . Experience Graphs work by computing a new heuristic for weighted A { * } search on top of the domain 's original heuristic and the edges in an Experience Graph . The new heuristic biases the search toward relevant prior experience and uses the original heuristic for guidance otherwise . In previous work , Experience Graphs were always built on top of domain heuristics which were computed by dynamic programming ( a lower dimensional version of the original planning problem ) . When the original heuristic is computed this way the Experience Graph heuristic can be computed very efficiently . However , there are many commonly used heuristics in planning that are not computed in this fashion , such as euclidean distance . While the Experience Graph heuristic can be computed using these heuristics , it is not efficient , and in many cases the heuristic computation takes much of the planning time . In this work , we present a more efficient way to use these heuristics for motion planning problems by making use of popular nearest neighbor algorithms . Experimentally , we show an average 8 times reduction in heuristic computation time , resulting in overall planning time being reduced by 66\ % . with no change in the expanded states or resulting path .
2K_dev_1879	Many robot applications involve lifelong planning in relatively static environments e.g . assembling objects or sorting mail in an office building . In these types of scenarios , the robot performs many tasks over a long period of time . Thus , the time required for computing a motion plan becomes a significant concern , prompting the need for a fast and efficient motion planner . Since these environments remain similar in between planning requests , planning from scratch is wasteful . Recently , Experience Graphs ( E-Graphs ) were proposed to accelerate the planning process by reusing parts of previously computed paths to solve new motion planning queries more efficiently . This work describes a method to improve planning times with E-Graphs given changes in the environment by lazily evaluating the validity of past experiences during the planning process . We show the improvements with our method in a single-arm manipulation domain with simulations on the PR2 robot .
2K_dev_1880	In this paper , we present an algorithm to solve the Multi-Robot Persistent Coverage Problem ( MRPCP ) . Here , we seek to compute a schedule that will allow a fleet of agents to visit all targets of a given set while maximizing the frequency of visitation and maintaining a sufficient fuel capacity by refueling at depots . We also present a heuristic method to allow us to compute bounded suboptimal results in real time . The results produced by our algorithm will allow a team of robots to efficiently cover a given set of targets or tasks persistently over long periods of time , even when the cost to transition between tasks is dynamic .
2K_dev_1881	This paper reports on a 3D photomosaicing pipeline using data collected from an autonomous underwater vehicle performing simultaneous localization and mapping ( SLAM ) . The pipeline projects and blends 21 ) imaging sonar data onto a large-scale 3D mesh that is either given a priori or derived from SLAM . Compared to other methods that generate a 2D-only mosaic , our approach produces 3D models that are more structurally representative of the environment being surveyed . Additionally , our system leverages recent work in underwater SLAM using sparse point clouds derived from Doppler velocity log range returns to relax the need for a prior model . We show that the method produces reasonably accurate surface reconstruction and blending consistency , with and without the use of a prior mesh . We experimentally evaluate our approach with a Hovering Autonomous Underwater Vehicle ( HAUV ) performing inspection of a large underwater ship hull .
2K_dev_1882	Our goal is to automatically recognize hand grasps and to discover the visual structures ( relationships ) between hand grasps using wearable cameras . Wearable cameras provide a first-person perspective which enables continuous visual hand grasp analysis of everyday activities . In contrast to previous work focused on manual analysis of first-person videos of hand grasps , we propose a fully automatic vision-based approach for grasp analysis . A set of grasp classifiers are trained for discriminating between different grasp types based on large margin visual predictors . Building on the output of these grasp classifiers , visual structures among hand grasps are learned based on an iterative discriminative clustering procedure . We first evaluated our classifiers on a controlled indoor grasp dataset and then validated the analytic power of our approach on real-world data taken from a machinist . The average F1 score of our grasp classifiers achieves over 0.8 for the indoor grasp dataset . Analysis of real-world video shows that it is possible to automatically learn intuitive visual grasp structures that are consistent with expert-designed grasp taxonomies .
2K_dev_1883	Assembly of large structures requires large fixtures , often referred to as monuments . Their cost and massive size limit flexibility and scalability of the manufacturing process . Numerous small mobile robots can replace these large structures and , therefore , replicate the efficiency of the assembly line with far more flexibility . An assembly line made up of mobile manipulators can easily and rapidly be reconfigured to support scalability and a varied product mix , while allowing for near optimal resource assignment . The challenge to using small robots in place of monuments is making their joint behavior precise enough to accomplish the task and efficient enough to execute subtasks in a reasonable period of time . In this paper , we describe a set of techniques that we combine to achieve the necessary precision and overall efficiency to build a large structure . We describe and demonstrate these techniques in the context of a testbed we implemented for assembling a wing ladder .
2K_dev_1884	We propose a language-driven navigation approach for commanding mobile robots in outdoor environments . We consider unknown environments that contain previously unseen objects . The proposed approach aims at making interactions in human-robot teams natural . Robots receive from human teammates commands in natural language , such as `` Navigate around the building to the car left of the fire hydrant and near the tree { '' } . A robot needs first to classify its surrounding objects into categories , using images obtained from its sensors . The result of this classification is a map of the environment , where each object is given a list of semantic labels , such as `` tree { '' } and `` car { '' } , with varying degrees of confidence . Then , the robot needs to ground the nouns in the command . Grounding , the main focus of this paper , is mapping each noun in the command into a physical object in the environment . We use a probabilistic model for interpreting the spatial relations , such as `` left of { '' } and `` near { '' } . The model is learned from examples provided by humans . For each noun in the command , a distribution on the objects in the environment is computed by combining spatial constraints with a prior given as the semantic classifier 's confidence values . The robot needs also to ground the navigation mode specified in the command , such as `` navigate quickly { '' } and `` navigate covertly { '' } , as a cost map . The cost map is also learned from examples , using Inverse Optimal Control ( IOC ) . The cost map and the grounded goal are used to generate a path for the robot . This approach is evaluated on a robot in a real-world environment . Our experiments clearly show that the proposed approach is efficient for commanding outdoor robots .
2K_dev_1885	Here , we present a general framework for combining visual odometry and lidar odometry in a fundamental and first principle method . The method shows improvements in performance over the state of the art , particularly in robustness to aggressive motion and temporary lack of visual features . The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity . Then , scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously . We show results with datasets collected in our own experiments as well as using the KITTI odometry benchmark . Our proposed method is ranked \ # 1 on the benchmark in terms of average translation and rotation errors , with a 0.75\ % of relative position drift . In addition to comparison of the motion estimation accuracy , we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes .
2K_dev_1886	We formalize the problem of adapting a demonstrated trajectory to a new start and goal configuration as an optimization problem over a Hilbert space of trajectories : minimize the distance between the demonstration and the new trajectory subject to the new end point constraints . We show that the commonly used version of Dynamic Movement Primitives ( DMPs ) implement this minimization in the way they adapt demonstrations , for a particular choice of the Hilbert space norm . The generalization to arbitrary norms enables the robot to select a more appropriate norm for the task , as well as learn how to adapt the demonstration from the user . Our experiments show that this can significantly improve the robot 's ability to accurately generalize the demonstration .
2K_dev_1887	Many motion planning problems in robotics are high dimensional planning problems . While sampling-based motion planning algorithms handle the high dimensionality very well , the solution qualities are often hard to control due to the inherent randomization . In addition , they suffer severely when the configuration space has several `narrow passages ' . Search-based planners on the other hand typically provide good solution qualities and are not affected by narrow passages . However , in the absence of a good heuristic or when there are deep local minima in the heuristic , they suffer from the curse of dimensionality . In this work , our primary contribution is a method for dynamically generating heuristics , in addition to the original heuristic ( s ) used , to guide the search out of local minima . With the ability to escape local minima easily , the effect of dimensionality becomes less pronounced . On the theoretical side , we provide guarantees on completeness and bounds on suboptimality of the solution found . We compare our proposed method with the recently published Multi-Heuristic A { * } search , and the popular RRT-Connect in a full-body mobile manipulation domain for the PR2 robot , and show its benefits over these approaches .
2K_dev_1888	Autonomous systems that navigate in unknown environments encounter a variety of planning problems . The success of any one particular planning strategy depends on the validity of assumptions it leverages about the structure of the problem , e.g. , Is the cost map locally convex ? Does the feasible state space have good connectivity ? We address the problem of determining suitable motion planning strategies that can work on a diverse set of applications . We have developed a planning system that does this by running competing planners in parallel . In this paper , we present an approach that constructs a planner ensemble - a set of complementary planners that leverage a diverse set of assumptions . Our approach optimizes the submodular selection criteria with a greedy approach and lazy evaluation . We seed our selection with learnt priors on planner performance , thus allowing us to solve new applications without evaluating every planner on that application . We present results in simulation where the selected ensemble outperforms the best single planner and does almost as well as an off-line planner . We also present results from an autonomous helicopter that has flown missions several kilometers long at speeds of up to 56 ( m ) /s which involved avoiding unmapped mountains , no-fly zones and landing in cluttered areas with trees and buildings . This work opens the door on the more general problem of adaptive motion planning .
2K_dev_1889	We present a randomized kinodynamic planner that solves rearrangement planning problems . We embed a physics model into the planner to allow reasoning about interaction with objects in the environment . By carefully selecting this model , we are able to reduce our state and action space , gaining tractability in the search . The result is a planner capable of generating trajectories for full arm manipulation and simultaneous object interaction . We demonstrate the ability to solve more rearrangement by pushing tasks than existing primitive based solutions . Finally , we show the plans we generate are feasible for execution on a real robot .
2K_dev_1890	In this work we present a fast kinodynamic RRT-planner that uses dynamic nonprehensile actions to rearrange cluttered environments . In contrast to many previous works , the presented planner is not restricted to quasi-static interactions and monotonicity . Instead the results of dynamic robot actions are predicted using a black box physics model . Given a general set of primitive actions and a physics model , the planner randomly explores the configuration space of the environment to find a sequence of actions that transform the environment into some goal configuration . In contrast to a naive kinodynamic RRT-planner we show that we can exploit the physical fact that in an environment with friction any object eventually comes to rest . This allows a search on the configuration space rather than the state space , reducing the dimension of the search space by a factor of two without restricting us to non-dynamic interactions . We compare our algorithm against a naive kinodynamic RRT-planner and show that on a variety of environments we can achieve a higher planning success rate given a restricted time budget for planning .
2K_dev_1891	Modeling the effects of actions based on the state of the world enables robots to make intelligent decisions in different situations . However , it is often infeasible to have globally accurate models . Task performance is often hindered by discrepancies between models and the real world , since the 1 outcome of executing a plan may be significantly worse than the expected outcome used during planning . Furthermore , expectations about the world are often stochastic in robotics , making the discovery of model-world discrepancies non-trivial . We present an execution monitoring framework capable of finding statistically significant discrepancies , determining the situations in which they occur , and making simple corrections to the world model to improve performance . In our approach , plans are initially based on a model of the world that is only as faithful as computational and algorithmic limitations allow . Through experience , the monitor discovers previously unmodeled modes of the world , defined as regions of a feature space in which the experienced outcome of a plan deviates significantly from the predicted outcome . The monitor may then make suggestions to change the model to match the real world more accurately . We demonstrate this approach on the adversarial domain of robot soccer : we monitor pass interception performance of potentially unknown opponents to try to find unforeseen modes of behavior that affect their interception performance .
2K_dev_1892	The goal of this paper is to develop a regrasp planning algorithm general enough to perform statistical analysis with thousands of experiments and arbitrary mesh models . We focus on pick-and-place regrasp which reorients an object from one placement to another by using a sequence of pickups and place-downs . We improve the pick-and-place regrasp approach developed in 1990s and analyze its performance in robotic assembly with different work surfaces in the workcell . Our algorithm will automatically compute the stable placements of an object , find several force-closure grasps , generate a graph of regrasp actions , and search for regrasp sequences . We demonstrate the advantages of our algorithm with various mesh models and use the algorithm to evaluate the completeness , the cost and the length of regrasp sequences with different mesh models and different assembly tasks in the presence of different work surfaces . Our results show that spare work surfaces are beneficial to assembly . Tilted work surfaces are only sometimes beneficial , depending on the objects .
2K_dev_1893	Simultaneous localization and mapping with infinite planes is attractive because of the reduced complexity with respect to both sparse point-based and dense volumetric methods . We show how to include infinite planes into a least-squares formulation for mapping , using a homogeneous plane parametrization with a corresponding minimal representation for the optimization . Because it is a minimal representation , it is suitable for use with Gauss-Newton , Powell 's Dog Leg and incremental solvers such as iSAM . We also introduce a relative plane formulation that improves convergence . We evaluate our proposed approach on simulated data to show its advantages over alternative solutions . We also introduce a simple mapping system and present experimental results , showing real-time mapping of select indoor environments with a hand-held RGBD sensor .
2K_dev_1894	We develop a computationally efficient control policy for active perception that incorporates explicit models of sensing and mobility to build 3D maps with ground and aerial robots . Like previous work , our policy maximizes an information-theoretic objective function between the discrete occupancy belief distribution ( e.g. , voxel grid ) and future measurements that can be made by mobile sensors . However , our work is unique in three ways . First , we show that by using Cauchy-Schwarz Quadratic Mutual Information ( CSQMI ) , we get significant gains in efficiency . Second , while most previous methods adopt a myopic , gradient-following approach that yields poor convergence properties , our algorithm searches over a set of paths and is less susceptible to local minima . In doing so , we explicitly incorporate models of sensors , and model the dependence ( and independence ) of measurements over multiple time steps in a path . Third , because we consider models of sensing and mobility , our method naturally applies to both ground and aerial vehicles . The paper describes the basic models , the problem formulation and the algorithm , and demonstrates applications via simulation and experimentation .
2K_dev_1895	With the prevalence of social media , such as Twitter , short-length text like microblogs have become an important mode of text on the Internet . In contrast to other forms of media , such as newspaper , the text in these social media posts usually contains fewer words , and is concentrated on a much narrower selection of topics . For these reasons , traditional LDA-based sentiment and topic modeling techniques generally do not work well in case of social media data . Another characteristic feature of this data is the use of special meta tokens , such as hashtags , which contain unique semantic meanings that are not captured by other ordinary words . In the recent years , many topic modeling techniques have been proposed for social media data , but the majority of this work does not take into account the specialty of tokens , such as hashtags , and treats them as ordinary words . In this paper , we propose probabilistic graphical models to address the problem of discovering latent topics and their sentiment from social media data , mainly microblogs like Twitter . We first propose MTM ( Microblog Topic Model ) , a generative model that assumes each social media post generates from a single topic , and models both words and hashtags separately . We then propose MSTM ( Microblog Sentiment Topic Model ) , an extension of MTM , which also embodies the sentiment associated with the topics . We evaluated our models using Twitter dataset , and experimental
2K_dev_1896	There have been increasing interests in the robotics community in building smaller and more agile autonomous micro aerial vehicles ( MAVs ) . In particular , the monocular visual-inertial system ( VINS ) that consists of only a camera and an inertial measurement unit ( IMU ) forms a great minimum sensor suite due to its superior size , weight , and power ( SWaP ) characteristics . In this paper , we present a tightly-coupled nonlinear optimization-based monocular VINS estimator for autonomous rotorcraft MAVs . Our estimator allows the MAV to execute trajectories at 2 m/s with roll and pitch angles up to 30 degrees . We present extensive statistical analysis to verify the performance of our approach in different environments with varying flight speeds .
2K_dev_1897	Learning from demonstration ( LfD ) is a common technique applied to many problems in robotics , such as populating grasp databases , training for reinforcement learning of high-level skill sets and bootstrapping motion planners . While such approaches are generally highly valued , they rely on the often time-consuming process of gathering user demonstrations , and hence it becomes difficult to attain a sizeable dataset . In this paper , we present a tool capable of recording large numbers of high-dimensional demonstrations of mobile manipulation tasks provided by non-experts in the field . Our tool accomplishes this via a web interface that requires no additional software to be installed beyond a web browser , as well as a scalable architecture that is capable of supporting 10 concurrent demonstrators on a single server . Our architecture employs a lightweight simulation environment to reduce unnecessary computations and improve performance . Furthermore , we show how our tool can be used to gather a large set of demonstrations of a mobile manipulation task by leveraging existing crowdsource platforms . The data set collected has been made available to the robotics community . We also present experiments in which we apply demonstrations collected through our infrastructure to teach a robot how to grasp , to teach a robot how to perform dexterous manipulation tasks such as scooping and to accelerate motion planning for full-body manipulation tasks .
2K_dev_1898	We demonstrate distributed , online , and real-time cooperative localization and mapping between multiple robots operating throughout an unknown environment using indirect measurements . We present a novel Expectation Maximization ( EM ) based approach to efficiently identify inlier multi-robot loop closures by incorporating robot pose uncertainty , which significantly improves the trajectory accuracy over long-term navigation . An EM and hypothesis based method is used to determine a common reference frame . We detail a 2D laser scan correspondence method to form robust correspondences between laser scans shared amongst robots . The implementation is experimentally validated using teams of aerial vehicles , and analyzed to determine its accuracy , computational efficiency , scalability to many robots , and robustness to varying environments . We demonstrate through multiple experiments that our method can efficiently build maps of large indoor and outdoor environments in a distributed , online , and real-time setting .
2K_dev_1899	We study the problem of building a sensor model for the purpose of simulation . Our work is motivated by the potential impact of realistic simulators on the development cycle of software for real robots . The case is made for building models from approximate state information , relieving the burden of ground truth . Unlike calibration , where the goal is to identify and remove error from a signal , our aim to reproduce the signal in its entirety , including its error properties . Instead of physically modeling sensor behavior , a data-driven approach is taken . The implementation of our approach to simulate a simple but noisy laser rangefinder is described . Finally , approaches to validate the simulator are discussed . We compare not only raw sensor predictions , but also overall performance of algorithms on simulated versus real data .
2K_dev_1900	In social voting Web sites , how do the user actions - up-votes , down-votes and comments - evolve over time ? Are there relationships between votes and comments ? What is normal and what is suspicious ? These are the questions we focus on . We analyzed over 20000 submissions corresponding to more than 100 million user interactions from three social voting Web sites : Reddit , Imgur and Digg . Our first contribution is two discoveries : ( i ) the number of comments grows as a power-law on the number of votes and ( ii ) the time between a submission creation and a user 's reaction obeys a log-logistic distribution . Based on these patterns , we propose VNC ( VOTE-AND-COMMENT ) , a parsimonious but accurate and scalable model that models the coevolution of user activities . In our experiments on real data , VNC outperformed state-of-the-art baselines on accuracy . Additionally , we illustrate VNC usefulness for forecasting and outlier detection .
2K_dev_1901	Autonomous mobile robots are required to operate in partially known and unstructured environments . It is imperative to guarantee safety of such systems for their successful deployment . Current state of the art does not fully exploit the sensor and dynamic capabilities of a robot . Also , given the non-holonomic systems with non-linear dynamic constraints , it becomes computationally infeasible to find an optimal solution if the full dynamics are to be exploited online . In this paper we present an online algorithm to guarantee the safety of the robot through an emergency maneuver library . The maneuvers in the emergency maneuver library are optimized such that the probability of finding an emergency maneuver that lies in the known obstacle free space is maximized . We prove that the related trajectory set diversity problem is monotonic and submodular which enables one to develop an efficient trajectory set generation algorithm with bounded sub-optimality . We generate an off-line computed trajectory set that exploits the full dynamics of the robot and the known obstacle-free region . We test and validate the algorithm on a full-size autonomous helicopter flying up to speeds of 56m/s in partially-known environments . We present results from 4 months of flight testing where the helicopter has been avoiding trees , performing autonomous landing , avoiding mountains while being guaranteed safe .
2K_dev_1903	Let p be an unknown probability distribution on { [ } n ] : 0 \ { 1,2 , . . . n\ } that we can access via two kinds of queries : A SAM P query takes no input and returns x epsilon { [ } n ] with probability p { [ } x ] ; a PMF query takes as input x E { [ } n ] and returns the value p { [ } x ] . We consider the task of estimating the entropy of p to within +/- Delta ( with high probability ) . For the usual Shannon entropy H ( p ) , we show that Omega ( log ( 2 ) n/Delta ( 2 ) ) queries are necessary , matching a recent upper bound of Canonne and Rubinfeld . For the Renyi entropy Th ( p ) , where a > 1 , we show that Theta ( n ( 1-1/alpha ) ) queries are necessary and sufficient . This complements recent work of Acharya et al . in the SAMP-only model that showed Omicron ( n ( 1-1/alpha ) ) queries suffice when a is an integer , but Omega ( n ) queries are necessary when a is a non integer . All of our lower bounds also easily extend to the model where CDF queries ( given x , return Sigma ( y < 0 x ) p { [ } y ] ) are allowed .
2K_dev_1904	We examined the effects gender and moral identity on collaborative behavior among Face ( Chinese ) and Dignity ( Canadian ) cultures . 105 participants engaged in a dyadic intracultural interaction via the FireSim computer game . Each participant was assigned a village and was tasked to protect the village from seasonal fires . Participants had the option of requesting or providing help to the neighboring village , i.e . their counterpart . We examined collaborative behavior by measuring help given , while controlling for help request . Using theories of face and dignity cultures , moral identity , and gender roles , we predicted and found that overall , Chinese individuals were less helpful than Canadians . This effect was stronger for males than females . Interestingly , more helping behavior was observed among Canadians with high levels of internal moral identity . Yet , this effect was not observed among Chinese individuals . Theoretical and practical implications for collaboration across culture are discussed .
2K_dev_1905	Human activity recognition is an important and challenging task for video content analysis and understanding . Individual activity recognition has been well studied recently . However , recognizing the activities of human group with more than three people having complex interactions is still a formidable challenge . In this paper , a novel human group activity recognition method is proposed to deal with complex situation where there are multiple sub-groups . To characterize the inherent interactions of intra-subgroups and inter-subgroups with the varying number of participants , this paper proposes three types of group-activity descriptor using motion trajectory and appearance information of people . Experimental results on a public human group activity dataset demonstrate effectiveness of the proposed method .
2K_dev_1906	In many markets , products are highly complex with an extremely large set of features . In advertising auctions , for example , an impression , i.e. , a viewer on a web page , has numerous features describing the viewer 's demographics , browsing history , temporal aspects , etc . In these markets , an auctioneer must select a few key features to signal to bidders . These features should be selected such that the bidder with the highest value for the product can construct a bid so as to win the auction . We present an efficient algorithmic solution for this problem in a setting where the product 's features are drawn independently from a known distribution , the bidders ' values for a product are additive over their known values for the features of the product , and the number of features is exponentially larger than the number of bidders and the number of signals . Our approach involves solving a novel optimization problem regarding the expectation of a sum of independent random vectors that may be of independent interest . We complement our positive result with a hardness result for the problem when features are arbitrarily correlated . This result is based on the conjectured hardness of learning k-juntas , a central open problem in learning theory .
2K_dev_1907	In this paper , we present a new tool SReach , which solves probabilistic bounded reachability problems for two classes of models of stochastic hybrid systems . The first one is ( nonlinear ) hybrid automata with parametric uncertainty . The second one is probabilistic hybrid automata with additional randomness for both transition probabilities and variable resets . Standard approaches to reachability problems for linear hybrid systems require numerical solutions for large optimization problems , and become infeasible for systems involving both nonlinear dynamics over the reals and stochasticity . SReach encodes stochastic information by using a set of introduced random variables , and combines delta-complete decision procedures and statistical tests to solve delta-reachability problems in a sound manner . Compared to standard simulation-based methods , it supports non-deterministic branching , increases the coverage of simulation , and avoids the zero-crossing problem . We demonstrate SReach 's applicability by discussing three representative biological models and additional benchmarks for nonlinear hybrid systems with multiple probabilistic system parameters .
2K_dev_1908	Weighted signed networks ( WSNs ) are networks in which edges are labeled with positive and negative weights . WSNs can capture like/dislike , trust/distrust , and other social relationships between people . In this paper , we consider the problem of predicting the weights of edges in such networks . We propose two novel measures of node behavior : the goodness of a node intuitively captures how much this node is liked/trusted by other nodes , while the fairness of a node captures how fair the node is in rating other nodes ' likeability or trust level . We provide axioms that these two notions need to satisfy and show that past work does not meet these requirements for WSNs . We provide a mutually recursive definition of these two concepts and prove that they converge to a unique solution in linear time . We use the two measures to predict the edge weight in WSNs . Furthermore , we show that when compared against several individual algorithms from both the signed and unsigned social network literature , our fairness and goodness metrics almost always have the best predictive power . We then use these as features in different multiple regression models and show that we can predict edge weights on 2 Bitcoin WSNs , an Epinions WSN , 2 WSNs derived from Wikipedia , and a WSN derived from Twitter with more accurate results than past work . Moreover , fairness and goodness metrics form the most significant feature for prediction in most ( but not all ) cases .
2K_dev_1909	In this paper , we focus on demand side management in consumer collectives with community owned renewable energy generation and storage facilities for effective integration of renewable energy with the existing fossil fuel-based power supply system . The collective buys energy as a group through a central coordinator who also decides about the storage and usage of renewable energy produced by the collective . Our objective is to design coordination algorithms to minimize the cost of electricity consumption of the consumer collective while allowing the consumers to make their own consumption decisions based on their private consumption constraints and preferences . Minimizing the cost is not only of interest to the consumers but is also socially desirable because it reduces the consumption at times of peak demand . We develop an iterative coordination algorithm in which the coordinator makes the storage decision and shapes the demands of the consumers by designing a virtual price signal for the agents . We prove that our algorithm converges , and it achieves the optimal solution under realistic conditions . We also present simulation results based on real world consumption data to quantify the performance of our algorithm .
2K_dev_1910	A key challenge in ITS research and development is to support tutoring at scale , for example by embedding tutors in MOOCs . An obstacle to at-scale deployment is that ITS architectures tend to be complex , not easily deployed in browsers without significant server-side processing , and not easily embedded in a learning management system ( LMS ) . We present a case study in which a widely used ITS authoring tool suite , CTAT/TutorShop , was modified so that tutors can be embedded in MOOCs . Specifically , the inner loop ( the example-tracing tutor engine ) was moved to the client by reimplementing it in JavaScript , and the tutors were made compatible with the LTI e-learning standard . The feasibility of this general approach to ITS/MOOC integration was demonstrated with simple tutors in an edX MOOC `` Data Analytics and Learning . { '' }
2K_dev_1911	To be able to provide better support for collaborative learning in Intelligent Tutoring Systems , it is important to understand how collaboration patterns change . Prior work has looked at the interdependencies between utterances and the change of dialogue over time , but it has not addressed how dialogue changes during a lesson , an analysis that allows us to investigate the adaptivity of student strategies as students gain domain knowledge . We address this question by analyzing the shift in types of collaborative talk occurring within a single session and in particular how they relate to errors for 26 4th and 5th grade dyads working on a fractions tutor . We found that , over time , the frequency of interactive talk and errors both decrease in dyads working together on conceptual problems . Although interactive talk is often held as a gold standard in collaboration , as students become more proficient , it may not be as important .
2K_dev_1912	Better conversational alignment can lead to shared understanding , changed beliefs , and increased rapport . We investigate the relationship in peer tutoring of convergence , interpersonal rapport , and student learning . We develop an approach for computational modeling of convergence by accounting for the horizontal richness and time-based dependencies that arise in non-stationary and noisy longitudinal interaction streams . Our results , which illustrate that rapport as well as convergence are significantly correlated with learning gains , provide guidelines for development of peer tutoring agents that can increase learning gains through subtle changes to improve tutor-tutee alignment .
2K_dev_1913	Collaborative and individual learning appear to have complementary strengths ; however , the best way to combine these learning methods is still unclear . While previous work has demonstrated the effectiveness of Intelligent Tutoring Systems ( ITSs ) for individual learning , collaborative learning with ITSs is much less frequent - especially for young students . In this paper , we discuss our prior and future work with elementary school students that aims to investigate how to best combine individual and collaborative learning using their complementary strengths within an ITS . Our previous findings demonstrate that ITSs are able to support collaboration , as well as individual learning , for this population . In addition , we propose future research to understand how to best combine individual and collaborative learning within an ITS .
2K_dev_1914	Internet of Things ( IoT ) allows for cyber-physical applications to be created and composed to provide intelligent support or automation of end-user tasks . For many of such tasks , human participation is crucial to the success and the quality of the tasks . The cyber systems should proactively request help from the humans to accomplish the tasks when needed . However , the outcome of such system-human synergy may be affected by factors external to the systems . Failure to consider those factors when involving human participants in the tasks may result in suboptimal performance and negative experience on the humans . In this paper , we propose an approach for automated generation of control strategies of cyber-human systems . We investigate how explicit modeling of human participant can be used in automated planning to generate cooperative strategy of human and system to achieve a given task , by means of which best and appropriately utilize the human . Specifically , our approach consists of : ( 1 ) a formal framework for modeling cooperation between cyber system and human , and ( 2 ) a formalization of system-human cooperative task planning as strategy synthesis of stochastic multiplayer game . We illustrate our approach through an example of indoor air quality control in smart homes .
2K_dev_1915	We present MindMiner , a mixed-initiative interface for capturing subjective similarity measurements via a combination of new interaction techniques and machine learning algorithms . MindMiner collects qualitative , hard to express similarity measurements from users via active polling with uncertainty and example based visual constraint creation . MindMiner also formulates human prior knowledge into a set of inequalities and learns a quantitative similarity distance metric via convex optimization . In a 12-subject peer-review understanding task , we found MindMiner was easy to learn and use , and could capture users ' implicit knowledge about writing performance and cluster target entities into groups that match subjects ' mental models . We also found that MindMiner 's constraint suggestions and uncertainty polling functions could improve both efficiency and the quality of clustering .
2K_dev_1916	Gestures during spoken dialog play a central role in human communication . As a consequence , models of gesture generation are a key challenge in research on virtual humans , embodied agents capable of face-to-face interaction with people . Machine learning approaches to gesture generation must take into account the conceptual content in utterances , physical properties of speech signals and the physical properties of the gestures themselves . To address this challenge , we proposed a gestural sign scheme to facilitate supervised learning and presented the DCNF model , a model to jointly learn deep neural networks and second order linear chain temporal contingency . The approach we took realizes both the mapping relation between speech and gestures while taking account temporal relations among gestures . Our experiments on human co-verbal dataset shows significant improvement over previous work on gesture prediction . A generalization experiment performed on handwriting recognition also shows that DCNFs outperform the state-of-the-art approaches .
2K_dev_1917	Model Checking is an automatic verification technique for large state transition systems . It was originally developed for reasoning about finite-state concurrent systems . The technique has been used successfully to debug complex computer hardware , communication protocols , and software . It is beginning to be used for analyzing cyberphysical , biological , and financial systems as well . The major challenge for the technique is a phenomenon called the State Explosion Problem . This issue is impossible to avoid in the worst case ; but , by using sophisticated data structures and clever search algorithms , it is now possible to verify state transition systems with an astronomical number of states . In this paper , we will briefly review the development of Model Checking over the past 32 years , with an emphasis on model checking stochastic hybrid systems .
2K_dev_1918	Automated visual analysis is an effective method for understanding changes in natural phenomena over massive city-scale landscapes . However , the view-point spectrum across which image data can be acquired is extremely wide , ranging from macro-level overhead ( aerial ) images spanning several kilometers to micro-level front-parallel ( street-view ) images that might only span a few meters . This work presents a unified framework for robustly integrating image data taken at vastly different viewpoints to generate large-scale estimates of land surface conditions . To validate our approach we attempt to estimate the amount of post-Tsunami damage over the entire city of Kamaishi , Japan ( over 4 million square-meters ) . Our results show that our approach can efficiently integrate both micro and macro-level images , along with other forms of meta-data , to efficiently estimate city-scale phenomena . We evaluate our approach on two modes of land condition analysis , namely , city-scale debris and greenery estimation , to show the ability of our method to generalize to a diverse set of estimation tasks .
2K_dev_1919	Computer vision is increasingly becoming interested in the rapid estimation of object detectors . The canonical strategy of using Hard Negative Mining to train a Support Vector Machine is slow , since the large negative set must be traversed at least once per detector . Recent work has demonstrated that , with an assumption of signal stationarity , Linear Discriminant Analysis is able to learn comparable detectors without ever revisiting the negative set . Even with this insight , the time to learn a detector can still be on the order of minutes . Correlation filters , on the other hand , can produce a detector in under a second . However , this involves the unnatural assumption that the statistics are periodic , and requires the negative set to be re-sampled per detector size . These two methods differ chiefly in the structure which they impose on the covariance matrix of all examples . This paper is a comparative study which develops techniques ( i ) to assume periodic statistics without needing to revisit the negative set and ( ii ) to accelerate the estimation of detectors with aperiodic statistics . It is experimentally verified that periodicity is detrimental .
2K_dev_1920	Given the re-broadcasts ( i.e . retweets ) of posts in Twitter , how can we spot fake from genuine user reactions ? What will be the tell-tale sign - the connectivity of retweeters , their relative timing , or something else ? High retweet activity indicates influential users , and can be monetized . Hence , there are strong incentives for fraudulent users to artificially boost their retweets ' volume . Here , we explore the identification of fraudulent and genuine retweet threads . Our main contributions are : ( a ) the discovery of patterns that fraudulent activity seems to follow ( the `` triangles { '' } and `` homogeneity { '' } patterns , the formation of micro-clusters in appropriate feature spaces ) ; and ( b ) `` RTGen { '' } , a realistic generator that mimics the behaviors of both honest and fraudulent users . We present experiments on a dataset of more than 6 million retweets crawled from Twitter .
2K_dev_1921	Given the retweeting activity for the posts of several Twitter users , how can we distinguish organic activity from spammy retweets by paid followers to boost a post 's appearance of popularity ? More generally , given groups of observations , can we spot strange groups ? Our main intuition is that organic behavior has more variability , while fraudulent behavior , like retweets by botnet members , is more synchronized . We refer to the detection of such synchronized observations as the Synchonization Fraud problem , and we study a specific instance of it , Retweet Fraud Detection , manifested in Twitter . Here , we propose : ( A ) ND-Sync , an efficient method for detecting group fraud , and ( B ) a set of carefully designed features for characterizing retweet threads . ND-Sync is effective in spotting retweet fraudsters , robust to different types of abnormal activity , and adaptable as it can easily incorporate additional features . Our method achieves a 97\ % accuracy on a real dataset of 12 million retweets crawled from Twitter .
2K_dev_1922	This paper presents a novel descriptor , geodesic invariant feature ( GIF ) , for representing objects in depth images . Especially in the context of parts classification of articulated objects , it is capable of encoding the invariance of local structures effectively and efficiently . The contributions of this paper lie in our multi-level feature extraction hierarchy . ( 1 ) Low-level feature encodes the invariance to articulation . Geodesic gradient is introduced , which is covariant with the non-rigid deformation of objects and is utilized to rectify the feature extraction process . ( 2 ) Mid-level feature reduces the noise and improves the efficiency . With unsupervised clustering , the primitives of objects are changed from pixels to superpixels . The benefit is two-fold : firstly , superpixel reduces the effect of the noise introduced by depth sensors ; secondly , the processing speed can be improved by a big margin . ( 3 ) High-level feature captures nonlinear dependencies between the dimensions . Deep network is utilized to discover the high-level feature representation . As the feature propagates towards the deeper layers of the network , the ability of the feature capturing the data 's underlying regularities is improved . Comparisons with the state-of-the-art methods reveal the superiority of the proposed method .
2K_dev_1923	Ensemble methods for classification have been effectively used for decades , while for outlier detection it has only been studied recently . In this work , we design a new ensemble approach for outlier detection in multi-dimensional point data , which provides improved accuracy by reducing error through both bias and variance by considering outlier detection as a binary classification task with unobserved labels . In this paper , we propose a sequential ensemble approach called CARE that employs a two-phase aggregation of the intermediate results in each iteration to reach the final outcome . Unlike existing outlier ensembles , our ensemble incorporates both the parallel and sequential building blocks to reduce bias as well as variance by ( i ) successively eliminating outliers from the original dataset to build a better data model on which outlierness is estimated ( sequentially ) , and ( ii ) combining the results from individual base detectors and across iterations ( parallelly ) . Through extensive experiments on 16 real-world datasets mainly from the UCI machine learning repository { [ } 1 ] , we show that CARE performs significantly better than or at least similar to the individual baselines as well as the existing state-of-the-art outlier ensembles .
2K_dev_1924	An active learning method for identifying drug-target interactions is presented which considers the interaction between multiple drugs and multiple targets at the same time . The goal of the proposed method is not simply to predict such interactions from experiments that have already been conducted , but to iteratively choose as few new experiments as possible to improve the accuracy of the predictive model . Kernelized Bayesian matrix factorization ( KBMF ) is used to model the interactions . We demonstrate on four previously characterized drug effect data sets that active learning driven experimentation using KBMF can result in highly accurate models while performing as few as 14\ % of the possible experiments , and more accurately than random sampling of an equivalent number . We also provide a method for estimating the accuracy of the current model based on the learning curve ; and show how it can be used in practice to decide when to stop an active learning process .
2K_dev_1925	In this paper we present a method to improve the automatic detection of events in short sentences when in the presence of a large number of event classes . Contrary to standard classification techniques such as Support Vector Machines or Random Forest , the proposed Fuzzy Fingerprints method is able to detect all the event classes present in the ACE 2005 Multilingual Corpus , and largely improves the obtained G-Mean value .
2K_dev_1926	This paper presents a theoretical and experimental comparison of sound proof rules for proving invariance of algebraic sets , that is , sets satisfying polynomial equalities , under the flow of polynomial ordinary differential equations . Problems of this nature arise in formal verification of continuous and hybrid dynamical systems , where there is an increasing need for methods to expedite formal proofs . We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of heterogeneous benchmarks . The relationship between increased deductive power and running time performance of the proof rules is far from obvious ; we discuss and illustrate certain classes of problems where this relationship is interesting .
2K_dev_1927	The segmentation is the first step and core technology for semantic understanding of the video . Many tasks in the computer vision such as tracking , recognition and 3D reconstruction , etc . rely on the segmentation result as preprocessing . However , the video segmentation has been known to be a very complicated and hard problem . The objects in the video change their colors and shapes according to the surrounding illumination , the camera position , or the object motion . The color , motion , or depth has been utilized individually as a key clue for the segmentation in many researches . However , every object in the image is composed of several features such as color , texture , depth and motion . That is why single-feature based segmentation method often fails . Humans can segment the objects in video with ease because the human visual system enables to consider color , texture , depth and motion at the same time . In this paper , we propose the video segmentation algorithm which is motivated by the human visual system . The algorithm performs the video segmentation task by simultaneously utilizing the color histogram of the color , the optical flow of the motion , and the homography of the structure . Our results show that the proposed algorithm outperforms other appearance based segmentation method in terms of semantic quality of the segmentation { [ } 15 ] . The proposed segmentation method will serve as a basis for better high-level tasks such as recognition , tracking { [ } 3 ] , { [ } 4 ] and video understanding { [ } 1 ] .
2K_dev_1928	Many data structures ( e.g. , matrices ) are typically accessed with multiple access patterns . Depending on the layout of the data structure in physical address space , some access patterns result in non-unit strides . In existing systems , which are optimized to store and access cache lines , non-unit strided accesses exhibit low spatial locality . Therefore , they incur high latency , and waste memory bandwidth and cache space . We propose the Gather-Scatter DRAM ( GS-DRAM ) to address this problem . We observe that a commodity DRAM module contains many chips . Each chip stores a part of every cache line mapped to the module . Our idea is to enable the memory controller to access multiple values that belong to a strided pattern from different chips using a single read/write command . To realize this idea , GS-DRAM first maps the data of each cache line to different chips such that multiple values of a strided access pattern are mapped to different chips . Second , instead of sending a separate address to each chip , GS-DRAM maps each strided pattern to a small pattern ID that is communicated to the module . Based on the pattern ID , each chip independently computes the address of the value to be accessed . The cache line returned by the module contains different values of the strided pattern gathered from different chips . We show that this approach enables GS-DRAM to achieve near-ideal memory bandwidth and cache utilization for many common access patterns . We design an end-to-end system to exploit GS-DRAM . Our evaluations show that 1 ) for in-memory databases , GS-DRAM obtains the best of the row store and the column store layouts , in terms of both performance and energy , and 2 ) for matrix-matrix multiplication , GS-DRAM seamlessly enables SIMD optimizations and outperforms the best tiled layout . Our framework is general , and can benefit many modern data-intensive applications .
2K_dev_1929	Though most would agree that accountability and privacy are both valuable , today 's Internet provides little support for either . Previous efforts have explored ways to offer stronger guarantees for one of the two , typically at the expense of the other ; indeed , at first glance accountability and privacy appear mutually exclusive . At the center of the tussle is the source address : in an accountable Internet , source addresses undeniably link packets and senders so hosts can be punished for bad behavior . In a privacy-preserving Internet , source addresses are hidden as much as possible . In this paper , we argue that a balance is possible . We introduce the Accountable and Private Internet Protocol ( APIP ) , which splits source addresses into two separate fields - an accountability address and a return address - and introduces independent mechanisms for managing each . Accountability addresses , rather than pointing to hosts , point to accountability delegates , which agree to vouch for packets on their clients ' behalves , taking appropriate action when misbehavior is reported . With accountability handled by delegates , senders are now free to mask their return addresses ; we discuss a few techniques for doing so .
2K_dev_1930	The growing number of sensor-based interactive applications and services are pushing the limits of the on-board computing resources in vehicles . With vehicles increasingly being connected to the Internet , offloading the computation to cloud-computing infrastructures is an attractive solution . However , the large sensory data inputs of interactive applications makes offloading challenging across dynamic network conditions , and different application requirements or policies . To address this challenge , we design a system to adaptively offload specific vehicular application components or modules to the cloud . We particularly develop heuristic mechanisms for the placement and scheduling of modules on the On-Board Unit ( OBU ) and a cloud server under dynamic networking conditions during driving . Through an experimental evaluation of the end-end application response time using our prototype vehicular cloud offloading system , we show that our mechanism can help meet application response time constraints .
2K_dev_1931	This paper describes the design and implementation of HERD , a key-value system designed to make the best use of an RDMA network . Unlike prior RDMA-based key-value systems , HERD focuses its design on reducing network round trips while using efficient RDMA primitives ; the result is substantially lower latency , and throughput that saturates modern , commodity RDMA hardware . HERD has two unconventional decisions : First , it does not use RDMA reads , despite the allure of operations that bypass the remote CPU entirely . Second , it uses a mix of RDMA and messaging verbs , despite the conventional wisdom that the messaging primitives are slow . A HERD client writes its request into the server 's memory ; the server computes the reply . This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5 mu s average latency . Notably , for small key-value items , our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems . We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services .
2K_dev_1932	TTL caching models have recently regained significant research interest due to their connection to popular caching policies such as LRU . This paper advances the state-of-the-art analysis of TTL-based cache networks by developing two exact methods with orthogonal generality and computational complexity . The first method generalizes existing results for line networks under renewal requests to the broad class of caching policies whereby evictions are driven by stopping times ; in addition to classical policies used in DNS and web caching , our stopping time model captures an emerging new policy implemented in SON switches and Amazon web services . The second method further generalizes these results to feedforward networks with Markov arrival process ( MAP ) requests . MAPs are particularly suitable for non-line networks because they are closed not only under superposition and splitting , as known , but also under caching operations with phase-type ( PH ) TTL distributions . The crucial benefit of the two closure properties is that they jointly enable the first exact analysis of TTL feedforward cache networks in great generality . Moreover , numerical results highlight that existing Poisson approximations in binary-tree topologies are subject to relative errors as large as 30\ % , depending on the tree depth . ( C ) 2014 Elsevier B.V. All rights reserved .
2K_dev_1933	To date , the study of dispatching or load balancing in server farms has primarily focused on the minimization of response time . Server farms are typically modeled by a front-end router that employs a dispatching policy to route jobs to one of several servers , with each server scheduling all the jobs in its queue via Processor-Sharing . However , the common assumption has been that all jobs are equally important or valuable , in that they are equally sensitive to delay . Our work departs from this assumption : we model each arrival as having a randomly distributed value parameter , independent of the arrival 's service requirement ( job size ) . Given such value heterogeneity , the correct metric is no longer the minimization or response time , but rather , the minimization of value-weighted response time . In this context , we ask `` what is a good dispatching policy to minimize the value-weighted response time metric ? { '' } We propose a number of new dispatching policies that are motivated by the goal of minimizing the value-weighted response time . Via a combination of exact analysis , asymptotic analysis , and simulation , we are able to deduce many unexpected results regarding dispatching . ( C ) 2014 Elsevier B.V. All rights reserved .
2K_dev_1934	We have designed a new logic programming language called LM ( Linear Meld ) for programming graph- based algorithms in a declarative fashion . Our language is based on linear logic , an expressive logical system where logical facts can be consumed . Because LM integrates both classical and linear logic , LM tends to be more expressive than other logic programming languages . LM programs are naturally concurrent because facts are partitioned by nodes of a graph data structure . Computation is performed at the node level while communication happens between connected nodes . In this paper , we present the syntax and operational semantics of our language and illustrate its use through a number of examples .
2K_dev_1935	It is hard to efficiently model the light transport in scenes with translucent objects for interactive applications . The inter-reflection between objects and their environments and the subsurface scattering through the materials intertwine to produce visual effects like color bleeding , light glows , and soft shading . Monte-Carlo based approaches have demonstrated impressive results but are computationally expensive , and faster approaches model either only inter-reflection or only subsurface scattering . In this paper , we present a simple analytic model that combines diffuse inter-reflection and isotropic subsurface scattering . Our approach extends the classical work in radiosity by including a subsurface scattering matrix that operates in conjunction with the traditional form factor matrix . This subsurface scattering matrix can be constructed using analytic , measurement-based or simulation-based models and can capture both homogeneous and heterogeneous translucencies . Using a fast iterative solution to radiosity , we demonstrate scene relighting and dynamically varying object translucencies at near interactive rates .
2K_dev_1936	Motivation : Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the implications of transcriptional fluctuations on metabolism . The reporter algorithm ( RA ) was proposed to determine the hot spots in metabolic networks , around which transcriptional regulation is focused owing to a disease or a genetic perturbation . Using a z-score-based scoring scheme , RA calculates the average statistical change in the expression levels of genes that are neighbors to a target metabolite in the metabolic network . The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes . In this article , we propose a mutual information-based multivariate reporter algorithm ( MIRA ) with the goal of eliminating the following problems in detecting reporter metabolites : ( i ) conventional statistical methods suffer from small sample sizes , ( ii ) as z-score ranges from minus to plus infinity , calculating average scores can lead to canceling out opposite effects and ( iii ) analyzing genes one by one , then aggregating results can lead to information loss . MIRA is a multivariate and combinatorial algorithm that calculates the aggregate transcriptional response around a metabolite using mutual information . We show that MIRA 's results are biologically sound , empirically significant and more reliable than RA . Results : We apply MIRA to gene expression analysis of six knockout strains of Escherichia coli and show that MIRA captures the underlying metabolic dynamics of the switch from aerobic to anaerobic respiration . We also apply MIRA to an Autism Spectrum Disorder gene expression dataset . Results indicate that MIRA reports metabolites that highly overlap with recently found metabolic biomarkers in the autism literature . Overall , MIRA is a promising algorithm for detecting metabolic drug targets and understanding the relation between gene expression and metabolic activity .
2K_dev_1937	The aim of AEMINIUM is to study the implications of having a concurrent-by-default programming language . This includes language design , runtime system , performance and software engineering considerations . We conduct our study through the design of the concurrent-by-default AEMINIUM programming language . AEMINIUM leverages the permission flow of object and group permissions through the program to validate the program 's correctness and to automatically infer a possible parallelization strategy via a dataflow graph . AEMINIUM supports not only fork-join parallelism but more general dataflow patterns of parallelism . In this paper we present a formal system , called mu AEMINIUM , modeling the core concepts of AEMINIUM . mu AEMINIUM 's static type system is based on Featherweight Java with AEMINIUM-specific extensions . Besides checking for correctness AEMINIUM 's type system it also uses the permission flow to compute a potential parallel execution strategy for the program . mu AEMINIUM 's dynamic semantics use a concurrent-by-default evaluation approach . Along with the formal system we present its soundness proof . We provide a full description of the implementation along with the description of various optimization techniques we used . We implemented AEMINIUM as an extension of the Plaid programming language , which has first-class support for permissions built-in . The AEMINIUM implementation and all case studies are publicly available under the General Public License . We use various case studies to evaluate AEMINIUM 's applicability and to demonstrate that AEMINIUM parallelized code has performance improvements compared to its sequential counterpart . We chose to use case studies from common domains or problems that are known to benefit from parallelization , to show that AEMINIUM is powerful enough to encode them . We demonstrate through a webserver application , which evaluates AEMINIUM 's impact on latency-bound applications , that AEMINIUM can achieve a 70\ % performance improvement over the sequential counterpart . In another case study we chose to implement a dictionary function to evaluate AEMINIUM 's capabilities to express essential data structures . Our evaluation demonstrates that AEMINIUM can be used to express parallelism in such data-structures and that the performance benefits scale with the amount of annotation effort which is put into the implementation . We chose an integral computationally example to evaluate pure functional programming and computational intensive use cases . Our experiments show that AEMINIUM is capable of extracting parallelism from functional code and achieving performance improvements up to the limits of Plaid 's inherent performance bounds . Overall , we hope that the work helps to advance concurrent programming in modern programming environments .
2K_dev_1938	Vehicular multi-hop protocols typically employ distance-based metrics , which do not capture the complexity of vehicular connectivity . In this work we present LASP , a geographic protocol that uses a more accurate spatial connectivity-based metric . Spatial connectivity describes the historical probability of successfully delivering a packet from one geographic area to another . Analysis of data collected from a vehicular testbed showed that , unlike other metrics , spatial connectivity indirectly captures all major factors affecting wireless connectivity . Moreover , it is temporally stable , which makes it useful in estimating the quality of future co-located links . When forwarding , LASP uses spatial connectivity information to pick a well-connected geographic forwarding zone , inside which multiple nodes cooperate in relaying through a distributed prioritization scheme . Compared with other techniques where the sender picks a specific next hop relay , cooperative forwarding improves resilience to losses through vehicle diversity . We evaluated LASP on a 30-node testbed , where it achieved a 30\ % increase in packet delivery ratio over the benchmark GPSR protocol .
2K_dev_1939	Security requirements patterns represent reusable security practices that software engineers can apply to improve security in their system . Reusing best practices that others have employed could have a number of benefits , such as decreasing the time spent in the requirements elicitation process or improving the quality of the product by reducing product failure risk . Pattern selection can be difficult due to the diversity of applicable patterns from which an analyst has to choose . The challenge is that identifying the most appropriate pattern for a situation can be cumbersome and time-consuming . We propose a new method that combines an inquiry-cycle based approach with the feature diagram notation to review only relevant patterns and quickly select the most appropriate patterns for the situation . Similar to patterns themselves , our approach captures expert knowledge to relate patterns based on decisions made by the pattern user . The resulting pattern hierarchies allow users to be guided through these decisions by questions , which introduce related patterns in order to help the pattern user select the most appropriate patterns for their situation , thus resulting in better requirement generation . We evaluate our approach using access control patterns in a pattern user study .
2K_dev_1940	Government laws and regulations increasingly place requirements on software systems . Ideally , experts trained in law will analyze and interpret legal texts to inform the software requirements process . However , in small companies and development teams with short launch cycles , individuals with little or no legal training will be responsible for compliance . Two specific challenges commonly faced by non-experts are deciding if their system is covered by a law , and then deciding whether two legal requirements are similar or different . In this study , we assess the ability of laypersons , technical professionals , and legal experts to judge the similarity between legal coverage conditions and requirements . In so doing , we discovered that legal experts achieved higher rates of consensus more frequently than technical professionals or laypersons and that all groups had slightly greater agreement when judging coverage conditions than requirements , measured by Fleiss ' K. When comparing judgments between groups using a consensus-based Cohen 's Kappa , we found that technical professionals and legal experts exhibited consistently greater agreement than that found between laypersons and legal experts , and that each group tended towards different justifications , such as laypersons and technical professionals tendency towards categorizing different coverage conditions or requirements as equivalent if they believed them to possess the same underlying intent .
2K_dev_1941	Many traditional challenges in reconstructing 3D motion , such as matching across wide baselines and handling occlusion , reduce in significance as the number of unique viewpoints increases . However , to obtain this benefit , a new challenge arises : estimating precisely which cameras observe which points at each instant in time . We present a maximum a posteriori ( MAP ) estimate of the time-varying visibility of the target points to reconstruct the 3D motion of an event from a large number of cameras . Our algorithm takes , as input , camera poses and image sequences , and outputs the time-varying set of the cameras in which a target patch is visibile and its reconstructed trajectory . We model visibility estimation as a MAP estimate by incorporating various cues including photometric consistency , motion consistency , and geometric consistency , in conjunction with a prior that rewards consistent visibilities in proximal cameras . An optimal estimate of visibility is obtained by finding the minimum cut of a capacitated graph over cameras . We demonstrate that our method estimates visibility with greater accuracy , and increases tracking performance producing longer trajectories , at more locations , and at higher accuracies than methods that ignore visibility or use photometric consistency alone .
2K_dev_1942	Curse of dimensionality is a practical and challenging problem in image categorization , especially in cases with a large number of classes . Multi-class classification encounters severe computational and storage problems when dealing with these large scale tasks . In this paper , we propose hierarchical feature hashing to effectively reduce dimensionality of parameter space without sacrificing classification accuracy , and at the same time exploit information in semantic taxonomy among categories . We provide detailed theoretical analysis on our proposed hashing method . Moreover , experimental results on object recognition and scene classification further demonstrate the effectiveness of hierarchical feature hashing .
2K_dev_1943	With the widespread availability of video cameras , we are facing an ever-growing enormous collection of unedited and unstructured video data . Due to lack of an automatic way to generate summaries from this large collection of consumer videos , they can be tedious and time consuming to index or search . In this work , we propose online video highlighting , a principled way of generating short video summarizing the most important and interesting contents of an unedited and unstructured video , costly both time-wise and financially for manual processing . Specifically , our method learns a dictionary from given video using group sparse coding , and updates atoms in the dictionary on-the-fly . A summary video is then generated by combining segments that can not be sparsely reconstructed using the learned dictionary . The online fashion of our proposed method enables it to process arbitrarily long videos and start generating summaries before seeing the end of the video . Moreover , the processing time required by our proposed method is close to the original video length , achieving quasi real-time summarization speed . Theoretical analysis , together with experimental results on more than 12 hours of surveillance and YouTube videos are provided , demonstrating the effectiveness of online video highlighting .
2K_dev_1944	This paper poses object category detection in images as a type of 2D-to-3D alignment problem , utilizing the large quantities of 3D CAD models that have been made publicly available online . Using the `` chair { '' } class as a running example , we propose an exemplar-based 3D category representation , which can explicitly model chairs of different styles as well as the large variation in viewpoint . We develop an approach to establish part-based correspondences between 3D CAD models and real photographs . This is achieved by ( i ) representing each 3D model using a set of view-dependent mid-level visual elements learned from synthesized views in a discriminative fashion , ( ii ) carefully calibrating the individual element detectors on a common dataset of negative images , and ( iii ) matching visual elements to the test image allowing for small mutual deformations but preserving the viewpoint and style constraints . We demonstrate the ability of our system to align 3D models with 2D objects in the challenging PASCAL VOC images , which depict a wide variety of chairs in complex scenes .
2K_dev_1945	In this paper , we investigate an approach for reconstructing storyline graphs from large-scale collections of Internet images , and optionally other side information such as friendship graphs . The storyline graphs can be an effective summary that visualizes various branching narrative structure of events or activities recurring across the input photo sets of a topic class . In order to explore further the usefulness of the storyline graphs , we leverage them to perform the image sequential prediction tasks , from which photo recommendation applications can benefit . We formulate the storyline reconstruction problem as an inference of sparse time-varying directed graphs , and develop an optimization algorithm that successfully addresses a number of key challenges of Web-scale problems , including global optimality , linear complexity , and easy parallelization . With experiments on more than 3.3 millions of images of 24 classes and user studies via Amazon Mechanical Turk , we show that the proposed algorithm improves other candidate methods for both storyline reconstruction and image prediction tasks .
2K_dev_1946	In this paper , we address the problem of jointly summarizing large sets of Flickr images and YouTube videos . Starting from the intuition that the characteristics of the two media types are different yet complementary , we develop a fast and easily-parallelizable approach for creating not only high-quality video summaries but also novel structural summaries of online images as storyline graphs . The storyline graphs can illustrate various events or activities associated with the topic in a form of a branching network . The video summarization is achieved by diversity ranking on the similarity graphs between images and video frames . The reconstruction of storyline graphs is formulated as the inference of sparse time-varying directed graphs from a set of photo streams with assistance of videos . For evaluation , we collect the datasets of 20 outdoor activities , consisting of 2.7M Flickr images and 16K YouTube videos . Due to the large-scale nature of our problem , we evaluate our algorithm via crowdsourcing using Amazon Mechanical Turk . In our experiments , we demonstrate that the proposed joint summarization approach outperforms other baselines and our own methods using videos or images only .
2K_dev_1947	There is considerable interest in moving DBMS applications from inside enterprise data centers to the cloud , both to reduce cost and to increase flexibility and elasticity . Some of these applications are `` green field { '' } projects ( i.e. , new applications ) ; others are existing legacy systems that must be migrated to the cloud . In another dimension , some are decision support applications while others are update-oriented . In this paper , we discuss the technical and political challenges that these various enterprise applications face when considering cloud deployment . In addition , a requirement for quality-of-service ( QoS ) guarantees will generate additional disruptive issues . In some circumstances , achieving good DBMS performance on current cloud architectures and future hardware technologies will be non-trivial . In summary , there is a difficult road ahead for enterprise database applications .
2K_dev_1948	Web services offer a more reliable and efficient way to access online data than scraping web pages . However , web service data are often in complex hierarchical structures that make it difficult for people to extract the desired parts or to perform any further data manipulation without writing a significant amount of surprisingly intricate code . In this paper , we present Gneiss , a tool that extends the familiar spreadsheet metaphor to support working with data returned from web services . Gneiss allows users to extract the desired fields in web service data using drag-and-drop , and refine the results through spreadsheet formulas , along with sorting and filtering the data . Hierarchical data are stored as nested tables in the spreadsheet and can be flattened for future operations . Data flow is two-way between the spreadsheet and the web services , enabling people to easily make a new request by modifying spreadsheet cells . In addition , using the dependency between spreadsheet cells , our tool is able to create parallel-running data extractions based on the user 's sequential demonstration . We use a set of examples to demonstrate our tool 's ability to create fast and reusable data extraction and manipulation programs that work with complex web service data .
2K_dev_1949	Research shows that commonly accepted security requirements are not generally applied in practice . Instead of relying on requirements checklists , security experts rely on their expertise and background knowledge to identify security vulnerabilities . To understand the gap between available checklists and practice , we conducted a series of interviews to encode the decision-making process of security experts and novices during security requirements analysis . Participants were asked to analyze two types of artifacts : source code , and network diagrams for vulnerabilities and to apply a requirements checklist to mitigate some of those vulnerabilities . We framed our study using Situation Awareness-a cognitive theory from psychology-to elicit responses that we later analyzed using coding theory and grounded analysis . We report our preliminary results of analyzing two interviews that reveal possible decision-making patterns that could characterize how analysts perceive , comprehend and project future threats which leads them to decide upon requirements and their specifications , in addition , to how experts use assumptions to overcome ambiguity in specifications . Our goal is to build a model that researchers can use to evaluate their security requirements methods against how experts transition through different situation awareness levels in their decision-making process .
2K_dev_1950	The use of shared mutable state , commonly seen in object-oriented systems , is often problematic due to the potential conflicting interactions between aliases to the same state . We present a substructural type system outfitted with a novel lightweight interference control mechanism , rely-guarantee protocols , that enables controlled aliasing of shared resources . By assigning each alias separate roles , encoded in a novel protocol abstraction in the spirit of rely-guarantee reasoning , our type system ensures that challenging uses of shared state will never interfere in an unsafe fashion . In particular , rely-guarantee protocols ensure that each alias will never observe an unexpected value , or type , when inspecting shared memory regardless of how the changes to that shared state ( originating from potentially unknown program contexts ) are interleaved at run-time .
2K_dev_1951	Formal verification and validation play a crucial role in making cyberphysical systems ( CPS ) safe . Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained , including models of the controller and of the physical dynamics . In CPS , models are essential ; but any model we could possibly build necessarily deviates from the real world . If the real system fits to the model , its behavior is guaranteed to satisfy the correctness properties verified w.r.t . the model . Otherwise , all bets are off . This paper introduces ModelPlex , a method ensuring that verification results about models apply to CPS implementations . ModelPlex provides correctness guarantees for CPS executions at runtime : it combines offline verification of CPS models with runtime validation of system executions for compliance with the model . ModelPlex ensures that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model , assuming the system dynamics deviation is bounded . If , at some point , the observed behavior no longer complies with the model so that offline verification results no longer apply , ModelPlex initiates provably safe fallback actions . This paper , furthermore , develops a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic .
2K_dev_1952	We present Marvin , a system that can search physical objects using a mobile or wearable device . It integrates HOG-based object recognition , SURF-based localization information , automatic speech recognition , and user feedback information with a probabilistic model to recognize the `` object of interest { '' } at high accuracy and at interactive speeds . Once the object of interest is recognized , the information that the user is querying , e.g . reviews , options , etc. , is displayed on the user 's mobile or wearable device . We tested this prototype in a real-world retail store during business hours , with varied degree of background noise and clutter . We show that this multi-modal approach achieves superior recognition accuracy compared to using a vision system alone , especially in cluttered scenes where a vision system would be unable to distinguish which object is of interest to the user without additional input . It is computationally able to scale to large numbers of objects by focusing compute-intensive resources on the objects most likely to be of interest , inferred from user speech and implicit localization information . We present the system architecture , the probabilistic model that integrates the multi-modal information , and empirical results showing the benefits of multi-modal integration .
2K_dev_1953	We consider the problem of discovering discriminative exemplars suitable for object detection . Due to the diversity in appearance in real world objects , an object detector must capture variations in scale , viewpoint , illumination etc . The current approaches do this by using mixtures of models , where each mixture is designed to capture one ( or a few ) axis of variation . Current methods usually rely on heuristics to capture these variations ; however , it is unclear which axes of variation exist and are relevant to a particular task . Another issue is the requirement of a large set of training images to capture such variations . Current methods do not scale to large training sets either because of training time complexity I I or test time complexity I I . In this work , we explore the idea of compactly capturing task-appropriate variation from the data itself We propose a two stage data-driven process , which selects and learns a compact set of exemplar models for object detection . These selected models have an inherent ranking , which can be used for anytime/budgeted detection scenarios . Another benefit of our approach ( beyond the computational speedup ) is that the selected set of exemplar models performs better than the entire set .
2K_dev_1954	When people observe and interact with physical spaces , they are able to associate functionality to regions in the environment . Our goal is to automate dense functional understanding of large spaces by leveraging sparse activity demonstrations recorded from an ego-centric viewpoint . The method we describe enables functionality estimation in large scenes where people have behaved , as well as novel scenes where no behaviors are observed . Our method learns and predicts `` Action Maps { '' } , which encode the ability for a user to perform activities at various locations . With the usage of an egocentric camera to observe human activities , our method scales with the size of the scene without the need for mounting multiple static surveillance cameras and is well-suited to the task of observing activities up-close . We demonstrate that by capturing appearance-based attributes of the environment and associating these attributes with activity demonstrations , our proposed mathematical framework allows for the prediction of Action Maps in new environments . Additionally , we offer a preliminary glance of the applicability of Action Maps by demonstrating a proof-of-concept application in which they are used in concert with activity detections to perform localization .
2K_dev_1955	Growing traffic volumes and the increasing complexity of attacks pose a constant scaling challenge for network intrusion prevention systems ( NIPS ) . In this respect , offloading NIPS processing to compute clusters offers an immediately deployable alternative to expensive hardware upgrades . In practice , however , NIPS offloading is challenging on three fronts in contrast to passive network security functions : ( 1 ) NIPS offloading can impact other traffic engineering objectives ; ( 2 ) NIPS offloading impacts user perceived latency ; and ( 3 ) NIPS actively change traffic volumes by dropping unwanted traffic . To address these challenges , we present the SNIPS system . We design a formal optimization framework that captures tradeoffs across scalability , network load , and latency . We provide a practical implementation using recent advances in software-defined networking without requiring modifications to NIPS hardware . Our evaluations on realistic topologies show that SNIPS can reduce the maximum load by up to 10x while only increasing the latency by 2\ % .
2K_dev_1956	This paper studies the problem of matching images captured from an unmanned ground vehicle ( UGV ) to those from a satellite or high-flying vehicle . We focus on situations where the UGV navigates in remote areas with few man-made structures . This is a difficult problem due to the drastic change in perspective between the ground and aerial imagery and the lack of environmental features for image comparison . We do not rely on GPS , which may be jammed or uncertain . We propose a two-step approach : ( 1 ) the UGV images are warped to obtain a bird 's eye view of the ground , and ( 2 ) this view is compared to a grid of satellite locations using whole-image descriptors . We analyze the performance of a variety of descriptors for different satellite map sizes and various terrain and environment types . We incorporate the air-ground matching into a particle-filter framework for localization using the best-performing descriptor . The results show that vision-based UGV localization from satellite maps is not only possible , but often provides better position estimates than GPS estimates , enabling us to improve the location estimates of Google Street View .
2K_dev_1957	State lattice-based planning has been used in navigation for ground , water , aerial and space robots . State lattices are typically constructed of simple motion primitives connecting one state to another . There are situations where these metric motions may not be available , such as in GPS-denied areas . In many of these cases , however , the robot may have some additional sensing capability that is not being fully utilized by the planner . For example , if the robot has a camera it may be able to use simple visual servoing techniques to navigate through a GPS-denied region . Likewise , a LIDAR may allow the robot to skirt along an environmental feature even if there is not enough information to generate an accurate pose estimate . In this paper we present an expansion of the state lattice framework that allows us to incorporate controller-based motion primitives and external perceptual triggers directly into the planning process . We provide a formal description of our method of constructing the search graph in these cases as well as presenting real-world and simulated testing data showing the practical application of this approach .
2K_dev_1958	This paper introduces typy , a statically typed programming language embedded by reflection into Python . typy features a fragmentary semantics , i.e . it delegates semantic control over each term , drawn from Python 's fixed concrete and abstract syntax , to some contextually relevant user-defined semantic fragment . The delegated fragment programmatically 1 ) typechecks the term ( following a bidirectional protocol ) ; and 2 ) assigns dynamic meaning to the term by computing a translation to Python . We argue that this design is expressive with examples of fragments that express the static and dynamic semantics of 1 ) functional records ; 2 ) labeled sums ( with nested pattern matching a la ML ) ; 3 ) a variation on JavaScript 's prototypal object system ; and 4 ) typed foreign interfaces to Python and OpenCL . These semantic structures are , or would need to be , defined primitively in conventionally structured languages . We further argue that this design is compositionally well-behaved . It avoids the expression problem and the problems of grammar composition because the syntax is fixed . Moreover , programs are semantically stable under fragment composition ( i.e . defining a new fragment will not change the meaning of existing program components . )
2K_dev_1959	The field of object detection has made significant advances riding on the wave of region-based ConvNets , but their training procedure still includes many heuristics and hyperparameters that are costly to tune . We present a simple yet surprisingly effective online hard example mining ( OHEM ) algorithm for training region-based ConvNet detectors . Our motivation is the same as it has always been detection datasets contain an overwhelming number of easy examples and a small number of hard examples . Automatic selection of these hard examples can make training more effective and efficient . OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use . But more importantly , it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012 . Its effectiveness increases as datasets become larger and more difficult , as demonstrated by the results on the MS COCO dataset . Moreover , combined with complementary advances in the field , OHEM leads to state-of-the-art results of 78.9\ % and 76.3\ % mAP on PASCAL VOC 2007 and 2012 respectively .
2K_dev_1960	In this paper , we present both centralized and distributed algorithms for aligning coordinate frames in multi-robot systems based on inter-robot relative position measurements . Robot orientations are not measured , but are computed by our algorithms . Our algorithms are robust to measurement error and are useful in applications where a group of robots need to establish a common coordinate frame based on relative sensing information . The problem of establishing a common coordinate frame is formulated in a least squares error framework minimizing the total inconsistency of the measurements . We assume that robots that can sense each other can also communicate with each other . In this paper , our key contribution is a novel asynchronous distributed algorithm for multi-robot coordinate frame alignment that does not make any assumptions about the sensor noise model . After minimizing the least squares error ( LSE ) objective for coordinate frame alignment of two robots , we develop a novel algorithm that outperforms state-of-the-art centralized optimization algorithms for minimizing the LSE objective . Furthermore , we prove that for multi-robot systems ( a ) with redundant noiseless relative sensing information , we will achieve the globally optimal solution ( this is non-trivial because the LSE objective is nonconvex for our problem ) , ( b ) with noisy information but no redundant sensing ( e. g. sensing graph has a tree topology ) , our algorithm will optimally minimize the LSE objective . We also present preliminary results of the real-world performance of our algorithm on TurtleBots equipped with Kinect sensors .
2K_dev_1961	When one uses a hand-held tool , the fingers often make the tool to be in contact with the palm in the form of multi-contact manipulation . Multi-contact manipulation is useful for object-environment interaction tasks because it can provide both powerful grasping of the object body and dexterous manipulation of the object end-effector . However , dealing with the internal link contact with the object is not trivial . In this paper , we propose Reactional Internal Contact Hypothesis that regards the internal contact force as a reaction force so that the desired finger force can be reduced . By taking a handwriting task as an example , optimal configuration search and grasping force computation problems are addressed based on this hypothesis and validated via dynamic simulation .
2K_dev_1962	This paper presents a technique for automated intraocular laser surgery using a handheld micromanipulator known as Micron . The novel handheld manipulator enables the automated scanning of a laser probe within a cylinder of 4 mm long and 4 mm in diameter . For the automation , the surface of the retina is reconstructed using a stereomicroscope , and then preplanned targets are placed on the surface . The laser probe is precisely located on the target via visual servoing of the aiming beam , while maintaining a specific distance above the surface . In addition , the system is capable of tracking the surface of the eye in order to compensate for any eye movement introduced during the operation . We compared the performance of the automated scanning using various control thresholds , in order to find the most effective threshold in terms of accuracy and speed . Given the selected threshold , we conducted the handheld operation above a fixed target surface . The average error and execution time are reduced by 63.6\ % and 28.5\ % , respectively , compared to the unaided trials . Finally , the automated laser photocoagulation was demonstrated also in an eye phantom , including compensation for the eye movement .
2K_dev_1963	Transfemoral amputees often suffer from falls and a fear of falling that leads to a decreased quality of life . Existing control strategies for powered knee-ankle prostheses demonstrate only limited ability to react to disturbances that induce falls such as trips , slips , and obstacles . In contrast , prior work on neuromuscular modeling of human locomotion suggests that control strategies based on local reflexes exhibit robustness to unobserved terrain such as slopes and steps . Therefore , we propose that a powered knee-ankle prosthesis governed by reflexive local controls will more competently adapt to unperceived disturbances . To test this hypothesis , we simulate a neuromuscular model of a transfemoral amputee walking over rough ground with a powered knee-ankle prosthesis governed by the proposed reflexive controller . We show that the proposed control allows the amputee to walk farther over rough ground than does the state-of-the-art control . The proposed controller also more readily rejects deviations from nominal walking gaits such as those encountered during a trip . These results suggest that applying the proposed control to a powered knee-ankle prosthesis will substantially improve amputee gait stability .
2K_dev_1964	Control design of running robots is often based on mapping the behavior of lower order models onto the robotic systems , and the robustness of running is largely determined by the robustness of these underlying models . However , existing implementations do not take full advantage of the stability that the low order models can provide . In particular , analysis of the theoretical spring mass model suggests leg placement policies that generate near deadbeat rejection of large , unobserved changes in ground height . Here we show in simulation that this blind robustness to rough terrain can be carried over to bipedal robots . We design a control that stably embeds the spring mass model 's behavior in a planar robot model and show that resulting system rejects ground disturbances of up to 25\ % leg length , adapts to persistent ground slopes , and tolerates sensor noise , signal delays , and modeling errors . The results indicate that transferring control derived within the spring mass model is an effective technique for realizing highly robust running in robotic systems .
2K_dev_1965	Current GPS-based devices have difficulty localizing in cases where the GPS signal is unavailable or insufficiently accurate . This paper presents an algorithm for localizing a vehicle on an arbitrary road network using vision , road curvature estimates , or a combination of both . The method uses an extension of topometric localization , which is a hybrid between topological and metric localization . The extension enables localization on a network of roads rather than just a single , non-branching route . The algorithm , which does not rely on GPS , is able to localize reliably in situations where GPS-based devices fail , including `` urban canyons { '' } in downtown areas and along ambiguous routes with parallel roads . We demonstrate the algorithm experimentally on several road networks in urban , suburban , and highway scenarios . We also evaluate the road curvature descriptor and show that it is effective when imagery is sparsely available .
2K_dev_1966	Active illumination based methods have a trade-off between acquisition time and resolution of the estimated 3D shapes . Multi-shot approaches can generate dense reconstructions but require stationary scenes . In contrast , singleshot methods are applicable to dynamic objects but can only estimate sparse reconstructions and are sensitive to surface texture . In this work , we develop a single-shot approach to produce dense reconstructions of highly textured objects . The key to our approach is an image decomposition scheme that can recover the illumination and the texture images from their mixed appearance . Despite the complex appearances of the illuminated textured regions , our method can accurately compute per pixel warps from the illumination pattern and the texture template to the observed image . The texture template is obtained by interleaving the projection sequence with an all-white pattern . Our estimated warping functions are reliable even with infrequent interleaved projection . Thus , we obtain detailed shape reconstruction and dense motion tracking of the textured surfaces . We validate the approach on synthetic and real data containing subtle non-rigid surface deformations .
2K_dev_1967	In product design , designers often generate a large number of concepts in the form of sketches and drawings to develop and communicate their ideas . Concrete concepts typically evolve through a progressive refinement of initially coarse and ambiguous ideas . However , a lack of suitable means to visualize the emerging form at these early stages forces the designer to constantly maintain and negotiate an elusive mental image . To assist this process , we describe a predictive modeling technique that allows early , incomplete 2D sketches to be transformed into suggestive complete models . This helps designers take a sneak peek at the potential end result of a developing concept , without forcing them to commit to the suggestion . We demonstrate and discuss preliminary results of our technique on 2D shape design problems .
2K_dev_1968	In this paper we investigate 3D attributes as a means to understand the shape of an object in a single image . To this end , we make a number of contributions : ( i ) we introduce and define a set of 3D Shape attributes , including planarity , symmetry and occupied space ; ( ii ) we show that such properties can be successfully inferred from a single image using a Convolutional Neural Network ( CNN ) ; ( iii ) we introduce a 143K image dataset of sculptures with 2197 works over 242 artists for training and evaluating the CNN ; ( iv ) we show that the 3D attributes trained on this dataset generalize to images of other ( non-sculpture ) object classes ; and furthermore ( v ) we show that the CNN also provides a shape embedding that can be used to match previously unseen sculptures largely independent of viewpoint .
2K_dev_1969	Active intracellular cargo transport is essential to survival and function of eukaryotic cells . How this process is controlled spatially and temporally so that the right cargo is delivered to the right destination at the right time remains poorly understood . To address this question , it is essential to characterize and analyze the molecular machinery and spatiotemporal behavior of intracellular transport . To this end , we developed related computational image models . Specifically , to study the molecular machinery of intracellular transport , we developed anisotropic spatial density kernels for reconstruction and segmentation of related super-resolution STORM ( stochastic optical reconstruction microscopy ) images . To study the spatiotemporal behavior of intracellular transport , we developed hidden Markov models and principal component analysis for representation and analysis of movement of individual transported cargoes . We validated and benchmarked the image models using simulated and actual experimental images . The models and related computational analysis methods developed in this study are general and can be used for studying molecular machinery and spatiotemporal dynamics of other cellular processes .
2K_dev_1970	We explore the feasibility and performance of a data-driven approach to topology optimization problems involving structural mechanics . Our approach takes as input a set of images representing optimal 2-D topologies , each resulting from a random loading configuration applied to a common boundary support condition . These images represented in a high dimensional feature space are projected into a lower dimensional space using component analysis . Using the resulting components , a mapping between the loading configurations and the optimal topologies is learned . From this mapping , we estimate the optimal topologies for novel loading configurations . The results indicate that when there is an underlying structure in the set of existing solutions , the proposed method can successfully predict the optimal topologies in novel loading configurations . In addition , the topologies predicted by the proposed method can be used as effective initial conditions for conventional topology optimization routines , resulting in substantial performance gains . We discuss the advantages and limitations of the presented approach and show its performance on a number of examples .
2K_dev_1971	An increasing number of mobile devices are capable of automatically sensing and recording rich information about the surrounding environment . Spatial locations of such data can help to better learn about the environment . In this work , we address the problem of identifying the locations visited by a mobile device as it moves within an indoor environment . We focus on devices equipped with odometry sensors that capture changes in motion . Odometry suffers from cumulative errors of dead reckoning but it captures the relative shape of the traversed path well . Our approach will correct such errors by matching the shape of the trajectory from odometry to traversable paths of a known map . Our algorithm is inspired by prior vehicular GPS map matching techniques that snap global GPS measurements to known roads . We similarly wish to snap the trajectory from odometry to known hallways . Several modifications are required to ensure these techniques are robust when given relative measurements from odometry . If we assume an office-like environment with only straight hallways , then a significant rotation indicates a transition to another hallway . As a result , we partition the trajectory into line segments based on significant turns . Each trajectory segment is snapped to a corresponding hallway that best maintains the shape of the original trajectory . These snapping decisions are made based on the similarity of the two curves as well as the rotation to transition between hallways . We will show robustness under different types of noise in complex environments and the ability to propose coarse sensor noise errors .
2K_dev_1972	In this paper , we propose a novel planning framework that can greatly improve the level of intelligence and driving quality of autonomous vehicles . A reference planning layer first generates kinematically and dynamically feasible paths assuming no obstacles on the road , then a behavioral planning layer takes static and dynamic obstacles into account . Instead of directly commanding a desired trajectory , it searches for the best directives for the controller , such as lateral bias and distance keeping aggressiveness . It also considers the social cooperation between the autonomous vehicle and surrounding cars . Based on experimental results from both simulation and a real autonomous vehicle platform , the proposed behavioral planning architecture improves the driving quality considerably , with a 90.3\ % reduction of required computation time in representative scenarios .
2K_dev_1973	Localization is a central problem for intelligent vehicles . Visual localization can supplement or replace GPS-based localization approaches in situations where GPS is unavailable or inaccurate . Although visual localization has been demonstrated in a variety of algorithms and systems , the problem of how to best configure such a system remains largely an open question . Design choices , such as `` where should the camera be placed ? { '' } and `` how should it be oriented ? { '' } can have substantial effect on the cost and robustness of a fielded intelligent vehicle . This paper analyzes how different sensor configuration parameters and environmental conditions affect visual localization performance with the goal of understanding what causes certain configurations to perform better than others and providing general principles for configuring systems for visual localization . We ground the investigation using extensive field testing of a visual localization algorithm , and the data sets used for the analysis are made available for comparative evaluation .
2K_dev_1975	Illumination defocus and global illumination effects are major challenges for active illumination scene recovery algorithms . Illumination defocus limits the working volume of projector-camera systems and global illumination can induce large errors in shape estimates . In this paper , we develop an algorithm for scene recovery in the presence of both defocus and global light transport effects such as interreflections and sub-surface scattering . Our method extends the working volume by using structured light patterns at multiple projector focus settings . A careful characterization of projector blur allows us to decode even partially out-of-focus patterns . This enables our algorithm to recover scene shape and the direct and global illumination components over a large depth of field while still using a relatively small number of images ( typically 25-30 ) . We demonstrate the effectiveness of our approach by recovering high quality depth maps of scenes containing objects made of optically challenging materials such as wax , marble , soap , colored glass and translucent plastic .
2K_dev_1976	Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D point triangulation to reconstruct a static scene . The triangulation constraint however is invalid for moving points captured in multiple unsynchronized videos and bundle adjustment is not purposed to estimate the temporal alignment between cameras . In this paper , we present a spatiotemporal bundle adjustment approach that jointly optimizes four coupled sub-problems : estimating camera intrinsics and extrinsics , triangulating 3D static points , as well as subframe temporal alignment between cameras and estimating 3D trajectories of dynamic points . Key to our joint optimization is the careful integration of physics-based motion priors within the reconstruction pipeline , validated on a large motion capture corpus . We present an end-to-end pipeline that takes multiple uncalibrated and unsynchronized video streams and produces a dynamic reconstruction of the event . Because the videos are aligned with sub-frame precision , we reconstruct 3D trajectories of unconstrained outdoor activities at much higher temporal resolution than the input videos .
2K_dev_1977	Turbulence is studied extensively in remote sensing , astronomy , meteorology , aerodynamics and fluid dynamics . The strength of turbulence is a statistical measure of local variations in the turbulent medium . It influences engineering decisions made in these domains . Turbulence strength ( TS ) also affects safety of aircraft and tethered balloons , and reliability of free-space electromagnetic relays . We show that it is possible to estimate TS , without having to reconstruct instantaneous fluid flow fields . Instead , the TS field can be directly recovered , passively , using videos captured from different viewpoints . We formulate this as a linear tomography problem with a structure unique to turbulence fields . No tight synchronization between cameras is needed . Thus , realization is very simple to deploy using consumer-grade cameras . We experimentally demonstrate this both in a lab and in a large-scale uncontrolled complex outdoor environment , which includes industrial , rural and urban areas .
2K_dev_1978	In many behavioral domains , such as facial expression and gesture , sparse structure is prevalent . This sparsity would be well suited for event detection but for one problem . Features typically are confounded by alignment error in space and time . As a consequence , high-dimensional representations such as SIFT and Gabor features have been favored despite their much greater computational cost and potential loss of information . We propose a Kernel Structured Sparsity ( KSS ) method that can handle both the temporal alignment problem and the structured sparse reconstruction within a common framework , and it can rely on simple features . We characterize spatio-temporal events as time-series of motion patterns and by utilizing time-series kernels we apply standard structured-sparse coding techniques to tackle this important problem . We evaluated the KSS method using both gesture and facial expression datasets that include spontaneous behavior and differ in degree of difficulty and type of ground truth coding . KSS outperformed both sparse and non-sparse methods that utilize complex image features and their temporal extensions . In the case of early facial event classification KSS had 10\ % higher accuracy as measured by F-1 score over kernel SVM methods ( 1 ) .
2K_dev_1979	The primary goal of an automotive headlight is to improve safety in low light and poor weather conditions . But , despite decades of innovation on light sources , more than half of accidents occur at night even with less traffic on the road . Recent developments in adaptive lighting have addressed some limitations of standard headlights , however , they have limited flexibility - switching between high and low beams , turning off beams toward the opposing lane , or rotating the beam as the vehicle turns - and are not designed for all driving environments . This paper introduces an ultra-low latency reactive visual system that can sense , react , and adapt quickly to any environment while moving at highway speeds . Our single hardware design can be programmed to perform a variety of tasks . Anti-glare high beams , improved driver visibility during snowstorms , increased contrast of lanes , markings , and sidewalks , and early visual warning of obstacles are demonstrated .
2K_dev_1980	In this paper , we propose a method to parse human motion in unconstrained Internet videos without labeling any videos for training . We use the training samples from a public image pose dataset to avoid the tediousness of labeling video streams . There are two main problems confronted . First , the distribution of images and videos are different . Second , no temporal information is available in the training images . To smooth the inconsistency between the labeled images and unlabeled videos , our algorithm iteratively incorporates the pose knowledge harvested from the testing videos into the image pose detector via an adjust-and-refine method . During this process , continuity and tracking constraints are imposed to leverage the spatio-temporal information only available in videos . For our experiments , we have collected two datasets from YouTube and experiments show that our method achieves good performance for parsing human motions . Furthermore , we found that our method achieves better performance by using unlabeled video than adding more labeled pose images into the training set .
2K_dev_1981	Forecasting human activities from visual evidence is an emerging area of research which aims to allow computational systems to make predictions about unseen human actions . We explore the task of activity forecasting in the context of dual-agent interactions to understand how the actions of one person can be used to predict the actions of another . We model dual-agent interactions as an optimal control problem , where the actions of the initiating agent induce a cost topology over the space of reactive poses - a space in which the reactive agent plans an optimal pose trajectory . The technique developed in this work employs a kernel-based reinforcement learning approximation of the soft maximum value function to deal with the high-dimensional nature of human motion and applies a mean-shift procedure over a continuous cost function to infer a smooth reaction sequence . Experimental results show that our proposed method is able to properly model human interactions in a high dimensional space of human poses . When compared to several baseline models , results show that our method is able to generate highly plausible simulations of human interaction .
2K_dev_1982	The differential temporal dynamic logic dTL ( 2 ) is a logic to specify temporal properties of hybrid systems . It combines differential dynamic logic with temporal logic to reason about the intermediate states reached by a hybrid system . The logic dTL ( 2 ) supports some linear time temporal properties of LTL . It extends differential temporal dynamic logic dTL with nested temporalities . We provide a semantics and a proof system for the logic dTL ( 2 ) , and show its usefulness for nontrivial temporal properties of hybrid systems . We take particular care to handle the case of alternating universal dynamic and existential temporal modalities and its dual , solving an open problem formulated in previous work .
2K_dev_1983	In this work , we present a method for single-view reasoning about 3D surfaces and their relationships . We propose the use of midlevel constraints for 3D scene understanding in the form of convex and concave edges and introduce a generic framework capable of incorporating these and other constraints . Our method takes a variety of cues and uses them to infer a consistent interpretation of the scene . We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces .
2K_dev_1984	Cyber-physical systems ( CPS ) , which are computerized systems directly interfacing their real-world surroundings , leverage the construction of increasingly autonomous systems . To meet the high safety demands of CPS , verification of their behavior is crucial , which has led to a wide range of tools for modeling and verification of hybrid systems . These tools are often used in combination , because they employ a wide range of different formalisms for modeling , and aim at distinct verification goals and techniques . To manage and exchange knowledge in the verification process and to overcome a lack of a common classification , we unify different terminologies and concepts of a variety of modeling and verification tools in a conceptual reference model ( CRM ) . Furthermore , we illustrate how the CRM can support comparing models and propose future extension .
2K_dev_1985	Demographic information has a rich context from which to make decisions about how to filter or individualize computer users in forensic analysis . Although current explorations into technologies such as face and fingerprint analysis have seen varying rates of success , two main problems limit their applicability in the context of computer crimes : they can be intrusive , and they can require costly equipment . Our solution is to determine users ' demographic traits by analyzing the interactions between users and computers . We conducted a field study that gathered users ' keystroke and mouse data during interaction with a computer . From user interaction data , we extracted keystroke timing and mouse movement features , and developed weighted random forest classifiers for five demographic traits : gender , age , ethnicity , handedness , and language . Experiments showed that these demographics can be accurately inferred from user interaction behavior , with recognition rates expressed by the area under the ROC curve ( AUC ) ranging from 82.11 \ % to 87.32 \ % .
2K_dev_1986	Refactoring of code is a common device in software engineering . As cyber-physical systems ( CPS ) become ever more complex , similar engineering practices become more common in CPS development . Proper safe developments of CPS designs are accompanied by a proof of correctness . Since the inherent complexities of CPS practically mandate iterative development , frequent changes of models are standard practice , but require reverification of the resulting models after every change . To overcome this issue , we develop proof-aware refactorings for CPS . That is , we study model transformations on CPS and show how they correspond to relations on correctness proofs . As the main technical device , we show how the impact of model transformations on correctness can be characterized by different notions of refinement in differential dynamic logic . Furthermore , we demonstrate the application of refinements on a series of safety-preserving and liveness-preserving refactorings . For some of these we can give strong results by proving on a meta-level that they are correct . Where this is impossible , we construct proof obligations for showing that the refactoring respects the refinement relation .
2K_dev_1987	This paper investigates the effect of meta-cognitive help in the context of learning by teaching . Students learned to solve algebraic equations by tutoring a teachable agent , called SimStudent , using an online learning environment , called APLUS . A version of APLUS was developed to provide meta-cognitive help on what problems students should teach , as well as when to quiz SimStudent . A classroom study comparing APLUS with and without the metacognitive help was conducted with 173 seventh to ninth grade students . The data showed that students with the meta-cognitive help showed better problem selection and scored higher on the post-test than those who tutored SimStudent without the meta-cognitive help . These results suggest that , when carefully designed , learning by teaching can support students to not only learn cognitive skills but also employ meta-cognitive skills for effective tutoring .
2K_dev_1988	We bring together ideas from recent work on feature design for egocentric action recognition under one framework by exploring the use of deep convolutional neural networks ( CNN ) . Recent work has shown that features such as hand appearance , object attributes , local hand motion and camera ego-motion are important for characterizing first-person actions . To integrate these ideas under one framework , we propose a twin stream network architecture , where one stream analyzes appearance information and the other stream analyzes motion information . Our appearance stream encodes prior knowledge of the egocentric paradigm by explicitly training the network to segment hands and localize objects . By visualizing certain neuron activation of our network , we show that our proposed architecture naturally learns features that capture object attributes and hand-object configurations . Our extensive experiments on benchmark egocentric action datasets show that our deep architecture enables recognition rates that significantly outperform state-of-the-art techniques - an average 6.6\ % increase in accuracy over all datasets . Furthermore , by learning to recognize objects , actions and activities jointly , the performance of individual recognition tasks also increase by 30\ % ( actions ) and 14\ % ( objects ) . We also include the results of extensive ablative analysis to highlight the importance of network design decisions .
2K_dev_1989	Collaborative learning has been shown to be beneficial for older students , but there has not been much research to show if these results transfer to elementary school students . In addition , collaborative and individual modes of instruction may be better for acquiring different types of knowledge . Collaborative Intelligent Tutoring Systems ( ITS ) provide a platform that may be able to provide both the cognitive and collaborative support that students need . This paper presents a study comparing collaborative and individual methods while receiving instruction on either procedural or conceptual knowledge . The collaborative groups had the same learning gains as the individual groups in both the procedural and conceptual learning conditions but were able to do so with fewer problems . This work indicates that by embedding collaboration scripts in ITSs , collaborative learning can be an effective instructional method even with young children .
2K_dev_1990	The amount of data available to build simulation models of schools is immense , but using these data effectively is difficult . Traditional methods of computer modeling of educational systems often either lack transparency in their implementation , are complex , and often do not natively simulate nonlinear systems . In response , we advocate a Complex Adaptive Systems approach towards modeling and data mining . By simulating agent-level attributes rather than system-level attributes , the modeling is inherently transparent , easily adjustable , and facilitates analysis of the system due to the analogous nature of the simulated agents to real-world entities . We explore the design a CAS model of schools using multiple levels of data from varied data streams .
2K_dev_1991	Heterogeneity of wireless networks has become an increasing problem in the wireless spectrum that breaks down spectrum sharing and exacerbates interference . Many coexistence techniques have been proposed to alleviate this interference , however , they are difficult to deploy due to changes needed in the protocols , overhead , and rapid changes in technology . In this paper , we focus on the potential of spectrum management to provide a long-term solution . We introduce novel components to a spectrum management system that overcomes limitations of current models that have remained relatively focused on homogeneous environments . Our approach is a centralized one , where we analyze information collected from heterogeneous monitors available today , structure the information in a hypergraph , and perform an analysis to detect heterogeneous conflicts . Introducing a mixed integer program ( in addition to other novel components ) , we reconfigure devices in the spectrum to avoid conflicts and improve performance .
2K_dev_1992	When deployed in automated speech recognition ( ASR ) , deep neural networks ( DNNs ) can be treated as a complex feature extractor plus a simple linear classifier . Previous work has investigated the utility of multilingual DNNs acting as language-universal feature extractors ( LUFEs ) . In this paper , we explore different strategies to further improve LUFEs . First , we replace the standard sigmoid nonlinearity with the recently proposed maxout units . The resulting maxout LUFEs have the nice property of generating sparse feature representations . Second , the convolutional neural network ( CNN ) architecture is applied to obtain more invariant feature space . We evaluate the performance of LUFEs on a cross-language ASR task . Each of the proposed techniques results in word error rate reduction compared with the existing DNN-based LUFEs . Combining the two methods together brings additional improvement on the target language .
2K_dev_1993	Recent studies in computer vision have shown that , while practically invisible to a human observer , skin color changes due to blood flow can be captured on face videos and , surprisingly , be used to estimate the heart rate ( HR ) . While considerable progress has been made in the last few years , still many issues remain open . In particular , stateof- the-art approaches are not robust enough to operate in natural conditions ( e.g . in case of spontaneous movements , facial expressions , or illumination changes ) . Opposite to previous approaches that estimate the HR by processing all the skin pixels inside a fixed region of interest , we introduce a strategy to dynamically select face regions useful for robust HR estimation . Our approach , inspired by recent advances on matrix completion theory , allows us to predict the HR while simultaneously discover the best regions of the face to be used for estimation . Thorough experimental evaluation conducted on public benchmarks suggests that the proposed approach significantly outperforms state-of-the-art HR estimation methods in naturalistic conditions .
2K_dev_1994	Multilingual deep neural networks ( DNNs ) can act as deep feature extractors and have been applied successfully to cross language acoustic modeling . Learning these feature extractors becomes an expensive task , because of the enlarged multilingual training data and the sequential nature of stochastic gradient descent ( SGD ) . This paper investigates strategies to accelerate the learning process over multiple GPU cards . We propose the DistModel and DistLang frameworks which distribute feature extractor learning by models and languages respectively . The time-synchronous DistModel has the nice property of tolerating infrequent model averaging . With 3 GPUs , DistModel achieves 2.6x speed-up and causes no loss on word error rates . When using DistLang , we observe better acceleration but worse recognition performance . Further evaluations are conducted to scale DistModel to more languages and GPU cards .
2K_dev_1995	We show how to generate and validate logical proofs of unsatisfiability from delta-complete decision procedures that rely on error-prone numerical algorithms . Solving this problem is important for ensuring correctness of the decision procedures . At the same time , it is a new approach for automated theorem proving over real numbers . We design a first-order calculus , and transform the computational steps of constraint solving into logic proofs , which are then validated using proof-checking algorithms . As an application , we demonstrate how proofs generated from our solver can establish many nonlinear lemmas in the the formal proof of the Kepler Conjecture .
2K_dev_1996	Good communication is critical to seamless human-robot interaction . Among numerous communication channels , here we focus on gestures , and in particular on spacial deixis : pointing at objects in the environment in order to reference them . We propose a mathematical model that enables robots to generate pointing configurations that make the goal object as clear as possible - pointing configurations that are legible . We study the implications of legibility on pointing , e.g . that the robot will sometimes need to trade off efficiency for the sake of clarity . Finally , we test how well our model works in practice in a series of user studies , showing that the resulting pointing configurations make the goal object easier to infer for novice users .
2K_dev_1997	For compelling human-robot interaction , social gestures are widely believed to be important . This paper investigates the effects of adding gestures to a physical game between a human and a humanoid robot . Human participants repeatedly threw a ball to the robot , which attempted to catch it . If the catch was successful , the robot threw the ball back to the human . For half of the cases in which the catch was unsuccessful , the robot made a physical gesture , such as shrugging its shoulders , shaking its head , or throwing up its hands . In the other half of cases , no gestures were produced . We used questionnaires and smile detection to compare participants ' feelings about the robot when it made gestures after failure versus when it did not . Participants smiled more and rated the robot as more engaging , responsive , and humanlike when it gestured . We conclude that social gesturing of a robot enhances physical interactions between humans and robots .
2K_dev_1998	There is a saying that 95\ % of communication is body language , but few robot systems today make effective use of that ubiquitous channel . Motion is an essential area of social communication that will enable robots and people to collaborate naturally , develop rapport , and seamlessly share environments . The proposed work presents a principled set of motion features based on the Laban Effort system , a widespread and extensively tested acting ontology for the dynamics of `` how { '' } we enact motion . The features allow us to analyze and , in future work , generate expressive motion using position ( x , y ) and orientation ( theta ) . We formulate representative features for each Effort and parameterize them on expressive motion sample trajectories collected from experts in robotics and theater . We then produce classifiers for different `` manners { '' } of moving and assess the quality of results by comparing them to the humans labeling the same set of paths on Amazon Mechanical Turk . Results indicate that the machine analysis ( 41.7\ % match between intended and classified manner ) achieves similar accuracy overall compared to a human benchmark ( 41.2\ % match ) . We conclude that these motion features perform well for analyzing expression in low degree of freedom systems and could be used to help design more effectively expressive mobile robots .
2K_dev_1999	For service robots operating in indoor environments , the crucial task of navigation is often complicated by the presence of people . Simply treating humans in the environment as additional ( often moving ) obstacles can violate the complex set of social rules by which people navigate around each other . In contrast , emulating human behavior and navigating in a socially appropriate manner could positively affect people 's comfort with a robot 's presence and motion . We present a method of generating social paths for a robot to approach a person based on a small amount of human data . We also conducted a study in which a robot approached participants using both these social paths and straight-line , nonsocial paths . We found that both approaches were rated comparably when the robot approached from the participant 's front or side , but the social approach was significantly preferred when the robot came from behind the participant .
2K_dev_2000	Human operators in today 's control centers , such as air or road traffic control , need to monitor a plethora of information obtained from diverse sources . To support them in detecting critical situations within this information flood and taking timely actions , operators thus need adequate information fusion and decision support systems . Research efforts on such dedicated Situation Awareness ( SAW ) systems have concentrated on assisting the operator in managing the current situations . However , little focus has been so far on integratively supporting the different phases of knowledge management in SAW systems , which encompasses the acquisition , representation , validation , maintenance and reuse of knowledge gathered for and during the use of these systems , such as configuring and maintaining suitable situation templates and exploiting already assessed situations . If operators and domain experts are not supported in these tasks , however , this may discourage them from a successful adoption of such systems in real-world control center applications , as user studies revealed . Based on these , and the lessons learned from the application of our SAW system implementations BeAware ! and CSI to the domain of road traffic control , we therefore propose a first step towards a tool suite fostering knowledge management in SAW systems , which stretches from the configuration phase of the system to its runtime maintenance in the light of evolving environments and user needs .
2K_dev_2001	Near-Infrared ( NIR ) images of most materials exhibit less texture or albedo variations making them beneficial for vision tasks such as intrinsic image decomposition and structured light depth estimation . Understanding the reflectance properties ( BRDF ) of materials in the NIR wavelength range can be further useful for many photometric methods including shape from shading and inverse rendering . However , even with less albedo variation , many materials e.g . fabrics , leaves , etc . exhibit complex fine-scale surface detail making it hard to accurately estimate BRDF . In this paper , we present an approach to simultaneously estimate NIR BRDF and fine-scale surface details by imaging materials under different IR lighting and viewing directions . This is achieved by an iterative scheme that alternately estimates surface detail and NIR BRDF of materials . Our setup does not require complicated gantries or calibration and we present the first NIR dataset of 100 materials including a variety of fabrics ( knits , weaves , cotton , satin , leather ) , and organic ( skin , leaves , jute , trunk , fur ) and inorganic materials ( plastic , concrete , carpet ) . The NIR BRDFs measured from material samples are used with a shape-from-shading algorithm to demonstrate fine-scale reconstruction of objects from a single NIR image .
2K_dev_2002	This paper proposes a modified post-filter to recover a Modulation Spectrum ( MS ) in HMM-based speech synthesis . To alleviate the over-smoothing effect which is one of the major problems in HMM-based speech synthesis , the MS-based post-filter has been proposed . It recovers the utterance-level MS of the generated speech trajectory , and we have reported its benefit to the quality improvement . However , this post-filter is not applicable to various lengths of speech parameter trajectories , such as phrases or segments , which are shorter than an utterance . To address this problem , we propose two modified post-filters , ( 1 ) the time-invariant filter with a simplified conversion form and ( 2 ) the segment-level post-filter which applicable to a short-term parameter sequence . Furthermore , we also propose ( 3 ) the post-filter to recover the phoneme-level MS of HMM-state duration . Experimental results show that the modified post-filters also yield significant quality improvements in synthetic speech as yielded by the conventional post-filter .
2K_dev_2003	We will never really understand learning until we can build machines that learn many different things , over years , and become better learners over time . We describe our research to build a Never-Ending Language Learner ( NELL ) that runs 24 hours per day , forever , learning to read the web . Each day NELL extracts ( reads ) more facts from the web , into its growing knowledge base of beliefs . Each day NELL also learns to read better than the day before . NELL has been running 24 hours/day for over four years now . The result so far is a collection of 70 million interconnected beliefs ( e.g. , servedWtih ( coffee , applePie ) ) , NELL is considering at different levels of confidence , along with millions of learned phrasings , morphological features , and web page structures that NELL uses to extract beliefs from the web . NELL is also learning to reason over its extracted knowledge , and to automatically extend its ontology . Track NELL 's progress at http : //rtw.ml.cmu.edu , or follow it on Twitter at @ CMUNELL .
2K_dev_2004	How can we detect suspicious users in large online networks ? Online popularity of a user or product ( via follows , page-likes , etc . ) can be monetized on the premise of higher ad click-through rates or increased sales . Web services and social networks which incentivize popularity thus suffer from a major problem of fake connections from link fraudsters looking to make a quick buck . Typical methods of catching this suspicious behavior use spectral techniques to spot large groups of often blatantly fraudulent ( but sometimes honest ) users . However , small-scale , stealthy attacks may go unnoticed due to the nature of low-rank eigenanalysis used in practice . In this work , we take an adversarial approach to find and prove claims about the weaknesses of modern , state-of-the-art spectral methods and propose FBOX , an algorithm designed to catch small-scale , stealth attacks that slip below the radar . Our algorithm has the following desirable properties : ( a ) it has theoretical underpinnings , ( b ) it is shown to be highly effective on real data and ( c ) it is scalable ( linear on the input size ) . We evaluate FBOX on a large , public 41.7 million node , 1.5 billion edge who-follows-whom social graph from Twitter in 2010 and with high precision identify many suspicious accounts which have persisted without suspension even to this day .
2K_dev_2005	In this work , we propose to employ multi-channel correlation filters for recognizing human actions ( e.g . waking , riding ) in videos . In our framework , each action sequence is represented as a multi-channel signal ( frames ) and the goal is to learn a multi-channel filter for each action class that produces a set of desired outputs when correlated with training examples . The experiments on the Weizmann and UCF sport datasets demonstrate superior computational cost ( real-time ) , memory efficiency and very competitive performance of our approach compared to the state of the arts .
2K_dev_2007	The study of human control of robotic swarms involves designing interfaces and algorithms for allowing a human operator to influence a swarm of robots . One of the main difficulties , however , is determining how to most effectively influence the swarm after it has been deployed . Past work has focused on influencing the swarm via statically selected leaders-swarm members that the operator directly controls . This paper investigates the use of a small subset of the swarm as leaders that are dynamically selected during the scenario execution and are directly controlled by the human operator to guide the rest of the swarm , which is operating under a flocking-style algorithm . The goal of the operator in this study is to move the swarm to goal regions that arise dynamically in the environment . We experimentally investigated three different aspects of dynamic leader-based swarm control and their interactions : leader density ( in terms of guaranteed hops to a leader ) , sensing error , and method of information propagation from leaders to the rest of the swarm . Our results show that , while there was a large drop in the number of goals reached when moving from a 1-hop to a 2-hop guarantee , the difference between a 2-hop , 3-hop , and 4-hop guarantee was not statistically significant . Furthermore , we found that sensing error impacted the explicit information-propagation method more than the tacit method conditions , and caused participants more trouble the lower the density of leaders , although the explicit method performed better overall .
2K_dev_2008	We aim to understand the dynamics of social interactions between two people by recognizing their actions and reactions using a head-mounted camera . Our work will impact several first-person vision tasks that need the detailed understanding of social interactions , such as automatic video summarization of group events and assistive systems . To recognize micro-level actions and reactions , such as slight shifts in attention , subtle nodding , or small hand actions , where only subtle body motion is apparent , we propose to use paired egocentric videos recorded by two interacting people . We show that the first-person and second-person points-of-view features of two people , enabled by paired egocentric videos , are complementary and essential for reliably recognizing micro-actions and reactions . We also build a new dataset of dyadic ( two-persons ) interactions that comprises more than 1000 pairs of egocentric videos to enable systematic evaluations on the task of micro-action and reaction recognition .
2K_dev_2009	Speaker adaptive training ( SAT ) is a well studied technique for Gaussian mixture acoustic models ( GMMs ) . Recently we proposed to perform SAT for deep neural networks ( DNNs ) , with speaker i-vectors applied in feature learning . The resulting SAT-DNN models significantly outperform DNNs on word error rates ( WERs ) . In this paper , we present different methods to further improve and extend SAT-DNN . First , we conduct detailed analysis to investigate i-vector extractor training and flexible feature fusion . Second , the SAT-DNN approach is extended to improve tasks including bottleneck feature ( BNF ) generation , convolutional neural network ( CNN ) acoustic modeling and multilingual DNN-based feature extraction . Third , for transcribing multimedia data , we enrich the i-vector representation with global speaker attributes ( age , gender , etc . ) obtained automatically from video signals . On a collection of instructional videos , incorporation of the additional visual features is observed to boost the recognition accuracy of SAT-DNN .
2K_dev_2010	In this paper we introduce a simplified architecture for gated recurrent neural networks that can be used in single-pass applications , where word-spotting needs to be done in real-time and phoneme-level information is not available for training . The network operates as a self-contained block in a strictly forward-pass configuration to directly generate keyword labels . We call these simple networks causal networks , where the current output is only weighted by the the past inputs and outputs . Since the basic network has a simpler architecture as compared to traditional memory networks used in keyword spotting , it also requires less data to train . Experiments on a standard speech database highlight the behavior and efficacy of such networks . Comparisons with a standard HMM-based keyword spotter show that these networks , while simple , are still more accurate .
2K_dev_2011	Spoken language interfaces are being incorporated into various devices ( e.g . smart-phones , smart TVs , etc ) . However , current technology typically limits conversational interactions to a few narrow predefined domains/topics . For example , dialogue systems for smart-phone operation fail to respond when users ask for functions not supported by currently installed applications . We propose to dynamically add application-based domains according to users ' requests by using descriptions of applications as a retrieval cue to find relevant applications . The approach uses structured knowledge resources ( e.g . Freebase , Wikipedia , FrameNet ) to induce types of slots for generating semantic seeds , and enriches the semantics of spoken queries with neural word embeddings , where semantically related concepts can be additionally included for acquiring knowledge that does not exist in the predefined domains . The system can then retrieve relevant applications or dynamically suggest users install applications that support unexplored domains . We find that vendor descriptions provide a reliable source of information for this purpose .
2K_dev_2012	We present MergePoint , a new binary-only symbolic execution system for large-scale testing of commodity off-the-shelf ( COTS ) software . MergePoint introduces veritesting , a new technique that employs static symbolic execution to amplify the effect of dynamic symbolic execution . Veritesting allows MergePoint to find twice as many bugs , explore orders of magnitude more paths , and achieve higher code coverage than previous dynamic symbolic execution systems . MergePoint is currently running daily on a 100 node cluster analyzing 33248 Linux binaries ; has generated more than 15 billion SMT queries , 200 million test cases , 2347420 crashes , and found 11687 bugs in 4379 distinct applications .
2K_dev_2013	What defines an action like `` kicking ball { '' } ? We argue that the 1 meaning of an action lies in the change or transformation an action brings to the environment . In this paper , we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens ( precondition ) to the state after the action ( effect ) . Motivated by recent advancements of video representation using deep learning , we design a Siamese network which models the action as a transformation on a high-level feature space . We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51 . More importantly , our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset .
2K_dev_2014	Product architecture structures the coordination problem that the development organization must solve . The modularity strategy establishes design rules that fix module functionality and interfaces , and assigns development work for each module to a single team . The modules present relatively independent coordination problems that teams attempt to solve with all the traditional coordination mechanisms available to them . The applicability and effectiveness of this strategy is limited with increasing technical and organizational volatility . In the absence of theory explaining why and when modularity works , the technique is brittle , with very little firm basis for adjustment or for complementing it with other strategies . I present a theory of coordination , based on decision networks , that generalizes the modularity strategy . I review evidence testing several hypotheses derived from the theory , and explore how this theoretical view can drive coordination research and provide a theoretical basis for practical techniques to assist architects , developers , and managers .
2K_dev_2015	One of the pillars of the modern scientific method is model validation : comparing a scientific model 's predictions against empirical observations . Today , a scientist demonstrates the validity of a model by making an argument in a paper and submitting it for peer review , a process comparable to code review in software engineering . While human review helps to ensure that contributions meet high-level goals , software engineers typically supplement it with unit testing to get a more complete picture of the status of a project . We argue that a similar test-driven methodology would be valuable to scientific communities as they seek to validate increasingly complex models against growing repositories of empirical data . Scientific communities differ from software communities in several key ways , however . In this paper , we introduce Sci Unit , a framework for test-driven scientific model validation , and outline how , supported by new and existing collaborative infrastructure , it could integrate into the modern scientific process .
2K_dev_2016	Although different approaches to decision-making in self adaptive systems have shown their effectiveness in the past by factoring in predictions about the system and its environment ( e.g. , resource availability ) , no proposal considers the latency associated with the execution of tactics upon the target system . However , different adaptation tactics can take different amounts of time until their effects can be observed . In reactive adaptation , ignoring adaptation tactic latency can lead to suboptimal adaptation decisions ( e.g. , activating a server that takes more time to boot than the transient spike in traffic that triggered its activation ) . In proactive adaptation , taking adaptation latency into account is necessary to get the system into the desired state to deal with an upcoming situation . In this paper , we introduce a formal analysis technique based on model checking of stochastic multiplayer games ( SMGs ) that enables us to quantify the potential benefits of employing different types of algorithms for self-adaptation . In particular , we apply this technique to show the potential benefit of considering adaptation tactic latency in proactive adaptation algorithms . Our results show that factoring in tactic latency in decision making improves the outcome of adaptation . We also present an algorithm to do proactive adaptation that considers tactic latency , and show that it achieves higher utility than an algorithm that under the assumption of no latency is optimal .
2K_dev_2017	Mobile devices have become powerful ultra-portable personal computers supporting not only communication but also running a variety of complex , interactive applications . Because of the unique characteristics of mobile interaction , a better understanding of the time duration and context of mobile device uses could help to improve and streamline the user experience . In this paper , we first explore the anatomy of mobile device use and propose a classification of use based on duration and interaction type : glance , review , and engage . We then focus our investigation on short review interactions and identify opportunities for streamlining these mobile device uses through proactively suggesting short tasks to the user that go beyond simple application notifications . We evaluate the concept through a user evaluation of an interactive lock screen prototype , called ProactiveTasks . We use the findings from our study to create and explore the design space for proactively presenting tasks to the users . Our findings underline the need for a more nuanced set of interactions that support short mobile device uses , in particular review sessions .
2K_dev_2018	The growing size of modern storage systems is expected to exceed billions of objects , making metadata scalability critical to overall performance . Many existing distributed file systems only focus on providing highly parallel fast access to file data , and lack a scalable metadata service . In this paper , we introduce a middleware design called IndexFS that adds support to existing file systems such as PVFS , Lustre , and HDFS for scalable high-performance operations on metadata and small files . IndexFS uses a table-based architecture that incrementally partitions the namespace on a per-directory basis , preserving server and disk locality for small directories . An optimized log-structured layout is used to store metadata and small files efficiently . We also propose two client-based storm-free caching techniques : bulk namespace insertion for creation intensive workloads such as N-N checkpointing ; and stateless consistent metadata caching for hot spot mitigation . By combining these techniques , we have demonstrated IndexFS scaled to 128 metadata servers . Experiments show our out-of-core metadata throughput out-performing existing solutions such as PVFS , Lustre , and HDFS by 50\ % to two orders of magnitude .
2K_dev_2020	Recent advances in rendering and data-driven animation have enabled the creation of compelling characters with impressive levels of realism . While data-driven techniques can produce animations that are extremely faithful to the original motion , many challenging problems remain because of the high complexity of human motion . A better understanding of the factors that make human motion recognizable and appealing would be of great value in industries where creating a variety of appealing virtual characters with realistic motion is required . To investigate these issues , we captured thirty actors walking , jogging and dancing , and applied their motions to the same virtual character ( one each for the males and females ) . We then conducted a series of perceptual experiments to explore the distinctiveness and attractiveness of these human motions , and whether characteristic motion features transfer across an individual 's different gaits . Average faces are perceived to be less distinctive but more attractive , so we explored whether this was also 1 for body motion . We found that dancing motions were most easily recognized and that distinctiveness in one gait does not predict how recognizable the same actor is when performing a different motion . As hypothesized , average motions were always amongst the least distinctive and most attractive . Furthermore , as 50\ % of participants in the experiment were Caucasian European and 50\ % were Asian Korean , we found that the latter were as good as or better at recognizing the motions of the Caucasian actors than their European counterparts , in particular for dancing males , whom they also rated more highly for attractiveness .
2K_dev_2021	Transport protocols must accommodate diverse application and network requirements . As a result , TCP has evolved over time with new congestion control algorithms such as support for generalized AIMD , background flows , and multipath . On the other hand , explicit congestion control algorithms have been shown to be more efficient . However , they are inherently more rigid because they rely on in-network components . Therefore , it is not clear whether they can be made flexible enough to support diverse application requirements . This paper presents a flexible framework for network resource allocation , called FCP , that accommodates diversity by exposing a simple abstraction for resource allocation . FCP incorporates novel primitives for end-point flexibility ( aggregation and preloading ) into a single framework and makes economics-based congestion control practical by explicitly handling load variations and by decoupling it from actual billing . We show that FCP allows evolution by accommodating diversity and ensuring coexistence , while being as efficient as existing explicit congestion control algorithms .
2K_dev_2022	Phase-contrast microscopy is one of the most common and convenient imaging modalities to observe long-term multi-cellular processes , which generates images by the interference of lights passing through transparent specimens and background medium with different retarded phases . Despite many years of study , computer-aided phase contrast microscopy analysis on cell behavior is challenged by image qualities and artifacts caused by phase contrast optics . Addressing the unsolved challenges , the authors propose ( 1 ) a phase contrast microscopy image restoration method that produces phase retardation features , which are intrinsic features of phase contrast microscopy , and ( 2 ) a semi-supervised learning based algorithm for cell segmentation , which is a fundamental task for various cell behavior analysis . Specifically , the image formation process of phase contrast microscopy images is first computationally modeled with a dictionary of diffraction patterns ; as a result , each pixel of a phase contrast microscopy image is represented by a linear combination of the bases , which we call phase retardation features . Images are then partitioned into phase-homogeneous atoms by clustering neighboring pixels with similar phase retardation features . Consequently , cell segmentation is performed via a semi-supervised classification technique over the phase-homogeneous atoms . Experiments demonstrate that the proposed approach produces quality segmentation of individual cells and outperforms previous approaches . ( C ) 2013 Elsevier B.V. All rights reserved .
2K_dev_2023	Motivation : Several types of studies , including genome-wide association studies and RNA interference screens , strive to link genes to diseases . Although these approaches have had some success , genetic variants are often only present in a small subset of the population , and screens are noisy with low overlap between experiments in different labs . Neither provides a mechanistic model explaining how identified genes impact the disease of interest or the dynamics of the pathways those genes regulate . Such mechanistic models could be used to accurately predict downstream effects of knocking down pathway members and allow comprehensive exploration of the effects of targeting pairs or higher-order combinations of genes . Results : We developed methods to model the activation of signaling and dynamic regulatory networks involved in disease progression . Our model , SDREM , integrates static and time series data to link proteins and the pathways they regulate in these networks . SDREM uses prior information about proteins ' likelihood of involvement in a disease ( e.g . from screens ) to improve the quality of the predicted signaling pathways . We used our algorithms to study the human immune response to H1N1 influenza infection . The resulting networks correctly identified many of the known pathways and transcriptional regulators of this disease . Furthermore , they accurately predict RNA interference effects and can be used to infer genetic interactions , greatly improving over other methods suggested for this task . Applying our method to the more pathogenic H5N1 influenza allowed us to identify several strain-specific targets of this infection .
2K_dev_2024	Supporting students ' self-regulated learning ( SRL ) is an important topic in the learning sciences . Two critical processes involved in SRL are self-assessment and study choice . Intelligent tutoring systems ( ITSs ) have been shown to be effective in supporting students ' domain-level learning through guided problem-solving practice , but it is an open question how they can support SRL processes effectively , while maintaining or even enhancing their effectiveness at the domain level . We used a combination of user-centered design techniques and experimental classroom research to redesign and evaluate an ITS for linear equation solving so it supports self-assessment and study choice . We added three features to the tutor ' Open Learner Model ( OLM ) that may scaffold students ' self-assessment ( self-assessment prompts , delaying the update of students ' progress bars , and providing progress information on the problem type level ) . We also designed a problem selection screen with shared student/system control and game-like features . We went through two iterations of design and conducted two controlled experiments with 160 local middle school students to evaluate the effectiveness of the new features . The evaluations reveal that the new OLM with self-assessment support facilitates students ' learning processes , and enhances their learning outcomes significantly . However , we did not find significant learning gains due to the problem selection feature . This work informs the design of future ITS that supports SRL .
2K_dev_2025	When human annotators are given a choice about what to label in an image , they apply their own subjective judgments on what to ignore and what to mention . We refer to these noisy `` human-centric { '' } annotations as exhibiting human reporting bias . Examples of such annotations include image tags and keywords found on photo sharing sites , or in datasets containing image captions . In this paper , we use these noisy annotations for learning visually correct image classifiers . Such annotations do not use consistent vocabulary , and miss a significant amount of the information present in an image ; however , we demonstrate that the noise in these annotations exhibits structure and can be modeled . We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels . Our results are highly interpretable for reporting `` what 's in the image { '' } versus `` what 's worth saying . { '' } We demonstrate the algorithm 's efficacy along a variety of metrics and datasets , including MS COCO and Yahoo Flickr 100M . We show significant improvements over traditional algorithms for both image classification and image captioning , doubling the performance of existing methods in some cases .
2K_dev_2026	We introduce confidence-weighted ( CW ) online learning algorithms for robust , cost-sensitive classification . Our work extends the original confidence-weighted optimization framework in two important directions . First , we show how the original value at risk ( VaR ) probabilistic constraint in CW algorithms can be generalized to a worst-case conditional value at risk ( CVaR ) constraint for more robust learning from cost-weighted examples . Second , we show how to reduce adversarial feature noise , which can be useful in fraud detection scenarios , by reframing the optimization problem in terms of maximum a posteriori estimation . The resulting optimization problems can be solved efficiently . Experiments on real-world and synthetic datasets show that our robust , cost-sensitive extensions consistently reduce the cost incurred in both online and batch learning settings . We also demonstrate a correspondence between the VaR and CVaR constraints used for classification and uncertainty sets used in robust optimization , leading toward a rich family of potential extensions to CW algorithms .
2K_dev_2028	This paper proposes a vision-based method for detecting apoptosis ( programmed cell death ) , which is essential for non-perturbative monitoring of cell expansion . Our method targets non-adherent cells , which float or are suspended freely in the culture medium-in contrast to adherent cells , which are attached to a petri dish . The method first detects cell regions and tracks them over time , resulting in the construction of cell tracklets . For each of the tracklets , visual properties of the cell are then examined to know whether and when the tracklet shows a transition from a live cell to a dead cell , in order to determine the occurrence and timing of a cell death event . For the validation , a transductive learning framework is adopted to utilize unlabeled data in addition to labeled data . Our method achieved promising performance in the experiments with hematopoietic stem cell ( HSC ) populations , which are currently in clinical use for rescuing hematopoietic function during bone marrow transplants .
2K_dev_2029	Silhouettes provide rich information on three-dimensional shape , since the intersection of the associated visual cones generates the `` visual hull { '' } , which encloses and approximates the original shape . However , not all silhouettes can actually be projections of the same object in space : this simple observation has implications in object recognition and multi-view segmentation , and has been ( often implicitly ) used as a basis for camera calibration . In this paper , we investigate the conditions for multiple silhouettes , or more generally arbitrary closed image sets , to be geometrically `` consistent { '' } . We present this notion as a natural generalization of traditional multi-view geometry , which deals with consistency for points . After discussing some general results , we present a `` dual { '' } formulation for consistency , that gives conditions for a family of planar sets to be sections of the same object . Finally , we introduce a more general notion of silhouette `` compatibility { '' } under partial knowledge of the camera projections , and point out some possible directions for future research .
2K_dev_2030	Politeness is believed to facilitate communication in human interaction , as it can minimize the potential for conflict and confrontation . Regarding the role of politeness strategies for human-robot interaction , conflicting findings are presented in the literature . Thus , we conducted a between-participants experimental study with a receptionist robot to gain a deeper understanding of how politeness on the one hand , and the type of interaction itself on the other hand , might affect and shape user experience and evaluation of HRI . Our findings suggest that the interaction context has a greater impact on participants ' perception of the robot in HRI than the use - or lack - of politeness strategies .
2K_dev_2031	Large-scale information processing systems are able to extract massive collections of interrelated facts , but unfortunately transforming these candidate facts into useful knowledge is a formidable challenge . In this paper , we show how uncertain extractions about entities and their relations can be transformed into a knowledge graph . The extractions form an extraction graph and we refer to the task of removing noise , inferring missing information , and determining which candidate facts should be included into a knowledge graph as knowledge graph identification . In order to perform this task , we must reason jointly about candidate facts and their associated extraction confidences , identify co-referent entities , and incorporate ontological constraints . Our proposed approach uses probabilistic soft logic ( PSL ) , a recently introduced probabilistic modeling framework which easily scales to millions of facts . We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL project containing over 1M extractions and 70K ontological relations . We show that compared to existing methods , our approach is able to achieve improved AUC and F1 with significantly lower running time .
2K_dev_2032	Occlusions are common in real world scenes and are a major obstacle to robust object detection . In this paper , we present a method to coherently reason about occlusions on many types of detectors . Previous approaches primarily enforced local coherency or learned the occlusion structure from data . However , local coherency ignores the occlusion structure in real world scenes and learning from data requires tediously labeling many examples of occlusions for every view of every object . Other approaches require binary classifications of matching scores . We address these limitations by formulating occlusion reasoning as an efficient search over occluding blocks which best explain a probabilistic matching pattern . Our method demonstrates significant improvement in estimating the mask of the occluding region and improves object instance detection on a challenging dataset of objects under severe occlusions .
2K_dev_2033	Binary codes that are binarizations of features represented by real numbers have recently been used in the object recognition field , in order to achieve reduced memory and robustness with respect to noise . However , binarizing features represented by real numbers has a problem in that a great deal of the information within the features drops out . That is why we focus on quantization residual , which is information that drops out when features are binarized . With this study , we introduce a transition likelihood model into classifiers , in order to take into consideration the possibility that a binary code which has been observed from an image will transition to another binary code . This enables classifications that consider transitions to the desired binary code , even if the observed binary code differs from the actually desired binary code for some reason . From the results of experiments , we confirmed that the proposed method enables an increase in detection performance while maintaining the same levels of memory and computing costs as those for previous methods of binarizing features .
2K_dev_2034	`` Socially cooperative driving { '' } is an integral part of our everyday driving , hence requiring special attention to imbue the autonomous driving with a more natural driving behavior . In this paper , an intention-integrated Prediction-and Cost function-Based algorithm ( iPCB ) framework is proposed to enable an autonomous vehicle to perform cooperative social behavior . An intention estimator is developed to extract the probability of surrounding agents ' intentions in real time . Then for each candidate strategy , a prediction engine considering the interaction between host and surrounding agents is used to predict future scenarios . A cost function-based evaluation is applied to compute the cost for each scenario and select the decision corresponding to the lowest cost . The algorithm was tested in simulation on an autonomous vehicle cooperating with vehicles merging from freeway entrance ramps with 10000 randomly generated scenarios . Compared with approaches that do not take social behavior into account , the iPCB algorithm shows a 41.7\ % performance improvement based on the chosen cost functions .
2K_dev_2035	On-road motion planning for autonomous vehicles is in general a challenging problem . Past efforts have proposed solutions for urban and highway environments individually . We identify the key advantages/shortcomings of prior solutions , and propose a novel two-step motion planning system that addresses both urban and highway driving in a single framework . Reference Trajectory Planning ( I ) makes use of dense lattice sampling and optimization techniques to generate an easy-to-tune and human-like reference trajectory accounting for road geometry , obstacles and high-level directives . By focused sampling around the reference trajectory , Tracking Trajectory Planning ( II ) generates , evaluates and selects parametric trajectories that further satisfy kinodynamic constraints for execution . The described method retains most of the performance advantages of an exhaustive spatiotemporal planner while significantly reducing computation .
2K_dev_2036	Vehicular ad hoc networks ( VANETs ) are seen as an important enabling technology for improving both traffic safety and efficiency . Virtual Traffic Lights ( VTLs ) are a promising proposal for reducing travel time by efficiently controlling road intersections . VTLs use vehicle-to-vehicle communication to dynamically optimize traffic flow and they display traffic light information on the windshield . However , research so far has assumed that all vehicles are equipped with VTL support and it has ignored the incremental deployment phase , which could last decades . In this paper we present a solution for a VTL partial deployment scenario that is based on the idea of having VTL equipped cars display traffic light information on the outside of the vehicle . This allows drivers in non-equipped vehicles , or even pedestrians , to see the light color and respond accordingly . We show that the benefits of VTLs in terms of intersection throughput and average delay reduction grow as a function of the penetration rate of equipped vehicles .
2K_dev_2037	We propose an identity-aware multi-object tracker based on the solution path algorithm . Our tracker not only produces identity-coherent trajectories based on cues such as face recognition , but also has the ability to pinpoint potential tracking errors . The tracker is formulated as a quadratic optimization problem with l ( 0 ) norm constraints , which we propose to solve with the solution path algorithm . The algorithm successively solves the same optimization problem but under different l ( p ) norm constraints , where p gradually decreases from 1 to 0 . Inspired by the success of the solution path algorithm in various machine learning tasks , this strategy is expected to converge to a better local minimum than directly minimizing the hardly solvable l ( 0 ) norm or the roughly approximated l ( 1 ) norm constraints . Furthermore , the acquired solution path complies with the `` decision making process { '' } of the tracker , which provides more insight to locating potential tracking errors . Experiments show that not only is our proposed tracker effective , but also the solution path enables automatic pinpointing of potential tracking failures , which can be readily utilized in an active learning framework to improve identity-aware multi-object tracking .
2K_dev_2038	Spoken dialogue systems typically use predefined semantic slots to parse users ' natural language inputs into unified semantic representations . To define the slots , domain experts and professional annotators are often involved , and the cost can be expensive . In this paper , we ask the following question : given a collection of unlabeled raw audios , can we use the frame semantics theory to automatically induce and fill the semantic slots in an unsupervised fashion ? To do this , we propose the use of a state-of-the-art frame-semantic parser , and a spectral clustering based slot ranking model that adapts the generic output of the parser to the target semantic space . Empirical experiments on a real-world spoken dialogue dataset show that the automatically induced semantic slots are in line with the reference slots created by domain experts : we observe a mean averaged precision of 69.36\ % using ASR-transcribed data . Our slot filling evaluations also indicate the promising future of this proposed approach .
2K_dev_2039	Social media is being integrated into work environments making them more transparent . When the work environment is transparent , it has the potential to allow projects to transmit information about work artifacts and events quickly through a large network . Using signaling theory , we propose a theory that users interpret this information and then make work-related decisions about attention and effort allocation in a principled manner . In the open source context of voluntary participation , broadcast activity information act as signals that allow developers to make highly informed choices about where to expend their attention and effort and with whom to collaborate . We propose four potential signals from literature and interviews with developers in our research setting and discuss the implications for social media in software development environments .
2K_dev_2040	Smartphones are now targets of malicious viruses . Furthermore , the increasing `` connectedness { '' } of smartphones has resulted in new delivery vectors for malicious viruses , including proximity- , social- and other technology-based methods . In fact , Cabir and CommWarrior are two viruses-observed in the wild-that spread , at least in part , using proximity-based techniques ( line-of-sight bluetooth radio ) . In this paper , we propose and evaluate SI1I2S , a competition model that describes the spread of two mutually exclusive viruses across heterogeneous composite networks , one static ( social connections ) and one dynamic ( mobility pattern ) . To approximate dynamic network behavior , we use classic mobility models from ad hoc networking , e. g. , Random Waypoint , Random Walk and Levy Flight . We analyze our model using techniques from dynamic systems and find that the first eigenvalue of the system matrices lambda ( S1 ) , lambda ( S2 ) of the two networks ( static and dynamic networks ) appropriately captures the competitive interplay between two viruses and effectively predicts the competition 's `` winner { '' } , which provides a feasible way to defend against smartphone viruses .
2K_dev_2041	We investigated the dynamics of head motion in parents and infants during an age-appropriate , well-validated emotion induction , the Face-to-Face/Still-Face procedure . Participants were 12 ethnically diverse 6-month-old infants and their mother or father . During infant gaze toward the parent , infant angular amplitude and velocity of pitch and yaw decreased from face-to-face ( FF ) to still-face ( SF ) episodes and remained lower in the following Reunion . During infant gaze away from the parent , angular velocity of pitch decreased from FF to SF and remained lower in the Reunion ( RE ) . Windowed cross-correlation suggested strong bidirectional effects with frequent shifts in the direction of influence . The number of significant positive and negative peaks was higher during FF than RE . Gaze toward and away from the parent was modestly predicted by head orientation . Together , these findings suggest that head motion is strongly related to age-appropriate emotion challenge , are consistent with the hypothesis that perturbations of normal responsiveness carry-over even after the parent resumes normal responsiveness in the reunion , and that there are frequent changes in direction of influence in the postural domain .
2K_dev_2042	We present a near real-time algorithm for interactively exploring a collectively captured moment without explicit 3D reconstruction . Our system favors immediacy and local coherency to global consistency . It is common to represent photos as vertices of a weighted graph , where edge weights measure similarity or distance between pairs of photos . We introduce Angled Graphs as a new data structure to organize collections of photos in a way that enables the construction of visually smooth paths . Weighted angled graphs extend weighted graphs with angles and angle weights which penalize turning along paths . As a result , locally straight paths can be computed by specifying a photo and a direction . The weighted angled graphs of photos used in this paper can be regarded as the result of discretizing the Riemannian geometry of the high dimensional manifold of all possible photos . Ultimately , our system enables everyday people to take advantage of each others ' perspectives in order to create on-the-spot spatiotemporal visual experiences similar to the popular bullet-time sequence . We believe that this type of application will greatly enhance shared human experiences spanning from events as personal as parents watching their children 's football game to highly publicized red carpet galas .
2K_dev_2043	We study SMT problems over the reals containing ordinary differential equations , . They are important for formal verification of realistic hybrid systems and embedded software . We develop delta-complete algorithms for SMT formulas that are purely existentially quantified , as well as there exists for all-formulas whose universal quantification is restricted to the time variables . We demonstrate scalability of the algorithms , as implemented in our open-source solver dReal , on SMT benchmarks with several hundred nonlinear ODEs and variables .
2K_dev_2044	A robotic swarm is a decentralized group of robots which overcome failure of individual robots with robust emergent behaviors based on local interactions . These behaviors are not well built for accomplishing complex tasks , however , because of the changing assumptions required in various applications and environments . A new movement in the research field is to add human input to influence the swarm in order to help make the robots goal directed and overcome these problems . This research in Human Swarm Interaction ( HSI ) focuses on different control laws and ways to integrate the human intent with local control laws of the robots . Previous studies have all used visual feedback through a computer interface to give the user the swarm state information . This study adapted swarm control algorithms to give the operator haptic feedback as well as visual feedback . The study shows the benefits of the additional feedback in a target searching class . Researchers in multi-robot systems have shown benefits of haptic feedback in obstacle navigation before , but this study is a novel method because of the decentralized formation of the robotic swarm . In most environments , operators were able to cover significantly more area , increasing the chance of finding more targets . The other environment found no significant difference , showing that the haptic feedback does not degrade performance in any of the tested environments . This supports our hypothesis that haptic feedback is useful in HSI and requires further research to maximize its potential .
2K_dev_2045	Clustering is the task of grouping a set of objects so that objects in the same cluster are more similar to each other than to those in other clusters . The crucial step in most clustering algorithms is to find an appropriate similarity metric , which is both challenging and problem-dependent . Supervised clustering approaches , which can exploit labeled clustered training data that share a common metric with the test set , have thus been proposed . Unfortunately , current metric learning approaches for supervised clustering do not scale to large or even medium-sized datasets . In this paper , we propose a new structured Mahalanobis Distance Metric Learning method for supervised clustering . We formulate our problem as an instance of large margin structured prediction and prove that it can be solved very efficiently in closed-form . The complexity of our method is ( in most cases ) linear in the size of the training dataset . We further reveal a striking similarity between our approach and multivariate linear regression . Experiments on both synthetic and real datasets confirm several orders of magnitude speedup while still achieving state-of-the-art performance .
2K_dev_2047	Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D . Manual needle insertion allows the clinician to control the insertion speed , ensuring patient safety . This paper presents a control law for automatic 3D steering of manually inserted flexible needles , enabling path-following control . A look-ahead proportional controller for position and orientation is presented . The look-ahead distance is a linear function of insertion speed . Simulations in a 3D brain-like environment demonstrate the performance of the proposed controller . Experimental results also show the feasibility of this technique in 2D and 3D environments .
2K_dev_2048	Generating meaningful digests of videos by extracting interesting frames remains a difficult task . In this paper , we define interesting events as unusual events which occur rarely in the entire video and we propose a novel interesting event summarization framework based on the technique of density ratio estimation recently introduced in machine learning . Our proposed framework is unsupervised and it can be applied to general video sources , including videos from moving cameras . We evaluated the proposed approach on a publicly available dataset in the context of anomalous crowd behavior and with a challenging personal video dataset . We demonstrated competitive performance both in accuracy relative to human annotation and computation time .
2K_dev_2049	We consider the problem of semi-supervised bootstrap learning for scene categorization . Existing semi-supervised approaches are typically unreliable and face semantic drift because the learning task is under-constrained . This is primarily because they ignore the strong interactions that often exist between scene categories , such as the common attributes shared across categories as well as the attributes which make one scene different from another . The goal of this paper is to exploit these relationships and constrain the semi-supervised learning problem . For example , the knowledge that an image is an auditorium can improve labeling of amphitheaters by enforcing constraint that an amphitheater image should have more circular structures than an auditorium image . We propose constraints based on mutual exclusion , binary attributes and comparative attributes and show that they help us to constrain the learning problem and avoid semantic drift . We demonstrate the effectiveness of our approach through extensive experiments , including results on a very large dataset of one million images .
2K_dev_2050	We address the task of inferring the future actions of people from noisy visual input . We denote this task activity forecasting . To achieve accurate activity forecasting , our approach models the effect of the physical environment on the choice of human actions . This is accomplished by the use of state-of-the-art semantic scene understanding combined with ideas from optimal control theory . Our unified model also integrates several other key elements of activity analysis , namely , destination forecasting , sequence smoothing and transfer learning . As proof-of-concept , we focus on the domain of trajectory-based activity analysis from visual input . Experimental results demonstrate that our model accurately predicts distributions over future actions of individuals . We show how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories .
2K_dev_2051	Reconstructing an arbitrary configuration of 3D points from their projection in an image is an ill-posed problem . When the points hold semantic meaning , such as anatomical landmarks on a body , human observers can often infer a plausible 3D configuration , drawing on extensive visual memory . We present an activity-independent method to recover the 3D configuration of a human figure from 2D locations of anatomical landmarks in a single image , leveraging a large motion capture corpus as a proxy for visual memory . Our method solves for anthropometrically regular body pose and explicitly estimates the camera via a matching pursuit algorithm operating on the image projections . Anthropometric regularity ( i.e. , that limbs obey known proportions ) is a highly informative prior , but directly applying such constraints is intractable . Instead , we enforce a necessary condition on the sum of squared limb-lengths that can be solved for in closed form to discourage implausible configurations in 3D . We evaluate performance on a wide variety of human poses captured from different viewpoints and show generalization to novel 3D configurations and robustness to missing data .
2K_dev_2052	Human pose estimation requires a versatile yet well-constrained spatial model for grouping locally ambiguous parts together to produce a globally consistent hypothesis . Previous works either use local deformable models deviating from a certain template , or use a global mixture representation in the pose space . In this paper , we propose a new hierarchical spatial model that can capture an exponential number of poses with a compact mixture representation on each part . Using latent nodes , it can represent high-order spatial relationship among parts with exact inference . Different from recent hierarchical models that associate each latent node to a mixture of appearance templates ( like HoG ) , we use the hierarchical structure as a pure spatial prior avoiding the large and often confounding appearance space . We verify the effectiveness of this model in three ways . First , samples representing human-like poses can be drawn from our model , showing its ability to capture high-order dependencies of parts . Second , our model achieves accurate reconstruction of unseen poses compared to a nearest neighbor pose representation . Finally , our model achieves state-of-art performance on three challenging datasets , and substantially outperforms recent hierarchical models .
2K_dev_2053	Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition . This success can be largely attributed to learning shared representations from multiple supervisory tasks . However , existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand , that do not generalize . In this paper , we propose a principled approach to learn shared representations in ConvNets using multi-task learning . Specifically , we propose a new sharing unit : `` cross-stitch { '' } unit . These units combine the activations from multiple networks and can be trained end-to-end . A network with cross-stitch units can learn an optimal combination of shared and task-specific representations . Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples .
2K_dev_2054	We address the problem of understanding scenes from multiple sources of sensor data ( e.g. , a camera and a laser scanner ) in the case where there is no one-to-one correspondence across modalities ( e. g. , pixels and 3-D points ) . This is an important scenario that frequently arises in practice not only when two different types of sensors are used , but also when the sensors are not co-located and have different sampling rates . Previous work has addressed this problem by restricting interpretation to a single representation in one of the domains , with augmented features that attempt to encode the information from the other modalities . Instead , we propose to analyze all modalities simultaneously while propagating information across domains during the inference procedure . In addition to the immediate benefit of generating a complete interpretation in all of the modalities , we demonstrate that this co-inference approach also improves performance over the canonical approach .
2K_dev_2055	Object discovery algorithms group together image regions that originate from the same object . This process is effective when the input collection of images contains a large number of densely sampled views of each object , thereby creating strong connections between nearby views . However , existing approaches are less effective when the input data only provide sparse coverage of object views . We propose an approach for object discovery that addresses this problem . We collect a database of about 5 million product images that capture 1.2 million objects from multiple views . We represent each region in the input image by a `` bag { '' } of database object regions . We group input regions together if they share similar `` bags of regions . { '' } Our approach can correctly discover links between regions of the same object even if they are captured from dramatically different viewpoints . With the help from these added links , our proposed approach can robustly discover object instances even with sparse coverage of the viewpoints .
2K_dev_2056	The problem of training classifiers from limited data is one that particularly affects large-scale and social applications , and as a result , although carefully trained machine learning forms the backbone of many current techniques in research , it sees dramatically fewer applications for end-users . Recently we demonstrated a technique for selecting or recommending a single good classifier from a large library even with highly impoverished training data . We consider alternatives for extending our recommendation technique to sets of classifiers , including a modification to the AdaBoost algorithm that incorporates recommendation . Evaluating on an action recognition problem , we present two viable methods for extending model recommendation to sets .
2K_dev_2057	We study type-directed encodings of the simply-typed lambda-calculus in a session-typed pi-calculus . The translations proceed in two steps : standard embeddings of simply-typed lambda-calculus in a linear lambda-calculus , followed by a standard translation of linear natural deduction to linear sequent calculus . We have shown in prior work how to give a Curry-Howard interpretation of the proofs in the linear sequent calculus as pi-calculus processes subject to a session type discipline . We show that the resulting translations induce sharing and copying parallel evaluation strategies for the original lambda-terms , thereby providing a new logically motivated explanation for these strategies .
2K_dev_2058	The detection of apoptosis , or programmed cell death , is important to understand the underlying mechanism of cell development . At present , apoptosis detection resorts to fluorescence or colorimetric assays , which may affect cell behavior and thus not allow long-term monitoring of intact cells . In this work , we present an image analysis method to detect apoptosis in time-lapse phase-contrast microscopy , which is non-destructive imaging . The method first detects candidates for apoptotic cells based on the optical principle of phase-contrast microscopy in connection with the properties of apoptotic cells . The temporal behavior of each candidate is then examined in its neighboring frames in order to determine if the candidate is indeed an apoptotic cell . When applied to three C2C12 myoblastic stem cell populations , which contain more than 1000 apoptosis , the method achieved around 90\ % accuracy in terms of average precision and recall .
2K_dev_2059	Cache compression is a promising technique to increase on-chip cache capacity and to decrease on-chip and off-chip bandwidth usage . Unfortunately , directly applying well-known compression algorithms ( usually implemented in software ) leads to high hardware complexity and unacceptable decompression/compression latencies , which in turn can negatively affect performance . Hence , there is a need for a simple yet efficient compression technique that can effectively compress common in-cache data patterns , and has minimal effect on cache access latency . In this paper , we introduce a new compression algorithm called Base-Delta-Immediate ( B Delta I ) compression , a practical technique for compressing data in on-chip caches . The key idea is that , for many cache lines , the values within the cache line have a low dynamic range - i. e. , the differences between values stored within the cache line are small . As a result , a cache line can be represented using a base value and an array of differences whose combined size is much smaller than the original cache line ( we call this the base+ delta encoding ) . Moreover , many cache lines intersperse such base+ delta values with small values - our B Delta I technique efficiently incorporates such immediate values into its encoding . Compared to prior cache compression approaches , our studies show that B Delta I strikes a sweet-spot in the tradeoff between compression ratio , decompression/compression latencies , and hardware complexity . Our results show that B Delta I compression improves performance for both single-core ( 8.1\ % improvement ) and multi-core workloads ( 9.5\ % /11.2\ % improvement for two/four cores ) . For many applications , B Delta I provides the performance benefit of doubling the cache size of the baseline system , effectively increasing average cache capacity by 1.53X .
2K_dev_2060	Most contemporary object detection approaches assume each object instance in the training data to be uniquely represented by a single bounding box . In this paper , we go beyond this conventional view by allowing an object instance to be described by multiple bounding boxes . The new bounding box annotations are determined based on the alignment of an object instance with the other training instances in the dataset . Our proposal enables the training data to be reused multiple times for training richer multi-component category models . We operationalize this idea by two complementary operations : bounding box shrinking , which finds subregions of an object instance that could be shared ; and bounding box enlarging , which enlarges object instances to include local contextual cues . We empirically validate our approach on the PASCAL VOC detection dataset .
2K_dev_2061	In practical applications of robot swarms with bio-inspired behaviors , a human operator will need to exert control over the swarm to fulfill the mission objectives . In many operational settings , human operators are remotely located and the communication environment is harsh . Hence , there exists some latency in information ( or control command ) transfer between the human and the swarm . In this paper , we conduct experiments of human-swarm interaction to investigate the effects of communication latency on the performance of a human-swarm system in a swarm foraging task . We develop and investigate the concept of neglect benevolence , where a human operator allows the swarm to evolve on its own and stabilize before giving new commands . Our experimental results indicate that operators exploited neglect benevolence in different ways to develop successful strategies in the foraging task . Furthermore , we show experimentally that the use of a predictive display can help mitigate the adverse effects of communication latency .
2K_dev_2062	Human interaction with robot swarms ( HSI ) is a young field with very few user studies that explore operator behavior . All these studies assume perfect communication between the operator and the swarm . A key challenge in the use of swarm robotic systems in human supervised tasks is to understand human swarm interaction in the presence of limited communication bandwidth , which is a constraint arising in many practical scenarios . In this paper , we present results of human-subject experiments designed to study the effect of bandwidth limitations in human swarm interaction . We consider three levels of bandwidth availability in a swarm foraging task . The lowest bandwidth condition performs poorly , but the medium and high bandwidth condition both perform well . In the medium bandwidth condition , we display useful aggregated swarm information ( like swarm centroid and spread ) to compress the swarm state information . We also observe interesting operator behavior and adaptation of operators ' swarm reaction .
2K_dev_2063	Pose Machines provide a sequential prediction framework for learning rich implicit spatial models . In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation . The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation . We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages , producing increasingly refined estimates for part locations , without the need for explicit graphical model-style inference . Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision , thereby replenishing back-propagated gradients and conditioning the learning procedure . We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII , LSP , and FLIC datasets .
2K_dev_2064	Finding meaningful , structured representations of 3D point cloud data ( PCD ) has become a core task for spatial perception applications . In this paper we introduce a method for constructing compact generative representations of PCD at multiple levels of detail . As opposed to deterministic structures such as voxel grids or octrees , we propose probabilistic subdivisions of the data through local mixture modeling , and show how these subdivisions can provide a maximum likelihood segmentation of the data . The final representation is hierarchical , compact , parametric , and statistically derived , facilitating run-time occupancy calculations through stochastic sampling . Unlike traditional deterministic spatial subdivision methods , our technique enables dynamic creation of voxel grids according the application 's best needs . In contrast to other generative models for PCD , we explicitly enforce sparsity among points and mixtures , a technique which we call expectation sparsification . This leads to a highly parallel hierarchical Expectation Maximization ( EM ) algorithm well-suited for the GPU and real-time execution . We explore the trade-offs between model fidelity and model size at various levels of detail , our tests showing favorable performance when compared to octree and NDT-based methods .
2K_dev_2065	How are the populations of the world likely to shift ? Which countries will be impacted by sea-level rise ? This paper uses a country-level agent-based dynamic network model to examine shifts in population given network relations among countries , which influences overall population change . Some of the networks considered include : alliance networks , shared language networks , economic influence networks , and proximity networks . Validation of model is done for migration probabilities between countries , as well as for country populations and distributions . The proposed framework provides a way to explore the interaction between climate change and policy factors at a global scale .
2K_dev_2066	Sudden weight gain in patients living with Congestive Heart Failure ( CHF ) is often an indication that the individual is retaining fluid , which often means that patient 's heart has weakened leading to increased risk of kidney or cardiac failure . Clinical interventions can be made at this stage , leading to better outcomes , however it is essential that the interventions take place before the patient 's health declines too drastically . In this work , we present a latent variable autoregression model that tracks patient weight and blood pressure over time , allowing us to predict weight values into the future . We are also able to model continuous heart-rate signals and evaluate a subject 's response to physical activity . This allows us to detect signs of health decline days earlier than existing rule-based systems , leading to the possibility of earlier clinical interventions , potentially preventing deadly medical emergencies .
2K_dev_2067	Many Android apps heavily depend on collecting and sharing sensitive privacy information , such as device ID , location , and postal address , to provide service and value . To protect user privacy , apps are typically required by market places to provide privacy policies informing users about how their private information will be processed . In this paper , we present PVDetector , an automatic tool that analyzes Android apps to detect privacy-policy violations , i.e. , inconsistencies between an app 's data collection code and the corresponding description in its privacy policy .
2K_dev_2068	This work focuses on data-driven discovery of the temporally co-occurring and contingent behavioral patterns that signal high and low interpersonal rapport . We mined a reciprocal peer tutoring corpus reliably annotated for nonverbals like eye gaze and smiles , conversational strategies like self-disclosure and social norm violation , and for rapport ( in 30s thin slices ) . We then performed a fine-grained investigation of how the temporal profiles of sequences of interlocutor behaviors predict increases and decreases of rapport , and how this rapport management manifests differently in friends and strangers . We validated the discovered behavioral patterns by predicting rapport against our ground truth via a forecasting model involving two-step fusion of learned temporal associated rules . Our framework performs significantly better than a baseline linear regression method that does not encode temporal information among behavioral features . Implications for the understanding of human behavior and social agent design are discussed .
2K_dev_2069	Human communication literature states that people with different culture backgrounds act differently in conversations . Currently most virtual agents are designed for a single targeted popular culture . We implemented two versions of a virtual agent targeting American and Chinese cultures . We found that users from different culture context express engagement differently .
2K_dev_2070	In this paper , we inspect the performance of regularized linear mixed effect models , as an extension of linear mixed effect model , when multiple confounding factors coexist . We first review its parameter estimation algorithms before we introduce three different methods for multiple confounding factors correction , namely concatenation , sequence , and interpolation . Then we investigate the performance on variable selection task and predictive task on three different data sets , synthetic data set , semi-empirical synthetic data set based on genome sequences and brain wave data set connecting to confused mental states . Our results suggest that sequence multiple confounding factors corrections behave the best when different confounders contribute equally to response variables . On the other hand , when various confounders affect the response variable unevenly , results mainly rely on the degree of how the major confounder is corrected .
