{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run 7: Load Consistent Groundtruth and Two-level Validation\n",
    "\n",
    "This notebook focuses on two things:\n",
    "1. <b>Consistently</b> associate snorkel Segments (i.e., clauses, or candidates) with their corresponding ground-truth label, i.e., candidates' count should match. \n",
    "2. Do a <b>two-level validation</b>: the first level will be the <b>dev set</b>: some validation on a small set of papers, whose author affiliation is from MLD, RI, HCI, CSD, LTI, ISR; then the second level, <b>test set</b> expands this dev set to be from general AI papers instead. \n",
    "\n",
    "Current `train/valid/test=5560/478/1842`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "debug_mode=1 # if not, debug_mode=0\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TSVDocPreprocessor\n",
    "session = SnorkelSession()\n",
    "\n",
    "# # Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 9000 # this is the upper limit of number of docs\n",
    "doc_preprocessor = TSVDocPreprocessor('data/70kpaper_061418_cleaned_noBookLecture_10cscw_2k.tsv', encoding=\"utf-8\",max_docs=n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such table: context [SQL: 'DELETE FROM context'] (Background on this error at: http://sqlalche.me/e/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1192\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1194\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: context",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/snorkel/snorkel/udf.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, xs, clear, parallelism, progress_bar, count, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mSnorkelSession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sessionmaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnorkelSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/snorkel/snorkel/parser/corpus_parser.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self, session, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m                                            fn=fn)\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# We cannot cascade up from child contexts to parent Candidates,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# so we delete all Candidates too\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/query.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, synchronize_session)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         delete_op = persistence.BulkDelete.factory(\n\u001b[1;32m   3282\u001b[0m             self, synchronize_session)\n\u001b[0;32m-> 3283\u001b[0;31m         \u001b[0mdelete_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdelete_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py\u001b[0m in \u001b[0;36mexec_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_pre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_pre_synchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_post_synchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py\u001b[0m in \u001b[0;36m_do_exec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1516\u001b[0m                                  self.context.whereclause)\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete_stmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py\u001b[0m in \u001b[0;36m_execute_stmt\u001b[0;34m(self, stmt)\u001b[0m\n\u001b[1;32m   1331\u001b[0m         self.result = self.query.session.execute(\n\u001b[1;32m   1332\u001b[0m             \u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m             mapper=self.mapper)\n\u001b[0m\u001b[1;32m   1334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, clause, params, mapper, bind, **kw)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         return self._connection_for_bind(\n\u001b[0;32m-> 1176\u001b[0;31m             bind, close_with_result=True).execute(clause, params or {})\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclause\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object, *multiparams, **params)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_clauseelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params)\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mdistilled_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistilled_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m         )\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m                 context)\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 util.raise_from_cause(\n\u001b[1;32m   1412\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m                     \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m                 )\n\u001b[1;32m   1415\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_from_cause\u001b[0;34m(exception, exc_info)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpy3k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb, cause)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1191\u001b[0m                         \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1194\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             self._handle_dbapi_exception(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: context [SQL: 'DELETE FROM context'] (Background on this error at: http://sqlalche.me/e/e3q8)"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such table: context [SQL: 'SELECT count(*) AS count_1 \\nFROM (SELECT document.id AS document_id, context.id AS context_id, context.type AS context_type, context.stable_id AS context_stable_id, document.name AS document_name, document.meta AS document_meta \\nFROM context JOIN document ON context.id = document.id) AS anon_1'] (Background on this error at: http://sqlalche.me/e/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1192\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1194\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: context",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-65ec81e3e0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msnorkel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentence\u001b[0m  \u001b[0;31m# defined in context.py file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Documents:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sentences:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/query.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3158\u001b[0m         \"\"\"\n\u001b[1;32m   3159\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynchronize_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/query.py\u001b[0m in \u001b[0;36mscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2911\u001b[0m         \"\"\"\n\u001b[1;32m   2912\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2913\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2914\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2915\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/query.py\u001b[0m in \u001b[0;36mone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2882\u001b[0m         \"\"\"\n\u001b[1;32m   2883\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2884\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_or_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2885\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0morm_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultipleResultsFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m             raise orm_exc.MultipleResultsFound(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/query.py\u001b[0m in \u001b[0;36mone_or_none\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m         \"\"\"\n\u001b[0;32m-> 2854\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2856\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/query.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2923\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autoflush\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_populate_existing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2924\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autoflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2925\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_and_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2927\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/orm/query.py\u001b[0m in \u001b[0;36m_execute_and_instances\u001b[0;34m(self, querycontext)\u001b[0m\n\u001b[1;32m   2946\u001b[0m             close_with_result=True)\n\u001b[1;32m   2947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2948\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquerycontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2949\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquerycontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquerycontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object, *multiparams, **params)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_clauseelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params)\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mdistilled_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0mcompiled_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistilled_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m         )\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m                 context)\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 util.raise_from_cause(\n\u001b[1;32m   1412\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m                     \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m                 )\n\u001b[1;32m   1415\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_from_cause\u001b[0;34m(exception, exc_info)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpy3k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb, cause)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1191\u001b[0m                         \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m                         context)\n\u001b[0m\u001b[1;32m   1194\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             self._handle_dbapi_exception(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: context [SQL: 'SELECT count(*) AS count_1 \\nFROM (SELECT document.id AS document_id, context.id AS context_id, context.type AS context_type, context.stable_id AS context_stable_id, document.name AS document_name, document.meta AS document_meta \\nFROM context JOIN document ON context.id = document.id) AS anon_1'] (Background on this error at: http://sqlalche.me/e/e3q8)"
     ]
    }
   ],
   "source": [
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs)\n",
    "\n",
    "from snorkel.models import Document, Sentence  # defined in context.py file\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following two cells should load in 7880 papers haven't been loaded. Skip them if you have already ran the two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 7880\n",
      "Sentences: 29036\n",
      "The longest sentence has 309 tokens.\n",
      "dev_sents length 2193 dev_doc_set size 478 avg n_sent per doc 4.588\n",
      "test_sents length 7963 test_doc_set size 1842 avg n_sent per doc 4.323\n"
     ]
    }
   ],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "session = SnorkelSession()\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "\n",
    "docs = session.query(Document).all()\n",
    "sents = session.query(Sentence).all()  # get all sentences from snorkel.db\n",
    "n_max_corpus=0\n",
    "for sent in sents:\n",
    "    n_max_corpus=max(n_max_corpus,len(sent.words))\n",
    "print(\"The longest sentence has \"+str(n_max_corpus)+\" tokens.\")\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "dev_doc_set = set()\n",
    "test_doc_set = set()\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name[:7]==\"2K_dev_\":\n",
    "            dev_sents.add(s)\n",
    "            dev_doc_set.add(doc.name)\n",
    "        elif doc.name[:8]==\"2K_test_\":\n",
    "            test_sents.add(s)\n",
    "            test_doc_set.add(doc.name)\n",
    "        else:\n",
    "            train_sents.add(s)\n",
    "            \n",
    "print(\"dev_sents length\", len(dev_sents),\"dev_doc_set size\", len(dev_doc_set), \"avg n_sent per doc\",\"%.3f\"%(float(len(dev_sents))/float(len(dev_doc_set))))\n",
    "print(\"test_sents length\", len(test_sents),\"test_doc_set size\", len(test_doc_set), \"avg n_sent per doc\",\"%.3f\"%(float(len(test_sents))/float(len(test_doc_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 18s, sys: 800 ms, total: 1min 19s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 35228**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 40.2 s, sys: 334 ms, total: 40.5 s\n",
      "Wall time: 40.9 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 6006**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 36s, sys: 1.27 s, total: 2min 38s\n",
      "Wall time: 2min 42s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 21687**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import *\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "\n",
    "def extract_and_display(matcher,candidate_class,candidate_class_name,train_breakdown_map=None,dev_doc_breakdown_map=None,selected_split=0,is_print=True):  # split over train/dev/test but returns only train set\n",
    "#     input(candidate_class)\n",
    "    for (i, sents) in ([(0,train_sents), (1,dev_sents), (2,test_sents)] if selected_split==0 else ([(2,test_sents)] if selected_split==2 else [(1,dev_sents)])):\n",
    "        %time matcher.apply(sents, split=i)\n",
    "        printmd(\"**Split \"+str(i)+\" - number of candidates extracted: \"+str(session.query(candidate_class).filter(candidate_class.split == i).count())+\"**\\n\\n\")\n",
    "    train_cands = session.query(candidate_class).filter(candidate_class.split == selected_split).all()\n",
    "    if is_print:\n",
    "        for i in range(min(4,len(train_cands))): # to print at most 4 cands \n",
    "            printmd(\"**\"+str(i)+\"/\"+str(len(train_cands))+\" Candidate/Span:**\\t`\"+str(train_cands[i])+\"`\")\n",
    "            printmd(\"**Its parent Sentence's text:**\\t\"+str(train_cands[i].get_parent().text))\n",
    "            printmd(\"**Its parent Document's text:**\\t\"+str(train_cands[i].get_parent().get_parent().__dict__))\n",
    "            print() \n",
    "        \n",
    "    for cand in train_cands:\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in train_breakdown_map:\n",
    "            train_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in train_breakdown_map[doc_name]:\n",
    "            train_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        train_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "        \n",
    "    dev_cands = session.query(candidate_class).filter(candidate_class.split == 1).all()\n",
    "    for cand in dev_cands:\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in dev_doc_breakdown_map:\n",
    "            dev_doc_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in dev_doc_breakdown_map[doc_name]:\n",
    "            dev_doc_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        dev_doc_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "    test_cands=session.query(candidate_class).filter(candidate_class.split==2).all()\n",
    "    \n",
    "    return train_cands,dev_cands,test_cands\n",
    "\n",
    "Segment = candidate_subclass('Segment', ['segment_cue'])\n",
    "ngrams = Ngrams(n_max=n_max_corpus) \n",
    "non_comma_matcher=DictionaryMatch(d=[','],longest_match_only=True,reverse=True)  \n",
    "non_comma_segment_extractor=CandidateExtractor(Segment, [ngrams], [non_comma_matcher])\n",
    "train_doc_breakdown_map=dict()\n",
    "dev_doc_breakdown_map=dict()\n",
    "train_segments,dev_segments,test_segments=extract_and_display(non_comma_segment_extractor,Segment,\"Segment\",train_doc_breakdown_map,dev_doc_breakdown_map,is_print=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many segments did Snorkel automatically extracted for dev set?\n",
    "<b>6006</b>, see about and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**All together we extracted 6006 segments**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A segment example** Span(\"b'For this goal in the context of social learning.'\", sentence=28625, chars=[114,161], words=[20,29])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Extracted text** For this goal in the context of social learning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A line in ground-truth file for this doc** `[Some_text]\tmechanism\t2K_dev_1230`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Can they get matched?** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This above example shows the thing that we are going to do: associate **extracted text** with **lines in ground-truth file**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(dev_segments[0].segment_cue.stable_id)\n",
    "from util import get_candidate_text\n",
    "printmd(\"**All together we extracted \"+str(len(dev_segments))+\" segments**\")\n",
    "printmd(\"**A segment example** \"+str(dev_segments[123].segment_cue))\n",
    "# printmd(\"**Its __dict__ element** \"+str(dev_segments[123].segment_cue.__dict__))\n",
    "# printmd(\"**Its parent Sentence** \"+str(dev_segments[123].segment_cue.sentence))\n",
    "printmd(\"**Extracted text** \"+str(get_candidate_text(dev_segments[123])))\n",
    "printmd(\"**A line in ground-truth file for this doc** `\"+str(\"[Some_text]\tmechanism\t2K_dev_1230`\"))\n",
    "printmd(\"**Can they get matched?** \"+str())\n",
    "printmd(\"This above example shows the thing that we are going to do: associate **extracted text** with **lines in ground-truth file**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_level=1\n",
    "current_docid_prefix=\"2K_dev\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 Small dev set groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Take a random example doc, what segments does it have<br /><br />** defaultdict(<function load_groundtruth_as_external_dict.<locals>.<lambda>.<locals>.<lambda> at 0x1a7102d400>, {'background': ['Finding meaningful structured representations of 3D point cloud data ( PCD ) has become a core task for spatial perception applications'], 'finding': ['our tests showing favorable performance when compared to octree and NDT-based methods'], 'mechanism': ['In this paper we introduce a method As opposed to deterministic structures such as voxel grids or octrees', 'we propose probabilistic subdivisions of the data through local mixture modeling', 'and show how these subdivisions can provide a maximum likelihood segmentation of the data', 'The final representation is hierarchical', 'compact parametric and statistically derived', 'facilitating run-time occupancy calculations through stochastic sampling', 'Unlike traditional deterministic spatial subdivision methods', \"our technique enables dynamic creation of voxel grids according the application 's best needs\", 'In contrast to other generative models for PCD', 'we explicitly enforce sparsity among points and mixtures', 'a technique which we call expectation sparsification', 'This leads to a highly parallel hierarchical Expectation Maximization ( EM ) algorithm well-suited for the GPU and real-time execution'], 'method': ['We explore the trade-offs between model fidelity and model size at various levels of detail'], 'purpose': ['for constructing compact generative representations of PCD at multiple levels of detail']})"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util import load_groundtruth_as_external_dict\n",
    "\n",
    "groundtruth_dict=load_groundtruth_as_external_dict(\"data/annotations_label-level_all-to-date-2018-4-25-WithTitle.labelled_level_\"+str(current_level)+\".csv\")\n",
    "printmd(\"**Take a random example doc, what segments does it have<br /><br />** \"+str(groundtruth_dict[current_docid_prefix+'_2064']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Generate a file  `[segment_name]_gold_dev.tsv`, which contains gold labels from dev sets\n",
    "\n",
    "Each line has the format of `[stable_label_id]\\t[label]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched - striped_query_text:  To advance on this insight to communicate planned adaptation and note the information ( logs and labels ) needed to make the desired inferences. Extracted six design patterns where UI adaptation can improve in-app navigation\n",
      "Unmatched - striped_query_text:  that is the 90-th percentile distance We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. We provide theoretical justification for this pattern under successful existing network models\n",
      "Unmatched - striped_query_text:  up to a difference of at most e. We establish a lower bound of ( ln ( 1/e ) /lnln ( 1/e ) ) on the complexity of this problem\n",
      "Unmatched - striped_query_text:  This introduces a combinatorial structure on the decision space We propose to address these challenges for achieving this approximation in an online fashion. Demonstrates the effectiveness of our approach\n",
      "Unmatched - striped_query_text:  the formation of micro-clusters in appropriate feature spaces ) ; and. ( b )\n",
      "Unmatched - striped_query_text:  allows SNC-Meister to pack together many more tenants. : in experiments with production traces\n",
      "Unmatched - striped_query_text:  lower bounding and lazy bounding. Validate the approach and show that our techniques dramatically improve scalability over a leading general-purpose MIP solver\n",
      "Unmatched - striped_query_text:  For general convex programming. Show that this often improves running times by an order of magnitude or more vs\n",
      "Unmatched - striped_query_text:  Of optimal voting under adversarial noise. Show that our approach produces significantly more accurate rankings than alternative approaches\n",
      "Unmatched - striped_query_text:  implicit trust and explicit distrust. Confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy\n",
      "Unmatched - striped_query_text:  differential substitutions and derivations as first-class axioms in dL. This paper introduces a new proof calculus that is entirely based on uniform substitution\n",
      "Unmatched - striped_query_text:  This paper is ( i ) to assume periodic statistics without needing to revisit the negative set and ( ii ) to accelerate the estimation of detectors with aperiodic statistics. Verified that periodicity is detrimental\n",
      "Unmatched - striped_query_text:  given that the problem of enumerating all pure-strategy SNEs is trivially in P. Our central result is that\n",
      "Unmatched - striped_query_text:  and may fail to scale to larger exchanges the clearing problem is solvable in polynomial time. Show that indeed small numbers of attributes suffice\n",
      "Unmatched - striped_query_text:  For single-view reasoning about 3D surfaces and their relationships. We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces\n",
      "Unmatched - striped_query_text:  Page Rank ) can provide a ranking that has the same ac- curacy in predicting winners of upcoming match-ups as more complicated systems ( e. We further explore the impact of the network structure on the prediction accuracy and\n",
      "Unmatched - striped_query_text:  There are numerous papers that study the problem of fairly dividing a cake. ; a small number of them also take into account self-interested agents and consequent strategic issues\n",
      "Unmatched - striped_query_text:  over time for which there are few published models ?. ; and we observe power law growth for both nodes and links\n",
      "Unmatched - striped_query_text:  to more accurately detect groups of fraudulent users. Showed that HoloScope achieved significant accuracy improvements on synthetic and real data\n",
      "Unmatched - striped_query_text:  including those mentioned above. Updates by our algorithms are up to a million times faster than the fastest batch algorithms Effective : our DENSESALERT successfully spots anomalies especially those overlooked by existing algorithms\n",
      "Unmatched - striped_query_text:  In this paper we discuss the algorithms used. We find that affect-and pose-based segmentation are more effective\n",
      "Unmatched - striped_query_text:  to distinguish between compromised vs. First we find that legitimate but compromised websites constitute 33\n",
      "\n",
      "\n",
      "Out of 6006, 22 are unmatched, please double check\n"
     ]
    }
   ],
   "source": [
    "from util import write_segment_name                        \n",
    "write_segment_name(dev_segments,\"data/purpose_gold_dev.tsv\",groundtruth_dict,segment_name=\"purpose\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Gold Labels from a real purpose_gold.tsv and evaluate one toy LF with this labeled dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels newly created: 5984\n",
      "AnnotatorLabels newly created: 0\n",
      "CPU times: user 57.1 s, sys: 1.03 s, total: 58.1 s\n",
      "Wall time: 60 s\n",
      "\n",
      "6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "%time external = load_external_labels(session, Segment, annotator_name='purpose_dev',isPrint=False,file_path=\"data/purpose_gold_dev.tsv\")\n",
    "print()\n",
    "\n",
    "# (Optional) Reload these gold labels from snorkel.db (for cases when you re-opened the snorkel.db in a different notebook)\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='purpose_dev', split=1)\n",
    "print((L_gold_dev).shape[0])\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LF.util_purpose_default import LF_comparative_degree_morethan,LF_comparative_degree_fooerthan,LF_purpose_verb_inorderto,LF_purpose_verb_implication,LF_purpose_verb_solve,LF_purpose_verb_hypothesis,LF_purpose_verb_toenable,LF_purpose_verb_toaid,LF_purpose_verb_toproduce,LF_purpose_verb_toinvestigate,LF_purpose_verb_togive,LF_purpose_verb_thatcan,LF_purpose_verb_examine,LF_purpose_verb_extend,LF_purpose_verb_offer\n",
    "from LF.util_common_default import negate, LF_expressing_contrast_however, LF_expressing_contrast_nevertheless, LF_expressing_contrast_despite, LF_expressing_contrast_spite, LF_expressing_contrast_yet, LF_expressing_contrast_but, LF_excluded_pseudo_contrast_butsometimes, LF_excluded_pseudo_contrast_butalso\n",
    "from LF.util_mechanism_default import LF_mechanism_verb_introduce, LF_mechanism_verb_propose, LF_mechanism_verb_develop, LF_mechanism_verb_approach, LF_mechanism_verb_applied, LF_mechanism_verb_apply, LF_mechanism_verb_develop, LF_mechanism_verb_using, LF_mechanism_verb_present, LF_mechanism_verb_contribute, LF_mechanism_verb_build, LF_mechanism_verb_built  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LF.util_purpose_default import LF_comparative_degree,LF_purpose_verb,LF_purpose_delimiter,LF_purpose_adj_problem,LF_purpose_leading_question_word\n",
    "from LF.util_common_default import LF_expressing_contrast, LF_excluded_pseudo_contrast,negate\n",
    "from LF.util_mechanism_default import LF_mechanism_verb,LF_mechanism_adv,LF_mechanism_noun,LF_mechanism_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate and tuning more LFs based on dev set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 6006\n",
      "test_marginals [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 1.0\n",
      "Neg. class recall: 0.0\n",
      "Precision            0.285\n",
      "Recall               1.0\n",
      "F1                   0.443\n",
      "----------------------------------------\n",
      "TP: 72 | FP: 181 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def LF_purpose_verb_test0(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*for study of.*$)|(.*in order to.*$)|(.* implication.*$)|(.* to solve.*$)|(.* hypothesis.*$)|(.*to enable.*$)|(.*to aid.*$)|(.*to produce.*$)|(.*to discuss.*$)|(.*to investigat.*$)|(.* give.*$)|(.* that can .*$)|(.* examine.*$)|(.* extend.*$)|(.* offer.*$)\",1) else 0\n",
    "\n",
    "# print(LF_purpose_verb_test0(dev_segments[19]))\n",
    "# print(dev_segments[19].segment_cue.stable_id)\n",
    "# print(dev_segments[19].__dict__)\n",
    "from snorkel.lf_helpers import test_LF\n",
    "tp, fp, tn, fn = test_LF(session, LF_purpose_verb_test0, split=1, annotator_name='purpose_dev',test_labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 6006\n",
      "test_marginals [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 1.0\n",
      "Neg. class recall: 0.0\n",
      "Precision            0.667\n",
      "Recall               1.0\n",
      "F1                   0.8\n",
      "----------------------------------------\n",
      "TP: 4 | FP: 2 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def LF_purpose_single_verb_test1(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*to enable.*$)\",1) else 0\n",
    "    \n",
    "tp, fp, tn, fn = test_LF(session, LF_purpose_single_verb_test1, split=1, annotator_name='purpose_dev',test_labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 6006\n",
      "test_marginals [1.  0.5 0.5 ... 0.5 0.5 0.5]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 1.0\n",
      "Neg. class recall: 0.0\n",
      "Precision            0.253\n",
      "Recall               1.0\n",
      "F1                   0.404\n",
      "----------------------------------------\n",
      "TP: 361 | FP: 1064 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def LF_purpose_single_verb_test2(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*for.*$)\",1) else 0\n",
    "    \n",
    "tp, fp, tn, fn = test_LF(session, LF_purpose_single_verb_test2, split=1, annotator_name='purpose_dev',test_labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 6006\n",
      "test_marginals [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 0.0\n",
      "Neg. class recall: 1.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 88 | FN: 5\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def negate_LF_mechanism_verb_test(c):\n",
    "    # where LF_mechanism_verb() is defined as: return 1 if rule_regex_search_candidate_text(c,\"((^|\\s)(introduce|propose|develop|approach|applied|apply|using|present|contribute|build|built).*$)\",1) else 0\n",
    "    return -1 if LF_mechanism_verb_introduce(c)==1 else 0   \n",
    "\n",
    "tp, fp, tn, fn = test_LF(session, negate_LF_mechanism_verb_test, split=1, annotator_name='purpose_dev',test_labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading more realistic LFs in aggregation, train a generative model, evaluate the model on dev set\n",
    "\n",
    "\n",
    "<b>Issue</b> the difference between `learned accuracy` and `empirical accuracy`.\n",
    "\n",
    "<b>Guess:</b>\n",
    "\n",
    "1. Learned accuracy is evaluating each LF <b>w.r.t. trained generative model</b>\n",
    "\n",
    "2. Empirical accuracy is evaluating each LF <b>w.r.t. gold label</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35228\n",
      "0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 58s, sys: 447 ms, total: 1min 58s\n",
      "Wall time: 1min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<35228x16 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1832 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "purpose_LFs=[LF_comparative_degree_morethan,LF_comparative_degree_fooerthan,LF_purpose_verb_inorderto,LF_purpose_verb_implication,LF_purpose_verb_solve,LF_purpose_verb_hypothesis,LF_purpose_verb_toenable,LF_purpose_verb_toaid,LF_purpose_verb_toproduce,LF_purpose_verb_toinvestigate,LF_purpose_verb_togive,LF_purpose_verb_thatcan,LF_purpose_verb_examine,LF_purpose_verb_extend,LF_purpose_verb_offer]\n",
    "purpose_LFs+=[negate(LF_mechanism_verb_introduce), negate(LF_mechanism_verb_propose), negate(LF_mechanism_verb_develop), negate(LF_mechanism_verb_approach), negate(LF_mechanism_verb_applied), negate(LF_mechanism_verb_apply), negate(LF_mechanism_verb_develop), negate(LF_mechanism_verb_using), negate(LF_mechanism_verb_present), negate(LF_mechanism_verb_contribute), negate(LF_mechanism_verb_build), negate(LF_mechanism_verb_built)]\n",
    "\n",
    "labeler = LabelAnnotator(lfs=purpose_LFs)\n",
    "\n",
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation: do a live dev accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  j  Coverage  Overlaps  Conflicts\n",
      "LF_comparative_degree_morethan    0  0.003861  0.000397   0.000000\n",
      "LF_comparative_degree_fooerthan   1  0.003974  0.000312   0.000000\n",
      "LF_purpose_verb_inorderto         2  0.003577  0.000255   0.000057\n",
      "LF_purpose_verb_implication       3  0.000710  0.000028   0.000028\n",
      "LF_purpose_verb_solve             4  0.001192  0.000085   0.000028\n",
      "LF_purpose_verb_hypothesis        5  0.001022  0.000199   0.000028\n",
      "LF_purpose_verb_toenable          6  0.001249  0.000057   0.000000\n",
      "LF_purpose_verb_toaid             7  0.000170  0.000000   0.000000\n",
      "LF_purpose_verb_toproduce         8  0.001050  0.000142   0.000000\n",
      "LF_purpose_verb_toinvestigate     9  0.000625  0.000028   0.000000\n",
      "LF_purpose_verb_togive           10  0.012348  0.000965   0.000085\n",
      "LF_purpose_verb_thatcan          11  0.004343  0.000426   0.000170\n",
      "LF_purpose_verb_examine          12  0.003293  0.000170   0.000000\n",
      "LF_purpose_verb_extend           13  0.004485  0.000483   0.000000\n",
      "LF_purpose_verb_offer            14  0.002413  0.000284   0.000000\n",
      "<lambda>                         15  0.007693  0.000397   0.000397\n"
     ]
    }
   ],
   "source": [
    "L_purpose_train = L_train\n",
    "print(L_purpose_train.lf_stats(session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n",
      "Finished training generative model, now checking performance against development set labels...\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=0.0)\n",
    "\n",
    "print(\"Finished training generative model, now checking performance against development set labels...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6006\n",
      "0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 36.6 s, sys: 329 ms, total: 37 s\n",
      "Wall time: 37.6 s\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 0.0669\n",
      "Neg. class recall: 0.947\n",
      "Precision            0.234\n",
      "Recall               0.0669\n",
      "F1                   0.104\n",
      "----------------------------------------\n",
      "TP: 78 | FP: 255 | TN: 4563 | FN: 1088\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinq/Desktop/snorkel/snorkel/annotations.py:137: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ac = (tp+tn) / (tp+tn+fp+fn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_comparative_degree_morethan</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.553651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_comparative_degree_fooerthan</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009158</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.565432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_inorderto</th>\n",
       "      <td>2</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.568488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_implication</th>\n",
       "      <td>3</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_solve</th>\n",
       "      <td>4</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_hypothesis</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_toenable</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.557996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_toaid</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.560078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_toproduce</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.576313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_toinvestigate</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.561792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_togive</th>\n",
       "      <td>10</td>\n",
       "      <td>0.017816</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>31</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.289720</td>\n",
       "      <td>0.557826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_thatcan</th>\n",
       "      <td>11</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.569676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_examine</th>\n",
       "      <td>12</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.579634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_extend</th>\n",
       "      <td>13</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.577783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_offer</th>\n",
       "      <td>14</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.562425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>15</td>\n",
       "      <td>0.015818</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>0.564378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  j  Coverage  Overlaps  Conflicts  TP  FP  \\\n",
       "LF_comparative_degree_morethan    0  0.006660  0.001332   0.000167   3  36   \n",
       "LF_comparative_degree_fooerthan   1  0.009158  0.001499   0.000000   3  51   \n",
       "LF_purpose_verb_inorderto         2  0.004496  0.000500   0.000000  10  17   \n",
       "LF_purpose_verb_implication       3  0.001998  0.000000   0.000000   0  12   \n",
       "LF_purpose_verb_solve             4  0.002331  0.000167   0.000000   5   9   \n",
       "LF_purpose_verb_hypothesis        5  0.000500  0.000000   0.000000   0   3   \n",
       "LF_purpose_verb_toenable          6  0.000999  0.000167   0.000000   4   2   \n",
       "LF_purpose_verb_toaid             7  0.000000  0.000000   0.000000   0   0   \n",
       "LF_purpose_verb_toproduce         8  0.000500  0.000000   0.000000   1   2   \n",
       "LF_purpose_verb_toinvestigate     9  0.000833  0.000000   0.000000   3   2   \n",
       "LF_purpose_verb_togive           10  0.017816  0.001832   0.000167  31  76   \n",
       "LF_purpose_verb_thatcan          11  0.003830  0.000333   0.000167   5  18   \n",
       "LF_purpose_verb_examine          12  0.002498  0.000500   0.000000   5  10   \n",
       "LF_purpose_verb_extend           13  0.003663  0.000167   0.000167   3  19   \n",
       "LF_purpose_verb_offer            14  0.003996  0.000167   0.000000   7  17   \n",
       "<lambda>                         15  0.015818  0.000666   0.000666   0   0   \n",
       "\n",
       "                                 FN  TN  Empirical Acc.  Learned Acc.  \n",
       "LF_comparative_degree_morethan    0   0        0.076923      0.553651  \n",
       "LF_comparative_degree_fooerthan   0   0        0.055556      0.565432  \n",
       "LF_purpose_verb_inorderto         0   0        0.370370      0.568488  \n",
       "LF_purpose_verb_implication       0   0        0.000000      0.560826  \n",
       "LF_purpose_verb_solve             0   0        0.357143      0.571429  \n",
       "LF_purpose_verb_hypothesis        0   0        0.000000      0.568132  \n",
       "LF_purpose_verb_toenable          0   0        0.666667      0.557996  \n",
       "LF_purpose_verb_toaid             0   0             NaN      0.560078  \n",
       "LF_purpose_verb_toproduce         0   0        0.333333      0.576313  \n",
       "LF_purpose_verb_toinvestigate     0   0        0.600000      0.561792  \n",
       "LF_purpose_verb_togive            0   0        0.289720      0.557826  \n",
       "LF_purpose_verb_thatcan           0   0        0.217391      0.569676  \n",
       "LF_purpose_verb_examine           0   0        0.333333      0.579634  \n",
       "LF_purpose_verb_extend            0   0        0.136364      0.577783  \n",
       "LF_purpose_verb_offer             0   0        0.291667      0.562425  \n",
       "<lambda>                          5  88        0.946237      0.564378  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "%time L_dev = labeler.apply_existing(split=1)\n",
    "_ = gen_model.error_analysis(session, L_dev, L_gold_dev,set_unlabeled_as_neg=False)\n",
    "\n",
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])  # learned_lf_stats(): For each labeling function, estimates of a few metrics are provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Repeat Section 2 with a larger test set groundtruth\n",
    "\n",
    "This requires loading the `L_gold_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched - striped_query_text:  up to a difference of at most e. We establish a lower bound of ( ln ( 1/e ) /lnln ( 1/e ) ) on the complexity of this problem\n",
      "Unmatched - striped_query_text:  we develop a classifier to distinguish between compromised vs. We conduct an extensive study and follow a website-centric and user-centric point of view\n",
      "Unmatched - striped_query_text:  How do individuals perceive algorithmic vs. We investigated people 's perceptions of mathematically-proven fair division algorithms making social division decisions\n",
      "Unmatched - striped_query_text:  Spoken Term Detection ( STD ) or Keyword Search ( KWS ) techniques can locate keyword instances but do not differentiate between meanings. Spoken Word Sense Induction ( SWSI )\n",
      "Unmatched - striped_query_text:  Next-generation information technologies will process unprecedented amounts of loosely structured data that overwhelm existing computing systems. Improves the energy efficiency of abundant-data applications\n",
      "Unmatched - striped_query_text:  For single-view reasoning about 3D surfaces and their relationships. We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces\n",
      "Unmatched - striped_query_text:  a framework and then use these models This paper presents the data-driven framework. Analyzes its behavior under unmodeled multipath interference from experimental measurements of an aluminum plate\n",
      "Unmatched - striped_query_text:  However how to effectively select the high-level semantic meaningful concepts from a large pool to assist complex event detection is rarely studied in the literature to automatically select semantic meaningful concepts for the event detection task. Demonstrate the efficacy of our proposed method\n",
      "Unmatched - striped_query_text:  which enables rich around-device. Suggesting that AuraSense can be low latency and robust across users and environments\n",
      "Unmatched - striped_query_text:  We show two recomposition methods using the Spectral Decomposition and Singular Value Decomposition equations. We also show results on both yaw and pitch estimation on the Pointing'04 dataset\n",
      "Unmatched - striped_query_text:  that is the 90-th percentile distance We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. We provide theoretical justification for this pattern under successful existing network models\n",
      "Unmatched - striped_query_text:  We conclude that a linear correlation ( the normalized covariance matrix ) captures the spatial relationship in most situations although it is significantly sensitive to choosing the appropriate window size. We conducted analyses on three different test beds where temperature measurements from 10 sensors were collected every minute\n",
      "Unmatched - striped_query_text:  in a restaurant outdoor ) and actions ( e. The positive exemplars which exactly convey the precise semantic of an event are hard to obtain\n",
      "Unmatched - striped_query_text:  that addresses this problem. { '' } Our approach can correctly discover links between regions of the same object even if they are captured from dramatically different viewpoints\n",
      "Unmatched - striped_query_text:  to yield useful and non-obvious inspirations for solutions. Crowd workers drawing inspirations from the distant domains produced more creative solutions to the original problem than did those who sought inspiration on their own\n",
      "Unmatched - striped_query_text:  to more accurately detect groups of fraudulent users. Showed that HoloScope achieved significant accuracy improvements on synthetic and real data\n",
      "Unmatched - striped_query_text:  including by iteratively advancing a solution. --\n",
      "Unmatched - striped_query_text:  to address straggler threads. Confirm the significance of the problem and the effectiveness of FlexRR 's solution\n",
      "Unmatched - striped_query_text:  Mainstream crowdwork platforms treat microtasks as indivisible units We reflect on the implications of these findings for the design of future crowd work platforms that effectively harness the potential of subcontracting workflows. ; however in this article\n",
      "Unmatched - striped_query_text:  Finally we describe the outcome of two tasks on Mechanical Turk meant to simulate aspects of subcontracting. :\n",
      "Unmatched - striped_query_text:  and the dynamics profiles of the A92E and G94D CypA escape mutants closely resemble that of wild-type CA assembly in complex with CypA. Through the analysis of backbone 1H-15N and 1H-13C dipolar tensors and peak intensities from 3D MAS NMR spectra of wild-type and the A92E and G94D CypA escape mutants\n",
      "Unmatched - striped_query_text:  whereas a speed only encoding model is adequate for LFP L. Differences in experimental paradigms\n",
      "Unmatched - striped_query_text:  Instead we enforce a necessary condition on the sum of squared limb-lengths that can be solved for in closed form to discourage implausible configurations in 3D. We evaluate performance on a wide variety of human poses captured from different viewpoints and\n",
      "Unmatched - striped_query_text:  In this work we explore how organization occurs in the kitchen to the problem of object return. Qualitative insights towards robot behavior during kitchen organization\n",
      "Unmatched - striped_query_text:  This introduces a combinatorial structure on the decision space We propose to address these challenges for achieving this approximation in an online fashion. Demonstrates the effectiveness of our approach\n",
      "Unmatched - striped_query_text:  that minimizes the overall number of such pairwise judgments needed. We show that with significantly lower number of pairwise judgments and feature-engineering effort\n",
      "Unmatched - striped_query_text:  consumers researching new purchases or patients learning about their conditions. As they search people build up rich mental schemas about their target domains ; which\n",
      "Unmatched - striped_query_text:  is an important rheostat of T-cell signaling. Spatiotemporalsignaling distributions\n",
      "Unmatched - striped_query_text:  We investigate the welfare implications and the allocative effects of different consumer data '' handling regimes in online targeted advertisin g. We find that there exist conditions under which the intermediary obtains the highest proportion of benefits from targeting and\n",
      "Unmatched - striped_query_text:  thereby making the latent representation more difficult to interpret that performs dimensionality reduction. Demonstrate that when time delays are present\n",
      "Unmatched - striped_query_text:  offering up to an order of magnitude performance improvement compared to the CPU and GPU memory subsystems which does not employ HAMLeT. This paper proposes a high-bandwidth and energy-efficient hardware accelerated memory layout transform ( HAMLeT ) system integrated within a 3D-stacked DRAM\n",
      "Unmatched - striped_query_text:  This paper studies attackers with control objectives against cyber-physical systems ( CPS ). The system is equipped with its own controller and attack detector\n",
      "Unmatched - striped_query_text:  community engagement and relatable prototyping materials in the iterative design of prosthetics. This paper presents to design prosthetic devices for specific tasks : playing the cello\n",
      "Unmatched - striped_query_text:  estimates core numbers of nodes. Demonstrate that Nimble Core gives space savings up to 60X\n",
      "Unmatched - striped_query_text:  was found to perform best among the different alternatives explored. Automating the training and classification process using these devices\n",
      "Unmatched - striped_query_text:  Current research is interested in identifying how topology impacts epidemics in networks. We can obtain a closed form description of the equilibrium distribution\n",
      "Unmatched - striped_query_text:  achieving cannulation and maintaining cannulation during drug delivery currently limit the feasibility of the procedure we respond to these problems. Demonstrates a significant improvement in the total time the needle could be maintained stably inside of the vein\n",
      "Unmatched - striped_query_text:  The timeline abstraction enables developers. Leveraging open source hardware and software components\n",
      "Unmatched - striped_query_text:  To assist this process that allows early incomplete 2D sketches to be transformed into suggestive complete models. We demonstrate and discuss preliminary results of our technique on 2D shape design problems\n",
      "Unmatched - striped_query_text:  allows SNC-Meister to pack together many more tenants. : in experiments with production traces\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched - striped_query_text:  that allows programmers to selectively undo fine-grained code changes made in the code editor. Showed that programmers can successfully use AZURITE\n",
      "Unmatched - striped_query_text:  differential substitutions and derivations as first-class axioms in dL. This paper introduces a new proof calculus that is entirely based on uniform substitution\n",
      "Unmatched - striped_query_text:  This paper is ( i ) to assume periodic statistics without needing to revisit the negative set and ( ii ) to accelerate the estimation of detectors with aperiodic statistics. Verified that periodicity is detrimental\n",
      "Unmatched - striped_query_text:  over time for which there are few published models ?. ; and we observe power law growth for both nodes and links\n",
      "Unmatched - striped_query_text:  labeling of the training data does not require prior knowledge about the damage characteristics ( e. In this paper a s upervised method is proposed That is\n",
      "Unmatched - striped_query_text:  Will future smart-home owners have to scroll though pages of apps to select and dim their lights to discover and rapidly utilize contextual functionality. Suggests high accuracy 98\n",
      "Unmatched - striped_query_text:  We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. { '' } We demonstrate the algorithm 's efficacy along a variety of metrics and datasets\n",
      "Unmatched - striped_query_text:  including those mentioned above. Updates by our algorithms are up to a million times faster than the fastest batch algorithms Effective : our DENSESALERT successfully spots anomalies especially those overlooked by existing algorithms\n",
      "Unmatched - striped_query_text:  To provide a novel driver behavior situational awareness system ( DB-SAW ). Abstract This paper presents a Grammar-aware Driver Parsing ( GDP ) algorithm\n",
      "Unmatched - striped_query_text:  ensuring patient safety for automatic 3D steering of manually inserted flexible needles. Demonstrate the performance of the proposed controller results also show the feasibility of this technique in 2D and 3D environments\n",
      "Unmatched - striped_query_text:  Bevel-tipped flexible needles can be robotically steered to reach clinical targets along curvilinear paths in 3D. Manual needle insertion allows the clinician to control the insertion speed\n",
      "Unmatched - striped_query_text:  We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. Under successful existing network models\n",
      "Unmatched - striped_query_text:  For compositional verification of security properties of extensible hypervisors written in C and Assembly. We validate uSpark and demonstrating only minor performance overhead with low verification costs\n",
      "Unmatched - striped_query_text:  This study explores the prevalence of Facebook groups associated with courses from MITx and HarvardX. Results suggests that a non-trivial number of MOOC students engage in Facebook groups\n",
      "Unmatched - striped_query_text:  There has been limited systematic investigation into how disciplinary and interdisciplinary stakeholders understand design features in G4H. We found evidence of conceptual differences suggesting that a G4H perspective is not simply the sum of game and health perspectives\n",
      "Unmatched - striped_query_text:  For disease outbreak detection. We present a new method the `` Non-Parametric Heterogeneous Graph Scan ( NPHGS ) ''\n",
      "Unmatched - striped_query_text:  To enhance the existing system 's localization capability. Showed promising results in terms of counting the number of road-lanes and the indices of the current road-lanes\n",
      "Unmatched - striped_query_text:  the local agent estimates are as good as the optimal centralized nonlinear least squares estimator having access to the entire network observation data at all times. Conforming to a given inter-agent communication or interaction topology\n",
      "Unmatched - striped_query_text:  consumers researching new purchases or patients learning about their conditions for others with similar interests. As they search people build up rich mental schemas about their target domains ; which\n",
      "Unmatched - striped_query_text:  given that the problem of enumerating all pure-strategy SNEs is trivially in P. Our central result is that\n",
      "Unmatched - striped_query_text:  multi-core parallelism and SIMD execution of programs. Exhibiting significant performance improvements over existing compilers\n",
      "Unmatched - striped_query_text:  grouping joining sorting and filtering hierarchical data in spreadsheets. Showed that our tool helped spreadsheet users complete data exploration tasks nearly two times faster than using Excel and even outperform programmers in most tasks\n",
      "Unmatched - striped_query_text:  For general convex programming. Show that this often improves running times by an order of magnitude or more vs\n",
      "Unmatched - striped_query_text:  implicit trust and explicit distrust. Confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy\n",
      "Unmatched - striped_query_text:  The work provides a framework for understanding and modulating plant responses to stress. Mapped the complex gene regulatory networks involved in the response to the plant hormone abscisic acid\n",
      "Unmatched - striped_query_text:  There are numerous papers that study the problem of fairly dividing a cake. ; a small number of them also take into account self-interested agents and consequent strategic issues\n",
      "Unmatched - striped_query_text:  For automated intraocular laser surgery. We compared the performance of the automated scanning using various control thresholds\n",
      "Unmatched - striped_query_text:  Thus it appears that disorders of the Rb/E2F axis can arise at multiple organ sites and produce tumors that simultaneously overexpress multiple E2F-responsive genes. Reasoning that overexpression of multiple E2F-responsive genes might be a useful marker for RB1 dysfunction\n",
      "Unmatched - striped_query_text:  In this paper we discuss the algorithms used. We find that affect-and pose-based segmentation are more effective\n",
      "Unmatched - striped_query_text:  We demonstrate the reasoning principles of System M. Motivated by these systems\n",
      "Unmatched - striped_query_text:  Crowd workers are distributed and decentralized. While decentralization is designed to utilize independent judgment to promote high-quality results\n",
      "Unmatched - striped_query_text:  but these scores are notoriously inflated and uninformative. Crowd guilds produced reputation signals more strongly correlated with ground-truth worker quality than signals available on current crowd working platforms\n",
      "Unmatched - striped_query_text:  To accelerate the processing of sparse matrix data that is held in a 3D DRAM system. Demonstrates more than two orders of magnitude of performance and energy efficiency improvements compared with the traditional multithreaded software implementation on modern processors\n",
      "Unmatched - striped_query_text:  To advance on this insight to communicate planned adaptation and note the information ( logs and labels ) needed to make the desired inferences. Extracted six design patterns where UI adaptation can improve in-app navigation\n",
      "Unmatched - striped_query_text:  We illustrate both new reasoning principles of System M. System M is a new program logic System M extends Hoare Type Theory ( HTT ) First\n",
      "Unmatched - striped_query_text:  Of optimal voting under adversarial noise. Show that our approach produces significantly more accurate rankings than alternative approaches\n",
      "Unmatched - striped_query_text:  For gated recurrent neural networks that can be used in single-pass applications. Highlight the behavior and efficacy of such networks show that these networks\n",
      "Unmatched - striped_query_text:  To automatically determine if a driver is holding a cell phone close to one of his/her ears ( thus keeping only one hand on the steering wheel ). Demonstrate the method 's efficacy\n",
      "Unmatched - striped_query_text:  we describe an active learning strategy by asking the most informative questions to human experts at each step of coreference resolution. Positing that these pairwise judgments are easy to obtain from humans given the right context\n",
      "Unmatched - striped_query_text:  Page Rank ) can provide a ranking that has the same ac- curacy in predicting winners of upcoming match-ups as more complicated systems ( e. We further explore the impact of the network structure on the prediction accuracy and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched - striped_query_text:  and may fail to scale to larger exchanges the clearing problem is solvable in polynomial time. Show that indeed small numbers of attributes suffice\n",
      "Unmatched - striped_query_text:  to predict enhancer-promoter interactions. Demonstrate that SPEID is effective in predicting enhancer-promoter interactions as compared to state-of-the-art methods that use non-sequence features from functional genomic signals\n",
      "Unmatched - striped_query_text:  a previously unformalized area. Leveraging this connection we provide a systematic black-box methodology based on experimental science and statistical analysis\n",
      "Unmatched - striped_query_text:  lower bounding and lazy bounding. Validate the approach and show that our techniques dramatically improve scalability over a leading general-purpose MIP solver\n",
      "Unmatched - striped_query_text:  whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. Validates the accuracy of sentence and attribute generation\n",
      "Unmatched - striped_query_text:  we will focus on this area to model tumor progression in non-small cell lung cancer ( NSCLC ). Validating our findings in independent datasets Further\n",
      "Unmatched - striped_query_text:  which in turn has high human annotation cost. Demonstrate the effectiveness of the proposed approach\n",
      "Unmatched - striped_query_text:  Current methods do not scale to large training sets either because of training time complexity I I or test time complexity I I. We consider the problem of discovering discriminative exemplars suitable for object detection\n",
      "Unmatched - striped_query_text:  The experimental results demonstrate that learning using SVMs and proposed eye movement features improves detection performance and that personalization further improves results. )\n",
      "Unmatched - striped_query_text:  Therefore a biological feature selection method is needed to reorganize these spatio-temporal features and represent the video in a feature space to select more efficient features for activity analysis. Validate our activity analysis approach is more effective than state-of-the-art methods\n",
      "Unmatched - striped_query_text:  the formation of micro-clusters in appropriate feature spaces ) ; and. ( b )\n",
      "Unmatched - striped_query_text:  to distinguish between compromised vs. First we find that legitimate but compromised websites constitute 33\n",
      "Unmatched - striped_query_text:  which manages the exploration of many state spaces to prioritize jobs to add new preemption points on the fly. 25x\n",
      "Unmatched - striped_query_text:  That is pose-tolerant under unconstrained face matching scenarios. And\n",
      "\n",
      "\n",
      "Out of 21687, 94 are unmatched, please double check\n"
     ]
    }
   ],
   "source": [
    "from util import load_groundtruth_as_external_dict\n",
    "current_level=2\n",
    "groundtruth_dict_test=load_groundtruth_as_external_dict(\"data/annotations_label-level_all-to-date-2018-4-25-WithTitle.labelled_level_\"+str(current_level)+\".csv\")\n",
    "\n",
    "from util import write_segment_name                        \n",
    "write_segment_name(test_segments,\"data/purpose_gold_test.tsv\",groundtruth_dict_test,segment_name=\"purpose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels newly created: 0\n",
      "AnnotatorLabels newly created: 21593\n",
      "CPU times: user 3min 24s, sys: 2.38 s, total: 3min 27s\n",
      "Wall time: 3min 30s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "%time external_test = load_external_labels(session, Segment, annotator_name='purpose_test',isPrint=False,file_path=\"data/purpose_gold_test.tsv\")\n",
    "print()\n",
    "\n",
    "# (Optional) Reload these gold labels from snorkel.db (for cases when you re-opened the snorkel.db in a different notebook)\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='purpose_test', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21687\n",
      "0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 15s, sys: 1.04 s, total: 2min 16s\n",
      "Wall time: 2min 19s\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 0.0689\n",
      "Neg. class recall: 0.946\n",
      "Precision            0.241\n",
      "Recall               0.0689\n",
      "F1                   0.107\n",
      "----------------------------------------\n",
      "TP: 298 | FP: 936 | TN: 16334 | FN: 4025\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_comparative_degree_morethan</th>\n",
       "      <td>0</td>\n",
       "      <td>0.007424</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>12</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.574521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_comparative_degree_fooerthan</th>\n",
       "      <td>1</td>\n",
       "      <td>0.006824</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.568798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_inorderto</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379032</td>\n",
       "      <td>0.571532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_implication</th>\n",
       "      <td>3</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.561084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_solve</th>\n",
       "      <td>4</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.561006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_hypothesis</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.562148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_toenable</th>\n",
       "      <td>6</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.564780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_toaid</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.568570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_toproduce</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.555389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_toinvestigate</th>\n",
       "      <td>9</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.561603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_togive</th>\n",
       "      <td>10</td>\n",
       "      <td>0.011897</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>71</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.553632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_thatcan</th>\n",
       "      <td>11</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>25</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189394</td>\n",
       "      <td>0.574348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_examine</th>\n",
       "      <td>12</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.567223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_extend</th>\n",
       "      <td>13</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>19</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.571324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb_offer</th>\n",
       "      <td>14</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>17</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.574416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>15</td>\n",
       "      <td>0.011943</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>237</td>\n",
       "      <td>0.922179</td>\n",
       "      <td>0.558420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  j  Coverage  Overlaps  Conflicts  TP   FP  \\\n",
       "LF_comparative_degree_morethan    0  0.007424  0.000876   0.000092  12  144   \n",
       "LF_comparative_degree_fooerthan   1  0.006824  0.000830   0.000000  14  132   \n",
       "LF_purpose_verb_inorderto         2  0.005718  0.000553   0.000046  47   77   \n",
       "LF_purpose_verb_implication       3  0.002859  0.000046   0.000000   3   57   \n",
       "LF_purpose_verb_solve             4  0.002721  0.000138   0.000000  20   39   \n",
       "LF_purpose_verb_hypothesis        5  0.000968  0.000138   0.000046   7   14   \n",
       "LF_purpose_verb_toenable          6  0.001798  0.000323   0.000000  22   17   \n",
       "LF_purpose_verb_toaid             7  0.000184  0.000000   0.000000   2    2   \n",
       "LF_purpose_verb_toproduce         8  0.000784  0.000092   0.000000   5   12   \n",
       "LF_purpose_verb_toinvestigate     9  0.001153  0.000092   0.000000  15   10   \n",
       "LF_purpose_verb_togive           10  0.011897  0.000738   0.000046  71  184   \n",
       "LF_purpose_verb_thatcan          11  0.006179  0.000323   0.000138  25  107   \n",
       "LF_purpose_verb_examine          12  0.002951  0.000184   0.000000  31   33   \n",
       "LF_purpose_verb_extend           13  0.004611  0.000507   0.000231  19   80   \n",
       "LF_purpose_verb_offer            14  0.004427  0.000369   0.000092  17   79   \n",
       "<lambda>                         15  0.011943  0.000692   0.000692   0    0   \n",
       "\n",
       "                                 FN   TN  Empirical Acc.  Learned Acc.  \n",
       "LF_comparative_degree_morethan    0    0        0.076923      0.574521  \n",
       "LF_comparative_degree_fooerthan   0    0        0.095890      0.568798  \n",
       "LF_purpose_verb_inorderto         0    0        0.379032      0.571532  \n",
       "LF_purpose_verb_implication       0    0        0.050000      0.561084  \n",
       "LF_purpose_verb_solve             0    0        0.338983      0.561006  \n",
       "LF_purpose_verb_hypothesis        0    0        0.333333      0.562148  \n",
       "LF_purpose_verb_toenable          0    0        0.564103      0.564780  \n",
       "LF_purpose_verb_toaid             0    0        0.500000      0.568570  \n",
       "LF_purpose_verb_toproduce         0    0        0.294118      0.555389  \n",
       "LF_purpose_verb_toinvestigate     0    0        0.600000      0.561603  \n",
       "LF_purpose_verb_togive            0    0        0.278431      0.553632  \n",
       "LF_purpose_verb_thatcan           0    0        0.189394      0.574348  \n",
       "LF_purpose_verb_examine           0    0        0.484375      0.567223  \n",
       "LF_purpose_verb_extend            0    0        0.191919      0.571324  \n",
       "LF_purpose_verb_offer             0    0        0.177083      0.574416  \n",
       "<lambda>                         20  237        0.922179      0.558420  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "%time L_test = labeler.apply_existing(split=2)\n",
    "_ = gen_model.error_analysis(session, L_test, L_gold_test,set_unlabeled_as_neg=False)\n",
    "\n",
    "L_test.lf_stats(session, L_gold_test, gen_model.learned_lf_stats()['Accuracy'])  # learned_lf_stats(): For each labeling function, estimates of a few metrics are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segments that all belong to one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**717/30866 Candidate/Span:**\t`Segment(Span(\"b'Proactive moderation tools'\", sentence=7615, chars=[0,25], words=[0,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tProactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**718/30866 Candidate/Span:**\t`Segment(Span(\"b'while reactive bans were able to discourage a wider variety of behaviors .'\", sentence=7615, chars=[150,223], words=[24,36]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tProactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**719/30866 Candidate/Span:**\t`Segment(Span(\"b'such as chat modes which restricted the ability to post certain content'\", sentence=7615, chars=[28,98], words=[4,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tProactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**720/30866 Candidate/Span:**\t`Segment(Span(\"b'proved effective at discouraging spam behaviors'\", sentence=7615, chars=[101,147], words=[17,22]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tProactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2278/30866 Candidate/Span:**\t`Segment(Span(\"b'and types of behaviors'\", sentence=7616, chars=[58,79], words=[10,13]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThis work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2279/30866 Candidate/Span:**\t`Segment(Span(\"b'This work considers the intersection of tools'\", sentence=7616, chars=[0,44], words=[0,6]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThis work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2280/30866 Candidate/Span:**\t`Segment(Span(\"b'offering a new frame through which to consider the development of moderation strategies .\\n'\", sentence=7616, chars=[82,171], words=[15,29]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThis work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2281/30866 Candidate/Span:**\t`Segment(Span(\"b'authority'\", sentence=7616, chars=[47,55], words=[8,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThis work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15728/30866 Candidate/Span:**\t`Segment(Span(\"b'cruel'\", sentence=7611, chars=[56,60], words=[9,9]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOnline communities have the potential to be supportive, cruel, or anywhere in between ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15729/30866 Candidate/Span:**\t`Segment(Span(\"b'Online communities have the potential to be supportive'\", sentence=7611, chars=[0,53], words=[0,7]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOnline communities have the potential to be supportive, cruel, or anywhere in between ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15730/30866 Candidate/Span:**\t`Segment(Span(\"b'or anywhere in between .'\", sentence=7611, chars=[63,86], words=[11,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOnline communities have the potential to be supportive, cruel, or anywhere in between ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15734/30866 Candidate/Span:**\t`Segment(Span(\"b'The development of positive norms for interaction can help users build bonds'\", sentence=7612, chars=[0,75], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe development of positive norms for interaction can help users build bonds, grow, and learn ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15735/30866 Candidate/Span:**\t`Segment(Span(\"b'grow'\", sentence=7612, chars=[78,81], words=[13,13]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe development of positive norms for interaction can help users build bonds, grow, and learn ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15736/30866 Candidate/Span:**\t`Segment(Span(\"b'and learn .'\", sentence=7612, chars=[84,94], words=[15,17]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe development of positive norms for interaction can help users build bonds, grow, and learn ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**26527/30866 Candidate/Span:**\t`Segment(Span(\"b'including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors .'\", sentence=7613, chars=[146,290], words=[22,40]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUsing millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**26528/30866 Candidate/Span:**\t`Segment(Span(\"b'we explore the effectiveness of methods for encouraging and discouraging specific behaviors'\", sentence=7613, chars=[53,143], words=[9,20]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUsing millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**26529/30866 Candidate/Span:**\t`Segment(Span(\"b'Using millions of messages sent in Twitch chatrooms'\", sentence=7613, chars=[0,50], words=[0,7]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUsing millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**29062/30866 Candidate/Span:**\t`Segment(Span(\"b'Consistent with aspects of imitation theory and deterrence theory'\", sentence=7614, chars=[0,64], words=[0,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tConsistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**29063/30866 Candidate/Span:**\t`Segment(Span(\"b'and more so for behaviors from high status users .'\", sentence=7614, chars=[118,167], words=[19,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tConsistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**29064/30866 Candidate/Span:**\t`Segment(Span(\"b'users imitated examples of behavior that they saw'\", sentence=7614, chars=[67,115], words=[10,17]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tConsistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_clauses_single_doc 20\n",
      "These clauses' parent Document's is  Document 2K_dev_0\n"
     ]
    }
   ],
   "source": [
    "num_clauses_single_doc=0\n",
    "for i in range(len(dev_segments)): # to print at most 4 cands \n",
    "    if dev_segments[i].get_parent().get_parent().name==\"2K_dev_0\":\n",
    "        printmd(\"**\"+str(i)+\"/\"+str(len(dev_segments))+\" Candidate/Span:**\\t`\"+str(dev_segments[i])+\"`\")\n",
    "        printmd(\"**Its parent Sentence's text:**\\t\"+str(dev_segments[i].get_parent().text))\n",
    "#         printmd(\"**Its parent Document's text:**\\t\"+str(dev_segments[i].get_parent().get_parent().__dict__))\n",
    "        print() \n",
    "        num_clauses_single_doc+=1\n",
    "        single_doc=dev_segments[i].get_parent().get_parent()\n",
    "print(\"num_clauses_single_doc\", num_clauses_single_doc)\n",
    "print(\"These clauses' parent Document's is \",single_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Loading Gold Labels (Encountered some issue and slightly deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'snorkel.annotations.csr_LabelMatrix'>\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "from snorkel.annotations import load_gold_labels\n",
    "# if load the 1st time\n",
    "# purposes = load_external_labels(session, Segment, annotator_name='background')\n",
    "\n",
    "# else:\n",
    "gold_dev_purposes_mixed_w_backgrounds=load_gold_labels(session, annotator_name='background', split=1)\n",
    "print(type(gold_dev_purposes_mixed_w_backgrounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 29s, sys: 1.75 s, total: 2min 30s\n",
      "Wall time: 2min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<31595x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6942 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "purpose_LFs=[LF_expressing_contrast,LF_excluded_pseudo_contrast,LF_comparative_degree,LF_purpose_verb,LF_purpose_delimiter,LF_purpose_adj_problem,LF_purpose_leading_question_word,negate(LF_mechanism_verb),negate(LF_mechanism_adv),negate(LF_mechanism_noun),negate(LF_mechanism_adj)]\n",
    "\n",
    "labeler = LabelAnnotator(lfs=purpose_LFs)\n",
    "\n",
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a peek at one of the labelled candidates and which LF does it come from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random candidate Segment(Span(\"b'Kinsey (1984) defined noncompliance with tax laws as the \\xe2\\x80\\x9cfailure'\", sentence=13067, chars=[0,64], words=[0,12]))\n",
      "Its LabelKey LabelKey (LF_purpose_verb)\n"
     ]
    }
   ],
   "source": [
    "print(\"A random candidate\",L_train.get_candidate(session, 3))\n",
    "print(\"Its LabelKey\",L_train.get_key(session, 3))\n",
    "# print(\"Annotations summary\",L_train.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view statistics about the resulting label matrix.\n",
    "\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a *conflicting* non-zero label for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_expressing_contrast</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_excluded_pseudo_contrast</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_comparative_degree</th>\n",
       "      <td>2</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb</th>\n",
       "      <td>3</td>\n",
       "      <td>0.034879</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.006552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_delimiter</th>\n",
       "      <td>4</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.003482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_adj_problem</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_leading_question_word</th>\n",
       "      <td>6</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.001551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>7</td>\n",
       "      <td>0.136192</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.012597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  j  Coverage  Overlaps  Conflicts\n",
       "LF_expressing_contrast            0  0.011173  0.000475   0.000317\n",
       "LF_excluded_pseudo_contrast       1  0.001013  0.000158   0.000095\n",
       "LF_comparative_degree             2  0.007501  0.001139   0.000855\n",
       "LF_purpose_verb                   3  0.034879  0.008134   0.006552\n",
       "LF_purpose_delimiter              4  0.017060  0.003925   0.003482\n",
       "LF_purpose_adj_problem            5  0.003482  0.000728   0.000506\n",
       "LF_purpose_leading_question_word  6  0.008419  0.002595   0.001551\n",
       "<lambda>                          7  0.136192  0.012660   0.012597"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we examine the metrics for Mechanism LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 46s, sys: 2.69 s, total: 2min 49s\n",
      "Wall time: 2min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<31595x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8523 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "mechanism_LFs=[LF_mechanism_verb, LF_mechanism_adv,LF_mechanism_noun, LF_mechanism_adj, negate(LF_expressing_contrast),negate(LF_excluded_pseudo_contrast),negate(LF_comparative_degree),negate(LF_purpose_verb),negate(LF_purpose_delimiter),negate(LF_purpose_adj_problem),negate(LF_purpose_leading_question_word)]\n",
    "labeler = LabelAnnotator(lfs=mechanism_LFs)\n",
    "\n",
    "np.random.seed(1702)\n",
    "%time L_train_mechanism = labeler.apply(split=0)\n",
    "L_train_mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_verb</th>\n",
       "      <td>0</td>\n",
       "      <td>0.136192</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_adv</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008831</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_noun</th>\n",
       "      <td>2</td>\n",
       "      <td>0.083209</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_adj</th>\n",
       "      <td>3</td>\n",
       "      <td>0.030353</td>\n",
       "      <td>0.011774</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>4</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   j  Coverage  Overlaps  Conflicts\n",
       "LF_mechanism_verb  0  0.136192  0.031746   0.000317\n",
       "LF_mechanism_adv   1  0.008831  0.001677   0.000032\n",
       "LF_mechanism_noun  2  0.083209  0.030511   0.000285\n",
       "LF_mechanism_adj   3  0.030353  0.011774   0.000095\n",
       "<lambda>           4  0.011173  0.000696   0.000696"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train_mechanism.lf_stats(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting the Generative Model\n",
    "\n",
    "The training step for 70K papers take approximately 10 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()  # Inferred cardinality: 2 would be binary label (excluding the uncertain class)\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we quantify a few characteristics, (1) LF accuracies; (2) the marginal probability distributions of each candidate being True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07856624, 0.07481565, 0.07709562, 0.08353626, 0.07912881,\n",
       "       0.07455556, 0.07590932, 0.12232697])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_marginals = gen_model.marginals(L_train)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.537883</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>0.542201</td>\n",
       "      <td>0.362007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.541904</td>\n",
       "      <td>0.6658</td>\n",
       "      <td>0.544689</td>\n",
       "      <td>0.360414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.538233</td>\n",
       "      <td>0.6722</td>\n",
       "      <td>0.537371</td>\n",
       "      <td>0.362206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.534887</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.533137</td>\n",
       "      <td>0.360414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529306</td>\n",
       "      <td>0.6688</td>\n",
       "      <td>0.531137</td>\n",
       "      <td>0.349861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.539391</td>\n",
       "      <td>0.6702</td>\n",
       "      <td>0.537954</td>\n",
       "      <td>0.357029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.534796</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>0.542962</td>\n",
       "      <td>0.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.565314</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>0.567007</td>\n",
       "      <td>0.386699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Coverage  Precision    Recall\n",
       "0  0.537883    0.6652   0.542201  0.362007\n",
       "1  0.541904    0.6658   0.544689  0.360414\n",
       "2  0.538233    0.6722   0.537371  0.362206\n",
       "3  0.534887    0.6736   0.533137  0.360414\n",
       "4  0.529306    0.6688   0.531137  0.349861\n",
       "5  0.539391    0.6702   0.537954  0.357029\n",
       "6  0.534796    0.6610   0.542962  0.354839\n",
       "7  0.565314    0.6752   0.567007  0.386699"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.learned_lf_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 0\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 0.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test one purpose LF over groundtruth ISSUE: gold_labels.tsv are context_stable_ids. How could we get that for our task \n",
    "\n",
    "from snorkel.lf_helpers import test_LF\n",
    "tp, fp, tn, fn = test_LF(session, LF_expressing_contrast, split=1, annotator_name='background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands=session.query(Segment).filter(Segment.split == 0).all()\n",
    "\n",
    "for ind,c in enumerate(cands): # session.query(Segment).filter(Segment.split == 0).all():\n",
    "    if ind%500==0:\n",
    "        print(\"Processed \", ind, len(cands))\n",
    "        \n",
    "    for lf in LFS:\n",
    "        if lf(c)!=0:\n",
    "            labeled.append(c)\n",
    "            continue\n",
    "        \n",
    "print(\"Number labeled:\", len(labeled),len(cands))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "labeled[4].get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "\n",
    "# Reference: \n",
    "# def load_external_labels(session, candidate_class, annotator_name='gold'):\n",
    "#  session.add(StableLabel(\n",
    "#                 context_stable_ids=context_stable_ids,\n",
    "#                 annotator_name=annotator_name, e.g. \"gold\"\n",
    "#                 value=row['label']))\n",
    "            \n",
    "            \n",
    "GoldBackground = candidate_subclass('GoldBackground', ['goldbackground_cue'])\n",
    "\n",
    "all_gold_background_extractor=CandidateExtractor(GoldBackground, [ngrams], None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to differentiate ``Background`` with ``Purpose``. Recall our ``Background`` definition from Google Doc:\n",
    "\n",
    "``Contains words that indicate prior work (e.g., “traditionally”, “researchers have…”), and then following sentence/span starts with some variant of “In this paper, we introduce…” (Exploring adjacency relationship through helper function in Snorkel)``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove if necessary \n",
    "# session.query(Background).all()\n",
    "import snorkel.models.candidate as candidate\n",
    "\n",
    "def del_defined_candidate_class(class_name):\n",
    "    print(\"Existing\", candidate.candidate_subclasses)\n",
    "    del(candidate.candidate_subclasses[class_name])\n",
    "    print(\"After deletion\", candidate.candidate_subclasses)\n",
    "    \n",
    "## Usage:    \n",
    "# del_defined_candidate_class('Purpose')\n",
    "\n",
    "non_comma_matcher=DictionaryMatch(d=[','],longest_match_only=True,reverse=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells describe our ``Span Matching`` phase: we respectively define the `Span` for the 5 segments are , and centrally stored in a document-id-indexed hashmap. This helps us to group the 5 segments back into documents.\n",
    "3. Only those documents that have at least 3 segments are considered as valid\n",
    "\n",
    "Following this ``Span Matching`` phase, we will be having an ``Document Aggregation`` phase, where adjacency relationship and (or) waterfall models are being emphasized through multiple criteria. For example, the following criteria prevents non-abstracts (e.g. proceeding cover letter, tutorial introduction, etc.) or less well-strctured abstracts from being included. \n",
    "\n",
    "1. Only those documents that have all 5 segments are considered as valid\n",
    "2. Only those documents that have at least 4 segments are cosidered as valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 28s, sys: 472 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 148**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 9.88 s, sys: 220 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 15**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 460 ms, sys: 22.7 ms, total: 483 ms\n",
      "Wall time: 474 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 0**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/148 Candidate/Span:**\t`Background(Span(\"b'The intersection of these domains should enable researchers to foster an improved understanding of student learning'\", sentence=12057, chars=[0,114], words=[0,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe intersection of these domains should enable researchers to foster an improved understanding of student learning, lead to the creation of more natural and enriching learning interfaces, and motivate the development of novel techniques for tackling challenges that are specific of education.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d5e8d0>, 'name': '5834868625ff05a97b01354e', 'stable_id': '5834868625ff05a97b01354e::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1886, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b01354e,0,b'This summary describes the 1st International Workshop on Multimodal Learning Analytics.'), Sentence(Document 5834868625ff05a97b01354e,1,b'This area of study brings together the technologies of multimodal analysis with the learning sciences.'), Sentence(Document 5834868625ff05a97b01354e,2,b'The intersection of these domains should enable researchers to foster an improved understanding of student learning, lead to the creation of more natural and enriching learning interfaces, and motivate the development of novel techniques for tackling challenges that are specific of education.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/148 Candidate/Span:**\t`Background(Span(\"b'researchers appear to be intrigued with the question; Given a set of points'\", sentence=5803, chars=[9,83], words=[3,16]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOf late, researchers appear to be intrigued with the question; Given a set of points, what is the region occupied by them?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1113a94e0>, 'name': '5834868425ff05a97b00c32e', 'stable_id': '5834868425ff05a97b00c32e::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 72, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00c32e,0,b'Of late, researchers appear to be intrigued with the question; Given a set of points, what is the region occupied by them?'), Sentence(Document 5834868425ff05a97b00c32e,1,b'The answer appears to be neither straight forward nor unique.'), Sentence(Document 5834868425ff05a97b00c32e,2,b'Convex hull, which gives a convex enclosure of the given set, concave hull, which generates non-convex polygons and other variants such as \\xce\\xb1-hull, poly hull, r-shape and s-shape etc. have been proposed.'), Sentence(Document 5834868425ff05a97b00c32e,3,b'In this paper, we extend the question of finding a minimum area enclosure (MAE) to a set of closed planar freeform curves, not resorting to sampling them.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/148 Candidate/Span:**\t`Background(Span(\"b'The feature-interaction problem has been keeping researchers and practitioners in suspense for years.'\", sentence=13452, chars=[0,100], words=[0,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe feature-interaction problem has been keeping researchers and practitioners in suspense for years."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d8dba8>, 'name': '5834868625ff05a97b012212', 'stable_id': '5834868625ff05a97b012212::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 2305, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b012212,0,b'The feature-interaction problem has been keeping researchers and practitioners in suspense for years.'), Sentence(Document 5834868625ff05a97b012212,1,b'Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems.'), Sentence(Document 5834868625ff05a97b012212,2,b'In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/148 Candidate/Span:**\t`Background(Span(\"b'previous work in this domain'\", sentence=13892, chars=[7,34], words=[1,5]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUnlike previous work in this domain, which focused on taking one reference sample and doing user authentication based on the reference sample only, we continuously sample user input and use the data for identi cation and further learning and re nement of the user model.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d9b550>, 'name': '5834868725ff05a97b0141e7', 'stable_id': '5834868725ff05a97b0141e7::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 2438, 'type': 'document', 'sentences': [Sentence(Document 5834868725ff05a97b0141e7,0,b'We analyze keystroke latency patterns to identify the person typing on the keyboard.'), Sentence(Document 5834868725ff05a97b0141e7,1,b'Unlike previous work in this domain, which focused on taking one reference sample and doing user authentication based on the reference sample only, we continuously sample user input and use the data for identi cation and further learning and re nement of the user model.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Background: \n",
    "Background = candidate_subclass('Background', ['background_cue'])\n",
    "transition_word=DictionaryMatch(d=['while','unlike','despite'],longest_match_only=True) \n",
    "transition_prev_work=DictionaryMatch(d=['previous','earlier','past'],longest_match_only=True) \n",
    "dict_background_matcher=DictionaryMatch(d=['previous work','traditionally','researchers'],longest_match_only=True) \n",
    "excluded_dict_background_matcher=DictionaryMatch(d=['we','unlike','our'],longest_match_only=True,reverse=True) \n",
    "non_comma_dict_background_matcher=CandidateExtractor(Background, [ngrams], [Intersection(non_comma_matcher,Union(dict_background_matcher,Intersection(transition_word,transition_prev_work)),excluded_dict_background_matcher)])\n",
    "background_cands=extract_and_display(non_comma_dict_background_matcher,Background,\"Background\",document_breakdown_map)\n",
    "# print(document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'snorkel.models.candidate.Purpose'>\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 35s, sys: 1.38 s, total: 1min 37s\n",
      "Wall time: 1min 38s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1501**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 11.8 s, sys: 468 ms, total: 12.3 s\n",
      "Wall time: 12.2 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 203**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 761 ms, sys: 117 ms, total: 878 ms\n",
      "Wall time: 878 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1501 Candidate/Span:**\t`Purpose(Span(\"b'In this paper'\", sentence=21545, chars=[0,12], words=[0,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d85b278>, 'name': '58cb6b8ec50f90cdd3875b14', 'stable_id': '58cb6b8ec50f90cdd3875b14::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4540, 'type': 'document', 'sentences': [Sentence(Document 58cb6b8ec50f90cdd3875b14,0,b'Visual inspection is, by far, the most widely used method in aircraft surface inspection.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,1,b'We are currently developing a prototype remote visual inspection system, designed to facilitate testing the hypothesized feasibility and advantages of remote visual inspection of aircraft surfaces.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,2,b'In this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1501 Candidate/Span:**\t`Purpose(Span(\"b'\\xe2\\x80\\x94here we examine shrinkage toward diagonality.\\n'\", sentence=11574, chars=[0,46], words=[0,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\t—here we examine shrinkage toward diagonality.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122c40f0>, 'name': '5834868625ff05a97b0122ab', 'stable_id': '5834868625ff05a97b0122ab::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1681, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b0122ab,0,b'The problem of estimating a covariance matrix in small samples has been considered by several authors following early work by Stein.'), Sentence(Document 5834868625ff05a97b0122ab,1,b'This problem can be especially important in hierarchical models where the standard errors of fixed and random effects depend on estimation of the covariance matrix of the distribution of the random effects.'), Sentence(Document 5834868625ff05a97b0122ab,2,b'We propose a set of hierarchical priors (HPs) for the covariance matrix that produce posterior shrinkage toward a specified structure'), Sentence(Document 5834868625ff05a97b0122ab,3,b'\\xe2\\x80\\x94here we examine shrinkage toward diagonality.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1501 Candidate/Span:**\t`Purpose(Span(\"b'which extend organisational boundaries.'\", sentence=6712, chars=[119,157], words=[19,23]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe quest for scalability, reliability and cost reduction has led to the development of massively distributed systems, which extend organisational boundaries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x111488240>, 'name': '5834868425ff05a97b00e0fe', 'stable_id': '5834868425ff05a97b00e0fe::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 315, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00e0fe,0,b'Modern computing paradigms have frequently adopted concepts from distributed systems.'), Sentence(Document 5834868425ff05a97b00e0fe,1,b'The quest for scalability, reliability and cost reduction has led to the development of massively distributed systems, which extend organisational boundaries.'), Sentence(Document 5834868425ff05a97b00e0fe,2,b'Voluntary computing environments (such as BOINC), Grids (such as EGEE and Globus), and more recently Cloud Computing (both open source and commercial) have established themselves as a range of distributed systems.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1501 Candidate/Span:**\t`Purpose(Span(\"b'however'\", sentence=12099, chars=[5,11], words=[2,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tFew, however, allow new interfaces to be created from scratch because they do not provide a means of demonstrating when a recorded macro should be invoked."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122fb828>, 'name': '5834868725ff05a97b013a8a', 'stable_id': '5834868725ff05a97b013a8a::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1836, 'type': 'document', 'sentences': [Sentence(Document 5834868725ff05a97b013a8a,0,b'Many programming by demonstration (PBD) systems elaborate on the idea of macro recording, and they allow users to extend existing applications.'), Sentence(Document 5834868725ff05a97b013a8a,1,b'Few, however, allow new interfaces to be created from scratch because they do not provide a means of demonstrating when a recorded macro should be invoked.'), Sentence(Document 5834868725ff05a97b013a8a,2,b'This paper discusses stimulus-response systems that allow both the when (stimulus/event) and the what (response macro) to be demonstrated.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21999fd0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26962, 'split': 2, 'id': 1705}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990080>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26963, 'split': 2, 'id': 1706}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219900f0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26964, 'split': 2, 'id': 1707}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990160>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26965, 'split': 2, 'id': 1708}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219901d0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26966, 'split': 2, 'id': 1709}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990240>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26967, 'split': 2, 'id': 1710}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219902b0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26968, 'split': 2, 'id': 1711}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990320>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26969, 'split': 2, 'id': 1712}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990390>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26970, 'split': 2, 'id': 1713}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990400>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26971, 'split': 2, 'id': 1714}\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Purpose:\n",
    "Purpose=candidate_subclass('Purpose',['purpose_cue'])\n",
    "\n",
    "transition_regex_matcher=RegexMatchSpan(rgx=\"((^|\\s)however.*$)|((^|\\s)but(?!(also))*$)\",longest_match_only=True)  # Correction: purpose \n",
    "excluded_dict_purpose_matcher=SentenceMatch(d=['but also','but without','but sometimes'],longest_match_only=True,reverse=True)  # the parent sentence shall not include \"but also\"\n",
    "transition_matcher=Intersection(transition_regex_matcher,excluded_dict_purpose_matcher)\n",
    "\n",
    "comparative_degree_matcher=Intersection(RegexMatchSpan(rgx=\"(.*more.*than.*$)|(.*er than.*$)\",longest_match_only=True),transition_prev_work)  # Correction: purpose \n",
    "other_regex_matcher=RegexMatchSpan(rgx=\"(.*extend.*$)|(.*offer.*$)\",longest_match_only=True)\n",
    "\n",
    "dict_purpose_matcher=DictionaryMatch(d=['in this paper','in the paper',' that can ','in this study','to examine','we examine','to investigate','implications'],longest_match_only=True) \n",
    "\n",
    "# Unit test\n",
    "# non_comma_dict_purpose_matcher=CandidateExtractor(Purpose, [ngrams], [Intersection(non_comma_matcher,other_regex_matcher)])\n",
    "\n",
    "non_comma_dict_purpose_matcher=CandidateExtractor(Purpose, [ngrams], [Intersection(non_comma_matcher,Union(comparative_degree_matcher,other_regex_matcher,dict_purpose_matcher,transition_matcher))]) #,intersection(excluded_dict_purpose_matcher,transition_regex_matcher)])\n",
    "purpose_cands=extract_and_display(non_comma_dict_purpose_matcher,Purpose,\"Purpose\",document_breakdown_map)\n",
    "# print(document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 54.1 s, sys: 852 ms, total: 54.9 s\n",
      "Wall time: 55.7 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1404**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 7.21 s, sys: 422 ms, total: 7.63 s\n",
      "Wall time: 7.52 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 169**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 475 ms, sys: 103 ms, total: 578 ms\n",
      "Wall time: 609 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1404 Candidate/Span:**\t`Mechanism(Span(\"b'We introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses.'\", sentence=11021, chars=[0,131], words=[0,22]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tWe introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10fe465c0>, 'name': '5834868625ff05a97b011292', 'stable_id': '5834868625ff05a97b011292::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1521, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b011292,0,b'We introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses.'), Sentence(Document 5834868625ff05a97b011292,1,b'Our approach is completely unsupervised, and our experiments show that Neural Network models perform much better (about 20% more accurate) than traditional feature-rich baseline models.'), Sentence(Document 5834868625ff05a97b011292,2,b'We also present a new dataset for Biomedical Language Processing which, with only about 25% of the original corpus vocabulary, still captures the essential distributional semantics of the corpus.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1404 Candidate/Span:**\t`Mechanism(Span(\"b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n'\", sentence=18541, chars=[0,198], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tAn algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2a2d30>, 'name': '5834868825ff05a97b01794c', 'stable_id': '5834868825ff05a97b01794c::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3693, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01794c,0,b'In this paper we describe an approach towards integrating mid-range sensing data into a dynamic path planning algorithm.'), Sentence(Document 5834868825ff05a97b01794c,1,b'The key problem, sensing for planning is addressed in the context of outdoor navigation.'), Sentence(Document 5834868825ff05a97b01794c,2,b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1404 Candidate/Span:**\t`Mechanism(Span(\"b'Our approach appears more practical than previous metamodule-based approaches.'\", sentence=17754, chars=[0,77], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOur approach appears more practical than previous metamodule-based approaches."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10fc3b9e8>, 'name': '5834868825ff05a97b0175d2', 'stable_id': '5834868825ff05a97b0175d2::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3472, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b0175d2,0,b'Abstract\\xe2\\x80\\x93'), Sentence(Document 5834868825ff05a97b0175d2,1,b'We describe a new set of prismatic movement primitives for cubic modular robots.'), Sentence(Document 5834868825ff05a97b0175d2,2,b'Our approach appears more practical than previous metamodule-based approaches.'), Sentence(Document 5834868825ff05a97b0175d2,3,b\"We also describe recent hardware developments in our cu-bic robot modules that have sufficient stiffness and actuator strength so that when they work together they can realize, in earth's gravity, all of the motion primitives we describe here.\\n\")]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1404 Candidate/Span:**\t`Mechanism(Span(\"b'In contrast to existing approaches that rely only on computer vision'\", sentence=19748, chars=[0,67], words=[0,10]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b341208>, 'name': '5834868825ff05a97b01791b', 'stable_id': '5834868825ff05a97b01791b::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4036, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01791b,0,b'In robotics research, perception is one of the most challenging tasks.'), Sentence(Document 5834868825ff05a97b01791b,1,b'In contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates.'), Sentence(Document 5834868825ff05a97b01791b,2,b'To evaluate, we apply this idea to a door detection problem.'), Sentence(Document 5834868825ff05a97b01791b,3,b'A set of preliminary experiments has been completed using software agents with real vision data.'), Sentence(Document 5834868825ff05a97b01791b,4,b'Our results demonstrate that information inferred from teammate observations significantly improves the perception precision.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7be0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3157, 'split': 2, 'mechanism_cue_id': 26962}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7ef0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3158, 'split': 2, 'mechanism_cue_id': 28446}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7eb8>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3159, 'split': 2, 'mechanism_cue_id': 28447}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d0908>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3160, 'split': 2, 'mechanism_cue_id': 28448}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d0ac8>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3161, 'split': 2, 'mechanism_cue_id': 28449}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d07f0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3162, 'split': 2, 'mechanism_cue_id': 28450}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f8080>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3163, 'split': 2, 'mechanism_cue_id': 28452}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f80f0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3164, 'split': 2, 'mechanism_cue_id': 28453}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f8160>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3165, 'split': 2, 'mechanism_cue_id': 28454}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f81d0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3166, 'split': 2, 'mechanism_cue_id': 28451}\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Mechanism: \n",
    "Mechanism = candidate_subclass('Mechanism', ['mechanism_cue']) \n",
    "dict_mechanism_matcher=DictionaryMatch(d=['introduce','introduces','propose','proposes','we propose','we develop','approach'],longest_match_only=True) \n",
    "non_comma_dict_mechanism_matcher=CandidateExtractor(Mechanism, [ngrams], [Intersection(non_comma_matcher,dict_mechanism_matcher)])\n",
    "mechanism_cands=extract_and_display(non_comma_dict_mechanism_matcher,Mechanism,\"Mechanism\",document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 53 s, sys: 864 ms, total: 53.9 s\n",
      "Wall time: 54.4 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 752**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6.82 s, sys: 364 ms, total: 7.18 s\n",
      "Wall time: 6.96 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 81**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 428 ms, sys: 71.6 ms, total: 499 ms\n",
      "Wall time: 464 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/752 Candidate/Span:**\t`Method(Span(\"b'The speech-to-speech translation is bi-directional for a two way dialog between participants.\\n'\", sentence=10296, chars=[0,93], words=[0,19]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe speech-to-speech translation is bi-directional for a two way dialog between participants.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10ff0a908>, 'name': '5834868425ff05a97b00e5b3', 'stable_id': '5834868425ff05a97b00e5b3::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1310, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00e5b3,0,b'Jibbigo is a speech-to-speech translation application for iPhone, iPod touch, and iPad devices.'), Sentence(Document 5834868425ff05a97b00e5b3,1,b'Jibbigo allows the user to simply speak a sentence, and it speaks the sentence aloud in the other language, much like a personal human interpreter would.'), Sentence(Document 5834868425ff05a97b00e5b3,2,b'The speech-to-speech translation is bi-directional for a two way dialog between participants.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/752 Candidate/Span:**\t`Method(Span(\"b'Both computational feasibility and improvement of estimation is demonstrated in the experiments.\\n'\", sentence=6557, chars=[0,96], words=[0,13]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tBoth computational feasibility and improvement of estimation is demonstrated in the experiments.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10f01d470>, 'name': '5834868425ff05a97b00de91', 'stable_id': '5834868425ff05a97b00de91::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 271, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00de91,0,b'The concept of Support Vector Regression is extended to a more general class of convex cost functions.'), Sentence(Document 5834868425ff05a97b00de91,1,b'Moreover it is shown how the resulting convex constrained optimization problems can be efficiently solved by a Primal-Dual Interior Point path following method.'), Sentence(Document 5834868425ff05a97b00de91,2,b'Both computational feasibility and improvement of estimation is demonstrated in the experiments.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/752 Candidate/Span:**\t`Method(Span(\"b'Both experiments seemto reveal that the Metaphor has poor effectiveness.\\n'\", sentence=18726, chars=[0,72], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tBoth experiments seemto reveal that the Metaphor has poor effectiveness.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2b7e80>, 'name': '5834868825ff05a97b017851', 'stable_id': '5834868825ff05a97b017851::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3743, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b017851,0,b'The Metaphor is intended to contribute to the Agile Programming value of communication.'), Sentence(Document 5834868825ff05a97b017851,1,b'Previously, some of the author studied the Metaphor as a means of communication among team members and between them and clients.'), Sentence(Document 5834868825ff05a97b017851,2,b\"This paper examines the Metaphor's contribution to the software architecture.\"), Sentence(Document 5834868825ff05a97b017851,3,b'Both experiments seemto reveal that the Metaphor has poor effectiveness.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/752 Candidate/Span:**\t`Method(Span(\"b'The commonality amongst the experiments has permitted the ability to objectively...\\n'\", sentence=21988, chars=[0,83], words=[0,12]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe commonality amongst the experiments has permitted the ability to objectively...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b453c88>, 'name': '59364828c50f90cdd3aca7f8', 'stable_id': '59364828c50f90cdd3aca7f8::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4676, 'type': 'document', 'sentences': [Sentence(Document 59364828c50f90cdd3aca7f8,0,b'This paper presents an experimental evaluation and comparison of basic strategies that have been proposed for force control of robot manipulators.'), Sentence(Document 59364828c50f90cdd3aca7f8,1,b'This experimental review of force control methodologies is unique in its breadth--never has such a complete spectrum of strategies been experimentally compared on the same system.'), Sentence(Document 59364828c50f90cdd3aca7f8,2,b'The commonality amongst the experiments has permitted the ability to objectively...\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40710>, 'type': 'method', 'method_cue_cid': None, 'id': 4000, 'split': 2, 'method_cue_id': 26962}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40b00>, 'type': 'method', 'method_cue_cid': None, 'id': 4001, 'split': 2, 'method_cue_id': 29212}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40ba8>, 'type': 'method', 'method_cue_cid': None, 'id': 4002, 'split': 2, 'method_cue_id': 29213}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40748>, 'type': 'method', 'method_cue_cid': None, 'id': 4003, 'split': 2, 'method_cue_id': 29214}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa404e0>, 'type': 'method', 'method_cue_cid': None, 'id': 4004, 'split': 2, 'method_cue_id': 29215}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49400>, 'type': 'method', 'method_cue_cid': None, 'id': 4005, 'split': 2, 'method_cue_id': 29216}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa495c0>, 'type': 'method', 'method_cue_cid': None, 'id': 4006, 'split': 2, 'method_cue_id': 29217}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49668>, 'type': 'method', 'method_cue_cid': None, 'id': 4007, 'split': 2, 'method_cue_id': 29218}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49630>, 'type': 'method', 'method_cue_cid': None, 'id': 4008, 'split': 2, 'method_cue_id': 29219}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa494a8>, 'type': 'method', 'method_cue_cid': None, 'id': 4009, 'split': 2, 'method_cue_id': 29220}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compound Matcher for Method: \n",
    "Method = candidate_subclass('Method', ['method_cue'])\n",
    "\n",
    "dict_method_matcher=DictionaryMatch(d=['dataset','benchmark','experiment ','experiments',\"empirical\",\"participant\",\"survey\",\" conduct\",\" analyze\"],longest_match_only=True) \n",
    "\n",
    "non_comma_dict_method_matcher=CandidateExtractor(Method, [ngrams], [Intersection(non_comma_matcher,dict_method_matcher)])\n",
    "method_cands=extract_and_display(non_comma_dict_method_matcher,Method,\"Method\",document_breakdown_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 56.2 s, sys: 803 ms, total: 57 s\n",
      "Wall time: 57.5 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1753**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6.98 s, sys: 347 ms, total: 7.32 s\n",
      "Wall time: 7.12 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 176**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 469 ms, sys: 94.4 ms, total: 564 ms\n",
      "Wall time: 529 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 12**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1753 Candidate/Span:**\t`Finding(Span(\"b'The effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented.'\", sentence=5983, chars=[0,173], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10ea76da0>, 'name': '5834868425ff05a97b00c935', 'stable_id': '5834868425ff05a97b00c935::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 117, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00c935,0,b'This paper proposes a fifteen level H-Bridge cascaded multilevel inverter with fundamental frequency switching for low power applications such as solar powered power supplies, battery powered standby power supplies.'), Sentence(Document 5834868425ff05a97b00c935,1,b'The effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented.'), Sentence(Document 5834868425ff05a97b00c935,2,b'Experimental...\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1753 Candidate/Span:**\t`Finding(Span(\"b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n'\", sentence=18541, chars=[0,198], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tAn algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2a2d30>, 'name': '5834868825ff05a97b01794c', 'stable_id': '5834868825ff05a97b01794c::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3693, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01794c,0,b'In this paper we describe an approach towards integrating mid-range sensing data into a dynamic path planning algorithm.'), Sentence(Document 5834868825ff05a97b01794c,1,b'The key problem, sensing for planning is addressed in the context of outdoor navigation.'), Sentence(Document 5834868825ff05a97b01794c,2,b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1753 Candidate/Span:**\t`Finding(Span(\"b'a people finder application'\", sentence=23141, chars=[123,149], words=[18,21]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tMore specifically, we have developed and evaluated three different applications, including a contextual instant messenger, a people finder application, and a phone-based application for access control."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b4e66a0>, 'name': '59382e72c50f90cdd3ada1ca', 'stable_id': '59382e72c50f90cdd3ada1ca::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5003, 'type': 'document', 'sentences': [Sentence(Document 59382e72c50f90cdd3ada1ca,0,b'We describe our current work in developing novel mechanisms for managing security and privacy in pervasive computing environments.'), Sentence(Document 59382e72c50f90cdd3ada1ca,1,b'More specifically, we have developed and evaluated three different applications, including a contextual instant messenger, a people finder application, and a phone-based application for access control.'), Sentence(Document 59382e72c50f90cdd3ada1ca,2,b'We also draw out some themes we have learned thus far for user-controllable security and privacy.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1753 Candidate/Span:**\t`Finding(Span(\"b'We show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\\n'\", sentence=24335, chars=[0,241], words=[0,45]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tWe show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b5852b0>, 'name': '595d391dc50f90cdd3bdeda2', 'stable_id': '595d391dc50f90cdd3bdeda2::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5357, 'type': 'document', 'sentences': [Sentence(Document 595d391dc50f90cdd3bdeda2,0,b'We empirically explore the application of (adaptations of) these algorithms to the kidney exchange problem, where patients with end-stage renal failure swap willing but incompatible donors.'), Sentence(Document 595d391dc50f90cdd3bdeda2,1,b'We show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff899e8>, 'type': 'finding', 'id': 5939, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30865}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff89a58>, 'type': 'finding', 'id': 5940, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30866}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82f28>, 'type': 'finding', 'id': 5941, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 29216}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82ef0>, 'type': 'finding', 'id': 5942, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30867}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff828d0>, 'type': 'finding', 'id': 5943, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30868}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82f60>, 'type': 'finding', 'id': 5944, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30869}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82128>, 'type': 'finding', 'id': 5945, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30870}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff825f8>, 'type': 'finding', 'id': 5946, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30872}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82438>, 'type': 'finding', 'id': 5947, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30871}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82160>, 'type': 'finding', 'id': 5948, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30873}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82be0>, 'type': 'finding', 'id': 5949, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30874}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82400>, 'type': 'finding', 'id': 5950, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30864}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compound Matcher for Finding: \n",
    "Finding = candidate_subclass('Finding', ['finding_cue'])\n",
    "\n",
    "dict_finding_matcher=DictionaryMatch(d=['show that','shows that','found','indicate','results','performance','find'],longest_match_only=True) \n",
    "\n",
    "non_comma_dict_finding_matcher=CandidateExtractor(Finding, [ngrams], [Intersection(non_comma_matcher,dict_finding_matcher)])\n",
    "finding_cands=extract_and_display(non_comma_dict_finding_matcher,Finding,\"Finding\",document_breakdown_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the ``Document Aggregation`` phase. First we show a few document examples, of how ``document_breakdown_map`` looks like, as well as how do we aggregate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Document 0/3039\n",
      "Extracted segments\n",
      "\n",
      " 59398333c50f90cdd3ae2761 {'Background': [Background(Span(\"b'recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'\", sentence=24132, chars=[83,244], words=[13,36]))], 'Purpose': [Purpose(Span(\"b'we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'\", sentence=24132, chars=[80,244], words=[12,36]))], 'Mechanism': [Mechanism(Span(\"b'We developed Cairo to fill this need.'\", sentence=24133, chars=[0,36], words=[0,7]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 59398333c50f90cdd3ae2761,0,b'While developing a suite of tools for statistical machine translation research, we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'), Sentence(Document 59398333c50f90cdd3ae2761,1,b'We developed Cairo to fill this need.'), Sentence(Document 59398333c50f90cdd3ae2761,2,b'Cairo is a free, open-source, portable, user-friendly, GUI-driven program written in Java that provides a visual representation of word correspondences between bilingual pairs of sentences, as well as relevant translation model parameters.\\n')]\n",
      "\n",
      "\n",
      "Document 1/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868725ff05a97b014e76 {'Background': [Background(Span(\"b'Researchers in psychometrics argue that before adding new labels to applications'\", sentence=12535, chars=[0,79], words=[0,10]))], 'Purpose': [Purpose(Span(\"b'In this paper'\", sentence=12536, chars=[0,12], words=[0,2]))], 'Method': [Method(Span(\"b'the labels must be empirically evaluated.'\", sentence=12535, chars=[82,122], words=[12,18])), Method(Span(\"b'We also show how we evaluate the labels empirically using a sample population from Amazon Mechanical Turk users.\\n'\", sentence=12537, chars=[0,112], words=[0,19]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868725ff05a97b014e76,0,b'Linguistic labels such as high, medium, and low are commonly used in different applications.'), Sentence(Document 5834868725ff05a97b014e76,1,b'Researchers in psychometrics argue that before adding new labels to applications, the labels must be empirically evaluated.'), Sentence(Document 5834868725ff05a97b014e76,2,b'In this paper, we explain the process of selecting labels for a security assessment application.'), Sentence(Document 5834868725ff05a97b014e76,3,b'We also show how we evaluate the labels empirically using a sample population from Amazon Mechanical Turk users.\\n')]\n",
      "\n",
      "\n",
      "Document 2/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868625ff05a97b013640 {'Background': [Background(Span(\"b'Previous work explored semantic concepts for content analysis to assist retrieval.'\", sentence=12041, chars=[0,81], words=[0,11]))], 'Purpose': [Purpose(Span(\"b'However'\", sentence=12042, chars=[0,6], words=[0,0]))], 'Mechanism': [Mechanism(Span(\"b'we propose a semi-automatic framework to discover the semantic concepts.\\n'\", sentence=12044, chars=[25,97], words=[5,18]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868625ff05a97b013640,0,b'Huge amount of videos on the Internet have rare textual information, which makes video retrieval challenging given a text query.'), Sentence(Document 5834868625ff05a97b013640,1,b'Previous work explored semantic concepts for content analysis to assist retrieval.'), Sentence(Document 5834868625ff05a97b013640,2,b\"However, the human-defined concepts might fail to cover the data and there is a potential gap between these concepts and the semantics expected from user's query.\"), Sentence(Document 5834868625ff05a97b013640,3,b'Also, building a corpus is expensive and time-consuming.'), Sentence(Document 5834868625ff05a97b013640,4,b'To address these issues, we propose a semi-automatic framework to discover the semantic concepts.\\n')]\n",
      "\n",
      "\n",
      "Document 3/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868625ff05a97b011a7a {'Background': [Background(Span(\"b'Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life'\", sentence=10746, chars=[0,116], words=[0,18]))], 'Purpose': [Purpose(Span(\"b'Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life'\", sentence=10746, chars=[0,116], words=[0,18])), Purpose(Span(\"b'However'\", sentence=10747, chars=[0,6], words=[0,0]))], 'Mechanism': [Mechanism(Span(\"b'Problem-solving approaches differ radically'\", sentence=10748, chars=[0,42], words=[0,5]))], 'Finding': [Finding(Span(\"b'and finding common ground for assessing results can be difficult.\\n'\", sentence=10748, chars=[45,110], words=[7,18]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868625ff05a97b011a7a,0,b\"Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life, they've made multidisciplinary teams the norm rather than the exception.\"), Sentence(Document 5834868625ff05a97b011a7a,1,b'However, overcoming boundaries between various disciplines is a significant challenge and in many cases represents a key factor for successful development.'), Sentence(Document 5834868625ff05a97b011a7a,2,b'Problem-solving approaches differ radically, and finding common ground for assessing results can be difficult.\\n')]\n"
     ]
    }
   ],
   "source": [
    "for ind,docid in enumerate(document_breakdown_map.keys()):\n",
    "    print(\"\\n\\nDocument \"+str(ind)+\"/\"+str(len(document_breakdown_map.keys())))\n",
    "    print(\"Extracted segments\\n\\n\",docid,document_breakdown_map[docid],\"\\n\\n\")\n",
    "    print(\"Complete abstract\\n\\n\",document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent().sentences)\n",
    "    if ind>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we count the number of documents that contain exactly N segments, where N=1, 2, 3, 4, 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_text(doc):\n",
    "    text_string=\" \".join([str(sentence.text) for sentence in doc.sentences])\n",
    "    return text_string\n",
    "\n",
    "\n",
    "def rank_by_matched_segments(document_breakdown_map):\n",
    "    for n in [5,4,3,2,1]:\n",
    "        print(\"Below are one or two document examples that contain exactly \"+str(n)+\" segments\\n\")\n",
    "        showed_examples=0\n",
    "        count=0\n",
    "        for ind,docid in enumerate(document_breakdown_map.keys()):\n",
    "            if len(document_breakdown_map[docid].keys())==n:\n",
    "                count+=1\n",
    "                if showed_examples<2:\n",
    "                    showed_examples+=1\n",
    "                    print(docid,\": \", get_document_text(document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent()))\n",
    "                    print(\"Extracted segments\\n\\n\",docid,document_breakdown_map[docid],\"\\n\\n\")\n",
    "        print(\"Total count is \"+str(count)+\"\\n\\n=================\\n\")\n",
    "        \n",
    "# rank_by_matched_segments(document_breakdown_map)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to some visualization of extracted Span as highlighted text on Document.\n",
    "\n",
    "Color notation: <b style=\"color:orange;\">Background</b> <b style=\"color:pink;\">Purpose</b> <b style=\"color:green;\">Mechanism</b> <b style=\"color:purple;\">Method</b> <b style=\"color:blue;\">Findings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "    \n",
    "def print_colored_text(docid,document_breakdown_map=document_breakdown_map):\n",
    "    \n",
    "    color_mapping={\"Background\":\"orange\",\"Purpose\":\"pink\",\"Mechanism\":\"green\",\"Method\":\"purple\",\"Finding\":\"blue\"}\n",
    "    this_document=document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent()\n",
    "    document_text=get_document_text(this_document)\n",
    "\n",
    "    added_segment=[]\n",
    "    print(\"This document has \"+str(len(document_breakdown_map[docid]))+\" spans\")\n",
    "    for segment in document_breakdown_map[docid]:\n",
    "        spans=document_breakdown_map[docid][segment]\n",
    "#         print(segment)\n",
    "        for ind,span in enumerate(spans):\n",
    "#             print(span.__dict__)\n",
    "            this_span=document_breakdown_map[docid][segment][ind].__dict__[list(document_breakdown_map[docid][segment][ind].__dict__)[6]]\n",
    "            span_sentence_text=this_span.sentence.text\n",
    "            span_text=str(span_sentence_text[this_span.char_start:(this_span.char_end+1)])\n",
    "#             print(\"span_text is \"+span_text)\n",
    "            document_text=document_text.replace(span_text.strip(),\"<font color='\"+color_mapping[segment]+\"'>\"+span_text.strip()+\"</font>\")\n",
    "\n",
    "    printmd(document_text)\n",
    "\n",
    "# docid='5834868625ff05a97b013016'\n",
    "# print_colored_text(docid)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document has 2 spans\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Security requirements analysis depends on how well-trained analysts perceive security risk, understand the impact of various vulnerabilities, and mitigate threats. When systems are composed of multiple machines, configurations, and software components that interact with each other, risk perception must account for the composition of security requirements. In this paper, we report on how changes to security requirements affect analysts risk perceptions and their decisions about how to modify the requirements to reach adequate security levels. <font color='purple'>We conducted two user surveys of 174 participants wherein participants assess security levels across 64 factorial vignettes.</font> <font color='purple'><font color='blue'>We analyzed the survey results using multi-level modeling to test for the effect of security requirements composition on participants' overall security adequacy ratings and on their ratings of individual requirements.</font></font> We accompanied this analysis with grounded analysis of elicited requirements aimed at lowering the security risk. <font color='blue'>Our results suggest that requirements composition affects experts' adequacy ratings on security requirements.</font> In addition, we identified three categories of requirements modifications, called refinements, replacements and reinforcements, and we measured how these categories compare with overall perceived security risk. Finally, we discuss the future impact of our work in security requirements assessment practice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_background_cands=extract_and_display(non_comma_dict_background_matcher,Background,\"Background\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_purpose_cands=extract_and_display(non_comma_dict_purpose_matcher,Purpose,\"Purpose\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_mechanism_cands=extract_and_display(non_comma_dict_mechanism_matcher,Mechanism,\"Mechanism\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_method_cands=extract_and_display(non_comma_dict_method_matcher,Method,\"Method\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_finding_cands=extract_and_display(non_comma_dict_finding_matcher,Finding,\"Finding\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "\n",
    "print_colored_text('cscw18assessment',document_breakdown_map=test_doc_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
