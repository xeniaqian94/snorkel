{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run 7: Load Consistent Groundtruth and Two-level Validation\n",
    "\n",
    "This notebook focuses on two things:\n",
    "1. <b>Consistently</b> associate snorkel Segments (i.e., clauses, or candidates) with their corresponding ground-truth label, i.e., candidates' count should match. \n",
    "2. Do a <b>two-level validation</b>: the first level will be the <b>dev set</b>: some validation on a small set of papers, whose author affiliation is from MLD, RI, HCI, CSD, LTI, ISR; then the second level, <b>test set</b> expands this dev set to be from general AI papers instead. \n",
    "\n",
    "Current `train/valid/test=5560/478/1842`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "debug_mode=1 # if not, debug_mode=0\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following two cells should load in 7880 papers haven't been loaded. Skip them if you have already ran the two cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TSVDocPreprocessor\n",
    "session = SnorkelSession()\n",
    "\n",
    "# # Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 9000 # this is the upper limit of number of docs\n",
    "doc_preprocessor = TSVDocPreprocessor('data/70kpaper_061418_cleaned_noBookLecture_10cscw_2k.tsv', encoding=\"utf-8\",max_docs=n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 19s, sys: 829 ms, total: 1min 20s\n",
      "Wall time: 1min 20s\n",
      "Documents: 7880\n",
      "Sentences: 29036\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs)\n",
    "\n",
    "from snorkel.models import Document, Sentence  # defined in context.py file\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 7880\n",
      "Sentences: 29036\n",
      "The longest sentence has 309 tokens.\n",
      "dev_sents length 2193 dev_doc_set size 478 avg n_sent per doc 4.588\n",
      "test_sents length 7963 test_doc_set size 1842 avg n_sent per doc 4.323\n"
     ]
    }
   ],
   "source": [
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "session = SnorkelSession()\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "\n",
    "docs = session.query(Document).all()\n",
    "sents = session.query(Sentence).all()  # get all sentences from snorkel.db\n",
    "n_max_corpus=0\n",
    "for sent in sents:\n",
    "    n_max_corpus=max(n_max_corpus,len(sent.words))\n",
    "print(\"The longest sentence has \"+str(n_max_corpus)+\" tokens.\")\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "dev_doc_set = set()\n",
    "test_doc_set = set()\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name[:7]==\"2K_dev_\":\n",
    "            dev_sents.add(s)\n",
    "            dev_doc_set.add(doc.name)\n",
    "        elif doc.name[:8]==\"2K_test_\":\n",
    "            test_sents.add(s)\n",
    "            test_doc_set.add(doc.name)\n",
    "        else:\n",
    "            train_sents.add(s)\n",
    "            \n",
    "print(\"dev_sents length\", len(dev_sents),\"dev_doc_set size\", len(dev_doc_set), \"avg n_sent per doc\",\"%.3f\"%(float(len(dev_sents))/float(len(dev_doc_set))))\n",
    "print(\"test_sents length\", len(test_sents),\"test_doc_set size\", len(test_doc_set), \"avg n_sent per doc\",\"%.3f\"%(float(len(test_sents))/float(len(test_doc_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 14s, sys: 577 ms, total: 1min 14s\n",
      "Wall time: 1min 15s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 35228**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 34.5 s, sys: 143 ms, total: 34.7 s\n",
      "Wall time: 34.7 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 6006**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 3s, sys: 318 ms, total: 2min 3s\n",
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 21687**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import *\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "\n",
    "def extract_and_display(matcher,candidate_class,candidate_class_name,train_breakdown_map=None,dev_doc_breakdown_map=None,selected_split=0,is_print=True):  # split over train/dev/test but returns only train set\n",
    "#     input(candidate_class)\n",
    "    for (i, sents) in ([(0,train_sents), (1,dev_sents), (2,test_sents)] if selected_split==0 else ([(2,test_sents)] if selected_split==2 else [(1,dev_sents)])):\n",
    "        %time matcher.apply(sents, split=i)\n",
    "        printmd(\"**Split \"+str(i)+\" - number of candidates extracted: \"+str(session.query(candidate_class).filter(candidate_class.split == i).count())+\"**\\n\\n\")\n",
    "    train_cands = session.query(candidate_class).filter(candidate_class.split == selected_split).all()\n",
    "    if is_print:\n",
    "        for i in range(min(4,len(train_cands))): # to print at most 4 cands \n",
    "            printmd(\"**\"+str(i)+\"/\"+str(len(train_cands))+\" Candidate/Span:**\\t`\"+str(train_cands[i])+\"`\")\n",
    "            printmd(\"**Its parent Sentence's text:**\\t\"+str(train_cands[i].get_parent().text))\n",
    "            printmd(\"**Its parent Document's text:**\\t\"+str(train_cands[i].get_parent().get_parent().__dict__))\n",
    "            print() \n",
    "        \n",
    "    for cand in train_cands:\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in train_breakdown_map:\n",
    "            train_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in train_breakdown_map[doc_name]:\n",
    "            train_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        train_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "        \n",
    "    dev_cands = session.query(candidate_class).filter(candidate_class.split == 1).all()\n",
    "    for cand in dev_cands:\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in dev_doc_breakdown_map:\n",
    "            dev_doc_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in dev_doc_breakdown_map[doc_name]:\n",
    "            dev_doc_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        dev_doc_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "    test_cands=session.query(candidate_class).filter(candidate_class.split==2).all()\n",
    "    \n",
    "    return train_cands,dev_cands,test_cands\n",
    "\n",
    "Segment = candidate_subclass('Segment', ['segment_cue'])\n",
    "ngrams = Ngrams(n_max=n_max_corpus) \n",
    "non_comma_matcher=DictionaryMatch(d=[','],longest_match_only=True,reverse=True)  \n",
    "non_comma_segment_extractor=CandidateExtractor(Segment, [ngrams], [non_comma_matcher])\n",
    "train_doc_breakdown_map=dict()\n",
    "dev_doc_breakdown_map=dict()\n",
    "train_segments,dev_segments,test_segments=extract_and_display(non_comma_segment_extractor,Segment,\"Segment\",train_doc_breakdown_map,dev_doc_breakdown_map,is_print=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many segments did Snorkel automatically extracted for dev set?\n",
    "<b>6006</b>, see about and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**All together we extracted 6006 segments**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A segment example** Span(\"b'too large to fit in memory'\", sentence=28305, chars=[64,89], words=[14,19])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Extracted text** too large to fit in memory"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**A line in ground-truth file for this doc** `too large to fit in memory\tmechanism\t2K_dev_1230`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Can they get matched?** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This above example shows the thing that we are going to do: associate **extracted text** with **lines in ground-truth file**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(dev_segments[0].segment_cue.stable_id)\n",
    "from util import get_candidate_text\n",
    "printmd(\"**All together we extracted \"+str(len(dev_segments))+\" segments**\")\n",
    "printmd(\"**A segment example** \"+str(dev_segments[123].segment_cue))\n",
    "# printmd(\"**Its __dict__ element** \"+str(dev_segments[123].segment_cue.__dict__))\n",
    "# printmd(\"**Its parent Sentence** \"+str(dev_segments[123].segment_cue.sentence))\n",
    "printmd(\"**Extracted text** \"+str(get_candidate_text(dev_segments[123])))\n",
    "printmd(\"**A line in ground-truth file for this doc** `\"+str(\"too large to fit in memory\tmechanism\t2K_dev_1230`\"))\n",
    "printmd(\"**Can they get matched?** \"+str())\n",
    "printmd(\"This above example shows the thing that we are going to do: associate **extracted text** with **lines in ground-truth file**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_level=1\n",
    "current_docid_prefix=\"2K_dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Take a random example doc, what segments does it have<br /><br />** defaultdict(<function load_groundtruth_as_external_dict.<locals>.<lambda>.<locals>.<lambda> at 0x16db2d488>, {'background': ['Finding meaningful structured representations of 3D point cloud data ( PCD ) has become a core task for spatial perception applications'], 'finding': ['our tests showing favorable performance when compared to octree and NDT-based methods'], 'mechanism': ['In this paper we introduce a method As opposed to deterministic structures such as voxel grids or octrees', 'we propose probabilistic subdivisions of the data through local mixture modeling', 'and show how these subdivisions can provide a maximum likelihood segmentation of the data', 'The final representation is hierarchical', 'compact parametric and statistically derived', 'facilitating run-time occupancy calculations through stochastic sampling', 'Unlike traditional deterministic spatial subdivision methods', \"our technique enables dynamic creation of voxel grids according the application 's best needs\", 'In contrast to other generative models for PCD', 'we explicitly enforce sparsity among points and mixtures', 'a technique which we call expectation sparsification', 'This leads to a highly parallel hierarchical Expectation Maximization ( EM ) algorithm well-suited for the GPU and real-time execution'], 'method': ['We explore the trade-offs between model fidelity and model size at various levels of detail'], 'purpose': ['for constructing compact generative representations of PCD at multiple levels of detail']})"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util import load_groundtruth_as_external_dict\n",
    "\n",
    "groundtruth_dict=load_groundtruth_as_external_dict(\"data/annotations_label-level_all-to-date-2018-4-25-WithTitle.labelled_level_\"+str(current_level)+\".csv\")\n",
    "printmd(\"**Take a random example doc, what segments does it have<br /><br />** \"+str(groundtruth_dict[current_docid_prefix+'_2064']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Generate a file  `[segment_name]_gold_dev.tsv`, which contains gold labels from dev sets\n",
    "\n",
    "Each line has the format of `[stable_label_id]\\t[label]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which provides guarantees against all adversary behavior models satisfying monotonicity\n",
      "including all in the family of Regular Quantal Response functions for computing monotonic maximin\n",
      "and provide no quality guarantee when the estimated model is inaccurate\n",
      "Most existing approaches for computing defender strategies against boundedly rational adversaries try to optimize against specific behavioral models of adversaries\n",
      "All prior lossy abstraction algorithms for extensive-form games either 1 ) had no bounds on solution quality or 2 ) depended on specific equilibrium computation approaches\n",
      "limited forms of abstraction\n",
      "and only decreased the number of information sets rather than nodes in the game tree to give bounds on solution quality for any perfect-recall extensive-form game\n",
      "Given these two approaches\n",
      "we ask : which is the correct one ?\n",
      "Current literature has seemingly contradictory results for IED : some studies claim good fits with power laws ; others with non-homogeneous Poisson processes\n",
      "subject to the envy freeness constraint\n",
      "that enables the computation of such solutions in polynomial time\n",
      "We therefore focus on solutions that optimize a criterion of social justice\n",
      "in order to pinpoint the `` fairest '' solutions\n",
      "What do graphs look like ?\n",
      "our main idea is to find the most succinct description of a graph in these `` vocabulary '' terms\n",
      "bipartite cores cliques and chains\n",
      "Starting with the observation that real graphs often consist of stars\n",
      "Yet kidney exchange is inherently dynamic\n",
      "So it is important to consider the future when matching\n",
      "for learning to match in a general dynamic model\n",
      "Also many planned exchange transplants do not go to surgery due to various failures\n",
      "with participants arriving and departing\n",
      "To learn a high-dimensional conditional distribution of outputs given inputs\n",
      "Can we reconcile them all ?\n",
      "How do they evolve over time ?\n",
      "Does the rising and falling pattern follow a simple universal law ?\n",
      "These developments raise privacy concerns and call for novel solutions to ensure adequate user awareness\n",
      "enhance user 's awareness of and control over the collection and use of video data about them\n",
      "and ideally control over the resulting collection and use of potentially sensitive data\n",
      "While cameras have become ubiquitous\n",
      "most of the time users are not even aware of their presence\n",
      "This paper presents a study comparing collaborative and individual methods while receiving instruction on either procedural or conceptual knowledge\n",
      "find the most appropriate labels for the rest ; and ( ii ) Mining and attention routing - that best represent the data\n",
      "Here we answer all these questions\n",
      "solution to two problems : ( i ) Low-labor labeling ( LLL )\n",
      "Unmatched - striped_query_text:  and may fail to scale to larger exchanges the clearing problem is solvable in polynomial time. Show that indeed small numbers of attributes suffice\n",
      "but these methods must be tailored to specific models and objective functions\n",
      "State-of-the-art matching engines use integer programming techniques to clear fielded kidney exchanges\n",
      "including clustering outlier detection\n",
      "Having such signatures will enable a wealth of graph mining and social network analysis tasks\n",
      "visualization etc for solving the above problem\n",
      "which can confuse or mislead users thus leading to an increase in privacy risk\n",
      "The theory predicts how vague modifiers to information actions and information types can be composed to increase or decrease overall vagueness\n",
      "ambiguity and vagueness in particular\n",
      "undermines the ability of organizations to align their privacy policies with their data practices\n",
      "For web-based and mobile information systems\n",
      "How does influence/news/viruses propagate\n",
      "that provides a simple account of the observed deviations\n",
      "Such deviations from the disparity energy model provide us with insight into how network interactions may play a role in disparity processing and help to solve the stereo correspondence problem\n",
      "Unmatched - striped_query_text:  the formation of micro-clusters in appropriate feature spaces ) ; and. ( b )\n",
      "Is there a simple universal model to depict the come-and-go patterns of various groups ? for group evolution\n",
      "the rise and fall patterns of information diffusion for the real-time monitoring of information diffusion\n",
      "for abstracting fine-grained audience segments into coarser abstract segments that are not too numerous for use in such optimization\n",
      "However it also dramatically increases the complexity that the publisher faces when optimizing campaign admission decisions and inventory allocation to campaigns\n",
      "( RQ2 )\n",
      "This is a tedious process and the Web search engines address only part of the overall problem\n",
      "estimates credibility of sources and correctness of claims\n",
      "producing only a list of relevant sources\n",
      "Given this scenario in this article\n",
      "we tackle a fundamental aspect of this new era of communication : How the time intervals between communication events behave for different technologies and means of communications Are there universal patterns for the Inter-Event Time Distribution ( IED ) q How do inter-event times behave differently among particular technologiesq To answer these questions to generate inter-event times between communications\n",
      "How do people interact with their Facebook wall ?\n",
      "This paper investigates how coverage can be maintained by automatically acquiring potential out-of-vocabulary ( OOV ) words\n",
      "Considering the skills and functional requirements of musicians leads to a number of predictions about future humancomputer music performance ( HCMP ) systems for popular music for such systems\n",
      "We use the notion of balance to give a more fine-grained understanding of several well-studied routing questions that are considerably harder in directed graphs for computing low-radius decompositions of directed graphs parameterized by balance\n",
      "For realtime generation of stylistic human motion to capture the complex relationships between styles of motion\n",
      "to predict the timings of synthesized poses in the output style\n",
      "For situational awareness in vehicular systems that span driverless and drivered vehicles\n",
      "can we find patterns and regularities ?\n",
      "Given electroencephalogram time series data from patients with epilepsy\n",
      "Which is essentially the sufficient statistics of the adopted generative models and does not involve the parameters of generative models\n",
      "for the score space that seeks to utilize label information\n",
      "how can we automatically spot anomalous\n",
      "Given a directed graph of millions of nodes\n",
      "suspicious nodes judging only from their connectivity patterns ?\n",
      "At a high level\n",
      "this question captures the essence of our work\n",
      "the much fewer Facebook studies focus on the friendship graph or are limited by the amount of users or the duration of the study\n",
      "While most prior efforts focus on Twitter\n",
      "However is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time ?\n",
      "For solving extensive-form games for the strategy spaces of sequential games\n",
      "How do the k-core structures of real-world graphs look like ?\n",
      "the structural patterns and their correlations with dynamics and semantics are largely unknown to quantify the structural characteristics of information cascades\n",
      "Although the dynamics and semantics of information cascades have been studied\n",
      "For users in cognitive decline\n",
      "changing over time how can we find patterns and anomalies ? which can discover both transient and periodic/ repeating communities\n",
      "Abstract : Given a large network\n",
      "That this crucial design consideration to meet interactive performance criteria limits data center consolidation\n",
      "In support of such safety arguments\n",
      "we analyze fo r avoiding both stationary and moving obstacles that describe and formally verify the robots discrete control decisions along with it s continuous\n",
      "How long does it take for a URL that distributes malware to be detected and shut down ?\n",
      "To help mobile-app developers check their privacy policies against their apps ' code for consistency\n",
      "To capture the notion of object persistence and continuity in our visual experience\n",
      "What can we say about these marked nodes ?\n",
      "Abstract : Suppose we are given a large graph in which\n",
      "by some external process\n",
      "a handful of nodes are marked\n",
      "In this paper we analyze time-stamp data from social media services that is able to match all four discovered patterns\n",
      "This article presents a resource analysis system for OCaml programs\n",
      "Can we mimic and measure the effect ? to evaluate robustness to construct secure networks operating within malicious environments\n",
      "In this paper we answer this question by studying the interactive tagging network constructed by Twitter lists\n",
      "We address the following research questions : ( RQ1 ) What is the common patterns and the difference between the interactive tagging network and the resource tagging networks ?\n",
      "will she also have dou- ble the phone-calls ( or wall-postings\n",
      "or tweets ) ?\n",
      "If Alice has double the friends of Bob\n",
      "( b ) In which country is the competition strong ? ( c )\n",
      "In this work\n",
      "we answer these questions to efficiently extrapolate crucial properties of the data from a small sample\n",
      "We formalize the problem for partitioning marked nodes as well as finding simple paths between nodes within parts\n",
      "Tensor decompositions have gained a steadily increasing popularity in data-mining applications ; however\n",
      "the current state-of-art decomposition algorithms operate on main memory and do not scale to truly large datasets\n",
      "for speeding up tensor decompositions\n",
      "To detect real-world errors\n",
      "In this paper we study whether recent password guidance carries over to the mobile setting\n",
      "However this research has generally been in the context of desktops and laptops\n",
      "while users are increasingly creating and entering passwords on mobile devices\n",
      "While much research has studied assignment policies\n",
      "little has taken into account server-side variability -- the fact that the server we choose might be temporarily and unpredictably slow that replicates each arrival to d servers chosen at random task assignment policy\n",
      "However the change in power consumption by cement crushing plants and also other industrial loads are often not granular enough to provide valuable ancillary services such as regulation and load following\n",
      "to overcome the granularity restriction with the help of an energy storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And if so\n",
      "to what extent ?\n",
      "to quantify both concepts ( synchronicity and normality )\n",
      "and we address the problem by finding interestingness-driven diffusion processes\n",
      "We study the topic of dynamic network summarization : how to summarize dynamic networks with millions of nodes by only capturing the few most interesting nodes or edges over time\n",
      "Here we explore the identifi- cation of fraudulent and genuine retweet threads\n",
      "( RQ3 )\n",
      "An important second challenge is the common presence of budget complementarities\n",
      "significantly limiting the space of options that can be explored\n",
      "Unmatched - striped_query_text:  This introduces a combinatorial structure on the decision space We propose to address these challenges for achieving this approximation in an online fashion. Demonstrates the effectiveness of our approach\n",
      "In particular simulation runs are time consuming\n",
      "where non-negligible budget increments are required for an appreciable marginal impact from a channel\n",
      "Do users tag each other on the interactive tagging network ?\n",
      "This has proven to be a key difficulty in developing therapies\n",
      "since the organisms develop resistance\n",
      "Which seminal papers have influenced the topic the most ?\n",
      "What is normal and what is suspicious ?\n",
      "Unmatched - striped_query_text:  This paper is ( i ) to assume periodic statistics without needing to revisit the negative set and ( ii ) to accelerate the estimation of detectors with aperiodic statistics. Verified that periodicity is detrimental\n",
      "We show here that\n",
      "surprisingly both approaches are correct\n",
      "In particular we prove that it selects allocations that are envy free up to one good -- - a compelling notion that is quite elusive when coupled with economic efficiency\n",
      "using information gathered by that dart to guide what to do next\n",
      "However deciding how to branch in the search tree is hard\n",
      "and impacts search time drastically\n",
      "( 2 ) How to detect points in time where the rating distribution differs from this base behavior\n",
      "due to attacks or spontaneous changes in the product 's quality ?\n",
      "but the majority of this work does not take into account the specialty of tokens\n",
      "many topic modeling techniques have been proposed for social media data\n",
      "such as hashtags and treats them as ordinary words\n",
      "In the recent years\n",
      "to address the problem of discovering latent topics and their sentiment from social media data\n",
      "mainly microblogs like Twitter\n",
      "for fitting the MQGM for sampling from the joint distribution that underlies the MQGM estimate\n",
      "For learning sparse graphical models\n",
      "Unmatched - striped_query_text:  lower bounding and lazy bounding. Validate the approach and show that our techniques dramatically improve scalability over a leading general-purpose MIP solver\n",
      "and is known to include features that are not acceptable in many applications\n",
      "such as favoring some bidders over others and randomization A second challenge in mechanism design for combinatorial auctions is that the prior distribution on each bidder 's valuation can be doubly exponential\n",
      "The optimal design is unknown\n",
      "for branching upper bounding\n",
      "that models the coevolution of user activities\n",
      "These are the questions we focus on\n",
      "We address this problem\n",
      "convergence ) for large sparse matrices\n",
      "However eigensolvers suffer from subtle problems ( e\n",
      "let alone for billion-scale ones\n",
      "Is it possible to monitor the entire traffic in Manhattan at a few intersections ? to handle complex\n",
      "and how they break ties for computing optimal commitment strategies\n",
      "This paper initiates a new direction via two simultaneous deviation points : generalization to imperfect-information games and a game-theoretic approach The question of how one should act when facing an opponent whose lookahead is limited is studied along multiple axes : lookahead depth\n",
      "too have imperfect information\n",
      "whether the opponent ( s )\n",
      "In order to conduct this study\n",
      "Unmatched - striped_query_text:  to distinguish between compromised vs. First we find that legitimate but compromised websites constitute 33\n",
      "in which agents simultaneously send messages containing a sketch of their preferences over the cake\n",
      "For cake cutting ( the fair allocation of a divisible good )\n",
      "to speed up performance-critical operations\n",
      "For improving performance in VM-based mobile computing systems implemented as thick clients on host PCs\n",
      "depending on the preferences of the agents\n",
      "We ask whether bounded protocols exist when the agents ' preferences are restricted\n",
      "Although an envy-free cake cutting protocol was ultimately devised\n",
      "it is unbounded in the sense that the number of operations can be arbitrarily large\n",
      "among Alice and other users ?\n",
      "How to detect abnormal search behaviors\n",
      "In this paper we address this issue\n",
      "for providing bounds on solution quality for discretization of continuous action spaces in extensive-form games\n",
      "We investigate the power of voting among diverse\n",
      "randomized software agents allows us to reason about a collection of agents with different biases ( determined by the first-stage noise models )\n",
      "This work tackles the problem of feature representation from the context of sparsity and affine rank minimization in order to provide answers to the aforementioned questions\n",
      "For example can a person compute a function in their head so that an eavesdropper with a powerful computer -- - who sees the responses to random input -- - still can not infer responses to new inputs ?\n",
      "The intent of this paper is to apply the ideas and methods of theoretical computer science to better understand what humans can compute in their heads\n",
      "However there is no exact analysis of systems with redundancy\n",
      "This paper presents the first exact analysis of systems with redundancy\n",
      "'Alice ' is submitting one web search per five minutes\n",
      "for three hours in a row - is it normal ?\n",
      "Is there a simple universal model to depict the come-and-go patterns of various groups ? for group evolution\n",
      "For finding an epsilon-Nash equilibrium for arbitrarily small epsilon\n",
      "little is known about their structural patterns to quantify the structural characteristics of millions of information cascades\n",
      "Although there has been much progress on understanding the dynamics and semantics of information cascades\n",
      "And so identifying users that have the potential of becoming strong contributers is an important task for owners of such communities\n",
      "for detecting influential and anomalous users in the underlying user interaction network\n",
      "How long does it take for a URL that distributes malware to be detected and shut down ?\n",
      "for per-class response time\n",
      "the system should also be fair in the sense that no job class should have a worse mean response time in the system with redundancy than it did in the system before redundancy is allowed In this paper we use scheduling to address the simultaneous goals of ( 1 ) achieving low response time and ( 2 ) maintaining fairness across job classes\n",
      "However response time is not the only important metric in redundancy systems : in addition to providing low overall response time\n",
      "as well as the behavioral responses\n",
      "that solves the CMTF problem\n",
      "In short we want to find latent variables\n",
      "that explain both the brain activity\n",
      "Given a large cloud of multi-dimensional points\n",
      "why does it take a week to finish ? to eliminate the problem\n",
      "and an off-theshelf outlier detection method\n",
      "For warm starting CFR\n",
      "In this work we seek to identify how we can design new movies with features tailored to a specific user population\n",
      "The memory and computational complexity associated with this step grow as a quadratic and cubic function of the problem dimension ( the number of samples / features )\n",
      "since they involve square root factorization of the correlation matrices of the views\n",
      "To circumvent such difficulties\n",
      "Existing ( G ) CCA algorithms have serious scalability issues\n",
      "However current tensor decomposition methods do not scale for tensors with millions and billions of rows\n",
      "columns and fibers that often appear in real datasets\n",
      "For narrow and well-defined tasks that require specialized knowledge and/or skills\n",
      "In this paper we empirically explore how the latency of transcriptions created by participants recruited on Amazon Mechanical Turk vary based on the accuracy of speech recognition output\n",
      "Open-source face recognition system for denaturing video streams for large camera networks using RTFace\n",
      "To probabilistic segmentation and modeling of time series data for solving the resulting ( large ) optimization problems for estimating recurring clusters\n",
      "Unmatched - striped_query_text:  Page Rank ) can provide a ranking that has the same ac- curacy in predicting winners of upcoming match-ups as more complicated systems ( e. We further explore the impact of the network structure on the prediction accuracy and\n",
      "What is the pattern of people quitting from groups ?\n",
      "Many such chains are quasi-birth-death processes with transitions that are skip-free in level\n",
      "in that one can only transition from lower-numbered phases to higher-numbered phases for determining the limiting probabilities of such Markov chains exactly\n",
      "in that one can only transition between consecutive levels\n",
      "and unidirectional in phase\n",
      "we answer these questions\n",
      "In this work\n",
      "In this paper we focus on how to estimate the confidence on the node classi- fication problem\n",
      "We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food for mining large-scale co-evolving online activities\n",
      "In this work we model Facebook user behavior : we analyze the wall activities of users focusing on identifying common patterns and surprising phenomena to fit our data\n",
      "We conjecture that online activities compete for user attention in the same way that species in an ecosystem compete for food for mining large-scale co-evolving online activities\n",
      "For understanding discussions between students in MOOC forums\n",
      "for discovering instances in which a response relation exists between a pair of posts in a forum thread\n",
      "while maintaining good accuracy ?\n",
      "Can we accelerate any CMTF solver\n",
      "so that it runs within a few minutes instead of tens of hours to a day\n",
      "under the flow of polynomial ordinary differential equations\n",
      "that is sets satisfying polynomial equalities\n",
      "This paper presents a theoretical and experimental comparison of sound proof rules for proving invariance of algebraic sets\n",
      "For a setting in which the human user can only receive assistance from a semi-trusted computer\n",
      "In the low-resource scenario\n",
      "to automatically learn pronunciations iteratively from acoustics\n",
      "we may not have linguistic resources such as diacritizers or hand-written rules for the language\n",
      "For computing the Voronoi diagram of a set of n points in constant-dimensional Euclidean space\n",
      "In this article the authors examine can overcome this challenge\n",
      "We report a of designing\n",
      "deploying and iterating on a series of playtesting workshops for novice game designers\n",
      "Usability studies have demonstrated that existing password composition policies can sometimes result in weaker password distributions ; hence a more principled approach is needed\n",
      "for optimizing password composition policies\n",
      "For a setting in which the human user can only receive assistance from a semi-trusted computer\n",
      "This is a direct consequence of the over-centralization of today 's cloud-based IoT hub designs\n",
      "We propose a solution\n",
      "# SAT that generalizes both of these languages\n",
      "Here we examine an expressive new language\n",
      "What is the genealogy of the seminal papers in this topic ?\n",
      "however due to constraints including accurate timing based on beats and adherence to a form or structure despite possible changes that may occur\n",
      "the problem of mapping from a conventional score\n",
      "takes into account latency due to communication delays and audio buffering\n",
      "There are significant challenges to overcome\n",
      "possibly even during performance\n",
      "Way of ranking sports teams\n",
      "Reconstructs building facades in 3D space\n",
      "We discuss how they make it easier to prove hybrid systems as well as help learn how to conduct proofs in the first place\n",
      "Due to undecidability verification tools need sufficient means for intervening during the verification and need to allow verification engineers to provide system design insights\n",
      "A well-known problem with BP\n",
      "however is that there are no known exact guarantees of convergence in graphs with loops that allows a closed-form solution that propagates information across every edge at most once\n",
      "which we try to answer in this paper\n",
      "that finds a set of seminal papers on a given topic\n",
      "that constructs a genealogy of the seminal papers\n",
      "These are the questions that they can raise\n",
      "For efficiently solving general convex optimization problems specified as disciplined convex programs ( DCP )\n",
      "which contains an unknown number of patterns of different durations\n",
      "how can we efficiently and effectively find typical patterns and the points of variation ?\n",
      "Given a large collection of co-evolving multiple time-series\n",
      "however between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide\n",
      "To address this deficiency that allows the theorem prover KeYmaera to locally reason about behaviors\n",
      "There is often a gap\n",
      "We explore how such infrastructure improves latency and energy consumption relative to the cloud\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But its role in security-feature adoption is unique and remains unclear\n",
      "We prove that finding a positive-price chain is NP-complete\n",
      "That is allocations where each player values her own allocated set of goods at least as highly as any other player 's allocated set of goods\n",
      "Given a large image set\n",
      "in which very few images have labels\n",
      "how to guess labels for the remaining majority ?\n",
      "These are exactly the problems we focus on in this work\n",
      "Our main contributions are\n",
      "However due to the lack of empirical data\n",
      "let alone the regularities or models governing these microscopic dynamics\n",
      "little is known about the empirical dynamic patterns of social connectivity at microscopic level\n",
      "How to spot images that need brand new labels different from the predefined ones ?\n",
      "That reduces the space requirements of CFR\n",
      "allowing for cycles in directed graphs )\n",
      "We revisit the problem of designing optimal\n",
      "individually rational matching mechanisms ( in a general sense\n",
      "where each player -- -who is associated with a subset of vertices -- -matches as many of his own vertices when he opts into the matching mechanism as when he opts out We offer a new perspective on this problem\n",
      "But have previously failed to model the eye accurately due to complexities in its material and motion captures eye region shape to allow independent eyeball movement\n",
      "So how can we find a combined model\n",
      "locations and time-ticks ?\n",
      "for all these diseases\n",
      "which is input to an automatic system to produce appropriate gestures\n",
      "This work proposes a middle ground where untrained human workers label semantic information\n",
      "Are there any seasonal/annual activities ?\n",
      "However the scheduling of steel plants is very complex and the involved computations are intense\n",
      "In this paper we focus on these difficulties to make the computations more tractable\n",
      "to detect independently developed red team inserts of malicious insider activities\n",
      "To develop integrate and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations ' information systems\n",
      "It seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule\n",
      "We challenge this assertion\n",
      "which simply counts the number of times each alternative was approved\n",
      "How does malware propagate ?\n",
      "necessitating a setup time for turning a server back on ; however\n",
      "at most one server may be in setup mode at any time\n",
      "where idle servers are turned off to save cost\n",
      "We consider the M/G/k/staggered-setup\n",
      "as most research focuses on detecting whether a website distributes malware\n",
      "In this paper we ask : How does web-based malware spread ?\n",
      "Yet there has been relatively little work in modeling the behaviors and temporal properties of websites\n",
      "To address this question\n",
      "for constructing optimal embeddings in snowflake spaces that runs in O ( m log log n ) time\n",
      "For solving symmetric diagonally dominant ( SDD ) linear systems with m non-zero entries to a relative error of e in O ( m log 1/2 n log c n log ( 1/ e ) ) time\n",
      "Will the enthusiasm drop exponentially with time\n",
      "or oscillate ?\n",
      "( d ) How can we automatically detect important world-wide ( or local ) events ?\n",
      "capable of doing exactly that\n",
      "State estimation techniques can be used to infer the necessary information from the aggregate power consumption of these loads\n",
      "replacing the need for an upstream communication platform carrying information from appliances to the main controller in real-time\n",
      "as an alternative to a Kalman filter approach\n",
      "that assesses the similarity between two graphs on the same nodes\n",
      "and evaluate when state-of-the-art methods fail to detect crucial connectivity changes in graphs\n",
      "In this work we formally state the axioms and desired properties of the graph similarity functions\n",
      "Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts\n",
      "That can significantly lower the barrier for modelers to specify and solve convex stochastic optimization problems\n",
      "We here investigate the effect of revisits ( successive visits from a single user ) on content popularity which captures the popularity dynamics of individual objects\n",
      "9th ) for tail latency SLOs\n",
      "but are only looking for SLOs at lower percentiles ( e\n",
      "Typical tenants do not require strict worst-case guarantees\n",
      "In such a setting\n",
      "dynamically monitoring communication between processes becomes a necessity for identifying undesirable actions\n",
      "In this paper we show how to dynamically monitor communication to enforce adherence to session types in a higher-order setting\n",
      "such as a social network\n",
      "Given a large graph with several millions or billions of nodes and edges\n",
      "how can we explore it efficiently and find out what is in the data ? that enables the comprehensive analysis of large graphs\n",
      "Given a large collection of epidemiological data consisting of the count of d contagious diseases for l locations of duration n\n",
      "how can we find patterns\n",
      "rules and outliers ?\n",
      "Is it possible to infer structure between movies simultaneously ? that accomplishes all of these goals\n",
      "which may indicate zombie followers and suspicious followees ?\n",
      "Can we spot the suspicious following behavior\n",
      "truthfulness can be verified efficiently ( in the computational sense )\n",
      "That describes the spread of two mutually exclusive viruses across heterogeneous composite networks\n",
      "one static ( social connections ) and one dynamic ( mobility pattern )\n",
      "We explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational Bayes ( SGVB ) with global model parameters inference of more flexibly parameterized posteriors often leading to better generalization\n",
      "How does malware propagate ?\n",
      "We study the problem of fairly allocating unused classrooms in public schools to charter schools\n",
      "Of human partial adaptation to the robot capturing the evolution of their expectations of the robot 's capabilities\n",
      "Here we explore pervasive patterns related to k-cores and emerging in graphs from diverse domains\n",
      "To study the existence of winning strategies for such hybrid games\n",
      "We study the design of pricing mechanisms and auctions when the mechanism designer does not know the distribution of buyers ' values\n",
      "to measure the intrinsic complexity of a variety of widely-studied single- and multi-item auction classes We demonstrate how to determine the precise level in a hierarchy with the optimal tradeoff between profit and generalization using structural profit maximization\n",
      "We show how cloudlets enable a new genre of applications called cognitive assistance applications that augment human perception and cognition for cognitive assistance\n",
      "which restricts the load from offering valuable ancillary services such as regulation and load following\n",
      "However in the cement plant as well as other industrial loads\n",
      "In this paper we overcome this restriction of poor granularity\n",
      "as continuous power changes are required for these services\n",
      "switching on/off the loading units only achieves discrete power changes\n",
      "A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists\n",
      "to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions\n",
      "A major complaint is that programmers often write minimal and low quality handlers\n",
      "a human pilot under stress is not necessarily able to understand the complexity of the distributed system and may not take the right course\n",
      "In tough scenarios where a large number of aircraft must execute a collision avoidance maneuver\n",
      "especially if actions must be taken quickly\n",
      "However current tensor decomposition methods do not scale to large and sparse real-world tensors with millions of rows and columns and `fibers for large-scale tensor decompositions\n",
      "we provide the first exact\n",
      "no exact analysis exists to date for the M/M/k/setup with $ $ k > 1 $ $ k In this paper\n",
      "While the M/M/1/setup was exactly analyzed in 1964\n",
      "closed-form analysis for the M/M/k/setup and some of its important variants including systems in which idle servers delay for a period of time before turning off or can be put to sleep\n",
      "Classic cake cutting protocols are susceptible to manipulation\n",
      "Do their strategic outcomes still guarantee fairness ?\n",
      "To design faster parallel graph algorithms involving distances\n",
      "unbounded-length chains are not desirable : planned donations can fail before transplant for a variety of reasons\n",
      "so parallel shorter chains are better in practice\n",
      "While chains can be quite long\n",
      "and the failure of a single donation causes the rest of that chain to fail\n",
      "For ensuring that verification results about models apply to cyber-physical systems ( CPS ) implementations\n",
      "as social interactions lead to correlated votes\n",
      "This assumption is unrealistic in settings where the voters are connected via an underlying social network structure\n",
      "for ranked voting to recover the ground truth\n",
      "With the increase of the size of the tensor data that need to be analyzed there grows the need for efficient and scalable algorithms to compute diagnostics such as CORCONDIA\n",
      "in order to assess the modelling quality\n",
      "Which patterns exist in real-world dynamic graphs\n",
      "and how can we find and rank them in terms of importance ?\n",
      "Unfortunately the approaches to modeling and analyzing the behavior of dynamic spatial systems are just as diverse as these application domains for the medium-term control of autonomous agents in dynamic spatial systems for integrating different approaches of dynamic spatial system analysis to achieve coverage of all required features\n",
      "for generating such abstractions\n",
      "Which enables CFR to scale to dramatically larger abstractions and numbers of cores\n",
      "Additionally existing solutions do not consider what vertices each partition will have\n",
      "as a result high-degree vertices may be concentrated into a few partitions\n",
      "which may lead to time-varying skewness for traversal-style graph workloads\n",
      "since they only explore part of the graph in each superstep\n",
      "causing imbalance the objective is to create an initial partitioning that will `` hold well '' over time without suffering from skewness\n",
      "like Breadth First Search\n",
      "However existing well-studied graph partitioners often assume that vertices of the graph are always active during the computation\n",
      "In this study we consider how Csound unit generators can be exposed to direct access by other audio processing languages\n",
      "For decomposing an undirected unweighted graph into small diameter pieces\n",
      "there are few analytic tools that can provide insights into user reviews\n",
      "Unfortunately beyond simple summaries such as histograms of user ratings\n",
      "In this article we focus on the following important question : Can we identify and use patterns of human communication to decide whether a human or a bot controls a user ? fit the distribution of IATs that detects if users are bots based only on the timing of their postings\n",
      "but they do not consider distrust to handle all three types of interaction information : explicit trust\n",
      "Most such methods use only explicit and implicit trust information ( e\n",
      "then Smith implicitly trusts Johnson )\n",
      "Unmatched - striped_query_text:  implicit trust and explicit distrust. Confirm that PIN-TRUST is scalable and outperforms existing methods in terms of prediction accuracy\n",
      "if Smith likes several of Johnson 's reviews\n",
      "how quickly will people notify their friends about it ?\n",
      "When a free catchy application shows up\n",
      "However displays are of little use without applications to drive them and yet the nature of application support has been largely ignored in the field with the prevailing assumption being that applications will be cloud-based and Web-centric\n",
      "to execute high-performance applications that would not be possible using purely Web-centric technologies\n",
      "In this paper we address this problem for object / scene contents mining for representation via CNN\n",
      "objects and scene context\n",
      "object detection recognition and scene understanding\n",
      "However automatically recognizing cultural events still remains a great challenge since it depends on understanding of complex image contents such as people\n",
      "Therefore it is intuitive to associate this task with other high-level vision problems\n",
      "That solves rearrangement planning problems\n",
      "How can we robustly separate regular patterns and outliers\n",
      "without requiring any prior information ? to capture both cyclic patterns and outliers\n",
      "which solves the above problem\n",
      "For VMs under contention\n",
      "For the first steps of optical music recognition\n",
      "We revisit the classic problem of designing voting rules that aggregate objective opinions\n",
      "in a setting where voters have noisy estimates of a true ranking of the alternatives\n",
      "and only weak algorithm-specific guarantees on solution quality are known for Nash equilibria\n",
      "However imperfect-recall abstractions are poorly understood\n",
      "we ask `` what is a good dispatching policy to minimize the value-weighted response time metric ?\n",
      "in that they are equally sensitive to delay\n",
      "However the common assumption has been that all jobs are equally important or valuable\n",
      "Our work departs from this assumption In this context\n",
      "For continuous collection of crowd-sourced video\n",
      "But existing approaches either make strong assumptions about the structure of the data\n",
      "or gather new data through online algorithms that are likely to play severely suboptimal strategies\n",
      "to learning the parameters of the behavioral model of a bounded rational attacker ( thereby pinpointing a near optimal strategy )\n",
      "how much should we trust this reconstruction ?\n",
      "Equally importantly\n",
      "For differential dynamic logic ( dL ) for differential dynamic logic to internalize differential invariants\n",
      "Unmatched - striped_query_text:  differential substitutions and derivations as first-class axioms in dL. This paper introduces a new proof calculus that is entirely based on uniform substitution\n",
      "Unmatched - striped_query_text:  For single-view reasoning about 3D surfaces and their relationships. We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces\n",
      "We prove that any invariant algebraic set of a given polynomial vector field can be algebraically represented to efficiently automate the generation\n",
      "We address the problem how high-fidelity verification results about the hybrid systems dynamics of cyber-physical flow systems can be provided at the scale of large ( traffic ) networks without prohibitive analytic cost\n",
      "for traffic flow components\n",
      "For long-term two-photon imaging in awake macaque monkeys\n",
      "However prior work has only considered problems with a single constraint\n",
      "while many real-world problems involve multiple constraints for eliciting multiple constraints inherent in a problem and using those constraints to find inspirations useful in solving it To do so we identify methods to elicit useful constraints at different levels of abstraction\n",
      "and empirical results that identify how the level of abstraction influences creative idea generation\n",
      "In recent years automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design\n",
      "though suggests that recipients need to adapt the practice to fit their local environment\n",
      "undercut its effectiveness and harm the performance of the recipient unit Other evidence\n",
      "Organizational scholars disagree about how much a recipient unit should modify a best practice when incorporating it Some evidence indicates that modifying a practice that has been successful in one environment will introduce problems\n",
      "Can we predict the values of unseen coalitions ?\n",
      "To execute software long after its creation to encapsulate legacy software\n",
      "like retweets by botnet members\n",
      "while fraud- ulent behavior\n",
      "Our main intuition is that organic behavior has more variability\n",
      "We refer to the detection of such synchronized observations as the Syn- chonization Fraud problem\n",
      "Retweet Fraud Detection manifested in Twitter\n",
      "and we study a specific instance of it\n",
      "for detecting group fraud for characterizing retweet threads\n",
      "In this paper we consider a natural metric with this property\n",
      "There- fore it is preferred to assign smaller costs to the paths that stay close to the input points\n",
      "But why would an agent believe that the mechanism is truthful ?\n",
      "We are interested in settings where individuals are uncertain about their own preferences\n",
      "and represent their uncertainty as distributions over rankings\n",
      "augmenting associated well-studied security games with a configurable punishment parameter\n",
      "to account for multiple audit resources enabling application to practical auditing scenarios\n",
      "Recent work on audit games models the strategic interaction between an auditor with a single audit resource and auditees as a Stackelberg game\n",
      "system integration and integration of the analysis results for vari- ous models remains ad hoc to ensure consistency and enable system-level verification in a hierarchical and compositional manner\n",
      "Without a rigorous unifying framework\n",
      "a big graph mining system data processing platform\n",
      "While algorithms for computing an optimal strategy for the defender to commit to have had a striking real-world impact\n",
      "deployed applications require significant information about potential attackers\n",
      "We address this problem\n",
      "Traditional classification can lead to poor generalization and high misclassification given few and possibly biased labels We tackle this problem to handle new and multimodal fraud types\n",
      "Requiring special attention to imbue the autonomous driving with a more natural driving behavior\n",
      "to predict future scenarios to compute the cost for each scenario and select the decision corresponding to the lowest cost\n",
      "to enable an autonomous vehicle to perform cooperative social behavior to extract the probability of surrounding agents ' intentions in real time\n",
      "in this work we answer the following questions : ( 1 ) How to detect the base behavior of users regarding a product 's evaluation over time ?\n",
      "Given a time-series of rating distributions\n",
      "However if two crossmatches could be performed per patient\n",
      "in principle significantly more successful exchanges could take place\n",
      "In this paper we ask : If we were allowed to perform two crossmatches per patient\n",
      "could we harness this additional power optimally and efficiently ? for this problem\n",
      "For dynamic barter marketsand kidney exchange\n",
      "which existing automatic modeling methods are incapable of handling\n",
      "and transmission towers )\n",
      "planes and joints to recover their topology\n",
      "truss bridges steel frame buildings under construction\n",
      "to automatically recognize and model beams\n",
      "Many types of infrastructure are primarily comprised of arbitrarily-shaped thin structures ( e\n",
      "Given a large cloud of multi-dimensional points\n",
      "and an off-the shelf outlier detection method\n",
      "why does it take a week to finish ? to eliminate the problem\n",
      "In this paper we introduce the problem of detecting frauds in CSS and\n",
      "dynamically evolve over time ?\n",
      "How do social groups\n",
      "such as Facebook groups and Wechat groups\n",
      "we define a model of human computation and a measure of security\n",
      "To answer this question\n",
      "Most of the literature focuses on computing concise representations to approximate the Pareto curve or on exploiting evolutionary approaches to generate approximately Pareto efficient samples of the curve for game-theoretic solution concepts that incorporate Pareto efficiency\n",
      "However finding or even approximating ( when the objective functions are not convex ) the Pareto curve is hard\n",
      "Can we forecast the volume of user activity for the coming month ?\n",
      "This paper is an empirical study of the effectiveness and usefulness of tiles and flashcards developed for Microsoft Kodu Game Lab to support students in learning how to program and develop games\n",
      "In practice existing solutions also do not scale with the size of the graph which estimates core numbers of nodes\n",
      "We address the problem of estimating core numbers of nodes by reading edges of a large graph stored in external memory Existing external memory solutions do not give bounds on the required space\n",
      "\\log\\log n ) ^4 $ )\n",
      "Data entries with incomplete values are ignored\n",
      "making some analytic queries fail to accurately describe how an organization is performing to choose a correct value after viewing possible values and why they were inferred\n",
      "these algorithms find an approximation with fewer rows\n",
      "Given a n * d matrix where n g d\n",
      "In practice the best performances are often obtained by invoking these routines in an iterative fashion to give theoretical guarantees comparable to and better than the current state of the art\n",
      "allowing one to solve a poly ( d ) sized problem instead\n",
      "Hence industrial loads such as the steel plants have both the motivation and the ability to support power system operation through demand response to maximize its profits\n",
      "In this paper we ask whether it is possible that supports fluid\n",
      "interactive user experience even over mobile networks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this paper we show that such a model of computation arises naturally from session-based communication to express the protocols of message exchange and to reason about concurrency and state\n",
      "We consider the mechanism design problem for agents with single-peaked preferences over multi-dimensional domains when multiple alternatives can be chosen derive worst-case approximation ratios for social cost and maximum load for optimizing the choice of percentiles relative to any prior distribution over preferences\n",
      "and achieve a meaningful segmentation ?\n",
      "How can we statistically summarize all the sequences\n",
      "This paper is motivated by major needs for fast and accurate on-line data analysis tools in the emerging electric energy systems topology estimation approach for the smart grid\n",
      "However deploying mobile crowdsensing applications in large-scale environments is not a trivial task\n",
      "In this paper we try to reveal the barriers hampering the scale-up of mobile crowdsensing applications\n",
      "It creates a tremendous burden on application developers as well as mobile users\n",
      "For this goal in the context of social learning\n",
      "To expedite the cycle of data leading to the analysis of student progress and the improvement of student support\n",
      "made by spammers hoping to turn a profit\n",
      "from legitimate Page Likes detects lockstep Page Like patterns on Facebook to find such suspicious lockstep behavior\n",
      "In this paper we focus on the social network Facebook and the problem of discerning ill-gotten Page Likes\n",
      "Our objective is to design coordination algorithms to minimize the cost of electricity consumption of the consumer collective while allowing the consumers to make their own consumption decisions based on their private consumption constraints and preferences\n",
      "In this paper we focus on demand side management in consumer collectives with community owned renewable energy generation and storage facilities for effective integration of renewable energy with the existing fossil fuel-based power supply system\n",
      "for detecting edge-attributed graph anomalies\n",
      "In this paper we aim to utilize exactly this information to discern suspicious from typical behavior in an unsupervised fashion\n",
      "lending well to the traditional scarcity of ground-truth labels in practical anomaly detection scenarios\n",
      "It interactively visualizes univariate and bivariate distributions for those invariants It summarizes the properties of the nodes that the user selects It efficiently visualizes the induced subgraph of a selected node and its neighbors\n",
      "like Facebook and WeChat ?\n",
      "What is the growth pattern of social networks\n",
      "Such a rich environment calls for novel analytic tools that can model the aforementioned types of interactions\n",
      "In particular we address the issue of what to do when there are multiple languages in the corpus\n",
      "For approximate maximum flow in undirected graphs with good separator structures\n",
      "For SNE finding in games with more than two agents\n",
      "Given a large dataset of users ' ratings of movies\n",
      "what is the best model to accurately predict which movies a person will like ?\n",
      "However available methods for detecting such dense blocks are not satisfactory in terms of speed\n",
      "accuracy or flexibility for finding dense blocks in tensors\n",
      "can we identify those nodes from which the infection started to spread ?\n",
      "Given a snapshot of a large graph\n",
      "in which an infection has been spreading for some time\n",
      "This work focuses on the task of finding latent vector representations of the words in a corpus\n",
      "But nowadays some of its tenets are challenged by internet environments\n",
      "which call for dynamic decision making under constantly changing preferences\n",
      "To predict accurately trust relationships of a target user even if he/she does not have much interaction information\n",
      "such as testing or simulation\n",
      "This paper advocates the use of formal verification techniques and in particulartheoremprovingfor hybridsoftware-intensivesystemsasawell-foundedcomplementaryapproachtothe classical aerospace verification and validation techniques\n",
      "Currently most virtual agents are designed for a single targeted popular culture\n",
      "we want to find a small number of patterns that succinctly summarize the dataset\n",
      "child low-cholesterol ) etc ? that provides a sequence of patterns\n",
      "say ( male adult\n",
      "* ) followed by ( *\n",
      "how can we find ( a ) that the `` most representative '' pattern is\n",
      "Given a table where rows correspond to records and columns correspond to attributes\n",
      "For example given a set of patient records with several attributes each\n",
      "How quickly does a piece of news spread over these media ?\n",
      "In this paper we uniformly integrate this computational interpretation in a functional language\n",
      "However this does not work in imperfect-information games because different endgames can contain states that belong to the same information set and can not be treated independently\n",
      "A grand challenge for state estimation in newly built smart grid lies in how to deal with the increasing uncertainties\n",
      "To solve the problem\n",
      "Recent proposals have advocated the use of consolidation of idle desktop Virtual Machines ( VMs )\n",
      "that transparently migrates only the working set of an idle VM\n",
      "However desktop VMs are often large\n",
      "Consolidating such VMs creates large network transfers lasting in the order of minutes and utilizes server memory inefficiently\n",
      "requiring gigabytes of memory\n",
      "networks become congested and the resulting migration latencies are prohibitive\n",
      "When multiple VMs migrate concurrently\n",
      "Our original design did not log necessary information nor did it induce users to provide good labels\n",
      "On reflection we realized UX designers should identify and refine UI adaptions when sketching wireframes\n",
      "Unmatched - striped_query_text:  To advance on this insight to communicate planned adaptation and note the information ( logs and labels ) needed to make the desired inferences. Extracted six design patterns where UI adaptation can improve in-app navigation\n",
      "can we reliably tell who the culprits are ?\n",
      "In other words\n",
      "To formally verify ACAS X\n",
      "a convex model that approximates a multi-dimensional signal via an approximately piecewise-constant signal\n",
      "To the group fused lasso\n",
      "Finding the best combination of cycles and chains is hard The leading algorithms for this optimization problem use either branch and pricea combination of branch and bound and column generationor constraint generation\n",
      "We show a correctness error in the leading prior branch-and-price-based approach [ Glorie et al\n",
      "2014 ] fix to it\n",
      "In fact we show that this approach can fail even in a simple game with a unique equilibrium and a single endgame\n",
      "to conduct endgame solving in a scalable way\n",
      "To enhance the search space of APR\n",
      "and provide a function to effectively traverse the search space\n",
      "Furthermore there is often a gap between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide directly for differential dynamic logic allows local reasoning to discover useful forward invariants to complete verification tasks\n",
      "Does it spread uniformly over countries ?\n",
      "each of which exists with some probability\n",
      "for these two problems\n",
      "This is a special case of stochastic k-set packing\n",
      "The stochastic matching problem deals with finding a maximum matching in a graph whose edges are unknown but can be accessed via queries\n",
      "where the problem is to find a maximum packing of sets\n",
      "Objective to generate a lesion severity score\n",
      "including how the market both reacted to and anticipated official news releases about the buildings opening day\n",
      "To examine issues of trader performance and market microstructure\n",
      "To parse human motion in unconstrained Internet videos without labeling any videos for training\n",
      "We consider the problem of fairly allocating indivisible goods\n",
      "focusing on a recently-introduced notion of fairness called maximin share guarantee : Each player 's value for his allocation should be at least as high as what he can guarantee by dividing the items into as many bundles as there are players and receiving his least desirable bundle\n",
      "and at the same time exploit information in semantic taxonomy among categories\n",
      "To effectively reduce dimensionality of parameter space without sacrificing classification accuracy\n",
      "Unmatched - striped_query_text:  Of optimal voting under adversarial noise. Show that our approach produces significantly more accurate rankings than alternative approaches\n",
      "to evaluate their security requirements methods against how experts transition through different situation awareness levels in their decision-making process\n",
      "To understand the gap between available checklists and practice\n",
      "We present an approach to utilize large amounts of web data for learning CNNs\n",
      "as predicted by textbook models like the Bass model\n",
      "Does it truly exhibit exponential early growth\n",
      "SI or the Branching Process ?\n",
      "to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic\n",
      "its behavior is guaranteed to satisfy the correctness properties verified with respect to the model\n",
      "Otherwise all bets are off\n",
      "ensuring that verification results about models apply to CPS implementations\n",
      "If the real system fits to the model\n",
      "The previously proposed semantics implements asynchronous ( non-blocking ) output ; we extend it here with non-blocking input\n",
      "In this work we take an adversarial approach to find and prove claims about the weaknesses of modern\n",
      "stealth attacks that slip below the radar\n",
      "However small-scale stealthy attacks may go unnoticed due to the nature of low-rank Eigen analysis used in practice\n",
      "state-of-the-art spectral methods to catch small-scale\n",
      "and only shipping extracted index information and meta-data to the cloud\n",
      "to human-in-the-loop content-based retrospective search\n",
      "Scalability is improved by performing video analytics on cloudlets at the edge of the Internet\n",
      "Hi5 Friendster and Multiply to guide the design of a new popularity competition model\n",
      "In this work we exploit the singular way in which Facebook wiped out the popularity of MySpace\n",
      "Unmatched - striped_query_text:  including those mentioned above. Updates by our algorithms are up to a million times faster than the fastest batch algorithms Effective : our DENSESALERT successfully spots anomalies especially those overlooked by existing algorithms\n",
      "However existing algorithms assume that tensors are static\n",
      "Thus several algorithms have been proposed for detecting dense subtensors rapidly and accurately\n",
      "while many real-world tensors\n",
      "heavy-tail distribution of the parameters\n",
      "Is there any distinct pattern in Alice 's ( or other users ' ) search behavior ? to describe such an IAT distribution to capture and explain the two-dimensional\n",
      "Although it is known that probabilistic forecasts ( which give a distribution over possible future outcomes ) can improve planning and control\n",
      "as it is challenging to represent high-dimensional non-Gaussian distributions over multiple spatial and temporal points\n",
      "many forecasting systems in practice are just used as point forecast tools\n",
      "In this paper we focus on the node classification problem on networks\n",
      "For monitoring virtual machines ( VMs ) in the cloud\n",
      "For the combined dynamics of differential hybrid games It shows how hybrid games subsume differential games for proving properties of differential games inductively\n",
      "Clinical interventions can be made at this stage\n",
      "however it is essential that the interventions take place before the patient 's health declines too drastically\n",
      "allowing us to predict weight values into the future\n",
      "leading to better outcomes\n",
      "In social voting Web sites\n",
      "down-votes and comments evolve over time ?\n",
      "how do the user actions up-votes\n",
      "For the task of pose estimation\n",
      "first-order or regret-based methods are usually preferred for large games\n",
      "We study the problem of computing a Nash equilibrium in large-scale two-player zero-sum extensive-form games While this problem can be solved in polynomial time\n",
      "In this paper we draw from the literature and our own prior Work to identify a number of problems that hinder engagement with achievement-based personal informatics systems-problems related to inadequate support for goal setting\n",
      "for mitigating these problems\n",
      "and the burden of system maintenance\n",
      "misalignment of user and system goals\n",
      "To improve the automatic detection of events in short sentences when in the presence of a large number of event classes\n",
      "for resource allocation with an additional punishment parameter\n",
      "focusing in particular on effective resource allocation and appropriate punishment schemes\n",
      "We study economic considerations in the design of these mechanisms\n",
      "A modification to this approach for performing endgame solving in large imperfect-information games\n",
      "for evaluating the performance of an agent that uses endgame solving\n",
      "in order to reduce the demanding computation to develop an effective and efficient solution to the multimodal data mining problem\n",
      "In this article we exploit the relations among different modalities in a multimedia database and for general multimodal data mining problem In addition\n",
      "However it does not have a closed-form nor does it provide convergence guarantees in general\n",
      "to perform fast BP on undirected heterogeneous graphs\n",
      "BP traditionally an inference algorithm for graphical models\n",
      "exploits so-called `` network effects '' to perform graph classification tasks when labels for a subset of nodes are provided ; and it has been successful in numerous settings like fraudulent entity detection in online retailers and classification in social networks\n",
      "evolve over time we represent the behavioral data for behavioral summary to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way\n",
      "such as tweets or papers\n",
      "However the human behaviors are multicontextual and dynamic : ( 1 ) each behavior takes place within multiple contexts in a few dimensions\n",
      "which requires the representation to enable non-value and set-values for each dimension ; ( 2 ) many behavior collections\n",
      "for large incomplete-information games\n",
      "We begin by proving that regrets on actions in one setting ( game ) can be transferred to warm start the regrets for solving a different setting with same structure but different payoffs that can be written as a function of parameters\n",
      "Does it form spikes over time ?\n",
      "These applications have in common the prediction of user trajectories that are in a constant state of flux over a hidden network ( e\n",
      "website links geographic location )\n",
      "Moreover what users are doing now may be unrelated to what they will be doing in an hour from now to cope with the complex challenges of learning personalized predictive models of non-stationary\n",
      "transient and time-heterogeneous user trajectories\n",
      "Yet verification results also need to transfer from components to composites\n",
      "We study a component-based approach to simplify the challenges of verifying large-scale hybrid systems\n",
      "to define the structure and behavior of components how to compose components\n",
      "And how can we prevent spammers from tricking our algorithms into suggesting a bad movie ?\n",
      "How does its popularity diminish over time ?\n",
      "We propose to extract concepts that describe groups of objects and their common properties from the integrated data\n",
      "For learning our model\n",
      "it has become relatively easy for CS educators to fabricate physical artifacts to help students explore computational ideas\n",
      "Thanks to the growing availability of rapid prototyping tools\n",
      "For solving a linear system arising from the 1-Laplacian corresponding to a collapsible simplicial complex with a known collapsing sequence\n",
      "In this context one possible objective for a social choice function is the maximization of ( expected ) social welfare relative to the information contained in these rankings We study such optimal social choice functions and underscore the important role played by scoring functions\n",
      "assuming that agents have ( possibly latent ) utility functions over some space of alternatives\n",
      "We adopt a utilitarian perspective on social choice\n",
      "such as software patches ?\n",
      "Does it resemble the propagation pattern of benign files\n",
      "In this paper we investigate 3D attributes as a means to understand the shape of an object in a single image\n",
      "between agents in co-operative games\n",
      "We investigate synergy or lack thereof\n",
      "building on the popular notion of Shapley value\n",
      "What other patterns emerge ?\n",
      "under the flow of polynomial ordinary differential equations\n",
      "This paper studies sound proof rules for checking positive invariance of algebraic and semi-algebraic sets\n",
      "that is sets satisfying polynomial equalities and those satisfying finite boolean combinations of polynomial equalities and inequalities\n",
      "To address such questions\n",
      "or hard-to-match patients Toward this end\n",
      "In this paper we focus on improving access to kidneys for highly-sensitized\n",
      "How do we find patterns and anomalies in very large graphs with billions of nodes and edges ?\n",
      "It is well known that strategic behavior in elections is essentially unavoidable ; we therefore ask : how bad can the rational outcome be ?\n",
      "In this study we assess the ability of laypersons\n",
      "technical professionals and legal experts to judge the similarity between legal coverage conditions and requirements\n",
      "The complexity of these judgments\n",
      "coupled with the time and effort required to meticulously assess video\n",
      "results in a training and evaluation process that can take days or weeks\n",
      "Computational analysis of video data is still limited due to the challenges introduced by objective interpretation and varied contexts\n",
      "Here we answer these questions which generate networks that mimic our discovered patterns\n",
      "they may have an incentive to withhold some of their incompatible donorpatient pairs and match them internally\n",
      "Assuming that hospitals wish to maximize the number of their own patients who receive a kidney\n",
      "for hospitals to report all their incompatible pairs\n",
      "thus harming social welfare\n",
      "We answer this question\n",
      "However binarizing features represented by real numbers has a problem in that a great deal of the information within the features drops out\n",
      "which is information that drops out when features are binarized in order to take into consideration the possibility that a binary code which has been observed from an image will transition to another binary code\n",
      "That is why we focus on quantization residual\n",
      "in full agreement with earlier results that real world graphs have no good cuts Instead\n",
      "we propose to envision graphs as a collection of hubs connecting spokes\n",
      "and so on recursively\n",
      "We show that the block-diagonal mental image of the cavemen graph is the wrong paradigm\n",
      "with super-hubs connecting the hubs\n",
      "However in many settings -- such as the FCC 's imminent incentive auction -- each bidder may be able to sell one from a set of options\n",
      "for the dynamics of each bidder 's state to optimize the trajectory of price offers to different bidders for different options\n",
      "in which the decision maker faces a budget constraint\n",
      "which is evident for renewable technologies\n",
      "While submodularity is natural in many domains\n",
      "early stages of innovation adoption are often better characterized by convexity\n",
      "such as rooftop solar to scale over a finite time horizon\n",
      "Designing optimalthat is revenue-maximizingcombinatorial auctions ( CAs ) is an important elusive problem\n",
      "making hybrid modeling more practical\n",
      "These issues can be addressed by component-based engineering\n",
      "This paper lays the foundation for using to provide component-based benefits to developing hybrid programs\n",
      "To answer this question\n",
      "To capture the relevant rotational and translation invariances in geometric data\n",
      "Our goal is to better understand the dynamic setting for dynamic resource allocation mechanisms\n",
      "What is the difference between the two types of relationships on Twitter : who-tags-whom and who-follows-whom ?\n",
      "that capture this loss\n",
      "both theoretically and empirically\n",
      "in real-life scenarios ( such as protection of the port of Boston ) this is not the case\n",
      "Our goal is to quantify the loss incurred by miscoordination between defenders\n",
      "We study security games with multiple defenders However\n",
      "We argue that this is too stringent a requirement\n",
      "and instead ask : How many votes does a voting rule need to reconstruct the true ranking ?\n",
      "We study the phase transition of the coalitional manipulation problem for generalized scoring rules\n",
      "For computing the Voronoi diagram of a set of $ $ n $ $ n points in constant-dimensional Euclidean space\n",
      "This paper assesses the potential cost-saving incentives for content distribution networks to shift traffic load among geographically distributed data centers in response to hourly variation in electricity prices\n",
      "In this paper we study steering such adaptation through sequential planning to compute a treatment plan\n",
      "However all such methods have low accuracy\n",
      "which is not true in many real-world applications such as social media and web\n",
      "or assume that tensors are small enough to fit in main memory\n",
      "To overcome these limitations\n",
      "We study voting rules that output a correct ranking of alternatives by quality from a large collection of noisy input rankings We seek voting rules that are supremely robust to noise\n",
      "in the sense of being correct in the face of any `` reasonable '' type of noise\n",
      "dynamically evolve over time ?\n",
      "How do social groups\n",
      "such as Facebook groups and Wechat groups\n",
      "How do people join the social groups\n",
      "uniformly or with burst ?\n",
      "we treat semantic correspondence as a constrained detection problem\n",
      "where an exemplar LDA classifier is learned for each pixel\n",
      "Motivated by object recognition literature and recent work on rapidly estimating linear classifiers\n",
      "Unmatched - striped_query_text:  In this paper we discuss the algorithms used. We find that affect-and pose-based segmentation are more effective\n",
      "In an effort to advance this research agenda\n",
      "However researchers have recently been exploring ways to use multimodal computational analysis in the service of studying how people learn in complex learning environments\n",
      "lie on an ( n - 1 ) -dimensional hyperplane\n",
      "Unmatched - striped_query_text:  given that the problem of enumerating all pure-strategy SNEs is trivially in P. Our central result is that\n",
      "in the case of 2 agents\n",
      "and in the case of n agents\n",
      "lie on the same line\n",
      "the agents ' payoffs restricted to the agents ' supports must\n",
      "in order for a game to have at least one non-pure-strategy SNE\n",
      "Leveraging this result we provide two contributions\n",
      "We investigate a notion of behavioral genericity in the context of session type disciplines\n",
      "to discover communities across facets\n",
      "Mining multi-faceted graphs have several applications\n",
      "tracking IP addresses of botnets over time\n",
      "including finding fraudster rings that launch advertising traffic fraud attacks\n",
      "analyzing interactions on social networks and co-authorship of scientific papers that does soft clustering on individual facets\n",
      "Illumination defocus limits the working volume of projector-camera systems and global illumination can induce large errors in shape estimates\n",
      "Reducing the risks associated with an AHE requires effective and efficient mining of data generated from multiple physiological time series\n",
      "to effectively predict AHE\n",
      "However current technology typically limits conversational interactions to a few narrow predefined domains/topics\n",
      "For example dialogue systems for smart-phone operation fail to respond when users ask for functions not supported by currently installed applications\n",
      "who competes with whom ?\n",
      "and specifically to answer the following questions : ( a ) Is there any sign of interaction/competition between two different keywords If so\n",
      "Our goal is to analyze a large collection of multi-evolving activities\n",
      "allowing for a weaker similarity among components\n",
      "PLUS models the components as independent or identical\n",
      "In this paper we extend that formulation\n",
      "Unmatched - striped_query_text:  over time for which there are few published models ?. ; and we observe power law growth for both nodes and links\n",
      "How about the count of links\n",
      "Which enables CFR to scale to dramatically larger abstractions and numbers of cores\n",
      "for generating such abstractions\n",
      "But are there regularities obeyed by the r-hop neighborhood in real networks ?\n",
      "An abstraction algorithm that takes all future rounds into account is called potential aware\n",
      "However one might benefit by considering the trajectory of distributions over strength in all future rounds\n",
      "not just the final round\n",
      "for computing potential-aware imperfect-recall abstractions\n",
      "We consider the task of designing sparse control laws for large-scale systems by directly minimizing an infinite horizon quadratic cost with an $ \\ell_1 $ penalty on the feedback controller gains that allows us to scale to large systems ( i\n",
      "those where sparsity is most useful )\n",
      "In this paper we explore the possibility of learning some basic music performance skills from rehearsal data\n",
      "In particular we consider the piano duet scenario where two musicians expressively interact with each other\n",
      "Our work extends previous automatic accompaniment systems\n",
      "As a consequence the results of human-computer interaction are often far less musical than the interaction between human musicians\n",
      "in many applications it has been observed that objects might belong to multiple clusters with different degrees\n",
      "to tackle the challenge of mixed membership clustering for vector data For learning our model\n",
      "While traditional clustering techniques assign each object to a single cluster only\n",
      "Control decisions are made\n",
      "Since the inherent complexities of CPS practically mandate iterative development\n",
      "To overcome this issue\n",
      "frequent changes of models are standard practice\n",
      "but require reverification of the resulting models after every change\n",
      "including 1 ) they occur in short bursts of time ; 2 ) fraudulent user accounts have skewed rating distributions\n",
      "for detecting fraudulent reviews\n",
      "Fake reviews are often detected based on several signs\n",
      "However these may both be true in any given dataset\n",
      "For solving symmetric diagonally dominant ( SDD ) linear systems\n",
      "Abstract : The multidisciplinary goal was to develop an integrated conceptualization of the mid-level encoding of 3D object structure from multiple surface cues\n",
      "The unprecedented dips of performance reduction in the component psychometric functions was captured\n",
      "A great deal of empirical work has demonstrated that redundancy can significantly reduce response time in systems ranging from Google 's BigTable service to kidney transplant waitlists\n",
      "To opportunistic near real-time search of untagged images on smartphones\n",
      "Now that we know when to best deliver information\n",
      "it raises the question : what information should we deliver at those interruptible moments ?\n",
      "Moreover despite the remarkable progress in automating formal verification of hybrid systems\n",
      "( 2 ) exchanging and comparing models and proofs\n",
      "and ( 3 ) managing verification tasks to tackle large-scale verification tasks\n",
      "tools for ( 1 ) graphical ( UML ) and textual modeling of hybrid systems\n",
      "It is thus not uncommon for development and verification teams to consist of many players with diverse expertise\n",
      "since hybrid systems verification tools solve undecidable problems\n",
      "the construction of proofs of complex systems often requires nontrivial human guidance\n",
      "it is difficult to support at global scale across cloudlets in multiple domains To address this problem\n",
      "Since cloud offload relies on precisely-configured back-end software\n",
      "An improvement that prunes any path of play in the tree\n",
      "and its descendants that has negative regret\n",
      "Whether model inversion attacks apply to settings outside theirs\n",
      "To answer the above question\n",
      "To address the ubiquity of error\n",
      "outputs a value for each feature corresponding to its influence in determining the classification outcome\n",
      "given a set of classified points\n",
      "In order to uniquely characterize an influence measure : a function that\n",
      "confluence and behavioral equality in the realm of session-based concurrency\n",
      "We investigate strong normalization\n",
      "Strong normalization and confluence are established for session-typed processes\n",
      "We study efficiency and budget balance for designing mechanisms in general quasi-linear domains\n",
      "Instead the TS field can be directly recovered\n",
      "without having to reconstruct instantaneous fluid flow fields\n",
      "We show that it is possible to estimate TS\n",
      "We focus on three topics : ( a ) anomaly detection in large static graphs ( b ) patterns and anomalies in large time-evolving graphs and ( c ) cascades and immunization\n",
      "Csound has a large library of unit generators and could be a useful source of reusable unit generators for other languages or for direct use in applications\n",
      "What can a human compute in his/her head that a powerful adversary can not infer ?\n",
      "how can we automatically spot anomalous\n",
      "Given a directed graph of millions of nodes\n",
      "suspicious nodes judging only from their connectivity patterns ?\n",
      "what is the best way to recover historical counts from aggregated\n",
      "In general\n",
      "in the presence of missing values ?\n",
      "possibly overlapping historical reports\n",
      "to detect users who offer the lockstep behavior\n",
      "In this paper we study a complete graph from a large who-follows-whom network and spot lockstep behavior that large groups of followers connect to the same groups of followees\n",
      "A well-spaced superset of points conforming to a given input set on the output points\n",
      "We study the problem of building a sensor model for the purpose of simulation our aim to reproduce the signal in its entirety\n",
      "including its error properties\n",
      "This paper presents the characterization and comparison of physiological tremor for pointing tasks in multiple environments\n",
      "as a baseline for performance evaluation of microsurgical robotics\n",
      "A way of preventing automated offline dictionary attacks against user selected passwords\n",
      "To enable doing this in a formal manner for multi-model verification of cyber-physical systems\n",
      "Therefore model-based design of CPS must make use of a collection of models in several different formalisms and use respective analysis methods and tools together to ensure correct system design\n",
      "In this paper we address the problem of cycles and chains in a proposed match failing after the matching algorithm has committed to them\n",
      "for the probabilistic exchange clearing problem\n",
      "( 1 ) what is the role of communication in the music performance ?\n",
      "In order to get our result\n",
      "in mixed strategies for finite games\n",
      "In this paper we consider strong Nash equilibria\n",
      "what are the interactions between ( groups of ) neurons ( also known as functional connectivity ) and how can we automatically infer those interactions\n",
      "More specifically given the stimulus\n",
      "given measurements of the brain activity ?\n",
      "For solving the geometric median problem given n points in d compute a point that minimizes the sum of Euclidean distances to the points In this paper we show how to compute such an approximate geometric median in time O ( nd log 3 n / ) and O ( d 2 )\n",
      "Under which the advice given by ACAS X is safe formally verify these configurations\n",
      "to relate patterns based on decisions made by the pattern user\n",
      "The challenge is that identifying the most appropriate pattern for a situation can be cumbersome and time-consuming\n",
      "to review only relevant patterns and quickly select the most appropriate patterns for the situation\n",
      "to help the pattern user select the most appropriate patterns for their situation\n",
      "What is the pattern of people quitting from groups ?\n",
      "to be geometrically `` consistent { '' }\n",
      "In this paper we investigate the conditions for multiple silhouettes\n",
      "or more generally arbitrary closed image sets\n",
      "Does it form spikes over time ?\n",
      "We show in this paper that the answer is no\n",
      "To quantify sharpness of tuning\n",
      "We revisit the classic problem of estimating the population mean of an unknown single-dimensional distribution from samples Our key question is whether the sample median is the best ( in terms of mean squared error ) truthful estimator of the population mean\n",
      "To analyze a control algorithm designed to provide directional force feedback for a surgical robot\n",
      "that provides safe operation along with directional force feedback\n",
      "That learns the optimal parameters of the neural population model\n",
      "Unmatched - striped_query_text:  up to a difference of at most e. We establish a lower bound of ( ln ( 1/e ) /lnln ( 1/e ) ) on the complexity of this problem\n",
      "We are interested in the problem of dividing a cake -- a heterogeneous divisible good -- among n players\n",
      "in a way that is e- equitable : every pair of players must have the same value for their own allocated pieces\n",
      "In this work we are trying to answer the above questions studying the expansion properties of large social graphs\n",
      "Unmatched - striped_query_text:  to more accurately detect groups of fraudulent users. Showed that HoloScope achieved significant accuracy improvements on synthetic and real data\n",
      "Existing approaches such as average degree maximization suffer from the bias of including more nodes than necessary\n",
      "resulting in lower accuracy and increased need for manual verification\n",
      "However review systems are often targeted by opinion spammers who seek to distort the perceived quality of a product by creating fraudulent reviews\n",
      "for spotting fraudsters and fake reviews in online review datasets\n",
      "For a component-based modeling and verification approach for hybrid systems\n",
      "It automatically extracts graph invariants\n",
      "To achieve these goals solving our objective\n",
      "However agents may also care about the pieces assigned to other agents ; such externalities naturally arise in fair division settings\n",
      "to capture externalities and generalize the classical fairness notions of proportionality and envyfreeness\n",
      "How to mine such big graphs efficiently ?\n",
      "What are the common patterns and the anomalies ?\n",
      "we are after that set of seed nodes that best explain the given snapshot\n",
      "In this paper\n",
      "we answer this question affirmatively Essentially\n",
      "to identify the best set of seed nodes and virus propagation ripple to identify likely sets of seed nodes\n",
      "For large graphs though\n",
      "and drawing hundred-thousand nodes results in cluttered images hard to comprehend\n",
      "To cope with these problems\n",
      "there are some challenges : the excessive processing requirements are prohibitive\n",
      "This paper presents a method for generating semi-algebraic invariants for systems governed by non-linear polynomial ordinary differential equations under semi-algebraic evolution constraints\n",
      "We study the envy-free allocation of indivisible goods between two players To rigorously quantify the efficiency gain from selling\n",
      "Given a large edge-labeled network\n",
      "a time-evolving network how can we find interesting patterns ? which can discover communities appearing over subsets of the labels\n",
      "We argue that doing well on this task requires the model to learn to recognize objects and their parts\n",
      "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation\n",
      "The game played by hospitals participating in pairwise kidney exchange programs\n",
      "We investigate the problem of optimal monitoring of targeted stealthy diffusion processes\n",
      "and for the setting in which an attacker optimally responds to the placement of monitoring nodes\n",
      "To create a new type of crowd-sourced system\n",
      "In this paper we analyze the causes for the information diffusion without explicit relationships in the blogosphere\n",
      "For automatic 3D steering of manually inserted flexible needles\n",
      "Are there relationships between votes and comments ?\n",
      "how can we find the daily counts of people infected with flu ?\n",
      "In this paper we address the challenge of recovering a time sequence of counts from aggregated historical data\n",
      "For example given a mixture of the monthly and weekly sums\n",
      "In this paper we investigated empirically how statistical regularities in natural 3D scenes are represented in the functional connectivity of disparity-tuned neurons in the primary visual cortex of primates\n",
      "to learn from 3D natural scenes\n",
      "In this paper we study how humans can develop an understanding of swarm dynamics so they can predict the effects of the timing of their input on the state and performance of the swarm allowing comparison between human and optimal input timing performance in control of swarms\n",
      "In this paper we discuss our prior and future work with elementary school students that aims to investigate how to best combine individual and collaborative learning using their complementary strengths within an ITS\n",
      "To support axiomatic theorem proving\n",
      "with such an explicit representation of proofs\n",
      "Unfortunately these theorem provers do not have explicit proof terms\n",
      "which makes the implementation of a number of important features unnecessarily complicated without soundness-critical and extra-logical extensions to the theorem prover\n",
      "In this paper we investigate the issue of evaluating efficiently a large set of models on an input image in detection and classification tasks\n",
      "To coherently reason about occlusions on many types of detectors\n",
      "However there is no exact analysis of systems with redundancy\n",
      "Despite their growing prominence\n",
      "optimization in generative adversarial networks ( GANs ) is still a poorly-understood topic which is able to guarantee local stability for both the WGAN and for the traditional GAN\n",
      "and also shows practical promise in speeding up convergence and addressing mode collapse\n",
      "and only weak algorithm-specific guarantees on solution quality are known\n",
      "However imperfect-recall abstractions are poorly understood\n",
      "This paper introduces reasoning about lawful behavior as an important computational thinking skill and provides examples from a novel introductory programming curriculum using Microsoft 's Kodu Game Lab\n",
      "or are they segregated into multiple groups ?\n",
      "Are they all close-by in the graph\n",
      "Existing studies in social networks have mainly focused on the information diffusion through explicit relationships between members\n",
      "In this work we address the problem of identifying the locations visited by a mobile device as it moves within an indoor environment\n",
      "and ( 2 ) how are the effects of communication latency ameliorated or incorporated into the performance ?\n",
      "In particular\n",
      "what are the esthetic and pragmatic justifications for performing music at a distance\n",
      "We study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems\n",
      "in particular learning the hierarchical semantic structure from the data\n",
      "thus generating false information\n",
      "While the latter can be realized through continuous tracking of a user 's whereabouts from the service provider\n",
      "However these incentives can also lead to people faking their location\n",
      "the majority of LBSNs allow users to voluntarily share their location\n",
      "LBSNs provide incentives to users to perform check-ins\n",
      "for spotting anomalies in the check-in behavior of users\n",
      "In this setting we typically want to determine the relative importance of a node ( or a set of nodes ) within the graph\n",
      "for example how valuable ( as a bridge ) a person or a group of persons is in a social network\n",
      "In this paper we consider the problem of predicting the weights of edges in such networks\n",
      "But unfortunately transforming these candidate facts into useful knowledge is a formidable challenge\n",
      "In this paper we show how uncertain extractions about entities and their relations\n",
      "A related harder problem is : how can we assign a numerical score to each lesson plan i\n",
      "way of conveying information ?\n",
      "To rearrange cluttered environments\n",
      "Naively combining the worst and best case execution costs of the two programs does not work well in many cases because such analysis forgets the similarities between the programs or the inputs\n",
      "But methods for analyzing how the execution costs of two programs compare to each other have not received attention\n",
      "that is capable of establishing precise bounds on the difference in the execution cost of two programs for a higher-order functional language with recursion and subtyping\n",
      "In this work we formally state the axioms and desired properties of the graph similarity functions\n",
      "that assesses the similarity between two graphs on the same nodes that enables attribution of change or dissimilarity to responsible nodes and edges\n",
      "While fielded kidney exchanges see huge benefit from altruistic kidney donors ( who give an organ without a paired needy candidate )\n",
      "a significantly higher medical risk to the donor deters similar altruism with livers\n",
      "both methods are too time-consuming to be used in a large image database\n",
      "To solve the problem\n",
      "Although providing good retrieval results\n",
      "In addition to addressing these two concerns\n",
      "for large scale epidemiological data which solves the above problem\n",
      "Offers provably fair solutions for the division of rent\n",
      "to support identifying and saving information in an intentionally uncertain way on mobile devices\n",
      "We introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration\n",
      "In this paper we present the syntax and operational semantics of our language and\n",
      "For programming graph- based algorithms in a declarative fashion\n",
      "for addressing the non-stationarity challenge of TD social networks\n",
      "This is due to the non-stationary ranking of shortest paths ( the underlying structure of betweenness and closeness ) between a pair of nodes which violates the assumptions of classical dynamic programming based techniques\n",
      "However scalable computation of these metrics for long time-intervals is challenging\n",
      "Several recent works propose methods on clus- tering and indexing trajectories data\n",
      "However these approaches are not especially well suited to pattern discovery with respect to the dynamics of social and economic behavior To further analyze a huge collection of taxi trajectories\n",
      "to find meaningful patterns and anomalies\n",
      "We show how a disruptive force in mobile computing can be created\n",
      "Do their strategic outcomes still guarantee fairness ?\n",
      "new techniques and practical\n",
      "and a better understanding of the computer as music performer\n",
      "it is surprising that there is not more technology for\n",
      "Our goal is to enable musicians to ncorporate computers into performances easily and effectively through a better understanding of requirements\n",
      "In spite of decades of work\n",
      "and do not refine the generative models or the feature mapping functions based on classification results\n",
      "However existing approaches typically feed features derived from generative models to discriminative classifiers\n",
      "to improve the classifier 's performance\n",
      "so that we can guide the dissemination process in a desired way for edge deletion and edge addition\n",
      "edge deletion and edge addition ) to optimize the leading eigenvalue of the underlying graph\n",
      "In this paper we study the problem of how to optimally place a set of edges ( e\n",
      "Domain adaptation methods attempt to address this problem\n",
      "but usually assume that the source domain is specified a priori for situations where more than one source domain available to choose the source domain most similar to the target domain to further adapt the chosen source domain to the target data\n",
      "but also for interoperability for reading and manipulating digital slides of diverse vendor formats\n",
      "Since no universal data format is in widespread use for these images today\n",
      "each vendor defines its own proprietary data formats\n",
      "This creates issues not only for pathologists\n",
      "analysis tools viewers and software libraries\n",
      "We answer this question\n",
      "We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems\n",
      "Our work shows a close connection between these problems and core questions in algorithmic graph theory\n",
      "Here we give a formal definition of this problem of forming learning units\n",
      "To investigate program understanding in young children\n",
      "to spot dense blocks that are worth inspecting\n",
      "No method gives a principled way to score the suspiciousness of dense blocks with different numbers of modes and rank them to draw human attention accordingly\n",
      "though the payoffs of potential attackers for various outcomes must be estimated ; inaccurate estimates can lead to significant inefficiencies that optimizes the defender 's strategy with no prior information\n",
      "In order to build the game model\n",
      "This is usually handled by heuristically discretizing the continuous action space without solution quality bounds\n",
      "In contrast many real-world domains require modeling with continuous action spaces\n",
      "However most solution algorithms require discrete\n",
      "for assessing students ability to mentally simulate and predict code behavior\n",
      "Assessing students ' ability to mentally simulate program execution can be challenging in graphical programming environments and on paper-based assessments\n",
      "that provably finds an MMS allocation with high probability\n",
      "Our goal is to understand when we can expect to be able to give each player his MMS guarantee\n",
      "for co-evolving time sequences\n",
      "we give a formal definition of this problem of forming learning units and for comparing different approaches for this problem\n",
      "Here\n",
      "where idle servers are turned off to save cost\n",
      "We consider the M/G/k/staggeredM/G/k/staggered-setup\n",
      "necessitating a setup time for turning a server back on ; however\n",
      "at most one server may be in setup mode at any time\n",
      "which is designed to carry heavy loads to locations that are too difficult to reach with a wheeled or tracked vehicle to allow a particular user to designate himself as the robot 's leader\n",
      "and guide the robot along a desired path\n",
      "We consider a walking logistics support robot\n",
      "that satisfies these desiderata\n",
      "Prior action translation mappings have been based on heuristics without theoretical justification\n",
      "We show that the prior mappings are highly exploitable and that most of them violate certain natural desiderata\n",
      "While voting is often used for this purpose\n",
      "the choice of voting method is typically not principled to better understand how different voting rules perform in practice\n",
      "To low-rank structured semidefinite programming\n",
      "to quantify both concepts ( `` synchronicity '' and `` normality '' )\n",
      "to provide itemized energy usage\n",
      "For single-channel source separation to estimate the correlations between these features and the unobserved signal decomposition\n",
      "What do real communities in social networks look like ? as a better representation of communities and the relationships between their members\n",
      "to detect communities with hyperbolic structure\n",
      "`` patent-cites-patent '' `` user-likes-page '' and `` actor/director-makes-movie '' networks\n",
      "how can we find unexpected behaviors ? to detect users who offer the lockstep behaviors in undirected/directed/bipartite graphs\n",
      "Given multimillion-node graphs such as `` who-follows-whom ''\n",
      "In this paper we will discuss and analyze the varieties of representational schemes of these internal models and how they might be used to perform learning and inference for relating the internal models to the observed neural phenomena and mechanisms in the visual cortex\n",
      "We argue that linear detection functions should be preferred in this regard due to their scalability and efficiency during estimation and evaluation\n",
      "In this paper we tackle the problem of efficient video event detection\n",
      "To study this problem\n",
      "we investigated an approach to automatically construct an information type ontology by identifying information type hyponymy in privacy policies using Tregex patterns\n",
      "Therefore it is important to find a suitable forecasting method that covers these special characteristics\n",
      "That allows a jammer and sender to choose ( 1 ) whether to transmit or sleep\n",
      "and ( 3 ) what channel to transmit on\n",
      "to choose on how many channels it simultaneously attacks\n",
      "( 2 ) a power level to transmit with\n",
      "Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts\n",
      "We study the performance of linear solvers for graph Laplacians based on the combinatorial cycle adjustment methodology proposed by [ Kelner-Orecchia-Sidford-Zhu STOC-13 ]\n",
      "for handling these adjustments\n",
      "How can we use them for algorithm design and applications ?\n",
      "That looks for clusters in subspaces of multidimensional data\n",
      "if any groups they form as well as find simple paths that connect the nodes in each group ?\n",
      "How can we automatically determine how many\n",
      "to automatically evaluate the truth of queries\n",
      "In this paper we investigate information validation tasks that are initiated as queries from either automated agents or humans new online information validation technique\n",
      "Unmatched - striped_query_text:  For general convex programming. Show that this often improves running times by an order of magnitude or more vs\n",
      "checking whether a stochastic system satisfies a certain temporal property with a probability greater ( or smaller ) than a fixed threshold\n",
      "We address the problem of model checking stochastic systems\n",
      "as well as subframe temporal alignment between cameras and estimating 3D trajectories of dynamic points\n",
      "triangulating 3D static points\n",
      "The triangulation constraint however is invalid for moving points captured in multiple unsynchronized videos and bundle adjustment is not purposed to estimate the temporal alignment between cameras that jointly optimizes four coupled sub-problems : estimating camera intrinsics and extrinsics\n",
      "This research in Human Swarm Interaction ( HSI ) focuses on different control laws and ways to integrate the human intent with local control laws of the robots\n",
      "to give the operator haptic feedback as well as visual feedback\n",
      "Unmatched - striped_query_text:  that is the 90-th percentile distance We term this relationship as power-hop and the corresponding power-law exponent as power-hop exponent h. We provide theoretical justification for this pattern under successful existing network models\n",
      "we explore pervasive patterns that are related to k-cores and emerging in graphs from several diverse domains\n",
      "How to summarize these data to route the user 's attention to what really matters ?\n",
      "For constructing compact generative representations of PCD at multiple levels of detail\n",
      "to naturally reduce the offer prices to the bidders through the bidding rounds to minimize expected payment\n",
      "This is a recognized\n",
      "However the DCA design has lacked a way to determine the prices to offer the bidders in each round\n",
      "the first techniques for this\n",
      "We present to our knowledge\n",
      "important and timely problem\n",
      "while computer music systems often use rigid and even numerical representations that are difficult to work with\n",
      "and where interfaces are supported in a natural way by music notation\n",
      "One particular challenge is that humans easily follow scores and chord charts\n",
      "and understand media locations in musical terms ( beats and measures )\n",
      "where musical material in various media is synchronized\n",
      "where musicians can quickly alter the performance order by specifying ( re- ) arrangements of the material\n",
      "adapt these to new performance plans\n",
      "we examine the limitations of using a single type of context to form groups\n",
      "We present a new technique that allows mobile devices to opportunistically group with one another\n",
      "Meeting service level objectives ( SLOs ) for tail latency is an important and challenging open problem in cloud computing infrastructures\n",
      "The challenges are exacerbated by burstiness in the workloads to provide tail latency QoS for shared networked storage\n",
      "such that the relative condition number of $ G $ with $ \\hat { G } $ is bounded above by $ \\tilde { O } ( k\\log^2 n ) $\n",
      "On input of an $ n $ -vertex $ m $ -edge weighted graph $ G $ and a value $ k $ produces an incremental sparsifier $ \\hat { G } $ with $ n-1 + m/k $ edges\n",
      "with probability $ 1-p $ ( we use the $ \\tilde { O } ( ) $ notation to hide a factor of at most $ (\n",
      "For inferring the purpose of sensitive data usage in the context of Android smartphone apps\n",
      "Under which the advice given by ACAS X is safe\n",
      "We study peer review systems in which proposals are reviewed by PIs who have submitted proposals themselves\n",
      "Does it spread uniformly over countries ?\n",
      "In order to reduce the offloading workload\n",
      "How do people join the social groups\n",
      "uniformly or with burst ?\n",
      "An Internet-scale repository of crowd-sourced video content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We tested whether neurons in macaque V1 interact in a similar manner\n",
      "Unmatched - striped_query_text:  allows SNC-Meister to pack together many more tenants. : in experiments with production traces\n",
      "This assumption has been challenged by recent papers\n",
      "which designed tailor-made algorithms that compute optimal defender strategies for security games with limited surveillance\n",
      "Does it resemble the propagation pattern of benign files\n",
      "such as software patches ?\n",
      "the new approach is slow due to the burden by sequential similarity check over large volumes of high dimensional historical measurements\n",
      "This calls for a general approach to preprocess the historical data\n",
      "Although able to achieve much higher accuracy\n",
      "In this paper we propose to achieve such a goal with three steps\n",
      "to remove redundancy To further reduce the computational time to group the clustered power system data into a tree structure\n",
      "making it unsuitable for online services\n",
      "In this work we tackle the following question : Given the time stamped rating data for a product or service\n",
      "how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous ?\n",
      "In this paper we study the intertwined propagation of two competing `` memes '' ( or data\n",
      ") in a composite network\n",
      "we ask two key questions : ( a ) which meme will prevail ? and ( b ) can one influence the outcome of the propagations ?\n",
      "Within the constraints of this scenario\n",
      "We argue that perhaps they should not : Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones\n",
      "To develop a formal framework for desirable weighting schemes\n",
      "When instead agents can form coalitions\n",
      "NE is inadequate and an appropriate solution concept is strong Nash equilibrium ( SNE )\n",
      "Few computational results are known about SNE\n",
      "However these can be adopted only when coalitions are not an issue\n",
      "In this paper we first study the problem of verifying whether a strategy profile is an SNE\n",
      "where a single class of delay-sensitive customers seek service from a server with an observable queue\n",
      "for the ( maximum ) revenue under this optimal threshold\n",
      "under state dependent pricing\n",
      "However no explicit expression for this threshold has been found\n",
      "We consider the social welfare model of Naor [ 20 ] and revenue-maximization model of Chen and Frank [ 7 ]\n",
      "By doing so we identify tightly knit\n",
      "hidden communities of users and locations which they frequent to study the temporal dynamics of hidden communities in LBSNs\n",
      "A novel extension of normal form games\n",
      "how does this connectivity differ across different human subjects ? which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity\n",
      "Furthermore\n",
      "We give minimax lower bound for this problem\n",
      "In this paper we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label to learn the mapping in a completely unsupervised manner\n",
      "In this article we give definitive answers to these questions\n",
      "and indeed it is natural to ask whether such algorithms exist\n",
      "However this does not preclude the existence of approximation algorithms for Dodgson that are monotonic or homogeneous\n",
      "but these papers focus on fairness and consider a strikingly weak notion of truthfulness In this paper we investigate the problem of cutting a cake in a way that is truthful\n",
      "Unmatched - striped_query_text:  There are numerous papers that study the problem of fairly dividing a cake. ; a small number of them also take into account self-interested agents and consequent strategic issues\n",
      "However it is difficult to know which actions should be included in the abstraction without first solving the game\n",
      "and it is infeasible to solve the game without first abstracting it\n",
      "Prior work has looked at the interdependencies between utterances and the change of dialogue over time\n",
      "an analysis that allows us to investigate the adaptivity of student strategies as students gain domain knowledge\n",
      "but it has not addressed how dialogue changes during a lesson\n",
      "To encode the global relationship context of visual events across time and space to use the contextual information to modulate the analysis\n",
      "The IR model is unrealistic and has led to theoretical results which can be at odds with computer systems implementation results to decouple the inherent job size ( X ) from the server-side slowdown ( S )\n",
      "In most cases the reason why users follow other users is unavailable In this work\n",
      "we answer this question\n",
      "Introductory robot programming courses must evolve to reflect this reality\n",
      "by teaching students to make use of the sophisticated tools their robots provide rather than reimplementing basic algorithms\n",
      "However existing algorithms for Tucker decomposition have limited scalability\n",
      "To avoid M-Bottleneck to minimize the materialized intermediate data\n",
      "whose size rapidly grows as the order increases ( 4 )\n",
      "We call this problem M-Bottleneck ( `` Materialization Bottleneck '' )\n",
      "and especially fail to decompose high-order tensors since they explicitly materialize intermediate data\n",
      "Capable of finding statistically significant discrepancies\n",
      "determining the situations in which they occur\n",
      "and making simple corrections to the world model to improve performance\n",
      "That can greatly improve the level of intelligence and driving quality of autonomous vehicles\n",
      "A type-safe C-like language\n",
      "semantics and proof calculus for dRL\n",
      "For refinement relations on hybrid systems for verifying such relations This paper gives a syntax\n",
      "\n",
      "\n",
      "Out of 6006, 22 are unmatched, please double check\n"
     ]
    }
   ],
   "source": [
    "from util import write_segment_name                        \n",
    "write_segment_name(dev_segments,\"data/purpose_gold_dev.tsv\",groundtruth_dict,segment_name=\"purpose\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Gold Labels from a real purpose_gold.tsv and evaluate one toy LF with this labeled dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6006\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "# %time external = load_external_labels(session, Segment, annotator_name='purpose_dev',isPrint=False,file_path=\"data/purpose_gold_dev.tsv\")\n",
    "print()\n",
    "\n",
    "# (Optional) Reload these gold labels from snorkel.db (for cases when you re-opened the snorkel.db in a different notebook)\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='purpose_dev', split=1)\n",
    "print((L_gold_dev).shape[0])\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LF.util_purpose_default import LF_comparative_degree,LF_purpose_verb,LF_purpose_delimiter,LF_purpose_adj_problem,LF_purpose_leading_question_word\n",
    "from LF.util_common_default import LF_expressing_contrast, LF_excluded_pseudo_contrast,negate\n",
    "from LF.util_mechanism_default import LF_mechanism_verb,LF_mechanism_adv,LF_mechanism_noun,LF_mechanism_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate and tuning more LFs based on dev set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2K_dev_378::span:198:368\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1723652b0>, 'id': 35248, 'type': 'segment', 'split': 1, 'segment_cue_cid': None, 'segment_cue_id': 72165, 'segment_cue': Span(\"b'All prior lossy abstraction algorithms for extensive-form games either 1 ) had no bounds on solution quality or 2 ) depended on specific equilibrium computation approaches'\", sentence=27237, chars=[0,170], words=[0,27])}\n",
      "test_candidates # 6006\n",
      "test_marginals [0.5 0.5 0.5 ... 1.  1.  0.5]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 0.0\n",
      "Neg. class recall: 0.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 1599 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def LF_purpose_verb_test0(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*for study of.*$)|(.*in order to.*$)|(.* implication.*$)|(.* to solve.*$)|(.* hypothesis.*$)|(.*to enable.*$)|(.*to aid.*$)|(.*to produce.*$)|(.*to discuss.*$)|(.*to investigat.*$)|(.* give.*$)|(.* that can .*$)|(.* examine.*$)|(.* extend.*$)|(.* offer.*$)\",1) else 0\n",
    "\n",
    "# print(LF_purpose_verb_test0(dev_segments[19]))\n",
    "# print(dev_segments[19].segment_cue.stable_id)\n",
    "# print(dev_segments[19].__dict__)\n",
    "from snorkel.lf_helpers import test_LF\n",
    "tp, fp, tn, fn = test_LF(session, LF_purpose_verb_test0, split=1, annotator_name='purpose_dev',test_labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 6006\n",
      "test_marginals [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 0.0\n",
      "Neg. class recall: 0.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 6 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def LF_purpose_single_verb_test1(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*to enable.*$)\",1) else 0\n",
    "    \n",
    "tp, fp, tn, fn = test_LF(session, LF_purpose_single_verb_test1, split=1, annotator_name='purpose_dev',test_labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 6006\n",
      "test_marginals [0.5 0.5 0.5 ... 1.  1.  0.5]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 0.0\n",
      "Neg. class recall: 0.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 1425 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def LF_purpose_single_verb_test2(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*for.*$)\",1) else 0\n",
    "    \n",
    "tp, fp, tn, fn = test_LF(session, LF_purpose_single_verb_test2, split=1, annotator_name='purpose_dev',test_labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 6006\n",
      "test_marginals [0.  0.5 0.5 ... 0.5 0.5 0.5]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of bounds: 0<=0<6006, 0<=0<0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3b0341b50543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mLF_mechanism_verb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_LF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegate_LF_mechanism_verb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'purpose'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL_gold_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/snorkel/snorkel/lf_helpers.py\u001b[0m in \u001b[0;36mtest_LF\u001b[0;34m(session, lf, split, annotator_name, test_labels)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_marginals\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_marginals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;31m# print(\"num predicted positive \",test_marginals.sum())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_marginals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_unlabeled_as_neg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_at_thresh_as_neg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# in this case, _score_binary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/snorkel/snorkel/learning/utils.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, test_marginals, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mcardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_marginals\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcardinality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_marginals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_marginals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/snorkel/snorkel/learning/utils.py\u001b[0m in \u001b[0;36m_score_binary\u001b[0;34m(self, test_marginals, train_marginals, b, set_unlabeled_as_neg, set_at_thresh_as_neg, display, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mtest_label_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_row_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_label_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/snorkel/snorkel/annotations.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsr_AnnotationMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# If X is an integer or float value, just return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;31m# [i, j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_single_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;31m# [i, 1:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_get_single_element\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             raise IndexError(\"index out of bounds: 0<=%d<%d, 0<=%d<%d\" %\n\u001b[0;32m--> 856\u001b[0;31m                              (row, M, col, N))\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mmajor_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of bounds: 0<=0<6006, 0<=0<0"
     ]
    }
   ],
   "source": [
    "def negate_LF_mechanism_verb_test(c):\n",
    "    # where LF_mechanism_verb() is defined as: return 1 if rule_regex_search_candidate_text(c,\"((^|\\s)(introduce|propose|develop|approach|applied|apply|using|present|contribute|build|built).*$)\",1) else 0\n",
    "    return -1 if LF_mechanism_verb(c)==1 else 0   \n",
    "\n",
    "tp, fp, tn, fn = test_LF(session, negate_LF_mechanism_verb_test, split=1, annotator_name='purpose',test_labels=L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading more realistic LFs in aggregation, train a generative model, evaluate the model on dev set\n",
    "\n",
    "\n",
    "<b>Issue</b> the difference between `learned accuracy` and `empirical accuracy`.\n",
    "\n",
    "<b>Guess:</b>\n",
    "\n",
    "1. Learned accuracy is evaluating each LF w.r.t. trained generative model\n",
    "\n",
    "2. Empirical accuracy is evaluating each LF w.r.t. gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LF.util_purpose_default import LF_comparative_degree,LF_purpose_verb,LF_purpose_delimiter,LF_purpose_adj_problem,LF_purpose_leading_question_word\n",
    "from LF.util_common_default import LF_expressing_contrast, LF_excluded_pseudo_contrast,negate\n",
    "from LF.util_mechanism_default import LF_mechanism_verb,LF_mechanism_adv,LF_mechanism_noun,LF_mechanism_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35228\n",
      "0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 3min 8s, sys: 1.26 s, total: 3min 9s\n",
      "Wall time: 3min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<35228x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7404 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "\n",
    "def LF_comparative_degree(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*more.*than.*$)|(.*er than.*$)\",1) else 0\n",
    "\n",
    "def LF_purpose_verb(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*in order to.*$)|(.* implication.*$)|(.* to solve.*$)|(.* hypothesis.*$)|(.*to enable.*$)|(.*to aid.*$)|(.*to produce.*$)|(.*to investigat.*$)|(.* give.*$)|(.* that can .*$)|(.* examine.*$)|(.* extend.*$)|(.* offer.*$)\",1) else 0\n",
    "\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "purpose_LFs=[LF_comparative_degree,LF_purpose_verb,LF_excluded_pseudo_contrast,LF_comparative_degree,LF_purpose_verb,LF_purpose_delimiter,LF_purpose_adj_problem,LF_purpose_leading_question_word,negate(LF_mechanism_verb),negate(LF_mechanism_adv),negate(LF_mechanism_noun),negate(LF_mechanism_adj)]\n",
    "\n",
    "labeler = LabelAnnotator(lfs=purpose_LFs)\n",
    "\n",
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train\n",
    "\n",
    "# ,LF_excluded_pseudo_contrast,LF_comparative_degree,LF_purpose_verb,LF_purpose_delimiter,LF_purpose_adj_problem,LF_purpose_leading_question_word] #negate(LF_mechanism_verb),negate(LF_mechanism_adv),negate(LF_mechanism_noun),negate(LF_mechanism_adj)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation: do a live dev accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  j  Coverage  Overlaps  Conflicts\n",
      "LF_comparative_degree             0  0.007664  0.001277   0.000937\n",
      "LF_purpose_verb                   1  0.035228  0.008289   0.006699\n",
      "LF_excluded_pseudo_contrast       2  0.001079  0.000170   0.000114\n",
      "LF_purpose_delimiter              3  0.017373  0.004088   0.003577\n",
      "LF_purpose_adj_problem            4  0.003548  0.000653   0.000483\n",
      "LF_purpose_leading_question_word  5  0.008402  0.002470   0.001504\n",
      "<lambda>                          6  0.136880  0.012660   0.012604\n",
      "Inferred cardinality: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a2e30d35a407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgen_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlf_propensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_purpose_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mL_purpose_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training generative model, now checking performance against development set labels...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/snorkel/snorkel/learning/gen_learning.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, L, deps, LF_acc_prior_weights, LF_acc_prior_weight_default, labels, label_prior_weight, init_deps, init_class_prior, epochs, step_size, decay, reg_param, reg_type, verbose, truncation, burn_in, cardinality, timer, candidate_ranges, threads)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numbskull/numbskull.py\u001b[0m in \u001b[0;36mlearning\u001b[0;34m(self, fgID, out)\u001b[0m\n\u001b[1;32m    385\u001b[0m                  \u001b[0mdiagnostics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                  learn_non_evidence=self.learn_non_evidence)\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             output_file = os.path.join(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numbskull/factorgraph.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, burnin_epochs, epochs, stepsize, decay, regularization, reg_param, truncation, diagnostics, verbose, learn_non_evidence, var_copy, weight_copy)\u001b[0m\n\u001b[1;32m    200\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_value_evid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                         self.weight_value, learn_non_evidence)\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mrun_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreadpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearnthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_epoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_total_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numbskull/factorgraph.py\u001b[0m in \u001b[0;36mrun_pool\u001b[0;34m(threadpool, threads, func, args)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"\"\"TODO.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfuture_to_samples\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "L_purpose_train = L_train\n",
    "print(L_purpose_train.lf_stats(session))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n",
      "Finished training generative model, now checking performance against development set labels...\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel(lf_propensity=True)\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=0.0)\n",
    "\n",
    "print(\"Finished training generative model, now checking performance against development set labels...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30866\n",
      "0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 4min 2s, sys: 3.56 s, total: 4min 5s\n",
      "Wall time: 4min 33s\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class recall: 0.106\n",
      "Neg. class recall: 0.942\n",
      "Precision            0.3\n",
      "Recall               0.106\n",
      "F1                   0.157\n",
      "----------------------------------------\n",
      "TP: 467 | FP: 1089 | TN: 17719 | FN: 3924\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_comparative_degree</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>22</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095652</td>\n",
       "      <td>0.559243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb</th>\n",
       "      <td>1</td>\n",
       "      <td>0.035249</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>172</td>\n",
       "      <td>527</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246066</td>\n",
       "      <td>0.565238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_excluded_pseudo_contrast</th>\n",
       "      <td>2</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.568272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_delimiter</th>\n",
       "      <td>3</td>\n",
       "      <td>0.017884</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>166</td>\n",
       "      <td>328</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.336032</td>\n",
       "      <td>0.572308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_adj_problem</th>\n",
       "      <td>4</td>\n",
       "      <td>0.006966</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>40</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264901</td>\n",
       "      <td>0.565955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_leading_question_word</th>\n",
       "      <td>5</td>\n",
       "      <td>0.010270</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>109</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488789</td>\n",
       "      <td>0.574218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>6</td>\n",
       "      <td>0.147411</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315</td>\n",
       "      <td>2510</td>\n",
       "      <td>0.888496</td>\n",
       "      <td>0.595269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  j  Coverage  Overlaps  Conflicts   TP   FP  \\\n",
       "LF_comparative_degree             0  0.011048  0.002235   0.001717   22  208   \n",
       "LF_purpose_verb                   1  0.035249  0.009460   0.008391  172  527   \n",
       "LF_excluded_pseudo_contrast       2  0.002462  0.000518   0.000259    0    0   \n",
       "LF_purpose_delimiter              3  0.017884  0.002138   0.002041  166  328   \n",
       "LF_purpose_adj_problem            4  0.006966  0.001458   0.001231   40  111   \n",
       "LF_purpose_leading_question_word  5  0.010270  0.002106   0.001361  109  114   \n",
       "<lambda>                          6  0.147411  0.014288   0.014028    0    0   \n",
       "\n",
       "                                   FN    TN  Empirical Acc.  Learned Acc.  \n",
       "LF_comparative_degree               0     0        0.095652      0.559243  \n",
       "LF_purpose_verb                     0     0        0.246066      0.565238  \n",
       "LF_excluded_pseudo_contrast         8    45        0.849057      0.568272  \n",
       "LF_purpose_delimiter                0     0        0.336032      0.572308  \n",
       "LF_purpose_adj_problem              0     0        0.264901      0.565955  \n",
       "LF_purpose_leading_question_word    0     0        0.488789      0.574218  \n",
       "<lambda>                          315  2510        0.888496      0.595269  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "%time L_dev = labeler.apply_existing(split=1)\n",
    "_ = gen_model.error_analysis(session, L_dev, L_gold_dev,set_unlabeled_as_neg=False)\n",
    "\n",
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])  # learned_lf_stats(): For each labeling function, estimates of a few metrics are provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segments that all belong to one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**717/30866 Candidate/Span:**\t`Segment(Span(\"b'Proactive moderation tools'\", sentence=7615, chars=[0,25], words=[0,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tProactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**718/30866 Candidate/Span:**\t`Segment(Span(\"b'while reactive bans were able to discourage a wider variety of behaviors .'\", sentence=7615, chars=[150,223], words=[24,36]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tProactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**719/30866 Candidate/Span:**\t`Segment(Span(\"b'such as chat modes which restricted the ability to post certain content'\", sentence=7615, chars=[28,98], words=[4,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tProactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**720/30866 Candidate/Span:**\t`Segment(Span(\"b'proved effective at discouraging spam behaviors'\", sentence=7615, chars=[101,147], words=[17,22]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tProactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2278/30866 Candidate/Span:**\t`Segment(Span(\"b'and types of behaviors'\", sentence=7616, chars=[58,79], words=[10,13]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThis work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2279/30866 Candidate/Span:**\t`Segment(Span(\"b'This work considers the intersection of tools'\", sentence=7616, chars=[0,44], words=[0,6]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThis work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2280/30866 Candidate/Span:**\t`Segment(Span(\"b'offering a new frame through which to consider the development of moderation strategies .\\n'\", sentence=7616, chars=[82,171], words=[15,29]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThis work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2281/30866 Candidate/Span:**\t`Segment(Span(\"b'authority'\", sentence=7616, chars=[47,55], words=[8,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThis work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies .\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15728/30866 Candidate/Span:**\t`Segment(Span(\"b'cruel'\", sentence=7611, chars=[56,60], words=[9,9]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOnline communities have the potential to be supportive, cruel, or anywhere in between ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15729/30866 Candidate/Span:**\t`Segment(Span(\"b'Online communities have the potential to be supportive'\", sentence=7611, chars=[0,53], words=[0,7]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOnline communities have the potential to be supportive, cruel, or anywhere in between ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15730/30866 Candidate/Span:**\t`Segment(Span(\"b'or anywhere in between .'\", sentence=7611, chars=[63,86], words=[11,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOnline communities have the potential to be supportive, cruel, or anywhere in between ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15734/30866 Candidate/Span:**\t`Segment(Span(\"b'The development of positive norms for interaction can help users build bonds'\", sentence=7612, chars=[0,75], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe development of positive norms for interaction can help users build bonds, grow, and learn ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15735/30866 Candidate/Span:**\t`Segment(Span(\"b'grow'\", sentence=7612, chars=[78,81], words=[13,13]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe development of positive norms for interaction can help users build bonds, grow, and learn ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**15736/30866 Candidate/Span:**\t`Segment(Span(\"b'and learn .'\", sentence=7612, chars=[84,94], words=[15,17]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe development of positive norms for interaction can help users build bonds, grow, and learn ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**26527/30866 Candidate/Span:**\t`Segment(Span(\"b'including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors .'\", sentence=7613, chars=[146,290], words=[22,40]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUsing millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**26528/30866 Candidate/Span:**\t`Segment(Span(\"b'we explore the effectiveness of methods for encouraging and discouraging specific behaviors'\", sentence=7613, chars=[53,143], words=[9,20]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUsing millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**26529/30866 Candidate/Span:**\t`Segment(Span(\"b'Using millions of messages sent in Twitch chatrooms'\", sentence=7613, chars=[0,50], words=[0,7]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUsing millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**29062/30866 Candidate/Span:**\t`Segment(Span(\"b'Consistent with aspects of imitation theory and deterrence theory'\", sentence=7614, chars=[0,64], words=[0,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tConsistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**29063/30866 Candidate/Span:**\t`Segment(Span(\"b'and more so for behaviors from high status users .'\", sentence=7614, chars=[118,167], words=[19,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tConsistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**29064/30866 Candidate/Span:**\t`Segment(Span(\"b'users imitated examples of behavior that they saw'\", sentence=7614, chars=[67,115], words=[10,17]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tConsistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_clauses_single_doc 20\n",
      "These clauses' parent Document's is  Document 2K_dev_0\n"
     ]
    }
   ],
   "source": [
    "num_clauses_single_doc=0\n",
    "for i in range(len(dev_segments)): # to print at most 4 cands \n",
    "    if dev_segments[i].get_parent().get_parent().name==\"2K_dev_0\":\n",
    "        printmd(\"**\"+str(i)+\"/\"+str(len(dev_segments))+\" Candidate/Span:**\\t`\"+str(dev_segments[i])+\"`\")\n",
    "        printmd(\"**Its parent Sentence's text:**\\t\"+str(dev_segments[i].get_parent().text))\n",
    "#         printmd(\"**Its parent Document's text:**\\t\"+str(dev_segments[i].get_parent().get_parent().__dict__))\n",
    "        print() \n",
    "        num_clauses_single_doc+=1\n",
    "        single_doc=dev_segments[i].get_parent().get_parent()\n",
    "print(\"num_clauses_single_doc\", num_clauses_single_doc)\n",
    "print(\"These clauses' parent Document's is \",single_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we want to extract the stable-label-id of each context within segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState at 0x1a2c4eb128>,\n",
       " 'type': 'segment',\n",
       " 'segment_cue_cid': None,\n",
       " 'id': 351062,\n",
       " 'split': 2,\n",
       " 'segment_cue_id': 59596,\n",
       " 'segment_cue': Span(\"b'But quality eye tracking hardware is expensive and can only be used with one person at a time.'\", sentence=24376, chars=[0,93], words=[0,18])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Loading Gold Labels (Encountered some issue and slightly deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'snorkel.annotations.csr_LabelMatrix'>\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "from snorkel.annotations import load_gold_labels\n",
    "# if load the 1st time\n",
    "# purposes = load_external_labels(session, Segment, annotator_name='background')\n",
    "\n",
    "# else:\n",
    "gold_dev_purposes_mixed_w_backgrounds=load_gold_labels(session, annotator_name='background', split=1)\n",
    "print(type(gold_dev_purposes_mixed_w_backgrounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 29s, sys: 1.75 s, total: 2min 30s\n",
      "Wall time: 2min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<31595x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6942 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "purpose_LFs=[LF_expressing_contrast,LF_excluded_pseudo_contrast,LF_comparative_degree,LF_purpose_verb,LF_purpose_delimiter,LF_purpose_adj_problem,LF_purpose_leading_question_word,negate(LF_mechanism_verb),negate(LF_mechanism_adv),negate(LF_mechanism_noun),negate(LF_mechanism_adj)]\n",
    "\n",
    "labeler = LabelAnnotator(lfs=purpose_LFs)\n",
    "\n",
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a peek at one of the labelled candidates and which LF does it come from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random candidate Segment(Span(\"b'Kinsey (1984) defined noncompliance with tax laws as the \\xe2\\x80\\x9cfailure'\", sentence=13067, chars=[0,64], words=[0,12]))\n",
      "Its LabelKey LabelKey (LF_purpose_verb)\n"
     ]
    }
   ],
   "source": [
    "print(\"A random candidate\",L_train.get_candidate(session, 3))\n",
    "print(\"Its LabelKey\",L_train.get_key(session, 3))\n",
    "# print(\"Annotations summary\",L_train.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view statistics about the resulting label matrix.\n",
    "\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a *conflicting* non-zero label for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_expressing_contrast</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_excluded_pseudo_contrast</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_comparative_degree</th>\n",
       "      <td>2</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb</th>\n",
       "      <td>3</td>\n",
       "      <td>0.034879</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.006552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_delimiter</th>\n",
       "      <td>4</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.003482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_adj_problem</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_leading_question_word</th>\n",
       "      <td>6</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.001551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>7</td>\n",
       "      <td>0.136192</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.012597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  j  Coverage  Overlaps  Conflicts\n",
       "LF_expressing_contrast            0  0.011173  0.000475   0.000317\n",
       "LF_excluded_pseudo_contrast       1  0.001013  0.000158   0.000095\n",
       "LF_comparative_degree             2  0.007501  0.001139   0.000855\n",
       "LF_purpose_verb                   3  0.034879  0.008134   0.006552\n",
       "LF_purpose_delimiter              4  0.017060  0.003925   0.003482\n",
       "LF_purpose_adj_problem            5  0.003482  0.000728   0.000506\n",
       "LF_purpose_leading_question_word  6  0.008419  0.002595   0.001551\n",
       "<lambda>                          7  0.136192  0.012660   0.012597"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we examine the metrics for Mechanism LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 46s, sys: 2.69 s, total: 2min 49s\n",
      "Wall time: 2min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<31595x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8523 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "mechanism_LFs=[LF_mechanism_verb, LF_mechanism_adv,LF_mechanism_noun, LF_mechanism_adj, negate(LF_expressing_contrast),negate(LF_excluded_pseudo_contrast),negate(LF_comparative_degree),negate(LF_purpose_verb),negate(LF_purpose_delimiter),negate(LF_purpose_adj_problem),negate(LF_purpose_leading_question_word)]\n",
    "labeler = LabelAnnotator(lfs=mechanism_LFs)\n",
    "\n",
    "np.random.seed(1702)\n",
    "%time L_train_mechanism = labeler.apply(split=0)\n",
    "L_train_mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_verb</th>\n",
       "      <td>0</td>\n",
       "      <td>0.136192</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_adv</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008831</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_noun</th>\n",
       "      <td>2</td>\n",
       "      <td>0.083209</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_adj</th>\n",
       "      <td>3</td>\n",
       "      <td>0.030353</td>\n",
       "      <td>0.011774</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>4</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   j  Coverage  Overlaps  Conflicts\n",
       "LF_mechanism_verb  0  0.136192  0.031746   0.000317\n",
       "LF_mechanism_adv   1  0.008831  0.001677   0.000032\n",
       "LF_mechanism_noun  2  0.083209  0.030511   0.000285\n",
       "LF_mechanism_adj   3  0.030353  0.011774   0.000095\n",
       "<lambda>           4  0.011173  0.000696   0.000696"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train_mechanism.lf_stats(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting the Generative Model\n",
    "\n",
    "The training step for 70K papers take approximately 10 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()  # Inferred cardinality: 2 would be binary label (excluding the uncertain class)\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we quantify a few characteristics, (1) LF accuracies; (2) the marginal probability distributions of each candidate being True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07856624, 0.07481565, 0.07709562, 0.08353626, 0.07912881,\n",
       "       0.07455556, 0.07590932, 0.12232697])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_marginals = gen_model.marginals(L_train)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.537883</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>0.542201</td>\n",
       "      <td>0.362007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.541904</td>\n",
       "      <td>0.6658</td>\n",
       "      <td>0.544689</td>\n",
       "      <td>0.360414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.538233</td>\n",
       "      <td>0.6722</td>\n",
       "      <td>0.537371</td>\n",
       "      <td>0.362206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.534887</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.533137</td>\n",
       "      <td>0.360414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529306</td>\n",
       "      <td>0.6688</td>\n",
       "      <td>0.531137</td>\n",
       "      <td>0.349861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.539391</td>\n",
       "      <td>0.6702</td>\n",
       "      <td>0.537954</td>\n",
       "      <td>0.357029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.534796</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>0.542962</td>\n",
       "      <td>0.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.565314</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>0.567007</td>\n",
       "      <td>0.386699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Coverage  Precision    Recall\n",
       "0  0.537883    0.6652   0.542201  0.362007\n",
       "1  0.541904    0.6658   0.544689  0.360414\n",
       "2  0.538233    0.6722   0.537371  0.362206\n",
       "3  0.534887    0.6736   0.533137  0.360414\n",
       "4  0.529306    0.6688   0.531137  0.349861\n",
       "5  0.539391    0.6702   0.537954  0.357029\n",
       "6  0.534796    0.6610   0.542962  0.354839\n",
       "7  0.565314    0.6752   0.567007  0.386699"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.learned_lf_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 0\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 0.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test one purpose LF over groundtruth ISSUE: gold_labels.tsv are context_stable_ids. How could we get that for our task \n",
    "\n",
    "from snorkel.lf_helpers import test_LF\n",
    "tp, fp, tn, fn = test_LF(session, LF_expressing_contrast, split=1, annotator_name='background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands=session.query(Segment).filter(Segment.split == 0).all()\n",
    "\n",
    "for ind,c in enumerate(cands): # session.query(Segment).filter(Segment.split == 0).all():\n",
    "    if ind%500==0:\n",
    "        print(\"Processed \", ind, len(cands))\n",
    "        \n",
    "    for lf in LFS:\n",
    "        if lf(c)!=0:\n",
    "            labeled.append(c)\n",
    "            continue\n",
    "        \n",
    "print(\"Number labeled:\", len(labeled),len(cands))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "labeled[4].get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "\n",
    "# Reference: \n",
    "# def load_external_labels(session, candidate_class, annotator_name='gold'):\n",
    "#  session.add(StableLabel(\n",
    "#                 context_stable_ids=context_stable_ids,\n",
    "#                 annotator_name=annotator_name, e.g. \"gold\"\n",
    "#                 value=row['label']))\n",
    "            \n",
    "            \n",
    "GoldBackground = candidate_subclass('GoldBackground', ['goldbackground_cue'])\n",
    "\n",
    "all_gold_background_extractor=CandidateExtractor(GoldBackground, [ngrams], None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to differentiate ``Background`` with ``Purpose``. Recall our ``Background`` definition from Google Doc:\n",
    "\n",
    "``Contains words that indicate prior work (e.g., traditionally, researchers have), and then following sentence/span starts with some variant of In this paper, we introduce (Exploring adjacency relationship through helper function in Snorkel)``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove if necessary \n",
    "# session.query(Background).all()\n",
    "import snorkel.models.candidate as candidate\n",
    "\n",
    "def del_defined_candidate_class(class_name):\n",
    "    print(\"Existing\", candidate.candidate_subclasses)\n",
    "    del(candidate.candidate_subclasses[class_name])\n",
    "    print(\"After deletion\", candidate.candidate_subclasses)\n",
    "    \n",
    "## Usage:    \n",
    "# del_defined_candidate_class('Purpose')\n",
    "\n",
    "non_comma_matcher=DictionaryMatch(d=[','],longest_match_only=True,reverse=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells describe our ``Span Matching`` phase: we respectively define the `Span` for the 5 segments are , and centrally stored in a document-id-indexed hashmap. This helps us to group the 5 segments back into documents.\n",
    "3. Only those documents that have at least 3 segments are considered as valid\n",
    "\n",
    "Following this ``Span Matching`` phase, we will be having an ``Document Aggregation`` phase, where adjacency relationship and (or) waterfall models are being emphasized through multiple criteria. For example, the following criteria prevents non-abstracts (e.g. proceeding cover letter, tutorial introduction, etc.) or less well-strctured abstracts from being included. \n",
    "\n",
    "1. Only those documents that have all 5 segments are considered as valid\n",
    "2. Only those documents that have at least 4 segments are cosidered as valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 28s, sys: 472 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 148**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 9.88 s, sys: 220 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 15**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 460 ms, sys: 22.7 ms, total: 483 ms\n",
      "Wall time: 474 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 0**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/148 Candidate/Span:**\t`Background(Span(\"b'The intersection of these domains should enable researchers to foster an improved understanding of student learning'\", sentence=12057, chars=[0,114], words=[0,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe intersection of these domains should enable researchers to foster an improved understanding of student learning, lead to the creation of more natural and enriching learning interfaces, and motivate the development of novel techniques for tackling challenges that are specific of education.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d5e8d0>, 'name': '5834868625ff05a97b01354e', 'stable_id': '5834868625ff05a97b01354e::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1886, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b01354e,0,b'This summary describes the 1st International Workshop on Multimodal Learning Analytics.'), Sentence(Document 5834868625ff05a97b01354e,1,b'This area of study brings together the technologies of multimodal analysis with the learning sciences.'), Sentence(Document 5834868625ff05a97b01354e,2,b'The intersection of these domains should enable researchers to foster an improved understanding of student learning, lead to the creation of more natural and enriching learning interfaces, and motivate the development of novel techniques for tackling challenges that are specific of education.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/148 Candidate/Span:**\t`Background(Span(\"b'researchers appear to be intrigued with the question; Given a set of points'\", sentence=5803, chars=[9,83], words=[3,16]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOf late, researchers appear to be intrigued with the question; Given a set of points, what is the region occupied by them?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1113a94e0>, 'name': '5834868425ff05a97b00c32e', 'stable_id': '5834868425ff05a97b00c32e::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 72, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00c32e,0,b'Of late, researchers appear to be intrigued with the question; Given a set of points, what is the region occupied by them?'), Sentence(Document 5834868425ff05a97b00c32e,1,b'The answer appears to be neither straight forward nor unique.'), Sentence(Document 5834868425ff05a97b00c32e,2,b'Convex hull, which gives a convex enclosure of the given set, concave hull, which generates non-convex polygons and other variants such as \\xce\\xb1-hull, poly hull, r-shape and s-shape etc. have been proposed.'), Sentence(Document 5834868425ff05a97b00c32e,3,b'In this paper, we extend the question of finding a minimum area enclosure (MAE) to a set of closed planar freeform curves, not resorting to sampling them.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/148 Candidate/Span:**\t`Background(Span(\"b'The feature-interaction problem has been keeping researchers and practitioners in suspense for years.'\", sentence=13452, chars=[0,100], words=[0,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe feature-interaction problem has been keeping researchers and practitioners in suspense for years."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d8dba8>, 'name': '5834868625ff05a97b012212', 'stable_id': '5834868625ff05a97b012212::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 2305, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b012212,0,b'The feature-interaction problem has been keeping researchers and practitioners in suspense for years.'), Sentence(Document 5834868625ff05a97b012212,1,b'Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems.'), Sentence(Document 5834868625ff05a97b012212,2,b'In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/148 Candidate/Span:**\t`Background(Span(\"b'previous work in this domain'\", sentence=13892, chars=[7,34], words=[1,5]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUnlike previous work in this domain, which focused on taking one reference sample and doing user authentication based on the reference sample only, we continuously sample user input and use the data for identi cation and further learning and re nement of the user model.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d9b550>, 'name': '5834868725ff05a97b0141e7', 'stable_id': '5834868725ff05a97b0141e7::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 2438, 'type': 'document', 'sentences': [Sentence(Document 5834868725ff05a97b0141e7,0,b'We analyze keystroke latency patterns to identify the person typing on the keyboard.'), Sentence(Document 5834868725ff05a97b0141e7,1,b'Unlike previous work in this domain, which focused on taking one reference sample and doing user authentication based on the reference sample only, we continuously sample user input and use the data for identi cation and further learning and re nement of the user model.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Background: \n",
    "Background = candidate_subclass('Background', ['background_cue'])\n",
    "transition_word=DictionaryMatch(d=['while','unlike','despite'],longest_match_only=True) \n",
    "transition_prev_work=DictionaryMatch(d=['previous','earlier','past'],longest_match_only=True) \n",
    "dict_background_matcher=DictionaryMatch(d=['previous work','traditionally','researchers'],longest_match_only=True) \n",
    "excluded_dict_background_matcher=DictionaryMatch(d=['we','unlike','our'],longest_match_only=True,reverse=True) \n",
    "non_comma_dict_background_matcher=CandidateExtractor(Background, [ngrams], [Intersection(non_comma_matcher,Union(dict_background_matcher,Intersection(transition_word,transition_prev_work)),excluded_dict_background_matcher)])\n",
    "background_cands=extract_and_display(non_comma_dict_background_matcher,Background,\"Background\",document_breakdown_map)\n",
    "# print(document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'snorkel.models.candidate.Purpose'>\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 35s, sys: 1.38 s, total: 1min 37s\n",
      "Wall time: 1min 38s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1501**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 11.8 s, sys: 468 ms, total: 12.3 s\n",
      "Wall time: 12.2 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 203**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 761 ms, sys: 117 ms, total: 878 ms\n",
      "Wall time: 878 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1501 Candidate/Span:**\t`Purpose(Span(\"b'In this paper'\", sentence=21545, chars=[0,12], words=[0,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d85b278>, 'name': '58cb6b8ec50f90cdd3875b14', 'stable_id': '58cb6b8ec50f90cdd3875b14::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4540, 'type': 'document', 'sentences': [Sentence(Document 58cb6b8ec50f90cdd3875b14,0,b'Visual inspection is, by far, the most widely used method in aircraft surface inspection.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,1,b'We are currently developing a prototype remote visual inspection system, designed to facilitate testing the hypothesized feasibility and advantages of remote visual inspection of aircraft surfaces.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,2,b'In this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1501 Candidate/Span:**\t`Purpose(Span(\"b'\\xe2\\x80\\x94here we examine shrinkage toward diagonality.\\n'\", sentence=11574, chars=[0,46], words=[0,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\there we examine shrinkage toward diagonality.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122c40f0>, 'name': '5834868625ff05a97b0122ab', 'stable_id': '5834868625ff05a97b0122ab::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1681, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b0122ab,0,b'The problem of estimating a covariance matrix in small samples has been considered by several authors following early work by Stein.'), Sentence(Document 5834868625ff05a97b0122ab,1,b'This problem can be especially important in hierarchical models where the standard errors of fixed and random effects depend on estimation of the covariance matrix of the distribution of the random effects.'), Sentence(Document 5834868625ff05a97b0122ab,2,b'We propose a set of hierarchical priors (HPs) for the covariance matrix that produce posterior shrinkage toward a specified structure'), Sentence(Document 5834868625ff05a97b0122ab,3,b'\\xe2\\x80\\x94here we examine shrinkage toward diagonality.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1501 Candidate/Span:**\t`Purpose(Span(\"b'which extend organisational boundaries.'\", sentence=6712, chars=[119,157], words=[19,23]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe quest for scalability, reliability and cost reduction has led to the development of massively distributed systems, which extend organisational boundaries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x111488240>, 'name': '5834868425ff05a97b00e0fe', 'stable_id': '5834868425ff05a97b00e0fe::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 315, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00e0fe,0,b'Modern computing paradigms have frequently adopted concepts from distributed systems.'), Sentence(Document 5834868425ff05a97b00e0fe,1,b'The quest for scalability, reliability and cost reduction has led to the development of massively distributed systems, which extend organisational boundaries.'), Sentence(Document 5834868425ff05a97b00e0fe,2,b'Voluntary computing environments (such as BOINC), Grids (such as EGEE and Globus), and more recently Cloud Computing (both open source and commercial) have established themselves as a range of distributed systems.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1501 Candidate/Span:**\t`Purpose(Span(\"b'however'\", sentence=12099, chars=[5,11], words=[2,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tFew, however, allow new interfaces to be created from scratch because they do not provide a means of demonstrating when a recorded macro should be invoked."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122fb828>, 'name': '5834868725ff05a97b013a8a', 'stable_id': '5834868725ff05a97b013a8a::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1836, 'type': 'document', 'sentences': [Sentence(Document 5834868725ff05a97b013a8a,0,b'Many programming by demonstration (PBD) systems elaborate on the idea of macro recording, and they allow users to extend existing applications.'), Sentence(Document 5834868725ff05a97b013a8a,1,b'Few, however, allow new interfaces to be created from scratch because they do not provide a means of demonstrating when a recorded macro should be invoked.'), Sentence(Document 5834868725ff05a97b013a8a,2,b'This paper discusses stimulus-response systems that allow both the when (stimulus/event) and the what (response macro) to be demonstrated.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21999fd0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26962, 'split': 2, 'id': 1705}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990080>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26963, 'split': 2, 'id': 1706}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219900f0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26964, 'split': 2, 'id': 1707}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990160>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26965, 'split': 2, 'id': 1708}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219901d0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26966, 'split': 2, 'id': 1709}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990240>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26967, 'split': 2, 'id': 1710}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219902b0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26968, 'split': 2, 'id': 1711}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990320>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26969, 'split': 2, 'id': 1712}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990390>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26970, 'split': 2, 'id': 1713}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990400>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26971, 'split': 2, 'id': 1714}\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Purpose:\n",
    "Purpose=candidate_subclass('Purpose',['purpose_cue'])\n",
    "\n",
    "transition_regex_matcher=RegexMatchSpan(rgx=\"((^|\\s)however.*$)|((^|\\s)but(?!(also))*$)\",longest_match_only=True)  # Correction: purpose \n",
    "excluded_dict_purpose_matcher=SentenceMatch(d=['but also','but without','but sometimes'],longest_match_only=True,reverse=True)  # the parent sentence shall not include \"but also\"\n",
    "transition_matcher=Intersection(transition_regex_matcher,excluded_dict_purpose_matcher)\n",
    "\n",
    "comparative_degree_matcher=Intersection(RegexMatchSpan(rgx=\"(.*more.*than.*$)|(.*er than.*$)\",longest_match_only=True),transition_prev_work)  # Correction: purpose \n",
    "other_regex_matcher=RegexMatchSpan(rgx=\"(.*extend.*$)|(.*offer.*$)\",longest_match_only=True)\n",
    "\n",
    "dict_purpose_matcher=DictionaryMatch(d=['in this paper','in the paper',' that can ','in this study','to examine','we examine','to investigate','implications'],longest_match_only=True) \n",
    "\n",
    "# Unit test\n",
    "# non_comma_dict_purpose_matcher=CandidateExtractor(Purpose, [ngrams], [Intersection(non_comma_matcher,other_regex_matcher)])\n",
    "\n",
    "non_comma_dict_purpose_matcher=CandidateExtractor(Purpose, [ngrams], [Intersection(non_comma_matcher,Union(comparative_degree_matcher,other_regex_matcher,dict_purpose_matcher,transition_matcher))]) #,intersection(excluded_dict_purpose_matcher,transition_regex_matcher)])\n",
    "purpose_cands=extract_and_display(non_comma_dict_purpose_matcher,Purpose,\"Purpose\",document_breakdown_map)\n",
    "# print(document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 54.1 s, sys: 852 ms, total: 54.9 s\n",
      "Wall time: 55.7 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1404**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 7.21 s, sys: 422 ms, total: 7.63 s\n",
      "Wall time: 7.52 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 169**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 475 ms, sys: 103 ms, total: 578 ms\n",
      "Wall time: 609 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1404 Candidate/Span:**\t`Mechanism(Span(\"b'We introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses.'\", sentence=11021, chars=[0,131], words=[0,22]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tWe introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10fe465c0>, 'name': '5834868625ff05a97b011292', 'stable_id': '5834868625ff05a97b011292::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1521, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b011292,0,b'We introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses.'), Sentence(Document 5834868625ff05a97b011292,1,b'Our approach is completely unsupervised, and our experiments show that Neural Network models perform much better (about 20% more accurate) than traditional feature-rich baseline models.'), Sentence(Document 5834868625ff05a97b011292,2,b'We also present a new dataset for Biomedical Language Processing which, with only about 25% of the original corpus vocabulary, still captures the essential distributional semantics of the corpus.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1404 Candidate/Span:**\t`Mechanism(Span(\"b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n'\", sentence=18541, chars=[0,198], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tAn algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2a2d30>, 'name': '5834868825ff05a97b01794c', 'stable_id': '5834868825ff05a97b01794c::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3693, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01794c,0,b'In this paper we describe an approach towards integrating mid-range sensing data into a dynamic path planning algorithm.'), Sentence(Document 5834868825ff05a97b01794c,1,b'The key problem, sensing for planning is addressed in the context of outdoor navigation.'), Sentence(Document 5834868825ff05a97b01794c,2,b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1404 Candidate/Span:**\t`Mechanism(Span(\"b'Our approach appears more practical than previous metamodule-based approaches.'\", sentence=17754, chars=[0,77], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOur approach appears more practical than previous metamodule-based approaches."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10fc3b9e8>, 'name': '5834868825ff05a97b0175d2', 'stable_id': '5834868825ff05a97b0175d2::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3472, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b0175d2,0,b'Abstract\\xe2\\x80\\x93'), Sentence(Document 5834868825ff05a97b0175d2,1,b'We describe a new set of prismatic movement primitives for cubic modular robots.'), Sentence(Document 5834868825ff05a97b0175d2,2,b'Our approach appears more practical than previous metamodule-based approaches.'), Sentence(Document 5834868825ff05a97b0175d2,3,b\"We also describe recent hardware developments in our cu-bic robot modules that have sufficient stiffness and actuator strength so that when they work together they can realize, in earth's gravity, all of the motion primitives we describe here.\\n\")]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1404 Candidate/Span:**\t`Mechanism(Span(\"b'In contrast to existing approaches that rely only on computer vision'\", sentence=19748, chars=[0,67], words=[0,10]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b341208>, 'name': '5834868825ff05a97b01791b', 'stable_id': '5834868825ff05a97b01791b::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4036, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01791b,0,b'In robotics research, perception is one of the most challenging tasks.'), Sentence(Document 5834868825ff05a97b01791b,1,b'In contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates.'), Sentence(Document 5834868825ff05a97b01791b,2,b'To evaluate, we apply this idea to a door detection problem.'), Sentence(Document 5834868825ff05a97b01791b,3,b'A set of preliminary experiments has been completed using software agents with real vision data.'), Sentence(Document 5834868825ff05a97b01791b,4,b'Our results demonstrate that information inferred from teammate observations significantly improves the perception precision.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7be0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3157, 'split': 2, 'mechanism_cue_id': 26962}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7ef0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3158, 'split': 2, 'mechanism_cue_id': 28446}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7eb8>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3159, 'split': 2, 'mechanism_cue_id': 28447}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d0908>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3160, 'split': 2, 'mechanism_cue_id': 28448}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d0ac8>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3161, 'split': 2, 'mechanism_cue_id': 28449}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d07f0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3162, 'split': 2, 'mechanism_cue_id': 28450}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f8080>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3163, 'split': 2, 'mechanism_cue_id': 28452}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f80f0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3164, 'split': 2, 'mechanism_cue_id': 28453}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f8160>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3165, 'split': 2, 'mechanism_cue_id': 28454}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f81d0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3166, 'split': 2, 'mechanism_cue_id': 28451}\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Mechanism: \n",
    "Mechanism = candidate_subclass('Mechanism', ['mechanism_cue']) \n",
    "dict_mechanism_matcher=DictionaryMatch(d=['introduce','introduces','propose','proposes','we propose','we develop','approach'],longest_match_only=True) \n",
    "non_comma_dict_mechanism_matcher=CandidateExtractor(Mechanism, [ngrams], [Intersection(non_comma_matcher,dict_mechanism_matcher)])\n",
    "mechanism_cands=extract_and_display(non_comma_dict_mechanism_matcher,Mechanism,\"Mechanism\",document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 53 s, sys: 864 ms, total: 53.9 s\n",
      "Wall time: 54.4 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 752**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6.82 s, sys: 364 ms, total: 7.18 s\n",
      "Wall time: 6.96 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 81**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 428 ms, sys: 71.6 ms, total: 499 ms\n",
      "Wall time: 464 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/752 Candidate/Span:**\t`Method(Span(\"b'The speech-to-speech translation is bi-directional for a two way dialog between participants.\\n'\", sentence=10296, chars=[0,93], words=[0,19]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe speech-to-speech translation is bi-directional for a two way dialog between participants.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10ff0a908>, 'name': '5834868425ff05a97b00e5b3', 'stable_id': '5834868425ff05a97b00e5b3::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1310, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00e5b3,0,b'Jibbigo is a speech-to-speech translation application for iPhone, iPod touch, and iPad devices.'), Sentence(Document 5834868425ff05a97b00e5b3,1,b'Jibbigo allows the user to simply speak a sentence, and it speaks the sentence aloud in the other language, much like a personal human interpreter would.'), Sentence(Document 5834868425ff05a97b00e5b3,2,b'The speech-to-speech translation is bi-directional for a two way dialog between participants.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/752 Candidate/Span:**\t`Method(Span(\"b'Both computational feasibility and improvement of estimation is demonstrated in the experiments.\\n'\", sentence=6557, chars=[0,96], words=[0,13]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tBoth computational feasibility and improvement of estimation is demonstrated in the experiments.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10f01d470>, 'name': '5834868425ff05a97b00de91', 'stable_id': '5834868425ff05a97b00de91::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 271, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00de91,0,b'The concept of Support Vector Regression is extended to a more general class of convex cost functions.'), Sentence(Document 5834868425ff05a97b00de91,1,b'Moreover it is shown how the resulting convex constrained optimization problems can be efficiently solved by a Primal-Dual Interior Point path following method.'), Sentence(Document 5834868425ff05a97b00de91,2,b'Both computational feasibility and improvement of estimation is demonstrated in the experiments.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/752 Candidate/Span:**\t`Method(Span(\"b'Both experiments seemto reveal that the Metaphor has poor effectiveness.\\n'\", sentence=18726, chars=[0,72], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tBoth experiments seemto reveal that the Metaphor has poor effectiveness.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2b7e80>, 'name': '5834868825ff05a97b017851', 'stable_id': '5834868825ff05a97b017851::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3743, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b017851,0,b'The Metaphor is intended to contribute to the Agile Programming value of communication.'), Sentence(Document 5834868825ff05a97b017851,1,b'Previously, some of the author studied the Metaphor as a means of communication among team members and between them and clients.'), Sentence(Document 5834868825ff05a97b017851,2,b\"This paper examines the Metaphor's contribution to the software architecture.\"), Sentence(Document 5834868825ff05a97b017851,3,b'Both experiments seemto reveal that the Metaphor has poor effectiveness.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/752 Candidate/Span:**\t`Method(Span(\"b'The commonality amongst the experiments has permitted the ability to objectively...\\n'\", sentence=21988, chars=[0,83], words=[0,12]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe commonality amongst the experiments has permitted the ability to objectively...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b453c88>, 'name': '59364828c50f90cdd3aca7f8', 'stable_id': '59364828c50f90cdd3aca7f8::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4676, 'type': 'document', 'sentences': [Sentence(Document 59364828c50f90cdd3aca7f8,0,b'This paper presents an experimental evaluation and comparison of basic strategies that have been proposed for force control of robot manipulators.'), Sentence(Document 59364828c50f90cdd3aca7f8,1,b'This experimental review of force control methodologies is unique in its breadth--never has such a complete spectrum of strategies been experimentally compared on the same system.'), Sentence(Document 59364828c50f90cdd3aca7f8,2,b'The commonality amongst the experiments has permitted the ability to objectively...\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40710>, 'type': 'method', 'method_cue_cid': None, 'id': 4000, 'split': 2, 'method_cue_id': 26962}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40b00>, 'type': 'method', 'method_cue_cid': None, 'id': 4001, 'split': 2, 'method_cue_id': 29212}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40ba8>, 'type': 'method', 'method_cue_cid': None, 'id': 4002, 'split': 2, 'method_cue_id': 29213}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40748>, 'type': 'method', 'method_cue_cid': None, 'id': 4003, 'split': 2, 'method_cue_id': 29214}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa404e0>, 'type': 'method', 'method_cue_cid': None, 'id': 4004, 'split': 2, 'method_cue_id': 29215}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49400>, 'type': 'method', 'method_cue_cid': None, 'id': 4005, 'split': 2, 'method_cue_id': 29216}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa495c0>, 'type': 'method', 'method_cue_cid': None, 'id': 4006, 'split': 2, 'method_cue_id': 29217}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49668>, 'type': 'method', 'method_cue_cid': None, 'id': 4007, 'split': 2, 'method_cue_id': 29218}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49630>, 'type': 'method', 'method_cue_cid': None, 'id': 4008, 'split': 2, 'method_cue_id': 29219}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa494a8>, 'type': 'method', 'method_cue_cid': None, 'id': 4009, 'split': 2, 'method_cue_id': 29220}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compound Matcher for Method: \n",
    "Method = candidate_subclass('Method', ['method_cue'])\n",
    "\n",
    "dict_method_matcher=DictionaryMatch(d=['dataset','benchmark','experiment ','experiments',\"empirical\",\"participant\",\"survey\",\" conduct\",\" analyze\"],longest_match_only=True) \n",
    "\n",
    "non_comma_dict_method_matcher=CandidateExtractor(Method, [ngrams], [Intersection(non_comma_matcher,dict_method_matcher)])\n",
    "method_cands=extract_and_display(non_comma_dict_method_matcher,Method,\"Method\",document_breakdown_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 56.2 s, sys: 803 ms, total: 57 s\n",
      "Wall time: 57.5 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1753**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6.98 s, sys: 347 ms, total: 7.32 s\n",
      "Wall time: 7.12 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 176**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 469 ms, sys: 94.4 ms, total: 564 ms\n",
      "Wall time: 529 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 12**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1753 Candidate/Span:**\t`Finding(Span(\"b'The effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented.'\", sentence=5983, chars=[0,173], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10ea76da0>, 'name': '5834868425ff05a97b00c935', 'stable_id': '5834868425ff05a97b00c935::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 117, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00c935,0,b'This paper proposes a fifteen level H-Bridge cascaded multilevel inverter with fundamental frequency switching for low power applications such as solar powered power supplies, battery powered standby power supplies.'), Sentence(Document 5834868425ff05a97b00c935,1,b'The effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented.'), Sentence(Document 5834868425ff05a97b00c935,2,b'Experimental...\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1753 Candidate/Span:**\t`Finding(Span(\"b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n'\", sentence=18541, chars=[0,198], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tAn algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2a2d30>, 'name': '5834868825ff05a97b01794c', 'stable_id': '5834868825ff05a97b01794c::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3693, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01794c,0,b'In this paper we describe an approach towards integrating mid-range sensing data into a dynamic path planning algorithm.'), Sentence(Document 5834868825ff05a97b01794c,1,b'The key problem, sensing for planning is addressed in the context of outdoor navigation.'), Sentence(Document 5834868825ff05a97b01794c,2,b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1753 Candidate/Span:**\t`Finding(Span(\"b'a people finder application'\", sentence=23141, chars=[123,149], words=[18,21]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tMore specifically, we have developed and evaluated three different applications, including a contextual instant messenger, a people finder application, and a phone-based application for access control."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b4e66a0>, 'name': '59382e72c50f90cdd3ada1ca', 'stable_id': '59382e72c50f90cdd3ada1ca::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5003, 'type': 'document', 'sentences': [Sentence(Document 59382e72c50f90cdd3ada1ca,0,b'We describe our current work in developing novel mechanisms for managing security and privacy in pervasive computing environments.'), Sentence(Document 59382e72c50f90cdd3ada1ca,1,b'More specifically, we have developed and evaluated three different applications, including a contextual instant messenger, a people finder application, and a phone-based application for access control.'), Sentence(Document 59382e72c50f90cdd3ada1ca,2,b'We also draw out some themes we have learned thus far for user-controllable security and privacy.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1753 Candidate/Span:**\t`Finding(Span(\"b'We show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\\n'\", sentence=24335, chars=[0,241], words=[0,45]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tWe show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b5852b0>, 'name': '595d391dc50f90cdd3bdeda2', 'stable_id': '595d391dc50f90cdd3bdeda2::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5357, 'type': 'document', 'sentences': [Sentence(Document 595d391dc50f90cdd3bdeda2,0,b'We empirically explore the application of (adaptations of) these algorithms to the kidney exchange problem, where patients with end-stage renal failure swap willing but incompatible donors.'), Sentence(Document 595d391dc50f90cdd3bdeda2,1,b'We show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff899e8>, 'type': 'finding', 'id': 5939, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30865}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff89a58>, 'type': 'finding', 'id': 5940, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30866}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82f28>, 'type': 'finding', 'id': 5941, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 29216}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82ef0>, 'type': 'finding', 'id': 5942, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30867}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff828d0>, 'type': 'finding', 'id': 5943, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30868}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82f60>, 'type': 'finding', 'id': 5944, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30869}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82128>, 'type': 'finding', 'id': 5945, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30870}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff825f8>, 'type': 'finding', 'id': 5946, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30872}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82438>, 'type': 'finding', 'id': 5947, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30871}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82160>, 'type': 'finding', 'id': 5948, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30873}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82be0>, 'type': 'finding', 'id': 5949, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30874}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82400>, 'type': 'finding', 'id': 5950, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30864}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compound Matcher for Finding: \n",
    "Finding = candidate_subclass('Finding', ['finding_cue'])\n",
    "\n",
    "dict_finding_matcher=DictionaryMatch(d=['show that','shows that','found','indicate','results','performance','find'],longest_match_only=True) \n",
    "\n",
    "non_comma_dict_finding_matcher=CandidateExtractor(Finding, [ngrams], [Intersection(non_comma_matcher,dict_finding_matcher)])\n",
    "finding_cands=extract_and_display(non_comma_dict_finding_matcher,Finding,\"Finding\",document_breakdown_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the ``Document Aggregation`` phase. First we show a few document examples, of how ``document_breakdown_map`` looks like, as well as how do we aggregate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Document 0/3039\n",
      "Extracted segments\n",
      "\n",
      " 59398333c50f90cdd3ae2761 {'Background': [Background(Span(\"b'recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'\", sentence=24132, chars=[83,244], words=[13,36]))], 'Purpose': [Purpose(Span(\"b'we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'\", sentence=24132, chars=[80,244], words=[12,36]))], 'Mechanism': [Mechanism(Span(\"b'We developed Cairo to fill this need.'\", sentence=24133, chars=[0,36], words=[0,7]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 59398333c50f90cdd3ae2761,0,b'While developing a suite of tools for statistical machine translation research, we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'), Sentence(Document 59398333c50f90cdd3ae2761,1,b'We developed Cairo to fill this need.'), Sentence(Document 59398333c50f90cdd3ae2761,2,b'Cairo is a free, open-source, portable, user-friendly, GUI-driven program written in Java that provides a visual representation of word correspondences between bilingual pairs of sentences, as well as relevant translation model parameters.\\n')]\n",
      "\n",
      "\n",
      "Document 1/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868725ff05a97b014e76 {'Background': [Background(Span(\"b'Researchers in psychometrics argue that before adding new labels to applications'\", sentence=12535, chars=[0,79], words=[0,10]))], 'Purpose': [Purpose(Span(\"b'In this paper'\", sentence=12536, chars=[0,12], words=[0,2]))], 'Method': [Method(Span(\"b'the labels must be empirically evaluated.'\", sentence=12535, chars=[82,122], words=[12,18])), Method(Span(\"b'We also show how we evaluate the labels empirically using a sample population from Amazon Mechanical Turk users.\\n'\", sentence=12537, chars=[0,112], words=[0,19]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868725ff05a97b014e76,0,b'Linguistic labels such as high, medium, and low are commonly used in different applications.'), Sentence(Document 5834868725ff05a97b014e76,1,b'Researchers in psychometrics argue that before adding new labels to applications, the labels must be empirically evaluated.'), Sentence(Document 5834868725ff05a97b014e76,2,b'In this paper, we explain the process of selecting labels for a security assessment application.'), Sentence(Document 5834868725ff05a97b014e76,3,b'We also show how we evaluate the labels empirically using a sample population from Amazon Mechanical Turk users.\\n')]\n",
      "\n",
      "\n",
      "Document 2/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868625ff05a97b013640 {'Background': [Background(Span(\"b'Previous work explored semantic concepts for content analysis to assist retrieval.'\", sentence=12041, chars=[0,81], words=[0,11]))], 'Purpose': [Purpose(Span(\"b'However'\", sentence=12042, chars=[0,6], words=[0,0]))], 'Mechanism': [Mechanism(Span(\"b'we propose a semi-automatic framework to discover the semantic concepts.\\n'\", sentence=12044, chars=[25,97], words=[5,18]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868625ff05a97b013640,0,b'Huge amount of videos on the Internet have rare textual information, which makes video retrieval challenging given a text query.'), Sentence(Document 5834868625ff05a97b013640,1,b'Previous work explored semantic concepts for content analysis to assist retrieval.'), Sentence(Document 5834868625ff05a97b013640,2,b\"However, the human-defined concepts might fail to cover the data and there is a potential gap between these concepts and the semantics expected from user's query.\"), Sentence(Document 5834868625ff05a97b013640,3,b'Also, building a corpus is expensive and time-consuming.'), Sentence(Document 5834868625ff05a97b013640,4,b'To address these issues, we propose a semi-automatic framework to discover the semantic concepts.\\n')]\n",
      "\n",
      "\n",
      "Document 3/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868625ff05a97b011a7a {'Background': [Background(Span(\"b'Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life'\", sentence=10746, chars=[0,116], words=[0,18]))], 'Purpose': [Purpose(Span(\"b'Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life'\", sentence=10746, chars=[0,116], words=[0,18])), Purpose(Span(\"b'However'\", sentence=10747, chars=[0,6], words=[0,0]))], 'Mechanism': [Mechanism(Span(\"b'Problem-solving approaches differ radically'\", sentence=10748, chars=[0,42], words=[0,5]))], 'Finding': [Finding(Span(\"b'and finding common ground for assessing results can be difficult.\\n'\", sentence=10748, chars=[45,110], words=[7,18]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868625ff05a97b011a7a,0,b\"Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life, they've made multidisciplinary teams the norm rather than the exception.\"), Sentence(Document 5834868625ff05a97b011a7a,1,b'However, overcoming boundaries between various disciplines is a significant challenge and in many cases represents a key factor for successful development.'), Sentence(Document 5834868625ff05a97b011a7a,2,b'Problem-solving approaches differ radically, and finding common ground for assessing results can be difficult.\\n')]\n"
     ]
    }
   ],
   "source": [
    "for ind,docid in enumerate(document_breakdown_map.keys()):\n",
    "    print(\"\\n\\nDocument \"+str(ind)+\"/\"+str(len(document_breakdown_map.keys())))\n",
    "    print(\"Extracted segments\\n\\n\",docid,document_breakdown_map[docid],\"\\n\\n\")\n",
    "    print(\"Complete abstract\\n\\n\",document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent().sentences)\n",
    "    if ind>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we count the number of documents that contain exactly N segments, where N=1, 2, 3, 4, 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_text(doc):\n",
    "    text_string=\" \".join([str(sentence.text) for sentence in doc.sentences])\n",
    "    return text_string\n",
    "\n",
    "\n",
    "def rank_by_matched_segments(document_breakdown_map):\n",
    "    for n in [5,4,3,2,1]:\n",
    "        print(\"Below are one or two document examples that contain exactly \"+str(n)+\" segments\\n\")\n",
    "        showed_examples=0\n",
    "        count=0\n",
    "        for ind,docid in enumerate(document_breakdown_map.keys()):\n",
    "            if len(document_breakdown_map[docid].keys())==n:\n",
    "                count+=1\n",
    "                if showed_examples<2:\n",
    "                    showed_examples+=1\n",
    "                    print(docid,\": \", get_document_text(document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent()))\n",
    "                    print(\"Extracted segments\\n\\n\",docid,document_breakdown_map[docid],\"\\n\\n\")\n",
    "        print(\"Total count is \"+str(count)+\"\\n\\n=================\\n\")\n",
    "        \n",
    "# rank_by_matched_segments(document_breakdown_map)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to some visualization of extracted Span as highlighted text on Document.\n",
    "\n",
    "Color notation: <b style=\"color:orange;\">Background</b> <b style=\"color:pink;\">Purpose</b> <b style=\"color:green;\">Mechanism</b> <b style=\"color:purple;\">Method</b> <b style=\"color:blue;\">Findings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "    \n",
    "def print_colored_text(docid,document_breakdown_map=document_breakdown_map):\n",
    "    \n",
    "    color_mapping={\"Background\":\"orange\",\"Purpose\":\"pink\",\"Mechanism\":\"green\",\"Method\":\"purple\",\"Finding\":\"blue\"}\n",
    "    this_document=document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent()\n",
    "    document_text=get_document_text(this_document)\n",
    "\n",
    "    added_segment=[]\n",
    "    print(\"This document has \"+str(len(document_breakdown_map[docid]))+\" spans\")\n",
    "    for segment in document_breakdown_map[docid]:\n",
    "        spans=document_breakdown_map[docid][segment]\n",
    "#         print(segment)\n",
    "        for ind,span in enumerate(spans):\n",
    "#             print(span.__dict__)\n",
    "            this_span=document_breakdown_map[docid][segment][ind].__dict__[list(document_breakdown_map[docid][segment][ind].__dict__)[6]]\n",
    "            span_sentence_text=this_span.sentence.text\n",
    "            span_text=str(span_sentence_text[this_span.char_start:(this_span.char_end+1)])\n",
    "#             print(\"span_text is \"+span_text)\n",
    "            document_text=document_text.replace(span_text.strip(),\"<font color='\"+color_mapping[segment]+\"'>\"+span_text.strip()+\"</font>\")\n",
    "\n",
    "    printmd(document_text)\n",
    "\n",
    "# docid='5834868625ff05a97b013016'\n",
    "# print_colored_text(docid)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document has 2 spans\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Security requirements analysis depends on how well-trained analysts perceive security risk, understand the impact of various vulnerabilities, and mitigate threats. When systems are composed of multiple machines, configurations, and software components that interact with each other, risk perception must account for the composition of security requirements. In this paper, we report on how changes to security requirements affect analysts risk perceptions and their decisions about how to modify the requirements to reach adequate security levels. <font color='purple'>We conducted two user surveys of 174 participants wherein participants assess security levels across 64 factorial vignettes.</font> <font color='purple'><font color='blue'>We analyzed the survey results using multi-level modeling to test for the effect of security requirements composition on participants' overall security adequacy ratings and on their ratings of individual requirements.</font></font> We accompanied this analysis with grounded analysis of elicited requirements aimed at lowering the security risk. <font color='blue'>Our results suggest that requirements composition affects experts' adequacy ratings on security requirements.</font> In addition, we identified three categories of requirements modifications, called refinements, replacements and reinforcements, and we measured how these categories compare with overall perceived security risk. Finally, we discuss the future impact of our work in security requirements assessment practice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_background_cands=extract_and_display(non_comma_dict_background_matcher,Background,\"Background\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_purpose_cands=extract_and_display(non_comma_dict_purpose_matcher,Purpose,\"Purpose\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_mechanism_cands=extract_and_display(non_comma_dict_mechanism_matcher,Mechanism,\"Mechanism\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_method_cands=extract_and_display(non_comma_dict_method_matcher,Method,\"Method\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_finding_cands=extract_and_display(non_comma_dict_finding_matcher,Finding,\"Finding\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "\n",
    "print_colored_text('cscw18assessment',document_breakdown_map=test_doc_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
