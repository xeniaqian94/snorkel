{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run 5: Loose Candidates and Crowd Labeling Function\n",
    "\n",
    "### Part I: Load loose candidates (prerequisite load documents from Test Run 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "debug_mode=1 # if not, debug_mode=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 5560\n",
      "Sentences: 18880\n",
      "The longest sentence has 157 tokens.\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "session = SnorkelSession()\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "\n",
    "sents = session.query(Sentence).all()\n",
    "n_max_corpus=0\n",
    "for sent in sents:\n",
    "    n_max_corpus=max(n_max_corpus,len(sent.words))\n",
    "\n",
    "print(\"The longest sentence has \"+str(n_max_corpus)+\" tokens.\")\n",
    "\n",
    "from snorkel.models import Document\n",
    "# from util import number_of_people\n",
    "\n",
    "docs = session.query(Document).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if i % 10 == 8 and \"cscw18\"!=doc.name[:6]:\n",
    "#             dev_sents.add(s)\n",
    "            continue\n",
    "#         elif i % 10 == 9:\n",
    "        elif \"cscw18\"==doc.name[:6]:  # replace the earlier 10% test documents as cscw'18 annotation guideline 10 examples\n",
    "            test_sents.add(s)\n",
    "        elif \"cscw18\"!=doc.name[:6]:\n",
    "            train_sents.add(s)\n",
    "            \n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 8s, sys: 730 ms, total: 1min 9s\n",
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 31595**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 11.9 ms, sys: 1.88 ms, total: 13.7 ms\n",
      "Wall time: 14.2 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 0**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 617 ms, sys: 50.7 ms, total: 668 ms\n",
      "Wall time: 660 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 138**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/31595 Candidate/Span:**\t`Segment(Span(\"b'. .'\", sentence=8382, chars=[0,2], words=[0,1]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\t. ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x115af4cc0>, 'name': '5834868425ff05a97b00e493', 'stable_id': '5834868425ff05a97b00e493::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 796, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00e493,0,b'SPECIAL ISSUE PAPERS Enabling Multimodal Human\\xe2\\x80\\x93'), Sentence(Document 5834868425ff05a97b00e493,1,b'Robot Interaction for the Karlsruhe Humanoid \\\\nRobot . .'), Sentence(Document 5834868425ff05a97b00e493,2,b'. .'), Sentence(Document 5834868425ff05a97b00e493,3,b'. .'), Sentence(Document 5834868425ff05a97b00e493,4,b'. .'), Sentence(Document 5834868425ff05a97b00e493,5,b'. .'), Sentence(Document 5834868425ff05a97b00e493,6,b'. .'), Sentence(Document 5834868425ff05a97b00e493,7,b'. .'), Sentence(Document 5834868425ff05a97b00e493,8,b'. .'), Sentence(Document 5834868425ff05a97b00e493,9,b'. .'), Sentence(Document 5834868425ff05a97b00e493,10,b'. .'), Sentence(Document 5834868425ff05a97b00e493,11,b'. .'), Sentence(Document 5834868425ff05a97b00e493,12,b'. .'), Sentence(Document 5834868425ff05a97b00e493,13,b'. .'), Sentence(Document 5834868425ff05a97b00e493,14,b'. .'), Sentence(Document 5834868425ff05a97b00e493,15,b'. .'), Sentence(Document 5834868425ff05a97b00e493,16,b'. .'), Sentence(Document 5834868425ff05a97b00e493,17,b'. .'), Sentence(Document 5834868425ff05a97b00e493,18,b'. .'), Sentence(Document 5834868425ff05a97b00e493,19,b'. .'), Sentence(Document 5834868425ff05a97b00e493,20,b'. .'), Sentence(Document 5834868425ff05a97b00e493,21,b'. .'), Sentence(Document 5834868425ff05a97b00e493,22,b'. .'), Sentence(Document 5834868425ff05a97b00e493,23,b'. .'), Sentence(Document 5834868425ff05a97b00e493,24,b'. .'), Sentence(Document 5834868425ff05a97b00e493,25,b'.'), Sentence(Document 5834868425ff05a97b00e493,26,b'.'), Sentence(Document 5834868425ff05a97b00e493,27,b'R. Stiefelhagen, HK Ekenel, \\\\nC. F\\xc3\\xbcgen, P. Gieselmann, H. Holzapfel, F. Kraft, K. Nickel, M. Voit, and A. Waibel   Human-Oriented \\\\nInteraction With an Anthropomorphic Robot . . . . . . . . . . . . . . . . . .'), Sentence(Document 5834868425ff05a97b00e493,28,b'.'), Sentence(Document 5834868425ff05a97b00e493,29,b'TP Spexard, M. Hanheide, and \\\\nG. Sagerer   A Linear Affect\\xe2\\x80\\x93Expression Space Model and Control Points for Mascot-Type Facial \\\\nRobot . .'), Sentence(Document 5834868425ff05a97b00e493,30,b'. .'), Sentence(Document 5834868425ff05a97b00e493,31,b'. .'), Sentence(Document 5834868425ff05a97b00e493,32,b'. .'), Sentence(Document 5834868425ff05a97b00e493,33,b'. .'), Sentence(Document 5834868425ff05a97b00e493,34,b'. .'), Sentence(Document 5834868425ff05a97b00e493,35,b'. .'), Sentence(Document 5834868425ff05a97b00e493,36,b'. .'), Sentence(Document 5834868425ff05a97b00e493,37,b'. .'), Sentence(Document 5834868425ff05a97b00e493,38,b'. .'), Sentence(Document 5834868425ff05a97b00e493,39,b'. .'), Sentence(Document 5834868425ff05a97b00e493,40,b'. .'), Sentence(Document 5834868425ff05a97b00e493,41,b'. .'), Sentence(Document 5834868425ff05a97b00e493,42,b'. .'), Sentence(Document 5834868425ff05a97b00e493,43,b'. .'), Sentence(Document 5834868425ff05a97b00e493,44,b'. .'), Sentence(Document 5834868425ff05a97b00e493,45,b'. .'), Sentence(Document 5834868425ff05a97b00e493,46,b'. .'), Sentence(Document 5834868425ff05a97b00e493,47,b'. .'), Sentence(Document 5834868425ff05a97b00e493,48,b'. .'), Sentence(Document 5834868425ff05a97b00e493,49,b'. .'), Sentence(Document 5834868425ff05a97b00e493,50,b'. .'), Sentence(Document 5834868425ff05a97b00e493,51,b'. .'), Sentence(Document 5834868425ff05a97b00e493,52,b'. .'), Sentence(Document 5834868425ff05a97b00e493,53,b'. .'), Sentence(Document 5834868425ff05a97b00e493,54,b'. .'), Sentence(Document 5834868425ff05a97b00e493,55,b'. .'), Sentence(Document 5834868425ff05a97b00e493,56,b'. .'), Sentence(Document 5834868425ff05a97b00e493,57,b'. .'), Sentence(Document 5834868425ff05a97b00e493,58,b'. .'), Sentence(Document 5834868425ff05a97b00e493,59,b'. .'), Sentence(Document 5834868425ff05a97b00e493,60,b'. .'), Sentence(Document 5834868425ff05a97b00e493,61,b'. .'), Sentence(Document 5834868425ff05a97b00e493,62,b'. .'), Sentence(Document 5834868425ff05a97b00e493,63,b'. .'), Sentence(Document 5834868425ff05a97b00e493,64,b'. .'), Sentence(Document 5834868425ff05a97b00e493,65,b'. .'), Sentence(Document 5834868425ff05a97b00e493,66,b'.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/31595 Candidate/Span:**\t`Segment(Span(\"b'It is our great pleasure to warmly welcome you to the 2013 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2013).'\", sentence=17365, chars=[0,126], words=[0,26]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIt is our great pleasure to warmly welcome you to the 2013 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2013)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1163f7a58>, 'name': '5834868825ff05a97b016e23', 'stable_id': '5834868825ff05a97b016e23::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3451, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b016e23,0,b'It is our great pleasure to warmly welcome you to the 2013 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2013).'), Sentence(Document 5834868825ff05a97b016e23,1,b'WI 2013 is the 12th edition of its series.'), Sentence(Document 5834868825ff05a97b016e23,2,b'On behalf of the WI 2013 Conference Committees, we would like to thank you for your contributions and participation, and we do hope that you will enjoy the technical events and social programs, and will have wonderful time at Atlanta.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/31595 Candidate/Span:**\t`Segment(Span(\"b'and forty.'\", sentence=23479, chars=[54,63], words=[10,12]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tFifty six million, six hundred eighty seven thousand, and forty."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x121d05b00>, 'name': '5942b359c50f90cdd3b1f54d', 'stable_id': '5942b359c50f90cdd3b1f54d::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5276, 'type': 'document', 'sentences': [Sentence(Document 5942b359c50f90cdd3b1f54d,0,b'Fifty six million, six hundred eighty seven thousand, and forty.'), Sentence(Document 5942b359c50f90cdd3b1f54d,1,b'A big number, to be sure.'), Sentence(Document 5942b359c50f90cdd3b1f54d,2,b'This is the number of possible semantic analyses for an average sized sentence in the Mikrokosmos Machine Translation project.'), Sentence(Document 5942b359c50f90cdd3b1f54d,3,b'Complex sentences have gone past the trillions.'), Sentence(Document 5942b359c50f90cdd3b1f54d,4,b'If every combination could be accurately judged in one thousandth of a second, it would still take almost a day to analyze the average sentence.'), Sentence(Document 5942b359c50f90cdd3b1f54d,5,b'And you can forget about the hard ones.'), Sentence(Document 5942b359c50f90cdd3b1f54d,6,b'And yet, understanding natural language sentences is intuitively not an exponential a air.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/31595 Candidate/Span:**\t`Segment(Span(\"b'six hundred eighty seven thousand'\", sentence=23479, chars=[19,51], words=[4,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tFifty six million, six hundred eighty seven thousand, and forty."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x121d05b00>, 'name': '5942b359c50f90cdd3b1f54d', 'stable_id': '5942b359c50f90cdd3b1f54d::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5276, 'type': 'document', 'sentences': [Sentence(Document 5942b359c50f90cdd3b1f54d,0,b'Fifty six million, six hundred eighty seven thousand, and forty.'), Sentence(Document 5942b359c50f90cdd3b1f54d,1,b'A big number, to be sure.'), Sentence(Document 5942b359c50f90cdd3b1f54d,2,b'This is the number of possible semantic analyses for an average sized sentence in the Mikrokosmos Machine Translation project.'), Sentence(Document 5942b359c50f90cdd3b1f54d,3,b'Complex sentences have gone past the trillions.'), Sentence(Document 5942b359c50f90cdd3b1f54d,4,b'If every combination could be accurately judged in one thousandth of a second, it would still take almost a day to analyze the average sentence.'), Sentence(Document 5942b359c50f90cdd3b1f54d,5,b'And you can forget about the hard ones.'), Sentence(Document 5942b359c50f90cdd3b1f54d,6,b'And yet, understanding natural language sentences is intuitively not an exponential a air.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import *\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "\n",
    "def extract_and_display(matcher,candidate_class,candidate_class_name,document_breakdown_map=None,test_doc_breakdown_map=None,selected_split=0,is_print=True):  # split over train/dev/test but returns only train set\n",
    "#     input(candidate_class)\n",
    "    for (i, sents) in ([(0,train_sents), (1,dev_sents), (2,test_sents)] if selected_split==0 else ([(2,test_sents)] if selected_split==2 else [(1,dev_sents)])):\n",
    "        %time matcher.apply(sents, split=i)\n",
    "        printmd(\"**Split \"+str(i)+\" - number of candidates extracted: \"+str(session.query(candidate_class).filter(candidate_class.split == i).count())+\"**\\n\\n\")\n",
    "    cands = session.query(candidate_class).filter(candidate_class.split == selected_split).all()\n",
    "    if is_print:\n",
    "        for i in range(min(4,len(cands))): # to print at most 4 cands \n",
    "            printmd(\"**\"+str(i)+\"/\"+str(len(cands))+\" Candidate/Span:**\\t`\"+str(cands[i])+\"`\")\n",
    "            printmd(\"**Its parent Sentence's text:**\\t\"+str(cands[i].get_parent().text))\n",
    "            printmd(\"**Its parent Document's text:**\\t\"+str(cands[i].get_parent().get_parent().__dict__))\n",
    "            print() \n",
    "        \n",
    "    for cand in cands:\n",
    "#         print(cand.__dict__)\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in document_breakdown_map:\n",
    "            document_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in document_breakdown_map[doc_name]:\n",
    "            document_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        document_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "        \n",
    "    cands = session.query(candidate_class).filter(candidate_class.split == 2).all()\n",
    "    for cand in cands:\n",
    "#         print(cand.__dict__)\n",
    "        doc_name=cand.get_parent().get_parent().name\n",
    "        if doc_name not in test_doc_breakdown_map:\n",
    "            test_doc_breakdown_map[doc_name]=dict()\n",
    "        if candidate_class_name not in test_doc_breakdown_map[doc_name]:\n",
    "            test_doc_breakdown_map[doc_name][candidate_class_name]=[]\n",
    "        test_doc_breakdown_map[doc_name][candidate_class_name]+=[cand]\n",
    "    \n",
    "    return cands\n",
    "\n",
    "\n",
    "# In this section, we want to split sentences into comma-separated candidates (named as segments)\n",
    "# In train split, we have number of candidates extracted: 31595, corresponding to 5560 * 0.9 = 5004 Documents\n",
    "Segment = candidate_subclass('Segment', ['segment_cue'])\n",
    "ngrams = Ngrams(n_max=n_max_corpus) # we define the maximum n value as n_max_corpus\n",
    "non_comma_matcher=DictionaryMatch(d=[','],longest_match_only=True,reverse=True)  \n",
    "non_comma_segment_extractor=CandidateExtractor(Segment, [ngrams], [non_comma_matcher])\n",
    "train_doc_breakdown_map=dict() # maps doc_id into a dict of [\"Background\", \"Purpose\", \"Mechanism\", \"Method\", \"Finding\"]\n",
    "test_doc_breakdown_map=dict()\n",
    "segments=extract_and_display(non_comma_segment_extractor,Segment,\"Segment\",train_doc_breakdown_map,test_doc_breakdown_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-10-3bd5f0d07104>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-3bd5f0d07104>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    segments=session.query(Segment).filter(Segment.split=0).all()\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "segments=session.query(Segment).filter(Segment.split==0).all()\n",
    "segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Loading Gold Labels (Encountered some issue and slightly deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'snorkel.annotations.csr_LabelMatrix'>\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "from snorkel.annotations import load_gold_labels\n",
    "# if load the 1st time\n",
    "# purposes = load_external_labels(session, Segment, annotator_name='background')\n",
    "\n",
    "# else:\n",
    "gold_dev_purposes_mixed_w_backgrounds=load_gold_labels(session, annotator_name='background', split=1)\n",
    "print(type(gold_dev_purposes_mixed_w_backgrounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Defining Labeling Function (for Purpose and Mechanism) \n",
    "\n",
    "Recall a labeling function is just a Python function that accepts a Candidate (in our case, Segments), and returns 1 to mark the Candidate as true, -1 to mark the Candidate as false, and 0 to abstain from labeling the Candidate. Let's start by defining some Purpose labeling function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from snorkel.lf_helpers import *\n",
    "\n",
    "def LF_expressing_contrast(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"((^|\\s)(however|nevertheless|despite|spite|yet).*$)|((^|\\s)but(?!(also))*$)\",1) else 0\n",
    "\n",
    "def LF_excluded_pseudo_contrast(c):\n",
    "    return -1 if rule_regex_search_candidate_text(c,\"(^.*but sometimes.*$)|(^.*but also.*$)\",1) else 0\n",
    "\n",
    "def LF_comparative_degree(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*more.*than.*$)|(.*er than.*$)\",1) else 0\n",
    "\n",
    "def LF_purpose_verb(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*in order to.*$)|(.* implication.*$)|(.* to solve.*$)|(.* hypothesis.*$)|(.*to enable.*$)|(.*to aid.*$)|(.*to produce.*$)|(.*to investigat.*$)|(.* give.*$)|(.* that can .*$)|(.* examine.*$)|(.* extend.*$)|(.* offer.*$)\",1) else 0\n",
    "\n",
    "def LF_purpose_delimiter(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*in this paper.*$)|(.*in the paper.*$)|(.*in this study.*$)\",1) else 0\n",
    "\n",
    "def LF_purpose_adj_problem(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.* challenging.*$)|(.* captivating.*$)|(.* engaging.*$)|(.* interesting.*$)\",1) else 0\n",
    "\n",
    "def LF_purpose_leading_question_word(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"(.*how to.*$)|(.*what .*$)|(.*why .*$)\",1) else 0\n",
    "\n",
    "\n",
    "def LF_mechanism_verb(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"((^|\\s)(introduce|propose|develop|approach|applied|apply|using|present|contribute|build|built).*$)\",1) else 0\n",
    "\n",
    "# def LF_not_purpose_but_mechanism_verb(c):\n",
    "#     return 0 if rule_regex_search_candidate_text(c,\"((^|\\s)(introduce|propose|develop|approach|applied|apply|using|present|contribute|build|built).*$)\",1) else 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def LF_mechanism_adv(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"((^|\\s)(specifically|particularly|particular).*$)\",1) else 0\n",
    "\n",
    "def LF_mechanism_noun(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"((^|\\s)(framework|algorithm|model|module|mechanism).*$)\",1) else 0\n",
    "\n",
    "def LF_mechanism_adj(c):\n",
    "    return 1 if rule_regex_search_candidate_text(c,\"((^|\\s)(simple|general|effective|efficient).*$)\",1) else 0\n",
    "\n",
    "labeled = []\n",
    "# negate=lambda x: -x\n",
    "\n",
    "def negate(f):\n",
    "    return lambda x: -f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First we examine the metrics for Purpose LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 13s, sys: 819 ms, total: 2min 14s\n",
      "Wall time: 2min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<31595x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6942 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "purpose_LFs=[LF_expressing_contrast,LF_excluded_pseudo_contrast,LF_comparative_degree,LF_purpose_verb,LF_purpose_delimiter,LF_purpose_adj_problem,LF_purpose_leading_question_word,negate(LF_mechanism_verb),negate(LF_mechanism_adv),negate(LF_mechanism_noun),negate(LF_mechanism_adj)]\n",
    "\n",
    "labeler = LabelAnnotator(lfs=purpose_LFs)\n",
    "\n",
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a peek at one of the labelled candidates and which LF does it come from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random candidate Segment(Span(\"b'Kinsey (1984) defined noncompliance with tax laws as the \\xe2\\x80\\x9cfailure'\", sentence=13067, chars=[0,64], words=[0,12]))\n",
      "Its LabelKey LabelKey (LF_purpose_verb)\n"
     ]
    }
   ],
   "source": [
    "print(\"A random candidate\",L_train.get_candidate(session, 3))\n",
    "print(\"Its LabelKey\",L_train.get_key(session, 3))\n",
    "# print(\"Annotations summary\",L_train.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view statistics about the resulting label matrix.\n",
    "\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a *conflicting* non-zero label for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_expressing_contrast</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_excluded_pseudo_contrast</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_comparative_degree</th>\n",
       "      <td>2</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_verb</th>\n",
       "      <td>3</td>\n",
       "      <td>0.034879</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.006552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_delimiter</th>\n",
       "      <td>4</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.003482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_adj_problem</th>\n",
       "      <td>5</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_purpose_leading_question_word</th>\n",
       "      <td>6</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.001551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>7</td>\n",
       "      <td>0.136192</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.012597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  j  Coverage  Overlaps  Conflicts\n",
       "LF_expressing_contrast            0  0.011173  0.000475   0.000317\n",
       "LF_excluded_pseudo_contrast       1  0.001013  0.000158   0.000095\n",
       "LF_comparative_degree             2  0.007501  0.001139   0.000855\n",
       "LF_purpose_verb                   3  0.034879  0.008134   0.006552\n",
       "LF_purpose_delimiter              4  0.017060  0.003925   0.003482\n",
       "LF_purpose_adj_problem            5  0.003482  0.000728   0.000506\n",
       "LF_purpose_leading_question_word  6  0.008419  0.002595   0.001551\n",
       "<lambda>                          7  0.136192  0.012660   0.012597"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next we examine the metrics for Mechanism LFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 2min 46s, sys: 2.69 s, total: 2min 49s\n",
      "Wall time: 2min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<31595x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8523 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "mechanism_LFs=[LF_mechanism_verb, LF_mechanism_adv,LF_mechanism_noun, LF_mechanism_adj, negate(LF_expressing_contrast),negate(LF_excluded_pseudo_contrast),negate(LF_comparative_degree),negate(LF_purpose_verb),negate(LF_purpose_delimiter),negate(LF_purpose_adj_problem),negate(LF_purpose_leading_question_word)]\n",
    "labeler = LabelAnnotator(lfs=mechanism_LFs)\n",
    "\n",
    "np.random.seed(1702)\n",
    "%time L_train_mechanism = labeler.apply(split=0)\n",
    "L_train_mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_verb</th>\n",
       "      <td>0</td>\n",
       "      <td>0.136192</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_adv</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008831</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_noun</th>\n",
       "      <td>2</td>\n",
       "      <td>0.083209</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_mechanism_adj</th>\n",
       "      <td>3</td>\n",
       "      <td>0.030353</td>\n",
       "      <td>0.011774</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "      <td>4</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   j  Coverage  Overlaps  Conflicts\n",
       "LF_mechanism_verb  0  0.136192  0.031746   0.000317\n",
       "LF_mechanism_adv   1  0.008831  0.001677   0.000032\n",
       "LF_mechanism_noun  2  0.083209  0.030511   0.000285\n",
       "LF_mechanism_adj   3  0.030353  0.011774   0.000095\n",
       "<lambda>           4  0.011173  0.000696   0.000696"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train_mechanism.lf_stats(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting the Generative Model\n",
    "\n",
    "The training step for 70K papers take approximately 10 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()  # Inferred cardinality: 2 would be binary label (excluding the uncertain class)\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we quantify a few characteristics, (1) LF accuracies; (2) the marginal probability distributions of each candidate being True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07856624, 0.07481565, 0.07709562, 0.08353626, 0.07912881,\n",
       "       0.07455556, 0.07590932, 0.12232697])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEShJREFUeJzt3X+s3XV9x/Hna60wMzUUWwgpNWWmJqLZQBsgIVuYZlAwWTHTBZZINWx1BjLNzCL6DwY0wSXqQqZsOBtLoiLxR+i0ruuIzrgIchXGz5HeFSLXEqgWFeOmK773x/k0nPE57T29p+25tzwfyTfne97n8/nez/dzT/vq98c5TVUhSdKw35j2ACRJi4/hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpM7yaQ9goVauXFlr166d9jAkaclYuXIlO3bs2FFVG+Zru2TDYe3atczMzEx7GJK0pCRZOU47TytJkjqGgySpYzhIkjqGgySpYzhIkjqGgySpM284JFmT5BtJHk7yYJJ3t/oHk/wwyb1tuWSoz/uTzCZ5JMlFQ/UNrTab5Jqh+hlJ7kqyK8kXkpxwpHdUkjS+cY4c9gPvrapXA+cBVyU5s7328ao6qy3bAdprlwGvATYAn0yyLMky4BPAxcCZwOVD2/lI29Y64GngyiO0f5KkBZg3HKrqiar6flt/BngYWH2ILhuBW6vql1X1KDALnNOW2araXVW/Am4FNiYJ8Abgi63/VuDShe6QJGlyh/UJ6SRrgbOBu4DzgauTXAHMMDi6eJpBcNw51G2O58Lk8efVzwVeDvykqvaPaC9Nxdprvrbgvo/d8KYjOBJpOsa+IJ3kJcCXgPdU1c+Am4BXAmcBTwAfPdB0RPdaQH3UGDYnmUkys3fv3nGHLkk6TGOFQ5IXMQiGz1bVlwGq6smqeraqfg18isFpIxj8y3/NUPfTgT2HqP8IOCnJ8ufVO1V1c1Wtr6r1q1atGmfokqQFGOdupQCfBh6uqo8N1U8bavZm4IG2vg24LMmJSc4A1gHfBe4G1rU7k05gcNF6W1UV8A3gLa3/JuD2yXZLkjSJca45nA+8Dbg/yb2t9gEGdxudxeAU0GPAOwGq6sEktwEPMbjT6aqqehYgydXADmAZsKWqHmzbex9wa5IPAfcwCCNJ0pTMGw5V9W1GXxfYfog+HwY+PKK+fVS/qtrNc6elJElT5iekJUkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1Jk3HJKsSfKNJA8neTDJu1v95CQ7k+xqjytaPUluTDKb5L4krxva1qbWfleSTUP11ye5v/W5MUmOxs5KksYzzpHDfuC9VfVq4DzgqiRnAtcAd1TVOuCO9hzgYmBdWzYDN8EgTIBrgXOBc4BrDwRKa7N5qN+GyXdNkrRQ84ZDVT1RVd9v688ADwOrgY3A1tZsK3BpW98I3FIDdwInJTkNuAjYWVX7quppYCewob32sqr6TlUVcMvQtiRJU3BY1xySrAXOBu4CTq2qJ2AQIMAprdlq4PGhbnOtdqj63Ii6JGlKxg6HJC8BvgS8p6p+dqimI2q1gPqoMWxOMpNkZu/evfMNWZK0QGOFQ5IXMQiGz1bVl1v5yXZKiPb4VKvPAWuGup8O7JmnfvqIeqeqbq6q9VW1ftWqVeMMXZK0AOPcrRTg08DDVfWxoZe2AQfuONoE3D5Uv6LdtXQe8NN22mkHcGGSFe1C9IXAjvbaM0nOaz/riqFtSZKmYPkYbc4H3gbcn+TeVvsAcANwW5IrgR8Ab22vbQcuAWaBXwDvAKiqfUmuB+5u7a6rqn1t/V3AZ4AXA19viyRpSuYNh6r6NqOvCwC8cUT7Aq46yLa2AFtG1GeA1843FknSseEnpCVJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJnXnDIcmWJE8leWCo9sEkP0xyb1suGXrt/UlmkzyS5KKh+oZWm01yzVD9jCR3JdmV5AtJTjiSOyhJOnzjHDl8Btgwov7xqjqrLdsBkpwJXAa8pvX5ZJJlSZYBnwAuBs4ELm9tAT7StrUOeBq4cpIdkiRNbt5wqKpvAfvG3N5G4Naq+mVVPQrMAue0ZbaqdlfVr4BbgY1JArwB+GLrvxW49DD3QZJ0hE1yzeHqJPe1004rWm018PhQm7lWO1j95cBPqmr/8+ojJdmcZCbJzN69eycYuiTpUBYaDjcBrwTOAp4APtrqGdG2FlAfqapurqr1VbV+1apVhzdiSdLYli+kU1U9eWA9yaeAr7anc8CaoaanA3va+qj6j4CTkixvRw/D7SVJU7KgI4ckpw09fTNw4E6mbcBlSU5McgawDvgucDewrt2ZdAKDi9bbqqqAbwBvaf03AbcvZEySpCNn3iOHJJ8HLgBWJpkDrgUuSHIWg1NAjwHvBKiqB5PcBjwE7Aeuqqpn23auBnYAy4AtVfVg+xHvA25N8iHgHuDTR2zvJEkLMm84VNXlI8oH/Qu8qj4MfHhEfTuwfUR9N4O7mSRJi4SfkJYkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVJn3nBIsiXJU0keGKqdnGRnkl3tcUWrJ8mNSWaT3JfkdUN9NrX2u5JsGqq/Psn9rc+NSXKkd1KSdHjGOXL4DLDhebVrgDuqah1wR3sOcDGwri2bgZtgECbAtcC5wDnAtQcCpbXZPNTv+T9LknSMzRsOVfUtYN/zyhuBrW19K3DpUP2WGrgTOCnJacBFwM6q2ldVTwM7gQ3ttZdV1XeqqoBbhrYlSZqShV5zOLWqngBoj6e0+mrg8aF2c612qPrciPpISTYnmUkys3fv3gUOXZI0nyN9QXrU9YJaQH2kqrq5qtZX1fpVq1YtcIiSpPksNByebKeEaI9PtfocsGao3enAnnnqp4+oS5KmaKHhsA04cMfRJuD2ofoV7a6l84CfttNOO4ALk6xoF6IvBHa0155Jcl67S+mKoW1JkqZk+XwNknweuABYmWSOwV1HNwC3JbkS+AHw1tZ8O3AJMAv8AngHQFXtS3I9cHdrd11VHbjI/S4Gd0S9GPh6WyRJUzRvOFTV5Qd56Y0j2hZw1UG2swXYMqI+A7x2vnFIko4dPyEtSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkzkThkOSxJPcnuTfJTKudnGRnkl3tcUWrJ8mNSWaT3JfkdUPb2dTa70qyabJdkiRN6kgcOfxBVZ1VVevb82uAO6pqHXBHew5wMbCuLZuBm2AQJsC1wLnAOcC1BwJFkjQdR+O00kZga1vfClw6VL+lBu4ETkpyGnARsLOq9lXV08BOYMNRGJckaUyThkMB/5Lke0k2t9qpVfUEQHs8pdVXA48P9Z1rtYPVJUlTsnzC/udX1Z4kpwA7k/znIdpmRK0OUe83MAigzQCveMUrDneskqQxTXTkUFV72uNTwFcYXDN4sp0uoj0+1ZrPAWuGup8O7DlEfdTPu7mq1lfV+lWrVk0ydEnSISw4HJL8VpKXHlgHLgQeALYBB+442gTc3ta3AVe0u5bOA37aTjvtAC5MsqJdiL6w1SRJUzLJaaVTga8kObCdz1XVPye5G7gtyZXAD4C3tvbbgUuAWeAXwDsAqmpfkuuBu1u766pq3wTjkiRNaMHhUFW7gd8dUf8x8MYR9QKuOsi2tgBbFjoWSdKR5SekJUkdw0GS1DEcJEkdw0GS1Jn0Q3A6htZe87UF933shjcdwZFIOt555CBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSOX9ktHUf8WncdKR45SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqWM4SJI6hoMkqbNowiHJhiSPJJlNcs20xyNJL2SL4ltZkywDPgH8ITAH3J1kW1U9NN2RaZom+YZR8FtGpUksliOHc4DZqtpdVb8CbgU2TnlMkvSCtSiOHIDVwONDz+eAc6c0Fh1Bk/7rXzoUjy6PnsUSDhlRq65RshnY3J7+PMkjY25/JfCjBY7tuJCPHPSl43ZuDrHP41rQ3ByBnzsVhznu4+J9cxR+V4t9XsYe22IJhzlgzdDz04E9z29UVTcDNx/uxpPMVNX6hQ/v+OXcHJxzc3DOzWjH07wslmsOdwPrkpyR5ATgMmDblMckSS9Yi+LIoar2J7ka2AEsA7ZU1YNTHpYkvWAtinAAqKrtwPajtPnDPhX1AuLcHJxzc3DOzWjHzbykqrvuK0l6gVss1xwkSYvIkg+Hcb92I8lbklSS9UO197d+jyS56NiM+NhY6LwkWZvkv5Pc25a/P3ajPjbmm5skb0+yd2gO/mzotU1JdrVl07Ed+dE34dw8O1Q/7m4oGefPVJI/SfJQkgeTfG6ovvTeN1W1ZBcGF6//C/ht4ATgP4AzR7R7KfAt4E5gfaud2dqfCJzRtrNs2vu0COZlLfDAtPdhmnMDvB34uxF9TwZ2t8cVbX3FtPdpMcxNe+3n096HKc/NOuCeA+8J4JSl/L5Z6kcO437txvXA3wD/M1TbCNxaVb+sqkeB2ba948Ek83K8m+SrWi4CdlbVvqp6GtgJbDhK45wGv8bm4MaZmz8HPtHeG1TVU62+JN83Sz0cRn3txurhBknOBtZU1VcPt+8SNsm8AJyR5J4k/5bk947iOKdh3N/7Hye5L8kXkxz4gObx/J6ByeYG4DeTzCS5M8mlR3Wkx944c/Mq4FVJ/r3NwYbD6LvoLPVwOOTXbiT5DeDjwHsPt+8SN8m8PAG8oqrOBv4K+FySlx2VUU7HOL/3fwLWVtXvAP8KbD2MvkvZJHMDg/fNeuBPgb9N8sqjM8ypGGduljM4tXQBcDnwj0lOGrPvorPUw2G+r914KfBa4JtJHgPOA7a1i69jfWXHErXgeWmn2X4MUFXfY3Ce9VXHZNTHxry/96r6cVX9sj39FPD6cfsucZPMDVW1pz3uBr4JnH00B3uMjfO7nwNur6r/baeqH2EQFkvzfTPtix4TXiRazuDizhk8d5HoNYdo/02eu/D6Gv7/BendHD8XpCeZl1UH5oHBxbcfAidPe5+O5dwApw2tvxm4s62fDDzK4KLiirbu3AzWVwAntvWVwC5G3ASxVJcx52YDsHVoDh4HXr5U3zeL5hPSC1EH+dqNJNcBM1V10NvpWrvbgIeA/cBVVfXsMRn4UTbJvAC/D1yXZD/wLPAXVbXv6I/62Bhzbv4yyR8xeF/sY3CHDlW1L8n1DL4LDOA652YwN8CrgX9I8msGZyRuqOPoP+sac252ABcmeYjBn52/rnYUvhTfN35CWpLUWerXHCRJR4HhIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnq/B8nGvGGtkeLhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_marginals = gen_model.marginals(L_train)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.537883</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>0.542201</td>\n",
       "      <td>0.362007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.541904</td>\n",
       "      <td>0.6658</td>\n",
       "      <td>0.544689</td>\n",
       "      <td>0.360414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.538233</td>\n",
       "      <td>0.6722</td>\n",
       "      <td>0.537371</td>\n",
       "      <td>0.362206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.534887</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.533137</td>\n",
       "      <td>0.360414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529306</td>\n",
       "      <td>0.6688</td>\n",
       "      <td>0.531137</td>\n",
       "      <td>0.349861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.539391</td>\n",
       "      <td>0.6702</td>\n",
       "      <td>0.537954</td>\n",
       "      <td>0.357029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.534796</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>0.542962</td>\n",
       "      <td>0.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.565314</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>0.567007</td>\n",
       "      <td>0.386699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Coverage  Precision    Recall\n",
       "0  0.537883    0.6652   0.542201  0.362007\n",
       "1  0.541904    0.6658   0.544689  0.360414\n",
       "2  0.538233    0.6722   0.537371  0.362206\n",
       "3  0.534887    0.6736   0.533137  0.360414\n",
       "4  0.529306    0.6688   0.531137  0.349861\n",
       "5  0.539391    0.6702   0.537954  0.357029\n",
       "6  0.534796    0.6610   0.542962  0.354839\n",
       "7  0.565314    0.6752   0.567007  0.386699"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.learned_lf_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_candidates # 0\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 0.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 0 | FN: 0\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test one purpose LF over groundtruth ISSUE: gold_labels.tsv are context_stable_ids. How could we get that for our task \n",
    "\n",
    "from snorkel.lf_helpers import test_LF\n",
    "tp, fp, tn, fn = test_LF(session, LF_expressing_contrast, split=1, annotator_name='background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands=session.query(Segment).filter(Segment.split == 0).all()\n",
    "\n",
    "for ind,c in enumerate(cands): # session.query(Segment).filter(Segment.split == 0).all():\n",
    "    if ind%500==0:\n",
    "        print(\"Processed \", ind, len(cands))\n",
    "        \n",
    "    for lf in LFS:\n",
    "        if lf(c)!=0:\n",
    "            labeled.append(c)\n",
    "            continue\n",
    "        \n",
    "print(\"Number labeled:\", len(labeled),len(cands))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "labeled[4].get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "\n",
    "# Reference: \n",
    "# def load_external_labels(session, candidate_class, annotator_name='gold'):\n",
    "#  session.add(StableLabel(\n",
    "#                 context_stable_ids=context_stable_ids,\n",
    "#                 annotator_name=annotator_name, e.g. \"gold\"\n",
    "#                 value=row['label']))\n",
    "            \n",
    "            \n",
    "GoldBackground = candidate_subclass('GoldBackground', ['goldbackground_cue'])\n",
    "\n",
    "all_gold_background_extractor=CandidateExtractor(GoldBackground, [ngrams], None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to differentiate ``Background`` with ``Purpose``. Recall our ``Background`` definition from Google Doc:\n",
    "\n",
    "``Contains words that indicate prior work (e.g., “traditionally”, “researchers have…”), and then following sentence/span starts with some variant of “In this paper, we introduce…” (Exploring adjacency relationship through helper function in Snorkel)``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove if necessary \n",
    "# session.query(Background).all()\n",
    "import snorkel.models.candidate as candidate\n",
    "\n",
    "def del_defined_candidate_class(class_name):\n",
    "    print(\"Existing\", candidate.candidate_subclasses)\n",
    "    del(candidate.candidate_subclasses[class_name])\n",
    "    print(\"After deletion\", candidate.candidate_subclasses)\n",
    "    \n",
    "## Usage:    \n",
    "# del_defined_candidate_class('Purpose')\n",
    "\n",
    "non_comma_matcher=DictionaryMatch(d=[','],longest_match_only=True,reverse=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells describe our ``Span Matching`` phase: we respectively define the `Span` for the 5 segments are , and centrally stored in a document-id-indexed hashmap. This helps us to group the 5 segments back into documents.\n",
    "3. Only those documents that have at least 3 segments are considered as valid\n",
    "\n",
    "Following this ``Span Matching`` phase, we will be having an ``Document Aggregation`` phase, where adjacency relationship and (or) waterfall models are being emphasized through multiple criteria. For example, the following criteria prevents non-abstracts (e.g. proceeding cover letter, tutorial introduction, etc.) or less well-strctured abstracts from being included. \n",
    "\n",
    "1. Only those documents that have all 5 segments are considered as valid\n",
    "2. Only those documents that have at least 4 segments are cosidered as valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 28s, sys: 472 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 148**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 9.88 s, sys: 220 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 15**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 460 ms, sys: 22.7 ms, total: 483 ms\n",
      "Wall time: 474 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 0**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/148 Candidate/Span:**\t`Background(Span(\"b'The intersection of these domains should enable researchers to foster an improved understanding of student learning'\", sentence=12057, chars=[0,114], words=[0,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe intersection of these domains should enable researchers to foster an improved understanding of student learning, lead to the creation of more natural and enriching learning interfaces, and motivate the development of novel techniques for tackling challenges that are specific of education.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d5e8d0>, 'name': '5834868625ff05a97b01354e', 'stable_id': '5834868625ff05a97b01354e::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1886, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b01354e,0,b'This summary describes the 1st International Workshop on Multimodal Learning Analytics.'), Sentence(Document 5834868625ff05a97b01354e,1,b'This area of study brings together the technologies of multimodal analysis with the learning sciences.'), Sentence(Document 5834868625ff05a97b01354e,2,b'The intersection of these domains should enable researchers to foster an improved understanding of student learning, lead to the creation of more natural and enriching learning interfaces, and motivate the development of novel techniques for tackling challenges that are specific of education.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/148 Candidate/Span:**\t`Background(Span(\"b'researchers appear to be intrigued with the question; Given a set of points'\", sentence=5803, chars=[9,83], words=[3,16]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOf late, researchers appear to be intrigued with the question; Given a set of points, what is the region occupied by them?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1113a94e0>, 'name': '5834868425ff05a97b00c32e', 'stable_id': '5834868425ff05a97b00c32e::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 72, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00c32e,0,b'Of late, researchers appear to be intrigued with the question; Given a set of points, what is the region occupied by them?'), Sentence(Document 5834868425ff05a97b00c32e,1,b'The answer appears to be neither straight forward nor unique.'), Sentence(Document 5834868425ff05a97b00c32e,2,b'Convex hull, which gives a convex enclosure of the given set, concave hull, which generates non-convex polygons and other variants such as \\xce\\xb1-hull, poly hull, r-shape and s-shape etc. have been proposed.'), Sentence(Document 5834868425ff05a97b00c32e,3,b'In this paper, we extend the question of finding a minimum area enclosure (MAE) to a set of closed planar freeform curves, not resorting to sampling them.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/148 Candidate/Span:**\t`Background(Span(\"b'The feature-interaction problem has been keeping researchers and practitioners in suspense for years.'\", sentence=13452, chars=[0,100], words=[0,15]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe feature-interaction problem has been keeping researchers and practitioners in suspense for years."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d8dba8>, 'name': '5834868625ff05a97b012212', 'stable_id': '5834868625ff05a97b012212::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 2305, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b012212,0,b'The feature-interaction problem has been keeping researchers and practitioners in suspense for years.'), Sentence(Document 5834868625ff05a97b012212,1,b'Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems.'), Sentence(Document 5834868625ff05a97b012212,2,b'In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/148 Candidate/Span:**\t`Background(Span(\"b'previous work in this domain'\", sentence=13892, chars=[7,34], words=[1,5]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tUnlike previous work in this domain, which focused on taking one reference sample and doing user authentication based on the reference sample only, we continuously sample user input and use the data for identi cation and further learning and re nement of the user model.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x127d9b550>, 'name': '5834868725ff05a97b0141e7', 'stable_id': '5834868725ff05a97b0141e7::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 2438, 'type': 'document', 'sentences': [Sentence(Document 5834868725ff05a97b0141e7,0,b'We analyze keystroke latency patterns to identify the person typing on the keyboard.'), Sentence(Document 5834868725ff05a97b0141e7,1,b'Unlike previous work in this domain, which focused on taking one reference sample and doing user authentication based on the reference sample only, we continuously sample user input and use the data for identi cation and further learning and re nement of the user model.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Background: \n",
    "Background = candidate_subclass('Background', ['background_cue'])\n",
    "transition_word=DictionaryMatch(d=['while','unlike','despite'],longest_match_only=True) \n",
    "transition_prev_work=DictionaryMatch(d=['previous','earlier','past'],longest_match_only=True) \n",
    "dict_background_matcher=DictionaryMatch(d=['previous work','traditionally','researchers'],longest_match_only=True) \n",
    "excluded_dict_background_matcher=DictionaryMatch(d=['we','unlike','our'],longest_match_only=True,reverse=True) \n",
    "non_comma_dict_background_matcher=CandidateExtractor(Background, [ngrams], [Intersection(non_comma_matcher,Union(dict_background_matcher,Intersection(transition_word,transition_prev_work)),excluded_dict_background_matcher)])\n",
    "background_cands=extract_and_display(non_comma_dict_background_matcher,Background,\"Background\",document_breakdown_map)\n",
    "# print(document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'snorkel.models.candidate.Purpose'>\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 1min 35s, sys: 1.38 s, total: 1min 37s\n",
      "Wall time: 1min 38s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1501**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 11.8 s, sys: 468 ms, total: 12.3 s\n",
      "Wall time: 12.2 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 203**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 761 ms, sys: 117 ms, total: 878 ms\n",
      "Wall time: 878 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1501 Candidate/Span:**\t`Purpose(Span(\"b'In this paper'\", sentence=21545, chars=[0,12], words=[0,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11d85b278>, 'name': '58cb6b8ec50f90cdd3875b14', 'stable_id': '58cb6b8ec50f90cdd3875b14::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4540, 'type': 'document', 'sentences': [Sentence(Document 58cb6b8ec50f90cdd3875b14,0,b'Visual inspection is, by far, the most widely used method in aircraft surface inspection.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,1,b'We are currently developing a prototype remote visual inspection system, designed to facilitate testing the hypothesized feasibility and advantages of remote visual inspection of aircraft surfaces.'), Sentence(Document 58cb6b8ec50f90cdd3875b14,2,b'In this paper, we describe several experiments with image understanding algorithms that were developed to aid remote visual inspection, in enhancing and recognizing surface cracks and corrosion from the live imagery of an aircraft surface.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1501 Candidate/Span:**\t`Purpose(Span(\"b'\\xe2\\x80\\x94here we examine shrinkage toward diagonality.\\n'\", sentence=11574, chars=[0,46], words=[0,8]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\t—here we examine shrinkage toward diagonality.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122c40f0>, 'name': '5834868625ff05a97b0122ab', 'stable_id': '5834868625ff05a97b0122ab::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1681, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b0122ab,0,b'The problem of estimating a covariance matrix in small samples has been considered by several authors following early work by Stein.'), Sentence(Document 5834868625ff05a97b0122ab,1,b'This problem can be especially important in hierarchical models where the standard errors of fixed and random effects depend on estimation of the covariance matrix of the distribution of the random effects.'), Sentence(Document 5834868625ff05a97b0122ab,2,b'We propose a set of hierarchical priors (HPs) for the covariance matrix that produce posterior shrinkage toward a specified structure'), Sentence(Document 5834868625ff05a97b0122ab,3,b'\\xe2\\x80\\x94here we examine shrinkage toward diagonality.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1501 Candidate/Span:**\t`Purpose(Span(\"b'which extend organisational boundaries.'\", sentence=6712, chars=[119,157], words=[19,23]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe quest for scalability, reliability and cost reduction has led to the development of massively distributed systems, which extend organisational boundaries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x111488240>, 'name': '5834868425ff05a97b00e0fe', 'stable_id': '5834868425ff05a97b00e0fe::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 315, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00e0fe,0,b'Modern computing paradigms have frequently adopted concepts from distributed systems.'), Sentence(Document 5834868425ff05a97b00e0fe,1,b'The quest for scalability, reliability and cost reduction has led to the development of massively distributed systems, which extend organisational boundaries.'), Sentence(Document 5834868425ff05a97b00e0fe,2,b'Voluntary computing environments (such as BOINC), Grids (such as EGEE and Globus), and more recently Cloud Computing (both open source and commercial) have established themselves as a range of distributed systems.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1501 Candidate/Span:**\t`Purpose(Span(\"b'however'\", sentence=12099, chars=[5,11], words=[2,2]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tFew, however, allow new interfaces to be created from scratch because they do not provide a means of demonstrating when a recorded macro should be invoked."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1122fb828>, 'name': '5834868725ff05a97b013a8a', 'stable_id': '5834868725ff05a97b013a8a::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1836, 'type': 'document', 'sentences': [Sentence(Document 5834868725ff05a97b013a8a,0,b'Many programming by demonstration (PBD) systems elaborate on the idea of macro recording, and they allow users to extend existing applications.'), Sentence(Document 5834868725ff05a97b013a8a,1,b'Few, however, allow new interfaces to be created from scratch because they do not provide a means of demonstrating when a recorded macro should be invoked.'), Sentence(Document 5834868725ff05a97b013a8a,2,b'This paper discusses stimulus-response systems that allow both the when (stimulus/event) and the what (response macro) to be demonstrated.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21999fd0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26962, 'split': 2, 'id': 1705}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990080>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26963, 'split': 2, 'id': 1706}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219900f0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26964, 'split': 2, 'id': 1707}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990160>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26965, 'split': 2, 'id': 1708}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219901d0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26966, 'split': 2, 'id': 1709}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990240>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26967, 'split': 2, 'id': 1710}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a219902b0>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26968, 'split': 2, 'id': 1711}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990320>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26969, 'split': 2, 'id': 1712}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990390>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26970, 'split': 2, 'id': 1713}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a21990400>, 'type': 'purpose', 'purpose_cue_cid': None, 'purpose_cue_id': 26971, 'split': 2, 'id': 1714}\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Purpose:\n",
    "Purpose=candidate_subclass('Purpose',['purpose_cue'])\n",
    "\n",
    "transition_regex_matcher=RegexMatchSpan(rgx=\"((^|\\s)however.*$)|((^|\\s)but(?!(also))*$)\",longest_match_only=True)  # Correction: purpose \n",
    "excluded_dict_purpose_matcher=SentenceMatch(d=['but also','but without','but sometimes'],longest_match_only=True,reverse=True)  # the parent sentence shall not include \"but also\"\n",
    "transition_matcher=Intersection(transition_regex_matcher,excluded_dict_purpose_matcher)\n",
    "\n",
    "comparative_degree_matcher=Intersection(RegexMatchSpan(rgx=\"(.*more.*than.*$)|(.*er than.*$)\",longest_match_only=True),transition_prev_work)  # Correction: purpose \n",
    "other_regex_matcher=RegexMatchSpan(rgx=\"(.*extend.*$)|(.*offer.*$)\",longest_match_only=True)\n",
    "\n",
    "dict_purpose_matcher=DictionaryMatch(d=['in this paper','in the paper',' that can ','in this study','to examine','we examine','to investigate','implications'],longest_match_only=True) \n",
    "\n",
    "# Unit test\n",
    "# non_comma_dict_purpose_matcher=CandidateExtractor(Purpose, [ngrams], [Intersection(non_comma_matcher,other_regex_matcher)])\n",
    "\n",
    "non_comma_dict_purpose_matcher=CandidateExtractor(Purpose, [ngrams], [Intersection(non_comma_matcher,Union(comparative_degree_matcher,other_regex_matcher,dict_purpose_matcher,transition_matcher))]) #,intersection(excluded_dict_purpose_matcher,transition_regex_matcher)])\n",
    "purpose_cands=extract_and_display(non_comma_dict_purpose_matcher,Purpose,\"Purpose\",document_breakdown_map)\n",
    "# print(document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 54.1 s, sys: 852 ms, total: 54.9 s\n",
      "Wall time: 55.7 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1404**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 7.21 s, sys: 422 ms, total: 7.63 s\n",
      "Wall time: 7.52 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 169**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 475 ms, sys: 103 ms, total: 578 ms\n",
      "Wall time: 609 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1404 Candidate/Span:**\t`Mechanism(Span(\"b'We introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses.'\", sentence=11021, chars=[0,131], words=[0,22]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tWe introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10fe465c0>, 'name': '5834868625ff05a97b011292', 'stable_id': '5834868625ff05a97b011292::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1521, 'type': 'document', 'sentences': [Sentence(Document 5834868625ff05a97b011292,0,b'We introduce a novel approach for resolving coreference when the trigger word refers to multiple (sometimes non-contiguous) clauses.'), Sentence(Document 5834868625ff05a97b011292,1,b'Our approach is completely unsupervised, and our experiments show that Neural Network models perform much better (about 20% more accurate) than traditional feature-rich baseline models.'), Sentence(Document 5834868625ff05a97b011292,2,b'We also present a new dataset for Biomedical Language Processing which, with only about 25% of the original corpus vocabulary, still captures the essential distributional semantics of the corpus.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1404 Candidate/Span:**\t`Mechanism(Span(\"b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n'\", sentence=18541, chars=[0,198], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tAn algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2a2d30>, 'name': '5834868825ff05a97b01794c', 'stable_id': '5834868825ff05a97b01794c::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3693, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01794c,0,b'In this paper we describe an approach towards integrating mid-range sensing data into a dynamic path planning algorithm.'), Sentence(Document 5834868825ff05a97b01794c,1,b'The key problem, sensing for planning is addressed in the context of outdoor navigation.'), Sentence(Document 5834868825ff05a97b01794c,2,b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1404 Candidate/Span:**\t`Mechanism(Span(\"b'Our approach appears more practical than previous metamodule-based approaches.'\", sentence=17754, chars=[0,77], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tOur approach appears more practical than previous metamodule-based approaches."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10fc3b9e8>, 'name': '5834868825ff05a97b0175d2', 'stable_id': '5834868825ff05a97b0175d2::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3472, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b0175d2,0,b'Abstract\\xe2\\x80\\x93'), Sentence(Document 5834868825ff05a97b0175d2,1,b'We describe a new set of prismatic movement primitives for cubic modular robots.'), Sentence(Document 5834868825ff05a97b0175d2,2,b'Our approach appears more practical than previous metamodule-based approaches.'), Sentence(Document 5834868825ff05a97b0175d2,3,b\"We also describe recent hardware developments in our cu-bic robot modules that have sufficient stiffness and actuator strength so that when they work together they can realize, in earth's gravity, all of the motion primitives we describe here.\\n\")]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1404 Candidate/Span:**\t`Mechanism(Span(\"b'In contrast to existing approaches that rely only on computer vision'\", sentence=19748, chars=[0,67], words=[0,10]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tIn contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b341208>, 'name': '5834868825ff05a97b01791b', 'stable_id': '5834868825ff05a97b01791b::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4036, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01791b,0,b'In robotics research, perception is one of the most challenging tasks.'), Sentence(Document 5834868825ff05a97b01791b,1,b'In contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates.'), Sentence(Document 5834868825ff05a97b01791b,2,b'To evaluate, we apply this idea to a door detection problem.'), Sentence(Document 5834868825ff05a97b01791b,3,b'A set of preliminary experiments has been completed using software agents with real vision data.'), Sentence(Document 5834868825ff05a97b01791b,4,b'Our results demonstrate that information inferred from teammate observations significantly improves the perception precision.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7be0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3157, 'split': 2, 'mechanism_cue_id': 26962}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7ef0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3158, 'split': 2, 'mechanism_cue_id': 28446}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d7eb8>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3159, 'split': 2, 'mechanism_cue_id': 28447}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d0908>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3160, 'split': 2, 'mechanism_cue_id': 28448}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d0ac8>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3161, 'split': 2, 'mechanism_cue_id': 28449}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7d07f0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3162, 'split': 2, 'mechanism_cue_id': 28450}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f8080>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3163, 'split': 2, 'mechanism_cue_id': 28452}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f80f0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3164, 'split': 2, 'mechanism_cue_id': 28453}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f8160>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3165, 'split': 2, 'mechanism_cue_id': 28454}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1f7f81d0>, 'type': 'mechanism', 'mechanism_cue_cid': None, 'id': 3166, 'split': 2, 'mechanism_cue_id': 28451}\n"
     ]
    }
   ],
   "source": [
    "# Compound Matcher for Mechanism: \n",
    "Mechanism = candidate_subclass('Mechanism', ['mechanism_cue']) \n",
    "dict_mechanism_matcher=DictionaryMatch(d=['introduce','introduces','propose','proposes','we propose','we develop','approach'],longest_match_only=True) \n",
    "non_comma_dict_mechanism_matcher=CandidateExtractor(Mechanism, [ngrams], [Intersection(non_comma_matcher,dict_mechanism_matcher)])\n",
    "mechanism_cands=extract_and_display(non_comma_dict_mechanism_matcher,Mechanism,\"Mechanism\",document_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 53 s, sys: 864 ms, total: 53.9 s\n",
      "Wall time: 54.4 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 752**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6.82 s, sys: 364 ms, total: 7.18 s\n",
      "Wall time: 6.96 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 81**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 428 ms, sys: 71.6 ms, total: 499 ms\n",
      "Wall time: 464 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 10**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/752 Candidate/Span:**\t`Method(Span(\"b'The speech-to-speech translation is bi-directional for a two way dialog between participants.\\n'\", sentence=10296, chars=[0,93], words=[0,19]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe speech-to-speech translation is bi-directional for a two way dialog between participants.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10ff0a908>, 'name': '5834868425ff05a97b00e5b3', 'stable_id': '5834868425ff05a97b00e5b3::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 1310, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00e5b3,0,b'Jibbigo is a speech-to-speech translation application for iPhone, iPod touch, and iPad devices.'), Sentence(Document 5834868425ff05a97b00e5b3,1,b'Jibbigo allows the user to simply speak a sentence, and it speaks the sentence aloud in the other language, much like a personal human interpreter would.'), Sentence(Document 5834868425ff05a97b00e5b3,2,b'The speech-to-speech translation is bi-directional for a two way dialog between participants.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/752 Candidate/Span:**\t`Method(Span(\"b'Both computational feasibility and improvement of estimation is demonstrated in the experiments.\\n'\", sentence=6557, chars=[0,96], words=[0,13]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tBoth computational feasibility and improvement of estimation is demonstrated in the experiments.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10f01d470>, 'name': '5834868425ff05a97b00de91', 'stable_id': '5834868425ff05a97b00de91::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 271, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00de91,0,b'The concept of Support Vector Regression is extended to a more general class of convex cost functions.'), Sentence(Document 5834868425ff05a97b00de91,1,b'Moreover it is shown how the resulting convex constrained optimization problems can be efficiently solved by a Primal-Dual Interior Point path following method.'), Sentence(Document 5834868425ff05a97b00de91,2,b'Both computational feasibility and improvement of estimation is demonstrated in the experiments.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/752 Candidate/Span:**\t`Method(Span(\"b'Both experiments seemto reveal that the Metaphor has poor effectiveness.\\n'\", sentence=18726, chars=[0,72], words=[0,11]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tBoth experiments seemto reveal that the Metaphor has poor effectiveness.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2b7e80>, 'name': '5834868825ff05a97b017851', 'stable_id': '5834868825ff05a97b017851::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3743, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b017851,0,b'The Metaphor is intended to contribute to the Agile Programming value of communication.'), Sentence(Document 5834868825ff05a97b017851,1,b'Previously, some of the author studied the Metaphor as a means of communication among team members and between them and clients.'), Sentence(Document 5834868825ff05a97b017851,2,b\"This paper examines the Metaphor's contribution to the software architecture.\"), Sentence(Document 5834868825ff05a97b017851,3,b'Both experiments seemto reveal that the Metaphor has poor effectiveness.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/752 Candidate/Span:**\t`Method(Span(\"b'The commonality amongst the experiments has permitted the ability to objectively...\\n'\", sentence=21988, chars=[0,83], words=[0,12]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe commonality amongst the experiments has permitted the ability to objectively...\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b453c88>, 'name': '59364828c50f90cdd3aca7f8', 'stable_id': '59364828c50f90cdd3aca7f8::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 4676, 'type': 'document', 'sentences': [Sentence(Document 59364828c50f90cdd3aca7f8,0,b'This paper presents an experimental evaluation and comparison of basic strategies that have been proposed for force control of robot manipulators.'), Sentence(Document 59364828c50f90cdd3aca7f8,1,b'This experimental review of force control methodologies is unique in its breadth--never has such a complete spectrum of strategies been experimentally compared on the same system.'), Sentence(Document 59364828c50f90cdd3aca7f8,2,b'The commonality amongst the experiments has permitted the ability to objectively...\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40710>, 'type': 'method', 'method_cue_cid': None, 'id': 4000, 'split': 2, 'method_cue_id': 26962}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40b00>, 'type': 'method', 'method_cue_cid': None, 'id': 4001, 'split': 2, 'method_cue_id': 29212}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40ba8>, 'type': 'method', 'method_cue_cid': None, 'id': 4002, 'split': 2, 'method_cue_id': 29213}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa40748>, 'type': 'method', 'method_cue_cid': None, 'id': 4003, 'split': 2, 'method_cue_id': 29214}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa404e0>, 'type': 'method', 'method_cue_cid': None, 'id': 4004, 'split': 2, 'method_cue_id': 29215}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49400>, 'type': 'method', 'method_cue_cid': None, 'id': 4005, 'split': 2, 'method_cue_id': 29216}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa495c0>, 'type': 'method', 'method_cue_cid': None, 'id': 4006, 'split': 2, 'method_cue_id': 29217}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49668>, 'type': 'method', 'method_cue_cid': None, 'id': 4007, 'split': 2, 'method_cue_id': 29218}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa49630>, 'type': 'method', 'method_cue_cid': None, 'id': 4008, 'split': 2, 'method_cue_id': 29219}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1fa494a8>, 'type': 'method', 'method_cue_cid': None, 'id': 4009, 'split': 2, 'method_cue_id': 29220}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compound Matcher for Method: \n",
    "Method = candidate_subclass('Method', ['method_cue'])\n",
    "\n",
    "dict_method_matcher=DictionaryMatch(d=['dataset','benchmark','experiment ','experiments',\"empirical\",\"participant\",\"survey\",\" conduct\",\" analyze\"],longest_match_only=True) \n",
    "\n",
    "non_comma_dict_method_matcher=CandidateExtractor(Method, [ngrams], [Intersection(non_comma_matcher,dict_method_matcher)])\n",
    "method_cands=extract_and_display(non_comma_dict_method_matcher,Method,\"Method\",document_breakdown_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 56.2 s, sys: 803 ms, total: 57 s\n",
      "Wall time: 57.5 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 0 - number of candidates extracted: 1753**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6.98 s, sys: 347 ms, total: 7.32 s\n",
      "Wall time: 7.12 s\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 1 - number of candidates extracted: 176**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 469 ms, sys: 94.4 ms, total: 564 ms\n",
      "Wall time: 529 ms\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Split 2 - number of candidates extracted: 12**\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**0/1753 Candidate/Span:**\t`Finding(Span(\"b'The effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented.'\", sentence=5983, chars=[0,173], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tThe effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x10ea76da0>, 'name': '5834868425ff05a97b00c935', 'stable_id': '5834868425ff05a97b00c935::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 117, 'type': 'document', 'sentences': [Sentence(Document 5834868425ff05a97b00c935,0,b'This paper proposes a fifteen level H-Bridge cascaded multilevel inverter with fundamental frequency switching for low power applications such as solar powered power supplies, battery powered standby power supplies.'), Sentence(Document 5834868425ff05a97b00c935,1,b'The effect of the variation of gate pulse on the performance of the inverter for different conditions of gate pulse variation is studied and simulation results are presented.'), Sentence(Document 5834868425ff05a97b00c935,2,b'Experimental...\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**1/1753 Candidate/Span:**\t`Finding(Span(\"b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n'\", sentence=18541, chars=[0,198], words=[0,28]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tAn algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b2a2d30>, 'name': '5834868825ff05a97b01794c', 'stable_id': '5834868825ff05a97b01794c::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 3693, 'type': 'document', 'sentences': [Sentence(Document 5834868825ff05a97b01794c,0,b'In this paper we describe an approach towards integrating mid-range sensing data into a dynamic path planning algorithm.'), Sentence(Document 5834868825ff05a97b01794c,1,b'The key problem, sensing for planning is addressed in the context of outdoor navigation.'), Sentence(Document 5834868825ff05a97b01794c,2,b'An algorithmic approach is described towards solving these problems and both simulation results and initial experimental results for outdoor navigation using wide baseline stereo data are presented.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**2/1753 Candidate/Span:**\t`Finding(Span(\"b'a people finder application'\", sentence=23141, chars=[123,149], words=[18,21]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tMore specifically, we have developed and evaluated three different applications, including a contextual instant messenger, a people finder application, and a phone-based application for access control."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b4e66a0>, 'name': '59382e72c50f90cdd3ada1ca', 'stable_id': '59382e72c50f90cdd3ada1ca::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5003, 'type': 'document', 'sentences': [Sentence(Document 59382e72c50f90cdd3ada1ca,0,b'We describe our current work in developing novel mechanisms for managing security and privacy in pervasive computing environments.'), Sentence(Document 59382e72c50f90cdd3ada1ca,1,b'More specifically, we have developed and evaluated three different applications, including a contextual instant messenger, a people finder application, and a phone-based application for access control.'), Sentence(Document 59382e72c50f90cdd3ada1ca,2,b'We also draw out some themes we have learned thus far for user-controllable security and privacy.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**3/1753 Candidate/Span:**\t`Finding(Span(\"b'We show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\\n'\", sentence=24335, chars=[0,241], words=[0,45]))`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Sentence's text:**\tWe show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Its parent Document's text:**\t{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x11b5852b0>, 'name': '595d391dc50f90cdd3bdeda2', 'stable_id': '595d391dc50f90cdd3bdeda2::document:0:0', 'meta': {'file_name': '70kpaper_061418_cleaned_noBookLecture.tsv'}, 'id': 5357, 'type': 'document', 'sentences': [Sentence(Document 595d391dc50f90cdd3bdeda2,0,b'We empirically explore the application of (adaptations of) these algorithms to the kidney exchange problem, where patients with end-stage renal failure swap willing but incompatible donors.'), Sentence(Document 595d391dc50f90cdd3bdeda2,1,b'We show on both generated data and on real data from the first 169 match runs of the UNOS nationwide kidney exchange that even a very small number of non-adaptive edge queries per vertex results in large gains in expected successful matches.\\n')]}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff899e8>, 'type': 'finding', 'id': 5939, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30865}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff89a58>, 'type': 'finding', 'id': 5940, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30866}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82f28>, 'type': 'finding', 'id': 5941, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 29216}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82ef0>, 'type': 'finding', 'id': 5942, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30867}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff828d0>, 'type': 'finding', 'id': 5943, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30868}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82f60>, 'type': 'finding', 'id': 5944, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30869}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82128>, 'type': 'finding', 'id': 5945, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30870}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff825f8>, 'type': 'finding', 'id': 5946, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30872}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82438>, 'type': 'finding', 'id': 5947, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30871}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82160>, 'type': 'finding', 'id': 5948, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30873}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82be0>, 'type': 'finding', 'id': 5949, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30874}\n",
      "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x1a1ff82400>, 'type': 'finding', 'id': 5950, 'split': 2, 'finding_cue_cid': None, 'finding_cue_id': 30864}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compound Matcher for Finding: \n",
    "Finding = candidate_subclass('Finding', ['finding_cue'])\n",
    "\n",
    "dict_finding_matcher=DictionaryMatch(d=['show that','shows that','found','indicate','results','performance','find'],longest_match_only=True) \n",
    "\n",
    "non_comma_dict_finding_matcher=CandidateExtractor(Finding, [ngrams], [Intersection(non_comma_matcher,dict_finding_matcher)])\n",
    "finding_cands=extract_and_display(non_comma_dict_finding_matcher,Finding,\"Finding\",document_breakdown_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the ``Document Aggregation`` phase. First we show a few document examples, of how ``document_breakdown_map`` looks like, as well as how do we aggregate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Document 0/3039\n",
      "Extracted segments\n",
      "\n",
      " 59398333c50f90cdd3ae2761 {'Background': [Background(Span(\"b'recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'\", sentence=24132, chars=[83,244], words=[13,36]))], 'Purpose': [Purpose(Span(\"b'we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'\", sentence=24132, chars=[80,244], words=[12,36]))], 'Mechanism': [Mechanism(Span(\"b'We developed Cairo to fill this need.'\", sentence=24133, chars=[0,36], words=[0,7]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 59398333c50f90cdd3ae2761,0,b'While developing a suite of tools for statistical machine translation research, we recognized the need for a visualization tool that would allow researchers to examine and evaluate specific word correspondences generated by a translation system.'), Sentence(Document 59398333c50f90cdd3ae2761,1,b'We developed Cairo to fill this need.'), Sentence(Document 59398333c50f90cdd3ae2761,2,b'Cairo is a free, open-source, portable, user-friendly, GUI-driven program written in Java that provides a visual representation of word correspondences between bilingual pairs of sentences, as well as relevant translation model parameters.\\n')]\n",
      "\n",
      "\n",
      "Document 1/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868725ff05a97b014e76 {'Background': [Background(Span(\"b'Researchers in psychometrics argue that before adding new labels to applications'\", sentence=12535, chars=[0,79], words=[0,10]))], 'Purpose': [Purpose(Span(\"b'In this paper'\", sentence=12536, chars=[0,12], words=[0,2]))], 'Method': [Method(Span(\"b'the labels must be empirically evaluated.'\", sentence=12535, chars=[82,122], words=[12,18])), Method(Span(\"b'We also show how we evaluate the labels empirically using a sample population from Amazon Mechanical Turk users.\\n'\", sentence=12537, chars=[0,112], words=[0,19]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868725ff05a97b014e76,0,b'Linguistic labels such as high, medium, and low are commonly used in different applications.'), Sentence(Document 5834868725ff05a97b014e76,1,b'Researchers in psychometrics argue that before adding new labels to applications, the labels must be empirically evaluated.'), Sentence(Document 5834868725ff05a97b014e76,2,b'In this paper, we explain the process of selecting labels for a security assessment application.'), Sentence(Document 5834868725ff05a97b014e76,3,b'We also show how we evaluate the labels empirically using a sample population from Amazon Mechanical Turk users.\\n')]\n",
      "\n",
      "\n",
      "Document 2/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868625ff05a97b013640 {'Background': [Background(Span(\"b'Previous work explored semantic concepts for content analysis to assist retrieval.'\", sentence=12041, chars=[0,81], words=[0,11]))], 'Purpose': [Purpose(Span(\"b'However'\", sentence=12042, chars=[0,6], words=[0,0]))], 'Mechanism': [Mechanism(Span(\"b'we propose a semi-automatic framework to discover the semantic concepts.\\n'\", sentence=12044, chars=[25,97], words=[5,18]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868625ff05a97b013640,0,b'Huge amount of videos on the Internet have rare textual information, which makes video retrieval challenging given a text query.'), Sentence(Document 5834868625ff05a97b013640,1,b'Previous work explored semantic concepts for content analysis to assist retrieval.'), Sentence(Document 5834868625ff05a97b013640,2,b\"However, the human-defined concepts might fail to cover the data and there is a potential gap between these concepts and the semantics expected from user's query.\"), Sentence(Document 5834868625ff05a97b013640,3,b'Also, building a corpus is expensive and time-consuming.'), Sentence(Document 5834868625ff05a97b013640,4,b'To address these issues, we propose a semi-automatic framework to discover the semantic concepts.\\n')]\n",
      "\n",
      "\n",
      "Document 3/3039\n",
      "Extracted segments\n",
      "\n",
      " 5834868625ff05a97b011a7a {'Background': [Background(Span(\"b'Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life'\", sentence=10746, chars=[0,116], words=[0,18]))], 'Purpose': [Purpose(Span(\"b'Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life'\", sentence=10746, chars=[0,116], words=[0,18])), Purpose(Span(\"b'However'\", sentence=10747, chars=[0,6], words=[0,0]))], 'Mechanism': [Mechanism(Span(\"b'Problem-solving approaches differ radically'\", sentence=10748, chars=[0,42], words=[0,5]))], 'Finding': [Finding(Span(\"b'and finding common ground for assessing results can be difficult.\\n'\", sentence=10748, chars=[45,110], words=[7,18]))]} \n",
      "\n",
      "\n",
      "Complete abstract\n",
      "\n",
      " [Sentence(Document 5834868625ff05a97b011a7a,0,b\"Because researchers embed successful ubicomp projects in rich real-world contexts that can touch many aspects of life, they've made multidisciplinary teams the norm rather than the exception.\"), Sentence(Document 5834868625ff05a97b011a7a,1,b'However, overcoming boundaries between various disciplines is a significant challenge and in many cases represents a key factor for successful development.'), Sentence(Document 5834868625ff05a97b011a7a,2,b'Problem-solving approaches differ radically, and finding common ground for assessing results can be difficult.\\n')]\n"
     ]
    }
   ],
   "source": [
    "for ind,docid in enumerate(document_breakdown_map.keys()):\n",
    "    print(\"\\n\\nDocument \"+str(ind)+\"/\"+str(len(document_breakdown_map.keys())))\n",
    "    print(\"Extracted segments\\n\\n\",docid,document_breakdown_map[docid],\"\\n\\n\")\n",
    "    print(\"Complete abstract\\n\\n\",document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent().sentences)\n",
    "    if ind>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we count the number of documents that contain exactly N segments, where N=1, 2, 3, 4, 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_text(doc):\n",
    "    text_string=\" \".join([str(sentence.text) for sentence in doc.sentences])\n",
    "    return text_string\n",
    "\n",
    "\n",
    "def rank_by_matched_segments(document_breakdown_map):\n",
    "    for n in [5,4,3,2,1]:\n",
    "        print(\"Below are one or two document examples that contain exactly \"+str(n)+\" segments\\n\")\n",
    "        showed_examples=0\n",
    "        count=0\n",
    "        for ind,docid in enumerate(document_breakdown_map.keys()):\n",
    "            if len(document_breakdown_map[docid].keys())==n:\n",
    "                count+=1\n",
    "                if showed_examples<2:\n",
    "                    showed_examples+=1\n",
    "                    print(docid,\": \", get_document_text(document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent()))\n",
    "                    print(\"Extracted segments\\n\\n\",docid,document_breakdown_map[docid],\"\\n\\n\")\n",
    "        print(\"Total count is \"+str(count)+\"\\n\\n=================\\n\")\n",
    "        \n",
    "# rank_by_matched_segments(document_breakdown_map)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to some visualization of extracted Span as highlighted text on Document.\n",
    "\n",
    "Color notation: <b style=\"color:orange;\">Background</b> <b style=\"color:pink;\">Purpose</b> <b style=\"color:green;\">Mechanism</b> <b style=\"color:purple;\">Method</b> <b style=\"color:blue;\">Findings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "    \n",
    "def print_colored_text(docid,document_breakdown_map=document_breakdown_map):\n",
    "    \n",
    "    color_mapping={\"Background\":\"orange\",\"Purpose\":\"pink\",\"Mechanism\":\"green\",\"Method\":\"purple\",\"Finding\":\"blue\"}\n",
    "    this_document=document_breakdown_map[docid][list(document_breakdown_map[docid].keys())[0]][0].get_parent().get_parent()\n",
    "    document_text=get_document_text(this_document)\n",
    "\n",
    "    added_segment=[]\n",
    "    print(\"This document has \"+str(len(document_breakdown_map[docid]))+\" spans\")\n",
    "    for segment in document_breakdown_map[docid]:\n",
    "        spans=document_breakdown_map[docid][segment]\n",
    "#         print(segment)\n",
    "        for ind,span in enumerate(spans):\n",
    "#             print(span.__dict__)\n",
    "            this_span=document_breakdown_map[docid][segment][ind].__dict__[list(document_breakdown_map[docid][segment][ind].__dict__)[6]]\n",
    "            span_sentence_text=this_span.sentence.text\n",
    "            span_text=str(span_sentence_text[this_span.char_start:(this_span.char_end+1)])\n",
    "#             print(\"span_text is \"+span_text)\n",
    "            document_text=document_text.replace(span_text.strip(),\"<font color='\"+color_mapping[segment]+\"'>\"+span_text.strip()+\"</font>\")\n",
    "\n",
    "    printmd(document_text)\n",
    "\n",
    "# docid='5834868625ff05a97b013016'\n",
    "# print_colored_text(docid)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document has 2 spans\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Security requirements analysis depends on how well-trained analysts perceive security risk, understand the impact of various vulnerabilities, and mitigate threats. When systems are composed of multiple machines, configurations, and software components that interact with each other, risk perception must account for the composition of security requirements. In this paper, we report on how changes to security requirements affect analysts risk perceptions and their decisions about how to modify the requirements to reach adequate security levels. <font color='purple'>We conducted two user surveys of 174 participants wherein participants assess security levels across 64 factorial vignettes.</font> <font color='purple'><font color='blue'>We analyzed the survey results using multi-level modeling to test for the effect of security requirements composition on participants' overall security adequacy ratings and on their ratings of individual requirements.</font></font> We accompanied this analysis with grounded analysis of elicited requirements aimed at lowering the security risk. <font color='blue'>Our results suggest that requirements composition affects experts' adequacy ratings on security requirements.</font> In addition, we identified three categories of requirements modifications, called refinements, replacements and reinforcements, and we measured how these categories compare with overall perceived security risk. Finally, we discuss the future impact of our work in security requirements assessment practice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_background_cands=extract_and_display(non_comma_dict_background_matcher,Background,\"Background\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_purpose_cands=extract_and_display(non_comma_dict_purpose_matcher,Purpose,\"Purpose\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_mechanism_cands=extract_and_display(non_comma_dict_mechanism_matcher,Mechanism,\"Mechanism\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_method_cands=extract_and_display(non_comma_dict_method_matcher,Method,\"Method\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "# test_finding_cands=extract_and_display(non_comma_dict_finding_matcher,Finding,\"Finding\",document_breakdown_map=test_doc_breakdown_map,selected_split=2,is_print=False)\n",
    "\n",
    "print_colored_text('cscw18assessment',document_breakdown_map=test_doc_breakdown_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
